<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-30T02:24:03Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Grokitach/Stalker_GAMMA</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/Grokitach/Stalker_GAMMA</id>
    <link href="https://github.com/Grokitach/Stalker_GAMMA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;S.T.A.L.K.E.R. Anomaly G.A.M.M.A. modpack definition files.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stalker GAMMA&lt;/h1&gt; &#xA;&lt;p&gt;S.T.A.L.K.E.R. Anomaly G.A.M.M.A. modpack definition files.&lt;/p&gt; &#xA;&lt;h2&gt;Get the client to use this repository&lt;/h2&gt; &#xA;&lt;p&gt;G.A.M.M.A. stands for Grok&#39;s Automated Modular Modpack for Anomaly.&lt;/p&gt; &#xA;&lt;p&gt;To get the latest Stalker GAMMA client and install intructions, head to the associated discord server:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/KT2bEHZQbh&#34;&gt;https://discord.gg/KT2bEHZQbh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;G.A.M.M.A. aims at providing a balanced survival, scavenging, cooking, crafting and repairing focused experience with a long progression and smooth gameplay.&lt;/p&gt; &#xA;&lt;p&gt;For people accustomed to Skyrim modding, G.A.M.M.A. is basically Wabbajack remade from scratch in PowerShell + a modlist of 200+ addons and its own developed modular add-ons for STALKER Anomaly.&lt;/p&gt; &#xA;&lt;p&gt;This modpack is 100% copyright free since it doesn&#39;t redistribute any copyrighted content developed by other modders. Indeed, G.A.M.M.A. downloads every add-on directly from moddb or github, and installs them automatically and in the correct order in a Mod Organizer 2 (MO2) instance. You will thus directly support modders by installing G.A.M.M.A.&lt;/p&gt; &#xA;&lt;p&gt;G.A.M.M.A. is easy to use, since by simply executing 3 scripts and following basic instructions, you&#39;ll find on your desktop the G.A.M.M.A. icon which will launch a MO2 instance directly linked to your STALKER Anomaly folder. From there, you will find every add-on installed and be able to deactivate the ones you don&#39;t like, or add new add-ons. This is especially easy since all add-ons used in G.A.M.M.A. are separated by their type, i.e. Audio, Gameplay, Gunplay, etc.&lt;/p&gt; &#xA;&lt;p&gt;This modpack also features its own new modular set of add-ons including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A new UI.&lt;/li&gt; &#xA; &lt;li&gt;A rebalanced survival experience focused on crafting, repairing and cooking.&lt;/li&gt; &#xA; &lt;li&gt;A new economy where traders will only sell you supplies, and where good ammunition will only be available to trustful stalkers.&lt;/li&gt; &#xA; &lt;li&gt;New starting loadouts focused on providing pistols, SMGs or WW2 guns to have a smoother start with calibers for which you can buy ammo. It also gives you the basic tools to wander the Zone, which is nice for Ironman runs.&lt;/li&gt; &#xA; &lt;li&gt;Rebalanced close quarter combat to make knives more useful and to free even more inventory slots, yup you can equip 2 pistols and 2 main weapons and still profit from your best knife with a reworked quick melee motion and system.&lt;/li&gt; &#xA; &lt;li&gt;Rebalanced actor damage.&lt;/li&gt; &#xA; &lt;li&gt;Rebalanced sleep.&lt;/li&gt; &#xA; &lt;li&gt;Rebalanced Skill System from Haruka.&lt;/li&gt; &#xA; &lt;li&gt;Overhauled radiations effects and dynamic radiation areas.&lt;/li&gt; &#xA; &lt;li&gt;Many fixes for some major addons including the Boomsticks and Sharpsticks gunpack.&lt;/li&gt; &#xA; &lt;li&gt;And more!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>wowchemy/starter-hugo-research-group</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/wowchemy/starter-hugo-research-group</id>
    <link href="https://github.com/wowchemy/starter-hugo-research-group" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üë• ËΩªÊùæÂàõÂª∫Á†îÁ©∂ÁªÑÊàñÁªÑÁªáÁΩëÁ´ô Create a stunning Research Group, Team, or Organization Website with Hugo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/wowchemy/starter-hugo-research-group&#34;&gt;Hugo Research Group Theme&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-research-group/main/preview.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Research Group Template&lt;/strong&gt; empowers your research group to easily create a beautiful website with a stunning homepage, news, academic publications, events, team profiles, and a contact form.&lt;/p&gt; &#xA;&lt;p&gt;Ô∏è&lt;strong&gt;Trusted by 250,000+ researchers, educators, and students.&lt;/strong&gt; Highly customizable via the integrated &lt;strong&gt;no-code, widget-based Wowchemy page builder&lt;/strong&gt;, making every site truly personalized ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Get%20started-ff4655?style=for-the-badge&#34; alt=&#34;Get Started&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/channels/722225264733716590/742892432458252370/742895548159492138&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/722225264733716590?style=for-the-badge&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/wowchemy&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/wowchemy?label=Follow%20on%20Twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Easily write technical content with plain text Markdown, LaTeX math, diagrams, RMarkdown, or Jupyter, and import publications from BibTeX.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://research-group.netlify.app/&#34;&gt;Check out the latest demo&lt;/a&gt; of what you&#39;ll get in less than 60 seconds, or &lt;a href=&#34;https://wowchemy.com/creators/&#34;&gt;view the showcase&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The integrated &lt;a href=&#34;https://wowchemy.com&#34;&gt;&lt;strong&gt;Wowchemy&lt;/strong&gt;&lt;/a&gt; website builder and CMS makes it easy to create a beautiful website for free. Edit your site in the CMS (or your favorite editor), generate it with &lt;a href=&#34;https://github.com/gohugoio/hugo&#34;&gt;Hugo&lt;/a&gt;, and deploy with GitHub or Netlify. Customize anything on your site with widgets, light/dark themes, and language packs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üëâ &lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://wowchemy.com/docs/&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí¨ &lt;a href=&#34;https://discord.gg/z8wNYzb&#34;&gt;Chat with the &lt;strong&gt;Wowchemy research community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Automatically import citations from BibTeX&lt;/strong&gt; with the &lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34;&gt;Hugo Academic CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê¶ Share your new site with the community: &lt;a href=&#34;https://twitter.com/wowchemy&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üó≥ &lt;a href=&#34;https://forms.gle/NioD9VhUg7PNmdCAA&#34;&gt;Take the survey and help us improve #OpenSource&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/raw/main/.github/contributing.md&#34;&gt;Contribute improvements&lt;/a&gt; or &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues&#34;&gt;suggest improvements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚¨ÜÔ∏è &lt;strong&gt;Updating?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/releases&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;We ask you, humbly, to support this open source movement&lt;/h2&gt; &#xA;&lt;p&gt;Today we ask you to defend the open source independence of the Wowchemy website builder and themes üêß&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re an open source movement that depends on your support to stay online and thriving, but 99.9% of our creators don&#39;t give; they simply look the other way.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/sponsors/gcushen&#34;&gt;‚ù§Ô∏è Click here to become a GitHub Sponsor, unlocking awesome perks such as &lt;em&gt;exclusive academic templates and widgets&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Demo credits&lt;/h2&gt; &#xA;&lt;p&gt;Please replace the demo images with your own.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/uVnRa6mOLOM&#34;&gt;Female scientist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/kwzWjTnDPLk&#34;&gt;2 Coders&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/RnDGGnMEOao&#34;&gt;Cafe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Blog posts &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/AndE50aaHn4&#34;&gt;https://unsplash.com/photos/AndE50aaHn4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/OYzbqk2y26c&#34;&gt;https://unsplash.com/photos/OYzbqk2y26c&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Avatars &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/5yENNRbbat4&#34;&gt;https://unsplash.com/photos/5yENNRbbat4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/WNoLnJo7tS8&#34;&gt;https://unsplash.com/photos/WNoLnJo7tS8&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Wandmalfarbe/pandoc-latex-template</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/Wandmalfarbe/pandoc-latex-template</id>
    <link href="https://github.com/Wandmalfarbe/pandoc-latex-template" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A pandoc LaTeX template to convert markdown files to PDF or LaTeX.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/icon.png&#34; align=&#34;right&#34; height=&#34;110&#34;&gt; &#xA;&lt;h1&gt;Eisvogel&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/Wandmalfarbe/pandoc-latex-template&#34;&gt;&lt;img src=&#34;https://travis-ci.com/Wandmalfarbe/pandoc-latex-template.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A clean &lt;strong&gt;pandoc LaTeX template&lt;/strong&gt; to convert your markdown files to PDF or LaTeX. It is designed for lecture notes and exercises with a focus on computer science. The template is compatible with pandoc 2.&lt;/p&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A custom title page&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A basic example page&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/title-page-custom/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/title-page-custom/preview.png&#34; alt=&#34;A custom title page&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/basic-example/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/basic-example/preview.png&#34; alt=&#34;A basic example page&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install pandoc from &lt;a href=&#34;http://pandoc.org/&#34;&gt;http://pandoc.org/&lt;/a&gt;. You also need to install &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Installation#Distributions&#34;&gt;LaTeX&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the latest version of the Eisvogel template from &lt;a href=&#34;https://github.com/Wandmalfarbe/pandoc-latex-template/releases/latest&#34;&gt;the release page&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extract the downloaded ZIP archive and open the folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Move the template &lt;code&gt;eisvogel.latex&lt;/code&gt; to your pandoc templates folder. The location of the templates folder depends on your operating system:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unix, Linux, macOS: &lt;code&gt;/Users/USERNAME/.local/share/pandoc/templates/&lt;/code&gt; or &lt;code&gt;/Users/USERNAME/.pandoc/templates/&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Windows Vista or later: &lt;code&gt;C:\Users\USERNAME\AppData\Roaming\pandoc\templates\&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;If there are no folders called &lt;code&gt;templates&lt;/code&gt; or &lt;code&gt;pandoc&lt;/code&gt; you need to create them and put the template &lt;code&gt;eisvogel.latex&lt;/code&gt; inside. You can find the default user data directory on your system by looking at the output of &lt;code&gt;pandoc --version&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the terminal and navigate to the folder where your markdown file is located.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute the following command&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --from markdown --template eisvogel --listings&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;where &lt;code&gt;example.md&lt;/code&gt; is the markdown file you want to convert to PDF.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In order to have nice headers and footers you need to supply metadata to your document. You can do that with a &lt;a href=&#34;http://pandoc.org/MANUAL.html#extension-yaml_metadata_block&#34;&gt;YAML metadata block&lt;/a&gt; at the top of your markdown document (see the &lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/basic-example/document.md&#34;&gt;example markdown file&lt;/a&gt;). Your markdown document may look like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;---&#xA;title: &#34;The Document Title&#34;&#xA;author: [Example Author, Another Author]&#xA;date: &#34;2017-02-20&#34;&#xA;keywords: [Markdown, Example]&#xA;...&#xA;&#xA;Here is the actual document text...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Custom Template Variables&lt;/h3&gt; &#xA;&lt;p&gt;This template defines some new variables to control the appearance of the resulting PDF document. The existing template variables from pandoc are all supported and their documentation can be found in &lt;a href=&#34;https://pandoc.org/MANUAL.html#variables-for-latex&#34;&gt;the pandoc manual&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;turns on the title page when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage-color&lt;/code&gt;&lt;/p&gt; &lt;p&gt;the background color of the title page. The color value must be given as an HTML hex color like &lt;code&gt;D8DE2C&lt;/code&gt; without the leading number sign (&lt;code&gt;#&lt;/code&gt;). When specifying the color in YAML, it is advisable to enclose it in quotes like so &lt;code&gt;titlepage-color: &#34;D8DE2C&#34;&lt;/code&gt; to avoid the truncation of the color (e.g.&amp;nbsp;&lt;code&gt;000000&lt;/code&gt; becoming &lt;code&gt;0&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage-text-color&lt;/code&gt; (defaults to &lt;code&gt;5F5F5F&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;the text color of the title page&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage-rule-color&lt;/code&gt; (defaults to &lt;code&gt;435488&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;the color of the rule on the top of the title page&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage-rule-height&lt;/code&gt; (defaults to &lt;code&gt;4&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;the height of the rule on the top of the title page (in points)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage-logo&lt;/code&gt;&lt;/p&gt; &lt;p&gt;path to an image that will be displayed on the title page. The path is always relative to where pandoc is executed. The option &lt;code&gt;--resource-path&lt;/code&gt; has no effect.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;titlepage-background&lt;/code&gt;&lt;/p&gt; &lt;p&gt;the path to a background image for the title page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;titlepage-background&lt;/code&gt; are a few example background images.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;page-background&lt;/code&gt;&lt;/p&gt; &lt;p&gt;the path to a background image for any page. The background image is scaled to cover the entire page. In the examples folder under &lt;code&gt;page-background&lt;/code&gt; are a few example background images.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;page-background-opacity&lt;/code&gt; (defaults to &lt;code&gt;0.2&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;the background image opacity&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;caption-justification&lt;/code&gt; (defaults to &lt;code&gt;raggedright&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;justification setting for captions (uses the &lt;code&gt;justification&lt;/code&gt; parameter of the &lt;a href=&#34;https://ctan.org/pkg/caption?lang=en&#34;&gt;caption&lt;/a&gt; package)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;toc-own-page&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;begin new page after table of contents, when &lt;code&gt;true&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;listings-disable-line-numbers&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;disables line numbers for all listings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;listings-no-page-break&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;avoid page break inside listings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;disable-header-and-footer&lt;/code&gt; (default to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;disables the header and footer completely on all pages&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;header-left&lt;/code&gt; (defaults to the title)&lt;/p&gt; &lt;p&gt;the text on the left side of the header&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;header-center&lt;/code&gt;&lt;/p&gt; &lt;p&gt;the text in the center of the header&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;header-right&lt;/code&gt; (defaults to the date)&lt;/p&gt; &lt;p&gt;the text on the right side of the header&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;footer-left&lt;/code&gt; (defaults to the author)&lt;/p&gt; &lt;p&gt;the text on the left side of the footer&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;footer-center&lt;/code&gt;&lt;/p&gt; &lt;p&gt;the text in the center of the footer&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;footer-right&lt;/code&gt; (defaults to the page number)&lt;/p&gt; &lt;p&gt;the text on the right side of the footer&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;footnotes-pretty&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;prettifies formatting of footnotes (requires package &lt;code&gt;footmisc&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;footnotes-disable-backlinks&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;disables making the reference from the footnote at the bottom of the page into a link back to the occurence of the footnote in the main text (enabling requires package &lt;code&gt;footnotebackref&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;book&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;typeset as book&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;logo-width&lt;/code&gt; (defaults to &lt;code&gt;35mm&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;the width of the logo. One needs to specify the width with a (TeX) unit e.g. &lt;code&gt;100pt&lt;/code&gt; or &lt;code&gt;35mm&lt;/code&gt;. The following units can be used:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pt&lt;/code&gt;: Point&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pc&lt;/code&gt;: pica (12 &lt;code&gt;pt&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;in&lt;/code&gt;: inch (72.27 &lt;code&gt;pt&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;bp&lt;/code&gt;: Big point (72 &lt;code&gt;bp&lt;/code&gt; = 1 &lt;code&gt;in&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cm&lt;/code&gt;: Centimeter&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;mm&lt;/code&gt;: Millimeter&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;dd&lt;/code&gt;: Didot point&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cc&lt;/code&gt;: cicero (12 &lt;code&gt;dd&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;sp&lt;/code&gt;: Scaled point (65,536 &lt;code&gt;sp&lt;/code&gt; = 1 &lt;code&gt;pt&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ex&lt;/code&gt;: Nomimal x-height&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;em&lt;/code&gt;: Nominal m-width&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;px&lt;/code&gt;: Pixel (only for pdfTeX and LuaTeX) The dimension given to the &lt;code&gt;\pdfpxdimen&lt;/code&gt; primitive; default value is 1 &lt;code&gt;bp&lt;/code&gt;, corresponding to a pixel density of 72 dpi.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;A visual overview of the length units is available at &lt;a href=&#34;https://github.com/tweh/tex-units&#34;&gt;https://github.com/tweh/tex-units&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;first-chapter&lt;/code&gt; (defaults to &lt;code&gt;1&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;if typesetting a book with chapter numbers, specifies the number that will be assigned to the first chapter&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;float-placement-figure&lt;/code&gt; (defaults to &lt;code&gt;H&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;Reset the default placement specifier for figure environments to the supplied value e.g.&amp;nbsp;&lt;code&gt;htbp&lt;/code&gt;. The available specifiers are listed below. The first four placement specifiers can be combined.&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;code&gt;h&lt;/code&gt;: Place the float &lt;em&gt;here&lt;/em&gt;, i.e., approximately at the same point it occurs in the source text.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;t&lt;/code&gt;: Place the float at the &lt;em&gt;top&lt;/em&gt; of the page.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;b&lt;/code&gt;: Place the float at the &lt;em&gt;bottom&lt;/em&gt; of the page.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;p&lt;/code&gt;: Place the float on the next &lt;em&gt;page&lt;/em&gt; that will contain only floats like figures and tables.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;H&lt;/code&gt;: Place the float &lt;em&gt;HERE&lt;/em&gt; (exactly where it occurs in the source text). The &lt;code&gt;H&lt;/code&gt; specifier is provided by the &lt;a href=&#34;https://ctan.org/pkg/float&#34;&gt;float package&lt;/a&gt; and may not be used in conjunction with any other placement specifiers.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;table-use-row-colors&lt;/code&gt; (defaults to &lt;code&gt;false&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;enables row colors for tables. The default value is &lt;code&gt;false&lt;/code&gt; because the coloring extends beyond the edge of the table and there is currently no way to change that.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;code-block-font-size&lt;/code&gt; (defaults to &lt;code&gt;\small&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;LaTeX command to change the font size for code blocks. The available values are &lt;code&gt;\tiny&lt;/code&gt;, &lt;code&gt;\scriptsize&lt;/code&gt;, &lt;code&gt;\footnotesize&lt;/code&gt;, &lt;code&gt;\small&lt;/code&gt;, &lt;code&gt;\normalsize&lt;/code&gt;, &lt;code&gt;\large&lt;/code&gt;, &lt;code&gt;\Large&lt;/code&gt;, &lt;code&gt;\LARGE&lt;/code&gt;, &lt;code&gt;\huge&lt;/code&gt; and &lt;code&gt;\Huge&lt;/code&gt;. This option will change the font size for default code blocks using the verbatim environment and for code blocks generated with listings.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Required LaTeX Packages&lt;/h2&gt; &#xA;&lt;p&gt;LaTeX manages addons and additional functionality in so called packages. You might get the following error when compiling a document with the Eisvogel template:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;! LaTeX Error: File `footnotebackref.sty&#39; not found.&#xA;&#xA;Type X to quit or &amp;lt;RETURN&amp;gt; to proceed,&#xA;or enter new name. (Default extension: sty)&#xA;&#xA;Enter file name:&#xA;! Emergency stop.&#xA;&amp;lt;read *&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LaTeX informs you that the additional package &lt;code&gt;footnotebackref&lt;/code&gt; is required to render the document.&lt;/p&gt; &#xA;&lt;h3&gt;Texlive&lt;/h3&gt; &#xA;&lt;p&gt;Eisvogel requires a full texlive distribution that can be installed by running &lt;code&gt;apt-get install texlive-full&lt;/code&gt; in the terminal. Because &lt;code&gt;texlive-full&lt;/code&gt; is very large (about 5 Gigabytes) you can also install the smaller texlive bundles and add any missing packages manually.&lt;/p&gt; &#xA;&lt;p&gt;A smaller texlive bundle is &lt;code&gt;texlive-latex-extra&lt;/code&gt;. With &lt;code&gt;texlive-latex-extra&lt;/code&gt; you also need to install these packages manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;adjustbox babel-german background bidi collectbox csquotes everypage filehook&#xA;footmisc footnotebackref framed fvextra letltxmacro ly1 mdframed mweights&#xA;needspace pagecolor sourcecodepro sourcesanspro titling ucharcat ulem&#xA;unicode-math upquote xecjk xurl zref&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install them with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tlmgr install adjustbox babel-german background bidi collectbox csquotes everypage filehook footmisc footnotebackref framed fvextra letltxmacro ly1 mdframed mweights needspace pagecolor sourcecodepro sourcesanspro titling ucharcat ulem unicode-math upquote xecjk xurl zref&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional information about the different texlive packages can be found at this TeX-StackExchange answer: &lt;a href=&#34;https://tex.stackexchange.com/a/504566&#34;&gt;https://tex.stackexchange.com/a/504566&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;MiKTeX&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want to install all missing packages manually, &lt;a href=&#34;https://miktex.org/howto/miktex-console&#34;&gt;MiKTeX might be an alternative&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;MiKTeX has the ability to automatically install missing packages. You can turn this feature on or off. And you can let MiKTeX ask you each time a package has to be installed:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Click &lt;code&gt;Settings&lt;/code&gt; to navigate to the settings page.&lt;/li&gt; &#xA;  &lt;li&gt;Click the &lt;code&gt;General&lt;/code&gt; tab.&lt;/li&gt; &#xA;  &lt;li&gt;Click one of the radio buttons: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;code&gt;Ask me&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;Always install missing packages on-the-fly&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;Never install missing packages on-the-fly&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Numbered Sections&lt;/h3&gt; &#xA;&lt;p&gt;For PDFs with &lt;a href=&#34;http://pandoc.org/MANUAL.html#options-affecting-specific-writers&#34;&gt;numbered sections&lt;/a&gt; use the &lt;code&gt;--number-sections&lt;/code&gt; or &lt;code&gt;-N&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel --number-sections&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Syntax Highlighting with Listings&lt;/h3&gt; &#xA;&lt;p&gt;You can get syntax highlighting of delimited code blocks by using the LaTeX package listings with the option &lt;code&gt;--listings&lt;/code&gt;. This example will produce the same syntax highlighting as in the example PDF.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel --listings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Syntax Highlighting Without Listings&lt;/h3&gt; &#xA;&lt;p&gt;The following examples show &lt;a href=&#34;http://pandoc.org/MANUAL.html#syntax-highlighting&#34;&gt;syntax highlighting of delimited code blocks&lt;/a&gt; without using listings. To see a list of all the supported highlight styles, type &lt;code&gt;pandoc --list-highlight-styles&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style pygments&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style kate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style espresso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel --highlight-style tango&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Standalone LaTeX Document&lt;/h3&gt; &#xA;&lt;p&gt;To produce a standalone LaTeX document for compiling with any LaTeX editor use &lt;code&gt;.tex&lt;/code&gt; as an output file extension.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.tex --template eisvogel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Changing the Document Language&lt;/h3&gt; &#xA;&lt;p&gt;The default language of this template is American English. The &lt;code&gt;lang&lt;/code&gt; variable identifies the main language of the document, using a code according to &lt;a href=&#34;https://tools.ietf.org/html/bcp47&#34;&gt;BCP 47&lt;/a&gt; (e.g.&amp;nbsp;&lt;code&gt;en&lt;/code&gt; or &lt;code&gt;en-GB&lt;/code&gt;). For an incomplete list of the supported language codes see &lt;a href=&#34;http://mirrors.ctan.org/language/hyph-utf8/doc/generic/hyph-utf8/hyph-utf8.pdf&#34;&gt;the documentation for the hyph-utf8 package (Section 2)&lt;/a&gt;. The following example changes the language to British English:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=en-GB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following example changes the language to German:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pandoc example.md -o example.pdf --template eisvogel -V lang=de&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Typesetting a Book&lt;/h3&gt; &#xA;&lt;p&gt;To typeset a book supply the template variable &lt;code&gt;-V book&lt;/code&gt; from the command line or via &lt;code&gt;book: true&lt;/code&gt; in the metadata.&lt;/p&gt; &#xA;&lt;p&gt;To get the correct chapter headings you need to tell pandoc that it should convert first level headings (indicated by one &lt;code&gt;#&lt;/code&gt; in markdown) to chapters with the command line option &lt;code&gt;--top-level-division=chapter&lt;/code&gt;. Chapter numbers start at 1. If you need to change that, specify &lt;code&gt;first-chapter&lt;/code&gt; in the template variables.&lt;/p&gt; &#xA;&lt;p&gt;There will be one blank page before each chapter because the template is two-sided per default. So if you plan to publish your book as a PDF and don‚Äôt need a blank page you should add the class option &lt;code&gt;onesided&lt;/code&gt; which can be done by supplying a template variable &lt;code&gt;-V classoption=oneside&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example Images&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A green title page&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A background image on the title page&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/title-page-green/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/title-page-green/preview.png&#34; alt=&#34;A green title page&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/title-page-background/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/title-page-background/preview.png&#34; alt=&#34;A background image on the title page&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;images and tables&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code blocks styled without listings&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/images-and-tables/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/images-and-tables/preview.png&#34; alt=&#34;images and tables&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/code-blocks-without-listings/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/code-blocks-without-listings/preview.png&#34; alt=&#34;Code blocks styled without listings&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A book&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code blocks styled with listings&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/book/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/book/preview.png&#34; alt=&#34;A book&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/code-blocks-listings/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/code-blocks-listings/preview.png&#34; alt=&#34;Code blocks styled with listings&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A background images on all pages&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CJK Support (when using XeLaTeX)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/page-background/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/page-background/preview.png&#34; alt=&#34;A background images on all pages&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/language-japanese/document.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/examples/language-japanese/preview.png&#34; alt=&#34;CJK Support (when using XeLaTeX)&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Common Errors / Issues&lt;/h2&gt; &#xA;&lt;p&gt;The following section lists common errors and their solutions when using the Eisvogel template.&lt;/p&gt; &#xA;&lt;h3&gt;LaTeX Errors &lt;code&gt;Missing endcsname inserted&lt;/code&gt; or &lt;code&gt;File x not found&lt;/code&gt; when using &lt;code&gt;titlepage-background&lt;/code&gt;, &lt;code&gt;logo&lt;/code&gt;, or &lt;code&gt;titlepage-logo&lt;/code&gt;.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;Error producing PDF.&#xA;! Missing endcsname inserted.&#xA;&amp;lt;to be read again&amp;gt;&#xA;                   protect&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;Error producing PDF.&#xA;! Package pdftex.def Error: File `logo\T1\textunderscoreimage.pdf&#39; not fou&#xA;nd: using draft setting.&#xA;&#xA;See the pdftex.def package documentation for explanation.&#xA;Type  H &amp;lt;return&amp;gt;  for immediate help.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These errors occur when one includes a background image on the title page or a logo that has an underscore (&lt;code&gt;_&lt;/code&gt;) in the filename.&lt;/p&gt; &#xA;&lt;p&gt;A quick fix would be to replace all underscores in the filename of the image with a hyphen (&lt;code&gt;-&lt;/code&gt;). If the background image is specified in your YAML front matter like so,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;titlepage-background: &#34;background_image.pdf&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;you can advise pandoc to interpret this as LaTeX and include it in the document without parsing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;titlepage-background: &#34;`background_image.pdf`{=latex}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same fix can be used for the logo image as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;logo: &#34;`logo_image.pdf`{=latex}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Corresponding issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wandmalfarbe/pandoc-latex-template/issues/100&#34;&gt;Wandmalfarbe/pandoc-latex-template#100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wandmalfarbe/pandoc-latex-template/issues/166&#34;&gt;Wandmalfarbe/pandoc-latex-template#166&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LaTeX Error &lt;code&gt;Missing \begin{document}&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;! LaTeX Error: Missing \begin{document}.&#xA;&#xA;See the LaTeX manual or LaTeX Companion for explanation.&#xA;Type  H &amp;lt;return&amp;gt;  for immediate help.&#xA; ...&#xA;&#xA;l.7 &amp;lt;&#xA;     !DOCTYPE html&amp;gt;&#xA;!  ==&amp;gt; Fatal error occurred, no output PDF file produced!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This error indicates that you try to use some text file for conversion that is not the Eisvogel template. Please download the &lt;a href=&#34;https://github.com/Wandmalfarbe/pandoc-latex-template/releases/latest&#34;&gt;latest Eisvogel template&lt;/a&gt; from the releases page and start the conversion again.&lt;/p&gt; &#xA;&lt;h3&gt;LaTeX Error &lt;code&gt;auto expansion is only possible with scalable fonts&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;Error producing PDF.&#xA;! pdfTeX error (font expansion): auto expansion is only possible with scalable&#xA;fonts.&#xA;\AtBegShi@Output ...ipout \box \AtBeginShipoutBox&#xA;                                                  \fi \fi&#xA;l.643 \begin{lstlisting}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This error likely occurs on Windows with MiKTeX installed. StackOverflow user &lt;a href=&#34;https://tex.stackexchange.com/a/392467&#34;&gt;Krebto provided the following fix&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To solve the problem navigate to &lt;code&gt;C:\Program Files\MiKTeX 2.9\miktex\bin\x64&lt;/code&gt; and run &lt;code&gt;updmap.exe&lt;/code&gt;. The program may seem as it hangs for a while, but its probably because it tries to update the whole font tree. This solved the problem for me. After re-compiling everything should work fine.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Corresponding issue:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wandmalfarbe/pandoc-latex-template/issues/133&#34;&gt;Wandmalfarbe/pandoc-latex-template#133&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LaTeX Error &lt;code&gt;cannot find image file&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;Error producing PDF.&#xA;! error:  (file &#34;/tmp/tex2pdf.-be734e802ef6d0c3/&#34;&#34;fdcfc29edcf252186f1b0a52f18f50&#xA;43abaeb2d0&#34;.png) (pdf backend): cannot find image file &#39;&#34;/tmp/tex2pdf.-be734e802&#xA;ef6d0c3/&#34;&#34;fdcfc29edcf252186f1b0a52f18f5043abaeb2d0&#34;.png&#39;&#xA;!  ==&amp;gt; Fatal error occurred, no output PDF file produced!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In general this error means that LaTeX is unable to find the included image file. Please check all image references and file names for correctness.&lt;/p&gt; &#xA;&lt;p&gt;This error also occurs if you use an old version of Eisvogel with the package &lt;code&gt;grffile&lt;/code&gt; and have an old LaTeX distribution installed. Please update Eisvogel and your LaTeX distribution.&lt;/p&gt; &#xA;&lt;p&gt;Corresponding issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jgm/pandoc/issues/5848&#34;&gt;jgm/pandoc#5848&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wandmalfarbe/pandoc-latex-template/issues/149&#34;&gt;Wandmalfarbe/pandoc-latex-template#149&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This template includes code for styling block quotations from &lt;a href=&#34;https://github.com/aaronwolen/pandoc-letter&#34;&gt;pandoc-letter&lt;/a&gt; by &lt;a href=&#34;https://github.com/aaronwolen&#34;&gt;Aaron Wolen&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is open source licensed under the BSD 3-Clause License. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Wandmalfarbe/pandoc-latex-template/master/LICENSE&#34;&gt;LICENSE file&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wowchemy/starter-hugo-academic</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/wowchemy/starter-hugo-academic</id>
    <link href="https://github.com/wowchemy/starter-hugo-academic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üéì Hugo Academic Theme ÂàõÂª∫‰∏Ä‰∏™Â≠¶ÊúØÁΩëÁ´ô. Easily create a beautiful academic r√©sum√© or educational website using Hugo, GitHub, and Netlify.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/wowchemy/starter-hugo-academic&#34;&gt;Hugo Academic Theme&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/main/preview.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Hugo &lt;strong&gt;Academic Resum√© Template&lt;/strong&gt; empowers you to easily create your job-winning online resum√©, showcase your academic publications, and create online courses or knowledge bases to grow your audience.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Get%20started-ff4655?style=for-the-badge&#34; alt=&#34;Get Started&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/channels/722225264733716590/742892432458252370/742895548159492138&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/722225264733716590?style=for-the-badge&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/wowchemy&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/wowchemy?label=Follow%20on%20Twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ô∏è&lt;strong&gt;Trusted by 250,000+ researchers, educators, and students.&lt;/strong&gt; Highly customizable via the integrated &lt;strong&gt;no-code, widget-based Wowchemy page builder&lt;/strong&gt;, making every site truly personalized ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê&lt;/p&gt; &#xA;&lt;p&gt;Easily write technical content with plain text Markdown, LaTeX math, diagrams, RMarkdown, or Jupyter, and import publications from BibTeX.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.app/&#34;&gt;Check out the latest demo&lt;/a&gt; of what you&#39;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/creators/&#34;&gt;get inspired by our academics and research groups&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The integrated &lt;a href=&#34;https://wowchemy.com&#34;&gt;&lt;strong&gt;Wowchemy&lt;/strong&gt;&lt;/a&gt; website builder and CMS makes it easy to create a beautiful website for free. Edit your site in the CMS (or your favorite editor), generate it with &lt;a href=&#34;https://github.com/gohugoio/hugo&#34;&gt;Hugo&lt;/a&gt;, and deploy with GitHub or Netlify. Customize anything on your site with widgets, light/dark themes, and language packs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üëâ &lt;a href=&#34;https://wowchemy.com/hugo-themes/&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://wowchemy.com/docs/&#34;&gt;View the &lt;strong&gt;documentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí¨ &lt;a href=&#34;https://discord.gg/z8wNYzb&#34;&gt;Chat with the &lt;strong&gt;Wowchemy research community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üê¶ Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Automatically import your publications from BibTeX&lt;/strong&gt; with the &lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34;&gt;Hugo Academic CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí° &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/issues&#34;&gt;Suggest an improvement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚¨ÜÔ∏è &lt;strong&gt;Updating?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-themes/releases&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;We ask you, humbly, to support this open source movement&lt;/h2&gt; &#xA;&lt;p&gt;Today we ask you to defend the open source independence of the Wowchemy website builder and themes üêß&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re an open source movement that depends on your support to stay online and thriving, but 99.9% of our creators don&#39;t give; they simply look the other way.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/sponsors/gcushen&#34;&gt;‚ù§Ô∏è Click here to become a GitHub Sponsor, unlocking awesome perks such as &lt;em&gt;exclusive academic templates and widgets&lt;/em&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://wowchemy.com/uploads/readmes/academic_logo_200px.png&#34; alt=&#34;Hugo Academic Theme for Wowchemy Website Builder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo image credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/J4kK8b9Fgj8&#34;&gt;Open book&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unsplash.com/photos/JKUTrJ4vK00&#34;&gt;Course&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Latest news&lt;/h2&gt; &#xA;&lt;!--START_SECTION:news--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wowchemy.com/blog/easily-make-academic-website/&#34;&gt;Easily make an academic CV website to get more cites and grow your audience üöÄ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wowchemy.com/blog/whats-new-in-v5.2/&#34;&gt;What&#39;s new in v5.2?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wowchemy.com/blog/whats-new-in-v5.1/&#34;&gt;What&#39;s new in v5.1?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wowchemy.com/blog/version-5.0-february-2021/&#34;&gt;Version 5.0 (February 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wowchemy.com/blog/version-5.0-beta-3-february-2021/&#34;&gt;Version 5.0 Beta 3 (February 2021)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--END_SECTION:news--&gt;</summary>
  </entry>
  <entry>
    <title>openbmc/docs</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/openbmc/docs</id>
    <link href="https://github.com/openbmc/docs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenBMC Documentation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenBMC documentation&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://www.openbmc.org/&#34;&gt;OpenBMC project&lt;/a&gt; is a Linux Foundation project whose goal is to produce a customizable, open-source firmware stack for Baseboard Management Controllers (BMCs). This repository contains documentation for OpenBMC as a whole. There may be component-specific documentation in the repository for each component.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/features.md&#34;&gt;features&lt;/a&gt; document lists the project&#39;s major features with links to more information.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mail: &lt;a href=&#34;mailto:openbmc@lists.ozlabs.org&#34;&gt;openbmc@lists.ozlabs.org&lt;/a&gt; &lt;a href=&#34;https://lists.ozlabs.org/listinfo/openbmc&#34;&gt;https://lists.ozlabs.org/listinfo/openbmc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;List Archive: &lt;a href=&#34;https://lore.kernel.org/openbmc/&#34;&gt;https://lore.kernel.org/openbmc/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord: &lt;a href=&#34;https://discord.gg/69Km47zH98&#34;&gt;https://discord.gg/69Km47zH98&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenBMC Development&lt;/h2&gt; &#xA;&lt;p&gt;These documents contain details on developing OpenBMC code itself&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/cheatsheet.md&#34;&gt;cheatsheet.md&lt;/a&gt;: Quick reference for some common development tasks&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;: Guidelines for contributing to OpenBMC&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/development/README.md&#34;&gt;development tutorials&lt;/a&gt;: Tutorials for getting up to speed on OpenBMC development&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/kernel-development.md&#34;&gt;kernel-development.md&lt;/a&gt;: Reference for common kernel development tasks&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenBMC Usage&lt;/h2&gt; &#xA;&lt;p&gt;These documents describe how to use OpenBMC, including using the programmatic interfaces to an OpenBMC system.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/architecture/code-update&#34;&gt;code-update&lt;/a&gt;: Updating OpenBMC and host platform firmware&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/console.md&#34;&gt;console.md&lt;/a&gt;: Using the host console&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/host-management.md&#34;&gt;host-management.md&lt;/a&gt;: Performing host management tasks with OpenBMC&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/rest-api.md&#34;&gt;rest-api.md&lt;/a&gt;: Introduction to using the OpenBMC REST API&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/REDFISH-cheatsheet.md&#34;&gt;REDFISH-cheatsheet.md&lt;/a&gt;: Quick reference for some common OpenBMC Redfish commands&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openbmc/docs/master/REST-cheatsheet.md&#34;&gt;REST-cheatsheet.md&lt;/a&gt;: Quick reference for some common OpenBMC REST API commands&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lib-pku/libpku</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/lib-pku/libpku</id>
    <link href="https://github.com/lib-pku/libpku" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ë¥µÊ†°ËØæÁ®ãËµÑÊñôÊ∞ëÈó¥Êï¥ÁêÜ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;libpku - Ë¥µÊ†°ËØæÁ®ãËµÑÊñôÊ∞ëÈó¥Êï¥ÁêÜ&lt;/h1&gt; &#xA;&lt;h2&gt;Preface&lt;/h2&gt; &#xA;&lt;p&gt;ÔºàÂºïÁî®Ëá™ &lt;a href=&#34;https://github.com/QSCTech/zju-icicles&#34;&gt;QSCTech/zju-icicles&lt;/a&gt; Ôºâ&lt;/p&gt; &#xA;&lt;p&gt;Êù•Âà∞‰∏ÄÊâÄÂ§ßÂ≠¶Ôºå‰ªéÁ¨¨‰∏ÄÊ¨°Êé•Ëß¶ËÆ∏Â§öËØæÔºåÁõ¥Âà∞‰∏ÄÈó®‰∏ÄÈó®ÂÆåÊàêÔºåËøô‰∏™ËøáÁ®ã‰∏≠Êàë‰ª¨Êó∂Â∏∏Êî∂ÈõÜËµ∑ËÆ∏Â§öËµÑÊñôÂíåÊÉÖÊä•„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Êúâ‰∫õÊòØÈúÄË¶ÅÂú®ÁΩë‰∏äÊêúÁ¥¢ÁöÑÁîµÂ≠ê‰π¶ÔºåÊØèÊ¨°ËßÅÂà∞‰∏ÄÈó®Êñ∞ËØæÁ®ãÔºåGoogle ‰∏Ä‰∏ãÊïôÊùêÂêçÁß∞ÔºåÊúâÁöÑÂèØ‰ª•Á´ãÂç≥ÊâæÂà∞ÔºåÊúâÁöÑÂç¥ÊòØË¶ÅËä±Ë¥πËÆ∏Â§öÁúºÂäõÔºõÊúâ‰∫õÊòØÂéÜÂπ¥ËØïÂç∑ÊàñËÄÖ A4 Á∫∏ÔºåÂâç‰∫∫Á≤æÂøÉÊî∂ÈõÜÂà∂‰ΩúÔºåÊä±ÁùÄËÉΩÂØπ‰ªñ‰∫∫ÊúâÁî®ÁöÑÊÉ≥Ê≥ïÂÖ¨ÂºÄÔºåÂç¥ÈúÄË¶ÅÂú®ÂêÑ‰∏™Áæ§ÊàñËÄÖÁßÅ‰∏ã‰∏≠Êë∏Á¥¢‰ª•Ëá≥‰∫é‰ªéÂ≠¶ÈïøÊâã‰∏≠‰ª£‰ª£Áõ∏‰º†ÔºõÊúâ‰∫õÊòØ‰∏äÂÆå‰∏ÄÈó®ËØæÊâçÊÅçÁÑ∂È¢ÜÊÇüÁöÑÊäÄÂ∑ßÔºåÂéüÊù•ËøôÈó®ËØæÈáçÁÇπÂ¶ÇÊ≠§ÔºåÂΩìÂàùÊú¨ÂèØ‰ª•Êõ¥ËΩªÊùæÂú∞ÂÆåÊàêÂæóÊõ¥Â•Ω‚Ä¶‚Ä¶&lt;/p&gt; &#xA;&lt;p&gt;Êàë‰πüÊõæÂæàÂä™ÂäõÂú∞Êî∂ÈõÜÂêÑÁßçËØæÁ®ãËµÑÊñôÔºå‰ΩÜÂà∞ÊúÄÂêéÔºåÊüê‰∫õÈáçË¶Å‰ø°ÊÅØÁöÑÂæóÂà∞Âç¥ÂæÄÂæÄ‰æùÁÑ∂ÊòØÁ∫ØÂ±ûÂÅ∂ÁÑ∂„ÄÇËøôÁßçÁä∂ÊÄÅÊó∂Â∏∏‰ª§ÊàëÊÑüÂà∞ÂêéÊÄï‰∏é‰∏çÂÆâ„ÄÇÊàë‰πüÊõæÂú®ËØæÁ®ãÁªìÊùüÂêéÁªà‰∫éÊúâ‰∫Ü‰∫õËÆ∏ÊñπÊ≥ï‰∏éÊÄªÁªìÔºå‰ΩÜËøô‰∫õÊÉ≥Ê≥ïÊó†Â§ÑËØâËØ¥ÔºåÊúÄÁªàÂè™ËÉΩÊääËä±Ë¥πÊó∂Èó¥‰∏éÁ≤æÂäõÊâçÊç¢Êù•ÁöÑÁªèÈ™åËÄóÊï£Âú®‰∫ÜÊº´Êº´ÁöÑÈÅóÂøò‰πã‰∏≠„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Êàë‰∏∫Ëøô‰∏ÄÂπ¥‰∏ÄÂπ¥ÔºåËøô‰πàÂ§ö‰∫∫Â≠§ÂÜõÂ•ãÊàòÁöÑÈáçÂ§çÂä≥Âä®ÊÑüÂà∞‰∏çÂπ≥„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÊàëÂ∏åÊúõËÉΩÂ§üÂ∞ÜËøô‰∫õÈöêÊô¶ÁöÑ„ÄÅ‰∏çÁ°ÆÂÆöÁöÑ„ÄÅÂè£Âè£Áõ∏‰º†ÁöÑËµÑÊñôÂíåÁªèÈ™åÔºåÂèò‰∏∫ÂÖ¨ÂºÄÁöÑ„ÄÅÊòì‰∫éËé∑ÂèñÁöÑÂíåÂ§ßÂÆ∂ËÉΩÂ§üÂÖ±ÂêåÂÆåÂñÑ„ÄÅÁßØÁ¥ØÁöÑÂÖ±‰∫´ËµÑÊñô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ÊàëÂ∏åÊúõÂè™Ë¶ÅÊòØÂâç‰∫∫Ëµ∞ËøáÁöÑÂºØË∑ØÔºåÂêé‰∫∫Â∞±‰∏çÂøÖÂÜçËµ∞„ÄÇ&lt;/strong&gt; ËøôÊòØÊàëÁöÑ‰ø°ÂøµÔºå‰πüÊòØÊàëÂª∫Á´ãËøô‰∏™È°πÁõÆÁöÑÂéüÂõ†„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;‰ΩøÁî®ÊñπÊ≥ïÔºöËÆøÈóÆ &lt;a href=&#34;https://lib-pku.github.io/&#34;&gt;https://lib-pku.github.io/&lt;/a&gt; ÔºåÁÇπÂáªËµÑÊñôÈìæÊé•Âç≥ÂèØ‰∏ãËΩΩ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home&#34;&gt;https://minhaskamal.github.io/DownGit/#/home&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ê¨¢ËøéË¥°ÁåÆÔºÅ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ê¨¢ËøéË¥°ÁåÆÔºÅ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ê¨¢ËøéË¥°ÁåÆÔºÅ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚Äî‚ÄîÂõ†‰∏∫ÂæàÈáçË¶ÅÊâÄ‰ª•ËØ¥‰∫Ü‰∏âÈÅç&lt;/p&gt; &#xA;&lt;p&gt;Issue„ÄÅPR„ÄÅÁ∫†Èîô„ÄÅËµÑÊñô„ÄÅÈÄâËØæ/ËÄÉËØïÊîªÁï•ÔºåÂÆåÂÖ®Ê¨¢ËøéÔºÅ&lt;/p&gt; &#xA;&lt;p&gt;Êù•Ëá™Â§ßÂÆ∂ÁöÑÂÖ≥Ê≥®„ÄÅÁª¥Êä§ÂíåË¥°ÁåÆÔºåÊâçÊòØËÆ©Ëøô‰∏™ÊîªÁï•ÁªßÁª≠Â≠òÂú®ÁöÑÂä®Âäõ~&lt;/p&gt; &#xA;&lt;p&gt;ÂØπ‰∫éËØæÁ®ãÁöÑËØÑ‰ª∑ÂèØÂÜôÂú®ÂØπÂ∫îËØæÁ®ãÊñá‰ª∂Â§πÁöÑ &lt;code&gt;README.md&lt;/code&gt; ‰∏≠„ÄÇÂ¶ÇÊûúÊÉ≥‰∏ä‰º†ËØæ‰ª∂ÔºàËØ∑Á°Æ‰øùÊó†ÁâàÊùÉÈóÆÈ¢òÔºâÔºåÊé®Ëçê‰ΩøÁî® PDF Ê†ºÂºèÔºåÈÅøÂÖçÁ≥ªÁªüÂ∑Æ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Áî±‰∫éÊú¨È°πÁõÆ‰ΩìÁßØÂæàÂ§ßÔºåÊïÖÊé®ËçêÈááÁî®Âú® &lt;strong&gt;GitHub Web Á´ØÁõ¥Êé•‰∏ä‰º†&lt;/strong&gt; ÁöÑÊñπÂºèÔºåÂÖ∑‰ΩìÊìç‰ΩúÂ¶Ç‰∏ãÔºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;È¶ñÂÖà Fork Êú¨È°πÁõÆ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰∏ä‰º†Êñá‰ª∂Âà∞Â∑≤ÊúâÊñá‰ª∂Â§πÔºöÊâìÂºÄÂØπÂ∫îÊñá‰ª∂Â§πÔºåÁÇπÂáªÁªøËâ≤ Download ÊåâÈíÆÊóÅÁöÑ uploadÔºå‰∏ä‰º†‰Ω†ÁöÑÊñá‰ª∂„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰∏ä‰º†Êñá‰ª∂Âà∞Êñ∞Êñá‰ª∂Â§πÔºöÊâìÂºÄ‰ªªÊÑèÊñá‰ª∂Â§πÔºåÁÇπÂáªÁªøËâ≤ Download ÊåâÈíÆÊóÅÁöÑ uploadÔºå&lt;strong&gt;ÊääÊµèËßàÂô®Âú∞ÂùÄÊ†è‰∏≠Êñá‰ª∂Â§πÂêçÁß∞Êîπ‰∏∫‰Ω†ÊÉ≥Ë¶ÅÊñ∞Âª∫ÁöÑÊñá‰ª∂Â§πÂêçÁß∞ÔºåÁÑ∂ÂêéÂõûËΩ¶&lt;/strong&gt;Ôºå‰∏ä‰º†‰Ω†ÁöÑÊñá‰ª∂„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êèê‰∫§ PRÔºöFork Êú¨È°πÁõÆÔºåÁÑ∂ÂêéÂú® GitHub ÁΩëÈ°µÁ´ØÁÇπÂáª Upload File ‰∏ä‰º†Êñá‰ª∂ÔºåÂèëËµ∑ PR Âç≥ÂèØ„ÄÇÁïôÊÑè‰∏Ä‰∏ãÈ°πÁõÆÁöÑÊñá‰ª∂ÁªÑÁªáÂñî„ÄÇ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ÊàñËÄÖ‰πüÂèØ‰ª•Áõ¥Êé•ÈôÑÂä†Âú® &lt;strong&gt;Issue&lt;/strong&gt; ‰∏≠ÔºåÁî±Áª¥Êä§ËÄÖËøõË°åÊ∑ªÂä†„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÊàñËÄÖ‰πüÂèØ‰ª•ÂèëÈÄÅÈÇÆ‰ª∂Ëá≥ &lt;strong&gt;&lt;a href=&#34;mailto:libpku@protonmail.com&#34;&gt;libpku@protonmail.com&lt;/a&gt;&lt;/strong&gt; ÔºåÁî±Áª¥Êä§ËÄÖËøõË°åÊ∑ªÂä†„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Ëøô‰∏çÊòØÂåó‰∫¨Â§ßÂ≠¶Âõæ‰π¶È¶Ü„ÄÇ Êàë‰ª¨‰πü‰∏çÂØπÈ°πÁõÆ‰∏≠‰ø°ÊÅØÁöÑÂáÜÁ°ÆÊÄßÊàñÁúüÂÆûÊÄßÂÅö‰ªª‰ΩïÊâøËØ∫„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Â¶ÇÊûúÊúâ‰æµÊùÉÊÉÖÂÜµÔºåÈ∫ªÁÉ¶ÊÇ®ÂèëÈÄÅÂøÖË¶ÅÁöÑ‰ø°ÊÅØËá≥ &lt;a href=&#34;mailto:libpku@protonmail.com&#34;&gt;libpku@protonmail.com&lt;/a&gt; ÔºåÂ∏¶Êù•‰∏ç‰æøËøòËØ∑ÊÇ®Ë∞ÖËß£„ÄÇ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;ËµÑÊñôÊù•Ëá™ÁΩëÁªúÔºåÁõ∏ÂÖ≥ÊùÉÂà©Áî±Âéü‰ΩúËÄÖÊâÄÊúâÔºåËøô‰∏™ repo ‰ªÖÁî®‰∫éÊî∂ÈõÜÁé∞ÊúâËµÑÊñô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÂΩìÁÑ∂ÔºåÊàë‰ª¨‰∏ç‰ºö‰∏∫Êî∂ÈõÜÂà∞ÁöÑËµÑÊñôÊî∂Ë¥πÔºåÊàñÊòØÂ∞ùËØïÊî∂ÂèñÊçêËµ†„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Êàë‰ª¨Âè™ÊòØÂ∞ùËØï‰∏∫ÂêéÊù•ÁöÑÂêåÂ≠¶ËäÇÁúÅ‰∏Ä‰∫õÊó∂Èó¥„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/QSCTech/zju-icicles&#34;&gt;ÊµôÊ±üÂ§ßÂ≠¶ËØæÁ®ãÊîªÁï•ÂÖ±‰∫´ËÆ°Âàí&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/martinwu42/project-hover&#34;&gt;Ê∞îÂû´ËàπËÆ°Âàí‚Äî‚ÄîÂÖçË¥π„ÄÅÂéª‰∏≠ÂøÉÂåñÁöÑÂåó‰∫¨Â§ßÂ≠¶ÂæÄÂπ¥È¢òËµÑÊñôÂ∫ì&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EECS-PKU-XSB/Shared-learning-materials&#34;&gt;Âåó‰∫¨Â§ßÂ≠¶‰ø°ÁßëÂ≠¶Áîü‰ºöÂ≠¶ÊúØÈÉ®ËµÑÊñôÂ∫ì&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tongtzeho/PKUCourse&#34;&gt;ÂåóÂ§ßËÆ°ÁÆóÊú∫ËØæÁ®ãÂ§ß‰Ωú‰∏ö&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKUanonym/REKCARC-TSC-UHT&#34;&gt;Ê∏ÖÂçéÂ§ßÂ≠¶ËÆ°ÁÆóÊú∫Á≥ªËØæÁ®ãÊîªÁï•&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zjdx1998/seucourseshare&#34;&gt;‰∏úÂçóÂ§ßÂ≠¶ËØæÁ®ãÂÖ±‰∫´ËÆ°Âàí&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/USTC-Resource/USTC-Course&#34;&gt;‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶ËÆ°ÁÆóÊú∫Â≠¶Èô¢ËØæÁ®ãËµÑÊ∫ê&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CoolPhilChen/SJTU-Courses/&#34;&gt;‰∏äÊµ∑‰∫§ÈÄöÂ§ßÂ≠¶ËØæÁ®ãËµÑÊñôÂàÜ‰∫´&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sysuexam/SYSU-Exam&#34;&gt;‰∏≠Â±±Â§ßÂ≠¶ËØæÁ®ãËµÑÊñôÂàÜ‰∫´&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/idealclover/NJU-Review-Materials&#34;&gt;Âçó‰∫¨Â§ßÂ≠¶ËØæÁ®ãÂ§ç‰π†ËµÑÊñô&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CooperNiu/ZZU-Courses-Resource&#34;&gt;ÈÉëÂ∑ûÂ§ßÂ≠¶ËØæÁ®ãÂ§ç‰π†ËµÑÊñô&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brenner8023/gdut-course&#34;&gt;Âπø‰∏úÂ∑•‰∏öÂ§ßÂ≠¶ËÆ°ÁÆóÊú∫Â≠¶Èô¢ËØæÁ®ãÊîªÁï•&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(more to be added....)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cplusplus/draft</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/cplusplus/draft</id>
    <link href="https://github.com/cplusplus/draft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ standards drafts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;========================== C++ Standard Draft Sources&lt;/h1&gt; &#xA;&lt;p&gt;These are the sources used to generate drafts of the C++ standard. These sources should not be considered an ISO publication, nor should documents generated from them unless officially adopted by the C++ working group (ISO/IEC JTC1/SC22/WG21).&lt;/p&gt; &#xA;&lt;p&gt;Get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;How to submit an editorial issue &amp;lt;https://github.com/cplusplus/draft/wiki/How-to-submit-an-editorial-issue&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to tell if an issue is editorial &amp;lt;https://github.com/cplusplus/draft/wiki/How-to-tell-if-an-issue-is-editorial&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to submit a new issue/defect report &amp;lt;https://isocpp.org/std/submit-issue&amp;gt;&lt;/code&gt;_ for non-editorial issues&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More information about the C++ standard can be found at &lt;code&gt;isocpp.org &amp;lt;http://isocpp.org/std&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started on Mac OS X&lt;/h2&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;MacTeX distribution &amp;lt;http://tug.org/mactex/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;If you are on a slow network, you&#39;ll want to get the &lt;code&gt;BasicTeX package &amp;lt;http://tug.org/mactex/morepackages.html&amp;gt;&lt;/code&gt;_ instead, then run the following command to install the other packages that the draft requires:&lt;/p&gt; &#xA;&lt;p&gt;sudo tlmgr install latexmk isodate substr relsize ulem fixme rsfs extract layouts enumitem l3packages l3kernel imakeidx splitindex&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started on Debian-based Systems&lt;/h2&gt; &#xA;&lt;p&gt;Install the following packages:&lt;/p&gt; &#xA;&lt;p&gt;sudo apt-get install latexmk texlive-latex-recommended texlive-latex-extra texlive-fonts-recommended lmodern&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started on Fedora&lt;/h2&gt; &#xA;&lt;p&gt;Install the following packages:&lt;/p&gt; &#xA;&lt;p&gt;dnf install latexmk texlive texlive-isodate texlive-relsize texlive-ulem texlive-fixme texlive-extract texlive-l3kernel texlive-l3packages texlive-splitindex texlive-imakeidx&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started on Arch Linux&lt;/h2&gt; &#xA;&lt;p&gt;Install the following packages:&lt;/p&gt; &#xA;&lt;p&gt;pacman -S texlive-latexextra&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting Started on Microsoft Windows&lt;/h2&gt; &#xA;&lt;p&gt;Install Perl (for example, using a &lt;code&gt;Cygwin installation &amp;lt;https://cygwin.com/install.html&amp;gt;&lt;/code&gt;_ and adding perl. See &lt;code&gt;sample instructions &amp;lt;https://bennierobinson.com/programming/2016/01/24/perl-windows-2016.html&amp;gt;&lt;/code&gt;_ for more details)&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;code&gt;MiKTeX &amp;lt;https://miktex.org/download&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;To typeset the draft document, from the &lt;code&gt;source&lt;/code&gt; directory run::&lt;/p&gt; &#xA;&lt;p&gt;make&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s it! You should now have an &lt;code&gt;std.pdf&lt;/code&gt; containing the typeset draft.&lt;/p&gt; &#xA;&lt;h1&gt;Generated input files&lt;/h1&gt; &#xA;&lt;p&gt;To regenerate figures from .dot files, run::&lt;/p&gt; &#xA;&lt;p&gt;make &#xA; &lt;pdfname&gt;&lt;/pdfname&gt;&lt;/p&gt; &#xA;&lt;p&gt;For example::&lt;/p&gt; &#xA;&lt;p&gt;make figvirt.pdf&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;A great deal of gratitude goes out to Pete Becker for his amazing work in the original conversion of the C++ standard drafts to LaTeX, and his subsequent maintenance of the standard drafts up to C++11. Thank you Pete.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to Walter Brown for suggesting the use of &lt;code&gt;latexmk&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sysprog21/lkmpg</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/sysprog21/lkmpg</id>
    <link href="https://github.com/sysprog21/lkmpg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Linux Kernel Module Programming Guide (updated for 5.x kernels)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Linux Kernel Module Programming Guide&lt;/h1&gt; &#xA;&lt;p&gt;This project keeps the Linux Kernel Module Programming Guide up to date, with &lt;a href=&#34;https://raw.githubusercontent.com/sysprog21/lkmpg/master/examples/&#34;&gt;working examples&lt;/a&gt; for recent 5.x kernel versions. The guide has been around since 2001 and most copies of it on the web only describe old 2.6.x kernels.&lt;/p&gt; &#xA;&lt;p&gt;The book can be freely accessed via &lt;a href=&#34;https://sysprog21.github.io/lkmpg/&#34;&gt;https://sysprog21.github.io/lkmpg/&lt;/a&gt; or &lt;a href=&#34;https://github.com/sysprog21/lkmpg/releases&#34;&gt;latest PDF file&lt;/a&gt;. The original guide may be found at &lt;a href=&#34;http://www.tldp.org/LDP/lkmpg/&#34;&gt;Linux Documentation Project&lt;/a&gt;. You may check other &lt;a href=&#34;https://ebookfoundation.github.io/free-programming-books/books/free-programming-books.html&#34;&gt;freely available programming books&lt;/a&gt; listed by The &lt;a href=&#34;https://ebookfoundation.org/&#34;&gt;Free Ebook Foundation&lt;/a&gt; or &lt;a href=&#34;https://onlinebooks.library.upenn.edu/webbin/book/browse?type=lcsubc&amp;amp;key=Linux&#34;&gt;Linux online books&lt;/a&gt; collected by &lt;a href=&#34;https://onlinebooks.library.upenn.edu/&#34;&gt;The Online Books Page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Summary&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the latest source code from the &lt;a href=&#34;https://github.com/sysprog21/lkmpg&#34;&gt;GitHub page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install the prerequisites.&lt;/li&gt; &#xA; &lt;li&gt;Generate PDF and/or HTML documents.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 1: Get the latest source code&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you can run &lt;code&gt;git&lt;/code&gt; with an Internet connection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/sysprog21/lkmpg.git &amp;amp;&amp;amp; cd lkmpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Install the prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;To generate the book from source, &lt;a href=&#34;https://www.tug.org/texlive/&#34;&gt;TeXLive&lt;/a&gt; (&lt;a href=&#34;https://www.tug.org/mactex/&#34;&gt;MacTeX&lt;/a&gt;) is required.&lt;/p&gt; &#xA;&lt;p&gt;For Ubuntu Linux, macOS, and other Unix-like systems, run the following command(s):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Debian / Ubuntu&#xA;$ sudo apt install make texlive-full&#xA;&#xA;# Arch / Manjaro&#xA;$ sudo pacman -S make texlive-most texlive-bin&#xA;&#xA;# macOS&#xA;$ brew install --cask mactex&#xA;$ sudo tlmgr update --self&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;latexmk&lt;/code&gt; is required to generated PDF, and it probably has been installed on your OS already. If not, please follow the &lt;a href=&#34;https://mg.readthedocs.io/latexmk.html#installation&#34;&gt;installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, using &lt;a href=&#34;https://docs.docker.com/&#34;&gt;Docker&lt;/a&gt; is recommended, as it guarantees the same dependencies with our GitHub Actions workflow. After install &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;docker engine&lt;/a&gt; on your machine, pull the docker image &lt;a href=&#34;https://hub.docker.com/r/twtug/lkmpg&#34;&gt;twtug/lkmpg&lt;/a&gt; and run in isolated containers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# pull docker image and run it as container&#xA;$ docker pull twtug/lkmpg&#xA;$ docker run --rm -it -v $(pwd):/workdir twtug/lkmpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/containerd/nerdctl&#34;&gt;nerdctl&lt;/a&gt; is a Docker-compatible command line tool for &lt;a href=&#34;https://containerd.io/&#34;&gt;containerd&lt;/a&gt;, and you can replace the above &lt;code&gt;docker&lt;/code&gt; commands with &lt;code&gt;nerdctl&lt;/code&gt; counterparts.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Generate PDF and/or HTML documents&lt;/h3&gt; &#xA;&lt;p&gt;Now we could build document with following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make all              # Generate PDF document&#xA;$ make html             # Convert TeX to HTML&#xA;$ make clean            # Delete generated files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Linux Kernel Module Programming Guide is a free book; you may reproduce and/or modify it under the terms of the &lt;a href=&#34;https://opensource.org/licenses/OSL-3.0&#34;&gt;Open Software License&lt;/a&gt;. Use of this work is governed by a copyleft license that can be found in the &lt;code&gt;LICENSE&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;The complementary sample code is licensed under GNU GPL version 2, as same as Linux kernel.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>billryan/resume</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/billryan/resume</id>
    <link href="https://github.com/billryan/resume" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An elegant \LaTeX\ r√©sum√© template. Â§ßÈôÜÈïúÂÉè https://gods.coding.net/p/resume/git&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;R√©sum√©&lt;/h1&gt; &#xA;&lt;p&gt;Hit branch &lt;a href=&#34;https://github.com/billryan/resume/tree/zh_CN&#34;&gt;zh_CN&lt;/a&gt; if you want a Simplified Chinese r√©sum√©.&lt;/p&gt; &#xA;&lt;p&gt;‰∏≠ÊñáÁî®Êà∑ËØ∑ÂâçÂæÄ &lt;a href=&#34;https://github.com/billryan/resume/tree/zh_CN&#34;&gt;zh_CN&lt;/a&gt; ÂàÜÊîØ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;An elegant \LaTeX\ r√©sum√© template, compiled with \XeLaTeX. Inspired by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zachscrivena/simple-resume-cv&#34;&gt;zachscrivena/simple-resume-cv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/res&#34;&gt;res&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.jianxu.net/en/files/JianXu_CV.pdf&#34;&gt;JianXu&#39;s CV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.stat.berkeley.edu/~paciorek/computingTips/Latex_template_creating_CV_.html&#34;&gt;paciorek&#39;s CV/Resume template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sharelatex.com/blog/2011/03/27/how-to-write-a-latex-class-file-and-design-your-own-cv.html&#34;&gt;How to write a LaTeX class file and design your own CV (Part 1) - ShareLaTeX&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easy to further customize or extend&lt;/li&gt; &#xA; &lt;li&gt;Full support for unicode characters (e.g. CJK) with \XeLaTeX\&lt;/li&gt; &#xA; &lt;li&gt;Perfect Simplified Chinese fonts supported with Adobefonts&lt;/li&gt; &#xA; &lt;li&gt;FontAwesome 4.6.3 support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this repository&lt;/li&gt; &#xA; &lt;li&gt;Add information about you directly in GitHub&lt;/li&gt; &#xA; &lt;li&gt;Compile TeX file to PDF with &lt;a href=&#34;https://latexonline.cc/&#34;&gt;LaTeX.Online&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Can also use Overleaf for online compilation with &lt;a href=&#34;https://www.overleaf.com/latex/templates/bill-ryans-elegant-latex-resume/xcqmhktmzmsw&#34;&gt;template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Sample Output&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/25968335/131621921-65ab1862-1f56-47ef-9d58-8d5149bec841.png&#34; alt=&#34;English&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25968335/131621960-1cafb3c2-114b-4e90-8b04-bd9b949a6e9d.png&#34; alt=&#34;English with photo&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25968335/131621980-c004f2a6-4199-4676-8a97-5d2cb165402f.png&#34; alt=&#34;ÁÆÄ‰Ωì‰∏≠Êñá&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/billryan/resume/files/3463503/resume.pdf&#34;&gt;English PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/billryan/resume/files/3463501/resume_photo.pdf&#34;&gt;English with photo PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/billryan/resume/files/3463502/resume-zh_CN.pdf&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Edit in Overleaf online Web &lt;a href=&#34;https://www.overleaf.com/latex/templates/bill-ryans-elegant-latex-resume/xcqmhktmzmsw&#34;&gt;template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Compile tex on your Computer&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you only need a r√©sum√© in English or have installed Adobe Simplified Chinese on your OS, &lt;strong&gt;It would be better to clone only the master branch,&lt;/strong&gt; since the Simplified Chinese fonts files are too large.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/billryan/resume.git --branch master --depth 1 --single-branch &amp;lt;folder&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://opensource.org/licenses/MIT&#34;&gt;The MIT License (MIT)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Copyrighted fonts are not subjected to this License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yenchenlin/awesome-NeRF</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/yenchenlin/awesome-NeRF</id>
    <link href="https://github.com/yenchenlin/awesome-NeRF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A curated list of awesome neural radiance fields papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Neural Radiance Fields &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A curated list of awesome neural radiance fields papers, inspired by &lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision&#34;&gt;awesome-computer-vision&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/how-to-PR.md&#34;&gt;How to submit a pull request?&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/#papers&#34;&gt;Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/#talks&#34;&gt;Talks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.05204&#34;&gt;Neural Volume Rendering: NeRF And Beyond&lt;/a&gt;, Dellaert and Yen-Chen, Arxiv 2020 | &lt;a href=&#34;https://dellaert.github.io/NeRF/&#34;&gt;blog&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/nerf-survey.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.matthewtancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;, Mildenhall et al., ECCV 2020 | &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L168-L173&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Mildenhall20eccv_nerf--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Faster Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lingjie0206.github.io/papers/NSVF/&#34;&gt;Neural Sparse Voxel Fields&lt;/a&gt;, Liu et al., NeurIPS 2020 | &lt;a href=&#34;https://github.com/facebookresearch/NSVF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L135-L141&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Liu20neurips_sparse_nerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computationalimaging.org/publications/automatic-integration/&#34;&gt;AutoInt: Automatic Integration for Fast Neural Volume Rendering&lt;/a&gt;, Lindell et al., CVPR 2021 | &lt;a href=&#34;https://github.com/computational-imaging/automatic-integration&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L127-L133&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Lindell20arxiv_AutoInt--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.12490&#34;&gt;DeRF: Decomposed Radiance Fields&lt;/a&gt;, Rebain et al. Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L222-L228&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Rebain20arxiv_derf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://depthoraclenerf.github.io/&#34;&gt;DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks&lt;/a&gt;, Neff et al., CGF 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/donerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--neff2021donerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.10380&#34;&gt;FastNeRF: High-Fidelity Neural Rendering at 200FPS&lt;/a&gt;, Garbin et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/fastnerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Garbin21arxiv_FastNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.13744&#34;&gt;KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs &lt;/a&gt;, Reiser et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/creiser/kilonerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/kilonerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--reiser2021kilonerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://alexyu.net/plenoctrees/&#34;&gt;PlenOctrees for Real-time Rendering of Neural Radiance Fields&lt;/a&gt;, Yu et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/sxyu/volrend&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/plenoctrees.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--yu2021plenoctrees--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.01954&#34;&gt;Mixture of Volumetric Primitives for Efficient Neural Rendering&lt;/a&gt;, Lombardi et al., SIGGRAPH 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/mixture.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vsitzmann.github.io/lfns/&#34;&gt;Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering&lt;/a&gt;, Sitzmann et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/lfn.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Faster Training&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2107.02791.pdf&#34;&gt;Depth-supervised NeRF: Fewer Views and Faster Training for Free&lt;/a&gt;, Deng et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/dunbar12138/DSNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/dsnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11215.pdf&#34;&gt;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction&lt;/a&gt;, Sun et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/sunset1995/DirectVoxGO&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/DirectVoxGO.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--sun2021direct--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Unconstrained Images&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections&lt;/a&gt;, Martin-Brualla et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L152-L158&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--MartinBrualla20arxiv_nerfw--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rover-xingyu.github.io/Ha-NeRF/&#34;&gt;Ha-NeRF&lt;span&gt;üòÜ&lt;/span&gt;: Hallucinated Neural Radiance Fields in the Wild&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/rover-xingyu/Ha-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/Ha-NeRF.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--chen2021hallucinated--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Deformable&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nerfies.github.io/&#34;&gt;Deformable Neural Radiance Fields&lt;/a&gt;, Park et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/google/nerfies&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L206-L212&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Park20arxiv_nerfies--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.albertpumarola.com/research/D-NeRF/index.html&#34;&gt;D-NeRF: Neural Radiance Fields for Dynamic Scenes&lt;/a&gt;, Pumarola et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L214-L220&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Pumarola20arxiv_D_NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gafniguy.github.io/4D-Facial-Avatars/&#34;&gt;Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction&lt;/a&gt;, Gafni et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L87-L93&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Gafni20arxiv_DNRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/&#34;&gt;Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Deforming Scene from Monocular Video&lt;/a&gt;, Tretschk et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/facebookresearch/nonrigid_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L283-L289&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Tretschk20arxiv_NR-NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://volumetric-avatars.github.io/&#34;&gt;PVA: Pixel-aligned Volumetric Avatars&lt;/a&gt;, Raj et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/pva.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nogu-atsu/NARF&#34;&gt;Neural Articulated Radiance Field&lt;/a&gt;, Noguchi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/narf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.00181&#34;&gt;CLA-NeRF: Category-Level Articulated Neural Radiance Field&lt;/a&gt;, Tseng et al., ICRA 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/cla-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/animatable_nerf/&#34;&gt;Animatable Neural Radiance Fields for Human Body Modeling&lt;/a&gt;, Peng et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/animatable_nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Peng21arxiv_animatable_nerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypernerf.github.io/&#34;&gt;A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields&lt;/a&gt;, Park et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/hypernerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13629&#34;&gt;Animatable Neural Radiance Fields from Monocular RGB Videos&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/JanaldoChen/Anim-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/anim_nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/NeuralActor/&#34;&gt;Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control&lt;/a&gt;, Liu et al., SIGGRAPH Asia 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neuralactor.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Video&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes&lt;/a&gt;, Li et al., CVPR 2021 | &lt;a href=&#34;https://github.com/zhengqili/Neural-Scene-Flow-Fields&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L119-L125&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Li20arxiv_nsff--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://video-nerf.github.io/&#34;&gt;Space-time Neural Irradiance Fields for Free-Viewpoint Video&lt;/a&gt;, Xian et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L299-L305&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Xian20arxiv_stnif--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yilundu.github.io/nerflow/&#34;&gt;Neural Radiance Flow for 4D View Synthesis and Video Processing&lt;/a&gt;, Du et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L79-L85&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Du20arxiv_nerflow--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/neuralbody/&#34;&gt;Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans&lt;/a&gt;, Peng et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neuralbody.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Peng20arxiv_neuralbody--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://neural-3d-video.github.io/&#34;&gt;Neural 3D Video Synthesis&lt;/a&gt;, Li et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/3d-video.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://free-view-video.github.io/&#34;&gt;Dynamic View Synthesis from Dynamic Monocular Video&lt;/a&gt;, Gao et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/dvs_dmv.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Generalization&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.02442&#34;&gt;GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis&lt;/a&gt;, Schwarz et al., NeurIPS 2020 | &lt;a href=&#34;https://github.com/autonomousvision/graf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L237-L243&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Schwarz20neurips_graf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.04595&#34;&gt;GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering&lt;/a&gt;, Trevithick and Yang, Arxiv 2020 | &lt;a href=&#34;https://github.com/alextrevithick/GRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L291-L297&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Trevithick20arxiv_GRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.02190&#34;&gt;pixelNeRF: Neural Radiance Fields from One or Few Images&lt;/a&gt;, Yu et al., CVPR 2021 | &lt;a href=&#34;https://github.com/sxyu/pixel-nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L329-L335&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Yu20arxiv_pixelNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.02189&#34;&gt;Learned Initializations for Optimizing Coordinate-Based Neural Representations&lt;/a&gt;, Tancik et al., CVPR 2021 | &lt;a href=&#34;https://github.com/tancik/learnit&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L268-L274&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Tancik20arxiv_meta--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis&lt;/a&gt;, Chan et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L24-L30&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Chan20arxiv_piGAN--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://portrait-nerf.github.io/&#34;&gt;Portrait Neural Radiance Fields from a Single Image&lt;/a&gt;, Gao et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L95-L101&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Gao20arxiv_pNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.08860.pdf&#34;&gt;ShaRF: Shape-conditioned Radiance Fields from a Single View&lt;/a&gt;, Rematas et al., ICML 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/sharf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ibrnet.github.io/static/paper.pdf&#34;&gt;IBRNet: Learning Multi-View Image-Based Rendering&lt;/a&gt;, Wang et al., CVPR 2021 | &lt;a href=&#34;https://github.com/googleinterns/IBRNet&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/ibr.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.17269.pdf&#34;&gt;CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields&lt;/a&gt;, Niemeyer &amp;amp; Geiger, Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/CAMPARI.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.00587.pdf&#34;&gt;NeRF-VAE: A Geometry Aware 3D Scene Generative Model&lt;/a&gt;, Kosiorek et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerf-vae.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apple.github.io/ml-gsn/&#34;&gt;Unconstrained Scene Generation with Locally Conditioned Radiance Fields&lt;/a&gt;, DeVries et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/gsn.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apchenstu.github.io/mvsnerf/&#34;&gt;MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/apchenstu/mvsnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/mvsnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtualhumans.mpi-inf.mpg.de/srf/&#34;&gt;Stereo Radiance Fields (SRF): Learning View Synthesis from Sparse Views of Novel Scenes&lt;/a&gt;, Chibane et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/srf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liuyuan-pal.github.io/NeuRay/&#34;&gt;Neural Rays for Occlusion-aware Image-based Rendering&lt;/a&gt;, Liu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neuray.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ajayj.com/dietnerf&#34;&gt;Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis&lt;/a&gt;, Matthew Tancik et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/DietNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vincentfung13.github.io/projects/mine/&#34;&gt;MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis&lt;/a&gt;, Jiaxin Li et al., ICCV 2021 | &lt;a href=&#34;https://github.com/vincentfung13/MINE&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/MINE.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imaging.cs.cmu.edu/torf/&#34;&gt;T√∂RF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis&lt;/a&gt;, Benjamin Attal et al., NeurIPS 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/turf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/wbjang/home/codenerf&#34;&gt;CodeNeRF: Disentangled Neural Radiance Fields for Object Categories&lt;/a&gt;, Jang et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/CodeNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis&lt;/a&gt;, Gu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/stylenerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bmild.github.io/rawnerf/&#34;&gt;NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images&lt;/a&gt;, Ben Mildenhall et al, arXiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/rawnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pose Estimation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://yenchenlin.me/inerf/&#34;&gt;iNeRF: Inverting Neural Radiance Fields for Pose Estimation&lt;/a&gt;, Yen-Chen et al. IROS 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L321-L327&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--YenChen20arxiv_iNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lemonatsu.github.io/ANeRF-Surface-free-Pose-Refinement/&#34;&gt;A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering&lt;/a&gt;, Su et al. Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/a-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Su21arxiv_A_NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nerfmm.active.vision/&#34;&gt;NeRF--: Neural Radiance Fields Without Known Camera Parameters&lt;/a&gt;, Wang et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/ActiveVisionLab/nerfmm&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerf--.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Wang21arxiv_nerfmm--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://edgarsucar.github.io/iMAP/&#34;&gt;iMAP: Implicit Mapping and Positioning in Real-Time&lt;/a&gt;, Sucar et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/imap.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pengsongyou.github.io/nice-slam&#34;&gt;NICE-SLAM: Neural Implicit Scalable Encoding for SLAM&lt;/a&gt;, Zhu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nice-slam.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.15606&#34;&gt;GNeRF: GAN-based Neural Radiance Field without Posed Camera&lt;/a&gt;, Meng et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/gnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/&#34;&gt;BARF: Bundle-Adjusting Neural Radiance Fields&lt;/a&gt;, Lin et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/barf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://postech-cvlab.github.io/SCNeRF/&#34;&gt;Self-Calibrating Neural Radiance Fields&lt;/a&gt;, Park et al., ICCV 2021 | &lt;a href=&#34;https://github.com/POSTECH-CVLab/SCNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/SCNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lighting&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://markboss.me/publication/2021-nerd/&#34;&gt;NeRD: Neural Reflectance Decomposition from Image Collections&lt;/a&gt;, Boss et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L9-L15&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Boss20arxiv_NeRD--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/nerv/&#34;&gt;NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis&lt;/a&gt;, Srinivasan et al. CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L260-L266&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Srinivasan20arxiv_NeRV--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nex-mpi.github.io/&#34;&gt;NeX: Real-time View Synthesis with Neural Basis Expansion&lt;/a&gt;, Wizadwongsa et al. Arxiv 2021 | &lt;a href=&#34;https://github.com/nex-mpi/nex-code&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nex.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/xiuming/projects/nerfactor/&#34;&gt;NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination&lt;/a&gt;, Zhang et al. Arxiv 2021 | &lt;a href=&#34;https://github.com/google/nerfactor&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerfactor.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Compositionality&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07492&#34;&gt;NeRF++: Analyzing and Improving Neural Radiance Fields&lt;/a&gt;, Zhang et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/Kai-46/nerfplusplus&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L345-L351&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Zhang20arxiv_nerf++--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.12100&#34;&gt;GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields&lt;/a&gt;, Niemeyer et al., CVPR 2021, &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L175-L181&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Niemeyer20arxiv_GIRAFFE--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shellguo.com/osf/&#34;&gt;Object-Centric Neural Scene Rendering&lt;/a&gt;, Guo et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L111-L117&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Guo20arxiv_OSF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ziyanw1.github.io/hybrid_nerf/&#34;&gt;Learning Compositional Radiance Fields of Dynamic Human Heads&lt;/a&gt;, Wang et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/hybrid-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Wang20arxiv_hybrid_NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://light.princeton.edu/neural-scene-graphs/&#34;&gt;Neural Scene Graphs for Dynamic Scenes&lt;/a&gt;, Ost et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L353-L358&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Ost20arxiv_NeuralSceneGraphs--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kovenyu.com/uorf/&#34;&gt;Unsupervised Discovery of Object Radiance Fields&lt;/a&gt;, Yu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/uorf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/object_nerf/&#34;&gt;Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering&lt;/a&gt;, Yang et al., ICCV 2021 | &lt;a href=&#34;https://github.com/zju3dv/object_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/object-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--yang2021objectnerf--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Scene Labelling and Understanding&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shuaifengzhi.com/Semantic-NeRF/&#34;&gt;In-Place Scene Labelling and Understanding with Implicit Scene Representation&lt;/a&gt;, Zhi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/semantic-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Editing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://editnerf.csail.mit.edu/&#34;&gt;Editing Conditional Radiance Fields&lt;/a&gt;, Liu et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/stevliu/editnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/editnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jiakai-zhang.github.io/st-nerf/&#34;&gt;Editable Free-viewpoint Video Using a Layered Neural Representation&lt;/a&gt;, Zhang et al., SIGGRAPH 2021 | &lt;a href=&#34;https://github.com/DarlingHang/st-nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/st-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Object Category Modeling&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fig-nerf.github.io/&#34;&gt;FiG-NeRF: Figure Ground Neural Radiance Fields for 3D Object Category Modelling&lt;/a&gt;, Xie et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/fig-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/nvidia-research-nerf-tex-neural-reflectance-field-textures/&#34;&gt;NeRF-Tex: Neural Reflectance Field Textures&lt;/a&gt;, Baatz et al., EGSR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerf-tex.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-scale&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jonbarron.info/mipnerf/&#34;&gt;Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields&lt;/a&gt;, Barron et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/mip-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Model Reconstruction&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.10078&#34;&gt;UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction&lt;/a&gt;, Oechsle et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/unisurf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.10689&#34;&gt;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction&lt;/a&gt;, Wang et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neus.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12052&#34;&gt;Volume Rendering of Neural Implicit Surfaces&lt;/a&gt;, Yariv et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/ventusff/neurecon&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/volsdf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Depth Estimation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://weiyithu.github.io/NerfingMVS/&#34;&gt;NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo&lt;/a&gt;, Wei et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/NerfingMVS.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Talks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LCTYRqW-ne8&amp;amp;t=10190s&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;, Ben Mildenhall&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nRyOzHpcr4Q&amp;amp;feature=emb_logo&amp;amp;ab_channel=cvprtum&#34;&gt;Understanding and Extending Neural Radiance Fields&lt;/a&gt;, Barron et al.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/Rd0nBO6--bM?t=1992&#34;&gt;Towards Photorealism (2nd half)&lt;/a&gt;, Vladlen Koltun&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dPWLybp4LL0&#34;&gt;Neural Radiance Fields for View Synthesis&lt;/a&gt;, Matthew Tancik&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;Tensorflow&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;NeRF&lt;/a&gt;, Mildenhall et al., 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L168-L173&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;PyTorch&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;NeRF-PyTorch&lt;/a&gt;, Yen-Chen Lin, 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/pytorch-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl&#34;&gt;NeRF-PyTorch-Lighting&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123&#34;&gt;@kwea123&lt;/a&gt;, 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;NeRF-W&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123&#34;&gt;@kwea123&lt;/a&gt;, 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf&#34;&gt;NeRF-PyTorch3D&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch&#34;&gt;@facebookresearch&lt;/a&gt;, 2020&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Jax&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/jaxnerf&#34;&gt;JaxNeRF&lt;/a&gt;, Deng et al., 2020 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/NeRF-and-Beyond.bib#L55-L60&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;Mip-NeRF&lt;/a&gt;, &lt;a href=&#34;https://github.com/google&#34;&gt;@google&lt;/a&gt;, 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/mipnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AllenDowney/ThinkPython2</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/AllenDowney/ThinkPython2</id>
    <link href="https://github.com/AllenDowney/ThinkPython2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX source and supporting code for Think Python, 2nd edition, by Allen Downey.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ThinkPython&lt;/h1&gt; &#xA;&lt;p&gt;LaTeX source, code examples, and exercise solutions for Think Python, 2nd edition, by Allen Downey.&lt;/p&gt; &#xA;&lt;p&gt;You can download this book in PDF from &lt;a href=&#34;http://greenteapress.com/wp/think-python-2e/&#34;&gt;Green Tea Press&lt;/a&gt; or buy it in paper and other formats from &lt;a href=&#34;http://shop.oreilly.com/product/0636920045267.do&#34;&gt;O&#39;Reilly Media&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build the book from source you will need a LaTeX installion. I recommend the TeX Live distribution with the following packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;texlive-latex-base&lt;/li&gt; &#xA; &lt;li&gt;texlive-latex-extra&lt;/li&gt; &#xA; &lt;li&gt;texlive-fonts-recommended&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>vdumoulin/conv_arithmetic</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/vdumoulin/conv_arithmetic</id>
    <link href="https://github.com/vdumoulin/conv_arithmetic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Convolution arithmetic&lt;/h1&gt; &#xA;&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt; &#xA;&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the licence and subject to proper attribution:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt; (&lt;a href=&#34;https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214&#34;&gt;BibTeX&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Convolution animations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%; table-layout:fixed;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, no strides&lt;/td&gt; &#xA;   &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt; &#xA;   &lt;td&gt;Half padding, no strides&lt;/td&gt; &#xA;   &lt;td&gt;Full padding, no strides&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, strides&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides (odd)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Transposed convolution animations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%; table-layout:fixed;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, no strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Dilated convolution animations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:25%&#34; ; table-layout:fixed;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, no stride, dilation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Generating the Makefile&lt;/h2&gt; &#xA;&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./bin/generate_makefile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating the animations&lt;/h2&gt; &#xA;&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make all_animations&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the &lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Compiling the document&lt;/h2&gt; &#xA;&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>HarisIqbal88/PlotNeuralNet</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/HarisIqbal88/PlotNeuralNet</id>
    <link href="https://github.com/HarisIqbal88/PlotNeuralNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Latex code for making neural networks diagrams&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PlotNeuralNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.2526396&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Latex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Following are some network representations:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png&#34; width=&#34;85%&#34; height=&#34;85%&#34;&gt;&lt;/p&gt; &#xA;&lt;h6 align=&#34;center&#34;&gt;FCN-8 (&lt;a href=&#34;https://www.overleaf.com/read/kkqntfxnvbsk&#34;&gt;view on Overleaf&lt;/a&gt;)&lt;/h6&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png&#34; width=&#34;85%&#34; height=&#34;85%&#34;&gt;&lt;/p&gt; &#xA;&lt;h6 align=&#34;center&#34;&gt;FCN-32 (&lt;a href=&#34;https://www.overleaf.com/read/wsxpmkqvjnbs&#34;&gt;view on Overleaf&lt;/a&gt;)&lt;/h6&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png&#34; width=&#34;85%&#34; height=&#34;85%&#34;&gt;&lt;/p&gt; &#xA;&lt;h6 align=&#34;center&#34;&gt;Holistically-Nested Edge Detection (&lt;a href=&#34;https://www.overleaf.com/read/jxhnkcnwhfxp&#34;&gt;view on Overleaf&lt;/a&gt;)&lt;/h6&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the following packages on Ubuntu.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Ubuntu 16.04&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-extra&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Ubuntu 18.04.2 Base on this &lt;a href=&#34;https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d&#34;&gt;website&lt;/a&gt;, please install the following packages.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-base&#xA;sudo apt-get install texlive-fonts-recommended&#xA;sudo apt-get install texlive-fonts-extra&#xA;sudo apt-get install texlive-latex-extra&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and install &lt;a href=&#34;https://miktex.org/download&#34;&gt;MikTeX&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Download and install bash runner on Windows, recommends &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;Git bash&lt;/a&gt; or Cygwin(&lt;a href=&#34;https://www.cygwin.com/&#34;&gt;https://www.cygwin.com/&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute the example as followed.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd pyexamples/&#xA;bash ../tikzmake.sh test_simple&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Python interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add easy legend functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more layer shapes like TruncatedPyramid, 2DSheet etc&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add examples for RNN and likes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Latex usage&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/HarisIqbal88/PlotNeuralNet/master/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for usage.&lt;/p&gt; &#xA;&lt;h2&gt;Python usage&lt;/h2&gt; &#xA;&lt;p&gt;First, create a new directory and a new Python file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ mkdir my_project&#xA;$ cd my_project&#xA;vim my_arch.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add the following code to your new file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys&#xA;sys.path.append(&#39;../&#39;)&#xA;from pycore.tikzeng import *&#xA;&#xA;# defined your arch&#xA;arch = [&#xA;    to_head( &#39;..&#39; ),&#xA;    to_cor(),&#xA;    to_begin(),&#xA;    to_Conv(&#34;conv1&#34;, 512, 64, offset=&#34;(0,0,0)&#34;, to=&#34;(0,0,0)&#34;, height=64, depth=64, width=2 ),&#xA;    to_Pool(&#34;pool1&#34;, offset=&#34;(0,0,0)&#34;, to=&#34;(conv1-east)&#34;),&#xA;    to_Conv(&#34;conv2&#34;, 128, 64, offset=&#34;(1,0,0)&#34;, to=&#34;(pool1-east)&#34;, height=32, depth=32, width=2 ),&#xA;    to_connection( &#34;pool1&#34;, &#34;conv2&#34;),&#xA;    to_Pool(&#34;pool2&#34;, offset=&#34;(0,0,0)&#34;, to=&#34;(conv2-east)&#34;, height=28, depth=28, width=1),&#xA;    to_SoftMax(&#34;soft1&#34;, 10 ,&#34;(3,0,0)&#34;, &#34;(pool1-east)&#34;, caption=&#34;SOFT&#34;  ),&#xA;    to_connection(&#34;pool2&#34;, &#34;soft1&#34;),&#xA;    to_end()&#xA;    ]&#xA;&#xA;def main():&#xA;    namefile = str(sys.argv[0]).split(&#39;.&#39;)[0]&#xA;    to_generate(arch, namefile + &#39;.tex&#39; )&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, run the program as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash ../tikzmake.sh my_arch&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>posquit0/Awesome-CV</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/posquit0/Awesome-CV</id>
    <link href="https://github.com/posquit0/Awesome-CV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üìÑ Awesome CV is LaTeX template for your outstanding job application&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/posquit0/Awesome-CV&#34; title=&#34;AwesomeCV Documentation&#34;&gt; &lt;img alt=&#34;AwesomeCV&#34; src=&#34;https://github.com/posquit0/Awesome-CV/raw/master/icon.png&#34; width=&#34;200px&#34; height=&#34;200px&#34;&gt; &lt;/a&gt; &lt;br&gt; Awesome CV &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; LaTeX template for your outstanding job application &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.paypal.me/posquit0&#34;&gt; &lt;img alt=&#34;Donate&#34; src=&#34;https://img.shields.io/badge/Donate-PayPal-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/posquit0/Awesome-CV/actions/workflows/main.yml&#34;&gt; &lt;img alt=&#34;GitHub Actions&#34; src=&#34;https://github.com/posquit0/Awesome-CV/actions/workflows/main.yml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&#34;&gt; &lt;img alt=&#34;Example Resume&#34; src=&#34;https://img.shields.io/badge/resume-pdf-green.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf&#34;&gt; &lt;img alt=&#34;Example CV&#34; src=&#34;https://img.shields.io/badge/cv-pdf-green.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&#34;&gt; &lt;img alt=&#34;Example Coverletter&#34; src=&#34;https://img.shields.io/badge/coverletter-pdf-green.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;What is Awesome CV?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;R√©sum√©&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href=&#34;https://www.sharelatex.com/templates/cv-or-resume/fancy-cv&#34;&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt; &#xA;&lt;h2&gt;Donate&lt;/h2&gt; &#xA;&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0&#xA;BTC: 1Je3DxJVM2a9nTVPNo55SfQwpmxA6N2KKb&#xA;BCH: 1Mg1wG7PwHGrHYSWS67TsGSjo5GHEVbF16&#xA;ETH: 0x77ED9B4659F80205E9B9C9FB1E26EDB9904AFCC7&#xA;QTUM: QZT7D6m3QtTTqp7s4ZWAwLtGDsoHMMaM8E&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;h4&gt;R√©sum√©&lt;/h4&gt; &#xA;&lt;p&gt;You can see &lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Page. 1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Page. 2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png&#34; alt=&#34;R√©sum√©&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png&#34; alt=&#34;R√©sum√©&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Cover Letter&lt;/h4&gt; &#xA;&lt;p&gt;You can see &lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Without Sections&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;With Sections&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png&#34; alt=&#34;Cover Letter(Traditional)&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png&#34; alt=&#34;Cover Letter(Awesome)&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp&#34;&gt;&lt;strong&gt;Edit R√©sum√© on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk&#34;&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;h4&gt;Requirements&lt;/h4&gt; &#xA;&lt;p&gt;A full TeX distribution is assumed. &lt;a href=&#34;http://tex.stackexchange.com/q/55437&#34;&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended. You can &lt;a href=&#34;https://tex.stackexchange.com/q/1092&#34;&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that. (It&#39;s generally a few years behind.)&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t want to install the dependencies on your system, this can also be obtained via &lt;a href=&#34;https://docker.com&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;p&gt;At a command prompt, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ xelatex {your-cv}.tex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or using docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --rm --user $(id -u):$(id -g) -i -w &#34;/doc&#34; -v &#34;$PWD&#34;:/doc thomasweise/texlive make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In either case, this should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.latex-project.org&#34;&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/furl/latex-fontawesome&#34;&gt;&lt;strong&gt;LaTeX FontAwesome&lt;/strong&gt;&lt;/a&gt; is bindings for FontAwesome icons to be used in XeLaTeX.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/roboto&#34;&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google‚Äôs visual language, Material Design.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/adobe-fonts/source-sans-pro&#34;&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don&#39;t use my resume for anything else without my permission, though!&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions, feel free to join me at &lt;a href=&#34;irc://irc.freenode.net/posquit0&#34;&gt;&lt;code&gt;#posquit0&lt;/code&gt; on Freenode&lt;/a&gt; and ask away. Click &lt;a href=&#34;https://kiwiirc.com/client/irc.freenode.net/posquit0&#34;&gt;here&lt;/a&gt; to connect.&lt;/p&gt; &#xA;&lt;p&gt;Good luck!&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/posquit0&#34;&gt;posquit0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OJFord&#34;&gt;OJFord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;See Also&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/posquit0/hugo-awesome-identity&#34;&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>tuna/thuthesis</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/tuna/thuthesis</id>
    <link href="https://github.com/tuna/thuthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX Thesis Template for Tsinghua University&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/actions&#34;&gt;&lt;img src=&#34;https://github.com/tuna/thuthesis/workflows/Test/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tuna/thuthesis/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/tuna/thuthesis/total&#34; alt=&#34;GitHub downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tuna/thuthesis/commits/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commits-since/tuna/thuthesis/latest&#34; alt=&#34;GitHub commits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tuna/thuthesis/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/tuna/thuthesis&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;&lt;img src=&#34;https://img.shields.io/ctan/v/thuthesis&#34; alt=&#34;CTAN&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ThuThesis&lt;/h1&gt; &#xA;&lt;p&gt;Scroll down for the English version of README.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThuThesis&lt;/strong&gt; ÊòØ &lt;strong&gt;T&lt;/strong&gt;sing&lt;strong&gt;h&lt;/strong&gt;ua &lt;strong&gt;U&lt;/strong&gt;niversity &lt;strong&gt;Thesis&lt;/strong&gt; LaTeX Template ÁöÑÁº©ÂÜô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Ê≠§ÂÆèÂåÖÊó®Âú®Âª∫Á´ã‰∏Ä‰∏™ÁÆÄÂçïÊòìÁî®ÁöÑÊ∏ÖÂçéÂ§ßÂ≠¶Â≠¶‰ΩçËÆ∫Êñá LaTeX Ê®°ÊùøÔºåÂåÖÊã¨Êú¨ÁßëÁªºÂêàËÆ∫ÊñáËÆ≠ÁªÉ„ÄÅÁ°ïÂ£´ËÆ∫Êñá„ÄÅÂçöÂ£´ËÆ∫Êñá‰ª•ÂèäÂçöÂ£´ÂêéÂá∫Á´ôÊä•Âëä„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Áî±‰∫éÊ®°ÊùøÂçáÁ∫ßÈ¢ëÁπÅÔºåÂú®ÂºÄÂßã‰ΩøÁî®ÂíåÊèêÈóÆÂâçÔºåËØ∑Á°Æ‰øùÊÇ®Â∑≤ÁªèËÆ§ÁúüÂÆåÊï¥Âú∞ÈòÖËØª‰∫Ü‰ΩøÁî®ËØ¥ÊòéÊñáÊ°£ÂíåÁ§∫‰æã‰ª£Á†Å„ÄÇ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;‰ªª‰ΩïËøùÂèç &lt;a href=&#34;https://www.latex-project.org/lppl/lppl-1-3c/&#34;&gt;LaTeXÈ°πÁõÆÂÖ¨ÂÖ±ËÆ∏ÂèØËØÅ v1.3c&lt;/a&gt; ‰ΩøÁî® ThuThesis ÁöÑË°å‰∏∫Â∞ÜË¢´ËÆ∞ÂΩïÂú® &lt;a href=&#34;https://github.com/tuna/thuthesis/issues/754&#34;&gt;ËÄªËæ±Êü±&lt;/a&gt; È°µÈù¢‰∏≠Ôºå‰ª•Á§∫Ë≠¶Âëä„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;‰∏ãËΩΩ&lt;/h2&gt; &#xA;&lt;p&gt;Êé®Ëçê‰∏ãËΩΩ&lt;strong&gt;ÂèëÂ∏ÉÁâà&lt;/strong&gt;Ê®°ÊùøÔºåÈáåÈù¢ÂåÖÊã¨ÂÖ∑‰Ωì‰ΩøÁî®ËØ¥Êòé‰ª•ÂèäÁ§∫‰æãÊñáÊ°£Ôºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ê®°Êùø‰ΩøÁî®ËØ¥ÊòéÔºàthuthesis.pdfÔºâ&lt;/li&gt; &#xA; &lt;li&gt;Á§∫‰æãÊñáÊ°£Ôºàthuthesis-example.pdfÔºâ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ÂºÄÂèëÁâà‰∏≠‰∏çÊèê‰æõÈ¢ÑÁîüÊàêÁöÑ &lt;code&gt;cls&lt;/code&gt; Êñá‰ª∂ÂíåÊñáÊ°£Ôºå‰ªÖÂåÖÂê´Ê∫êÁ†Å„ÄÇÂÖ∂‰ªÖ‰æõÂºÄÂèëËÄÖ‰∏éÈúÄË¶ÅÂ∞öÊú™ÂèëÂ∏ÉÁöÑÂäüËÉΩÁöÑÊúâÁªèÈ™åÁöÑ TeX Áî®Êà∑‰ΩøÁî®Ôºå‰∏çÊèê‰æõ‰ªª‰Ωï‰øùËØÅ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;‰∏ãËΩΩÈÄîÂæÑÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ÂèëÂ∏ÉÁâàÔºö &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt;ÔºöÂèØËÉΩÊªûÂêéÊ≠£ÂºèÂèëÂ∏ÉÂ∞ëËÆ∏Êó∂Èó¥„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/releases&#34;&gt;GitHub Releases&lt;/a&gt;ÔºöÊúÄÊñ∞ÁâàÁöÑÂèäÊó∂ÂèëÂ∏ÉÈÄîÂæÑ„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/github-release/tuna/thuthesis/&#34;&gt;TUNA ÈïúÂÉèÁ´ô&lt;/a&gt;ÔºöGitHub Releases ÁöÑÈïúÂÉè„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/thuthesis-tsinghua-university-thesis-latex-template/wddqnwbyhtnk&#34;&gt;Overleaf&lt;/a&gt;ÔºöOverleaf ÁöÑÊ®°Êùø„ÄÇ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ÂºÄÂèëÁâàÔºö&lt;a href=&#34;https://github.com/tuna/thuthesis&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;‰ªª‰ΩïÂú®ÂÖ∂‰ªñÈÄîÂæÑÂàÜÂèëÁöÑ ThuThesisÔºàÂåÖÂê´ÂÖ∂Âèò‰ΩìÊàñË°çÁîüÁâ©ÔºâÂùá‰∏çÊòØÂÆòÊñπÁâàÊú¨ÔºåËØ∑Ë∞®ÊÖé‰ΩøÁî®„ÄÇ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; &#xA;&lt;p&gt;ÊØè‰∏™ÁâàÊú¨ÁöÑËØ¶ÁªÜÊõ¥Êñ∞Êó•ÂøóÔºåËØ∑ËßÅ &lt;a href=&#34;https://raw.githubusercontent.com/tuna/thuthesis/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt;„ÄÇ‰ΩøÁî®ÊñáÊ°£‰∏≠‰πüÂåÖÂê´‰∫ÜËøô‰∏ÄÂÜÖÂÆπ„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;ÂçáÁ∫ß&lt;/h2&gt; &#xA;&lt;h3&gt;Ëá™Âä®Êõ¥Êñ∞&lt;/h3&gt; &#xA;&lt;p&gt;ÈÄöËøá TeX ÂèëË°åÁâàÂ∑•ÂÖ∑ÔºàÂ¶Ç &lt;code&gt;tlmgr&lt;/code&gt;ÔºâËá™Âä®‰ªé &lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt; Êõ¥Êñ∞„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;ÊâãÂä®Êõ¥Êñ∞&lt;/h3&gt; &#xA;&lt;h4&gt;ÂèëÂ∏ÉÁâà&lt;/h4&gt; &#xA;&lt;p&gt;‰∏ãËΩΩÂèëÂ∏ÉÁâàÁöÑÁöÑ zip ÂåÖÔºå‰ΩøÁî®ÂÖ∂‰∏≠ÁöÑ &lt;code&gt;thuthesis.cls&lt;/code&gt; Á≠âÊñá‰ª∂Ë¶ÜÁõñÂéüÊúâÁöÑÂç≥ÂèØÔºåÊó†È°ªÈ¢ùÂ§ñÊìç‰Ωú„ÄÇ&lt;/p&gt; &#xA;&lt;h4&gt;ÂºÄÂèëÁâà&lt;/h4&gt; &#xA;&lt;p&gt;‰ªé GitHub clone È°πÁõÆÊ∫êÁ†ÅÊàñËÄÖ‰∏ãËΩΩÊ∫êÁ†Å zip ÂåÖÔºåÊâßË°åÂëΩ‰ª§ÔºàWindows Áî®Êà∑Âú®Êñá‰ª∂Â§πÁ©∫ÁôΩÂ§ÑÊåâ &lt;code&gt;Shift + Èº†Ê†áÂè≥ÈîÆ&lt;/code&gt;ÔºåÁÇπÂáª‚ÄúÂú®Ê≠§Â§ÑÊâìÂºÄÂëΩ‰ª§Ë°åÁ™óÂè£‚ÄùÔºâÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xetex thuthesis.ins&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Âç≥ÂèØÂæóÂà∞ &lt;code&gt;thuthesis.cls&lt;/code&gt; Á≠âÊ®°ÊùøÊñá‰ª∂„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;ÊèêÈóÆ&lt;/h2&gt; &#xA;&lt;p&gt;ÊåâÊé®ËçêÈ°∫Â∫èÊéíÂ∫èÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ÂÖàÂà∞ &lt;a href=&#34;https://github.com/tuna/thuthesis/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; ÁúãÁúãÂ∏∏ËßÅÈóÆÈ¢ò&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/issues&#34;&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MakefileÁöÑÁî®Ê≥ï&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make [{all|thesis|spine|doc|clean|cleanall|distclean}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ÁõÆÊ†á&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt; ÁîüÊàêËÆ∫Êñá thuthesis-example.pdfÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make spine&lt;/code&gt; ÁîüÊàê‰π¶ËÑä spine.pdfÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make doc&lt;/code&gt; ÁîüÊàêÊ®°Êùø‰ΩøÁî®ËØ¥Êòé‰π¶ thuthesis.pdfÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make all&lt;/code&gt; ÁîüÊàêËÆ∫ÊñáÂíå‰π¶ËÑäÔºåÁõ∏ÂΩì‰∫é &lt;code&gt;make thesis &amp;amp;&amp;amp; make spine&lt;/code&gt;Ôºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; Âà†Èô§Á§∫‰æãÊñá‰ª∂ÁöÑ‰∏≠Èó¥Êñá‰ª∂Ôºà‰∏çÂê´ thuthesis-example.pdfÔºâÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt; Âà†Èô§Á§∫‰æãÊñá‰ª∂ÁöÑ‰∏≠Èó¥Êñá‰ª∂Âíå thuthesis-example.pdfÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; Âà†Èô§Á§∫‰æãÊñá‰ª∂ÂíåÊ®°ÊùøÁöÑÊâÄÊúâ‰∏≠Èó¥Êñá‰ª∂Âíå PDF„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ThuThesis&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThuThesis&lt;/strong&gt; is an abbreviation of &lt;strong&gt;T&lt;/strong&gt;sing&lt;strong&gt;h&lt;/strong&gt;ua &lt;strong&gt;U&lt;/strong&gt;niversity &lt;strong&gt;Thesis&lt;/strong&gt; LaTeX Template.&lt;/p&gt; &#xA;&lt;p&gt;This package establishes a simple and easy-to-use LaTeX template for Tsinghua dissertations, including general undergraduate research papers, masters theses, doctoral dissertations, and postdoctoral reports. An English translation of this README follows the Chinese below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This template is subject to frequent changes. Please make sure you have read the usage documentation and example code completely and carefully before using and asking questions.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Any use of ThuThesis in violation of &lt;a href=&#34;https://www.latex-project.org/lppl/lppl-1-3c/&#34;&gt;The LaTeX project public license v1.3c&lt;/a&gt; will be recorded in the &lt;a href=&#34;https://github.com/tuna/thuthesis/issues/754&#34;&gt;Hall of Shame&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloads&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Published versions&lt;/strong&gt; are recommended. Specific usage documentation and examples can be found in the archive. At present, these documents are &lt;b&gt;only available in Chinese&lt;/b&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Template usage documentation (thuthesis.pdf)&lt;/li&gt; &#xA; &lt;li&gt;Template example (thuthesis-example.pdf)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Developer versions contain only source code but no pre-compiled &lt;code&gt;cls&lt;/code&gt; file and documentations. They are only for the usage of developers and experienced TeX users in need of unpublished features. No warranties are provided.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Published versionsÔºö &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/github-release/tuna/thuthesis/&#34;&gt;TUNA Mirrors&lt;/a&gt;: mirror of GitHub Releases&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/thuthesis-tsinghua-university-thesis-latex-template/wddqnwbyhtnk&#34;&gt;Overleaf Template&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Developer versions: &lt;a href=&#34;https://github.com/tuna/thuthesis&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThuThesis (including its variants / derivatives) distributed in any other way is NOT an official version. Use at your own risk.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tuna/thuthesis/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for detailed changes in each release. They are also included in the usage documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;h3&gt;Automatic&lt;/h3&gt; &#xA;&lt;p&gt;Get the most up-to-date published version with your TeX distribution from &lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Manual&lt;/h3&gt; &#xA;&lt;h4&gt;Published versions&lt;/h4&gt; &#xA;&lt;p&gt;Download the published zip files, extract &lt;code&gt;thuthesis.cls&lt;/code&gt; and other files (if needed) and override the existing ones in your thesis.&lt;/p&gt; &#xA;&lt;h4&gt;Developer versions&lt;/h4&gt; &#xA;&lt;p&gt;Download the source code package and unzip to the root directory of your thesis (or clone this project), then execute the command (Windows users &lt;code&gt;Shift + right click&lt;/code&gt; white area in the file window and click &#34;Open command line window here&#34; from the popup menu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xetex thuthesis.ins&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll get &lt;code&gt;thuthesis.cls&lt;/code&gt; along with other template files.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the procedure below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the &lt;a href=&#34;https://github.com/tuna/thuthesis/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/issues&#34;&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Makefile Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make [{all|thesis|spine|doc|clean|cleanall|distclean}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Targets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt; generate thesis thuthesis-example.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make spine&lt;/code&gt; generate book spine for printing spine.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make doc&lt;/code&gt; generate template documentation thuthesis.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make all&lt;/code&gt; generate thesis and spine, same as &lt;code&gt;make thesis &amp;amp;&amp;amp; make spine&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; delete all examples&#39; files (excluding thuthesis-example.pdf);&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt; delete all examples&#39; files and thuthesis-example.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; delete all examples&#39; and templates&#39; files and PDFs.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>hmemcpy/milewski-ctfp-pdf</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/hmemcpy/milewski-ctfp-pdf</id>
    <link href="https://github.com/hmemcpy/milewski-ctfp-pdf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bartosz Milewski&#39;s &#39;Category Theory for Programmers&#39; unofficial PDF and LaTeX source&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Category Theory for Programmers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/601206/43392303-f770d7be-93fb-11e8-8db8-b7e915b435ba.png&#34; alt=&#34;image&#34;&gt; &lt;b&gt;Direct link: &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/download/v1.3.0/category-theory-for-programmers.pdf&#34;&gt;category-theory-for-programmers.pdf&lt;/a&gt;&lt;/b&gt;&lt;br&gt; (Latest release: v1.3.0, August 2019. See &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases&#34;&gt;releases&lt;/a&gt; for additional formats and languages.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/hmemcpy/milewski-ctfp-pdf&#34;&gt;&lt;img src=&#34;https://travis-ci.org/hmemcpy/milewski-ctfp-pdf.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://s3.amazonaws.com/milewski-ctfp-pdf/category-theory-for-programmers.pdf&#34;&gt;(latest CI build)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/601206/47271389-8eea0900-d581-11e8-8e81-5b932e336336.png&#34; alt=&#34;Buy Category Theory for Programmers&#34; width=&#34;410&#34;&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://www.blurb.com/b/9621951-category-theory-for-programmers-new-edition-hardco&#34;&gt;Available in full-color hardcover print&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; Publish date: 12 August, 2019. Based off release tag &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0&#34;&gt;v1.3.0&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hmemcpy/milewski-ctfp-pdf/master/errata-1.3.0.md&#34;&gt;errata-1.3.0&lt;/a&gt; for changes and fixes since print.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.blurb.com/b/9603882-category-theory-for-programmers-scala-edition-pape&#34;&gt;Scala Edition is now available in paperback&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; Publish date: 12 August, 2019. Based off release tag &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0&#34;&gt;v1.3.0&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hmemcpy/milewski-ctfp-pdf/master/errata-scala.md&#34;&gt;errata-scala&lt;/a&gt; for changes and fixes since print.&lt;/p&gt; &#xA;&lt;p&gt;This is an &lt;em&gt;unofficial&lt;/em&gt; PDF version of &#34;Category Theory for Programmers&#34; by Bartosz Milewski, converted from his &lt;a href=&#34;https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/&#34;&gt;blogpost series&lt;/a&gt; (with permission!)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;The best way to build the book is using the &lt;a href=&#34;https://nixos.org/nix/&#34;&gt;Nix&lt;/a&gt; package manager. After &lt;a href=&#34;https://nixos.org/download.html&#34;&gt;installing Nix&lt;/a&gt;, if you&#39;re using a non-NixOS operating system, you need to install &lt;code&gt;nixFlakes&lt;/code&gt; in your environment following the steps below (&lt;a href=&#34;https://nixos.wiki/wiki/Flakes#Non-NixOS&#34;&gt;source&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ nix-env -iA nixpkgs.nixFlakes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit either &lt;code&gt;~/.config/nix/nix.conf&lt;/code&gt; or &lt;code&gt;/etc/nix/nix.conf&lt;/code&gt; and add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;experimental-features = nix-command flakes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is needed to expose the Nix 2.0 CLI and flakes support that are hidden behind feature-flags.&lt;/p&gt; &#xA;&lt;p&gt;Also, if the Nix installation is in multi-user mode, don‚Äôt forget to restart the nix-daemon.&lt;/p&gt; &#xA;&lt;p&gt;Afterwards, type &lt;code&gt;nix flake show&lt;/code&gt; in the root directory of the project to see all the available versions of this book. Then type &lt;code&gt;nix build .#&amp;lt;edition&amp;gt;&lt;/code&gt; to build the edition you want (Haskell, Scala, OCaml, Reason and their printed versions). For example, to build the Scala edition you&#39;ll have to type &lt;code&gt;nix build .#ctfp-scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Upon successful compilation, the PDF file will be placed in the &lt;code&gt;result&lt;/code&gt; directory inside the root directory &lt;code&gt;milewski-ctfp-pdf&lt;/code&gt; of the repository.&lt;/p&gt; &#xA;&lt;p&gt;The file &lt;code&gt;preamble.tex&lt;/code&gt; contains all the configuration and style declarations.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;PDF LaTeX source and the tools to create it are based on the work by Andres Raba et al., available here: &lt;a href=&#34;https://github.com/sarabander/sicp-pdf&#34;&gt;https://github.com/sarabander/sicp-pdf&lt;/a&gt;.&lt;br&gt; The book content is taken, with permission, from Bartosz Milewski&#39;s blogpost series, and adapted to the LaTeX format.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to the following people for contributing corrections/conversions and misc:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Oleg Rakitskiy&lt;/li&gt; &#xA; &lt;li&gt;Jared Weakly&lt;/li&gt; &#xA; &lt;li&gt;Paolo G. Giarrusso&lt;/li&gt; &#xA; &lt;li&gt;Adi Shavit&lt;/li&gt; &#xA; &lt;li&gt;Mico Loretan&lt;/li&gt; &#xA; &lt;li&gt;Marcello Seri&lt;/li&gt; &#xA; &lt;li&gt;Erwin Maruli Tua Pakpahan&lt;/li&gt; &#xA; &lt;li&gt;Markus Hauck&lt;/li&gt; &#xA; &lt;li&gt;Yevheniy Zelenskyy&lt;/li&gt; &#xA; &lt;li&gt;Ross Kirsling&lt;/li&gt; &#xA; &lt;li&gt;...and many others!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original blog post acknowledgments by Bartosz are consolidated in the &lt;em&gt;Acknowledgments&lt;/em&gt; page at the end of the book.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note from Bartosz&lt;/strong&gt;: I really appreciate all your contributions. You made this book much better than I could have imagined. Thank you!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The PDF book, &lt;code&gt;.tex&lt;/code&gt; files, and associated images and figures in directories &lt;code&gt;src/fig&lt;/code&gt; and &lt;code&gt;src/content&lt;/code&gt; are licensed under Creative Commons Attribution-ShareAlike 4.0 International License (&lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;cc by-sa&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The script files &lt;code&gt;scraper.py&lt;/code&gt; and others are licensed under GNU General Public License version 3 (for details, see &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/raw/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openmlsys/openmlsys-zh</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/openmlsys/openmlsys-zh</id>
    <link href="https://github.com/openmlsys/openmlsys-zh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;„ÄäMachine Learning Systems: Design and Implementation„Äã- Chinese Version&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Êú∫Âô®Â≠¶‰π†Á≥ªÁªüÔºöËÆæËÆ°ÂíåÂÆûÁé∞&lt;/h1&gt; &#xA;&lt;p&gt;Êú¨ÂºÄÊ∫êÈ°πÁõÆËØïÂõæÁªôËØªËÄÖËÆ≤Ëß£Áé∞‰ª£Êú∫Âô®Â≠¶‰π†Á≥ªÁªüÁöÑËÆæËÆ°ÂéüÁêÜÂíåÂÆûÁé∞ÁªèÈ™å„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;üî• &lt;strong&gt;‰π¶Á±çÁΩëÈ°µÁâàÔºö&lt;/strong&gt; &lt;a href=&#34;https://openmlsys.github.io/&#34;&gt;Êú∫Âô®Â≠¶‰π†Á≥ªÁªüÔºöËÆæËÆ°ÂíåÂÆûÁé∞&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üî• &lt;strong&gt;‰π¶Á±çPDFÔºö&lt;/strong&gt; Â∞ÜÂú®ÂãòËØØÂêéÔºåÂõõÊúàÂ∫ïÂèëÂ∏É&lt;/p&gt; &#xA;&lt;h2&gt;ÂèëÂ∏É&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;17/03/2022: Êú¨‰π¶Â§Ñ‰∫éÂãòËØØÈò∂ÊÆµ„ÄÇÂ¶ÇÂèëÁé∞ÊñáÂ≠óÂíåÂõæÁâáÈîôËØØÔºåÂèØÂàõÂª∫IssueÂπ∂@&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/editors.md&#34;&gt;Á´†ËäÇÁºñËæë&lt;/a&gt;„ÄÇÊàë‰ª¨ÈùûÂ∏∏Ê¨¢ËøéÁ§æÂå∫Êèê‰∫§PRÁõ¥Êé•ÂãòËØØ„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÈÄÇÁî®ËØªËÄÖ&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨‰π¶ÁöÑÂ∏∏ËßÅËØªËÄÖÂåÖÊã¨Ôºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Â≠¶ÁîüÔºö&lt;/strong&gt; ÈöèÁùÄÂ§ßÈáèÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÂú®Â§ßÂ≠¶‰∏≠ÁöÑÊôÆÂèäÔºåÂ≠¶ÁîüÂ∑≤ÁªèÂºÄÂßãÊéåÊè°Â§ßÈáèÊú∫Âô®Â≠¶‰π†ÁöÑÂü∫Á°ÄÁêÜËÆ∫ÂíåÁ•ûÁªèÁΩëÁªúÁöÑÂÆûÁé∞„ÄÇÁÑ∂ËÄåÔºåÈúÄË¶ÅËÆ≠ÁªÉÂá∫ÂèØ‰ª•ÂÆûÈôÖÂ∫îÁî®ÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåÈúÄË¶ÅÂØπÁé∞‰ª£Êú∫Âô®Â≠¶‰π†Á≥ªÁªüÊúâÂÖÖÂàÜÁöÑËÆ§ËØÜ„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÁßëÁ†î‰∫∫ÂëòÔºö&lt;/strong&gt; Á†îÂèëÊñ∞ÂûãÁöÑÊú∫Âô®Â≠¶‰π†Ê®°Âûã‰∏ç‰ªÖ‰ªÖÈúÄË¶Å‰ºö‰ΩøÁî®Âü∫Á°ÄÁöÑÊú∫Âô®Â≠¶‰π†Á≥ªÁªüÊé•Âè£„ÄÇÂêåÊó∂ÔºåÊñ∞ÂûãÁöÑÊ®°ÂûãÈúÄË¶ÅÁªôÁ≥ªÁªüÊèê‰æõÊñ∞ÁöÑËá™ÂÆö‰πâÁÆóÂ≠êÔºàCustom OperatorsÔºâÔºåÂèàÊàñËÄÖÊòØ‰ºöÂà©Áî®È´òÁ∫ßÁöÑÂàÜÂ∏ÉÂºèÊâßË°åÁÆóÂ≠êÊù•ÂÆûÁé∞Â§ßÊ®°ÂûãÁöÑÂºÄÂèë„ÄÇËøô‰∏ÄÁ≥ªÂàóÈúÄÊ±ÇÈÉΩÈúÄË¶ÅÂØπÂ∫ïÂ±ÇÁ≥ªÁªüÂÖ∑ÊúâÂÖÖÂàÜËÆ§ËØÜ„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂºÄÂèë‰∫∫ÂëòÔºö&lt;/strong&gt; Â§ßÈáèÁöÑÊï∞ÊçÆÂíåAIÈ©±Âä®ÁöÑÂÖ¨Âè∏ÈÉΩÈÉ®ÁΩ≤‰∫ÜÊú∫Âô®Â≠¶‰π†Âü∫Á°ÄËÆæÊñΩ„ÄÇËøô‰∏ÄËÆæÊñΩÁöÑÊ†∏ÂøÉÂ∞±ÊòØÊú∫Âô®Â≠¶‰π†Á≥ªÁªü„ÄÇÂõ†Ê≠§‰∫ÜËß£Êú∫Âô®Â≠¶‰π†Á≥ªÁªüÊúâÂä©‰∫éÂºÄÂèë‰∫∫ÂëòÂØπ‰∫éÁ≥ªÁªüÊÄßËÉΩË∞É‰ºòÔºå‰ª•ÂÆö‰ΩçÈóÆÈ¢òÔºåÂπ∂‰∏îÊ†πÊçÆ‰∏öÂä°ÈúÄÊ±ÇÂØπÊú∫Âô®Â≠¶‰π†Á≥ªÁªüËøõË°åÊ∑±Â∫¶ÂÆöÂà∂„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÂÜÖÂÆπ‰ªãÁªç&lt;/h2&gt; &#xA;&lt;p&gt;Áé∞‰ª£Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÂÖ∑ÊúâÂ§çÊùÇÁöÑÂÜÖÈÉ®Êû∂ÊûÑÂíåÁπÅÂ§öÁöÑÂ§ñÈÉ®Áõ∏ÂÖ≥ÁªÑ‰ª∂„ÄÇÂú®Êú¨‰π¶‰∏≠ÔºåÊàë‰ª¨Â∞ÜÂØπÂÖ∂ÁªÜËá¥ÊãÜÂàÜÔºåÊ∑±ÂÖ•Ëß£ËØªÔºö&lt;/p&gt; &#xA;&lt;p&gt;Âü∫Á°ÄÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÁºñÁ®ãÊé•Âè£Ôºö&lt;/strong&gt; ‰∏∫‰∫ÜÊîØÊåÅÊµ∑ÈáèÂ∫îÁî®ÔºåÊú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÁöÑÁºñÁ®ãÊé•Âè£ËÆæËÆ°ÂÖ∑ÊúâÂ§ßÈáèÁöÑËÆæËÆ°Âì≤Â≠¶ÔºåÂú®ÊòìÁî®ÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÂèñÂæóÂπ≥Ë°°„ÄÇÊú¨‰π¶Â∞ÜËÆ≤Ëø∞ÁºñÁ®ãÊé•Âè£ÁöÑÊºîËøõÔºåÊú∫Âô®Â≠¶‰π†Â∑•‰ΩúÊµÅÔºåÂÆö‰πâÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºå‰ª•ÂèäÁî®C/C++ËøõË°åÊ°ÜÊû∂ÂºÄÂèë„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ËÆ°ÁÆóÂõæÔºö&lt;/strong&gt; Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÈúÄË¶ÅÊîØÊåÅËá™Âä®ÂæÆÂàÜÔºåÁ°¨‰ª∂Âä†ÈÄüÂô®ÔºåÂ§öÁºñÁ®ãÂâçÁ´ØÁ≠â„ÄÇÂÆûÁé∞Ëøô‰∫õÊîØÊåÅÁöÑÊ†∏ÂøÉÊäÄÊúØÊòØÔºöËÆ°ÁÆóÂõæÔºàComputational GraphÔºâ„ÄÇÊú¨‰π¶Â∞ÜËÆ≤Ëø∞ËÆ°ÁÆóÂõæÁöÑÂü∫Êú¨ÊûÑÊàêÔºåÁîüÊàêÊñπÊ≥ïÂíåË∞ÉÂ∫¶Á≠ñÁï•„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ÊÄßËÉΩËøõÈò∂Ôºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÁºñËØëÂô®ÂâçÁ´ØÔºö&lt;/strong&gt; Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÈúÄË¶ÅÂà©Áî®ÁºñËØëÂô®ÂâçÁ´ØÊäÄÊúØÂØπËÆ°ÁÆóÂõæËøõË°åÂäüËÉΩÊãìÂ±ïÂíåÊÄßËÉΩ‰ºòÂåñ„ÄÇÊú¨‰π¶Â∞ÜËÆ≤Ëø∞Â∏∏ËßÅÁöÑÂâçÁ´ØÊäÄÊúØÔºåÂåÖÊã¨Á±ªÂûãÊé®ÂØºÔºå‰∏≠Èó¥Ë°®Á§∫ÔºàIntermediate RepresentationÔºâÔºåËá™Âä®ÂæÆÂàÜÁ≠â„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÁºñËØëÂô®ÂêéÁ´ØÂíåËøêË°åÊó∂Ôºö&lt;/strong&gt; Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÁöÑ‰∏Ä‰∏™Ê†∏ÂøÉÁõÆÊ†áÊòØÔºöÂ¶Ç‰ΩïÂÖÖÂàÜÂà©Áî®ÂºÇÊûÑÁ°¨‰ª∂„ÄÇËøôÂÖ∂‰∏≠‰ºöÊ∂âÂèäÁºñËØëÂô®ÂêéÁ´ØÊäÄÊúØÔºå‰ª•ÂèäÂ∞ÜËÆ°ÁÆóÂõæÁÆóÂ≠êÔºàOperatorÔºâË∞ÉÂ∫¶Âà∞Á°¨‰ª∂‰∏äÁöÑËøêË°åÊó∂ÔºàRuntimeÔºâ„ÄÇÊú¨‰π¶Â∞ÜËÆ≤Ëø∞ËÆ°ÁÆóÂõæ‰ºòÂåñÔºåÁÆóÂ≠êÈÄâÊã©ÔºåÂÜÖÂ≠òÂàÜÈÖçÂíåËÆ°ÁÆóË∞ÉÂ∫¶‰∏éÊâßË°å„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á°¨‰ª∂Âä†ÈÄüÂô®Ôºö&lt;/strong&gt; Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÁöÑÂü∫Êú¨ËøêË°åÂçïÂÖÉÊòØÁÆóÂ≠êÔºåËÄåÁÆóÂ≠êÁöÑÂÆûÁé∞ÂøÖÈ°ªÂÖÖÂàÜÂà©Áî®Á°¨‰ª∂Âä†ÈÄüÂô®ÔºàGPUÂíåAscendÔºâÁöÑÁâπÊÄß„ÄÇÊú¨‰π¶Â∞Ü‰ºöËÆ≤Ëø∞Á°¨‰ª∂Âä†ÈÄüÂô®ÁöÑÂü∫Êú¨ÊûÑÊàêÂéüÁêÜÂíåÂ∏∏ËßÅÁöÑÈ´òÊÄßËÉΩÁºñÁ®ãÊé•Âè£„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êï∞ÊçÆÂ§ÑÁêÜÊ°ÜÊû∂Ôºö&lt;/strong&gt; Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂‰ºöÈõÜÊàêÈ´òÊÄßËÉΩÊ°ÜÊû∂Êù•ËøõË°åÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÇÊú¨‰π¶Â∞Ü‰ºöËÆ≤Ëø∞Ëøô‰∏ÄÁ±ªÊï∞ÊçÆÂ§ÑÁêÜÊ°ÜÊû∂Âú®ËÆæËÆ°‰∏≠ÈúÄË¶ÅËææÂà∞ÁöÑÂ§ö‰∏™ÁõÆÊ†áÔºöÊòìÁî®ÊÄßÔºåÈ´òÊïàÊÄßÔºå‰øùÂ∫èÊÄßÔºåÂàÜÂ∏ÉÂºèÁ≠â„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ê®°ÂûãÈÉ®ÁΩ≤Ôºö&lt;/strong&gt; Âú®Ê®°ÂûãÂÆåÊàêËÆ≠ÁªÉÂêéÔºåÁî®Êà∑ÈúÄË¶ÅÂ∞ÜÊ®°ÂûãÈÉ®ÁΩ≤Âà∞ÁªàÁ´ØËÆæÂ§áÔºàÂ¶Ç‰∫ëÊúçÂä°Âô®ÔºåÁßªÂä®ÁªàÁ´ØÂíåÊó†‰∫∫ËΩ¶Ôºâ„ÄÇËøôÂÖ∂‰∏≠Ê∂âÂèäÂà∞ÁöÑÊ®°ÂûãËΩ¨Êç¢ÔºåÊ®°ÂûãÂéãÁº©ÔºåÊ®°ÂûãÊé®ÁêÜÂíåÂÆâÂÖ®‰øùÊä§Á≠âÁü•ËØÜ‰πü‰ºöÂú®Êú¨‰π¶‰∏≠ËÆ®ËÆ∫„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÔºö&lt;/strong&gt; Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑËÆ≠ÁªÉÈúÄË¶ÅÊ∂àËÄóÂ§ßÈáèËµÑÊ∫ê„ÄÇË∂äÊù•Ë∂äÂ§öÁöÑÊú∫Âô®Â≠¶‰π†Ê°ÜÊû∂Âõ†Ê≠§ÂéüÁîüÊîØÊåÅÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ„ÄÇÂú®Êú¨‰π¶‰∏≠Êàë‰ª¨Â∞Ü‰ºöËÆ®ËÆ∫Â∏∏ËßÅÁöÑÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÊñπÊ≥ïÔºàÂåÖÊã¨Êï∞ÊçÆÂπ∂Ë°åÔºåÊ®°ÂûãÂπ∂Ë°åÂíåÊµÅÊ∞¥Á∫øÂπ∂Ë°åÔºâÔºå‰ª•ÂèäÂÆûÁé∞Ëøô‰∫õÊñπÊ≥ïÁöÑÁ≥ªÁªüÊû∂ÊûÑÔºàÂåÖÊã¨ÈõÜÂêàÈÄöËÆØÂíåÂèÇÊï∞ÊúçÂä°Âô®Ôºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ÂäüËÉΩÊãìÂ±ïÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ê∑±Â∫¶Â≠¶‰π†Êé®ËçêÁ≥ªÁªüÔºö&lt;/strong&gt; Êé®ËçêÁ≥ªÁªüÊòØÁõÆÂâçÊú∫Âô®Â≠¶‰π†Â∫îÁî®ÊúÄÊàêÂäüÁöÑÈ¢ÜÂüü‰πã‰∏Ä„ÄÇÊú¨‰π¶Â∞Ü‰ºöÊ¶ÇÊã¨Êé®ËçêÁ≥ªÁªüÁöÑËøê‰ΩúÂéüÁêÜÔºåËØ¶ÁªÜÊèèËø∞Â§ßËßÑÊ®°Â∑•‰∏öÂú∫ÊôØ‰∏ãÁöÑÊé®ËçêÁ≥ªÁªüÊû∂ÊûÑËÆæËÆ°„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ËÅîÈÇ¶Â≠¶‰π†Á≥ªÁªüÔºö&lt;/strong&gt; ÈöèÁùÄÊï∞ÊçÆ‰øùÊä§Ê≥ïËßÑÂíåÈöêÁßÅ‰øùÊä§ÁöÑÂ¥õËµ∑ÔºåËÅîÈÇ¶Â≠¶‰π†Ê≠£Êàê‰∏∫Êó•ÁõäÈáçË¶ÅÁöÑÁ†îÁ©∂È¢ÜÂüü„ÄÇÊú¨‰π¶Â∞Ü‰ºö‰ªãÁªçËÅîÈÇ¶Â≠¶‰π†ÁöÑÂ∏∏Áî®ÊñπÊ≥ï‰ª•ÂèäÁõ∏ÂÖ≥Á≥ªÁªüÂÆûÁé∞„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Âº∫ÂåñÂ≠¶‰π†Á≥ªÁªüÔºö&lt;/strong&gt; Âº∫ÂåñÂ≠¶‰π†ÊòØËµ∞ÂêëÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÖ≥ÈîÆÊäÄÊúØ„ÄÇÊú¨‰π¶Â∞Ü‰ºö‰ªãÁªçÁõÆÂâçÂ∏∏ËßÅÁöÑÂº∫ÂåñÂ≠¶‰π†Á≥ªÁªüÔºàÂåÖÊã¨ÂçïÊô∫ËÉΩ‰ΩìÂíåÂ§öÊô∫ËÉΩ‰ΩìÁ≠âÔºâ„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂèØËß£ÈáäÊÄßAIÁ≥ªÁªüÔºö&lt;/strong&gt; ÈöèÁùÄÊú∫Âô®Â≠¶‰π†Âú®ÂÆâÂÖ®Êî∏ÂÖ≥ÔºàSafety-criticalÔºâÈ¢ÜÂüüÁöÑÂ∫îÁî®ÔºåÊú∫Âô®Â≠¶‰π†Á≥ªÁªüË∂äÊù•Ë∂äÈúÄË¶ÅÂØπÂÜ≥Á≠ñÁªôÂá∫ÂÖÖÂàÜËß£Èáä„ÄÇÊú¨‰π¶Â∞Ü‰ºöËÆ®ËÆ∫ÂèØËß£ÈáäAIÁ≥ªÁªüÁöÑÂ∏∏Áî®ÊñπÊ≥ïÂíåËêΩÂú∞ÂÆûË∑µÁªèÈ™å„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫Á≥ªÁªüÔºö&lt;/strong&gt; Êú∫Âô®‰∫∫ÔºàÊó†‰∫∫ËΩ¶ÔºåÊó†‰∫∫Êú∫ÔºåÂÆ∂Áî®Êú∫Âô®‰∫∫Á≠âÔºâ‰Ωú‰∏∫Êú∫Âô®Â≠¶‰π†ÊäÄÊúØÈáçË¶ÅÁöÑÂ∫îÁî®È¢ÜÂüüÔºåÂú®ÊúÄËøëÊï∞Âπ¥ÂæóÂà∞‰∫ÜÂπøÊ≥õÂ∫îÁî®„ÄÇÂú®ÂÆûË∑µ‰∏≠ÔºåÊú∫Âô®‰∫∫Á≥ªÁªüÂú®ÂÆûÊó∂ÊÄßÔºåÂÆâÂÖ®ÊÄßÔºåÈ≤ÅÊ£íÊÄßÁ≠âÊñπÈù¢ÈÉΩÊúâÊûÅÈ´òË¶ÅÊ±ÇÔºåËøôË¶ÅÊ±ÇÂºÄÂèëËÄÖÂÖ∑ÊúâÁÆóÊ≥ïÂíåÁ≥ªÁªüÁöÑÂèåÈáçÊÄùÁª¥Ôºå‰ªéËÄåËß£ÂÜ≥ÂÆûÈôÖÈóÆÈ¢ò„ÄÇÊú¨‰π¶‰∏≠Êàë‰ª¨Â∞ÜÁªìÂêàÊúÄÊñ∞Á†îÁ©∂ÊàêÊûúÂíåÊú∫Âô®‰∫∫Á≥ªÁªüÂÆûË∑µÁªèÈ™åËÆ≤Ëß£ËØ•Á±ªÁ≥ªÁªüÁöÑËÆæËÆ°ÂéüÂàôÂíåÂÆûÁé∞ÁªÜËäÇ„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Êàë‰ª¨Âú®ÊåÅÁª≠ÊãìÂ±ïÊãìÂ±ïÊú¨‰π¶ÁöÑÂÜÖÂÆπÔºåÂ¶ÇÂÖÉÂ≠¶‰π†Á≥ªÁªüÔºåËá™Âä®Âπ∂Ë°åÔºåÊ∑±Â∫¶Â≠¶‰π†ÈõÜÁæ§Ë∞ÉÂ∫¶ÔºåÁªøËâ≤AIÁ≥ªÁªüÔºåÂõæÂ≠¶‰π†Á≥ªÁªüÁ≠â„ÄÇÊàë‰ª¨‰πüÈùûÂ∏∏Ê¨¢ËøéÁ§æÂå∫ÂØπ‰∫éÊñ∞ÂÜÖÂÆπÊèêÂá∫Âª∫ËÆÆÔºåË¥°ÁåÆÁ´†ËäÇ„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;ÊûÑÂª∫ÊåáÂçó&lt;/h2&gt; &#xA;&lt;p&gt;ËØ∑ÂèÇËÄÉ&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/info.md&#34;&gt;ÊûÑÂª∫ÊåáÂçó&lt;/a&gt;Êù•‰∫ÜËß£Â¶Ç‰ΩïÊûÑÂª∫Êú¨‰π¶ÁöÑÁΩëÈ°µÁâàÊú¨ÂíåPDFÁâàÊú¨„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;ÂÜô‰ΩúÊåáÂçó&lt;/h2&gt; &#xA;&lt;p&gt;Êàë‰ª¨Ê¨¢ËøéÂ§ßÂÆ∂Êù•‰∏ÄËµ∑Ë¥°ÁåÆÂíåÊõ¥Êñ∞Êú¨‰π¶ÁöÑÂÜÖÂÆπ„ÄÇÂ∏∏ËßÅÁöÑË¥°ÁåÆÊñπÂºèÊòØÊèê‰∫§PRÊù•Êõ¥Êñ∞ÂíåÊ∑ªÂä†MarkdownÊñá‰ª∂„ÄÇÂÜô‰ΩúÁöÑÈ£éÊ†ºÂíåÂõæÁâáË¶ÅÊ±ÇËØ∑ÂèÇËÄÉ&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/style.md&#34;&gt;È£éÊ†ºÊåáÂçó&lt;/a&gt;„ÄÇÂêåÊó∂ÔºåÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÊ∂âÂèäÂà∞Â§ßÈáèÁöÑ‰∏≠Ëã±ÊñáÁøªËØëÔºåÁõ∏ÂÖ≥ÁöÑÁøªËØëË¶ÅÊ±ÇËØ∑ÂèÇËÄÉ&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/terminology.md&#34;&gt;ÊúØËØ≠ÊåáÂçó&lt;/a&gt;„ÄÇ&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deedy/Deedy-Resume</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/deedy/Deedy-Resume</id>
    <link href="https://github.com/deedy/Deedy-Resume" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A one page , two asymmetric column resume template in XeTeX that caters to an undergraduate Computer Science student&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deedy-Resume&lt;/h1&gt; &#xA;&lt;p&gt;A &lt;strong&gt;one-page&lt;/strong&gt;, &lt;strong&gt;two asymmetric column&lt;/strong&gt; resume template in &lt;strong&gt;XeTeX&lt;/strong&gt; that caters particularly to an &lt;strong&gt;undergraduate Computer Science&lt;/strong&gt; student. As of &lt;strong&gt;v1.2&lt;/strong&gt;, there is an option to choose from two templates:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;MacFonts&lt;/strong&gt; - uses fonts native to OSX - &lt;em&gt;Helvetica&lt;/em&gt;, &lt;em&gt;Helvetica Neue&lt;/em&gt; (and it&#39;s Light and Ultralight versions) and the CJK fonts &lt;em&gt;Heiti SC&lt;/em&gt;, and &lt;em&gt;Heiti TC&lt;/em&gt;. The EULA of these fonts prevents distribution on Open Source.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenFonts&lt;/strong&gt; - uses free, open-source fonts that resemble the above - &lt;em&gt;Lato&lt;/em&gt; (and its various variants) and &lt;em&gt;Raleway&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;It is licensed under the Apache License 2.0.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;Common LaTeX resume-builders such as &lt;a href=&#34;http://www.latextemplates.com/template/moderncv-cv-and-cover-letter&#34;&gt;&lt;strong&gt;moderncv&lt;/strong&gt;&lt;/a&gt; and the &lt;a href=&#34;https://github.com/afriggeri/cv&#34;&gt;&lt;strong&gt;friggeri-cv&lt;/strong&gt;&lt;/a&gt; look great if you&#39;re looking for a multi-page resume with numerous citations, but usually imperfect for making a thorough, single-page one. A lot of companies today search resumes based on &lt;a href=&#34;http://www.businessinsider.com/most-big-companies-have-a-tracking-system-that-scans-your-resume-for-keywords-2012-1&#34;&gt;keywords&lt;/a&gt; but at the same time require/prefer a one-page resume, especially for undergraduates.&lt;/p&gt; &#xA;&lt;p&gt;This template attempts to &lt;strong&gt;look clean&lt;/strong&gt;, highlight &lt;strong&gt;details&lt;/strong&gt;, be a &lt;strong&gt;single page&lt;/strong&gt;, and allow useful &lt;strong&gt;LaTeX templating&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;h3&gt;OpenFonts&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/OpenFonts/sample-image.png&#34; alt=&#34;alt tag&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;MacFonts&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deedydas/Deedy-Resume/master/MacFonts/sample-image.png&#34; alt=&#34;alt tag&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compiles only with &lt;strong&gt;XeTeX&lt;/strong&gt; and required &lt;strong&gt;BibTex&lt;/strong&gt; for compiling publications and the .bib filetype.&lt;/li&gt; &#xA; &lt;li&gt;Uses fonts that are usually only available to &lt;strong&gt;Mac&lt;/strong&gt; users such as Helvetica Neue Light.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Availability&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;MacFonts version - &lt;a href=&#34;http://debarghyadas.com/resume/debarghya-das-resume.pdf&#34;&gt;as an online preview&lt;/a&gt; and &lt;a href=&#34;https://github.com/deedydas/Deedy-Resume/raw/master/MacFonts/deedy_resume.pdf&#34;&gt;as a direct download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenFonts version - &lt;a href=&#34;https://github.com/deedydas/Deedy-Resume/raw/master/OpenFonts/deedy_resume-openfont.pdf&#34;&gt;as a direct download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overleaf&lt;/strong&gt;.com (formerly &lt;strong&gt;WriteLatex&lt;/strong&gt;.com) (v1 fonts/colors changed) - &lt;a href=&#34;https://www.writelatex.com/templates/deedy-resume/sqdbztjjghvz#.U2H9Kq1dV18&#34;&gt;compilable online&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ShareLatex&lt;/strong&gt;.com (v1 fonts changes) - &lt;a href=&#34;https://www.sharelatex.com/templates/cv-or-resume/deedy-resume&#34;&gt;compilable online&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;v1.2&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Added publications in place of societies.&lt;/li&gt; &#xA; &lt;li&gt;Collapsed a portion of education.&lt;/li&gt; &#xA; &lt;li&gt;Fixed a bug with alignment of overflowing long last updated dates on the top right.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;v1.1&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fixed several compilation bugs with \renewcommand&lt;/li&gt; &#xA; &lt;li&gt;Got Open-source fonts (Windows/Linux support)&lt;/li&gt; &#xA; &lt;li&gt;Added Last Updated&lt;/li&gt; &#xA; &lt;li&gt;Moved Title styling into .sty&lt;/li&gt; &#xA; &lt;li&gt;Commented .sty file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Merge OpenFont and MacFonts as a single sty with options.&lt;/li&gt; &#xA; &lt;li&gt;Figure out a smoother way for the document to flow onto the next page.&lt;/li&gt; &#xA; &lt;li&gt;Add styling information for a &#34;Projects/Hacks&#34; section.&lt;/li&gt; &#xA; &lt;li&gt;Add location/address information&lt;/li&gt; &#xA; &lt;li&gt;Fix the hacky &#39;References&#39; omission outside the .cls file in the MacFonts version.&lt;/li&gt; &#xA; &lt;li&gt;Add various styling and section options and allow for multiple pages smoothly.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Known Issues:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Overflows onto second page if any column&#39;s contents are more than the vertical limit&lt;/li&gt; &#xA; &lt;li&gt;Hacky space on the first bullet point on the second column.&lt;/li&gt; &#xA; &lt;li&gt;Hacky redefinition of \refname to omit &#39;References&#39; text for publications in the MacFonts version.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2014 Debarghya Das&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ElegantLaTeX/ElegantBook</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/ElegantLaTeX/ElegantBook</id>
    <link href="https://github.com/ElegantLaTeX/ElegantBook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Elegant LaTeX Template for Books&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://elegantlatex.org/&#34;&gt;Homepage&lt;/a&gt; | &lt;a href=&#34;https://github.com/ElegantLaTeX/ElegantBook&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://ctan.org/pkg/elegantbook&#34;&gt;CTAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/ElegantLaTeX/ElegantBook/releases&#34;&gt;Download&lt;/a&gt; | &lt;a href=&#34;https://github.com/ElegantLaTeX/ElegantBook/wiki&#34;&gt;Wiki&lt;/a&gt; | &lt;a href=&#34;https://weibo.com/elegantlatex&#34;&gt;Weibo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/ctan/l/elegantbook.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/ctan/v/elegantbook.svg?sanitize=true&#34; alt=&#34;CTAN Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/ElegantLaTeX/ElegantBook.svg?sanitize=true&#34; alt=&#34;Github Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/repo-size/ElegantLaTeX/ElegantBook.svg?sanitize=true&#34; alt=&#34;Repo Size&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ElegantBook: An Elegant LaTeX Template for Books&lt;/h1&gt; &#xA;&lt;p&gt;ElegantBook is designed for writing books, created by &lt;a href=&#34;https://ddswhu.me/&#34;&gt;Dongsheng Deng&lt;/a&gt; and &lt;a href=&#34;https://liam.page/&#34;&gt;Liam Huang&lt;/a&gt;. Just enjoy it! If you have any questions, suggestions or bug reports, you can create issues or contact us at &lt;a href=&#34;mailto:elegantlatex2e@gmail.com&#34;&gt;elegantlatex2e@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Important Notes&lt;/h2&gt; &#xA;&lt;p&gt;For some reasons, &lt;strong&gt;unauthorized&lt;/strong&gt; pull requests are &lt;strong&gt;UNACCEPTABLE&lt;/strong&gt; since May 20, 2019. For those who want to help revise the templates, submit issues or clone to your own repository to modify under the LPPL-1.3c.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thank &lt;a href=&#34;https://github.com/sikouhjw&#34;&gt;sikouhjw&lt;/a&gt; and &lt;a href=&#34;https://github.com/syvshc&#34;&gt;syvshc&lt;/a&gt; for their quick response to Github issues and continuously support work for ElegantLaTeX.&lt;/p&gt; &#xA;&lt;p&gt;Thank ChinaTeX and &lt;a href=&#34;http://www.latexstudio.net/&#34;&gt;LaTeX Studio&lt;/a&gt; for their promotion.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This work is released under the LaTeX Project Public License, v1.3c or later.&lt;/p&gt; &#xA;&lt;h2&gt;Derivative Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XiangyunHuang/ElegantBookdown&#34;&gt;ElegantBookdown&lt;/a&gt;Ôºö&lt;a href=&#34;https://github.com/XiangyunHuang&#34;&gt;XiangyunHuang&lt;/a&gt; developed a Bookdown template based on ElegantBook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pzhaonet/bookdownplus&#34;&gt;bookdownplus&lt;/a&gt;: maintained by &lt;a href=&#34;https://github.com/pzhaonet&#34;&gt;pzhaonet&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/annProg/PanBook&#34;&gt;PanBook&lt;/a&gt;Ôºöa markdown-based writing workflow Developed by &lt;a href=&#34;https://github.com/annProg&#34;&gt;annProg&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>rstudio/cheatsheets</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/rstudio/cheatsheets</id>
    <link href="https://github.com/rstudio/cheatsheets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RStudio Cheat Sheets&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;RStudio Cheatsheets&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/rstudio/cheatsheets/main/pngs/rstudio-ide.png&#34; width=&#34;364&#34; height=&#34;288&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;The cheatsheets make it easy to learn about and use some of our favorite packages. They are published in their respective PDF versions here: &lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;https://www.rstudio.com/resources/cheatsheets/&lt;/a&gt;, some are also available in the RStudio IDE under Help &amp;gt; Cheat Sheets.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the source files of the current, archived and translated versions.&lt;/p&gt; &#xA;&lt;p&gt;The cheatsheets use the creative commons copyright. Please see the LICENSE document for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to contribute to this effort by translating a cheatsheet, please feel free to use the source Keynote file. To submit a translation, please use a Pull Request via GitHub. See the &lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/main/.github/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Tips for making a new RStudio cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;RStudio cheatsheets are not meant to be text or documentation!&lt;/strong&gt; They are scannable visual aids that use layout and visual mnemonics to help people zoom to the functions they need. Think of cheatsheets as a quick reference, with the emphasis on quick. Here&#39;s an analogy:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A cheatsheet is more like a well-organized computer menu bar that leads you to a command than like a manual that documents each command.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Everything about your cheatsheet should be designed to lead users to essential information &lt;em&gt;quickly&lt;/em&gt;. If you are summarizing the documentation manual, you are doing it wrong! Here are some tips to help you do it right:&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;RStudio cheatsheets are hosted at &lt;a href=&#34;https://github.com/rstudio/cheatsheets&#34;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;. You can submit new cheatsheets to the repository with a pull request. See the &lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/main/.github/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The files &lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/main/keynotes/0-template.key&#34;&gt;keynotes/0-template.key&lt;/a&gt; and &lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/main/powerpoints/0-template.pptx&#34;&gt;powerpoints/0-template.ppt&lt;/a&gt; are official templates that contain some helpful tips.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You may find it easiest to create a new cheatsheet by duplicating the most recent Keynote / Powerpoint cheatsheet and then heavily editing it‚Äîthat&#39;s what I do!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Process&lt;/h3&gt; &#xA;&lt;p&gt;Budget more time than you expect to make the sheets. So far, I&#39;ve found this process to be the least time consuming:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Identify which functions to include&lt;/strong&gt; by reading the package web page and vignettes. I try to limit my cheatsheets to the essentials.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize the functions&lt;/strong&gt; into meaningful, self-explanatory groups. Each group should address a common problem or task.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Think about how to visualize the purpose of each function.&lt;/strong&gt; Visual mnemonics are easier to scan than text, which all looks the same.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Think about&lt;/strong&gt; what &lt;strong&gt;key mental models&lt;/strong&gt;, definitions, or explanations the cheatsheet should contain in addition to the functions. Ideally, use these to explain the visualizations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sketch out several possible layouts&lt;/strong&gt; for the sheet. Take care to put the more basic and/or pre-requisite content above and to the left of other content. Try to keep related content on the same side of the page. often your final layout will itself be a &#34;mental map&#34; for the topic of the cheatsheet.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Type out all of the explanations and function descriptions&lt;/strong&gt; that you plan to include. Lay them out. Use placeholders for the visuals. Verify that everything fits. White space is very important. Use it to make the sheet scannable and to isolate content groups. Retain white space, even if it means smaller text.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make the visuals.&lt;/strong&gt; They take the longest, so I save them for last or make them as I do step 6.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tweak until happy.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Visual Design&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use the existing theme&lt;/strong&gt; that you see in the cheatsheets. It is cohesive and black and white printer friendly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Choose a highlight color&lt;/strong&gt; to use throughout your cheatsheet, and repeat this highlight color in the background of the top right corner. Ideally you could find a color that is different enough from the other cheatsheets that you can quickly tell yours apart when flipping through a booklet of cheatsheets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use a second color sparingly or not at all&lt;/strong&gt; to draw attention to where it is needed and to differentiate different groupings of content.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Include lots of white space.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visually differentiate groups of content.&lt;/strong&gt; Backgrounds, boxes, side bars, and headers are helpful here. It is very useful for the user to know immediately where one group of content begins and where one ends. Our &#34;gradation headers&#34; fail here, so think of better solutions if possible.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Align things&lt;/strong&gt; to guides, i.e. align things across the page. It helps define the white space and makes the cheat more orderly and professional.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make the text no smaller than ~10pt.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;If the letters are white on a colored background&lt;/strong&gt;, make the font thicker - semibold or bold.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Save bold text&lt;/strong&gt; for simple, important statements, or to draw scanning eyes to important words, such as words that identify the topic discussed. Don&#39;t make an entire paragraph bold text.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Content&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Include a hex sticker, IDE screenshot, or other branding material&lt;/strong&gt;. The cheatsheets have a second function as marketing material.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Include a &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons Copyright&lt;/a&gt;&lt;/strong&gt; to make the sheet easy to share. You&#39;ll find one baked into every cheatsheet and the template.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Be very concise&lt;/strong&gt; - rely on diagrams where possible.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pay attention to the details!&lt;/strong&gt; Your readers sure will... so be correct.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;If in doubt, leave it out.&lt;/strong&gt; There is a documentation manual after all.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code comments inform, but fail&lt;/strong&gt; to draw the readers attention. It is better to use arrows, speech bubbles, etc. for important information. If it is not important information, leave it out.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple working examples are more helpful than documentation details.&lt;/strong&gt; They meet the user at his or her pain points, demonstrating code, and reminding users how to run it, with the least context shifting.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add some concise text to &lt;strong&gt;help the user make sense of your sections and diagrams&lt;/strong&gt;. Images are best, but readers need to be able to interpret them.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Summary&lt;/h3&gt; &#xA;&lt;p&gt;Your cheatsheet has two goals. First, to help users find essential information quickly, and second, to prevent confusion while doing the above. Your best strategy will be to limit the amount of information you put into the cheatsheet and to lay that information out intuitively and visually. This approach will make your cheatsheet equally useful as a teaching tool, programming tool, or marketing tool.&lt;/p&gt; &#xA;&lt;p&gt;Cheatsheets fall squarely on the &lt;em&gt;human-facing side of software design&lt;/em&gt;. They focus on human attention. What does that mean? When you write documentation, your job is to fill in all of the relevant details‚Äîthat&#39;s a software facing job, you need to know the software to do it. You assume that interested humans will find their way to your details on their own (and understand them when they do!). When you make a cheatsheet, your job flips. You assume that the relevant details already exist in the documentation. Your job is to help interested humans find them and understand them. Your job is to guide the human&#39;s attention. Don&#39;t just write, design.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>terryum/awesome-deep-learning-papers</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/terryum/awesome-deep-learning-papers</id>
    <link href="https://github.com/terryum/awesome-deep-learning-papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The most cited deep learning papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt; &#xA;&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt; &#xA;&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href=&#34;https://github.com/kjw0612/awesome-deep-vision&#34;&gt;Deep Vision&lt;/a&gt; and &lt;a href=&#34;https://github.com/kjw0612/awesome-rnn&#34;&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href=&#34;https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap&#34;&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt; &#xA;&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt; &#xA;&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href=&#34;https://twitter.com/TerryUm_ML&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;https://www.facebook.com/terryum.io/&#34;&gt;facebook page&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Awesome list criteria&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt; &#xA; &lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016&#34; section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt; &#xA; &lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt; &#xA; &lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; : +60 citations or &#34;More Papers from 2016&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; : +200 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; : +400 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; : +600 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; : +800 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request. (Please read the &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/Contributing.md&#34;&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt; &#xA;&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/fetch_papers.py&#34;&gt;this&lt;/a&gt; and collect all authors&#39; names with &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/get_authors.py&#34;&gt;this&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/top100papers.bib&#34;&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href=&#34;https://github.com/sunshinemyson&#34;&gt;Sven&lt;/a&gt; and &lt;a href=&#34;https://github.com/grepinsight&#34;&gt;grepinsight&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#understanding--generalization--transfer&#34;&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#optimization--training-techniques&#34;&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#unsupervised--generative-models&#34;&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#convolutional-neural-network-models&#34;&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#image-segmentation--object-detection&#34;&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#image--video--etc&#34;&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#natural-language-processing--rnns&#34;&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#speech--other-domain&#34;&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#reinforcement-learning--robotics&#34;&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#more-papers-from-2016&#34;&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#new-papers&#34;&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#old-papers&#34;&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#hw--sw--dataset&#34;&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#book--survey--review&#34;&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#video-lectures--tutorials--blogs&#34;&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#appendix-more-than-top-100&#34;&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Understanding / Generalization / Transfer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href=&#34;http://arxiv.org/pdf/1503.02531&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href=&#34;http://arxiv.org/pdf/1412.1897&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href=&#34;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href=&#34;http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href=&#34;http://arxiv.org/pdf/1311.2901&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href=&#34;http://arxiv.org/pdf/1310.1531&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Jason Yosinski](https://scholar.google.ca/citations?hl=en&amp;user=gxL1qj8AAAAJ) --&gt; &#xA;&lt;h3&gt;Optimization / Training Techniques&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href=&#34;http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href=&#34;http://arxiv.org/pdf/1502.03167&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href=&#34;http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href=&#34;http://arxiv.org/pdf/1412.6980&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href=&#34;http://arxiv.org/pdf/1207.0580.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href=&#34;http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers] [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Christian Szegedy](https://scholar.google.ca/citations?hl=en&amp;user=3QeF7mAAAAAJ), [Sergey Ioffe](https://scholar.google.ca/citations?user=S5zOyIkAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&amp;user=DhtAFkwAAAAJ), [Diederik P. Kingma](https://scholar.google.ca/citations?hl=en&amp;user=yyIoQu4AAAAJ)--&gt; &#xA;&lt;h3&gt;Unsupervised / Generative Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href=&#34;http://arxiv.org/pdf/1601.06759v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href=&#34;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href=&#34;https://arxiv.org/pdf/1511.06434v2&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href=&#34;http://arxiv.org/pdf/1502.04623&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href=&#34;http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href=&#34;http://arxiv.org/pdf/1312.6114&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href=&#34;http://arxiv.org/pdf/1112.6209&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers] [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Ian Goodfellow](https://scholar.google.ca/citations?user=iYN86KEAAAAJ), [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)--&gt; &#xA;&lt;h3&gt;Convolutional Neural Network Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.07261&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.05027v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href=&#34;http://arxiv.org/pdf/1512.03385&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href=&#34;http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href=&#34;http://arxiv.org/pdf/1409.1556&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href=&#34;http://arxiv.org/pdf/1405.3531&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href=&#34;http://arxiv.org/pdf/1312.6229&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href=&#34;http://arxiv.org/pdf/1302.4389v4&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href=&#34;http://arxiv.org/pdf/1312.4400&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href=&#34;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Christian Szegedy](https://scholar.google.ca/citations?hl=en&amp;user=3QeF7mAAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&amp;user=DhtAFkwAAAAJ), [Shaoqing Ren](https://scholar.google.ca/citations?hl=en&amp;user=AUhj438AAAAJ), [Jian Sun](https://scholar.google.ca/citations?hl=en&amp;user=ALVSZAYAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Yann LeCun](https://scholar.google.ca/citations?hl=en&amp;user=WLN3QrAAAAAJ)--&gt; &#xA;&lt;h3&gt;Image: Segmentation / Object Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href=&#34;http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href=&#34;http://arxiv.org/pdf/1406.4729&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href=&#34;https://arxiv.org/pdf/1412.7062&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href=&#34;https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Ross Girshick](https://scholar.google.ca/citations?hl=en&amp;user=W8VIEZgAAAAJ), [Jeff Donahue](https://scholar.google.ca/citations?hl=en&amp;user=UfbuDH8AAAAJ), [Trevor Darrell](https://scholar.google.ca/citations?hl=en&amp;user=bh-uRFMAAAAJ)--&gt; &#xA;&lt;h3&gt;Image / Video / Etc&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href=&#34;https://arxiv.org/pdf/1501.00092v3.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.06576&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href=&#34;http://arxiv.org/pdf/1502.03044&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;:&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href=&#34;http://vision.stanford.edu/pdf/karpathy14.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href=&#34;http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Andrej Karpathy](https://scholar.google.ca/citations?user=l8WuQJgAAAAJ)--&gt; &#xA;&lt;!--[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)--&gt; &#xA;&lt;h3&gt;Natural Language Processing / RNNs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href=&#34;http://aclweb.org/anthology/N/N16/N16-1030.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.02410&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href=&#34;http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.04025&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href=&#34;https://arxiv.org/pdf/1410.3916&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href=&#34;https://arxiv.org/pdf/1410.5401&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href=&#34;http://arxiv.org/pdf/1409.0473&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href=&#34;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href=&#34;http://arxiv.org/pdf/1406.1078&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href=&#34;http://arxiv.org/pdf/1404.2188v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href=&#34;http://arxiv.org/pdf/1408.5882&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href=&#34;http://anthology.aclweb.org/D/D14/D14-1162.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href=&#34;http://arxiv.org/pdf/1405.4053&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href=&#34;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href=&#34;http://arxiv.org/pdf/1301.3781&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href=&#34;https://arxiv.org/pdf/1308.0850&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Kyunghyun Cho](https://scholar.google.ca/citations?user=0RAmmIAAAAAJ), [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Richard Socher](https://scholar.google.ca/citations?hl=en&amp;user=FaOcyfMAAAAJ), [Tomas Mikolov](https://scholar.google.ca/citations?user=oBu8kMMAAAAJ), [Christopher D. Manning](https://scholar.google.ca/citations?user=1zmDOdwAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ)--&gt; &#xA;&lt;h3&gt;Speech / Other Domain&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.04395&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href=&#34;https://arxiv.org/pdf/1512.02595&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href=&#34;http://arxiv.org/pdf/1303.5778.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href=&#34;http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href=&#34;http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Dong Yu](https://scholar.google.ca/citations?hl=en&amp;user=tMY31_gAAAAJ)--&gt; &#xA;&lt;h3&gt;Reinforcement Learning / Robotics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.02199&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href=&#34;https://arxiv.org/pdf/1509.06461.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href=&#34;http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href=&#34;https://arxiv.org/pdf/1509.02971&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href=&#34;http://www.davidqiu.com:8888/research/nature14236.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href=&#34;http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href=&#34;http://arxiv.org/pdf/1312.5602.pdf&#34;&gt;[pdf]&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Sergey Levine](https://scholar.google.ca/citations?user=8R35rCwAAAAJ), [Volodymyr Mnih](https://scholar.google.ca/citations?hl=en&amp;user=rLdfJ1gAAAAJ), [David Silver](https://scholar.google.ca/citations?user=-8DNE4UAAAAJ)--&gt; &#xA;&lt;h3&gt;More Papers from 2016&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href=&#34;https://arxiv.org/pdf/1607.06450v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href=&#34;http://arxiv.org/pdf/1606.04474v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href=&#34;https://arxiv.org/pdf/1609.03499v2&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.08511&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href=&#34;https://arxiv.org/pdf/1609.03552&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href=&#34;https://arxiv.org/pdf/1512.02325&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.07360&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.01528&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href=&#34;https://arxiv.org/pdf/1602.02830&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href=&#34;https://www.gwern.net/docs/2016-graves.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Google&#39;s neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href=&#34;https://arxiv.org/pdf/1609.08144&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;New papers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href=&#34;https://arxiv.org/pdf/1704.04861.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href=&#34;https://arxiv.org/pdf/1705.03122&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href=&#34;https://arxiv.org/pdf/1702.01932&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href=&#34;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.07511v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.03864v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.06211v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href=&#34;https://128.84.21.199/pdf/1703.06870&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.05192v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href=&#34;http://arxiv.org/pdf/1702.07825v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href=&#34;http://arxiv.org/pdf/1702.06506v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href=&#34;https://arxiv.org/abs/1702.03275&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href=&#34;https://arxiv.org/pdf/1701.07875v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href=&#34;https://arxiv.org/pdf/1611.03530&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href=&#34;https://arxiv.org/abs/1611.04076v2&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Old Papers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href=&#34;http://arxiv.org/pdf/1103.0398&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href=&#34;http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href=&#34;http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href=&#34;http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href=&#34;http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href=&#34;http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href=&#34;http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href=&#34;http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;HW / SW / Dataset&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.05250.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.01540&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href=&#34;http://arxiv.org/pdf/1603.04467&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt; &#xA; &lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href=&#34;https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href=&#34;http://arxiv.org/pdf/1412.4564&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href=&#34;http://arxiv.org/pdf/1409.0575&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href=&#34;http://arxiv.org/pdf/1408.5093&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Book / Survey / Review&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href=&#34;https://arxiv.org/pdf/1702.07800&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href=&#34;http://arxiv.org/pdf/1701.07274v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href=&#34;http://arxiv.org/pdf/1703.01619v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/index.html&#34;&gt;[html]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;[html]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href=&#34;https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href=&#34;https://arxiv.org/pdf/1606.05908&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href=&#34;http://arxiv.org/pdf/1404.7828&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href=&#34;http://arxiv.org/pdf/1206.5538&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href=&#34;http://cs231n.stanford.edu/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href=&#34;http://cs224d.stanford.edu/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href=&#34;https://github.com/oxford-cs-deepnlp-2017/lectures&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href=&#34;https://nips.cc/Conferences/2016/Schedule?type=Tutorial&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href=&#34;http://techtalks.tv/icml/2016/tutorials/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href=&#34;http://videolectures.net/iclr2016_san_juan/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href=&#34;http://videolectures.net/deeplearning2016_montreal/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href=&#34;https://www.bayareadlschool.org/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI &lt;a href=&#34;https://www.openai.com/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Distill &lt;a href=&#34;http://distill.pub/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Andrej Karpathy Blog &lt;a href=&#34;http://karpathy.github.io/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Colah&#39;s Blog &lt;a href=&#34;http://colah.github.io/&#34;&gt;[Web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WildML &lt;a href=&#34;http://www.wildml.com/&#34;&gt;[Web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastML &lt;a href=&#34;http://www.fastml.com/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TheMorningPaper &lt;a href=&#34;https://blog.acolyer.org&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Appendix: More than Top 100&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.06147&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href=&#34;http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html&#34;&gt;[html]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href=&#34;https://arxiv.org/pdf/1503.00949&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href=&#34;https://arxiv.org/pdf/1505.03540&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href=&#34;https://arxiv.org/pdf/1610.09038&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href=&#34;https://ishmaelbelghazi.github.io/ALI/&#34;&gt;[web]&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.00704v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href=&#34;https://arxiv.org/pdf/1605.09081v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href=&#34;https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href=&#34;http://arxiv.org/pdf/1603.08983&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href=&#34;https://arxiv.org/pdf/1608.06993v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt; &#xA; &lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/gu16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.02858&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href=&#34;https://arxiv.org/pdf/1604.00788&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.01781&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href=&#34;https://arxiv.org/pdf/1607.01759&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href=&#34;https://arxiv.org/pdf/1601.01705&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.08155&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href=&#34;http://arxiv.org/pdf/1412.1842&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href=&#34;https://arxiv.org/pdf/1502.05082&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href=&#34;http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href=&#34;https://arxiv.org/pdf/1603.09382&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href=&#34;http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href=&#34;http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mind&#39;s eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href=&#34;http://arxiv.org/pdf/1502.05698&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href=&#34;http://arxiv.org/pdf/1506.07285&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href=&#34;https://arxiv.org/pdf/1510.00149&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href=&#34;https://arxiv.org/pdf/1503.00075&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.06615&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href=&#34;http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href=&#34;https://arxiv.org/pdf/1505.04366v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href=&#34;https://arxiv.org/pdf/1506.06579&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep generative image models using aÔøº laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href=&#34;http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/chung15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href=&#34;https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href=&#34;http://papers.nips.cc/paper/5866-pointer-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href=&#34;https://arxiv.org/pdf/1506.02078&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href=&#34;http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href=&#34;http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href=&#34;https://arxiv.org/pdf/1506.05869.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&#34;&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href=&#34;http://aclweb.org/anthology/P/P15/P15-1033.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href=&#34;http://aclweb.org/anthology/D/D15/D15-1041.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href=&#34;http://aclweb.org/anthology/D/D15/D15-1176.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href=&#34;https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href=&#34;http://arxiv.org/pdf/1406.6247.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href=&#34;https://arxiv.org/pdf/1412.3555&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href=&#34;https://arxiv.org/pdf/1410.8206&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href=&#34;http://arxiv.org/pdf/1409.2329&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href=&#34;https://arxiv.org/pdf/1312.6199.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v32/graves14.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href=&#34;https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href=&#34;http://www.aclweb.org/anthology/N13-1#page=784&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href=&#34;http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href=&#34;http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/Contributing.md&#34;&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;&lt;img src=&#34;http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg?sanitize=true&#34; alt=&#34;CC0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To the extent possible under law, &lt;a href=&#34;https://www.facebook.com/terryum.io/&#34;&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jeffrey-xiao/papers</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/jeffrey-xiao/papers</id>
    <link href="https://github.com/jeffrey-xiao/papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;papers&lt;/h1&gt; &#xA;&lt;p&gt;A collection of academic papers, articles, and other resources that I plan to read or have read. The content has a focus on distributed systems.&lt;/p&gt; &#xA;&lt;p&gt;I have also included my notes on select resources to summarize important takeaways and to help me better understand the material.&lt;/p&gt; &#xA;&lt;h2&gt;Blog Posts and Online Articles&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jepsen.io/consistency&#34;&gt;Consistency Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jepsen.io/analyses&#34;&gt;Jepsen Analyses&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aphyr.com/posts/313-strong-consistency-models&#34;&gt;Strong Consistency Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://christophermeiklejohn.com/distributed/systems/2013/07/12/readings-in-distributed-systems.html&#34;&gt;Readings in Distributed Systems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&#34;&gt;The Log: What every software engineering should know about real-time data&#39;s unifying abstraction&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Consensus&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/unreliable-failure-detectors-for-reliable-distributed-systems.pdf&#34;&gt;Unreliable Failure Detectors for Reliable Distributed Systems&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Chandra, Tushar Deepak, and Sam Toueg. 1996. ‚ÄúUnreliable Failure Detectors for Reliable Distributed Systems.‚Äù &lt;em&gt;Journal of the ACM (JACM)&lt;/em&gt; 43 (2). ACM: 225‚Äì67.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/impossibility-of-distributed-consensus-with-one-faulty-process.pdf&#34;&gt;Impossibility of Distributed Consensus with One Faulty Process&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Fischer, Michael J, Nancy A Lynch, and Michael S Paterson. 1982. ‚ÄúImpossibility of Distributed Consensus with One Faulty Process.‚Äù MASSACHUSETTS INST OF TECH CAMBRIDGE LAB FOR COMPUTER SCIENCE.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/viewstamped-replication-a-new-primary-copy-method-to-support-highly-available-distributed-systems.pdf&#34;&gt;Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Oki, Brian M, and Barbara H Liskov. 1988. ‚ÄúViewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems.‚Äù In &lt;em&gt;Proceedings of the Seventh Annual Acm Symposium on Principles of Distributed Computing&lt;/em&gt;, 8‚Äì17. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/viewstamped-replication-revisited.pdf&#34;&gt;Viewstamped Replication Revisited&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Liskov, Barbara, and James Cowling. 2012. ‚ÄúViewstamped Replication Revisited.‚Äù&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/the-part-time-parliament.pdf&#34;&gt;The Part Time Parliament&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Lamport, Leslie, and others. 1998. ‚ÄúThe Part-Time Parliament.‚Äù &lt;em&gt;ACM Transactions on Computer Systems&lt;/em&gt; 16 (2): 133‚Äì69.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/paxos-made-simple.pdf&#34;&gt;Paxos Made Simple&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Lamport, Leslie, and others. 2001. ‚ÄúPaxos Made Simple.‚Äù &lt;em&gt;ACM Sigact News&lt;/em&gt; 32 (4): 18‚Äì25.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/paxos-made-live-an-engineering-perspective.pdf&#34;&gt;Paxos Made Live - An Engineering Perspective&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Chandra, Tushar D, Robert Griesemer, and Joshua Redstone. 2007. ‚ÄúPaxos Made Live: An Engineering Perspective.‚Äù In &lt;em&gt;Proceedings of the Twenty-Sixth Annual Acm Symposium on Principles of Distributed Computing&lt;/em&gt;, 398‚Äì407. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/flexible-paxos-quorum-intersection-revisited.pdf&#34;&gt;Flexible Paxos: Quorum Intersection Revisited&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Howard, Heidi, Dahlia Malkhi, and Alexander Spiegelman. 2016. ‚ÄúFlexible Paxos: Quorum Intersection Revisited.‚Äù &lt;em&gt;arXiv Preprint arXiv:1608.06696&lt;/em&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/zab-high-performance-broadcast-for-primary-backup-systems.pdf&#34;&gt;Zab: High-Performance Broadcast for Primary-Backup Systems&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Junqueira, Flavio P, Benjamin C Reed, and Marco Serafini. 2011. ‚ÄúZab: High-Performance Broadcast for Primary-Backup Systems.‚Äù In &lt;em&gt;2011 Ieee/Ifip 41st International Conference on Dependable Systems &amp;amp; Networks (Dsn)&lt;/em&gt;, 245‚Äì56. IEEE.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/zookeepers-atomic-broadcast-protocol-theory-and-practice.pdf&#34;&gt;ZooKeeper&#39;s Atomic Broadcast Protocol: Theory and Practice&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Medeiros, Andr√©. 2012. ‚ÄúZooKeeper‚Äôs Atomic Broadcast Protocol: Theory and Practice.‚Äù &lt;em&gt;Aalto University School of Science&lt;/em&gt; 20.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/viva-la-difference-paxos-vs-viewstamped-replication-vs-zab&#34;&gt;Vive la Diff√©rence: Paxos vs. Viewstamped Replication vs. Zab&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Van Renesse, Robbert, Nicolas Schiper, and Fred B Schneider. 2015. ‚ÄúVive La Diff√©rence: Paxos Vs. Viewstamped Replication Vs. Zab.‚Äù &lt;em&gt;IEEE Transactions on Dependable and Secure Computing&lt;/em&gt; 12 (4). IEEE: 472‚Äì84.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/in-search-of-an-understandable-consensus-algorithm.pdf&#34;&gt;In Search of an Understandable Consensus Algorithm&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Ongaro, Diego, and John Ousterhout. 2014. ‚ÄúIn Search of an Understandable Consensus Algorithm.‚Äù In &lt;em&gt;2014 {Usenix} Annual Technical Conference ({Usenix}{ATC} 14)&lt;/em&gt;, 305‚Äì19.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/consensus-bridging-theory-and-practice.pdf&#34;&gt;Consensus: Bridging Theory and Practice&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Ongaro, Diego. 2014. ‚ÄúConsensus: Bridging Theory and Practice.‚Äù PhD thesis, Stanford University.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/consensus/raft-refloated-do-we-have-consensus.pdf&#34;&gt;Raft Refloated: Do We Have Consensus?&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Howard, Heidi, Malte Schwarzkopf, Anil Madhavapeddy, and Jon Crowcroft. 2015. ‚ÄúRaft Refloated: Do We Have Consensus?‚Äù &lt;em&gt;Operating Systems Review&lt;/em&gt; 49 (1): 12‚Äì21.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Causality&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/causality/time-clocks-and-the-ordering-of-events-in-a-distributed-system.pdf&#34;&gt;Time, Clocks, and the Ordering of Events in a Distributed System&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Lamport, Leslie. 1978. ‚ÄúTime, Clocks, and the Ordering of Events in a Distributed System.‚Äù &lt;em&gt;Communications of the ACM&lt;/em&gt; 21 (7). ACM: 558‚Äì65.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/causality/timestamps-in-message-passing-systems-that-preserve-the-partial-ordering.pdf&#34;&gt;Timestamps in Message-Passing Systems That Preserve the Partial Ordering&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Fidge, Colin J. 1987. &lt;em&gt;Timestamps in Message-Passing Systems That Preserve the Partial Ordering&lt;/em&gt;. Australian National University. Department of Computer Science.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/causality/virtual-time-and-global-states-of-distributed-systems.pdf&#34;&gt;Virtual Time and Global States of Distributed Systems&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Mattern, Friedemann, and others. 1988. &lt;em&gt;Virtual Time and Global States of Distributed Systems&lt;/em&gt;. Citeseer.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/causality/logical-physical-clocks-and-consistent-snapshots-in-globally-distributed-databases.pdf&#34;&gt;Logical Physical Clocks and Consistent Snapshots in Globally Distributed Databases&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Demirbas, Murat, Marcelo Leone, Bharadwaj Avva, Deepak Madeppa, and Sandeep Kulkarni. 2014. ‚ÄúLogical Physical Clocks and Consistent Snapshots in Globally Distributed Databases.‚Äù&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/causality/the-bloom-clock.pdf&#34;&gt;The Bloom Clock&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Ramabaja, Lum. 2019. ‚ÄúThe Bloom Clock.‚Äù &lt;em&gt;CoRR&lt;/em&gt; abs/1905.13064. &lt;a href=&#34;http://arxiv.org/abs/1905.13064&#34;&gt;http://arxiv.org/abs/1905.13064&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Consistency&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/brewers-conjecture-and-the-feasibility-of-consistent-available-partition-tolerant-web-services.pdf&#34;&gt;Brewer&#39;s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Gilbert, Seth, and Nancy Lynch. 2002. ‚ÄúBrewer‚Äôs Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services.‚Äù &lt;em&gt;Acm Sigact News&lt;/em&gt; 33 (2). ACM: 51‚Äì59.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Structures&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/fast-set-operations-using-treaps.pdf&#34;&gt;Fast set operations using treaps&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Blelloch, Guy E, and Margaret Reid-Miller. 1998. ‚ÄúFast Set Operations Using Treaps.‚Äù In &lt;em&gt;SPAA&lt;/em&gt;, 98:16‚Äì26.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/a-skip-list-cookbook.pdf&#34;&gt;A Skip List Cookbook&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Pugh, William. 1998. ‚ÄúA Skip List Cookbook.‚Äù&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/skip-lists-a-probabilistic-alternative-to-balanced-trees.pdf&#34;&gt;Skip Lists: A Probabilistic Alternative to Balanced Trees&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Pugh, William. 1990. ‚ÄúSkip Lists: A Probabilistic Alternative to Balanced Trees.‚Äù &lt;em&gt;Communications of the ACM&lt;/em&gt; 33 (6).&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/less-hashing-same-performance-building-a-better-bloom-filter.pdf&#34;&gt;Less hashing, same performance: Building a better Bloom filter&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Kirsch, Adam, and Michael Mitzenmacher. 2006. ‚ÄúLess Hashing, Same Performance: Building a Better Bloom Filter.‚Äù In &lt;em&gt;European Symposium on Algorithms&lt;/em&gt;, 456‚Äì67. Springer.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/advanced-bloom-filter-based-algorithms-for-efficient-approximate-data-de-duplication-in-streams.pdf&#34;&gt;Advanced bloom filter based algorithms for efficient approximate data de-duplication in streams&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Bera, Suman K, Sourav Dutta, Ankur Narang, and Souvik Bhattacherjee. 2012. ‚ÄúAdvanced Bloom Filter Based Algorithms for Efficient Approximate Data de-Duplication in Streams.‚Äù &lt;em&gt;arXiv Preprint arXiv:1212.3964&lt;/em&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/cuckoo-filter-practically-better-than-bloom.pdf&#34;&gt;Cuckoo filter: Practically better than bloom&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Fan, Bin, Dave G Andersen, Michael Kaminsky, and Michael D Mitzenmacher. 2014. ‚ÄúCuckoo Filter: Practically Better Than Bloom.‚Äù In &lt;em&gt;Proceedings of the 10th Acm International on Conference on Emerging Networking Experiments and Technologies&lt;/em&gt;, 75‚Äì88. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/dont-thrash-how-to-cache-your-hash-on-flash.pdf&#34;&gt;Don&#39;t thrash: how to cache your hash on flash&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Bender, Michael A, Martin Farach-Colton, Rob Johnson, Russell Kraner, Bradley C Kuszmaul, Dzejla Medjedovic, Pablo Montes, Pradeep Shetty, Richard P Spillane, and Erez Zadok. 2012. ‚ÄúDon‚Äôt Thrash: How to Cache Your Hash on Flash.‚Äù &lt;em&gt;Proceedings of the VLDB Endowment&lt;/em&gt; 5 (11). VLDB Endowment: 1627‚Äì37.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/an-improved-data-stream-summary-the-count-min-sketch-and-its-applications.pdf&#34;&gt;An improved data stream summary: the count-min sketch and its applications&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Cormode, Graham, and Shan Muthukrishnan. 2005. ‚ÄúAn Improved Data Stream Summary: The Count-Min Sketch and Its Applications.‚Äù &lt;em&gt;Journal of Algorithms&lt;/em&gt; 55 (1). Elsevier: 58‚Äì75.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/a-general-purpose-counting-filter-making-every-bit-count.pdf&#34;&gt;A general-purpose counting filter: Making every bit count&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Pandey, Prashant, Michael A Bender, Rob Johnson, and Rob Patro. 2017. ‚ÄúA General-Purpose Counting Filter: Making Every Bit Count.‚Äù In &lt;em&gt;Proceedings of the 2017 Acm International Conference on Management of Data&lt;/em&gt;, 775‚Äì87. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/hyperloglog-the-analysis-of-a-near-optimal-cardinality-estimation-algorithm.pdf&#34;&gt;HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Flajolet, Philippe, √âric Fusy, Olivier Gandouet, and Fr√©d√©ric Meunier. 2007. ‚ÄúHyperloglog: The Analysis of a Near-Optimal Cardinality Estimation Algorithm.‚Äù In &lt;em&gt;Discrete Mathematics and Theoretical Computer Science&lt;/em&gt;, 137‚Äì56. Discrete Mathematics; Theoretical Computer Science.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/hyperloglog-in-practice-algorithmic-engineering-of-a-state-of-the-art-cardinality-estimation-algorithm.pdf&#34;&gt;HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Heule, Stefan, Marc Nunkesser, and Alexander Hall. 2013. ‚ÄúHyperLogLog in Practice: Algorithmic Engineering of a State of the Art Cardinality Estimation Algorithm.‚Äù In &lt;em&gt;Proceedings of the 16th International Conference on Extending Database Technology&lt;/em&gt;, 683‚Äì92. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/lsm-tree.pdf&#34;&gt;LSM Tree&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;O‚ÄôNeil, Patrick, Edward Cheng, Dieter Gawlick, and Elizabeth O‚ÄôNeil. 1996. ‚ÄúThe Log-Structured Merge-Tree (Lsm-Tree).‚Äù &lt;em&gt;Acta Informatica&lt;/em&gt; 33 (4). Springer: 351‚Äì85.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/blsm-a-general-purpose-lsm-tree.pdf&#34;&gt;bLSM: A General Purpose LSM Tree&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Sears, Russell, and Raghu Ramakrishnan. 2012. ‚ÄúBLSM: A General Purpose Log Structured Merge Tree.‚Äù In &lt;em&gt;Proceedings of the 2012 Acm Sigmod International Conference on Management of Data&lt;/em&gt;, 217‚Äì28. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/a-comprehensive-study-of-convergent-and-commutative-replicated-data-types.pdf&#34;&gt;A Comprehensive Study of Convergent and Commutative Replicated Data Types&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Shapiro, Marc, Nuno Pregui√ßa, Carlos Baquero, and Marek Zawirski. 2011. ‚ÄúA Comprehensive Study of Convergent and Commutative Replicated Data Types.‚Äù PhD thesis, Inria‚ÄìCentre Paris-Rocquencourt; INRIA.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/data-structures/efficient-synchronization-of-state-based-crdts.pdf&#34;&gt;Efficient Synchronization of State-based CRDTs&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Enes, Vitor, Paulo S√©rgio Almeida, Carlos Baquero, and Jo√£o Leit√£o. 2018. ‚ÄúEfficient Synchronization of State-Based Crdts.‚Äù &lt;em&gt;arXiv Preprint arXiv:1803.02750&lt;/em&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;P2P&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/p2p/chord-a-scalable-peer-to-peer-lookup-service-for-internet-applications.pdf&#34;&gt;Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Stoica, Ion, Robert Morris, David Liben-Nowell, David R Karger, M Frans Kaashoek, Frank Dabek, and Hari Balakrishnan. 2003. ‚ÄúChord: A Scalable Peer-to-Peer Lookup Protocol for Internet Applications.‚Äù &lt;em&gt;IEEE/ACM Transactions on Networking (TON)&lt;/em&gt; 11 (1). IEEE Press: 17‚Äì32.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/p2p/kademlia-a-peer-to-peer-information-system-based-on-the-xor-metric.pdf&#34;&gt;Kademlia: A Peer-to-peer Information System Based on the XOR Metric&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Maymounkov, Petar, and David Mazieres. 2002. ‚ÄúKademlia: A Peer-to-Peer Information System Based on the Xor Metric.‚Äù In &lt;em&gt;International Workshop on Peer-to-Peer Systems&lt;/em&gt;, 53‚Äì65. Springer.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Systems&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/systems/dynamo-amazons-highly-available-key-value-store.pdf&#34;&gt;Dynamo: Amazon&#39;s Highly Available Key-value Store&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;DeCandia, Giuseppe, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, Alex Pilchin, Swaminathan Sivasubramanian, Peter Vosshall, and Werner Vogels. 2007. ‚ÄúDynamo: Amazon‚Äôs Highly Available Key-Value Store.‚Äù In &lt;em&gt;ACM Sigops Operating Systems Review&lt;/em&gt;, 41:205‚Äì20. 6. ACM.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/systems/bigtable-a-distributed-storage-system-for-structured-data.pdf&#34;&gt;Bigtable: A Distributed Storage System for Structured Data&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Chang, Fay, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E Gruber. 2008. ‚ÄúBigtable: A Distributed Storage System for Structured Data.‚Äù &lt;em&gt;ACM Transactions on Computer Systems (TOCS)&lt;/em&gt; 26 (2). ACM: 4.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/systems/cassandra-a-decentralized-structured-storage-system.pdf&#34;&gt;Cassandra - A Decentralized Structured Storage System&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Lakshman, Avinash, and Prashant Malik. 2010. ‚ÄúCassandra: A Decentralized Structured Storage System.‚Äù &lt;em&gt;ACM SIGOPS Operating Systems Review&lt;/em&gt; 44 (2). ACM: 35‚Äì40.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/systems/kafka-a-distributed-messaging-system-for-log-processing.pdf&#34;&gt;Kafka: A Distributed Messaging System for Log Processing&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Kreps, Jay, Neha Narkhede, Jun Rao, and others. 2011. ‚ÄúKafka: A Distributed Messaging System for Log Processing.‚Äù In &lt;em&gt;Proceedings of the Netdb&lt;/em&gt;, 1‚Äì7.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/systems/spanner-googles-globally-distributed-database.pdf&#34;&gt;Spanner: Google&#39;s Globally-Distributed Database&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Corbett, James C., Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, et al. 2012. ‚ÄúSpanner: Google‚Äôs Globally-Distributed Database.‚Äù In &lt;em&gt;OSDI&lt;/em&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/systems/spanner-becoming-a-sql-system.pdf&#34;&gt;Spanner: Becoming a SQL System&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Bacon, David F., Nathan Bales, Nico Bruno, Brian F. Cooper, Adam Dickinson, Andrew Fikes, Campbell Fraser, et al. 2017. ‚ÄúSpanner: Becoming a Sql System.‚Äù In &lt;em&gt;Proc. SIGMOD 2017&lt;/em&gt;, 331‚Äì43.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/testing/an-analysis-of-network-partitioning-failures-in-cloud-systems.pdf&#34;&gt;An Analysis of Network-Partitioning Failures in Cloud Systems&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Alquraan, Ahmed, Hatem Takruri, Mohammed Alfatafta, and Samer Al-Kiswany. 2018. ‚ÄúAn Analysis of Network-Partitioning Failures in Cloud Systems.‚Äù In &lt;em&gt;13th Usenix Symposium on Operating Systems Design and Implementation Osdi 18)&lt;/em&gt;, 51‚Äì68.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/testing/why-is-random-testing-effective-for-partition-tolerance-bugs.pdf&#34;&gt;Why Is Random Testing Effective for Partition Tolerance Bugs?&lt;/a&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Majumdar, Rupak, and Filip Niksic. 2017. ‚ÄúWhy Is Random Testing Effective for Partition Tolerance Bugs?‚Äù &lt;em&gt;Proceedings of the ACM on Programming Languages&lt;/em&gt; 2 (POPL). ACM: 46.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Textbooks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Operating Systems: Principles and Practice &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/operating-systems-principles-and-practice-vol-1-kernels-and-processes.pdf&#34;&gt;Volume 1: Kernels and Processes&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/operating-systems-principles-and-practice-vol-2-concurrency.pdf&#34;&gt;Volume 2: Concurrency&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/operating-systems-principles-and-practice-vol-3-memory-management.pdf&#34;&gt;Volume 3: Memory Management&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/operating-systems-principles-and-practice-vol-4-persistent-storage.pdf&#34;&gt;Volume 4: Persistent Storage&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/designing-data-intensive-applications.pdf&#34;&gt;Designing Data-Intensive Applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/distributed-systems.pdf&#34;&gt;Distributed Systems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://book.mixu.net/distsys&#34;&gt;Distributed Systems for Fun and Profit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer&#34;&gt;Distributed Systems Theory for the Distributed Systems Engineer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffrey-xiao/papers/master/textbooks/readings-in-database-systems.pdf&#34;&gt;Readings in Database Systems&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ustctug/ustcthesis</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/ustctug/ustcthesis</id>
    <link href="https://github.com/ustctug/ustcthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX template for USTC thesis&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶Â≠¶‰ΩçËÆ∫Êñá LaTeX Ê®°Êùø&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ustctug/ustcthesis/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/ustctug/ustcthesis/all.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ustctug/ustcthesis/commits/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commits-since/ustctug/ustcthesis/latest.svg?sanitize=true&#34; alt=&#34;GitHub commits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ustctug/ustcthesis/actions&#34;&gt;&lt;img src=&#34;https://github.com/ustctug/ustcthesis/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆÊòØ‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶ÁöÑÂ≠¶‰ΩçËÆ∫Êñá LaTeX Ê®°Êùø ustcthesisÔºåÊåâÁÖßÊúÄÊñ∞ÁâàÁöÑ „Ää&lt;a href=&#34;https://gradschool.ustc.edu.cn/static/upload/article/picture/ce3b02e5f0274c90b9331ef50ae1ac26.pdf&#34;&gt;Á†îÁ©∂ÁîüÂ≠¶‰ΩçËÆ∫ÊñáÊí∞ÂÜôÊâãÂÜå&lt;/a&gt;„Äã Âíå „Ää&lt;a href=&#34;https://www.teach.ustc.edu.cn/?attachment_id=13867&#34;&gt;‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶Êú¨ÁßëÊØï‰∏öËÆ∫ÊñáÔºàËÆæËÆ°ÔºâÊ†ºÂºè&lt;/a&gt;„Äã ÁöÑË¶ÅÊ±ÇÁºñÂÜôÔºåÂÖºÂÆπÊúÄÊñ∞ÁâàÁöÑ TeX Live„ÄÅMacTeX „ÄÅMiKTeX ÂèëË°åÁâàÔºåÊîØÊåÅË∑®Âπ≥Âè∞‰ΩøÁî®„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Ê≥®ÊÑèÔºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;‰ΩøÁî®ËØ¥ÊòéÊñáÊ°£ &lt;code&gt;ustcthesis-doc.pdf&lt;/code&gt; Âú®ÂèëÂ∏ÉÁâà‰∏≠ÈôÑÂ∏¶ÔºåÁî®Êà∑‰πüÂèØËá™Ë°åÁºñËØëÔºõ&lt;strong&gt;‰ΩøÁî®Ê®°ÊùøÂâçÂ∫î‰ªîÁªÜÈòÖËØª&lt;/strong&gt;„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Êú¨Ê®°ÊùøË¶ÅÊ±Ç TeX Live„ÄÅMacTeX„ÄÅMiKTeX ‰∏ç‰Ωé‰∫é 2017 Âπ¥ÁöÑÂèëË°åÁâàÔºå Âπ∂‰∏îÂ∞ΩÂèØËÉΩÂçáÁ∫ßÂà∞ÊúÄÊñ∞„ÄÇÂÆâË£ÖÂíåÂçáÁ∫ßÊñπÊ≥ïËßÅ &lt;a href=&#34;https://github.com/ustctug/ustcthesis/wiki/%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97&#34;&gt;Êñ∞ÊâãÊåáÂçó&lt;/a&gt;„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;‰∏çÊîØÊåÅ&lt;/strong&gt; &lt;a href=&#34;https://github.com/ustctug/ustcthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#3-%E6%A8%A1%E6%9D%BF%E6%94%AF%E6%8C%81%E7%94%A8-ctex-%E5%A5%97%E8%A3%85%E7%BC%96%E8%AF%91%E5%90%97&#34;&gt;CTeX Â•óË£Ö&lt;/a&gt;„ÄÇ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;GitHub ReleasesÔºö&lt;a href=&#34;https://github.com/ustctug/ustcthesis/releases&#34;&gt;https://github.com/ustctug/ustcthesis/releases&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ê†°ÂÜÖÈïúÂÉèÔºö&lt;a href=&#34;https://git.lug.ustc.edu.cn/ustctug/ustcthesis&#34;&gt;https://git.lug.ustc.edu.cn/ustctug/ustcthesis&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Overleaf Ê®°Êùø &lt;a href=&#34;https://www.overleaf.com/latex/templates/latex-template-for-ustc-thesis/jvtdfkqsrycm&#34;&gt;https://www.overleaf.com/latex/templates/latex-template-for-ustc-thesis/jvtdfkqsrycm&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Á†îÁ©∂ÁîüÈô¢ÁΩëÁ´ôÔºàÁâàÊú¨ËæÉÊóßÔºå‰∏çÊé®ËçêÔºâÔºö&lt;a href=&#34;https://gradschool.ustc.edu.cn/column/65&#34;&gt;https://gradschool.ustc.edu.cn/column/65&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÁºñËØëÊñáÊ°£&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ÁºñËØëÊ®°ÊùøÁöÑ‰ΩøÁî®ËØ¥ÊòéÊñáÊ°£ &lt;code&gt;ustcthesis-doc.pdf&lt;/code&gt;Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code&gt;latexmk -xelatex ustcthesis-doc.tex&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ÁºñËØëËÆ∫Êñá &lt;code&gt;main.pdf&lt;/code&gt;Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code&gt;latexmk -xelatex main.tex&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Â¶ÇÈúÄÊ∏ÖÁêÜËÆ∫ÊñáÁºñËØëËøáÁ®ã‰∏≠ÁöÑ‰∏¥Êó∂Êñá‰ª∂ÔºåÂèØ‰ª•Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code&gt;latexmk -c&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‰ª•‰∏äÁºñËØëËøáÁ®ã‰πüÂèØ‰ª•Áî® &lt;code&gt;make&lt;/code&gt; Â∑•ÂÖ∑Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code&gt;make doc        # ÁºñËØëÁîüÊàê ustcthesis-doc.pdf&#xA;make            # ÁºñËØëÁîüÊàêËÆ∫Êñá main.pdf&#xA;make clean      # Âà†Èô§ÁºñËØëËøáÁ®ã‰∏≠ÁîüÊàêÁöÑ‰∏¥Êó∂Êñá‰ª∂&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÂèçÈ¶àÈóÆÈ¢ò&lt;/h2&gt; &#xA;&lt;p&gt;Â¶ÇÊûúÂèëÁé∞Ê®°ÊùøÊúâÈóÆÈ¢òÔºåËØ∑ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§Êìç‰ΩúÔºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÈòÖËØªÂ≠¶Ê†°ÁöÑÊ†áÂáÜÔºåÂà§Êñ≠ÊòØÂê¶Á¨¶ÂêàÂ≠¶Ê†°ÁöÑË¶ÅÊ±ÇÔºõ&lt;/li&gt; &#xA; &lt;li&gt;ÈòÖËØª &lt;a href=&#34;https://github.com/ustctug/ustcthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;Â∏∏ËßÅÈóÆÈ¢ò FAQ&lt;/a&gt;Ôºõ&lt;/li&gt; &#xA; &lt;li&gt;Â∞Ü TeX ÂèëË°åÁâàÂíåÂÆèÂåÖÂçáÁ∫ßÂà∞ÊúÄÊñ∞ÔºåÂπ∂‰∏îÂ∞ÜÊ®°ÊùøÂçáÁ∫ßÂà∞ Github ‰∏äÊúÄÊñ∞ÁâàÊú¨Ôºå Êü•ÁúãÈóÆÈ¢òÊòØÂê¶Â∑≤Áªè‰øÆÂ§çÔºõ&lt;/li&gt; &#xA; &lt;li&gt;Âú® &lt;a href=&#34;https://github.com/ustctug/ustcthesis/issues&#34;&gt;GitHub Issues&lt;/a&gt; ‰∏≠ÊêúÁ¥¢ËØ•ÈóÆÈ¢òÁöÑÂÖ≥ÈîÆËØçÔºõ&lt;/li&gt; &#xA; &lt;li&gt;Âú® &lt;a href=&#34;https://github.com/ustctug/ustcthesis/issues&#34;&gt;GitHub Issues&lt;/a&gt; ‰∏≠ÊèêÂá∫Êñ∞ issueÔºåÂπ∂ÂõûÁ≠î‰ª•‰∏ãÈóÆÈ¢òÔºö &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‰ΩøÁî®‰∫Ü‰ªÄ‰πàÁâàÊú¨ÁöÑ TeX Live / MacTeX / MiKTeX Ôºü&lt;/li&gt; &#xA;   &lt;li&gt;ÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÊòØ‰ªÄ‰πàÔºü&lt;/li&gt; &#xA;   &lt;li&gt;Ê≠£Á°ÆÁöÑÁªìÊûúÂ∫îËØ•ÊòØ‰ªÄ‰πàÊ†∑ÁöÑÔºü&lt;/li&gt; &#xA;   &lt;li&gt;ÊòØÂê¶Â∫îËØ•ÈôÑ‰∏äÁõ∏ÂÖ≥Ê∫êÁ†ÅÊàñËÄÖÊà™ÂõæÔºü&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Â¶ÇÊûúÂØºÂ∏àÊàñËÄÖÈô¢Á≥ªÂú®Ê†ºÂºè‰∏äÊúâÈ¢ùÂ§ñÁöÑË¶ÅÊ±ÇÔºåËØ∑Â∞ÜËÄÅÂ∏àÁöÑÈÇÆ‰ª∂ËΩ¨ÂèëÁªôÊ®°Êùø‰ΩúËÄÖ„ÄÇ ‰ΩúËÄÖ‰ºöËÄÉËôëÂ¢ûÂä†Êé•Âè£‰ª•‰æø‰øÆÊîπÊ†ºÂºè„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Êõ¥Â§öËµÑÊñô&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ustctug/ustcthesis/wiki/%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97&#34;&gt;LaTeX Êñ∞ÊâãÂÖ•Èó®ÊåáÂçó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ustctug/ustcthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;Â∏∏ËßÅÈóÆÈ¢ò FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ustctug/ustcthesis/wiki/%E5%8F%82%E4%B8%8E%E5%BC%80%E5%8F%91&#34;&gt;ÂèÇ‰∏éÂºÄÂèë&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>alabid/scratch</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/alabid/scratch</id>
    <link href="https://github.com/alabid/scratch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository for Mini-Projects&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Scratch: A Repository for Mini-Projects&lt;/h1&gt; &#xA;&lt;h2&gt;JSON Collapser&lt;/h2&gt; &#xA;&lt;p&gt;Python and Javascript implemention of JSON Collapser algorithm.&lt;/p&gt; &#xA;&lt;p&gt;For more information, visit &lt;a href=&#34;http://alabidan.me/2012/07/21/json-pretty-print-and-json-multi-level-collapse-code-in-javascript-and-python/&#34;&gt;my website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;at /scratch/jsoncollapser/&lt;/p&gt; &#xA;&lt;h2&gt;Linear Counting Probabilistic Algorithm Implementation&lt;/h2&gt; &#xA;&lt;p&gt;Python implementation of the Linear Counting algorithm.&lt;/p&gt; &#xA;&lt;p&gt;at /scratch/linearprob/&lt;/p&gt; &#xA;&lt;h2&gt;Basic Traditional Bloom Filter Implementation&lt;/h2&gt; &#xA;&lt;p&gt;A C++ implementation of a bloom filter&lt;/p&gt; &#xA;&lt;p&gt;at /scratch/bloomfilter/&lt;/p&gt; &#xA;&lt;p&gt;For a better, well-tested, scalable, counting bloom filter, check out &lt;a href=&#34;http://github.com/bitly/dablooms&#34;&gt;dablooms&lt;/a&gt; built by Justin Hines, a HackNY fellow (2012) during Summer 2012.&lt;/p&gt; &#xA;&lt;h2&gt;MIT Open Source License&lt;/h2&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>matze/mtheme</title>
    <updated>2022-05-30T02:24:03Z</updated>
    <id>tag:github.com,2022-05-30:/matze/mtheme</id>
    <link href="https://github.com/matze/mtheme" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern LaTeX Beamer theme&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Metropolis&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT NOTICES FOR VERSION 1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The package and theme name changed from &lt;em&gt;m&lt;/em&gt; to &lt;em&gt;metropolis&lt;/em&gt;!&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;title format&lt;/code&gt; values have been restructured. Please refer to the &lt;a href=&#34;http://mirrors.ctan.org/macros/latex/contrib/beamer-contrib/themes/metropolis/doc/metropolistheme.pdf&#34;&gt;manual&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Metropolis is a simple, modern Beamer theme suitable for anyone to use. It tries to minimize noise and maximize space for content; the only visual flourish it offers is an (optional) progress bar added to each slide. The core design principles of the theme were described in a blog post &lt;a href=&#34;http://bloerg.net/2014/09/20/a-modern-beamer-theme.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Not convinced? Have a look at the &lt;a href=&#34;http://mirrors.ctan.org/macros/latex/contrib/beamer-contrib/themes/metropolis/demo/demo.pdf&#34;&gt;demo slides&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/Bxu52fz.png&#34; alt=&#34;Sample&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install a stable version of this theme, please refer to update instructions of your TeX distribution. Metropolis is on &lt;a href=&#34;http://ctan.org/pkg/beamertheme-metropolis&#34;&gt;CTAN&lt;/a&gt; since December 2014 thus it is part of MikTeX and will be part of TeX Live 2016.&lt;/p&gt; &#xA;&lt;p&gt;Installing Metropolis from source, like any Beamer theme, involves four easy steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Download the source&lt;/strong&gt; with a &lt;code&gt;git clone&lt;/code&gt; of the &lt;a href=&#34;https://github.com/matze/mtheme&#34;&gt;Metropolis repository&lt;/a&gt; or as a &lt;a href=&#34;https://github.com/matze/mtheme/archive/master.zip&#34;&gt;zip archive&lt;/a&gt; of the latest development version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compile the style files&lt;/strong&gt; by running &lt;code&gt;make sty&lt;/code&gt; inside the downloaded directory. (Or run LaTeX directly on &lt;code&gt;source/metropolistheme.ins&lt;/code&gt;.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Move the resulting &lt;code&gt;*.sty&lt;/code&gt; files&lt;/strong&gt; to the folder containing your presentation. To use Metropolis with many presentations, run &lt;code&gt;make install&lt;/code&gt; or move the &lt;code&gt;*.sty&lt;/code&gt; files to a folder in your TeX path instead (might require &lt;code&gt;sudo&lt;/code&gt; rights).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use the theme for your presentation&lt;/strong&gt; by declaring &lt;code&gt;\usetheme{metropolis}&lt;/code&gt; in the preamble of your Beamer document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;For best results&lt;/strong&gt; install Mozilla&#39;s &lt;a href=&#34;https://github.com/bBoxType/FiraSans&#34;&gt;Fira Sans&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The following code shows a minimal example of a Beamer presentation using Metropolis.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;\documentclass{beamer}&#xA;\usetheme{metropolis}           % Use metropolis theme&#xA;\title{A minimal example}&#xA;\date{\today}&#xA;\author{Matthias Vogelgesang}&#xA;\institute{Centre for Modern Beamer Themes}&#xA;\begin{document}&#xA;  \maketitle&#xA;  \section{First Section}&#xA;  \begin{frame}{First Frame}&#xA;    Hello, world!&#xA;  \end{frame}&#xA;\end{document}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Detailed information on using Metropolis can be found in the &lt;a href=&#34;http://mirrors.ctan.org/macros/latex/contrib/beamer-contrib/themes/metropolis/doc/metropolistheme.pdf&#34;&gt;manual&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an alternative dark color theme, please have a look at Ross Churchley&#39;s excellent &lt;a href=&#34;https://github.com/rchurchley/beamercolortheme-owl&#34;&gt;owl&lt;/a&gt; theme.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The theme itself is licensed under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;. This means that if you change the theme and re-distribute it, you &lt;em&gt;must&lt;/em&gt; retain the copyright notice header and license it under the same CC-BY-SA license. This does not affect the presentation that you create with the theme.&lt;/p&gt;</summary>
  </entry>
</feed>