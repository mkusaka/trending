<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-05T02:27:57Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yenchenlin/awesome-NeRF</title>
    <updated>2022-06-05T02:27:57Z</updated>
    <id>tag:github.com,2022-06-05:/yenchenlin/awesome-NeRF</id>
    <link href="https://github.com/yenchenlin/awesome-NeRF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A curated list of awesome neural radiance fields papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Neural Radiance Fields &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A curated list of awesome neural radiance fields papers, inspired by &lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision&#34;&gt;awesome-computer-vision&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/how-to-PR.md&#34;&gt;How to submit a pull request?&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/#papers&#34;&gt;Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/#talks&#34;&gt;Talks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.05204&#34;&gt;Neural Volume Rendering: NeRF And Beyond&lt;/a&gt;, Dellaert and Yen-Chen, Arxiv 2020 | &lt;a href=&#34;https://dellaert.github.io/NeRF/&#34;&gt;blog&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/nerf-survey.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.matthewtancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;, Mildenhall et al., ECCV 2020 | &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L168-L173&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Mildenhall20eccv_nerf--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Faster Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lingjie0206.github.io/papers/NSVF/&#34;&gt;Neural Sparse Voxel Fields&lt;/a&gt;, Liu et al., NeurIPS 2020 | &lt;a href=&#34;https://github.com/facebookresearch/NSVF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L135-L141&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Liu20neurips_sparse_nerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.computationalimaging.org/publications/automatic-integration/&#34;&gt;AutoInt: Automatic Integration for Fast Neural Volume Rendering&lt;/a&gt;, Lindell et al., CVPR 2021 | &lt;a href=&#34;https://github.com/computational-imaging/automatic-integration&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L127-L133&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Lindell20arxiv_AutoInt--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.12490&#34;&gt;DeRF: Decomposed Radiance Fields&lt;/a&gt;, Rebain et al. Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L222-L228&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Rebain20arxiv_derf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://depthoraclenerf.github.io/&#34;&gt;DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks&lt;/a&gt;, Neff et al., CGF 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/donerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--neff2021donerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.10380&#34;&gt;FastNeRF: High-Fidelity Neural Rendering at 200FPS&lt;/a&gt;, Garbin et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/fastnerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Garbin21arxiv_FastNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.13744&#34;&gt;KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs &lt;/a&gt;, Reiser et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/creiser/kilonerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/kilonerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--reiser2021kilonerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://alexyu.net/plenoctrees/&#34;&gt;PlenOctrees for Real-time Rendering of Neural Radiance Fields&lt;/a&gt;, Yu et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/sxyu/volrend&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/plenoctrees.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--yu2021plenoctrees--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.01954&#34;&gt;Mixture of Volumetric Primitives for Efficient Neural Rendering&lt;/a&gt;, Lombardi et al., SIGGRAPH 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/mixture.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vsitzmann.github.io/lfns/&#34;&gt;Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering&lt;/a&gt;, Sitzmann et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/lfn.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Faster Training&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2107.02791.pdf&#34;&gt;Depth-supervised NeRF: Fewer Views and Faster Training for Free&lt;/a&gt;, Deng et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/dunbar12138/DSNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/dsnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11215.pdf&#34;&gt;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction&lt;/a&gt;, Sun et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/sunset1995/DirectVoxGO&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/DirectVoxGO.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--sun2021direct--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Unconstrained Images&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections&lt;/a&gt;, Martin-Brualla et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L152-L158&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--MartinBrualla20arxiv_nerfw--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rover-xingyu.github.io/Ha-NeRF/&#34;&gt;Ha-NeRF&lt;span&gt;😆&lt;/span&gt;: Hallucinated Neural Radiance Fields in the Wild&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/rover-xingyu/Ha-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/Ha-NeRF.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--chen2021hallucinated--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Deformable&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nerfies.github.io/&#34;&gt;Deformable Neural Radiance Fields&lt;/a&gt;, Park et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/google/nerfies&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L206-L212&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Park20arxiv_nerfies--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.albertpumarola.com/research/D-NeRF/index.html&#34;&gt;D-NeRF: Neural Radiance Fields for Dynamic Scenes&lt;/a&gt;, Pumarola et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L214-L220&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Pumarola20arxiv_D_NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gafniguy.github.io/4D-Facial-Avatars/&#34;&gt;Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction&lt;/a&gt;, Gafni et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L87-L93&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Gafni20arxiv_DNRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/&#34;&gt;Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Deforming Scene from Monocular Video&lt;/a&gt;, Tretschk et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/facebookresearch/nonrigid_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L283-L289&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Tretschk20arxiv_NR-NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://volumetric-avatars.github.io/&#34;&gt;PVA: Pixel-aligned Volumetric Avatars&lt;/a&gt;, Raj et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/pva.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nogu-atsu/NARF&#34;&gt;Neural Articulated Radiance Field&lt;/a&gt;, Noguchi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/narf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.00181&#34;&gt;CLA-NeRF: Category-Level Articulated Neural Radiance Field&lt;/a&gt;, Tseng et al., ICRA 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/cla-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/animatable_nerf/&#34;&gt;Animatable Neural Radiance Fields for Human Body Modeling&lt;/a&gt;, Peng et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/animatable_nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Peng21arxiv_animatable_nerf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypernerf.github.io/&#34;&gt;A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields&lt;/a&gt;, Park et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/hypernerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13629&#34;&gt;Animatable Neural Radiance Fields from Monocular RGB Videos&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/JanaldoChen/Anim-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/anim_nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/NeuralActor/&#34;&gt;Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control&lt;/a&gt;, Liu et al., SIGGRAPH Asia 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neuralactor.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Video&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes&lt;/a&gt;, Li et al., CVPR 2021 | &lt;a href=&#34;https://github.com/zhengqili/Neural-Scene-Flow-Fields&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L119-L125&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Li20arxiv_nsff--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://video-nerf.github.io/&#34;&gt;Space-time Neural Irradiance Fields for Free-Viewpoint Video&lt;/a&gt;, Xian et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L299-L305&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Xian20arxiv_stnif--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yilundu.github.io/nerflow/&#34;&gt;Neural Radiance Flow for 4D View Synthesis and Video Processing&lt;/a&gt;, Du et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L79-L85&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Du20arxiv_nerflow--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/neuralbody/&#34;&gt;Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans&lt;/a&gt;, Peng et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neuralbody.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Peng20arxiv_neuralbody--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://neural-3d-video.github.io/&#34;&gt;Neural 3D Video Synthesis&lt;/a&gt;, Li et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/3d-video.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://free-view-video.github.io/&#34;&gt;Dynamic View Synthesis from Dynamic Monocular Video&lt;/a&gt;, Gao et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/dvs_dmv.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Generalization&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.02442&#34;&gt;GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis&lt;/a&gt;, Schwarz et al., NeurIPS 2020 | &lt;a href=&#34;https://github.com/autonomousvision/graf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L237-L243&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Schwarz20neurips_graf--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.04595&#34;&gt;GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering&lt;/a&gt;, Trevithick and Yang, Arxiv 2020 | &lt;a href=&#34;https://github.com/alextrevithick/GRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L291-L297&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Trevithick20arxiv_GRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.02190&#34;&gt;pixelNeRF: Neural Radiance Fields from One or Few Images&lt;/a&gt;, Yu et al., CVPR 2021 | &lt;a href=&#34;https://github.com/sxyu/pixel-nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L329-L335&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Yu20arxiv_pixelNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.02189&#34;&gt;Learned Initializations for Optimizing Coordinate-Based Neural Representations&lt;/a&gt;, Tancik et al., CVPR 2021 | &lt;a href=&#34;https://github.com/tancik/learnit&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L268-L274&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Tancik20arxiv_meta--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis&lt;/a&gt;, Chan et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L24-L30&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Chan20arxiv_piGAN--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://portrait-nerf.github.io/&#34;&gt;Portrait Neural Radiance Fields from a Single Image&lt;/a&gt;, Gao et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L95-L101&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Gao20arxiv_pNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.08860.pdf&#34;&gt;ShaRF: Shape-conditioned Radiance Fields from a Single View&lt;/a&gt;, Rematas et al., ICML 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/sharf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ibrnet.github.io/static/paper.pdf&#34;&gt;IBRNet: Learning Multi-View Image-Based Rendering&lt;/a&gt;, Wang et al., CVPR 2021 | &lt;a href=&#34;https://github.com/googleinterns/IBRNet&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/ibr.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.17269.pdf&#34;&gt;CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields&lt;/a&gt;, Niemeyer &amp;amp; Geiger, Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/CAMPARI.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.00587.pdf&#34;&gt;NeRF-VAE: A Geometry Aware 3D Scene Generative Model&lt;/a&gt;, Kosiorek et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerf-vae.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apple.github.io/ml-gsn/&#34;&gt;Unconstrained Scene Generation with Locally Conditioned Radiance Fields&lt;/a&gt;, DeVries et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/gsn.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apchenstu.github.io/mvsnerf/&#34;&gt;MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/apchenstu/mvsnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/mvsnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtualhumans.mpi-inf.mpg.de/srf/&#34;&gt;Stereo Radiance Fields (SRF): Learning View Synthesis from Sparse Views of Novel Scenes&lt;/a&gt;, Chibane et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/srf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liuyuan-pal.github.io/NeuRay/&#34;&gt;Neural Rays for Occlusion-aware Image-based Rendering&lt;/a&gt;, Liu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neuray.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ajayj.com/dietnerf&#34;&gt;Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis&lt;/a&gt;, Matthew Tancik et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/DietNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vincentfung13.github.io/projects/mine/&#34;&gt;MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis&lt;/a&gt;, Jiaxin Li et al., ICCV 2021 | &lt;a href=&#34;https://github.com/vincentfung13/MINE&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/MINE.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imaging.cs.cmu.edu/torf/&#34;&gt;TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis&lt;/a&gt;, Benjamin Attal et al., NeurIPS 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/turf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/wbjang/home/codenerf&#34;&gt;CodeNeRF: Disentangled Neural Radiance Fields for Object Categories&lt;/a&gt;, Jang et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/CodeNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis&lt;/a&gt;, Gu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/stylenerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bmild.github.io/rawnerf/&#34;&gt;NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images&lt;/a&gt;, Ben Mildenhall et al, arXiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/rawnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Pose Estimation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://yenchenlin.me/inerf/&#34;&gt;iNeRF: Inverting Neural Radiance Fields for Pose Estimation&lt;/a&gt;, Yen-Chen et al. IROS 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L321-L327&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--YenChen20arxiv_iNeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lemonatsu.github.io/ANeRF-Surface-free-Pose-Refinement/&#34;&gt;A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering&lt;/a&gt;, Su et al. Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/a-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Su21arxiv_A_NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nerfmm.active.vision/&#34;&gt;NeRF--: Neural Radiance Fields Without Known Camera Parameters&lt;/a&gt;, Wang et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/ActiveVisionLab/nerfmm&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerf--.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Wang21arxiv_nerfmm--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://edgarsucar.github.io/iMAP/&#34;&gt;iMAP: Implicit Mapping and Positioning in Real-Time&lt;/a&gt;, Sucar et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/imap.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pengsongyou.github.io/nice-slam&#34;&gt;NICE-SLAM: Neural Implicit Scalable Encoding for SLAM&lt;/a&gt;, Zhu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nice-slam.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.15606&#34;&gt;GNeRF: GAN-based Neural Radiance Field without Posed Camera&lt;/a&gt;, Meng et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/gnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/&#34;&gt;BARF: Bundle-Adjusting Neural Radiance Fields&lt;/a&gt;, Lin et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/barf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://postech-cvlab.github.io/SCNeRF/&#34;&gt;Self-Calibrating Neural Radiance Fields&lt;/a&gt;, Jeong et al., ICCV 2021 | &lt;a href=&#34;https://github.com/POSTECH-CVLab/SCNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/SCNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Lighting&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://markboss.me/publication/2021-nerd/&#34;&gt;NeRD: Neural Reflectance Decomposition from Image Collections&lt;/a&gt;, Boss et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L9-L15&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Boss20arxiv_NeRD--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/nerv/&#34;&gt;NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis&lt;/a&gt;, Srinivasan et al. CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L260-L266&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Srinivasan20arxiv_NeRV--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nex-mpi.github.io/&#34;&gt;NeX: Real-time View Synthesis with Neural Basis Expansion&lt;/a&gt;, Wizadwongsa et al. Arxiv 2021 | &lt;a href=&#34;https://github.com/nex-mpi/nex-code&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nex.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/xiuming/projects/nerfactor/&#34;&gt;NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination&lt;/a&gt;, Zhang et al. Arxiv 2021 | &lt;a href=&#34;https://github.com/google/nerfactor&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerfactor.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Compositionality&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07492&#34;&gt;NeRF++: Analyzing and Improving Neural Radiance Fields&lt;/a&gt;, Zhang et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/Kai-46/nerfplusplus&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L345-L351&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Zhang20arxiv_nerf++--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.12100&#34;&gt;GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields&lt;/a&gt;, Niemeyer et al., CVPR 2021, &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L175-L181&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Niemeyer20arxiv_GIRAFFE--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shellguo.com/osf/&#34;&gt;Object-Centric Neural Scene Rendering&lt;/a&gt;, Guo et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L111-L117&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Guo20arxiv_OSF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ziyanw1.github.io/hybrid_nerf/&#34;&gt;Learning Compositional Radiance Fields of Dynamic Human Heads&lt;/a&gt;, Wang et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/hybrid-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Wang20arxiv_hybrid_NeRF--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://light.princeton.edu/neural-scene-graphs/&#34;&gt;Neural Scene Graphs for Dynamic Scenes&lt;/a&gt;, Ost et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L353-L358&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Ost20arxiv_NeuralSceneGraphs--&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kovenyu.com/uorf/&#34;&gt;Unsupervised Discovery of Object Radiance Fields&lt;/a&gt;, Yu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/uorf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/object_nerf/&#34;&gt;Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering&lt;/a&gt;, Yang et al., ICCV 2021 | &lt;a href=&#34;https://github.com/zju3dv/object_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/object-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--yang2021objectnerf--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Scene Labelling and Understanding&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://shuaifengzhi.com/Semantic-NeRF/&#34;&gt;In-Place Scene Labelling and Understanding with Implicit Scene Representation&lt;/a&gt;, Zhi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/semantic-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Editing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://editnerf.csail.mit.edu/&#34;&gt;Editing Conditional Radiance Fields&lt;/a&gt;, Liu et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/stevliu/editnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/editnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jiakai-zhang.github.io/st-nerf/&#34;&gt;Editable Free-viewpoint Video Using a Layered Neural Representation&lt;/a&gt;, Zhang et al., SIGGRAPH 2021 | &lt;a href=&#34;https://github.com/DarlingHang/st-nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/st-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Object Category Modeling&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fig-nerf.github.io/&#34;&gt;FiG-NeRF: Figure Ground Neural Radiance Fields for 3D Object Category Modelling&lt;/a&gt;, Xie et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/fig-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/nvidia-research-nerf-tex-neural-reflectance-field-textures/&#34;&gt;NeRF-Tex: Neural Reflectance Field Textures&lt;/a&gt;, Baatz et al., EGSR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/nerf-tex.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Multi-scale&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jonbarron.info/mipnerf/&#34;&gt;Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields&lt;/a&gt;, Barron et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/mip-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Model Reconstruction&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.10078&#34;&gt;UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction&lt;/a&gt;, Oechsle et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/unisurf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.10689&#34;&gt;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction&lt;/a&gt;, Wang et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/neus.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12052&#34;&gt;Volume Rendering of Neural Implicit Surfaces&lt;/a&gt;, Yariv et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/ventusff/neurecon&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/volsdf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Depth Estimation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://weiyithu.github.io/NerfingMVS/&#34;&gt;NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo&lt;/a&gt;, Wei et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/NerfingMVS.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Talks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LCTYRqW-ne8&amp;amp;t=10190s&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;, Ben Mildenhall&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nRyOzHpcr4Q&amp;amp;feature=emb_logo&amp;amp;ab_channel=cvprtum&#34;&gt;Understanding and Extending Neural Radiance Fields&lt;/a&gt;, Barron et al.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/Rd0nBO6--bM?t=1992&#34;&gt;Towards Photorealism (2nd half)&lt;/a&gt;, Vladlen Koltun&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dPWLybp4LL0&#34;&gt;Neural Radiance Fields for View Synthesis&lt;/a&gt;, Matthew Tancik&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;Tensorflow&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;NeRF&lt;/a&gt;, Mildenhall et al., 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib#L168-L173&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;PyTorch&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;NeRF-PyTorch&lt;/a&gt;, Yen-Chen Lin, 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/pytorch-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl&#34;&gt;NeRF-PyTorch-Lighting&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123&#34;&gt;@kwea123&lt;/a&gt;, 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;NeRF-W&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123&#34;&gt;@kwea123&lt;/a&gt;, 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf&#34;&gt;NeRF-PyTorch3D&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch&#34;&gt;@facebookresearch&lt;/a&gt;, 2020&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Jax&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/jaxnerf&#34;&gt;JaxNeRF&lt;/a&gt;, Deng et al., 2020 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/NeRF-and-Beyond.bib#L55-L60&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;Mip-NeRF&lt;/a&gt;, &lt;a href=&#34;https://github.com/google&#34;&gt;@google&lt;/a&gt;, 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/citations/mipnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lib-pku/libpku</title>
    <updated>2022-06-05T02:27:57Z</updated>
    <id>tag:github.com,2022-06-05:/lib-pku/libpku</id>
    <link href="https://github.com/lib-pku/libpku" rel="alternate"></link>
    <summary type="html">&lt;p&gt;贵校课程资料民间整理&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;libpku - 贵校课程资料民间整理&lt;/h1&gt; &#xA;&lt;h2&gt;Preface&lt;/h2&gt; &#xA;&lt;p&gt;（引用自 &lt;a href=&#34;https://github.com/QSCTech/zju-icicles&#34;&gt;QSCTech/zju-icicles&lt;/a&gt; ）&lt;/p&gt; &#xA;&lt;p&gt;来到一所大学，从第一次接触许多课，直到一门一门完成，这个过程中我们时常收集起许多资料和情报。&lt;/p&gt; &#xA;&lt;p&gt;有些是需要在网上搜索的电子书，每次见到一门新课程，Google 一下教材名称，有的可以立即找到，有的却是要花费许多眼力；有些是历年试卷或者 A4 纸，前人精心收集制作，抱着能对他人有用的想法公开，却需要在各个群或者私下中摸索以至于从学长手中代代相传；有些是上完一门课才恍然领悟的技巧，原来这门课重点如此，当初本可以更轻松地完成得更好……&lt;/p&gt; &#xA;&lt;p&gt;我也曾很努力地收集各种课程资料，但到最后，某些重要信息的得到却往往依然是纯属偶然。这种状态时常令我感到后怕与不安。我也曾在课程结束后终于有了些许方法与总结，但这些想法无处诉说，最终只能把花费时间与精力才换来的经验耗散在了漫漫的遗忘之中。&lt;/p&gt; &#xA;&lt;p&gt;我为这一年一年，这么多人孤军奋战的重复劳动感到不平。&lt;/p&gt; &#xA;&lt;p&gt;我希望能够将这些隐晦的、不确定的、口口相传的资料和经验，变为公开的、易于获取的和大家能够共同完善、积累的共享资料。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;我希望只要是前人走过的弯路，后人就不必再走。&lt;/strong&gt; 这是我的信念，也是我建立这个项目的原因。&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;使用方法：访问 &lt;a href=&#34;https://lib-pku.github.io/&#34;&gt;https://lib-pku.github.io/&lt;/a&gt; ，点击资料链接即可下载。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://minhaskamal.github.io/DownGit/#/home&#34;&gt;https://minhaskamal.github.io/DownGit/#/home&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;——因为很重要所以说了三遍&lt;/p&gt; &#xA;&lt;p&gt;Issue、PR、纠错、资料、选课/考试攻略，完全欢迎！&lt;/p&gt; &#xA;&lt;p&gt;来自大家的关注、维护和贡献，才是让这个攻略继续存在的动力~&lt;/p&gt; &#xA;&lt;p&gt;对于课程的评价可写在对应课程文件夹的 &lt;code&gt;README.md&lt;/code&gt; 中。如果想上传课件（请确保无版权问题），推荐使用 PDF 格式，避免系统差。&lt;/p&gt; &#xA;&lt;p&gt;由于本项目体积很大，故推荐采用在 &lt;strong&gt;GitHub Web 端直接上传&lt;/strong&gt; 的方式，具体操作如下：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;首先 Fork 本项目&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;上传文件到已有文件夹：打开对应文件夹，点击绿色 Download 按钮旁的 upload，上传你的文件。&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;上传文件到新文件夹：打开任意文件夹，点击绿色 Download 按钮旁的 upload，&lt;strong&gt;把浏览器地址栏中文件夹名称改为你想要新建的文件夹名称，然后回车&lt;/strong&gt;，上传你的文件。&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;提交 PR：Fork 本项目，然后在 GitHub 网页端点击 Upload File 上传文件，发起 PR 即可。留意一下项目的文件组织喔。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;或者也可以直接附加在 &lt;strong&gt;Issue&lt;/strong&gt; 中，由维护者进行添加。&lt;/p&gt; &#xA;&lt;p&gt;或者也可以发送邮件至 &lt;strong&gt;&lt;a href=&#34;mailto:libpku@protonmail.com&#34;&gt;libpku@protonmail.com&lt;/a&gt;&lt;/strong&gt; ，由维护者进行添加。&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;这不是北京大学图书馆。 我们也不对项目中信息的准确性或真实性做任何承诺。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;如果有侵权情况，麻烦您发送必要的信息至 &lt;a href=&#34;mailto:libpku@protonmail.com&#34;&gt;libpku@protonmail.com&lt;/a&gt; ，带来不便还请您谅解。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;资料来自网络，相关权利由原作者所有，这个 repo 仅用于收集现有资料。&lt;/p&gt; &#xA;&lt;p&gt;当然，我们不会为收集到的资料收费，或是尝试收取捐赠。&lt;/p&gt; &#xA;&lt;p&gt;我们只是尝试为后来的同学节省一些时间。&lt;/p&gt; &#xA;&lt;h2&gt;Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/QSCTech/zju-icicles&#34;&gt;浙江大学课程攻略共享计划&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/martinwu42/project-hover&#34;&gt;气垫船计划——免费、去中心化的北京大学往年题资料库&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EECS-PKU-XSB/Shared-learning-materials&#34;&gt;北京大学信科学生会学术部资料库&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tongtzeho/PKUCourse&#34;&gt;北大计算机课程大作业&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKUanonym/REKCARC-TSC-UHT&#34;&gt;清华大学计算机系课程攻略&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zjdx1998/seucourseshare&#34;&gt;东南大学课程共享计划&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/USTC-Resource/USTC-Course&#34;&gt;中国科学技术大学计算机学院课程资源&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CoolPhilChen/SJTU-Courses/&#34;&gt;上海交通大学课程资料分享&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sysuexam/SYSU-Exam&#34;&gt;中山大学课程资料分享&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/idealclover/NJU-Review-Materials&#34;&gt;南京大学课程复习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CooperNiu/ZZU-Courses-Resource&#34;&gt;郑州大学课程复习资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brenner8023/gdut-course&#34;&gt;广东工业大学计算机学院课程攻略&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(more to be added....)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ElegantLaTeX/ElegantBook</title>
    <updated>2022-06-05T02:27:57Z</updated>
    <id>tag:github.com,2022-06-05:/ElegantLaTeX/ElegantBook</id>
    <link href="https://github.com/ElegantLaTeX/ElegantBook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Elegant LaTeX Template for Books&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://elegantlatex.org/&#34;&gt;Homepage&lt;/a&gt; | &lt;a href=&#34;https://github.com/ElegantLaTeX/ElegantBook&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://ctan.org/pkg/elegantbook&#34;&gt;CTAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/ElegantLaTeX/ElegantBook/releases&#34;&gt;Download&lt;/a&gt; | &lt;a href=&#34;https://github.com/ElegantLaTeX/ElegantBook/wiki&#34;&gt;Wiki&lt;/a&gt; | &lt;a href=&#34;https://weibo.com/elegantlatex&#34;&gt;Weibo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/ctan/l/elegantbook.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/ctan/v/elegantbook.svg?sanitize=true&#34; alt=&#34;CTAN Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/ElegantLaTeX/ElegantBook.svg?sanitize=true&#34; alt=&#34;Github Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/repo-size/ElegantLaTeX/ElegantBook.svg?sanitize=true&#34; alt=&#34;Repo Size&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ElegantBook: An Elegant LaTeX Template for Books&lt;/h1&gt; &#xA;&lt;p&gt;ElegantBook is designed for writing books, created by &lt;a href=&#34;https://ddswhu.me/&#34;&gt;Dongsheng Deng&lt;/a&gt; and &lt;a href=&#34;https://liam.page/&#34;&gt;Liam Huang&lt;/a&gt;. Just enjoy it! If you have any questions, suggestions or bug reports, you can create issues or contact us at &lt;a href=&#34;mailto:elegantlatex2e@gmail.com&#34;&gt;elegantlatex2e@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Important Notes&lt;/h2&gt; &#xA;&lt;p&gt;For some reasons, &lt;strong&gt;unauthorized&lt;/strong&gt; pull requests are &lt;strong&gt;UNACCEPTABLE&lt;/strong&gt; since May 20, 2019. For those who want to help revise the templates, submit issues or clone to your own repository to modify under the LPPL-1.3c.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thank &lt;a href=&#34;https://github.com/sikouhjw&#34;&gt;sikouhjw&lt;/a&gt; and &lt;a href=&#34;https://github.com/syvshc&#34;&gt;syvshc&lt;/a&gt; for their quick response to Github issues and continuously support work for ElegantLaTeX.&lt;/p&gt; &#xA;&lt;p&gt;Thank ChinaTeX and &lt;a href=&#34;http://www.latexstudio.net/&#34;&gt;LaTeX Studio&lt;/a&gt; for their promotion.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This work is released under the LaTeX Project Public License, v1.3c or later.&lt;/p&gt; &#xA;&lt;h2&gt;Derivative Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XiangyunHuang/ElegantBookdown&#34;&gt;ElegantBookdown&lt;/a&gt;：&lt;a href=&#34;https://github.com/XiangyunHuang&#34;&gt;XiangyunHuang&lt;/a&gt; developed a Bookdown template based on ElegantBook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pzhaonet/bookdownplus&#34;&gt;bookdownplus&lt;/a&gt;: maintained by &lt;a href=&#34;https://github.com/pzhaonet&#34;&gt;pzhaonet&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/annProg/PanBook&#34;&gt;PanBook&lt;/a&gt;：a markdown-based writing workflow Developed by &lt;a href=&#34;https://github.com/annProg&#34;&gt;annProg&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>