<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-03T02:01:56Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fnzhan/Generative-AI</title>
    <updated>2023-09-03T02:01:56Z</updated>
    <id>tag:github.com,2023-09-03:/fnzhan/Generative-AI</id>
    <link href="https://github.com/fnzhan/Generative-AI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multimodal Image Synthesis and Editing: The Generative AI Era [TPAMI 2023]&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/title.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13592&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2107.05399-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Survey&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Naereen/StrapDown.js/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/Naereen/Strapdown.js&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![made-with-Markdown](https://img.shields.io/badge/Made%20with-Markdown-1f425f.svg)](http://commonmark.org) --&gt; &#xA;&lt;!-- [![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](http://ansicolortags.readthedocs.io/?badge=latest) --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/teaser.gif&#34; align=&#34;center&#34;&gt; &#xA;&lt;!-- ![Teaser](teaser.gif) --&gt; &#xA;&lt;!-- ### TODO --&gt; &#xA;&lt;!-- - MISE Dataset for multimodel image synthesis and editing --&gt; &#xA;&lt;p&gt;This project is associated with our survey paper which comprehensively contextualizes the advance of the recent Multimodal Image Synthesis &amp;amp; Editing (MISE) and formulates taxonomies according to data modality and model architectures. The survey is featured on DeepAI and &lt;a href=&#34;https://mp.weixin.qq.com/s/DJXiydi6wHwk7s5Ad2ulaQ&#34;&gt;机器之心&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- For more details, please refer to: &lt;br&gt; --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/logo.png&#34; align=&#34;center&#34; width=&#34;20&#34;&gt; &lt;strong&gt;Multimodal Image Synthesis and Editing: The Generative AI Era [&lt;a href=&#34;https://arxiv.org/abs/2112.13592&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fnzhan.com/Generative-AI/&#34;&gt;Project&lt;/a&gt;]&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://fnzhan.com/&#34;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&#34;https://yingchen001.github.io/&#34;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=SZkh3iAAAAAJ&amp;amp;hl=en&#34;&gt;Rongliang Wu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=DXpYbWkAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=uYmK-A0AAAAJ&amp;amp;hl=en&#34;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://generativevision.mpi-inf.mpg.de/&#34;&gt;Adam Kortylewsk&lt;/a&gt;, &lt;br&gt; &lt;a href=&#34;https://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~epxing/&#34;&gt;Eric Xing&lt;/a&gt; &lt;br&gt; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;!--[DeepAI](https://deepai.org/publication/multimodal-image-synthesis-and-editing-a-survey).**--&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; You are welcome to promote papers via pull request. &lt;br&gt; The process to submit a pull request:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a. Fork the project into your own repository.&lt;/li&gt; &#xA; &lt;li&gt;b. Add the Title, Author, Conference, Paper link, Project link, and Code link in &lt;code&gt;README.md&lt;/code&gt; with below format:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;**Title**&amp;lt;br&amp;gt;&#xA;*Author*&amp;lt;br&amp;gt;&#xA;Conference&#xA;[[Paper](Paper link)]&#xA;[[Code](Project link)]&#xA;[[Project](Code link)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c. Submit the pull request to this branch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Related Surveys &amp;amp; Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Text-to-Image Synthesis: A Review&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Stanislav Frolov, Tobias Hinz, Federico Raue, Jörn Hees, Andreas Dengel&lt;/em&gt;&lt;br&gt; Neural Networks 2021 [&lt;a href=&#34;https://arxiv.org/abs/2101.09983&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Inversion: A Survey&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang&lt;/em&gt;&lt;br&gt; TPAMI 2022 [&lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/awesome-gan-inversion&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Image Synthesis from Intuitive User Input: A Review and Perspectives&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuan Xue, Yuan-Chen Guo, Han Zhang, Tao Xu, Song-Hai Zhang, Xiaolei Huang&lt;/em&gt;&lt;br&gt; Computational Visual Media 2022 [&lt;a href=&#34;https://arxiv.org/abs/2107.04240&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Yutong-Zhou-cv/awesome-Text-to-Image&#34;&gt;Awesome-Text-to-Image&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of Contents (Work in Progress)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methods:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- ### Methods: --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#NeRF-based-Methods&#34;&gt;NeRF-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Diffusion-based-Methods&#34;&gt;Diffusion-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Autoregressive-Methods&#34;&gt;Autoregressive Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Image-Quantizer&#34;&gt;Image Quantizer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#GAN-based-Methods&#34;&gt;GAN-based Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#GAN-Inversion-Methods&#34;&gt;GAN-Inversion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Other-Methods&#34;&gt;Other Methods&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modalities &amp;amp; Datasets:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Text-Encoding&#34;&gt;Text Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Audio-Encoding&#34;&gt;Audio Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NeRF-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local 3D Editing via 3D Distillation of CLIP Knowledge&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.12570&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, You Li&lt;/em&gt;&lt;br&gt; IJCAI 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.05668&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.12422&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://itsallagi.com/dreamtime-a-new-way-to-create-3d-content-from-text/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.00547&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/AvatarStudio/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ori Gordon, Omri Avrahami, Dani Lischinski&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.12760&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.vision.huji.ac.il/blended-nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Youtan Yin, Zhoujie Fu, Fan Yang, Guosheng Lin&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.10503&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ornerf.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cuteyyt/or-nerf&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junzhe Zhu, Peiye Zhuang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hifa-team.github.io/HiFA-site/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://eckertzhang.github.io/Text2NeRF.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yukangcao.github.io/DreamAvatar/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.02827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://janeyeon.github.io/ditto-nerf/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/janeyeon/ditto-nerf-code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13843&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13450&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://danacohen95.github.io/Set-the-Scene/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DanaCohen95/Set-the-Scene&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instruct-nerf2nerf.github.io&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ayaanzhaque/instruct-nerf2nerf&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/3DFuse/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KU-CVLAB/3DFuse&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-To-4D Dynamic Scene Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, Yaniv Taigman&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2301.11280&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://make-a-video3d.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2211.10440&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deepimagination.cc/Magic3D/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gwanghyun Kim, Se Young Chun&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2211.16374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gwang-kim/DATID-3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://datid-3d.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://3ddesigner-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2209.14988&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/dreamfields&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/dreamfields&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2205.15517.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MrTornado24/IDE-3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mrtornado24.github.io/IDE-3D/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.10821&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/donydchen/sem2nerf&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://donydchen.github.io/sem2nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cassiePython/CLIPNeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://cassiepython.github.io/clipnerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CG-NeRF: Conditional Generative Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kyungmin Jo, Gyumin Shim, Sanghun Jung, Soyoung Yang, Jaegul Choo&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.03517&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01455&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/dreamfields&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, Juyong Zhang&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YudongGuo/AD-NeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://yudongguo.github.io/ADNeRF/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=TQO2EBYXLyU&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Diffusion-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dongxu Li, Junnan Li, Steven C.H. Hoi&lt;/em&gt;&lt;br&gt; Arxiv 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2305.14720.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dxli94.github.io/BLIP-Diffusion-website/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka&lt;/em&gt;&lt;br&gt; Arxiv 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2305.18047.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://qianwangx.github.io/InstructEdit/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qianwangx/instructedit&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani Yael, Pritch Michael, Rubinstein Kfir Aberman&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2208.12242.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/dreambooth&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Concept Customization of Text-to-Image Diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~custom-diffusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adobe-research/custom-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Diffusion for Multi-Modal Face Generation and Editing&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, Ziwei Liu&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2304.10530v1.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ziqihuangg.github.io/projects/collaborative-diffusion.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ziqihuangg/Collaborative-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pnp-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MichalGeyer/plug-and-play&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SINE: SINgle Image Editing with Text-to-Image Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zhang-zx.github.io/SINE/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhang-zx/SINE&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://null-text-inversion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Example: Exemplar-Based Image Editing With Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Fantasy-Studio/Paint-by-Example&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaText: Spatio-Textual Representation for Controllable Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2211.14305.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/spatext/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2304.08818.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructPix2Pix Learning to Follow Image Editing Instructions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tim Brooks, Aleksander Holynski, Alexei A. Efros&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix/&#34;&gt;Project&lt;/a&gt;] [[Code]https://github.com/timothybrooks/instruct-pix2pix)]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unite and Conquer: Plug &amp;amp; Play Multi-Modal Synthesis using Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Chaminda Bandara, Vishal M Patel&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nithin-gk.github.io/projectpages/Multidiff/index.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Nithin-GK/UniteandConquer&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffEdit: Diffusion-based semantic image editing with mask guidance&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2210.11427.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu&lt;/em&gt;&lt;br&gt; Arxiv 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2211.01324.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/eDiff-I/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-to-Prompt Image Editing with Cross-Attention Control&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman1 Yael Pritch, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; Arxiv 2022 [&lt;a href=&#34;https://prompt-to-prompt.github.io/ptp_files/Prompt-to-Prompt_preprint.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://prompt-to-prompt.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/prompt-to-prompt&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; Arxiv 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2208.01618.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Human: Text-Driven Controllable Human Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2205.15996.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yumingj.github.io/projects/Text2Human.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Text2Human&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DALL-E 2] Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;https://cdn.openai.com/papers/dall-e-2.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/DALLE2-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;v objective diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Katherine Crowson&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gwanghyun Kim, Jong Chul Ye&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.02711&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Diffusion for Text-driven Editing of Natural Images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omri Avrahami, Dani Lischinski, Ohad Fried&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2111.14818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Autoregressive-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MaskGIT: Masked Generative Image Transformer&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2202.04200&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Project](https://wenxin.baidu.com/wenxin/ernie-vilg)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.15283&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wenxin.baidu.com/wenxin/ernie-vilg&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.12417&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/NUWA&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/C9CTnZJ9ZE0&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L-Verse: Bidirectional Generation Between Image and Text&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.11133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tgisaturday/L-Verse&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Video](https://youtu.be/C9CTnZJ9ZE0)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, Hongxia Yang&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://arxiv.org/abs/2105.14211v3&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Project](https://compvis.github.io/imagebart/)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://openreview.net/pdf?id=-1AAgrS5FF&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/imagebart&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/imagebart/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yupan Huang, Bei Liu, Jianlong Fu, Yutong Lu&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.09756&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/generate-it&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Multimodal Transformer for Bi-directional Image and Text Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.09753&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/generate-it&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Transformers for High-Resolution Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/taming-transformers/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RuDOLPH: One Hyper-Modal Transformer can be creative as DALL-E and smart as CLIP&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Shonenkov and Michael Konstantinov&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://github.com/sberbank-ai/ru-dolph&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generate Images from Texts in Russian (ruDALL-E)&lt;/strong&gt;&lt;br&gt; [&lt;a href=&#34;https://github.com/sberbank-ai/ru-dalle&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://rudalle.ru/en/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-to-Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2102.12092&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/DALL-E&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://openai.com/blog/dall-e/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Transformers for Scene Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Drew A. Hudson, C. Lawrence Zitnick&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://openreview.net/pdf?id=YQeWoRnwTnE&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dorarad/gansformer&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi&lt;/em&gt;&lt;br&gt; EMNLP 2020 [&lt;a href=&#34;https://arxiv.org/abs/2009.11278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/allenai/x-lxmert&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Suzhen Wang, Lincheng Li, Yu Ding, Xin Yu&lt;/em&gt;&lt;br&gt; AAAI 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.02749&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Image-Quantizer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;[TE-VQGAN] Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, Edward Choi&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wcshin-git/TE-VQGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[ViT-VQGAN] Vector-quantized Image Modeling with Improved VQGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/CompVis/taming-transformers)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;[PeCo] PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.12710&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/CompVis/taming-transformers)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-GAN] Taming Transformers for High-Resolution Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Gumbel-VQ] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alexei Baevski, Steffen Schneider, Michael Auli&lt;/em&gt;&lt;br&gt; ICLR 2020 [&lt;a href=&#34;https://openreview.net/pdf?id=rylwJxrYDS&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pytorch/fairseq/raw/main/examples/wav2vec/README.md&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[EM VQ-VAE] Theory and Experiments on Vector Quantized Autoencoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aurko Roy, Ashish Vaswani, Arvind Neelakantan, Niki Parmar&lt;/em&gt;&lt;br&gt; arxiv 2018 [&lt;a href=&#34;https://arxiv.org/abs/1805.11063&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jaywalnut310/Vector-Quantized-Autoencoders&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-VAE] Neural Discrete Representation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu&lt;/em&gt;&lt;br&gt; NIPS 2017 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ritheshkumar95/pytorch-vqvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-VAE2 or EMA-VQ] Generating Diverse High-Fidelity Images with VQ-VAE-2&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Razavi, Aaron van den Oord, Oriol Vinyals&lt;/em&gt;&lt;br&gt; NIPS 2019 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/vq-vae-2-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Discrete VAE] Discrete Variational Autoencoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jason Tyler Rolfe&lt;/em&gt;&lt;br&gt; ICLR 2017 [&lt;a href=&#34;https://arxiv.org/abs/1609.02200&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/DALL-E&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DVAE++] DVAE++: Discrete Variational Autoencoders with Overlapping Transformations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arash Vahdat, William G. Macready, Zhengbing Bian, Amir Khoshaman, Evgeny Andriyash&lt;/em&gt;&lt;br&gt; ICML 2018 [&lt;a href=&#34;https://arxiv.org/abs/1802.04920&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmax1/dvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DVAE#] DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arash Vahdat, Evgeny Andriyash, William G. Macready&lt;/em&gt;&lt;br&gt; NIPS 2018 [&lt;a href=&#34;https://arxiv.org/abs/1805.07445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmax1/dvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;GAN-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;GauGAN2&lt;/strong&gt;&lt;br&gt; &lt;em&gt;NVIDIA&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;http://gaugan.org/gaugan2/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=p9MAvRpT6Cg&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Conditional Image Synthesis with Product-of-Experts GANs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xun Huang, Arun Mallya, Ting-Chun Wang, Ming-Yu Liu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05130&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RiFeGAN2: Rich Feature Generation for Text-to-Image Synthesis from Constrained Prior Knowledge&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, Dapeng Tao&lt;/em&gt;&lt;br&gt; TCSVT 2021 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9656731/authors#authors&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TRGAN: Text to Image Generation Through Optimizing Initial Image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Liang Zhao, Xinwei Li, Pingda Huang, Zhikui Chen, Yanqi Dai, Tianyu Li&lt;/em&gt;&lt;br&gt; ICONIP 2021 [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-92307-5_76&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- **Image Synthesis From Layout With Locality-Aware Mask Adaption [Layout2Image]**&lt;br&gt;&#xA;*Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, Lingyun Sun*&lt;br&gt;&#xA;GCPR 2021&#xA;[[Paper](https://arxiv.org/pdf/2103.13722.pdf)]&#xA;[[Code](https://github.com/stanifrolov/AttrLostGAN)]&#xA;&#xA;**AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style [Layout2Image]**&lt;br&gt;&#xA;*Stanislav Frolov, Avneesh Sharma, Jörn Hees, Tushar Karayil, Federico Raue, Andreas Dengel*&lt;br&gt;&#xA;ICCV 2021&#xA;[[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Image_Synthesis_From_Layout_With_Locality-Aware_Mask_Adaption_ICCV_2021_paper.pdf)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio-Driven Emotional Video Portraits [Audio2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.07452&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jixinya/EVP/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://jixinya.github.io/projects/evp/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SketchyCOCO: Image Generation from Freehand Scene Sketches&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, Changqing Zou&lt;/em&gt;&lt;br&gt; CVPR 2020 [&lt;a href=&#34;https://arxiv.org/pdf/2003.02683.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sysu-imsl/SketchyCOCO&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mikexuq.github.io/test_building_pages/index.html&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Speech-to-Image Translation [Audio2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiguo Li, Xinfeng Zhang, Chuanmin Jia, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao&lt;/em&gt;&lt;br&gt; JSTSP 2020 [&lt;a href=&#34;https://ieeexplore.ieee.org/document/9067083/authors#authors&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/smallflyingpig/speech-to-image-translation-without-text&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://smallflyingpig.github.io/speech-to-image/main&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MirrorGAN: Learning Text-to-image Generation by Redescription [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao&lt;/em&gt;&lt;br&gt; CVPR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1903.05854&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qiaott/MirrorGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He&lt;/em&gt;&lt;br&gt; CVPR 2018 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/taoxugit/AttnGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug &amp;amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski&lt;/em&gt;&lt;br&gt; CVPR 2017 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Evolving-AI-Lab/ppgn&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas&lt;/em&gt;&lt;br&gt; TPAMI 2018 [&lt;a href=&#34;https://arxiv.org/abs/1710.10916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanzhanggit/StackGAN-v2&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas&lt;/em&gt;&lt;br&gt; ICCV 2017 [&lt;a href=&#34;https://arxiv.org/abs/1612.03242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanzhanggit/StackGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;GAN-Inversion-Methods&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, Christian Theobalt&lt;/em&gt;&lt;br&gt; SIGGRAPH 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.10973&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/DragGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+ GAN Space Optimization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, Qiang Liu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01573&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gnobitab/FuseDream&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Umut Kocasari, Alara Dirik, Mert Tiftikci, Pinar Yanardag&lt;/em&gt;&lt;br&gt; WACV 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.08493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/stylemc&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/stylemc/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cycle-Consistent Inverse GAN for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3474085.3475226&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Patashnik_StyleCLIP_Text-Driven_Manipulation_of_StyleGAN_Imagery_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/orpatashnik/StyleCLIP&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=PhR1gpXDu0w&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Talk-to-Edit: Fine-Grained Facial Editing via Dialog&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, Ziwei Liu&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Talk-To-Edit_Fine-Grained_Facial_Editing_via_Dialog_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Talk-to-Edit&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.mmlab-ntu.com/project/talkedit/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Face Image Generation and Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Xia_TediGAN_Text-Guided_Diverse_Face_Image_Generation_and_Manipulation_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IIGROUP/TediGAN&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=L8Na2f5viAM&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01573&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Other-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Driven Image Style Transfer&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tsu-Jui Fu, Xin Eric Wang, William Yang Wang&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2106.00178&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIPstyler: Image Style Transfer with a Single Text Condition&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.00374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paper11667/CLIPstyler&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Text-Encoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLAVA: A Foundational Language And Vision Alignment Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.04482&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/paper11667/CLIPstyler)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Transferable Visual Models From Natural Language Supervision (CLIP)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OpenAI/CLIP&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Audio-Encoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wav2CLIP: Learning Robust Audio Representations From CLIP (Wav2CLIP)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, Juan Pablo Bello&lt;/em&gt;&lt;br&gt; ICASSP 2022 [&lt;a href=&#34;https://arxiv.org/abs/2110.11499&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/descriptinc/lyrebird-wav2clip&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Multimodal CelebA-HQ (&lt;a href=&#34;https://github.com/IIGROUP/MM-CelebA-HQ-Dataset&#34;&gt;https://github.com/IIGROUP/MM-CelebA-HQ-Dataset&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;DeepFashion MultiModal (&lt;a href=&#34;https://github.com/yumingj/DeepFashion-MultiModal&#34;&gt;https://github.com/yumingj/DeepFashion-MultiModal&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our papers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{zhan2023mise,&#xA;  title={Multimodal Image Synthesis and Editing: The Generative AI Era},&#xA;  author={Zhan, Fangneng and Yu, Yingchen and Wu, Rongliang and Zhang, Jiahui and Lu, Shijian and Liu, Lingjie and Kortylewski, Adam and Theobalt, Christian and Xing, Eric},&#xA;  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},&#xA;  year={2023},&#xA;  publisher={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>bingjin/ThinkPython2-CN</title>
    <updated>2023-09-03T02:01:56Z</updated>
    <id>tag:github.com,2023-09-03:/bingjin/ThinkPython2-CN</id>
    <link href="https://github.com/bingjin/ThinkPython2-CN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《Think Python 2e》最新版中文翻译，已完结。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;《Think Python 2ed》最新版中文翻译&lt;/h1&gt; &#xA;&lt;p&gt;本书中文翻译已经完结，感谢各位贡献者的参与和大家的关注及支持。&lt;/p&gt; &#xA;&lt;p&gt;贡献者名单如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bingjin&#34;&gt;@bingjin&lt;/a&gt; 全书校对，完成第1-4、17-21章的翻译。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iphyer&#34;&gt;@ipyher&lt;/a&gt; 完成第5、9、13、15章的翻译。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thejian&#34;&gt;@theJian&lt;/a&gt; 完成第6、11章的翻译。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lroolle&#34;&gt;@lroolle&lt;/a&gt; 完成第7章的翻译。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xpgeng&#34;&gt;@xpgeng&lt;/a&gt; 完成第8章的翻译。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/obserthinker&#34;&gt;@obserthinker&lt;/a&gt; 完成第10、14章的翻译。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SeikaScarlet&#34;&gt;@SeikaScarlet&lt;/a&gt; 完成第12章的翻译，独自完成全书TEX版的制作和校对。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cxyfreedom&#34;&gt;@cxyfreedom&lt;/a&gt; 完成第16章的翻译。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;本书目前托管在&lt;a href=&#34;https://codingpy.com/books/thinkpython2/&#34;&gt;编程派网站&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;使用 Sphinx 制作的 PDF、EPUB 等电子版的分享链接为：&lt;a href=&#34;http://pan.baidu.com/s/1eRZOrHC%E3%80%82&#34;&gt;http://pan.baidu.com/s/1eRZOrHC。&lt;/a&gt; 提取码: n24y。&lt;/p&gt; &#xA;&lt;p&gt;TEX 精校版 PDF 的分享链接为：&lt;a href=&#34;https://pan.baidu.com/s/1pLiwSAn%E3%80%82&#34;&gt;https://pan.baidu.com/s/1pLiwSAn。&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你觉得这本书对你有帮助，可以考虑扫描下面的二维码，打赏我一杯咖啡。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://ww1.sinaimg.cn/mw690/006faQNTgw1f1g1gf903aj308w0ai74q.jpg&#34; alt=&#34;赞助编程派的翻译计划&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;同时也欢迎大家关注我的微信公众号：编程派（ID：codingpy），现在基本是每天定期更新，分享 Python 精彩教程和资源。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;该中译版以知识共享协议（&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/3.0/us/&#34;&gt;Creative Commons Attribution-NonCommercial 3.0 Unported License&lt;/a&gt;）发布，你可以自由分享、修改、复制该中译版，但请不要用于商业用途。&lt;/p&gt; &#xA;&lt;p&gt;本书原著版权归&lt;a href=&#34;http://greenteapress.com/wp/think-python-2e/&#34;&gt;Allen Downey&lt;/a&gt;所有，原作链接如下：&lt;a href=&#34;https://github.com/AllenDowney/ThinkPython2&#34;&gt;https://github.com/AllenDowney/ThinkPython2&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;该中译版在翻译过程中，参考了车万翔老师组织翻译的 Python 2 版：&lt;a href=&#34;https://github.com/carfly/thinkpython-cn&#34;&gt;https://github.com/carfly/thinkpython-cn&lt;/a&gt;，在此表示感谢。&lt;/p&gt; &#xA;&lt;p&gt;本书中如仍存在细微错误，会不定期更新，也希望大家指正。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Just-A-Visitor/Algorithmic-Pseudocode</title>
    <updated>2023-09-03T02:01:56Z</updated>
    <id>tag:github.com,2023-09-03:/Just-A-Visitor/Algorithmic-Pseudocode</id>
    <link href="https://github.com/Just-A-Visitor/Algorithmic-Pseudocode" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains the pseudocode(pdf) of various algorithms and data structures necessary for Interview Preparation and Competitive Coding&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;200&#34; height=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/Just-A-Visitor/Algorithmic-Pseudocode/master/Images/Icon.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/algorithmic-pseudocode/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/algorithmic-pseudocode/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://just-a-visitor.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/website-up-down-green-red/http/shields.io.svg?sanitize=true&#34; alt=&#34;Website shields.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A strange game. The only winning move is not to play&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the pseudo-code of various algorithms and data structures necessary for &lt;strong&gt;Interview Preparation&lt;/strong&gt; and &lt;strong&gt;Competitive Coding&lt;/strong&gt;. The pseudocodes are written such that they can be easily adapted to any language. Let us remove the clutter of language and focus on the core concepts of the question!&lt;/p&gt; &#xA;&lt;h1&gt;Sample GIF&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Just-A-Visitor/Algorithmic-Pseudocode/master/Images/Sample.gif&#34; alt=&#34;Images&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Read this section carefully if you are planning on contributing to this repository.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The What&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;In the &lt;code&gt;Pseudocode&lt;/code&gt; folder, you can find a lot of algorithms. If you&#39;ve come across any interesting algorithms that changed the way you think about any topic, please consider contributing it to this repo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;There are a lot of pseudocodes with no explanation. If you want to write a detailed explanation on the workings and intuition of these algorithms, please raise an issue and start working on it after it is approved). I would prefer if the explanation is in &lt;code&gt;pdf&lt;/code&gt; format. However, markdown format is equally acceptable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are familiar with &lt;code&gt;tikz&lt;/code&gt;, &lt;code&gt;pgf&lt;/code&gt; or &lt;code&gt;beamer&lt;/code&gt;, consider making some animations/graphs/diagrams/plots to explain the various algorithms.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to contribute anything other than pseudocodes, feel free to explore the repository and pick up a code and explain its logic and working (either in &lt;code&gt;pdf&lt;/code&gt; or &lt;code&gt;Markdown&lt;/code&gt; format). If you don&#39;t see your desired code, feel free to add it. However, remember that this repository is not a code dump and you should only add new codes if you have written a good post explaining the intricacies of the algorithm.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Why&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You&#39;ll understand the algorithm in depth once you start working on its pseudocode because now you need to explain your code to people who code in a variety of languages.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Your work might help other people preparing for interviews/competitive programming get acquainted with the core concepts of the algorithms rather than being confused by the clutter of the programming language.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lastly, you&#39;ll get to learn &lt;strong&gt;LaTeX&lt;/strong&gt; which is a great experience in itself.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;The How&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If this is your first time contributing to a public repository, please refer to this &lt;a href=&#34;https://akrabat.com/the-beginners-guide-to-contributing-to-a-github-project/&#34;&gt;link&lt;/a&gt;. For more clarity, you can refer to this &lt;a href=&#34;https://github.com/MarcDiethelm/contributing&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are not familiar with &lt;strong&gt;TeX&lt;/strong&gt; or TypeSetting in general, please refer to this &lt;a href=&#34;https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes&#34;&gt;link&lt;/a&gt;. You don&#39;t need to install anything to contribute to this repository. Just make sure that you have an &lt;strong&gt;Overleaf&lt;/strong&gt; account and you are good to go.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create an &lt;em&gt;issue&lt;/em&gt; if you&#39;ve decided to work on an algorithm and get it approved before the coding phase. &lt;strong&gt;Please do not start working on the issue before commenting on that particular thread.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure to follow the coding standards. Put the source code in a file called &lt;code&gt;SourceCode.tex&lt;/code&gt;. (Notice the Capitalisation).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to code a different implementation than what is already present (for example, &lt;em&gt;iterative&lt;/em&gt; instead of &lt;em&gt;recursive&lt;/em&gt;, constant space instead of linear space, etc), please create a new sub-folder inside the root directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Don&#39;t include a lot of comments in the pseudocode (it just means that the code is not self-expressive). However, if the algorithm is highly non-trivial and you would like to include some explanation, please do so before or after the pseudocode. Refer to this &lt;a href=&#34;https://raw.githubusercontent.com/Just-A-Visitor/Algorithmic-Pseudocode/master/Pseudocode/Heaps/Median%20in%20a%20Stream%20of%20Integers/Median%20in%20Stream.pdf&#34;&gt;link&lt;/a&gt; for example.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Do &lt;strong&gt;not&lt;/strong&gt; create a &lt;strong&gt;ReadMe&lt;/strong&gt; file inside the newly created folder. If you want to submit the code with which you tested your pseudocode, you can add it in the &lt;strong&gt;Validation Codes&lt;/strong&gt; folder following the same hierarchy.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you borrow the code from any online/offline source, please remember to cite it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, &lt;strong&gt;please do not include a &lt;code&gt;pdf&lt;/code&gt; file of the final source code&lt;/strong&gt; (This is to avoid untracked binary in the repo&#39;s history). The pdf files would be generated after everything has been finalized.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make a pull request. Sit back and relax while your pull request gets merged.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stuck?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you need any clarifications or are stuck on something for a long time, feel free to ping us. &lt;a href=&#34;https://gitter.im/algorithmic-pseudocode/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/algorithmic-pseudocode/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;Icon made by &lt;a href=&#34;https://www.flaticon.com/authors/freepik&#34;&gt;Freepik&lt;/a&gt; from &lt;a href=&#34;https://www.flaticon.com&#34;&gt;Flaticon&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>