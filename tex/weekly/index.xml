<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-26T01:59:42Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yyeboah/Awesome-Text-to-3D</title>
    <updated>2023-11-26T01:59:42Z</updated>
    <id>tag:github.com,2023-11-26:/yyeboah/Awesome-Text-to-3D</id>
    <link href="https://github.com/yyeboah/Awesome-Text-to-3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A growing curation of Text-to-3D, Diffusion-to-3D works.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Text-to-3D &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A growing curation of Text-to-3D, Diffusion-to-3D works. Heavily inspired by &lt;a href=&#34;https://github.com/awesome-NeRF/awesome-NeRF&#34;&gt;awesome-NeRF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Recent Updates &lt;span&gt;ðŸ“°&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;11.11.2023&lt;/code&gt; - Added Tutorial Videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;05.08.2023&lt;/code&gt; - Provided citations in BibTeX&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;06.07.2023&lt;/code&gt; - Created initial list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers &lt;span&gt;ðŸ“œ&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.01455&#34;&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/a&gt;, Ajay Jain et al., CVPR 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L1-L6&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.02624&#34;&gt;CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation&lt;/a&gt;, Aditya Sanghi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L8-L13&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields&lt;/a&gt;, Can Wang et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L15-L20&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.03517&#34;&gt;CG-NeRF: Conditional Generative Neural Radiance Fields&lt;/a&gt;, Kyungmin Jo et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L22-L27&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.15172&#34;&gt;PureCLIPNERF: Understanding Pure CLIP Guidance for Voxel Grid NeRF Models&lt;/a&gt;, Han-Hung Lee et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L29-L34&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11277&#34;&gt;TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition&lt;/a&gt;, Yongwei Chen et al., NeurIPS 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L36-L41&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.04493&#34;&gt;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&lt;/a&gt;, Yen-Chi Cheng et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L43-L48&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/a&gt;, Gang Li et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L50-L55&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/a&gt;, Ben Poole et al., ICLR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L57-L62&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models&lt;/a&gt;, Jiale Xu et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L64-L69&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08070&#34;&gt;NeRF-Art: Text-Driven Neural Radiance Fields Stylization&lt;/a&gt;, Can Wang et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L71-L76&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.04628&#34;&gt;Novel View Synthesis with Diffusion Models&lt;/a&gt;, Daniel Watson et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L78-L83&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.16431&#34;&gt;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360Â° Views&lt;/a&gt;, Dejia Xu et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L85-L90&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08751&#34;&gt;Point-E: A System for Generating 3D Point Clouds from Complex Prompts&lt;/a&gt;, Alex Nichol et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L92-L97&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.07600&#34;&gt;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/a&gt;, Gal Metzer et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L99-L104&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/magic3d/&#34;&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/a&gt;, Chen-Hsuan Linet et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L106-L111&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10663&#34;&gt;RealFusion: 360Â° Reconstruction of Any Object from a Single Image&lt;/a&gt;, Luke Melas-Kyriazi et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L113-L118&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.14816&#34;&gt;Monocular Depth Estimation using Diffusion Models&lt;/a&gt;, Saurabh Saxena et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L120-L125&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.00792&#34;&gt;SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction&lt;/a&gt;, Zhizhuo Zho et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L127-L132&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10109&#34;&gt;NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion&lt;/a&gt;, Jiatao Gu et al., ICML 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L134-L139&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.00774&#34;&gt;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/a&gt;, Haochen Wang et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L141-L146&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.03302&#34;&gt;High-fidelity 3D Face Generation from Natural Language Descriptions&lt;/a&gt;, Menghua Wu et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L148-L153&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/a&gt;, Elad Richardson Chen et al., SIGGRAPH 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L155-L160&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.03267&#34;&gt;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a&gt;, Congyue Deng et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L162-L167&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.12231&#34;&gt;DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models&lt;/a&gt;, Jamie Wynn et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L169-L174&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.10406&#34;&gt;3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process&lt;/a&gt;, Yuhan Li et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L540-L545&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gwang-kim.github.io/datid_3d/&#34;&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/a&gt;, Gwanghyun Kim et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L176-L181&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.04628&#34;&gt;Novel View Synthesis with Diffusion Models&lt;/a&gt;, Daniel Watson et al., ICLR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L183-L188&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/a&gt;, Zhengyi Wang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L190-L195&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://3d-avatar-diffusion.microsoft.com/&#34;&gt;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion&lt;/a&gt;, Tengfei Wang et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L197-L202&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17905&#34;&gt;3D-aware Image Generation using 2D Diffusion Models&lt;/a&gt;, Jianfeng Xiang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L204-L209&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.09375&#34;&gt;DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars&lt;/a&gt;, David Svitov et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L666-L671&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior&lt;/a&gt;, Junshu Tang et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L211-L216&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.05916&#34;&gt;GECCO: Geometrically-Conditioned Point Diffusion Models&lt;/a&gt;, MichaÅ‚ J. Tyszkiewicz et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L694-L699&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/a&gt;, Mohammadreza Armandpour et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L218-L223&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.11280&#34;&gt;Text-To-4D Dynamic Scene Generation&lt;/a&gt;, Uriel Singer et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L225-L230&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.02602&#34;&gt;Generative Novel View Synthesis with 3D-Aware Diffusion Models&lt;/a&gt;, Eric R. Chan et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L232-L237&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/a&gt;, Jingbo Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L239-L244&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://guochengqian.github.io/project/magic123/&#34;&gt;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/a&gt;, Guocheng Qian et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L246-L251&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.13508/&#34;&gt;DreamBooth3D: Subject-Driven Text-to-3D Generation&lt;/a&gt;, Amit Raj et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L253-L258&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/a&gt;, Ruoshi Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L260-L265&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image&lt;/a&gt;, Zhenzhen Weng et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L267-L272&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17606&#34;&gt;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/a&gt;, Ruixiang Jiang et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L274-L279&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.13348&#34;&gt;TextDeformer: Geometry Manipulation using Text Guidance&lt;/a&gt;, William Gao et al., Arxiv 2033 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L281-L286&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/ATT3D/&#34;&gt;ATT3D: Amortized Text-to-3D Object Synthesis&lt;/a&gt;, Jonathan Lorraine et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L288-L293&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://neuralcarver.github.io/michelangelo/&#34;&gt;Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation&lt;/a&gt;, Zibo Zhao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L295-L300&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://light.princeton.edu/publication/diffusion-sdf/&#34;&gt;Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions&lt;/a&gt;, Gene Chou et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L302-L307&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hifa-team.github.io/HiFA-site/&#34;&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/a&gt;, Junzhe Zhu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L309-L314&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.lerf.io/&#34;&gt;LERF: Language Embedded Radiance Fields&lt;/a&gt;, Justin Kerr et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L316-L321&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://instruct-nerf2nerf.github.io/&#34;&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/a&gt;, Ayaan Haque et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L323-L328&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ku-cvlab.github.io/3DFuse/&#34;&gt;3DFuse: Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/a&gt;, Junyoung Seo et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L330-L335&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mvdiffusion.github.io/&#34;&gt;MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion&lt;/a&gt;, Shitao Tang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L337-L342&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://one-2-3-45.github.io/&#34;&gt;One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization&lt;/a&gt;, Minghua Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L344-L349&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/a&gt;, Christina Tsalicoglou Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L351-L356&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16223&#34;&gt;Prompt-Free Diffusion: Taking &#34;Text&#34; out of Text-to-Image Diffusion Models&lt;/a&gt;, Xingqian Xu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L358-L363&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scenescape.github.io/&#34;&gt;SceneScape: Text-Driven Consistent Scene Generation&lt;/a&gt;, Rafail Fridman et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L365-L370&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.12570&#34;&gt;Local 3D Editing via 3D Distillation of CLIP Knowledge&lt;/a&gt;, Junha Hyung et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L372-L377&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.nasir.lol/clipmesh&#34;&gt;CLIP-Mesh: Generating textured meshes from text using pretrained image-text models&lt;/a&gt;, Nasir Khalid et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L379-L384&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lukashoel.github.io/text-to-room/&#34;&gt;Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models&lt;/a&gt;, Lukas HÃ¶llein et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L386-L391&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.06714&#34;&gt;Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction&lt;/a&gt;, Hansheng Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L393-L398&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01900&#34;&gt;PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion&lt;/a&gt;, Gwanghyun Kim et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L561-L566&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.11870&#34;&gt;Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models&lt;/a&gt;, Byungjun Kim et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L568-L573&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.02463&#34;&gt;Shap-E: Generating Conditional 3D Implicit Functions&lt;/a&gt;, Heewoo Jun et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L400-L405&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.03869&#34;&gt;Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation&lt;/a&gt;, Aditya Sanghi et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L407-L412&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.05668&#34;&gt;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models&lt;/a&gt;, Xingchen Zhou et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L414-L419&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://daveredrum.github.io/Text2Tex/&#34;&gt;Text2Tex: Text-driven Texture Synthesis via Diffusion Models&lt;/a&gt;, Dave Zhenyu Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L421-L426&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://snap-research.github.io/3DVADER/&#34;&gt;3D VADER - AutoDecoding Latent 3D Diffusion Models&lt;/a&gt;, Evangelos Ntavelis et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L428-L433&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://control4darxiv.github.io/&#34;&gt;Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor&lt;/a&gt;, Ruizhi Shao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L435-L440&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.03414&#34;&gt;DreamSparse: Escaping from Plato&#39;s Cave with 2D Frozen Diffusion Model Given Sparse Views&lt;/a&gt;, Paul Yoo et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L42-L447&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/a&gt;, Rui Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L449-L454&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03117&#34;&gt;DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance&lt;/a&gt;, Longwen Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L456-L461&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.13450&#34;&gt;Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes&lt;/a&gt;, Dana Cohen-Bar et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L463-L468&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.03038&#34;&gt;HeadSculpt: Crafting 3D Head Avatars with Text&lt;/a&gt;, Xiao Han et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L470-L475&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.07279&#34;&gt;Cap3D: Scalable 3D Captioning with Pretrained Models&lt;/a&gt;, Tiange Luo et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L477-L482&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.07154&#34;&gt;InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions&lt;/a&gt;, Jiale Xu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L484-L489&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.09329&#34;&gt;DreamHuman: Animatable 3D Avatars from Text&lt;/a&gt;, Nikos Kolotouros et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L554-L559&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.11418&#34;&gt;FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields&lt;/a&gt;, Sungwon Hwang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L491-L496&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.12981&#34;&gt;3D-LLM: Injecting the 3D World into Large Language Models&lt;/a&gt;, Yining Hong et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L498-L503&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.13908&#34;&gt;Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation&lt;/a&gt;, Chaohui Yu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L505-L510&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15988&#34;&gt;RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects&lt;/a&gt;, Sascha Kirch et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L512-L517&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.03610&#34;&gt;AvatarVerse: High-quality &amp;amp; Stable 3D Avatar Creation from Text and Pose&lt;/a&gt;, Huichao Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L547-L552&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.08545&#34;&gt;TeCH: Text-guided Reconstruction of Lifelike Clothed Humans&lt;/a&gt;, Yangyi Huang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L575-L580&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://skhu101.github.io/HumanLiff/&#34;&gt;HumanLiff: Layer-wise 3D Human Generation with Diffusion Model&lt;/a&gt;, Hu Shoukang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L582-L587&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://tada.is.tue.mpg.de&#34;&gt;TADA! Text to Animatable Digital Avatars&lt;/a&gt;, Tingting Liao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L589-L594&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.09278&#34;&gt;MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR&lt;/a&gt;, Xudong Xu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L596-L601&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.11473&#34;&gt;IT3D: Improved Text-to-3D Generation with Explicit View Synthesis&lt;/a&gt;, Yiwen Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L603-L608&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.04909&#34;&gt;SATR: Zero-Shot Semantic Segmentation of 3D Shapes&lt;/a&gt;, Ahmed Abdelreheem et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L610-L615&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.02469v2&#34;&gt;One-shot Implicit Animatable Avatars with Model-based Priors&lt;/a&gt;, Yangyi Huang et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L617-L622&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.10490&#34;&gt;Texture Generation on 3D Meshes with Point-UV Diffusion&lt;/a&gt;, Xin Yu et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L638-L643&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.16512&#34;&gt;MVDream: Multi-view Diffusion for 3D Generation&lt;/a&gt;, Yichun Shi et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L624-L629&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.16911&#34;&gt;PointLLM: Empowering Large Language Models to Understand Point Clouds&lt;/a&gt;, Xu Runsen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L631-L636&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03453&#34;&gt;SyncDreamer: Generating Multiview-consistent Images from a Single-view Image&lt;/a&gt;, Yuan Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L645-L650&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.03550&#34;&gt;Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model&lt;/a&gt;, Sungwon Hwang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L652-L657&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.07125&#34;&gt;Text-Guided Generation and Editing of Compositional 3D Avatars&lt;/a&gt;, Hao Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L659-L664&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.07920&#34;&gt;Large-Vocabulary 3D Diffusion Model with Transformer&lt;/a&gt;, Ziang Cao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L673-L678&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.14600&#34;&gt;Progressive Text-to-3D Generation for Automatic 3D Prototyping&lt;/a&gt;, Han Yi et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L680-L685&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.16653&#34;&gt;DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation&lt;/a&gt;, Jiaxiang Tang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L687-L692&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.02596&#34;&gt;SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D&lt;/a&gt;, Weiyu Li et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L701-L706&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.01406&#34;&gt;HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation&lt;/a&gt;, Xin Huang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L708-L713&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.17261&#34;&gt;Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors&lt;/a&gt;, Yukang Lin et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L715-L720&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.08529&#34;&gt;GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors&lt;/a&gt;,Taoran Yi et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L722-L727&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.16585&#34;&gt;Text-to-3D using Gaussian Splatting&lt;/a&gt;, Zilong Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L729-L734&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.11784&#34;&gt;Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts&lt;/a&gt;, Xinhua Cheng et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L736-L741&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.12945&#34;&gt;3D-GPT: Procedural 3D Modeling with Large Language Models&lt;/a&gt;, Chunyi Sun et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L743-L748&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.15110&#34;&gt;Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model&lt;/a&gt;, Ruoxi Shi et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L750-L755&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.16818&#34;&gt;DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior&lt;/a&gt;, Jingxiang Sun et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L757-L762&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.17075&#34;&gt;HyperFields: Towards Zero-Shot Generation of NeRFs from Text&lt;/a&gt;, Sudarshan Babu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L764-L769&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.12474&#34;&gt;Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping&lt;/a&gt;, Zijie Pan et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L771-L776&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.19415&#34;&gt;Text-to-3D with classifier score distillation&lt;/a&gt;, Xin Yu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L778-L783&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.17590&#34;&gt;Noise-Free Score Distillation&lt;/a&gt;, Oren Katzir et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L785-L790&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.19784&#34;&gt;CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models&lt;/a&gt;, Ziyang Yuan et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L792-L797&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.02848&#34;&gt;Consistent4D: Consistent 360 Degree Dynamic Object Generation from Monocular Video&lt;/a&gt;, Yanqin Jiang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L799-L809&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.04400&#34;&gt;LRM: Large Reconstruction Model for Single Image to 3D&lt;/a&gt;, Yicong Hong et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L806-L811&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.07885&#34;&gt;One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion&lt;/a&gt;, Minghua Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L813-L818&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.11284&#34;&gt;LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching&lt;/a&gt;, Yixun Liang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L820-L825&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10123&#34;&gt;MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture&lt;/a&gt;, Lincong Feng et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L827-L832&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets &lt;span&gt;ðŸ’¾&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08051&#34;&gt;Objaverse: A Universe of Annotated 3D Objects&lt;/a&gt;, Matt Deitke et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L519-L524&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://objaverse.allenai.org/objaverse-xl-paper.pdf&#34;&gt;Objaverse-XL: A Universe of 10M+ 3D Objects&lt;/a&gt;, Matt Deitke et al., Preprint 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L526-L531&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.03302&#34;&gt;Describe3D: High-Fidelity 3D Face Generation from Natural Language Descriptions&lt;/a&gt;, Menghua Wu et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L533-L538&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Frameworks &lt;span&gt;ðŸ–¥&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/threestudio-project/threestudio&#34;&gt;threestudio: A unified framework for 3D content generation&lt;/a&gt;, Yuan-Chen Guo et al., Github 2023&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/index.html&#34;&gt;Nerfstudio: A Modular Framework for Neural Radiance Field Development&lt;/a&gt;, Matthew Tancik et al., SIGGRAPH 2023&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/MirageML/Mirage3D&#34;&gt;Mirage3D: Open-Source Implementations of 3D Diffusion Models Optimized for GLB Output&lt;/a&gt;, Mirageml et al., Github 2023&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorial Videos &lt;span&gt;ðŸ“º&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=EoAm1yZR-ao&#34;&gt;AI 3D Generation, explained&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Initial List of the STOA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Provide citations in BibTeX&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sub-categorize based on input conditioning&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>bogwonch/cv</title>
    <updated>2023-11-26T01:59:42Z</updated>
    <id>tag:github.com,2023-11-26:/bogwonch/cv</id>
    <link href="https://github.com/bogwonch/cv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;My CV. If you want to talk jobs email me.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Joseph Hallett&lt;/h1&gt; &#xA;&lt;p&gt;Email: ~ &lt;a href=&#34;mailto:josephhallett@gmail.com&#34;&gt;josephhallett@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I am a PhD student at the University of Edinburgh researching how digital evidence can be used to improve the security for app stores. My research covers topics from information security, hacking and malware and logic and theorem proving. I have experience working as a Linux security engineer for the digital television markets where I helped write the Linux and Android specifications for conditional access vendors. My passion is for cutting edge security research and helping users and developers understand the issues and limitations of the security of their devices. Through my teaching I have helped show others just how cool security research is. *&lt;/p&gt; &#xA;&lt;h2&gt;Education&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2013&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Expected 2017.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Working in the &lt;em&gt;AppGuarden&lt;/em&gt; project developing an evidence based secure app store.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Supervised by Dr.&amp;nbsp;David&amp;nbsp;Aspinall&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;2012&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Specialised in cryptography and security.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Won the Infineon Prize for best final year project in Computer Architecture.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Wrote thesis on a steganographic method to create architecture independent bytecode. Looked at the techniques to find valid instructions on multiple architectures, assessed their steganographic properties, and showed how a distinguisher could be written for programs using them.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Experience&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2014&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Developed labs for new &lt;em&gt;Secure Programming&lt;/em&gt; course.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;2012â€“2013&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Developed Linux and Android security specifications for conditional access vendors.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Worked to create a dynamic analysis tool for assessing a systems conformance to the security specifications. Involved writing a kernel module and rootkit to hook into the system under test, and SQL database to implement the tests&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Updating a set-top box system to a more modern kernel. Rewrote BSP to support new kernel; integrated Grsecurity patch set to harden system; helped port their main application from a chroot into an LXC container.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2009â€“2012&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Worked for the &lt;em&gt;Procedural Programming&lt;/em&gt; and &lt;em&gt;Principles of Programming&lt;/em&gt; units.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2008&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Year in industry&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Worked on testing sub-sea&amp;nbsp;software, development of coding standards for C++&amp;nbsp;&amp;amp;&amp;nbsp;Visual&amp;nbsp;Basic inside GE&amp;nbsp;Oil&amp;nbsp;and&amp;nbsp;Gas as well as the development of automatic testing equipment for watchdog PLCs. Witnessed the testing of several hardware and software projects.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;I was awarded a 2:1 for my year of work and report on the design and implementation of automatic test equipment.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2007&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Summer Placement Worked on soldering and testing PCBs as well as modelling the effects of temperature on equipment.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Techincal skills&lt;/h2&gt; &#xA;&lt;h3&gt;Experience with:&lt;/h3&gt; &#xA;&lt;p&gt;Linux kernel development, Android Development, Linux system development, Grsecurity, OpenCL, OpenMP, MPI, Git, Radare2&lt;/p&gt; &#xA;&lt;h3&gt;Programming languages:&lt;/h3&gt; &#xA;&lt;p&gt;C, C++, Assembly languages, Python, Ruby, SQL, Haskell, Java, Prolog, Lisp, Matlab, R and LaTeX&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>csmithbrian/Resume</title>
    <updated>2023-11-26T01:59:42Z</updated>
    <id>tag:github.com,2023-11-26:/csmithbrian/Resume</id>
    <link href="https://github.com/csmithbrian/Resume" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Resume&lt;/h1&gt; &#xA;&lt;p&gt;No description currently.&lt;/p&gt;</summary>
  </entry>
</feed>