<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-22T02:03:15Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>acl-org/acl-style-files</title>
    <updated>2023-10-22T02:03:15Z</updated>
    <id>tag:github.com,2023-10-22:/acl-org/acl-style-files</id>
    <link href="https://github.com/acl-org/acl-style-files" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official style files for papers submitted to venues of the Association for Computational Linguistics&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;*ACL Paper Styles&lt;/h1&gt; &#xA;&lt;p&gt;This directory contains the latest LaTeX and Word templates for *ACL conferences.&lt;/p&gt; &#xA;&lt;h2&gt;Instructions for authors&lt;/h2&gt; &#xA;&lt;p&gt;Paper submissions to *ACL conferences must use the official ACL style templates.&lt;/p&gt; &#xA;&lt;p&gt;The LaTeX style files are available&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;as an &lt;a href=&#34;https://www.overleaf.com/read/crtcwgxzjskr&#34;&gt;Overleaf template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;in this repository, in the &lt;a href=&#34;https://github.com/acl-org/acl-style-files/raw/master/latex&#34;&gt;&lt;code&gt;latex&lt;/code&gt;&lt;/a&gt; subdirectory&lt;/li&gt; &#xA; &lt;li&gt;as a &lt;a href=&#34;https://github.com/acl-org/acl-style-files/archive/refs/heads/master.zip&#34;&gt;.zip file&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://github.com/acl-org/acl-style-files/raw/master/acl_latex.tex&#34;&gt;&lt;code&gt;latex/acl_latex.tex&lt;/code&gt;&lt;/a&gt; for an example.&lt;/p&gt; &#xA;&lt;p&gt;The Microsoft Word template is available in this repository at &lt;a href=&#34;https://github.com/acl-org/acl-style-files/raw/master/word/acl.docx&#34;&gt;&lt;code&gt;word/acl.docx&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please follow the paper formatting guidelines general to *ACL conferences:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://acl-org.github.io/ACLPUB/formatting.html&#34;&gt;Paper formatting guidelines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Authors may not modify these style files or use templates designed for other conferences.&lt;/p&gt; &#xA;&lt;h2&gt;Instructions for publications chairs&lt;/h2&gt; &#xA;&lt;p&gt;To adapt the style files for your conference, please fork this repository and make necessary changes. Minimally, you&#39;ll need to update the name of the conference and rename the files.&lt;/p&gt; &#xA;&lt;p&gt;If you make improvements to the templates that should be propagated to future conferences, please submit a pull request. Thank you in advance!&lt;/p&gt; &#xA;&lt;p&gt;In older versions of the templates, authors were asked to fill in the START submission ID so that it would be stamped at the top of each page of the anonymized version. This is no longer needed, because it is now possible to do this stamping automatically within START. Currently, the way to do this is for the program chair to email &lt;a href=&#34;mailto:support@softconf.com&#34;&gt;support@softconf.com&lt;/a&gt; and request it.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sdx-workshop/sdx-submissions</title>
    <updated>2023-10-22T02:03:15Z</updated>
    <id>tag:github.com,2023-10-22:/sdx-workshop/sdx-submissions</id>
    <link href="https://github.com/sdx-workshop/sdx-submissions" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sound Demixing Challenge Submission Repo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SDX Workshop Submissions&lt;/h1&gt; &#xA;&lt;p&gt;⏰ _Abstract Deadline: &lt;strong&gt;October 15th 2023 (Anywhere on Earth)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The SDX23 workshop is a satellite event at &lt;a href=&#34;https://ismir2023.ismir.net/&#34;&gt;ISMIR 2023&lt;/a&gt;. We will feature invited talks as well as presentations and posters from submitted extended abstracts. The contents of the abstracts can be descriptions of your submission to the SDX23 challenge as well as other topics around audio source separation that you would like to share with the community.&lt;/p&gt; &#xA;&lt;p&gt;This is the submission repository for the &lt;a href=&#34;https://sdx-workshop.github.io&#34;&gt;Sound Demixing Workshop 2023&lt;/a&gt;. The submission system is based on Github pull requests and is fully transparent and open for both, authors and reviewers.&lt;/p&gt; &#xA;&lt;p&gt;In case you have any question, please open an &lt;a href=&#34;https://github.com/sdx-workshop/sdx-submissions/issues&#34;&gt;issue in this repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Submissions&lt;/h3&gt; &#xA;&lt;p&gt;We encourage participants to submit a short abstract. Submissions are markdown abstracts. Please make sure that the abstract is within &lt;strong&gt;250 words&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After a reviewing, you would have the opportunity to present your work in virtually or on-site at the workshop.&lt;/p&gt; &#xA;&lt;h3&gt;How to write an article&lt;/h3&gt; &#xA;&lt;p&gt;All submissions are created via markdown files and uses the same template and syntax as in the &lt;a href=&#34;https://joss.readthedocs.io/en/latest/submitting.html&#34;&gt;Journal of Open Source Software&lt;/a&gt;. An example paper can be seen in &lt;a href=&#34;https://raw.githubusercontent.com/sdx-workshop/sdx-submissions/main/paper.md&#34;&gt;paper.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How to submit an article ?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;a href=&#34;https://github.com&#34;&gt;github&lt;/a&gt; account&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/fork-a-repo/&#34;&gt;Fork&lt;/a&gt; the &lt;a href=&#34;https://github.com/sdx-workshop/sdx-submissions&#34;&gt;SDX workshop submission&lt;/a&gt; repository&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this new repository into your desktop environment&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/YOUR-USERNAME/sdx-submissions&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a branch (the branch name should be author names separated with dashes)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git checkout -b AUTHOR1-AUTHOR2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your article and commit your changes:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git commit -a -m &#34;Some comment&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/pushing-to-a-remote/&#34;&gt;Push&lt;/a&gt; to github&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git push origin AUTHOR1-AUTHOR2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Issue a &lt;a href=&#34;https://help.github.com/articles/using-pull-requests/&#34;&gt;pull request&lt;/a&gt; (PR) with title containing author(s) name and follow the template that will appear once you opened the pull request.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Answer questions and requests by the reviewers made in the PR conversation page.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The pull request will be closed when the paper is accepted and published.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;How to preview an article&lt;/h3&gt; &#xA;&lt;p&gt;All markdown papers are automatically compiled to PDF using a github action service. This can be used to preview the PDF before submission. Look for the &lt;code&gt;paper&lt;/code&gt; artifacts of the &lt;code&gt;Paper Draft&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;img width=&#34;1369&#34; alt=&#34;screenshot&#34; src=&#34;https://user-images.githubusercontent.com/72940/128880968-51d10e51-c1d7-4892-bb4f-8071bb164594.png&#34;&gt;</summary>
  </entry>
  <entry>
    <title>mwxely/AIGS</title>
    <updated>2023-10-22T02:03:15Z</updated>
    <id>tag:github.com,2023-10-22:/mwxely/AIGS</id>
    <link href="https://github.com/mwxely/AIGS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI-Generated Images as Data Source: The Dawn of Synthetic Era&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/title.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.01830&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2310.01830-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Survey&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Naereen/StrapDown.js/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/Naereen/Strapdown.js&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;AI-Generated Images as Data Sources: The Dawn of Synthetic Era [&lt;a href=&#34;https://arxiv.org/abs/2310.01830&#34;&gt;Paper&lt;/a&gt;]&lt;/h1&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://mwxely.github.io/&#34; target=&#34;_blank&#34;&gt;Zuhao Yang&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://fnzhan.com/&#34; target=&#34;_blank&#34;&gt;Fangneng Zhan&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://kunhao-liu.github.io/&#34; target=&#34;_blank&#34;&gt;Kunhao Liu&lt;/a&gt;  Muyu Xu  &#xA;  &lt;a href=&#34;https://personal.ntu.edu.sg/shijian.lu/&#34; target=&#34;_blank&#34;&gt;Shijian Lu&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   Nanyang Technological University, Max Planck Institute for Informatics  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/teaser.png&#34; align=&#34;center&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is associated with our survey paper which comprehensively contextualizes the advance of the recent &lt;strong&gt;AI&lt;/strong&gt;-&lt;strong&gt;G&lt;/strong&gt;enerated Images as Data &lt;strong&gt;S&lt;/strong&gt;ources (&lt;strong&gt;AIGS&lt;/strong&gt;) and visual AIGC by formulating taxonomies according to methodologies and applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; You are welcome to promote papers via pull request. &lt;br&gt; The process to submit a pull request:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a. Fork the project into your own repository.&lt;/li&gt; &#xA; &lt;li&gt;b. Add the Title, Author, Conference, Paper link, Project link, Code link, and Video link in &lt;code&gt;README.md&lt;/code&gt; with below format:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;**Title**&amp;lt;br&amp;gt;&#xA;*Author*&amp;lt;br&amp;gt;&#xA;Conference&#xA;[[Paper](Paper link)]&#xA;[[Project](Project link)]&#xA;[[Code](Code link)]&#xA;[[Video](Video link)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c. Submit the pull request to this branch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Related Surveys &amp;amp; Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Machine Learning for Synthetic Data Generation: A Review&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yingzhou Lu, Minjie Shen, Huazheng Wang, Wenqi Wei&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.04062&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Image Data for Deep Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jason W Anderson, Marcin Ziolkowski, Ken Kennedy, Amy W Apon&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2212.06232&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Data in Human Analysis: A Survey&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Indu Joshi, Marcel Grimmer, Christian Rathgeb, Christoph Busch, Francois Bremond, Antitza Dantcheva&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2208.09191&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Review of Synthetic Image Data and Its Use in Computer Vision&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Keith Man, Javaan Chahl&lt;/em&gt;&lt;br&gt; J. Imaging 2022 [&lt;a href=&#34;https://www.mdpi.com/2313-433X/8/11/310&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Survey on Synthetic Data Generation, Evaluation Methods and GANs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alvaro Figueira, Bruno Vaz&lt;/em&gt;&lt;br&gt; Mathematics 2022 [&lt;a href=&#34;https://www.mdpi.com/2227-7390/10/15/2733&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents (Work in Progress)&lt;/h2&gt; &#xA;&lt;p&gt;Methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#GenerativeModels-link&#34;&gt;Generative Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#GenLabelAcquisition-link&#34;&gt;Label Acquisition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#GenDataAugmentation-link&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#NeuralRendering-link&#34;&gt;Neural Rendering&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#NeuLabelAcquisition-link&#34;&gt;Label Acquisition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#NeuDataAugmentation-link&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#2DVisualPerception-link&#34;&gt;2D Visual Perception&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Classification-link&#34;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Segmentation-link&#34;&gt;Image Segmentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Detection-link&#34;&gt;Object Detection&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#VisualGeneration-link&#34;&gt;Visual Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#SelfsupervisedLearning-link&#34;&gt;Self-supervised Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#3DVisualPerception-link&#34;&gt;3D Visual Perception&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Robotics-link&#34;&gt;Robotics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#AutonomousDriving-link&#34;&gt;Autonomous Driving&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#OtherApplications-link&#34;&gt;Other Applications&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Medical-link&#34;&gt;Medical&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Test-link&#34;&gt;Testing Data&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#TextimageAligned-link&#34;&gt;Text-image Aligned&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#HumanPreference-link&#34;&gt;Human Preference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#DeepfakeDetection-link&#34;&gt;Deepfake Detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Methods&lt;/h2&gt; &#xA;&lt;h3&gt;Generative Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;GenerativeModels-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Label Acquisition&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;GenLabelAcquisition-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[BigGAN] Large Scale GAN Training for High Fidelity Natural Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andrew Brock, Jeff Donahue, Karen Simonyan&lt;/em&gt;&lt;br&gt; ICLR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1809.11096&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-Diffusion] Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/cientgu/VQ-Diffusion#vector-quantized-diffusion-model-for-text-to-image-synthesis-cvpr2022-oral&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[LDM] High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Imagen] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi&lt;/em&gt;&lt;br&gt; NeurIPS 2022 [&lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://imagen-ai.com/?v=0j&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DALL-E 2] Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt;&lt;br&gt; ICML 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.06490&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/datasetGAN/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/datasetGAN_release/tree/master&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen&lt;/em&gt;&lt;br&gt; NeurIPS 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.06160&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://weijiawu.github.io/DatasetDM_page/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/showlab/DatasetDM&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2206.09592&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/gyhandy/Text2Image-for-Detection&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Data Augmentation&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;GenDataAugmentation-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[StyleGAN 2]Analyzing and Improving the Image Quality of StyleGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila&lt;/em&gt;&lt;br&gt; CVPR 2020 [&lt;a href=&#34;https://arxiv.org/abs/1912.04958&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=c-NJtV9Jvp0&amp;amp;feature=youtu.be&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data augmentation generative adversarial networks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn, Alexander Hammers, David Alexander Dickie, Maria Valdés Hernández, Joanna Wardlaw, Daniel Rueckert&lt;/em&gt;&lt;br&gt; arXiv 2018 [&lt;a href=&#34;https://arxiv.org/abs/1810.10863&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancement of Image Classification Using Transfer Learning and GAN-Based Synthetic Data Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Subhajit Chatterjee, Debapriya Hazra, Yung-Cheol Byun, Yong-Woon Kim&lt;/em&gt;&lt;br&gt; Mathmatics 2022 [&lt;a href=&#34;https://www.mdpi.com/2227-7390/10/9/1541&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A data augmentation perspective on diffusion models and retrieval&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Max F. Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, Chris Russell&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.10253&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Effective Data Augmentation With Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.07944&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;http://btrabuc.co/da-fusion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Neural Rendering&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;NeuralRendering-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Label Acquisition&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;NeuLabelAcquisition-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VMRF: View Matching Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu, Wenqing Zhang, Bai Song, Xiaoqin Zhang, Shijian Lu&lt;/em&gt;&lt;br&gt; ACM MM 2022 [&lt;a href=&#34;https://arxiv.org/abs/2207.02621&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Thomas Lips, Victor-Louis De Gusseme, Francis wyffels&lt;/em&gt;&lt;br&gt; ICRA 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.01913&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yenchenlin.me/nerf-supervision/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yenchenlin/nerf-supervision-public&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=_zN-wVwPH1s&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Data Augmentation&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;NeuDataAugmentation-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural-Sim: Learning to Generate Training Data with NeRF&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar, Neel Joshi, Yale Song, Xin Wang, Laurent Itti, Vibhav Vineet&lt;/em&gt;&lt;br&gt; ECCV 2022 [&lt;a href=&#34;https://arxiv.org/abs/2207.11368&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/gyhandy/Neural-Sim-NeRF&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Data Augmentation for Driving Scenes on Camera&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, Hongyang Li&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.10340&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;h3&gt;2D Visual Perception&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;2DVisualPerception-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Image Classification&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Classification-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This Dataset Does Not Exist: Training Models from Generated Images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, Patrick Pérez&lt;/em&gt;&lt;br&gt; ICASSP 2020 [&lt;a href=&#34;https://arxiv.org/abs/1911.02888&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://valeoai.github.io/blog/publications/gan-dataset/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenGAN: Open-Set Recognition via Open Data Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shu Kong, Deva Ramanan&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.02939&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.cs.cmu.edu/~shuk/OpenGAN.html&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/aimerykong/OpenGAN&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=CNYqYXyUHn0&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is synthetic data from generative models ready for image recognition?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, Xiaojuan Qi&lt;/em&gt;&lt;br&gt; ICLR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2206.09592&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/CVMI-Lab/SyntheticData&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fake it till you make it: Learning transferable representations from synthetic ImageNet clones&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2212.08420&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=C8cCvXBNvCI&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training on Thin Air: Improve Image Classification with Generated Data&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yongchao Zhou, Hshmat Sahak, Jimmy Ba&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.15316&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://sites.google.com/view/diffusion-inversion&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yongchao97/diffusion_inversion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Not Just Pretty Pictures: Text-to-Image Generators Enable Interpretable Interventions for Robust Representations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jianhao Yuan, Francesco Pinto, Adam Davies, Aarushi Gupta, Philip Torr&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2212.11237&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton Fookes&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.03298&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leaving Reality to Imagination: Robust Classification via Generated Datasets&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hritik Bansal, Aditya Grover&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2302.02503&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/Hritikbansal/generative-robustness&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Captions are Natural Prompts for Text-to-Image Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, Dacheng Tao&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.08526&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Data from Diffusion Models Improves ImageNet Classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, David J. Fleet&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.08466&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://lisadunlap.github.io/alia-website/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2210.09276&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://imagic-editing.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; ICLR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2208.01618&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fill-Up: Balancing Long-Tailed Data with Generative Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Joonghyuk Shin, Minguk Kang, Jaesik Park&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.07200&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://joonghyuk.com/Fill-Up/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/alex4727/Fill-Up&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu&lt;/em&gt;&lt;br&gt; NeurIPS 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.10586&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/ML-GSAI/DPT&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Image Segmentation&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Segmentation-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Learning Semantic Segmentation from Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuhua Chen, Wen Li, Xiaoran Chen, Luc Van Gool&lt;/em&gt;&lt;br&gt; CVPR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1812.05040&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.05833&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/semanticGAN/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/semanticGAN_code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repurposing GANs for One-shot Semantic Part Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nontawat Tritrong, Pitchaporn Rewatbowornwong, Supasorn Suwajanakorn&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2103.04379&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://repurposegans.github.io/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/bryandlee/repurpose-gan/&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot 3D Multi-modal Medical Image Segmentation using Generative Adversarial Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arnab Kumar Mondal, Jose Dolz, Christian Desrosiers&lt;/em&gt;&lt;br&gt; arXiv 2018 [&lt;a href=&#34;https://arxiv.org/abs/1810.12241&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/arnab39/FewShot_GAN-Unet3D&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nasim Souly, Concetto Spampinato, Mubarak Shah&lt;/em&gt;&lt;br&gt; ICCV 2017 [&lt;a href=&#34;https://arxiv.org/abs/1703.09695&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using GANs to Augment Data for Cloud Image Segmentation Task&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mayank Jain, Conor Meegan, Soumyabrata Dev&lt;/em&gt;&lt;br&gt; ICARSS 2021 [&lt;a href=&#34;https://arxiv.org/abs/2106.03064&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.09316&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/jain15mayank/GAN-augmentation-cloud-image-segmentation&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open-vocabulary Object Segmentation with Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie&lt;/em&gt;&lt;br&gt; ICCV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2301.05221&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://lipurple.github.io/Grounded_Diffusion/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/Lipurple/Grounded-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.05833&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/semanticGAN/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/semanticGAN_code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir Navab&lt;/em&gt;&lt;br&gt; arXiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2107.12518&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://segmentation-in-style.github.io/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/warmspringwinds/segmentation_in_style&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=ik-v3e1Qmd0&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Adela Barriuso, Sanja Fidler, Antonio Torralba&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2201.04684&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/big-datasetgan/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/bigdatasetgan_code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HandsOff: Labeled Dataset Generation With No Additional Human Annotations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Austin Xu, Mariya I. Vasileva, Achal Dave, Arjun Seshadri&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2212.12645&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://austinxu87.github.io/handsoff/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/austinxu87/handsoff/&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, Chunhua Shen&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.11681&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://weijiawu.github.io/DiffusionMask/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/weijiawu/DiffuMask&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Quang Nguyen, Truong Vu, Anh Tran, Khoi Nguyen&lt;/em&gt;&lt;br&gt; NeurIPS 2023 [&lt;a href=&#34;https://arxiv.org/abs/2309.14303&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/VinAIResearch/Dataset-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, Chen Change Loy&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2309.13042&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/Jiahao000/MosaicFusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation in Semantic Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaehoon Choi, Taekyung Kim, Changick Kim&lt;/em&gt;&lt;br&gt; ICCV 2019 [&lt;a href=&#34;https://arxiv.org/abs/1909.00589&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intra-Source Style Augmentation for Improved Domain Generalization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva&lt;/em&gt;&lt;br&gt; WACV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2210.10175&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/boschresearch/ISSA&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DifFSS: Diffusion Model for Few-Shot Semantic Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weimin Tan, Siyuan Chen, Bo Yan&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.00773&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pixel Level Data Augmentation for Semantic Image Segmentation using Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shuangting Liu, Jiaqi Zhang, Yuxin Chen, Yifan Liu, Zengchang Qin, Tao Wan&lt;/em&gt;&lt;br&gt; ICASSP 2019 [&lt;a href=&#34;https://arxiv.org/abs/1811.00174&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can segmentation models be trained with fully synthetically generated data?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru Daniel Tudosiu, Mark S. Graham, Tom Vercauteren, M. Jorge Cardoso&lt;/em&gt;&lt;br&gt; MICCAI 2022 [&lt;a href=&#34;https://arxiv.org/abs/2209.08256&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Label-Efficient Semantic Segmentation with Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, Artem Babenko&lt;/em&gt;&lt;br&gt; ICLR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.03126&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yandex-research.github.io/ddpm-segmentation/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yandex-research/ddpm-segmentation&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[ODISE] Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, Shalini De Mello&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.04803&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://jerryxu.net/ODISE/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/NVlabs/ODISE&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Object Detection&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Detection-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[GeoDiffusion] Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.04607&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://kaichen1998.github.io/projects/geodiffusion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explore the Power of Synthetic Data on Few-shot Object Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shaobo Lin, Kun Wang, Xingyu Zeng, Rui Zhao&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13221&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[SODGAN] Synthetic Data Supervised Salient Object Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhenyu Wu, Lin Wang, Wei Wang, Tengfei Shi, Chenglizhao Chen, Aimin Hao, Shuo Li&lt;/em&gt;&lt;br&gt; ACM MM 2022 [&lt;a href=&#34;https://arxiv.org/abs/2210.13835&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImaginaryNet: Learning Object Detectors without Real Images and Annotations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Minheng Ni, Zitong Huang, Kailai Feng, Wangmeng Zuo&lt;/em&gt;&lt;br&gt; ICLR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2210.06886&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/kodenii/ImaginaryNet&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Roy Voetman, Maya Aghaei, Klaas Dijkstra&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.09762&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Visual Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;VisualGeneration-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Re-Aging GAN: Toward Personalized Face Age Transformation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Farkhod Makhmudkhujaev, Sungeun Hong, and In Kyu Park&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Makhmudkhujaev_Re-Aging_GAN_Toward_Personalized_Face_Age_Transformation_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=NRl0GPgtcBY&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Only a Matter of Style: Age Transformation Using a Style-Based Regression Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; SIGGRAPH 2021 [&lt;a href=&#34;https://arxiv.org/abs/2102.02754&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yuval-alaluf.github.io/SAM/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yuval-alaluf/SAM&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=X_pYC_LtBFw&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Production-Ready Face Re-Aging for Visual Effects&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gaspard Zoss, Prashanth Chandran, Eftychios Sifakis, Markus Gross, Paulo Gotardo, Derek Bradley&lt;/em&gt;&lt;br&gt; TOG 2021 [&lt;a href=&#34;https://studios.disneyresearch.com/app/uploads/2022/10/Production-Ready-Face-Re-Aging-for-Visual-Effects.pdf&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=ZP1ApcdyAjk&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick&lt;/em&gt;&lt;br&gt; ICCV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.11328&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamBooth3D: Subject-Driven Text-to-3D Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13508&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://dreambooth3d.github.io/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://youtu.be/kKVDrbfvOoA&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Self-supervised Learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;SelfsupervisedLearning-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Generative Models as a Data Source for Multiview Representation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Jahanian, Xavier Puig, Yonglong Tian, Phillip Isola&lt;/em&gt;&lt;br&gt; ICLR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2106.05258&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://ali-design.github.io/GenRep/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/ali-design/GenRep&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=qYmGvVrGZno&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.00984&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling with Deep Generative Views&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.14551&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://chail.github.io/gan-ensembling/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/chail/gan-ensembling&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTeacher: Pretraining Image Backbones with Deep Generative Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; ICCV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.07487&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/DreamTeacher/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;3D Visual Perception&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;3DVisualPerception-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Robotics&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Robotics-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;INeRF: Inverting Neural Radiance Fields for Pose Estimation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin&lt;/em&gt;&lt;br&gt; IROS 2021 [&lt;a href=&#34;https://arxiv.org/abs/2012.05877&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yenchenlin.me/inerf/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yenchenlin/iNeRF-public&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=eQuCZaQN0tI&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LENS: Localization enhanced by NeRF synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle&lt;/em&gt;&lt;br&gt; CoRL 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.06558&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=DgIpVoS6ejY&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fu Li, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, Slobodan Ilic&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.04802&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vision-only robot navigation in a neural radiance world&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager&lt;/em&gt;&lt;br&gt; RAL 2022 [&lt;a href=&#34;https://arxiv.org/abs/2110.00168&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://mikh3x4.github.io/nerf-navigation/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/mikh3x4/nerf-navigation&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=5JjWpv9BaaE&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Event-based Camera Tracker by ∇t NeRF&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mana Masuda, Yusuke Sekikawa, Hideo Saito&lt;/em&gt;&lt;br&gt; WACV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.04559&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Autonomous Driving&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;AutonomousDriving-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, Ying-Cong Chen&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.03526&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://len-li.github.io/lift3d-web/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/EnVision-Research/Lift3D&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniSim: A Neural Closed-Loop Sensor Simulator&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, Raquel Urtasun&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.01898&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://waabi.ai/unisim/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://research-assets.waabi.ai/wp-content/uploads/2023/05/UniSim-video_compressed.mp4&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, Hao Zhao&lt;/em&gt;&lt;br&gt; CICAI 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.15058&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://open-air-sun.github.io/mars/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/OPEN-AIR-SUN/mars&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=AdC-jglWvfU&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Other Applications&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;OtherApplications-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Medical&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Medical-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[MedGAN] Generating Multi-label Discrete Patient Records using Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, Jimeng Sun&lt;/em&gt;&lt;br&gt; MLHC 2017 [&lt;a href=&#34;https://arxiv.org/abs/1703.06490&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CorGAN: Correlation-Capturing Convolutional Generative Adversarial Networks for Generating Synthetic Healthcare Records&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amirsina Torfi, Edward A. Fox&lt;/em&gt;&lt;br&gt; FLAIRS 2020 [&lt;a href=&#34;https://arxiv.org/abs/2001.09346&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/astorfi/cor-gan&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Pierre Chambon, Christian Bluethgen, Curtis P. Langlotz, Akshay Chaudhari&lt;/em&gt;&lt;br&gt; NeurIPS 2022 [&lt;a href=&#34;https://arxiv.org/abs/2210.04133&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Skin Lesion Classification Using GAN based Data Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rashid Haroon, Tanveer M. Asjid, Aqeel Khan Hassan&lt;/em&gt;&lt;br&gt; EMBC 2019 [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8857905&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan&lt;/em&gt;&lt;br&gt; Neurocomputing 2018 [&lt;a href=&#34;https://arxiv.org/abs/1803.01229&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic data augmentation using GAN for improved liver lesion classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan&lt;/em&gt;&lt;br&gt; ISBI 2018 [&lt;a href=&#34;https://arxiv.org/abs/1801.02385&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leveraging GANs for data scarcity of COVID-19: Beyond the hype&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hazrat Ali, Christer Gronlund, Zubair Shah&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.03536&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Testing Data&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Test-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2310.08528&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://guanjunwu.github.io/4dgs/index.html&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/hustvl/4DGaussians&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benchmarking Deepart Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yabin Wang, Zhiwu Huang, Xiaopeng Hong&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.14475&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benchmarking Robustness to Text-Guided Corruptions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mohammadreza Mofayezi, Yasamin Medghalchi&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.02963&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/ckoorosh/RobuText&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;h3&gt;Text-image Aligned&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;TextimageAligned-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DiffusionDB (&lt;a href=&#34;https://github.com/poloclub/diffusiondb&#34;&gt;https://github.com/poloclub/diffusiondb&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;JourneyDB (&lt;a href=&#34;https://github.com/JourneyDB/JourneyDB&#34;&gt;https://github.com/JourneyDB/JourneyDB&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h3&gt;Human Preference&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;HumanPreference-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pick-a-Pic v2 (&lt;a href=&#34;https://huggingface.co/datasets/yuvalkirstain/pickapic_v2&#34;&gt;https://huggingface.co/datasets/yuvalkirstain/pickapic_v2&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;ImageReward (&lt;a href=&#34;https://huggingface.co/datasets/THUDM/ImageRewardDB&#34;&gt;https://huggingface.co/datasets/THUDM/ImageRewardDB&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;HPD v2 (&lt;a href=&#34;https://huggingface.co/datasets/xswu/human_preference_dataset&#34;&gt;https://huggingface.co/datasets/xswu/human_preference_dataset&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h3&gt;Deepfake Detection&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;DeepfakeDetection-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DFFD (&lt;a href=&#34;http://cvlab.cse.msu.edu/dffd-dataset.html&#34;&gt;http://cvlab.cse.msu.edu/dffd-dataset.html&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;ForgeryNet (&lt;a href=&#34;https://yinanhe.github.io/projects/forgerynet.html&#34;&gt;https://yinanhe.github.io/projects/forgerynet.html&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;CNNSpot (&lt;a href=&#34;https://github.com/peterwang512/CNNDetection&#34;&gt;https://github.com/peterwang512/CNNDetection&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;CIFAKE (&lt;a href=&#34;https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images&#34;&gt;https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;GenImage (&lt;a href=&#34;https://github.com/GenImage-Dataset/GenImage&#34;&gt;https://github.com/GenImage-Dataset/GenImage&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🤟&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yang2023aigs,&#xA;  title={AI-Generated Images as Data Source: The Dawn of Synthetic Era},&#xA;  author={Zuhao Yang and Fangneng Zhan and Kunhao Liu and Muyu Xu and Shijian Lu},&#xA;  journal={arXiv preprint arXiv:2310.01830},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>