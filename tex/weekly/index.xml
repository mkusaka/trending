<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-08T02:07:42Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gaoxiang12/slambook-en</title>
    <updated>2023-01-08T02:07:42Z</updated>
    <id>tag:github.com,2023-01-08:/gaoxiang12/slambook-en</id>
    <link href="https://github.com/gaoxiang12/slambook-en" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The English version of 14 lectures on visual SLAM.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Welcome to Basic Knowledge on Visual SLAM: From Theory to Practice, by Xiang Gao, Tao Zhang, Qinrui Yan and Yi Liu&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gaoxiang12/slambook-en/master/resources/cheatsheet.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the English version of this book. If you are a Chinese reader, please check &lt;a href=&#34;https://item.jd.com/12666058.html&#34;&gt;this page&lt;/a&gt;. Our code is in github: &lt;a href=&#34;https://github.com/gaoxiang12/slambook2&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The English version is based on slambook2 which is still under review. For now the English version is open source and please feel free to download it.&lt;/p&gt; &#xA;&lt;p&gt;From 2019.6 the English version has been changed to LaTeX instead of markdown. In this repo you can find the LaTeX source files, and if you are only interested in the compiled pdf, click &lt;a href=&#34;https://raw.githubusercontent.com/gaoxiang12/slambook-en/master/slambook-en.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve finished the translation work in 2021.2.28. Thanks for all the patience.&lt;/p&gt; &#xA;&lt;p&gt;If you are doing academic works, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-@Book{Gao2017SLAM,&#34;&gt;title={14 Lectures on Visual SLAM: From Theory to Practice},&#xA;publisher = {Publishing House of Electronics Industry},&#xA;year = {2017},&#xA;author = {Xiang Gao and Tao Zhang and Yi Liu and Qinrui Yan},&#xA;} ```&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;For reviewers and collaborators&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please refer to chapter-cn directory for Chinese version.&lt;/li&gt; &#xA; &lt;li&gt;If you want to compile the cn version, install the fonts in font/ directory first, and use xelatex to compile. There might be some warnings or errors but you will see a compiled pdf to compare the results between the translated version and the original.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>jitinnair1/autoCV</title>
    <updated>2023-01-08T02:07:42Z</updated>
    <id>tag:github.com,2023-01-08:/jitinnair1/autoCV</id>
    <link href="https://github.com/jitinnair1/autoCV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;clean CV LaTex template with GitHub Action that compiles and publishes new changes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;autoCV&lt;/h1&gt; &#xA;&lt;p&gt;A clean CV template in LaTeX along with a GitHub action that complies the &lt;code&gt;*.tex&lt;/code&gt; file and publishes a new PDF version when new changes are pushed to the repo&lt;/p&gt; &#xA;&lt;h2&gt;Template Design&lt;/h2&gt; &#xA;&lt;p&gt;The template is designed to be clean with sections for&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tabular sections for Work Experience, Education and Projects&lt;/li&gt; &#xA; &lt;li&gt;Support for including a list of publications read from a &lt;code&gt;*.bib&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Header with Font Awesome icons&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this repo (you can use the &lt;code&gt;Use this template&lt;/code&gt; button)&lt;/li&gt; &#xA; &lt;li&gt;Modify the &lt;code&gt;cv.tex&lt;/code&gt; file and push changes to your repo&lt;/li&gt; &#xA; &lt;li&gt;The complied PDF will be available under the &lt;code&gt;build&lt;/code&gt; branch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Optionally, while forking this repo, you can get a direct link to the generated PDF which you can use on your website, LinkedIn etc. that will always point to the latest version of your CV. For this, after editing your copy of &lt;code&gt;cv.tex&lt;/code&gt; and pushing changes to your repo, under Settings -&amp;gt; Pages set your Github Pages source to the &lt;code&gt;build&lt;/code&gt; directory&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/lwATw1o.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now, once your site is published, your CV will be accessible at: &lt;a href=&#34;https://username.github.io/autoCV/cv.pdf&#34;&gt;https://username.github.io/autoCV/cv.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;This template on Overleaf&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/autocv/scfvqfpxncwb&#34;&gt;&lt;img alt=&#34;Overleaf&#34; src=&#34;https://img.shields.io/badge/Overleaf-47A141.svg?style=for-the-badge&amp;amp;logo=Overleaf&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, if you have a premium subscription to Overleaf, you can use Overleaf&#39;s GitHub integration to push changes to your GitHub repo directly from Overleaf.&lt;/p&gt; &#xA;&lt;h2&gt;Detailed Instructions..&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jitinnair1/autoCV/wiki/How-to-use-autoCV:-Detailed-Instructions&#34;&gt;.. are available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please start a new discussion or issue if you encounter problems&lt;/p&gt; &#xA;&lt;p&gt;PS: If you liked the template, do star &lt;span&gt;‚≠ê&lt;/span&gt; it! Thanks!&lt;/p&gt; &#xA;&lt;h3&gt;Also, check out:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jitinnair1/gradfolio&#34;&gt;gradfolio&lt;/a&gt; - a minimal, quick-setup template for a personal website/portfolio&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jitinnair1/tail&#34;&gt;Tail&lt;/a&gt; - a minimal, quick-setup template for a blog&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ilaria-manco/multimodal-ml-music</title>
    <updated>2023-01-08T02:07:42Z</updated>
    <id>tag:github.com,2023-01-08:/ilaria-manco/multimodal-ml-music</id>
    <link href="https://github.com/ilaria-manco/multimodal-ml-music" rel="alternate"></link>
    <summary type="html">&lt;p&gt;List of academic resources on Multimodal ML for Music&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Multimodal Machine Learning for Music (MML4Music) &lt;a href=&#34;https://awesome.re&#34;&gt;&lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains a curated list of academic papers, datasets and other resources on multimodal machine learning (MML) research applied to music. By &lt;a href=&#34;http://ilariamanco.com/&#34;&gt;Ilaria Manco&lt;/a&gt; (&lt;a href=&#34;mailto:i.manco@qmul.ac.uk&#34;&gt;i.manco@qmul.ac.uk&lt;/a&gt;), &lt;a href=&#34;http://c4dm.eecs.qmul.ac.uk/&#34;&gt;Centre for Digital Music&lt;/a&gt;, &lt;a href=&#34;https://www.qmul.ac.uk/&#34;&gt;QMUL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is not meant to be an exhaustive list, as MML for music is a varied and growing field, tackling a wide variety of tasks, from music information retrieval to generation, through many different methods. Since this research area is also not yet well established, conventions and definitions aren&#39;t set in stone and this list aims to provide a point of reference for its ongoing development.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#papers&#34;&gt;Academic Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#survey-papers&#34;&gt;Survey Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#journal-and-conference-papers&#34;&gt;Journal and Conference Papers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#workshops-tutorials-&amp;amp;-talks&#34;&gt;Workshops, Tutorials &amp;amp; Talks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#other-projects&#34;&gt;Other Projects&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#statistics-&amp;amp;-visualisations&#34;&gt;Statistics &amp;amp; Visualisations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#how-to-contribute&#34;&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#other-resources&#34;&gt;Other Resources&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;Survey Papers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.05347.pdf&#34;&gt;Multimodal music information processing and retrieval: Survey and future challenges&lt;/a&gt; (F. Simonetta et al., 2019)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.04397.pdf&#34;&gt;Cross-Modal Music Retrieval and Applications: An Overview of Key Methodologies&lt;/a&gt; (M. Muller et al., 2019)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Journal and Conference Papers&lt;/h3&gt; &#xA;&lt;p&gt;Summary of papers on multimodal machine learning for music, including the review papers highlighted &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#survey-papers&#34;&gt;above&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Audio-Text&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Paper Title&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.11671&#34;&gt;Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.google/pubs/pub51943/&#34;&gt;Conversational Music Retrieval with Synthetic Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.12208&#34;&gt;Contrastive audio-language learning for music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ilaria-manco/muscall&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04214&#34;&gt;Learning music audio representations via weak language supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ilaria-manco/mulap&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.12415&#34;&gt;Mulan: A joint embedding of music audio and natural language&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.10901v1&#34;&gt;RECAP: Retrieval Augmented Music Captioner&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.04769&#34;&gt;Clap: Learning audio concepts from natural language supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/CLAP&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.14558&#34;&gt;Toward Universal Text-to-Music Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SeungHeonDoh/music-text-representation&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.11984&#34;&gt;MusCaps: Generating Captions for Music Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ilaria-manco/muscaps&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.nlp4musa-1.13&#34;&gt;MusicBERT - learning multi-modal representations for music and text&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.nlp4musa-1.14&#34;&gt;Music autotagging as captioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.08976.pdf&#34;&gt;Deep cross-modal correlation learning for audio and lyrics in music retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.07276.pdf&#34;&gt;Music mood detection based on audio and lyrics with deep neural net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://repositori.upf.edu/bitstream/handle/10230/33063/Oramas_ISMIR2016_expl.pdf?sequence=1&amp;amp;isAllowed=y&#34;&gt;Exploring customer reviews for music genre classification and evolutionary studies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1608.04868.pdf&#34;&gt;Towards Music Captioning: Generating Music Playlist Descriptions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2008&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.426&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Multimodal Music Mood Classification using Audio and Lyrics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Other&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Paper Title&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.16030.pdf&#34;&gt;Multimodal metric learning for tag-based music retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/minzwon/tag-based-music-retrieval&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.00437&#34;&gt;Enriched music representations with multiple cross-modal contrastive learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/andrebola/contrastive-mir-learning&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9053240&#34;&gt;Large-Scale Weakly-Supervised Content Embeddings for Music Recommendation and Tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Gan_Music_Gesture_for_Visual_Sound_Separation_CVPR_2020_paper.pdf&#34;&gt;Music gesture for visual sound separation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560732.pdf&#34;&gt;Foley music: Learning to generate music from videos&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.14171.pdf&#34;&gt;Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio and Tags&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xavierfav/ae-w2v-attention&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1908.03744.pdf&#34;&gt;Audio-visual embedding for cross-modal music video retrieval through supervised deep CCA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://archives.ismir.net/ismir2019/paper/000015.pdf&#34;&gt;Query-by-Blending: a Music Exploration System Blending Latent Vector Representations of Lyric Word, Song Audio, and Artist&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.00150.pdf&#34;&gt;Learning Affective Correspondence between Music and Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.05347.pdf&#34;&gt;Multimodal music information processing and retrieval: Survey and future challenges&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.04397.pdf&#34;&gt;Cross-Modal Music Retrieval and Applications: An Overview of Key Methodologies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.08727.pdf&#34;&gt;Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/www.gracenote.com&#34;&gt;Query by Video: Cross-Modal Music Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.03160.pdf&#34;&gt;The Sound of Pixels&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hangzhaomit/Sound-of-Pixels&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w49/Qiu_Image_Generation_Associated_CVPR_2018_paper.pdf&#34;&gt;Image generation associated with music data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://transactions.ismir.net/articles/10.5334/tismir.10/&#34;&gt;Multimodal Deep Learning for Music Genre Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fvancesco/music_resnet_classification&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/1806.01483&#34;&gt;JTAV: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mengshor/JTAV&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3206025.3206046&#34;&gt;Cbvmr: content-based video-music retrieval using soft intra-modal structure constraint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/csehong/VM-NET&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3125486.3125492&#34;&gt;A deep multimodal approach for cold-start music recommendation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sergiooramas/tartarus&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.jair.org/index.php/jair/article/view/11101/26292&#34;&gt;Learning neural audio embeddings for grounding semantics in auditory perception&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ceur-ws.org/Vol-1905/recsys2017_poster18.pdf&#34;&gt;Music emotion recognition via end-To-end multimodal neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ohadf.com/papers/FriedFiebrink_NIME2013.pdf&#34;&gt;Cross-modal Sound Mapping Using Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/31911/Fazekas%20Music%20Emotion%20Recognition%202012%20Accepted.pdf;jsessionid=76AE783B989ED4CDBFB8B9C5CE013CE4?sequence=1&#34;&gt;Music emotion recognition: From content- to context-based models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.449.4173&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Musiclef: A benchmark activity in multimodal music information retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/2072529.2072531&#34;&gt;The need for music information retrieval with user-centered and multimodal strategies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2009&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.swarthmore.edu/~turnbull/Papers/Turnbull_CombineMusicTags_SIGIR09.pdf&#34;&gt;Combining audio content and social context for semantic music discovery&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Modalities&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;MARD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal album reviews dataset&lt;/td&gt; &#xA;   &lt;td&gt;Text, Metadata, Audio descriptors&lt;/td&gt; &#xA;   &lt;td&gt;65,566 albums and 263,525 reviews&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www2.ece.rochester.edu/projects/air/projects/URMP.html&#34;&gt;URMP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-instrument musical pieces of recorded performances&lt;/td&gt; &#xA;   &lt;td&gt;MIDI, Audio, Video&lt;/td&gt; &#xA;   &lt;td&gt;44 pieces (12.5GB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gaurav22verma.github.io/IMAC_Dataset.html&#34;&gt;IMAC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Affective correspondences between images and music&lt;/td&gt; &#xA;   &lt;td&gt;Images, Audio&lt;/td&gt; &#xA;   &lt;td&gt;85,000 images and 3,812 songs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ivyha010/EmoMV&#34;&gt;EmoMV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Affective Music-Video Correspondence&lt;/td&gt; &#xA;   &lt;td&gt;Audio, Video&lt;/td&gt; &#xA;   &lt;td&gt;5986 pairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Workshops, Tutorials &amp;amp; Talks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/nlp4musa&#34;&gt;First Workshop on NLP for Music and Audio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Song Describer: a Platform for Collecting Textual Descriptions of Music Recordings - [&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/song-describer.streamlit.app&#34;&gt;link&lt;/a&gt;] | [&lt;a href=&#34;https://ismir2022program.ismir.net/lbd_405.html&#34;&gt;paper&lt;/a&gt;] | [&lt;a href=&#34;https://github.com/ilaria-manco/song-describer&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Statistics &amp;amp; Visualisations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;42 papers referenced. See the details in &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/multimodal_ml_music.bib&#34;&gt;multimodal_ml_music.bib&lt;/a&gt;. Number of articles per year: &lt;img src=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/fig/articles_per_year.png&#34; alt=&#34;Number of articles per year&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are applying multimodal ML to music, there are &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/authors.md&#34;&gt;139 other researchers&lt;/a&gt; in your field.&lt;/li&gt; &#xA; &lt;li&gt;10 tasks investigated. See the list of &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/tasks.md&#34;&gt;tasks&lt;/a&gt;. Tasks pie chart: &lt;img src=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/fig/pie_chart_task.png&#34; alt=&#34;Tasks pie chart&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;Only 13 articles (30%) provide their source code. by &lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Yann Bayle&lt;/a&gt; has a very useful list of &lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music/raw/master/reproducibility.md&#34;&gt;resources on reproducibility for MIR and ML&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How To Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/contributing.md&#34;&gt;contributing.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;You are free to copy, modify, and distribute &lt;em&gt;&lt;strong&gt;Multimodal Machine Learning for Music (MML4Music)&lt;/strong&gt;&lt;/em&gt; with attribution under the terms of the MIT license. See the &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details. This project is heavily based on &lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music&#34;&gt;Deep Learning for Music&lt;/a&gt; by &lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Yann Bayle&lt;/a&gt; and uses other projects. You may refer to them for appropriate license information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ddbeck/readme-checklist&#34;&gt;Readme checklist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pylint.org/&#34;&gt;Pylint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.numpy.org/&#34;&gt;Numpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sciunto-org/python-bibtexparser&#34;&gt;Bibtexparser&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you use the information contained in this repository, please let us know!&lt;/p&gt;</summary>
  </entry>
</feed>