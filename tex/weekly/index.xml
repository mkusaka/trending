<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-07T02:23:46Z</updated>
  <subtitle>Weekly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mit-pdos/xv6-riscv-book</title>
    <updated>2022-08-07T02:23:46Z</updated>
    <id>tag:github.com,2022-08-07:/mit-pdos/xv6-riscv-book</id>
    <link href="https://github.com/mit-pdos/xv6-riscv-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text describing xv6 on RISC-V&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This edition of the book has been converted to LaTeX. In order to build it, ensure you have a TeX distribution that contains the &lt;code&gt;pdflatex&lt;/code&gt; command. With that, you should be able to build the book by running &lt;code&gt;make&lt;/code&gt;, which will clone the OS itself and build the book to &lt;code&gt;book.pdf&lt;/code&gt; in the main directory.&lt;/p&gt; &#xA;&lt;p&gt;Figures are drawn using &lt;code&gt;inkscape&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fnzhan/MISE</title>
    <updated>2022-08-07T02:23:46Z</updated>
    <id>tag:github.com,2022-08-07:/fnzhan/MISE</id>
    <link href="https://github.com/fnzhan/MISE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multimodal Image Synthesis and Editing: A Survey [Under Submission of TPAMI]&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;p align=&#34;center&#34;&gt;Multimodal Image Synthesis and Editing: A Survey&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13592&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2107.05399-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Survey&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Naereen/StrapDown.js/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/Naereen/Strapdown.js&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![made-with-Markdown](https://img.shields.io/badge/Made%20with-Markdown-1f425f.svg)](http://commonmark.org) --&gt; &#xA;&lt;!-- [![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](http://ansicolortags.readthedocs.io/?badge=latest) --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/teaser.png&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- ### TODO --&gt; &#xA;&lt;!-- - MISE Dataset for multimodel image synthesis and editing --&gt; &#xA;&lt;p&gt;This project is associated with our survey paper which comprehensively contextualizes the advance of the recent Multimodal Image Synthesis &amp;amp; Editing (MISE) and formulates taxonomies according to data modality and model architectures.&lt;/p&gt; &#xA;&lt;!-- For more details, please refer to: &lt;br&gt; --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Image Synthesis and Editing: A Survey&lt;/strong&gt; » [&lt;a href=&#34;https://arxiv.org/abs/2112.13592&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; &lt;a href=&#34;https://fnzhan.com/&#34;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&#34;https://yingchen001.github.io/&#34;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=SZkh3iAAAAAJ&amp;amp;hl=en&#34;&gt;Rongliang Wu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=DXpYbWkAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=uYmK-A0AAAAJ&amp;amp;hl=en&#34;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://generativevision.mpi-inf.mpg.de/&#34;&gt;Adam Kortylewsk&lt;/a&gt;, &lt;a href=&#34;https://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~epxing/&#34;&gt;Eric Xing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The survey is trending and featured on &lt;a href=&#34;https://deepai.org/publication/multimodal-image-synthesis-and-editing-a-survey&#34;&gt;DeepAI&lt;/a&gt; and &lt;a href=&#34;https://mp.weixin.qq.com/s/Tp73TvYtj-O05AT421jZjw&#34;&gt;机器之心&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents (Work in Progress)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methods:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- ### Methods: --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Transformer-based-Methods&#34;&gt;Transformer-based Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Image-Quantizer&#34;&gt;Image Quantizer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#NeRF-based-Methods&#34;&gt;NeRF-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Diffusion-based-Methods&#34;&gt;Diffusion-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#GAN-Inversion-Methods&#34;&gt;GAN-Inversion Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#GAN-based-Methods&#34;&gt;GAN-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Other-Methods&#34;&gt;Other Methods&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modalities &amp;amp; Datasets:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Text-Encoding&#34;&gt;Text Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Audio-Encoding&#34;&gt;Audio Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/MISE/main/#Datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transformer-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MaskGIT: Masked Generative Image Transformer&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2202.04200&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Project](https://wenxin.baidu.com/wenxin/ernie-vilg)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.15283&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wenxin.baidu.com/wenxin/ernie-vilg&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.12417&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/NUWA&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/C9CTnZJ9ZE0&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L-Verse: Bidirectional Generation Between Image and Text&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.11133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tgisaturday/L-Verse&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Video](https://youtu.be/C9CTnZJ9ZE0)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, Hongxia Yang&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://arxiv.org/abs/2105.14211v3&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Project](https://compvis.github.io/imagebart/)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://openreview.net/pdf?id=-1AAgrS5FF&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/imagebart&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/imagebart/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yupan Huang, Bei Liu, Jianlong Fu, Yutong Lu&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.09756&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/generate-it&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Multimodal Transformer for Bi-directional Image and Text Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.09753&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/generate-it&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Transformers for High-Resolution Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/taming-transformers/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RuDOLPH: One Hyper-Modal Transformer can be creative as DALL-E and smart as CLIP&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Shonenkov and Michael Konstantinov&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://github.com/sberbank-ai/ru-dolph&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generate Images from Texts in Russian (ruDALL-E)&lt;/strong&gt;&lt;br&gt; [&lt;a href=&#34;https://github.com/sberbank-ai/ru-dalle&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://rudalle.ru/en/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-to-Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2102.12092&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/DALL-E&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://openai.com/blog/dall-e/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Transformers for Scene Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Drew A. Hudson, C. Lawrence Zitnick&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://openreview.net/pdf?id=YQeWoRnwTnE&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dorarad/gansformer&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi&lt;/em&gt;&lt;br&gt; EMNLP 2020 [&lt;a href=&#34;https://arxiv.org/abs/2009.11278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/allenai/x-lxmert&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Suzhen Wang, Lincheng Li, Yu Ding, Xin Yu&lt;/em&gt;&lt;br&gt; AAAI 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.02749&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Image-Quantizer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;[TE-VQGAN] Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, Edward Choi&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wcshin-git/TE-VQGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[ViT-VQGAN] Vector-quantized Image Modeling with Improved VQGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/CompVis/taming-transformers)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;[PeCo] PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.12710&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/CompVis/taming-transformers)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-GAN] Taming Transformers for High-Resolution Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Gumbel-VQ] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alexei Baevski, Steffen Schneider, Michael Auli&lt;/em&gt;&lt;br&gt; ICLR 2020 [&lt;a href=&#34;https://openreview.net/pdf?id=rylwJxrYDS&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pytorch/fairseq/raw/main/examples/wav2vec/README.md&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[EM VQ-VAE] Theory and Experiments on Vector Quantized Autoencoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aurko Roy, Ashish Vaswani, Arvind Neelakantan, Niki Parmar&lt;/em&gt;&lt;br&gt; arxiv 2018 [&lt;a href=&#34;https://arxiv.org/abs/1805.11063&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jaywalnut310/Vector-Quantized-Autoencoders&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-VAE] Neural Discrete Representation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu&lt;/em&gt;&lt;br&gt; NIPS 2017 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ritheshkumar95/pytorch-vqvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-VAE2 or EMA-VQ] Generating Diverse High-Fidelity Images with VQ-VAE-2&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Razavi, Aaron van den Oord, Oriol Vinyals&lt;/em&gt;&lt;br&gt; NIPS 2019 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/vq-vae-2-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Discrete VAE] Discrete Variational Autoencoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jason Tyler Rolfe&lt;/em&gt;&lt;br&gt; ICLR 2017 [&lt;a href=&#34;https://arxiv.org/abs/1609.02200&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/DALL-E&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DVAE++] DVAE++: Discrete Variational Autoencoders with Overlapping Transformations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arash Vahdat, William G. Macready, Zhengbing Bian, Amir Khoshaman, Evgeny Andriyash&lt;/em&gt;&lt;br&gt; ICML 2018 [&lt;a href=&#34;https://arxiv.org/abs/1802.04920&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmax1/dvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DVAE#] DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arash Vahdat, Evgeny Andriyash, William G. Macready&lt;/em&gt;&lt;br&gt; NIPS 2018 [&lt;a href=&#34;https://arxiv.org/abs/1805.07445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmax1/dvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;NeRF-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2205.15517.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MrTornado24/IDE-3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mrtornado24.github.io/IDE-3D/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CG-NeRF: Conditional Generative Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kyungmin Jo, Gyumin Shim, Sanghun Jung, Soyoung Yang, Jaegul Choo&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.03517&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.10821&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/donydchen/sem2nerf&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://donydchen.github.io/sem2nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01455&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/dreamfields&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cassiePython/CLIPNeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://cassiepython.github.io/clipnerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, Juyong Zhang&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YudongGuo/AD-NeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://yudongguo.github.io/ADNeRF/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=TQO2EBYXLyU&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Diffusion-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Human: Text-Driven Controllable Human Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2205.15996.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yumingj.github.io/projects/Text2Human.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Text2Human&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DALL-E 2] Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;https://cdn.openai.com/papers/dall-e-2.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/DALLE2-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;v objective diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Katherine Crowson&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gwanghyun Kim, Jong Chul Ye&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.02711&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Diffusion for Text-driven Editing of Natural Images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omri Avrahami, Dani Lischinski, Ohad Fried&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2111.14818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;GAN-Inversion-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+ GAN Space Optimization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, Qiang Liu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01573&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gnobitab/FuseDream&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Umut Kocasari, Alara Dirik, Mert Tiftikci, Pinar Yanardag&lt;/em&gt;&lt;br&gt; WACV 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.08493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/stylemc&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/stylemc/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cycle-Consistent Inverse GAN for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3474085.3475226&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Patashnik_StyleCLIP_Text-Driven_Manipulation_of_StyleGAN_Imagery_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/orpatashnik/StyleCLIP&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=PhR1gpXDu0w&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Talk-to-Edit: Fine-Grained Facial Editing via Dialog&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, Ziwei Liu&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Talk-To-Edit_Fine-Grained_Facial_Editing_via_Dialog_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Talk-to-Edit&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.mmlab-ntu.com/project/talkedit/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Face Image Generation and Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Xia_TediGAN_Text-Guided_Diverse_Face_Image_Generation_and_Manipulation_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IIGROUP/TediGAN&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=L8Na2f5viAM&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01573&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;GAN-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;GauGAN2&lt;/strong&gt;&lt;br&gt; &lt;em&gt;NVIDIA&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;http://gaugan.org/gaugan2/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=p9MAvRpT6Cg&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Conditional Image Synthesis with Product-of-Experts GANs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xun Huang, Arun Mallya, Ting-Chun Wang, Ming-Yu Liu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05130&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RiFeGAN2: Rich Feature Generation for Text-to-Image Synthesis from Constrained Prior Knowledge&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, Dapeng Tao&lt;/em&gt;&lt;br&gt; TCSVT 2021 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9656731/authors#authors&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TRGAN: Text to Image Generation Through Optimizing Initial Image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Liang Zhao, Xinwei Li, Pingda Huang, Zhikui Chen, Yanqi Dai, Tianyu Li&lt;/em&gt;&lt;br&gt; ICONIP 2021 [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-92307-5_76&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- **Image Synthesis From Layout With Locality-Aware Mask Adaption [Layout2Image]**&lt;br&gt;&#xA;*Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, Lingyun Sun*&lt;br&gt;&#xA;GCPR 2021&#xA;[[Paper](https://arxiv.org/pdf/2103.13722.pdf)]&#xA;[[Code](https://github.com/stanifrolov/AttrLostGAN)]&#xA;&#xA;**AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style [Layout2Image]**&lt;br&gt;&#xA;*Stanislav Frolov, Avneesh Sharma, Jörn Hees, Tushar Karayil, Federico Raue, Andreas Dengel*&lt;br&gt;&#xA;ICCV 2021&#xA;[[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Image_Synthesis_From_Layout_With_Locality-Aware_Mask_Adaption_ICCV_2021_paper.pdf)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio-Driven Emotional Video Portraits [Audio2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.07452&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jixinya/EVP/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://jixinya.github.io/projects/evp/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Speech-to-Image Translation [Audio2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiguo Li, Xinfeng Zhang, Chuanmin Jia, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao&lt;/em&gt;&lt;br&gt; JSTSP 2020 [&lt;a href=&#34;https://ieeexplore.ieee.org/document/9067083/authors#authors&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/smallflyingpig/speech-to-image-translation-without-text&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://smallflyingpig.github.io/speech-to-image/main&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MirrorGAN: Learning Text-to-image Generation by Redescription [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao&lt;/em&gt;&lt;br&gt; CVPR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1903.05854&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qiaott/MirrorGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He&lt;/em&gt;&lt;br&gt; CVPR 2018 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/taoxugit/AttnGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug &amp;amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski&lt;/em&gt;&lt;br&gt; CVPR 2017 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Evolving-AI-Lab/ppgn&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas&lt;/em&gt;&lt;br&gt; TPAMI 2018 [&lt;a href=&#34;https://arxiv.org/abs/1710.10916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanzhanggit/StackGAN-v2&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas&lt;/em&gt;&lt;br&gt; ICCV 2017 [&lt;a href=&#34;https://arxiv.org/abs/1612.03242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanzhanggit/StackGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Other-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Driven Image Style Transfer&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tsu-Jui Fu, Xin Eric Wang, William Yang Wang&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2106.00178&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIPstyler: Image Style Transfer with a Single Text Condition&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.00374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paper11667/CLIPstyler&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Text-Encoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLAVA: A Foundational Language And Vision Alignment Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.04482&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/paper11667/CLIPstyler)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Transferable Visual Models From Natural Language Supervision (CLIP)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OpenAI/CLIP&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Audio-Encoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wav2CLIP: Learning Robust Audio Representations From CLIP (Wav2CLIP)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, Juan Pablo Bello&lt;/em&gt;&lt;br&gt; ICASSP 2022 [&lt;a href=&#34;https://arxiv.org/abs/2110.11499&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/descriptinc/lyrebird-wav2clip&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Multimodal CelebA-HQ (&lt;a href=&#34;https://github.com/IIGROUP/MM-CelebA-HQ-Dataset&#34;&gt;https://github.com/IIGROUP/MM-CelebA-HQ-Dataset&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;DeepFashion MultiModal (&lt;a href=&#34;https://github.com/yumingj/DeepFashion-MultiModal&#34;&gt;https://github.com/yumingj/DeepFashion-MultiModal&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vdumoulin/conv_arithmetic</title>
    <updated>2022-08-07T02:23:46Z</updated>
    <id>tag:github.com,2022-08-07:/vdumoulin/conv_arithmetic</id>
    <link href="https://github.com/vdumoulin/conv_arithmetic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Convolution arithmetic&lt;/h1&gt; &#xA;&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt; &#xA;&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the licence and subject to proper attribution:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt; (&lt;a href=&#34;https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214&#34;&gt;BibTeX&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Convolution animations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%; table-layout:fixed;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, no strides&lt;/td&gt; &#xA;   &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt; &#xA;   &lt;td&gt;Half padding, no strides&lt;/td&gt; &#xA;   &lt;td&gt;Full padding, no strides&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, strides&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides (odd)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Transposed convolution animations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%; table-layout:fixed;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, no strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd_transposed.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides, transposed&lt;/td&gt; &#xA;   &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Dilated convolution animations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:25%&#34; ; table-layout:fixed;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img width=&#34;150px&#34; src=&#34;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;No padding, no stride, dilation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Generating the Makefile&lt;/h2&gt; &#xA;&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./bin/generate_makefile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating the animations&lt;/h2&gt; &#xA;&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make all_animations&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the &lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Compiling the document&lt;/h2&gt; &#xA;&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>