<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-01T02:14:58Z</updated>
  <subtitle>Monthly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hmemcpy/milewski-ctfp-pdf</title>
    <updated>2023-08-01T02:14:58Z</updated>
    <id>tag:github.com,2023-08-01:/hmemcpy/milewski-ctfp-pdf</id>
    <link href="https://github.com/hmemcpy/milewski-ctfp-pdf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bartosz Milewski&#39;s &#39;Category Theory for Programmers&#39; unofficial PDF and LaTeX source&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hmemcpy/milewski-ctfp-pdf.svg?style=flat-square&#34; alt=&#34;GitHub stars&#34;&gt; &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/hmemcpy/milewski-ctfp-pdf/nix-flake-check.yaml?branch=master&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Download-latest-green.svg?style=flat-square&#34; alt=&#34;Download&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC_By_SA-green.svg?style=flat-square&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Category Theory For Programmers&lt;/h1&gt; &#xA;&lt;p&gt;An &lt;em&gt;unofficial&lt;/em&gt; PDF version of &#34;&lt;strong&gt;C&lt;/strong&gt;ategory &lt;strong&gt;T&lt;/strong&gt;heory &lt;strong&gt;F&lt;/strong&gt;or &lt;strong&gt;P&lt;/strong&gt;rogrammers&#34; by &lt;a href=&#34;https://github.com/BartoszMilewski&#34;&gt;Bartosz Milewski&lt;/a&gt;, converted from his &lt;a href=&#34;https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/&#34;&gt;blogpost series&lt;/a&gt; (&lt;em&gt;with permission!&lt;/em&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/601206/47271389-8eea0900-d581-11e8-8e81-5b932e336336.png&#34; alt=&#34;Category Theory for Programmers&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Buy the book&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.blurb.com/b/9621951-category-theory-for-programmers-new-edition-hardco&#34;&gt;Standard edition in full-color hardcover print&lt;/a&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Publish date: 12 August, 2019.&lt;/li&gt; &#xA;   &lt;li&gt;Based off release tag &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0&#34;&gt;v1.3.0&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hmemcpy/milewski-ctfp-pdf/master/errata-1.3.0.md&#34;&gt;errata-1.3.0&lt;/a&gt; for changes and fixes since print.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.blurb.com/b/9603882-category-theory-for-programmers-scala-edition-pape&#34;&gt;Scala Edition in paperback&lt;/a&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Publish date: 12 August, 2019.&lt;/li&gt; &#xA;   &lt;li&gt;Based off release tag &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0&#34;&gt;v1.3.0&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hmemcpy/milewski-ctfp-pdf/master/errata-scala.md&#34;&gt;errata-scala&lt;/a&gt; for changes and fixes since print.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build the book&lt;/h2&gt; &#xA;&lt;p&gt;The building workflow requires &lt;a href=&#34;https://nixos.org/nix/&#34;&gt;Nix&lt;/a&gt;. After &lt;a href=&#34;https://nixos.org/download.html&#34;&gt;installing Nix&lt;/a&gt;, you need to enable the upcoming &#34;flake&#34; feature which must be &lt;a href=&#34;https://nixos.wiki/wiki/Flakes&#34;&gt;enabled manually&lt;/a&gt; the time being. This is needed to expose the new Nix commands and flakes support that are hidden behind feature-flags.&lt;/p&gt; &#xA;&lt;p&gt;Afterwards, type &lt;code&gt;nix flake show&lt;/code&gt; in the root directory of the project to see all the available versions of this book. Then type &lt;code&gt;nix build .#&amp;lt;edition&amp;gt;&lt;/code&gt; to build the edition you want (Haskell, Scala, OCaml, Reason and their printed versions). For example, to build the Scala edition you&#39;ll have to type &lt;code&gt;nix build .#ctfp-scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Upon successful compilation, the PDF file will be placed in the &lt;code&gt;result&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The command &lt;code&gt;nix develop&lt;/code&gt; will provide a shell containing all the required dependencies to build the book manually using the provided &lt;code&gt;Makefile&lt;/code&gt;. To build the &lt;code&gt;ctfp-scala&lt;/code&gt; edition, just run &lt;code&gt;make ctfp-scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Contributors are welcome to contribute to this book by sending pull-requests. Once reviewed, the changes are merged in the main branch and will be incorporated in the next release.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note from &lt;a href=&#34;https://github.com/BartoszMilewski&#34;&gt;Bartosz&lt;/a&gt;&lt;/strong&gt;: I really appreciate all your contributions. You made this book much better than I could have imagined. Thank you!&lt;/p&gt; &#xA;&lt;p&gt;Find the &lt;a href=&#34;https://github.com/hmemcpy/milewski-ctfp-pdf/graphs/contributors&#34;&gt;list of contributors on Github&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;PDF LaTeX source and the tools to create it are based on the work by &lt;a href=&#34;https://github.com/sarabander&#34;&gt;Andres Raba&lt;/a&gt;. The book content is taken, with permission, from &lt;a href=&#34;https://github.com/BartoszMilewski&#34;&gt;Bartosz Milewski&lt;/a&gt;&#39;s blogpost series, and adapted to the LaTeX format.&lt;/p&gt; &#xA;&lt;p&gt;The original blog post acknowledgments by Bartosz are consolidated in the &lt;em&gt;Acknowledgments&lt;/em&gt; page at the end of the book.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The PDF book, &lt;code&gt;.tex&lt;/code&gt; files, and associated images and figures in directories &lt;code&gt;src/fig&lt;/code&gt; and &lt;code&gt;src/content&lt;/code&gt; are licensed under &lt;a href=&#34;https://spdx.org/licenses/CC-BY-SA-4.0.html&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The script files &lt;code&gt;scraper.py&lt;/code&gt; and others are licensed under &lt;a href=&#34;https://spdx.org/licenses/GPL-3.0.html&#34;&gt;GNU General Public License version 3&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jakegut/resume</title>
    <updated>2023-08-01T02:14:58Z</updated>
    <id>tag:github.com,2023-08-01:/jakegut/resume</id>
    <link href="https://github.com/jakegut/resume" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX template for my personal resume&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;resume&lt;/h1&gt; &#xA;&lt;p&gt;LaTeX template for my personal resume&lt;/p&gt; &#xA;&lt;p&gt;Based off of &lt;a href=&#34;https://github.com/sb2nov/resume/&#34;&gt;sb2nov/resume&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use it on overleaf: &lt;a href=&#34;https://www.overleaf.com/latex/templates/jakes-resume/syzfjbzwjncs&#34;&gt;Jake&#39;s Resume&lt;/a&gt; (Not updated)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jakegut/resume/master/resume.png&#34; alt=&#34;Resume Preview&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chaofengc/Awesome-Image-Quality-Assessment</title>
    <updated>2023-08-01T02:14:58Z</updated>
    <id>tag:github.com,2023-08-01:/chaofengc/Awesome-Image-Quality-Assessment</id>
    <link href="https://github.com/chaofengc/Awesome-Image-Quality-Assessment" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A comprehensive collection of IQA papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Image Quality Assessment (IQA)&lt;/h1&gt; &#xA;&lt;p&gt;A comprehensive collection of IQA papers, datasets and codes. We also provide PyTorch implementations of mainstream metrics in &lt;a href=&#34;https://github.com/chaofengc/IQA-PyTorch&#34;&gt;IQA-PyTorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chaofengc/IQA-PyTorch&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Toolbox-IQA--PyTorch-critical&#34; alt=&#34;toolbox&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyiqa/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pyiqa&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=chaofengc/Awesome-Image-Quality-Assessment&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#awesome-image-quality-assessment-iqa&#34;&gt;Awesome Image Quality Assessment (IQA)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#no-reference-nr&#34;&gt;No Reference (NR)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#full-reference-fr&#34;&gt;Full Reference (FR)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#image-aesthetic-assessment&#34;&gt;Image Aesthetic Assessment&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#others&#34;&gt;Others&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#datasets&#34;&gt;Datasets&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#iqa-datasets&#34;&gt;IQA datasets&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#perceptual-similarity-datasets&#34;&gt;Perceptual similarity datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Related Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bcmi/Awesome-Aesthetic-Evaluation-and-Cropping&#34;&gt;Awesome Image Aesthetic Assessment and Cropping&lt;/a&gt;. A curated list of resources including papers, datasets, and relevant links to aesthetic evaluation and cropping.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;No Reference (NR)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[ICCV2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.14735&#34;&gt;Test Time Adaptation for Blind Image Quality Assessment&lt;/a&gt;, Roy et al. &lt;a href=&#34;https://github.com/Shankhanil006/TTA-IQA&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L798-L804&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[CVPR2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.00451&#34;&gt;Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild&lt;/a&gt;, Saha et al. &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L791-L796&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[CVPR2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.14968&#34;&gt;Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective&lt;/a&gt;, Zhang et al. &lt;a href=&#34;https://github.com/zwx8981/LIQE&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L770-L775&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[CVPR2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.00521&#34;&gt;Quality-aware Pre-trained Models for Blind Image Quality Assessment&lt;/a&gt;, Zhao et al. &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L763-L768&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[AAAI2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.12396&#34;&gt;Exploring CLIP for Assessing the Look and Feel of Images&lt;/a&gt;, Wang et al. &lt;a href=&#34;https://github.com/IceClear/CLIP-IQA&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L745-L750&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[AAAI2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.04952&#34;&gt;Data-Efficient Image Quality Assessment with Attention-Panel Decoder&lt;/a&gt;, Qin et al. &lt;a href=&#34;https://github.com/narthchin/DEIQT&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L745-L750&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[TPAMI2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.09717&#34;&gt;Continual Learning for Blind Image Quality Assessment &lt;/a&gt;, Zhang et al. &lt;a href=&#34;https://github.com/zwx8981/BIQA_CL&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L738-L743&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[TIP2022]&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/9694502&#34;&gt;VCRNet: Visual Compensation Restoration Network for No-Reference Image Quality Assessment&lt;/a&gt;, Pan et al. &lt;a href=&#34;https://github.com/NUIST-Videocoding/VCRNet&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L752-L761&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[TMM2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.07666&#34;&gt;GraphIQA: Learning Distortion Graph Representations for Blind Image Quality Assessment&lt;/a&gt;, Sun et al. &lt;a href=&#34;https://github.com/geekyutao/GraphIQA&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L701-L707&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;[CVPR2021]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.06747&#34;&gt;Troubleshooting Blind Image Quality Models in the Wild&lt;/a&gt;, Wang et al. &lt;a href=&#34;https://github.com/wangzhihua520/troubleshooting_BIQA&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L716-L722&#34;&gt;Bibtex&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Link&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Keywords&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.08958&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MANIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPRW2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IIGROUP/MANIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer, multi-dimension attention, dual branch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06858&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TReS&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;WACV2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/isalirezag/TReS&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer, relative ranking, self-consistency&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bmvc2021-virtualconference.com/assets/papers/0868.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KonIQ++&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;BMVC2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SSL92/koniqplusplus&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-task with distortion prediction&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.05997&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MUSIQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ICCV2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/musiq&#34;&gt;Official&lt;/a&gt; / &lt;a href=&#34;https://github.com/anse3832/MUSIQ&#34;&gt;Pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-scale, transformer, Aspect Ratio Preserved (ARP) resizing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.07948&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CKDN&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ICCV2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/researchmm/CKDN&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Degraded reference, Conditional knowledge distillation (related to HIQA)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HyperIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SSL92/hyperIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-aware hyper network&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.05508&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta-IQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhuhancheng/MetaIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta-learning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.08932&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ECCV2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cientgu/GIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generated image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1809.07517&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PI&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;2018 PIRM Challenge&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/roimehrez/PIRM2018&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1/2 * (NIQE + (10 - NRQM)).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.01681&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://kwanyeelin.github.io/projects/HIQA/HIQA.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Hallucinated reference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1805.08493v1.pdf&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BPSQM&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pixel-wise quality map&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.08347&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RankIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ICCV2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xialeiliu/RankIQA&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pretrain on synthetically ranked data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Convolutional_Neural_Networks_2014_CVPR_paper.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CNNIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/CNNIQA&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;First CNN-based NR-IQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.13983&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;UNIQUE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zwx8981/UNIQUE&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Combine synthetic and authentic image pairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.02665.pdf&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DBCNN&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TCSVT2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zwx8981/DBCNN-PyTorch&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Two branches for synthetic and authentic distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.jdl.link/doc/2011/20191226_08489929.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SFA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TMM2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/SFA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Aggregate ResNet50 features of multiple cropped patches&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tMjcllKP8SzTn-dWVmogxaCLpzL1L7nO/view&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;https://arxiv.org/abs/1708.08190&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PQR&lt;/td&gt; &#xA;   &lt;td&gt;NR/Aesthetic&lt;/td&gt; &#xA;   &lt;td&gt;TIP2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HuiZeng/Unified_IAA&#34;&gt;Official1&lt;/a&gt;/&lt;a href=&#34;https://github.com/HuiZeng/BIQA_Toolbox&#34;&gt;Official2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Unify different type of aesthetic labels&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.01697&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaDIQaM (deepIQA)&lt;/td&gt; &#xA;   &lt;td&gt;NR/FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/WaDIQaM&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Weighted average of patch qualities, shared FR/NR models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/ielx7/83/8347140/08352823.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NIMA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kentsyx/Neural-IMage-Assessment&#34;&gt;PyTorch&lt;/a&gt;/&lt;a href=&#34;https://github.com/idealo/image-quality-assessment&#34;&gt;Tensorflow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Squared EMD loss&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/publications/TIP_E2E_BIQA.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MEON&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-task: distortion learning and quality prediction&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.06505&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;dipIQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~k29ma/codes/dipIQ.rar&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Similar to RankIQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.05890&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NRQM (Ma)&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVIU2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/chaoma99/sr-metric&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional, Super resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.04757&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FRIQUEE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;JoV2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/FRIQUEE&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Authentically Distorted, Bag of Features&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7501619&#34;&gt;IEEE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HOSA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7501619&#34;&gt;Matlab download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2015/zhang2015feature.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ILNIQE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www4.comp.polyu.edu.hk/~cslzhang/IQA/ILNIQE/ILNIQE.htm&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2012/TIP%20BRISQUE.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BRISQUE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/BRISQUE&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2012/saad_2012_tip.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BLIINDS-II&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/BLIINDS2&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.359.7510&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CORNIA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HuiZeng/BIQA_Toolbox&#34;&gt;Matlab download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Codebook Representation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2013/mittal2013.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NIQE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;SPL2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/niqe&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.imaging.utk.edu/research/wcho/references/2011%20TIP%20BLINDS2.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DIIVINE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/DIIVINE&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | []() | | NR | | []() |  --&gt; &#xA;&lt;h3&gt;Full Reference (FR)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[ECCV2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.052152207.13686&#34;&gt;Shift-tolerant Perceptual Similarity Metric&lt;/a&gt;, Ghildyal et al. &lt;a href=&#34;https://github.com/abhijay9/ShiftTolerant-LPIPS&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L731-L736&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[BMVC2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.05215&#34;&gt;Content-Diverse Comparisons improve IQA&lt;/a&gt;, Thong et al. &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L724-L729&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[ACM MM2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.08689&#34;&gt;Quality Assessment of Image Super-Resolution: Balancing Deterministic and Statistical Fidelity&lt;/a&gt;, Zhou et al. &lt;a href=&#34;https://github.com/weizhou-geek/SRIF&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L709-L714&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Link&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Keywords&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.10485&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AHIQ&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2022 NTIRE workshop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IIGROUP/AHIQ&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Attention, Transformer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.08763&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JSPL&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/happycaoyue/JSPL&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;semi-supervised and positive-unlabeled (PU) learning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.13123&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CVRKD&lt;/td&gt; &#xA;   &lt;td&gt;NAR&lt;/td&gt; &#xA;   &lt;td&gt;AAAI2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/guanghaoyin/CVRKD-IQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Non-Aligned content reference, knowledge distillation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.14730&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IQT&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPRW2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/anse3832/IQT&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.08521&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A-DISTS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;ACMM2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dingkeyan93/A-DISTS&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.07728&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DISTS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TPAMI2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dingkeyan93/DISTS&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.03924&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LPIPS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://richzhang.github.io/PerceptualSimilarity/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Perceptual similarity, Pairwise Preference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.02067&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PieAPP&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://civc.ucsb.edu/graphics/Papers/CVPR2018_PieAPP/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Perceptual similarity, Pairwise Preference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.01697&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaDIQaM&lt;/td&gt; &#xA;   &lt;td&gt;NR/FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/WaDIQaM&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.05316&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JND-SalCAR&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TCSVT2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JND (Just-Noticeable-Difference)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nottingham-repository.worktribe.com/preview/1589753/Visual%20IEEE-TIP-2019.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;QADS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.vista.ac.cn/super-resolution/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sse.tongji.edu.cn/linzhang/IQA/FSIM/Files/Fsim%20a%20feature%20similarity%20index%20for%20image%20quality%20assessment.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FSIM&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sse.tongji.edu.cn/linzhang/IQA/FSIM/FSIM.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2004/hrs_ieeetip_2004_imginfo.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;VIF/IFC&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2006&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/Quality/VIF.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/publications/msssim.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MS-SSIM&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/research/ssim/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SSIM&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2004&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/research/ssim/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PSNR&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | []() | | FR | | []() |  --&gt; &#xA;&lt;h3&gt;Image Aesthetic Assessment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[CVPR2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.14302&#34;&gt;VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining&lt;/a&gt;, Ke et al. &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L784-L789&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[CVPR2023]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.14968&#34;&gt;Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method&lt;/a&gt;, Yi et al. &lt;a href=&#34;https://github.com/Dreemurr-T/BAID&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L777-L782&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Keywords&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.03889&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NiNLoss&lt;/td&gt; &#xA;   &lt;td&gt;ACMM2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/LinearityIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Norm-in-Norm Loss&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;h3&gt;IQA datasets&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Link&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Images&lt;/th&gt; &#xA;   &lt;th&gt;Annotations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.10088&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PaQ-2-PiQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baidut/PaQ-2-PiQ&#34;&gt;Official github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;40k, 120k patches&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_Perceptual_Quality_Assessment_of_Smartphone_Photography_CVPR_2020_paper.html&#34;&gt;CVF&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SPAQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/h4nwei/SPAQ&#34;&gt;Offical github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11k (smartphone)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.06180&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KonIQ-10k&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://database.mmsp-kn.de/koniq-10k-database.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;10k from &lt;a href=&#34;http://projects.dfki.uni-kl.de/yfcc100m/&#34;&gt;YFCC100M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.2M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.01621v2&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AADB&lt;/td&gt; &#xA;   &lt;td&gt;NR/Aesthentic&lt;/td&gt; &#xA;   &lt;td&gt;ECCV2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/aimerykong/deepImageAestheticsAnalysis&#34;&gt;Official github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;10k images (8500/500/1000), 11 attributes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.02919&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CLIVE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/ChallengeDB/index.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1200&lt;/td&gt; &#xA;   &lt;td&gt;350k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://refbase.cvc.uab.es/files/MMP2012a.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AVA&lt;/td&gt; &#xA;   &lt;td&gt;NR / Aesthentic&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mtobeiyf/ava_downloader&#34;&gt;Github&lt;/a&gt;/&lt;a href=&#34;http://www.lucamarchesotti.com/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;250k (60 categories)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.12142&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PIPAL&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;ECCV2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.jasongt.com/projectpages/pipal.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;250&lt;/td&gt; &#xA;   &lt;td&gt;1.13M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2001.08113&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KADIS-700k&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;arXiv&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://database.mmsp-kn.de/kadid-10k-database.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;140k pristine / 700k distorted&lt;/td&gt; &#xA;   &lt;td&gt;30 ratings (DCRs) per image.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8743252&#34;&gt;IEEE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KADID-10k&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;QoMEX2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://database.mmsp-kn.de/kadid-10k-database.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;81&lt;/td&gt; &#xA;   &lt;td&gt;10k distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~k29ma/papers/17_TIP_EXPLORATION.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Waterloo-Exp&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~k29ma/exploration/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4744&lt;/td&gt; &#xA;   &lt;td&gt;94k distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://daneshyari.com/article/preview/533080.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MDID&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;PR2017&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;1600 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/papers/euvip_tid2013.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TID2013&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;SP2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/tid2013.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;3000 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.298.9133&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LIVEMD&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;ACSSC2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/Quality/live_multidistortedimage.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15 pristine images&lt;/td&gt; &#xA;   &lt;td&gt;two successive distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Damon-Chandler/publication/220050520_Most_apparent_distortion_Full-reference_image_quality_assessment_and_the_role_of_strategy/links/5629cd1c08ae518e347e1445/Most-apparent-distortion-Full-reference-image-quality-assessment-and-the-role-of-strategy.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CSIQ&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;Journal of Electronic Imaging 2010&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;866 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/papers/mre2009tid.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TID2008&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;2009&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/tid2008.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;1700 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2006/hrs-transIP-06.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LIVE IQA&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2006&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/Quality/subjective.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;29 images, 780 synthetic distortions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://hal.univ-nantes.fr/hal-00580755/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IVC&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;2005&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;185 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Perceptual similarity datasets&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Title&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Images&lt;/th&gt; &#xA;   &lt;th&gt;Annotations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.03924&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BAPPS(LPIPS)&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://richzhang.github.io/PerceptualSimilarity/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;187.7k&lt;/td&gt; &#xA;   &lt;td&gt;484k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.02067&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PieAPP&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://civc.ucsb.edu/graphics/Papers/CVPR2018_PieAPP/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200 images&lt;/td&gt; &#xA;   &lt;td&gt;2.3M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>