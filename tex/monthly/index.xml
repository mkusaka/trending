<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-01T02:23:58Z</updated>
  <subtitle>Monthly Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tuna/thuthesis</title>
    <updated>2023-06-01T02:23:58Z</updated>
    <id>tag:github.com,2023-06-01:/tuna/thuthesis</id>
    <link href="https://github.com/tuna/thuthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX Thesis Template for Tsinghua University&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/actions&#34;&gt;&lt;img src=&#34;https://github.com/tuna/thuthesis/workflows/Test/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tuna/thuthesis/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/tuna/thuthesis/total&#34; alt=&#34;GitHub downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tuna/thuthesis/commits/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commits-since/tuna/thuthesis/latest&#34; alt=&#34;GitHub commits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tuna/thuthesis/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/tuna/thuthesis&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;&lt;img src=&#34;https://img.shields.io/ctan/v/thuthesis&#34; alt=&#34;CTAN&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ThuThesis&lt;/h1&gt; &#xA;&lt;p&gt;Scroll down for the English version of README.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThuThesis&lt;/strong&gt; 是 &lt;strong&gt;T&lt;/strong&gt;sing&lt;strong&gt;h&lt;/strong&gt;ua &lt;strong&gt;U&lt;/strong&gt;niversity &lt;strong&gt;Thesis&lt;/strong&gt; LaTeX Template 的缩写。&lt;/p&gt; &#xA;&lt;p&gt;此宏包旨在建立一个简单易用的清华大学学位论文 LaTeX 模板，包括本科综合论文训练、硕士论文、博士论文以及博士后出站报告。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;由于模板升级频繁，在开始使用和提问前，请确保您已经认真完整地阅读了使用说明文档和示例代码。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;任何违反 &lt;a href=&#34;https://www.latex-project.org/lppl/lppl-1-3c/&#34;&gt;LaTeX项目公共许可证 v1.3c&lt;/a&gt; 使用 ThuThesis 的行为将被记录在 &lt;a href=&#34;https://github.com/tuna/thuthesis/issues/754&#34;&gt;耻辱柱&lt;/a&gt; 页面中，以示警告。&lt;/p&gt; &#xA;&lt;h2&gt;下载&lt;/h2&gt; &#xA;&lt;p&gt;推荐下载&lt;strong&gt;发布版&lt;/strong&gt;模板，里面包括具体使用说明以及示例文档：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;模板使用说明（thuthesis.pdf）&lt;/li&gt; &#xA; &lt;li&gt;示例文档（thuthesis-example.pdf）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;开发版中不提供预生成的 &lt;code&gt;cls&lt;/code&gt; 文件和文档，仅包含源码。其仅供开发者与需要尚未发布的功能的有经验的 TeX 用户使用，不提供任何保证。&lt;/p&gt; &#xA;&lt;p&gt;下载途径：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;发布版： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;仅下载： &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt;：可能滞后正式发布少许时间。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/releases&#34;&gt;GitHub Releases&lt;/a&gt;：最新版的及时发布途径。&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/github-release/tuna/thuthesis/&#34;&gt;TUNA 镜像站&lt;/a&gt;：GitHub Releases 的镜像。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;在线编辑： &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.texpage.com/template/72b580ca-51fa-4ecc-82b3-0509bc1d6a07&#34;&gt;TeXPage 模板&lt;/a&gt;（提供 Windows 中文字体）&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/thuthesis-tsinghua-university-thesis-latex-template/wddqnwbyhtnk&#34;&gt;Overleaf 模板&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;开发版：&lt;a href=&#34;https://github.com/tuna/thuthesis&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;任何在其他途径分发的 ThuThesis（包含其变体或衍生物）均不是官方版本，请谨慎使用。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;p&gt;每个版本的详细更新日志，请见 &lt;a href=&#34;https://raw.githubusercontent.com/tuna/thuthesis/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt;。使用文档中也包含了这一内容。&lt;/p&gt; &#xA;&lt;h2&gt;升级&lt;/h2&gt; &#xA;&lt;h3&gt;自动更新&lt;/h3&gt; &#xA;&lt;p&gt;通过 TeX 发行版工具（如 &lt;code&gt;tlmgr&lt;/code&gt;）自动从 &lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt; 更新。&lt;/p&gt; &#xA;&lt;h3&gt;手动更新&lt;/h3&gt; &#xA;&lt;h4&gt;发布版&lt;/h4&gt; &#xA;&lt;p&gt;下载发布版的的 zip 包，使用其中的 &lt;code&gt;thuthesis.cls&lt;/code&gt; 等文件覆盖原有的即可，无须额外操作。&lt;/p&gt; &#xA;&lt;h4&gt;开发版&lt;/h4&gt; &#xA;&lt;p&gt;从 GitHub clone 项目源码或者下载源码 zip 包，执行命令（Windows 用户在文件夹空白处按 &lt;code&gt;Shift + 鼠标右键&lt;/code&gt;，点击“在此处打开命令行窗口”）：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xetex thuthesis.ins&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;即可得到 &lt;code&gt;thuthesis.cls&lt;/code&gt; 等模板文件。&lt;/p&gt; &#xA;&lt;h2&gt;提问&lt;/h2&gt; &#xA;&lt;p&gt;按推荐顺序排序：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;先到 &lt;a href=&#34;https://github.com/tuna/thuthesis/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; 看看常见问题&lt;/li&gt; &#xA; &lt;li&gt;在 &lt;a href=&#34;https://github.com/tuna/thuthesis/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; 搜索已有讨论，如果没有则提出新问题&lt;/li&gt; &#xA; &lt;li&gt;如果认为模板存在问题，可在 Issues 中提出&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Makefile的用法&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make [{all|thesis|spine|doc|clean|cleanall|distclean}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;目标&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt; 生成论文 thuthesis-example.pdf；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make spine&lt;/code&gt; 生成书脊 spine.pdf；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make doc&lt;/code&gt; 生成模板使用说明书 thuthesis.pdf；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make all&lt;/code&gt; 生成论文和书脊，相当于 &lt;code&gt;make thesis &amp;amp;&amp;amp; make spine&lt;/code&gt;；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; 删除示例文件的中间文件（不含 thuthesis-example.pdf）；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt; 删除示例文件的中间文件和 thuthesis-example.pdf；&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; 删除示例文件和模板的所有中间文件和 PDF。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ThuThesis&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThuThesis&lt;/strong&gt; is an abbreviation of &lt;strong&gt;T&lt;/strong&gt;sing&lt;strong&gt;h&lt;/strong&gt;ua &lt;strong&gt;U&lt;/strong&gt;niversity &lt;strong&gt;Thesis&lt;/strong&gt; LaTeX Template.&lt;/p&gt; &#xA;&lt;p&gt;This package establishes a simple and easy-to-use LaTeX template for Tsinghua dissertations, including general undergraduate research papers, masters theses, doctoral dissertations, and postdoctoral reports. An English translation of this README follows the Chinese below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This template is subject to frequent changes. Please make sure you have read the usage documentation and example code completely and carefully before using and asking questions.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Any use of ThuThesis in violation of &lt;a href=&#34;https://www.latex-project.org/lppl/lppl-1-3c/&#34;&gt;The LaTeX project public license v1.3c&lt;/a&gt; will be recorded in the &lt;a href=&#34;https://github.com/tuna/thuthesis/issues/754&#34;&gt;Hall of Shame&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloads&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Published versions&lt;/strong&gt; are recommended. Specific usage documentation and examples can be found in the archive. At present, these documents are &lt;b&gt;only available in Chinese&lt;/b&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Template usage documentation (thuthesis.pdf)&lt;/li&gt; &#xA; &lt;li&gt;Template example (thuthesis-example.pdf)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Developer versions contain only source code but no pre-compiled &lt;code&gt;cls&lt;/code&gt; file and documentations. They are only for the usage of developers and experienced TeX users in need of unpublished features. No warranties are provided.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Published versions: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Download only: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/tuna/thuthesis/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/github-release/tuna/thuthesis/&#34;&gt;TUNA Mirrors&lt;/a&gt;: mirror of GitHub Releases&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Online editor: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.texpage.com/template/72b580ca-51fa-4ecc-82b3-0509bc1d6a07&#34;&gt;TeXPage template&lt;/a&gt; (providing Chinese fonts of Windows)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.overleaf.com/latex/templates/thuthesis-tsinghua-university-thesis-latex-template/wddqnwbyhtnk&#34;&gt;Overleaf template&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Developer versions: &lt;a href=&#34;https://github.com/tuna/thuthesis&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThuThesis (including its variants / derivatives) distributed in any other way is NOT an official version. Use at your own risk.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/tuna/thuthesis/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for detailed changes in each release. They are also included in the usage documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;h3&gt;Automatic&lt;/h3&gt; &#xA;&lt;p&gt;Get the most up-to-date published version with your TeX distribution from &lt;a href=&#34;https://www.ctan.org/pkg/thuthesis&#34;&gt;CTAN&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Manual&lt;/h3&gt; &#xA;&lt;h4&gt;Published versions&lt;/h4&gt; &#xA;&lt;p&gt;Download the published zip files, extract &lt;code&gt;thuthesis.cls&lt;/code&gt; and other files (if needed) and override the existing ones in your thesis.&lt;/p&gt; &#xA;&lt;h4&gt;Developer versions&lt;/h4&gt; &#xA;&lt;p&gt;Download the source code package and unzip to the root directory of your thesis (or clone this project), then execute the command (Windows users &lt;code&gt;Shift + right click&lt;/code&gt; white area in the file window and click &#34;Open command line window here&#34; from the popup menu):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;xetex thuthesis.ins&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll get &lt;code&gt;thuthesis.cls&lt;/code&gt; along with other template files.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the procedure below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the &lt;a href=&#34;https://github.com/tuna/thuthesis/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Search &lt;a href=&#34;https://github.com/tuna/thuthesis/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; and create if not existed&lt;/li&gt; &#xA; &lt;li&gt;Create an issue if you believe there is a bug&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Makefile Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make [{all|thesis|spine|doc|clean|cleanall|distclean}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Targets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt; generate thesis thuthesis-example.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make spine&lt;/code&gt; generate book spine for printing spine.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make doc&lt;/code&gt; generate template documentation thuthesis.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make all&lt;/code&gt; generate thesis and spine, same as &lt;code&gt;make thesis &amp;amp;&amp;amp; make spine&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; delete all examples&#39; files (excluding thuthesis-example.pdf);&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt; delete all examples&#39; files and thuthesis-example.pdf;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; delete all examples&#39; and templates&#39; files and PDFs.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>harryzhangOG/Deep-RL-Notes</title>
    <updated>2023-06-01T02:23:58Z</updated>
    <id>tag:github.com,2023-06-01:/harryzhangOG/Deep-RL-Notes</id>
    <link href="https://github.com/harryzhangOG/Deep-RL-Notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of comprehensive notes on Deep Reinforcement Learning, customized for UC Berkeley&#39;s CS 285 (prev. CS 294-112)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Reinforcement Learning Textbook&lt;/h1&gt; &#xA;&lt;h2&gt;A collection of comprehensive notes on Deep Reinforcement Learning, based on UC Berkeley&#39;s CS 285 (prev. CS 294-112) taught by Professor Sergey Levine.&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compile the latex source code into PDF locally.&lt;/li&gt; &#xA; &lt;li&gt;Alternatively, you could download this repo as a zip file and upload the zip file to Overleaf and start editing online.&lt;/li&gt; &#xA; &lt;li&gt;This repo is linked to my Overleaf editor so it is regularly updated.m&lt;/li&gt; &#xA; &lt;li&gt;Please let me know if you have any questions or suggestions. Reach me via &lt;a href=&#34;mailto:harryhzhang@berkeley.edu&#34;&gt;harryhzhang@berkeley.edu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;In recent years, deep reinforcement learning (DRL) has emerged as a transformative paradigm, bridging the domains of artificial intelligence, machine learning, and robotics to enable the creation of intelligent, adaptive, and autonomous systems. This textbook is designed to provide a comprehensive, in-depth introduction to the principles, techniques, and applications of deep reinforcement learning, empowering students, researchers, and practitioners to advance the state of the art in this rapidly evolving field. As the first DRL class I have taken was Prof. Levine&#39;s CS 294-112, this book&#39;s organization and materials are based strongly on the CS 294-112 (now CS 285)&#39;s slides and syllabus.&lt;/p&gt; &#xA;&lt;p&gt;The primary objective of this textbook is to offer a systematic and rigorous treatment of DRL, from foundational concepts and mathematical formulations to cutting-edge algorithms and practical implementations. We strive to strike a balance between theoretical clarity and practical relevance, providing readers with the knowledge and tools needed to develop novel DRL solutions for a wide array of real-world problems.&lt;/p&gt; &#xA;&lt;p&gt;The textbook is organized into several parts, each dedicated to a specific aspect of DRL:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fundamentals: This part covers the essential background material in reinforcement learning, including Markov decision processes, value functions, and fundamental algorithms such as Q-learning and policy gradients.&lt;/li&gt; &#xA; &lt;li&gt;Deep Learning for Reinforcement Learning: Here, we delve into the integration of deep learning techniques with reinforcement learning, discussing topics such as function approximation, representation learning, and the use of deep neural networks as function approximators.&lt;/li&gt; &#xA; &lt;li&gt;Advanced Techniques and Algorithms: This part presents state-of-the-art DRL algorithms, such as Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC), along with their theoretical underpinnings and practical considerations.&lt;/li&gt; &#xA; &lt;li&gt;Exploration and Exploitation: We explore strategies for balancing exploration and exploitation in DRL, examining methods such as intrinsic motivation, curiosity-driven learning, and Bayesian optimization.&lt;/li&gt; &#xA; &lt;li&gt;Real-World Applications: This section showcases the application of DRL to various domains, including robotics, computer vision, natural language processing, and healthcare, highlighting the challenges and opportunities in each area. Throughout the textbook, we supplement the theoretical exposition with practical examples, case studies, and programming exercises, allowing readers to gain hands-on experience in implementing DRL algorithms and applying them to diverse problems. We also provide references to relevant literature, guiding the reader towards further resources for deepening their understanding and pursuing advanced topics.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We envision this textbook as a valuable resource for students, researchers, and practitioners seeking a solid grounding in deep reinforcement learning, as well as a springboard for future innovation and discovery in this exciting and dynamic field. It is our hope that this work will contribute to the ongoing growth and development of DRL, facilitating the creation of intelligent systems that can learn, adapt, and thrive in complex, ever-changing environments.&lt;/p&gt; &#xA;&lt;p&gt;We extend our deepest gratitude to our colleagues, reviewers, and students, whose invaluable feedback and insights have helped shape this textbook. We also wish to acknowledge the pioneering researchers whose contributions have laid the foundation for DRL and inspired us to embark on this journey.&lt;/p&gt; &#xA;&lt;h2&gt;Update Log&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Aug 26, 2020: Started adding Fall 2020 materials&lt;/li&gt; &#xA; &lt;li&gt;Aug 28, 2020: Fixed typos in Intro. Credit: Warren Deng.&lt;/li&gt; &#xA; &lt;li&gt;Aug 30, 2020: Added more explanation to the imitation learning chapter.&lt;/li&gt; &#xA; &lt;li&gt;Sep 13, 2020: Added advanced PG in PG and fixed typos in PG.&lt;/li&gt; &#xA; &lt;li&gt;Sep 14, 2020: AC chapter format, typos fix, more analysis on A2C&lt;/li&gt; &#xA; &lt;li&gt;Sep 16, 2020: Chapter 10.1 KL div typo fix. Credit: Cong Wang.&lt;/li&gt; &#xA; &lt;li&gt;Sep 19, 2020: Chapter 3.7.1 parathesis typo fix. Credit: Yunkai Zhang.&lt;/li&gt; &#xA; &lt;li&gt;Sep 23, 2020: Q learning chapter fix&lt;/li&gt; &#xA; &lt;li&gt;Sep 26, 2020: More explanation and fix to the advanced PG chapter (specifically intuition behind TRPO).&lt;/li&gt; &#xA; &lt;li&gt;Sep 28, 2020: Typo fixed and more explanation in Optimal Control. Typos were pointed out in Professor Levine&#39;s lecture.&lt;/li&gt; &#xA; &lt;li&gt;Oct 6, 2021: Model-based RL chapter fixed. Added Distillation subsection.&lt;/li&gt; &#xA; &lt;li&gt;Nov. 20, 2021: Fixed typos in DDPG, Online Actor Critic, and PG theory. Credit: Javier Leguina.&lt;/li&gt; &#xA; &lt;li&gt;Apr. 2, 2023: Fixed typos in VAE and PG theory. Credit: wangcongrobot&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>awesome-NeRF/awesome-NeRF</title>
    <updated>2023-06-01T02:23:58Z</updated>
    <id>tag:github.com,2023-06-01:/awesome-NeRF/awesome-NeRF</id>
    <link href="https://github.com/awesome-NeRF/awesome-NeRF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A curated list of awesome neural radiance fields papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Neural Radiance Fields &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A curated list of awesome neural radiance fields papers, inspired by &lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision&#34;&gt;awesome-computer-vision&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/how-to-PR.md&#34;&gt;How to submit a pull request?&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/awesome-NeRF/awesome-NeRF/issues/108#issuecomment-1350732470&#34;&gt;Want to help maintain the list?&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/#papers&#34;&gt;Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/#talks&#34;&gt;Talks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/#implementations&#34;&gt;Implementations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.05204&#34;&gt;Neural Volume Rendering: NeRF And Beyond&lt;/a&gt;, Dellaert and Yen-Chen, Arxiv 2020 | &lt;a href=&#34;https://dellaert.github.io/NeRF/&#34;&gt;blog&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yenchenlin/awesome-NeRF/main/NeRF-and-Beyond.bib&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/nerf-survey.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.00379&#34;&gt;NeRF: Neural Radiance Field in 3D Vision, Introduction and Review&lt;/a&gt;, Kyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu, Jonathan Li&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.matthewtancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;, Mildenhall et al., ECCV 2020 | &lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L168-L173&#34;&gt;bibtex&lt;/a&gt; &#xA;  &lt;!--Mildenhall20eccv_nerf--&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Faster Inference&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://cseweb.ucsd.edu/~viscomp/projects/NeuralTransmittance/index.html&#34;&gt;Learning Neural Transmittance for Efficient Rendering of Reflectance Fields&lt;/a&gt;, Mohammad Shafiei et al., BMVC 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/NeuralTransmittance.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://lingjie0206.github.io/papers/NSVF/&#34;&gt;Neural Sparse Voxel Fields&lt;/a&gt;, Liu et al., NeurIPS 2020 | &lt;a href=&#34;https://github.com/facebookresearch/NSVF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L135-L141&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Liu20neurips_sparse_nerf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://www.computationalimaging.org/publications/automatic-integration/&#34;&gt;AutoInt: Automatic Integration for Fast Neural Volume Rendering&lt;/a&gt;, Lindell et al., CVPR 2021 | &lt;a href=&#34;https://github.com/computational-imaging/automatic-integration&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L127-L133&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Lindell20arxiv_AutoInt--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.12490&#34;&gt;DeRF: Decomposed Radiance Fields&lt;/a&gt;, Rebain et al. Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L222-L228&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Rebain20arxiv_derf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://depthoraclenerf.github.io/&#34;&gt;DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks&lt;/a&gt;, Neff et al., CGF 2021 | &lt;a href=&#34;https://github.com/facebookresearch/DONERF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/donerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--neff2021donerf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.10380&#34;&gt;FastNeRF: High-Fidelity Neural Rendering at 200FPS&lt;/a&gt;, Garbin et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/fastnerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Garbin21arxiv_FastNeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.13744&#34;&gt;KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs &lt;/a&gt;, Reiser et al., ICCV 2021 | &lt;a href=&#34;https://github.com/creiser/kilonerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/kilonerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--reiser2021kilonerf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://alexyu.net/plenoctrees/&#34;&gt;PlenOctrees for Real-time Rendering of Neural Radiance Fields&lt;/a&gt;, Yu et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/sxyu/volrend&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/plenoctrees.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--yu2021plenoctrees--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.01954&#34;&gt;Mixture of Volumetric Primitives for Efficient Neural Rendering&lt;/a&gt;, Lombardi et al., SIGGRAPH 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/mixture.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://vsitzmann.github.io/lfns/&#34;&gt;Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering&lt;/a&gt;, Sitzmann et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/vsitzmann/light-field-networks&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/lfn.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.01120&#34;&gt;RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive AR/VR Rendering&lt;/a&gt;, Li et al., ICCAD 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/rt-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/enerf/&#34;&gt;ENeRF: Efficient Neural Radiance Fields for Interactive Free-viewpoint Video&lt;/a&gt;, Lin et al., SIGGRAPH 2022 | &lt;a href=&#34;https://github.com/zju3dv/ENeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/enerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.17261&#34;&gt;R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis&lt;/a&gt;, Wang et al., ECCV 2022 | &lt;a href=&#34;https://github.com/snap-research/R2L&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/r2l.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--wang2022r2l--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08057&#34;&gt;Real-Time Neural Light Field on Mobile Devices&lt;/a&gt;, Cao et al., Arxiv 2022 | &lt;a href=&#34;https://github.com/snap-research/MobileR2L&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/r2l-mobile.txt&#34;&gt;bibtext&lt;/a&gt; &#xA;   &lt;!--cao2022mobiler2l--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.05735.pdf&#34;&gt;Hardware Acceleration of Neural Graphics&lt;/a&gt;, Mubarik et al., ISCA 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/hw_accelaration.txt&#34;&gt;bibtext&lt;/a&gt; &#xA;   &lt;!--mubarik2023hardware--&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Faster Training&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2107.02791.pdf&#34;&gt;Depth-supervised NeRF: Fewer Views and Faster Training for Free&lt;/a&gt;, Deng et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/dunbar12138/DSNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/dsnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11215.pdf&#34;&gt;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction&lt;/a&gt;, Sun et al., CVPR 2022 | &lt;a href=&#34;https://github.com/sunset1995/DirectVoxGO&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/DirectVoxGO.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--sun2021direct--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;Instant Neural Graphics Primitives with a Multiresolution Hash Encoding&lt;/a&gt;, Müller et al., SIGGRAPH 2022 | &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/instant-ngp.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://alexyu.net/plenoxels/&#34;&gt;Plenoxels Radiance Fields without Neural Networks&lt;/a&gt;, Yu et al., CVPR 2022 | &lt;a href=&#34;https://github.com/sxyu/svox2&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/plenoxels.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://apchenstu.github.io/TensoRF/&#34;&gt;TensoRF: Tensorial Radiance Fields&lt;/a&gt;, Chen et al., ECCV 2022 | &lt;a href=&#34;https://github.com/apchenstu/TensoRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/tensorf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://bakedsdf.github.io/&#34;&gt;BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis&lt;/a&gt;, Yariv et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/bakedsdf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Compression&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/vqad/&#34;&gt;Variable Bitrate Neural Fields&lt;/a&gt;, Takikawa et al., SIGGRAPH 2022 | &lt;a href=&#34;https://github.com/nv-tlabs/vqad&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/Variable-bitrate-neural-fields.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Unconstrained Images&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://nerf-w.github.io/&#34;&gt;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections&lt;/a&gt;, Martin-Brualla et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L152-L158&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--MartinBrualla20arxiv_nerfw--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://rover-xingyu.github.io/Ha-NeRF/&#34;&gt;Ha-NeRF&lt;span&gt;😆&lt;/span&gt;: Hallucinated Neural Radiance Fields in the Wild&lt;/a&gt;, Chen et al., CVPR 2022 | &lt;a href=&#34;https://github.com/rover-xingyu/Ha-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/Ha-NeRF.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--chen2021hallucinated--&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Deformable&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://nerfies.github.io/&#34;&gt;Deformable Neural Radiance Fields&lt;/a&gt;, Park et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/google/nerfies&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L206-L212&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Park20arxiv_nerfies--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.albertpumarola.com/research/D-NeRF/index.html&#34;&gt;D-NeRF: Neural Radiance Fields for Dynamic Scenes&lt;/a&gt;, Pumarola et al., CVPR 2021 | &lt;a href=&#34;https://github.com/albertpumarola/D-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L214-L220&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Pumarola20arxiv_D_NeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://gafniguy.github.io/4D-Facial-Avatars/&#34;&gt;Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction&lt;/a&gt;, Gafni et al., CVPR 2021 | &lt;a href=&#34;https://github.com/gafniguy/4D-Facial-Avatars&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L87-L93&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Gafni20arxiv_DNRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/&#34;&gt;Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Deforming Scene from Monocular Video&lt;/a&gt;, Tretschk et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/facebookresearch/nonrigid_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L283-L289&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Tretschk20arxiv_NR-NeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://volumetric-avatars.github.io/&#34;&gt;PVA: Pixel-aligned Volumetric Avatars&lt;/a&gt;, Raj et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/pva.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/nogu-atsu/NARF&#34;&gt;Neural Articulated Radiance Field&lt;/a&gt;, Noguchi et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/nogu-atsu/NARF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/narf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.00181&#34;&gt;CLA-NeRF: Category-Level Articulated Neural Radiance Field&lt;/a&gt;, Tseng et al., ICRA 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/cla-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/animatable_nerf/&#34;&gt;Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies&lt;/a&gt;, Peng et al., ICCV 2021 | &lt;a href=&#34;https://github.com/zju3dv/animatable_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/animatable_nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Peng21arxiv_animatable_nerf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://hypernerf.github.io/&#34;&gt;A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields&lt;/a&gt;, Park et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/google/hypernerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/hypernerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13629&#34;&gt;Animatable Neural Radiance Fields from Monocular RGB Videos&lt;/a&gt;, Chen et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/JanaldoChen/Anim-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/anim_nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/NeuralActor/&#34;&gt;Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control&lt;/a&gt;, Liu et al., SIGGRAPH Asia 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/neuralactor.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jaminfong.cn/tineuvox/&#34;&gt;TiNeuVox: Fast Dynamic Radiance Fields with Time-Aware Neural Voxels&lt;/a&gt;, Fang et al., SIGGRAPH Asia 2022 | &lt;a href=&#34;https://github.com/hustvl/TiNeuVox&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/TiNeuVox.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://grail.cs.washington.edu/projects/humannerf/&#34;&gt;HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video&lt;/a&gt;, Weng et al., CVPR 2022 | &lt;a href=&#34;https://github.com/chungyiweng/humannerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/humannerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://yifanjiang19.github.io/alignerf&#34;&gt;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training&lt;/a&gt;, Jiang et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/AligNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Video&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~zl548/NSFF/&#34;&gt;Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes&lt;/a&gt;, Li et al., CVPR 2021 | &lt;a href=&#34;https://github.com/zhengqili/Neural-Scene-Flow-Fields&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L119-L125&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Li20arxiv_nsff--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://video-nerf.github.io/&#34;&gt;Space-time Neural Irradiance Fields for Free-Viewpoint Video&lt;/a&gt;, Xian et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L299-L305&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Xian20arxiv_stnif--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://yilundu.github.io/nerflow/&#34;&gt;Neural Radiance Flow for 4D View Synthesis and Video Processing&lt;/a&gt;, Du et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L79-L85&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Du20arxiv_nerflow--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/neuralbody/&#34;&gt;Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans&lt;/a&gt;, Peng et al., CVPR 2021 | &lt;a href=&#34;https://github.com/zju3dv/neuralbody&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/neuralbody.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Peng20arxiv_neuralbody--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://fanegg.github.io/UV-Volumes/&#34;&gt;UV Volumes for Real-time Rendering of Editable Free-view Human Performance&lt;/a&gt;, Chen et al., CVPR 2023 | &lt;a href=&#34;https://github.com/fanegg/UV-Volumes&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/UV-Volumes.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Chen22arxiv_uvvolumes--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://neural-3d-video.github.io/&#34;&gt;Neural 3D Video Synthesis from Multi-view Video&lt;/a&gt;, Li et al., CVPR 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/3d-video.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://free-view-video.github.io/&#34;&gt;Dynamic View Synthesis from Dynamic Monocular Video&lt;/a&gt;, Gao et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/dvs_dmv.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://waymo.com/research/block-nerf/&#34;&gt;Block-NeRF: Scalable Large Scene Neural View Synthesis&lt;/a&gt;, Tancik et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/Block-NeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.14831&#34;&gt;Streaming Radiance Fields for 3D Video Synthesis&lt;/a&gt; Li et al. NeurIPS 2022 | &lt;a href=&#34;https://github.com/AlgoHunt/StreamRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/StreamRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Generalization&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.02442&#34;&gt;GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis&lt;/a&gt;, Schwarz et al., NeurIPS 2020 | &lt;a href=&#34;https://github.com/autonomousvision/graf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L237-L243&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Schwarz20neurips_graf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.04595&#34;&gt;GRF: Learning a General Radiance Field for 3D Scene Representation and Rendering&lt;/a&gt;, Trevithick and Yang, Arxiv 2020 | &lt;a href=&#34;https://github.com/alextrevithick/GRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L291-L297&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Trevithick20arxiv_GRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.02190&#34;&gt;pixelNeRF: Neural Radiance Fields from One or Few Images&lt;/a&gt;, Yu et al., CVPR 2021 | &lt;a href=&#34;https://github.com/sxyu/pixel-nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L329-L335&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Yu20arxiv_pixelNeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.02189&#34;&gt;Learned Initializations for Optimizing Coordinate-Based Neural Representations&lt;/a&gt;, Tancik et al., CVPR 2021 | &lt;a href=&#34;https://github.com/tancik/learnit&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L268-L274&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Tancik20arxiv_meta--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis&lt;/a&gt;, Chan et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L24-L30&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Chan20arxiv_piGAN--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://portrait-nerf.github.io/&#34;&gt;Portrait Neural Radiance Fields from a Single Image&lt;/a&gt;, Gao et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L95-L101&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Gao20arxiv_pNeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.08860.pdf&#34;&gt;ShaRF: Shape-conditioned Radiance Fields from a Single View&lt;/a&gt;, Rematas et al., ICML 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/sharf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://ibrnet.github.io/static/paper.pdf&#34;&gt;IBRNet: Learning Multi-View Image-Based Rendering&lt;/a&gt;, Wang et al., CVPR 2021 | &lt;a href=&#34;https://github.com/googleinterns/IBRNet&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/ibr.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.17269.pdf&#34;&gt;CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields&lt;/a&gt;, Niemeyer &amp;amp; Geiger, Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/CAMPARI.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.00587.pdf&#34;&gt;NeRF-VAE: A Geometry Aware 3D Scene Generative Model&lt;/a&gt;, Kosiorek et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nerf-vae.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://apple.github.io/ml-gsn/&#34;&gt;Unconstrained Scene Generation with Locally Conditioned Radiance Fields&lt;/a&gt;, DeVries et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/gsn.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://apchenstu.github.io/mvsnerf/&#34;&gt;MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo&lt;/a&gt;, Chen et al., ICCV 2021 | &lt;a href=&#34;https://github.com/apchenstu/mvsnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/mvsnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://virtualhumans.mpi-inf.mpg.de/srf/&#34;&gt;Stereo Radiance Fields (SRF): Learning View Synthesis from Sparse Views of Novel Scenes&lt;/a&gt;, Chibane et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/srf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://liuyuan-pal.github.io/NeuRay/&#34;&gt;Neural Rays for Occlusion-aware Image-based Rendering&lt;/a&gt;, Liu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/neuray.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.ajayj.com/dietnerf&#34;&gt;Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis&lt;/a&gt;, Matthew Tancik et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/DietNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://vincentfung13.github.io/projects/mine/&#34;&gt;MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis&lt;/a&gt;, Jiaxin Li et al., ICCV 2021 | &lt;a href=&#34;https://github.com/vincentfung13/MINE&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/MINE.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://imaging.cs.cmu.edu/torf/&#34;&gt;TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis&lt;/a&gt;, Benjamin Attal et al., NeurIPS 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/turf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/wbjang/home/codenerf&#34;&gt;CodeNeRF: Disentangled Neural Radiance Fields for Object Categories&lt;/a&gt;, Jang et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/CodeNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis&lt;/a&gt;, Gu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/stylenerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://sheldontsui.github.io/projects/GOF&#34;&gt;Generative Occupancy Fields for 3D Surface-Aware Image Synthesis&lt;/a&gt;, Xu et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/SheldonTsui/GOF_NeurIPS2021&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/gof.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://bmild.github.io/rawnerf/&#34;&gt;NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images&lt;/a&gt;, Ben Mildenhall et al, arXiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/rawnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://xharlie.github.io/projects/project_sites/pointnerf/index.html&#34;&gt;Point-NeRF: Point-based Neural Radiance Fields&lt;/a&gt;, Xu et al., CVPR 2022 | &lt;a href=&#34;https://github.com/Xharlie/pointnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/Point-NeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://vita-group.github.io/SinNeRF/&#34;&gt;SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image&lt;/a&gt;, Xu et al., ECCV 2022 | &lt;a href=&#34;https://github.com/VITA-Group/SinNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/SinNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://3d-avatar-diffusion.microsoft.com/&#34;&gt;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion&lt;/a&gt;, Wang et al., CVPR 2023 | &lt;a href=&#34;https://3d-avatar-diffusion.microsoft.com/&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/rodin.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.08971&#34;&gt;SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes&lt;/a&gt;, Gao et al., CVPR 2023 | &lt;a href=&#34;https://gymat.github.io/SurfelNeRF-web/&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/SurfelNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Pose Estimation&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://yenchenlin.me/inerf/&#34;&gt;iNeRF: Inverting Neural Radiance Fields for Pose Estimation&lt;/a&gt;, Yen-Chen et al. IROS 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L321-L327&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--YenChen20arxiv_iNeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zollhoefer.com/papers/arXiv20_ANeRF/page.html&#34;&gt;A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering&lt;/a&gt;, Su et al. Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/a-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Su21arxiv_A_NeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://nerfmm.active.vision/&#34;&gt;NeRF--: Neural Radiance Fields Without Known Camera Parameters&lt;/a&gt;, Wang et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/ActiveVisionLab/nerfmm&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nerf--.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Wang21arxiv_nerfmm--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://edgarsucar.github.io/iMAP/&#34;&gt;iMAP: Implicit Mapping and Positioning in Real-Time&lt;/a&gt;, Sucar et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/imap.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pengsongyou.github.io/nice-slam&#34;&gt;NICE-SLAM: Neural Implicit Scalable Encoding for SLAM&lt;/a&gt;, Zhu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nice-slam.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.15606&#34;&gt;GNeRF: GAN-based Neural Radiance Field without Posed Camera&lt;/a&gt;, Meng et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/gnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/&#34;&gt;BARF: Bundle-Adjusting Neural Radiance Fields&lt;/a&gt;, Lin et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/barf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://postech-cvlab.github.io/SCNeRF/&#34;&gt;Self-Calibrating Neural Radiance Fields&lt;/a&gt;, Jeong et al., ICCV 2021 | &lt;a href=&#34;https://github.com/POSTECH-CVLab/SCNeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/SCNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://rover-xingyu.github.io/L2G-NeRF/&#34;&gt;L2G-NeRF: Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields&lt;/a&gt;, Chen et al., CVPR 2023 | &lt;a href=&#34;https://github.com/rover-xingyu/L2G-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/L2G-NeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09050&#34;&gt;Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields&lt;/a&gt;, Maggio et al., ICRA 2023 | &lt;a href=&#34;https://github.com/MIT-SPARK/Loc-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/locnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Lighting&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://markboss.me/publication/2021-nerd/&#34;&gt;NeRD: Neural Reflectance Decomposition from Image Collections&lt;/a&gt;, Boss et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L9-L15&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Boss20arxiv_NeRD--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/nerv/&#34;&gt;NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis&lt;/a&gt;, Srinivasan et al. CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L260-L266&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Srinivasan20arxiv_NeRV--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://nex-mpi.github.io/&#34;&gt;NeX: Real-time View Synthesis with Neural Basis Expansion&lt;/a&gt;, Wizadwongsa et al. Arxiv 2021 | &lt;a href=&#34;https://github.com/nex-mpi/nex-code&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nex.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://people.csail.mit.edu/xiuming/projects/nerfactor/&#34;&gt;NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination&lt;/a&gt;, Zhang et al. Arxiv 2021 | &lt;a href=&#34;https://github.com/google/nerfactor&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nerfactor.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://xingangpan.github.io/projects/ShadeGAN.html&#34;&gt;A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis&lt;/a&gt;, Pan et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/XingangPan/ShadeGAN&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/shadegan.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.10885&#34;&gt;KiloNeuS: Implicit Neural Representations with Real-Time Global Illumination&lt;/a&gt;, Esposito et al., Arxiv 2022 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/citations/kiloneus.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Compositionality&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07492&#34;&gt;NeRF++: Analyzing and Improving Neural Radiance Fields&lt;/a&gt;, Zhang et al., Arxiv 2020 | &lt;a href=&#34;https://github.com/Kai-46/nerfplusplus&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L345-L351&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Zhang20arxiv_nerf++--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.12100&#34;&gt;GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields&lt;/a&gt;, Niemeyer et al., CVPR 2021, &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L175-L181&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Niemeyer20arxiv_GIRAFFE--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://shellguo.com/osf/&#34;&gt;Object-Centric Neural Scene Rendering&lt;/a&gt;, Guo et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L111-L117&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Guo20arxiv_OSF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://ziyanw1.github.io/hybrid_nerf/&#34;&gt;Learning Compositional Radiance Fields of Dynamic Human Heads&lt;/a&gt;, Wang et al., Arxiv 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/hybrid-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Wang20arxiv_hybrid_NeRF--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://light.princeton.edu/neural-scene-graphs/&#34;&gt;Neural Scene Graphs for Dynamic Scenes&lt;/a&gt;, Ost et al., CVPR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L353-L358&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--Ost20arxiv_NeuralSceneGraphs--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://kovenyu.com/uorf/&#34;&gt;Unsupervised Discovery of Object Radiance Fields&lt;/a&gt;, Yu et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/uorf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zju3dv.github.io/object_nerf/&#34;&gt;Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering&lt;/a&gt;, Yang et al., ICCV 2021 | &lt;a href=&#34;https://github.com/zju3dv/object_nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/object-nerf.txt&#34;&gt;bibtex&lt;/a&gt; &#xA;   &lt;!--yang2021objectnerf--&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://neverstopzyy.github.io/mofanerf/&#34;&gt;MoFaNeRF: Morphable Facial Neural Radiance Field&lt;/a&gt;, Zhuang et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/zhuhao-nju/mofanerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/mofaNeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Scene Labelling and Understanding&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://shuaifengzhi.com/Semantic-NeRF/&#34;&gt;In-Place Scene Labelling and Understanding with Implicit Scene Representation&lt;/a&gt;, Zhi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/semantic-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zhiwenfan.github.io/NeRF-SOS/&#34;&gt;NeRF-SOS: Any-view Self-supervised Object Segmentation on Complex Real-world Scenes&lt;/a&gt;, Fan et al., ICLR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/NeRF-SOS.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Editing&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://editnerf.csail.mit.edu/&#34;&gt;Editing Conditional Radiance Fields&lt;/a&gt;, Liu et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/stevliu/editnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/editnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jiakai-zhang.github.io/st-nerf/&#34;&gt;Editable Free-viewpoint Video Using a Layered Neural Representation&lt;/a&gt;, Zhang et al., SIGGRAPH 2021 | &lt;a href=&#34;https://github.com/DarlingHang/st-nerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/st-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jdily.github.io/proj_site/nerfin_proj.html&#34;&gt;NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors&lt;/a&gt;, Liu et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nerf-in.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://zhiwenfan.github.io/INS/&#34;&gt;Unified Implicit Neural Stylization&lt;/a&gt;, Fan et al., ECCV 2022| &lt;a href=&#34;https://github.com/VITA-Group/INS&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/INS.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Object Category Modeling&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://fig-nerf.github.io/&#34;&gt;FiG-NeRF: Figure Ground Neural Radiance Fields for 3D Object Category Modelling&lt;/a&gt;, Xie et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/fig-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/nvidia-research-nerf-tex-neural-reflectance-field-textures/&#34;&gt;NeRF-Tex: Neural Reflectance Field Textures&lt;/a&gt;, Baatz et al., EGSR 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nerf-tex.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Multi-scale&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jonbarron.info/mipnerf/&#34;&gt;Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields&lt;/a&gt;, Barron et al., Arxiv 2021 | &lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/mipnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields&lt;/a&gt;, Barron et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/mip-nerf-360.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Model Reconstruction&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.10078&#34;&gt;UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction&lt;/a&gt;, Oechsle et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/unisurf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.10689&#34;&gt;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction&lt;/a&gt;, Wang et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/neus.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12052&#34;&gt;Volume Rendering of Neural Implicit Surfaces&lt;/a&gt;, Yariv et al., NeurIPS 2021 | &lt;a href=&#34;https://github.com/ventusff/neurecon&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/volsdf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.12012&#34;&gt;NeAT: Learning Neural Implicit Surfaces with Arbitrary Topologies from Multi-view Images&lt;/a&gt;, Meng et al., CVPR 2023 | &lt;a href=&#34;https://github.com/xmeng525/NeAT&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/neat.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Depth Estimation&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://weiyithu.github.io/NerfingMVS/&#34;&gt;NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo&lt;/a&gt;, Wei et al., ICCV 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/NerfingMVS.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Robotics&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://3d-representation-learning.github.io/nerf-dy/&#34;&gt;3D Neural Scene Representations for Visuomotor Control&lt;/a&gt;, Li et al., CoRL 2021 Oral | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/nerf-dy.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.00168&#34;&gt;Vision-Only Robot Navigation in a Neural Radiance World&lt;/a&gt;, Adamkiewicz et al., RA-L 2022 Vol.7 No.2 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/vision-only.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Large-scale scene&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://mizhenxing.github.io/switchnerf&#34;&gt;Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields&lt;/a&gt;, Mi et al., ICLR 2023 | &lt;a href=&#34;https://github.com/MiZhenxing/Switch-NeRF&#34;&gt;github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/Switch-NeRF.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Talks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LCTYRqW-ne8&amp;amp;t=10190s&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;, Ben Mildenhall&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nRyOzHpcr4Q&amp;amp;feature=emb_logo&amp;amp;ab_channel=cvprtum&#34;&gt;Understanding and Extending Neural Radiance Fields&lt;/a&gt;, Barron et al.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/Rd0nBO6--bM?t=1992&#34;&gt;Towards Photorealism (2nd half)&lt;/a&gt;, Vladlen Koltun&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dPWLybp4LL0&#34;&gt;Neural Radiance Fields for View Synthesis&lt;/a&gt;, Matthew Tancik&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implementations&lt;/h2&gt; &#xA;&lt;h4&gt;Tensorflow&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bmild/nerf&#34;&gt;NeRF&lt;/a&gt;, Mildenhall et al., 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/NeRF-and-Beyond.bib#L168-L173&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/code/ritzraha/nerual-radiance-fields&#34;&gt;Nerual-Radiance-Fields&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ariG23498&#34;&gt;@ariG23498&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ritwik_raha&#34;&gt;@ritwik_raha&lt;/a&gt;, 2022&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;PyTorch&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yenchenlin/nerf-pytorch&#34;&gt;NeRF-PyTorch&lt;/a&gt;, Yen-Chen Lin, 2020 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/pytorch-nerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl&#34;&gt;NeRF-PyTorch-Lighting&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123&#34;&gt;@kwea123&lt;/a&gt;, 2020&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kwea123/nerf_pl/tree/nerfw&#34;&gt;NeRF-W&lt;/a&gt;, &lt;a href=&#34;https://github.com/kwea123&#34;&gt;@kwea123&lt;/a&gt;, 2021&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf&#34;&gt;NeRF-PyTorch3D&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch&#34;&gt;@facebookresearch&lt;/a&gt;, 2020&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Jax&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/jaxnerf&#34;&gt;JaxNeRF&lt;/a&gt;, Deng et al., 2020 | &lt;a href=&#34;https://github.com/yenchenlin/awesome-NeRF/raw/main/NeRF-and-Beyond.bib#L55-L60&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;Mip-NeRF&lt;/a&gt;, &lt;a href=&#34;https://github.com/google&#34;&gt;@google&lt;/a&gt;, 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/awesome-NeRF/awesome-NeRF/main/citations/mipnerf.txt&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/code/sauravmaheshkar/jax-flax-minimal-implementation-of-nerf&#34;&gt;[Jax + Flax] Minimal Implementation of NeRF&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/soumikrakshit&#34;&gt;@soumikrakshit&lt;/a&gt;, &lt;a href=&#34;https://www.kaggle.com/sauravmaheshkar&#34;&gt;@sauravmaheshkar&lt;/a&gt;, 2022&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Libraries&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/visu3d&#34;&gt;Visu3d&lt;/a&gt;, &lt;a href=&#34;https://github.com/google-research&#34;&gt;@google&lt;/a&gt;, 2022&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>