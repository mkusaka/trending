<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-20T01:36:24Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jamesfang8499/physics2</title>
    <updated>2024-03-20T01:36:24Z</updated>
    <id>tag:github.com,2024-03-20:/jamesfang8499/physics2</id>
    <link href="https://github.com/jamesfang8499/physics2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;é«˜ä¸­ç‰©ç†ç”²ç§æœ¬ï¼ˆç¬¬äºŒå†Œï¼‰é‡æ’æœ¬&lt;/h1&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æ˜¯å¯¹é«˜ä¸­ç‰©ç†ç”²ç§æœ¬ï¼ˆç¬¬äºŒå†Œï¼‰çš„è‡´æ•¬ã€‚è™½ç„¶è¯¥ä¹¦å¹´ä»£ä¹…è¿œï¼ˆ1983â€”1985å¹´å‡ºç‰ˆï¼Œååœ¨ä¸Šä¸–çºª90å¹´ä»£ä»¥ã€Šé«˜ä¸­ç‰©ç†è¯»æœ¬ã€‹ä¸ºåå†ç‰ˆè¿‡ï¼‰ï¼Œä½†æ˜¯å†…å®¹ä½“ç³»å®‰æ’æ¯”å¦‚ä»Šçš„é«˜ä¸­æ•™æå®Œæ•´ä¸”åˆç†ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é‡æ’æœ¬æ˜¯æ ¹æ®ç½‘ç»œä¸Šæ‰¾åˆ°çš„æ­¤ä¹¦æ‰«æç‰ˆç”µå­æ–‡æ¡£ï¼Œä½¿ç”¨LaTeXåˆ¶ä½œè€Œæˆçš„é‡æ’æœ¬ç”µå­æ–‡æ¡£ï¼ˆPDFæ ¼å¼ï¼‰ã€‚ä¹¦ä¸­çš„çŸ¢é‡å›¾ç‰‡é‡‡ç”¨ç”µå­ç‰ˆæ•™æä¸­çš„çŸ¢é‡å›¾ï¼Œæˆ–é‡‡ç”¨TikzåŠTkz-euclideåˆ¶ä½œè€Œæ¥ã€‚å…¶ä½™çš„ç‚¹é˜µå›¾åˆ™æ˜¯æ¥è‡ªäºæ‰«æç‰ˆç”µå­æ–‡æ¡£ï¼ˆé™äºä½œè€…çš„èƒ½åŠ›å’Œç²¾åŠ›ï¼Œæ— æ³•å°†æ‰€æœ‰å†…å®¹å‡ä»¥çŸ¢é‡å›¾å…¨éƒ¨é‡ç»˜ï¼‰ã€‚æ–‡æ¡£å½“ä¸­çš„ç”µè·¯å›¾åŸºäºcircuitikzç»˜åˆ¶ï¼Œè¯·ä½¿ç”¨TeXLive2020ä»¥åçš„ç‰ˆæœ¬ç¼–è¯‘ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ³¨æ„ï¼šæœ¬é¡¹ç›®çš„å†…å®¹å‹¿ç”¨äºå•†ä¸šç›®çš„ã€‚ç”µå­ç‰ˆçš„åŸæ•™æï¼Œå¯é€šè¿‡å¦‚ä¸‹ç½‘å€ä¸‹è½½ï¼š&lt;a href=&#34;https://pan.baidu.com/s/1k2LGR&#34;&gt;https://pan.baidu.com/s/1k2LGR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;è¡¥å……ï¼šé«˜çº§ä¸­å­¦ç‰©ç†ï¼ˆç”²ç§æœ¬ï¼‰ç¬¬äºŒå†Œæ•™å­¦å‚è€ƒä¹¦ï¼ŒåŒ…å«äº†å„ç« çš„æ•™å­¦å†…å®¹ã€æ•™å­¦å»ºè®®ã€å®éªŒæŒ‡å¯¼ã€ä¹ é¢˜è§£ç­”ã€å‚è€ƒèµ„æ–™ç­‰ã€‚ç›¸å…³ä»£ç åœ¨2-refæ–‡ä»¶å¤¹ä¸­ã€‚&lt;/li&gt; &#xA; &lt;li&gt;PS: fig.rarå½“ä¸­å­˜å‚¨çš„æ˜¯æœ¬ä»£ç æ‰€è°ƒç”¨çš„æ‰€æœ‰å›¾ç‰‡ï¼Œè¯·è¿åŒæ–‡ä»¶å¤¹è§£å‹åæ”¾åœ¨å·¥ä½œç›®å½•ä¸­ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ç›®å½•&lt;/h1&gt; &#xA;&lt;h2&gt;è¯´æ˜&lt;/h2&gt; &#xA;&lt;h2&gt;ç¬¬ä¸€ç«  åˆ†å­è¿åŠ¨è®ºåŸºç¡€&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åˆ†å­è¿åŠ¨è®ºçš„å»ºç«‹&lt;/li&gt; &#xA; &lt;li&gt;ç‰©ä½“æ˜¯ç”±åˆ†å­ç»„æˆçš„&lt;/li&gt; &#xA; &lt;li&gt;å¸ƒæœ—è¿åŠ¨&lt;/li&gt; &#xA; &lt;li&gt;åˆ†å­é—´çš„ç›¸äº’ä½œç”¨åŠ›&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬äºŒç«  å†…èƒ½ï¼Œèƒ½çš„è½¬åŒ–å’Œå®ˆæ’å®šå¾‹&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç‰©ä½“çš„å†…èƒ½&lt;/li&gt; &#xA; &lt;li&gt;æ”¹å˜å†…èƒ½çš„ä¸¤ç§æ–¹å¼&lt;/li&gt; &#xA; &lt;li&gt;çƒ­åŠŸå½“é‡&lt;/li&gt; &#xA; &lt;li&gt;èƒ½çš„è½¬åŒ–å’Œå®ˆæ’å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;èƒ½çš„è½¬åŒ–å’Œå®ˆæ’å®šå¾‹çš„å»ºç«‹åŠå…¶æ„ä¹‰&lt;/li&gt; &#xA; &lt;li&gt;èƒ½æºçš„åˆ©ç”¨å’Œå¼€å‘&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬ä¸‰ç«  æ°”ä½“çš„æ€§è´¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ°”ä½“çš„çŠ¶æ€å’ŒçŠ¶æ€å‚é‡&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“çš„ç­‰æ¸©å˜åŒ–ç»æ„è€³-é©¬ç•¥ç‰¹å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“çš„ç­‰å®¹å˜åŒ–æŸ¥ç†å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;çƒ­åŠ›å­¦æ¸©æ ‡&lt;/li&gt; &#xA; &lt;li&gt;ç†æƒ³æ°”ä½“çš„çŠ¶æ€æ–¹ç¨‹&lt;/li&gt; &#xA; &lt;li&gt;å…‹æ‹‰ç€é¾™æ–¹ç¨‹&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“åˆ†å­è¿åŠ¨çš„ç‰¹ç‚¹&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“å®éªŒå®šå¾‹çš„å¾®è§‚è§£é‡Š&lt;/li&gt; &#xA; &lt;li&gt;ç†æƒ³æ°”ä½“çš„å†…èƒ½&lt;/li&gt; &#xA; &lt;li&gt;ç†æƒ³æ°”ä½“çš„å†…èƒ½å˜åŒ–&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬å››ç«  å›ºä½“å’Œæ¶²ä½“çš„æ€§è´¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ™¶ä½“å’Œéæ™¶ä½“&lt;/li&gt; &#xA; &lt;li&gt;ç©ºé—´ç‚¹é˜µ&lt;/li&gt; &#xA; &lt;li&gt;æ¶²ä½“çš„å¾®è§‚ç»“æ„&lt;/li&gt; &#xA; &lt;li&gt;æ¶²ä½“çš„è¡¨é¢ç°è±¡&lt;/li&gt; &#xA; &lt;li&gt;æµ¸æ¶¦å’Œä¸æµ¸æ¶¦&lt;/li&gt; &#xA; &lt;li&gt;æ¯›ç»†ç°è±¡&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬äº”ç«  ç‰©æ€å˜åŒ–&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç†”è§£å’Œå‡å›º&lt;/li&gt; &#xA; &lt;li&gt;ç†”è§£çƒ­&lt;/li&gt; &#xA; &lt;li&gt;è’¸å‘&lt;/li&gt; &#xA; &lt;li&gt;é¥±å’Œæ±½ä¸é¥±å’Œæ±½å‹&lt;/li&gt; &#xA; &lt;li&gt;æ²¸è…¾&lt;/li&gt; &#xA; &lt;li&gt;æ±½åŒ–çƒ­&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“çš„æ¶²åŒ–&lt;/li&gt; &#xA; &lt;li&gt;ç©ºæ°”çš„æ¹¿åº¦&lt;/li&gt; &#xA; &lt;li&gt;éœ²ç‚¹æ¹¿åº¦è®¡&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬å…­ç«  ç”µåœº&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸¤ç§ç”µè·ç”µè·å®ˆæ’å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;åº“ä»‘å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;ç”µåœºç”µåœºå¼ºåº¦&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠ›çº¿&lt;/li&gt; &#xA; &lt;li&gt;ç”µåœºä¸­çš„å¯¼ä½“&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠ¿èƒ½&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠ¿&lt;/li&gt; &#xA; &lt;li&gt;ç­‰åŠ¿é¢&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠ¿å·®&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠ¿å·®è·Ÿç”µåœºå¼ºåº¦çš„å…³ç³»&lt;/li&gt; &#xA; &lt;li&gt;å¸¦ç”µç²’å­åœ¨ç”µåœºä¸­çš„è¿åŠ¨&lt;/li&gt; &#xA; &lt;li&gt;åŸºæœ¬ç”µè·çš„æµ‹å®šï¼šå¯†ç«‹æ ¹å®éªŒ&lt;/li&gt; &#xA; &lt;li&gt;ç”µå®¹å™¨ç”µå®¹&lt;/li&gt; &#xA; &lt;li&gt;ç”µå®¹å™¨çš„è¿æ¥&lt;/li&gt; &#xA; &lt;li&gt;é™ç”µçš„é˜²æ­¢å’Œåº”ç”¨&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬ä¸ƒç«  ç¨³æ’ç”µæµ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç”µæµ&lt;/li&gt; &#xA; &lt;li&gt;æ¬§å§†å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;ç”µé˜»å®šå¾‹ç”µé˜»ç‡&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠŸå’Œç”µåŠŸç‡&lt;/li&gt; &#xA; &lt;li&gt;ç„¦è€³å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;ä¸²è”ç”µè·¯&lt;/li&gt; &#xA; &lt;li&gt;å¹¶è”ç”µè·¯&lt;/li&gt; &#xA; &lt;li&gt;åˆ†å‹å’Œåˆ†æµåœ¨ä¼ç‰¹è¡¨å’Œå®‰åŸ¹è¡¨ä¸­çš„åº”ç”¨&lt;/li&gt; &#xA; &lt;li&gt;ç”µè·¯çš„åˆ†æå’Œè®¡ç®—&lt;/li&gt; &#xA; &lt;li&gt;ç”µåŠ¨åŠ¿é—­åˆç”µè·¯çš„æ¬§å§†å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;è·¯ç«¯ç”µå‹&lt;/li&gt; &#xA; &lt;li&gt;ç”µæ± ç»„&lt;/li&gt; &#xA; &lt;li&gt;ç”µé˜»çš„æµ‹é‡&lt;/li&gt; &#xA; &lt;li&gt;æƒ æ–¯é€šç”µæ¡¥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç¬¬å…«ç«  ç‰©è´¨çš„å¯¼ç”µæ€§&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;é‡‘å±çš„å¯¼ç”µæ€§&lt;/li&gt; &#xA; &lt;li&gt;æ¶²ä½“çš„å¯¼ç”µæ€§&lt;/li&gt; &#xA; &lt;li&gt;æ³•æ‹‰ç¬¬ç”µè§£å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;ç”µå­ç”µé‡çš„ç¡®å®š&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“çš„å¯¼ç”µæ€§&lt;/li&gt; &#xA; &lt;li&gt;å‡ ç§è‡ªæ¿€æ”¾ç”µç°è±¡&lt;/li&gt; &#xA; &lt;li&gt;æ°”ä½“ç”µå…‰æº&lt;/li&gt; &#xA; &lt;li&gt;çœŸç©ºä¸­çš„ç”µæµ&lt;/li&gt; &#xA; &lt;li&gt;ç¤ºæ³¢ç®¡&lt;/li&gt; &#xA; &lt;li&gt;åŠå¯¼ä½“çš„å¯¼ç”µæ€§&lt;/li&gt; &#xA; &lt;li&gt;N å‹åŠå¯¼ä½“å’ŒP å‹åŠå¯¼ä½“&lt;/li&gt; &#xA; &lt;li&gt;PN ç»“æ™¶ä½“äºŒæç®¡&lt;/li&gt; &#xA; &lt;li&gt;æ™¶ä½“ä¸‰æç®¡&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å­¦ç”Ÿå®éªŒ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;éªŒè¯ç»æ„è€³-é©¬ç•¥ç‰¹å®šå¾‹&lt;/li&gt; &#xA; &lt;li&gt;éªŒè¯æ°”ä½“çŠ¶æ€æ–¹ç¨‹&lt;/li&gt; &#xA; &lt;li&gt;æµ‹å®šå†°çš„ç†”è§£çƒ­&lt;/li&gt; &#xA; &lt;li&gt;æµ‹å®šç©ºæ°”çš„ç›¸å¯¹æ¹¿åº¦&lt;/li&gt; &#xA; &lt;li&gt;ç”µåœºä¸­ç­‰åŠ¿çº¿çš„æç»˜&lt;/li&gt; &#xA; &lt;li&gt;åˆ©ç”¨ç”µå®¹å™¨æ”¾ç”µæµ‹ç”µå®¹&lt;/li&gt; &#xA; &lt;li&gt;æµ‹å®šé‡‘å±çš„ç”µé˜»ç‡&lt;/li&gt; &#xA; &lt;li&gt;æŠŠç”µæµè¡¨æ”¹è£…ä¸ºä¼ç‰¹è¡¨&lt;/li&gt; &#xA; &lt;li&gt;ç”¨å®‰åŸ¹è¡¨å’Œä¼ç‰¹è¡¨æµ‹å®šç”µæ± çš„ç”µåŠ¨åŠ¿å’Œå†…ç”µé˜»&lt;/li&gt; &#xA; &lt;li&gt;ç»ƒä¹ ä½¿ç”¨ä¸‡ç”¨ç”µè¡¨&lt;/li&gt; &#xA; &lt;li&gt;ç”¨æƒ æ–¯é€šç”µæ¡¥æµ‹ç”µé˜»&lt;/li&gt; &#xA; &lt;li&gt;æµ‹å®šé“œçš„ç”µåŒ–å½“é‡&lt;/li&gt; &#xA; &lt;li&gt;ç»ƒä¹ ä½¿ç”¨ç¤ºæ³¢å™¨&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;è¯¾å¤–å®éªŒæ´»åŠ¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;è§‚å¯Ÿæ‰©æ•£ç°è±¡&lt;/li&gt; &#xA; &lt;li&gt;è‡ªåˆ¶å†°æ·‡æ·‹&lt;/li&gt; &#xA; &lt;li&gt;äººé€ äº‘é›¾&lt;/li&gt; &#xA; &lt;li&gt;æµ‹å®šæ°´çš„æ±½åŒ–çƒ­&lt;/li&gt; &#xA; &lt;li&gt;ä¼°è®¡æ°´å‡é«˜çš„æ¸©åº¦&lt;/li&gt; &#xA; &lt;li&gt;ç”¨è‡ªåˆ¶çš„éªŒç”µå™¨åšé™ç”µå®éªŒ&lt;/li&gt; &#xA; &lt;li&gt;è‡ªåˆ¶ç”µæ± &lt;/li&gt; &#xA; &lt;li&gt;ç ”ç©¶ç”µç¯æ³¡çš„ç”µé˜»&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¸¸ç”¨çš„çƒ­å­¦é‡å’Œç”µå­¦é‡çš„å›½é™…å•ä½åˆ¶å•ä½&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>AlonzoLeeeooo/awesome-video-generation</title>
    <updated>2024-03-20T01:36:24Z</updated>
    <id>tag:github.com,2024-03-20:/AlonzoLeeeooo/awesome-video-generation</id>
    <link href="https://github.com/AlonzoLeeeooo/awesome-video-generation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of awesome video generation studies.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;A Collection of Video Generation Studies&lt;/h1&gt; &#xA;&lt;p&gt;This GitHub repository summarizes papers and resources related to the video generation task.&lt;/p&gt; &#xA;&lt;p&gt;If you have any suggestions about this repository, please feel free to &lt;a href=&#34;https://github.com/AlonzoLeeeooo/awesome-video-generation/issues/new&#34;&gt;start a new issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/AlonzoLeeeooo/awesome-video-generation/pulls&#34;&gt;pull requests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;&lt;span id=&#34;contents&#34;&gt;Contents&lt;/span&gt;&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#to-do-lists&#34;&gt;To-Do Lists&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#products&#34;&gt;Products&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-to-video-generation&#34;&gt;Text-to-Video Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#survey-papers&#34;&gt;Survey Papers&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2021&#34;&gt;Year 2021&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-to-video-generation&#34;&gt;Image-to-Video Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#video-editing&#34;&gt;Video Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#editing-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#audio-to-video-generation&#34;&gt;Audio-to-Video Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#audio-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#audio-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#qa&#34;&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;To-Do Lists&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Latest Papers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update CVPR 2024 Papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update AAAI 2024 Papers &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update PDFs and References of âš ï¸ Papers&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Published Versions of References&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update ICLR 2024 Papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update NeurIPS 2024 Papers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Previously Published Papers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update Previous CVPR papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ICCV papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ECCV papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous NeurIPS papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ICLR papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous AAAI papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ACM MM papers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Regular Maintenance of Preprint arXiv Papers and Missed Papers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Products&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Organization&lt;/th&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Research Paper&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Specialties&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sora&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.midjourney.com/home&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openai.com/sora&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lumiere&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.12945&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lumiere-video.github.io/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VideoPoet&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.research.google/videopoet/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;W.A.I.T&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.06662.pdf&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://walt-video-diffusion.github.io/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gen-2&lt;/td&gt; &#xA;   &lt;td&gt;Runaway&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.runwayml.com/gen2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gen-1&lt;/td&gt; &#xA;   &lt;td&gt;Runaway&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.runwayml.com/gen1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Animate Anyone&lt;/td&gt; &#xA;   &lt;td&gt;Alibaba&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.17117.pdf&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://humanaigc.github.io/animate-anyone/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Outfit Anyone&lt;/td&gt; &#xA;   &lt;td&gt;Alibaba&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://outfitanyone.app/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Video&lt;/td&gt; &#xA;   &lt;td&gt;StabilityAI&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.15127.pdf&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.stablevideo.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pixeling&lt;/td&gt; &#xA;   &lt;td&gt;HiDream.ai&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hidreamai.com/#/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DomoAI&lt;/td&gt; &#xA;   &lt;td&gt;DomoAI&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://domoai.app/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Emu&lt;/td&gt; &#xA;   &lt;td&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10709&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://emu-video.metademolab.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Genmo&lt;/td&gt; &#xA;   &lt;td&gt;Genmo&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.genmo.ai/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeverEnds&lt;/td&gt; &#xA;   &lt;td&gt;NeverEnds&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://neverends.life/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Moonvalley&lt;/td&gt; &#xA;   &lt;td&gt;Moonvalley&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://moonvalley.ai/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Morph Studio&lt;/td&gt; &#xA;   &lt;td&gt;Morph&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.morphstudio.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pika&lt;/td&gt; &#xA;   &lt;td&gt;Pika&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pika.art/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixelDance&lt;/td&gt; &#xA;   &lt;td&gt;ByteDance&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10982&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://makepixelsdance.github.io/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Papers&lt;/h1&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Survey Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;survey-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A Survey on Video Diffusion Models &lt;a href=&#34;&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Text-to-Video Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Vlogger:&lt;/strong&gt;&lt;/em&gt; Make Your Dream A Vlog &lt;a href=&#34;https://arxiv.org/pdf/2401.09414.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/Vlogger&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make Pixels Dance:&lt;/strong&gt;&lt;/em&gt; High-Dynamic Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.10982.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://makepixelsdance.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://makepixelsdance.github.io/demo.html&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VGen:&lt;/strong&gt;&lt;/em&gt; Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.04483&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/VGen&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://higen-t2v.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;GenTron:&lt;/strong&gt;&lt;/em&gt; Delving Deep into Diffusion Transformers for Image and Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.04557&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://www.shoufachen.com/gentron_website/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SimDA:&lt;/strong&gt;&lt;/em&gt; Simple Diffusion Adapter for Efficient Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2308.09710.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenHsing/SimDA&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://chenhsing.github.io/SimDA/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MicroCinema:&lt;/strong&gt;&lt;/em&gt; A Divide-and-Conquer Approach for Text-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.18829&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://wangyanhui666.github.io/MicroCinema.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://youtube.com/shorts/H7O-Ku_lqPA&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Generative Rendering:&lt;/strong&gt;&lt;/em&gt; Controllable 4D-Guided Video Generation with 2D Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2312.01409&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://primecai.github.io/generative_rendering/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PEEKABOO:&lt;/strong&gt;&lt;/em&gt; Interactive Video Generation via Masked-Diffusion &lt;a href=&#34;https://arxiv.org/pdf/2312.07509&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/Peekaboo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://jinga-lala.github.io/projects/Peekaboo/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/anshuln/peekaboo-demo&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;EvalCrafter:&lt;/strong&gt;&lt;/em&gt; Benchmarking and Evaluating Large Video Generation Models &lt;a href=&#34;https://arxiv.org/pdf/2310.11440&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/EvalCrafter/EvalCrafter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://evalcrafter.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;A Recipe for Scaling up Text-to-Video Generation with Text-free Videos &lt;a href=&#34;https://arxiv.org/pdf/2312.15770&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/damo-vilab/i2vgen-xl&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://tf-t2v.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;BIVDiff:&lt;/strong&gt;&lt;/em&gt; A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2312.02813&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://bivdiff.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Mind the Time:&lt;/strong&gt;&lt;/em&gt; Scaled Spatiotemporal Transformers for Text-to-Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2402.14797&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://snap-research.github.io/snapvideo/video_ldm.html&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Animate Anyone:&lt;/strong&gt;&lt;/em&gt; Consistent and Controllable Image-to-video Synthesis for Character Animation &lt;a href=&#34;https://arxiv.org/pdf/2311.17117.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://humanaigc.github.io/animate-anyone/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;âš ï¸ Simple but Effective Text-to-Video Generation with Grid Diffusion Models &lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;âš ï¸ Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation &lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;âš ï¸ DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation &lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VDT:&lt;/strong&gt;&lt;/em&gt; General-purpose Video Diffusion Transformers via Mask Modeling &lt;a href=&#34;https://arxiv.org/pdf/2305.13311.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/RERV/VDT&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vdt-2023.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VersVideo:&lt;/strong&gt;&lt;/em&gt; Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation &lt;a href=&#34;https://openreview.net/pdf?id=K9sVJ17zvB&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Follow Your Pose:&lt;/strong&gt;&lt;/em&gt; Pose-Guided Text-to-Video Generation using Pose-Free Videos &lt;a href=&#34;https://arxiv.org/pdf/2304.01186&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/mayuelala/FollowYourPose&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://follow-your-pose.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;E2HQV:&lt;/strong&gt;&lt;/em&gt; High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning &lt;a href=&#34;https://arxiv.org/pdf/2401.08117&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2310.07697&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/pengbo807/ConditionVideo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://pengbo807.github.io/conditionvideo-website/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;F3-Pruning:&lt;/strong&gt;&lt;/em&gt; A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text to-Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2312.03459&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Lumiere:&lt;/strong&gt;&lt;/em&gt; A Space-Time Diffusion Model for Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2401.12945.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://lumiere-video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Boximator:&lt;/strong&gt;&lt;/em&gt; Generating Rich and Controllable Motions for Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2402.01566.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://boximator.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=reto_TYsYyQ&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;World Model on Million-Length Video And Language With RingAttention &lt;a href=&#34;https://arxiv.org/abs/2402.08268&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/LargeWorldModel/LWM&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://largeworldmodel.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Direct-a-Video:&lt;/strong&gt;&lt;/em&gt; Customized Video Generation with User-Directed Camera Movement and Object Motion &lt;a href=&#34;https://arxiv.org/pdf/2402.03162.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://direct-a-video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;WorldDreamer:&lt;/strong&gt;&lt;/em&gt; Towards General World Models for Video Generation via Predicting Masked Tokens &lt;a href=&#34;https://arxiv.org/pdf/2401.09985&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/JeffWang987/WorldDreamer&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://world-dreamer.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MagicVideo-V2:&lt;/strong&gt;&lt;/em&gt; Multi-Stage High-Aesthetic Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2401.04468.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://magicvideov2.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Latte:&lt;/strong&gt;&lt;/em&gt; Latent Diffusion Transformer for Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2401.03048&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://maxin-cn.github.io/latte_project&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Others&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Sora:&lt;/strong&gt;&lt;/em&gt; Video Generation Models as World Simulators &lt;a href=&#34;https://openai.com/research/video-generation-models-as-world-simulators&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Align your Latents:&lt;/strong&gt;&lt;/em&gt; High-resolution Video Synthesis with Latent Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2304.08818.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://github.com/srpkdyy/VideoLDM&#34;&gt;[Reproduced code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Text2Video-Zero:&lt;/strong&gt;&lt;/em&gt; Text-to-image Diffusion Models are Zero-shot Video Generators &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PAIR/Text2Video-Zero&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://text2video-zero.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Video Probabilistic Diffusion Models in Projected Latent Space &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/sihyun-yu/PVDM&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://research.nvidia.com/labs/dir/pyoco/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Gen-1:&lt;/strong&gt;&lt;/em&gt; Structure and Content-guided Video Synthesis with Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://research.runwayml.com/gen1&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Video Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2204.03458.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://video-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CogVideo:&lt;/strong&gt;&lt;/em&gt; Large-scale Pretraining for Text-to-video Generation via Transformers &lt;a href=&#34;https://openreview.net/pdf?id=rB6TpjAuSRy&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://models.aminer.cn/cogvideo/&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make-A-Video:&lt;/strong&gt;&lt;/em&gt; Text-to-video Generation without Text-video Data &lt;a href=&#34;https://arxiv.org/pdf/2209.14792.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://makeavideo.studio/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/make-a-video-pytorch&#34;&gt;[Reproduced code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Phenaki:&lt;/strong&gt;&lt;/em&gt; Variable Length Video Generation From Open Domain Textual Description &lt;a href=&#34;https://openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/phenaki-pytorch&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AnimateDiff:&lt;/strong&gt;&lt;/em&gt; Animate Your Personalized Text-to-image Diffusion Models without Specific Tuning &lt;a href=&#34;https://openreview.net/pdf?id=Fx2SbBgcte&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://animatediff.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Control-A-Video:&lt;/strong&gt;&lt;/em&gt; Controllable Text-to-video Generation with Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2305.13840.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Weifeng-Chen/control-a-video&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/wf-genius/Control-A-Video&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.13840.pdf&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ControlVideo:&lt;/strong&gt;&lt;/em&gt; Training-free Controllable Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2305.13077.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/YBYBZhang/ControlVideo&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Imagen Video:&lt;/strong&gt;&lt;/em&gt; High Definition Video Generation with Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2210.02303.pdf&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Latent-Shift:&lt;/strong&gt;&lt;/em&gt; Latent Diffusion with Temporal Shift for Efficient Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2304.08477.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://latent-shift.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LAVIE:&lt;/strong&gt;&lt;/em&gt; High-quality Video Generation with Cascaded Latent Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2309.15103.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/LaVie&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/LaVie-project/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Show-1:&lt;/strong&gt;&lt;/em&gt; Marrying Pixel and Latent Diffusion Models for Text-to-video Generation &lt;a href=&#34;https://showlab.github.io/Show-1/assets/Show-1.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/showlab/Show-1&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://showlab.github.io/Show-1/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Stable Video Diffusion:&lt;/strong&gt;&lt;/em&gt; Scaling Latent Video Diffusion Models to Large Datasets &lt;a href=&#34;https://arxiv.org/pdf/2311.15127.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://stability.ai/news/stable-video-diffusion-open-ai-video-model&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoComposer:&lt;/strong&gt;&lt;/em&gt; Compositional Video Synthesis with Motion Controllability &lt;a href=&#34;https://arxiv.org/pdf/2306.02018.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/videocomposer&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://videocomposer.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoFactory:&lt;/strong&gt;&lt;/em&gt; Swap Attention in Spatiotemporal Diffusions for Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2305.10874.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoGen:&lt;/strong&gt;&lt;/em&gt; A Reference-guided Latent Diffusion Approach for High Definition Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2309.00398.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://videogen.github.io/VideoGen/&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InstructVideo:&lt;/strong&gt;&lt;/em&gt; Instructing Video Diffusion Models with Human Feedback &lt;a href=&#34;https://arxiv.org/pdf/2312.12490.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/i2vgen-xl/raw/main/doc/InstructVideo.md&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2312.12490.pdf&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Emu Video:&lt;/strong&gt;&lt;/em&gt; Factorizing Text-to-Video Generation by Explicit Image Conditioning &lt;a href=&#34;https://arxiv.org/pdf/2311.10709.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SEINE:&lt;/strong&gt;&lt;/em&gt; Short-to-Long Video Diffusion Model for Generative Transition and Prediction &lt;a href=&#34;https://arxiv.org/pdf/2310.20700.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/SEINE&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/SEINE-project/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoLCM:&lt;/strong&gt;&lt;/em&gt; Video Latent Consistency Model &lt;a href=&#34;https://arxiv.org/pdf/2312.09109.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2022&#34;&gt;&lt;strong&gt;Year 2022&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Show_Me_What_and_Tell_Me_How_Video_Synthesis_via_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/MMVID&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/MMVID/raw/main/mm_vox_celeb/README.md&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2021&#34;&gt;&lt;strong&gt;Year 2021&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoGPT:&lt;/strong&gt;&lt;/em&gt; Video Generation using VQ-VAE and Transformers &lt;a href=&#34;https://arxiv.org/pdf/2104.10157.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://wilson1yan.github.io/videogpt/index.html&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MagicVideo:&lt;/strong&gt;&lt;/em&gt; Efficient Video Generation With Latent Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2211.11018&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Image-to-Video Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;image-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoBooth:&lt;/strong&gt;&lt;/em&gt; Diffusion-based Video Generation with Image Prompts &lt;a href=&#34;https://arxiv.org/pdf/2312.00777&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/VideoBooth&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/VideoBooth-project/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=10DxH1JETzI&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Decouple Content and Motion for Conditional Image-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.14294&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;I2V-Adapter:&lt;/strong&gt;&lt;/em&gt; A General Image-to-Video Adapter for Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2312.16693.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/I2V-Adapter/I2V-Adapter-repo&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Follow-Your-Click:&lt;/strong&gt;&lt;/em&gt; Open-domain Regional Image Animation via Short Prompts &lt;a href=&#34;https://arxiv.org/pdf/2403.08268&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/mayuelala/FollowYourClick&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://follow-your-click.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AtomoVideo:&lt;/strong&gt;&lt;/em&gt; High Fidelity Image-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2403.01800.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://atomo-video.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/embed/36JIlk-U-vQ&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;image-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Conditional Image-to-Video Generation with Latent Flow Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/nihaomiao/CVPR23_LFDM&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;I2VGen-XL:&lt;/strong&gt;&lt;/em&gt; High-quality Image-to-video Synthesis via Cascaded Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2311.04145.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/i2vgen-xl&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DreamVideo:&lt;/strong&gt;&lt;/em&gt; High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance &lt;a href=&#34;https://arxiv.org/pdf/2312.03018&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/anonymous0769/DreamVideo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://anonymous0769.github.io/DreamVideo/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DynamiCrafter:&lt;/strong&gt;&lt;/em&gt; Animating Open-domain Images with Video Diffusion Priors &lt;a href=&#34;https://arxiv.org/pdf/2310.12190&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://doubiiu.github.io/projects/DynamiCrafter/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://github.com/Doubiiu/DynamiCrafter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=0NfmIsNAg-g&#34;&gt;[Video]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Doubiiu/DynamiCrafter&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;image-year-2022&#34;&gt;&lt;strong&gt;Year 2022&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make It Move:&lt;/strong&gt;&lt;/em&gt; Controllable Image-to-Video Generation with Text Descriptions &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Make_It_Move_Controllable_Image-to-Video_Generation_With_Text_Descriptions_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Youncy-Hu/MAGE&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Audio-to-Video Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;audio-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation &lt;a href=&#34;https://arxiv.org/pdf/2309.16429&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/guyyariv/TempoTokens&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;audio-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MM-Diffusion:&lt;/strong&gt;&lt;/em&gt; Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/researchmm/MM-Diffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Video Editing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Fairy:&lt;/strong&gt;&lt;/em&gt; Fast Parallellized Instruction-Guided Video-to-Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2312.13834&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://fairy-video2video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CCEdit:&lt;/strong&gt;&lt;/em&gt; Creative and Controllable Video Editing via Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2309.16496&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/RuoyuFeng/CCEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ruoyufeng.github.io/CCEdit.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=UQw4jq-igN4&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DynVideo-E:&lt;/strong&gt;&lt;/em&gt; Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2310.10624&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://showlab.github.io/DynVideo-E/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=xiRH4Q6B3Yk&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Video-P2P:&lt;/strong&gt;&lt;/em&gt; Video Editing with Cross-attention Control &lt;a href=&#34;https://arxiv.org/pdf/2303.04761&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/Video-P2P&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://video-p2p.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.05856&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/STEM-Inv/stem-inv&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://stem-inv.github.io/page/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MaskINT:&lt;/strong&gt;&lt;/em&gt; Video Editing via Interpolative Non-autoregressive Masked Transformers &lt;a href=&#34;https://arxiv.org/pdf/2312.12468&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://maskint.github.io/&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://maskint.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VidToMe:&lt;/strong&gt;&lt;/em&gt; Video Token Merging for Zero-Shot Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.10656&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lixirui142/VidToMe&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vidtome-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://youtu.be/cZPtwcRepNY&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Towards Language-Driven Video Inpainting via Multimodal Large Language Models &lt;a href=&#34;https://arxiv.org/pdf/2401.10226.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/jianzongwu/Language-Driven-Video-Inpainting&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://jianzongwu.github.io/projects/rovi/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/jianzongwu/rovi&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;âš ï¸ &lt;em&gt;&lt;strong&gt;CAMEL:&lt;/strong&gt;&lt;/em&gt; CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Ground-A-Video:&lt;/strong&gt;&lt;/em&gt; Zero-shot Grounded Video Editing using Text-to-image Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2310.01107&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Ground-A-Video/Ground-A-Video&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ground-a-video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;TokenFlow:&lt;/strong&gt;&lt;/em&gt; Consistent Diffusion Features for Consistent Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2307.10373&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/omerbt/TokenFlow&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://diffusion-tokenflow.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;UniEdit:&lt;/strong&gt;&lt;/em&gt; A Unified Tuning-Free Framework for Video Motion and Appearance Editing &lt;a href=&#34;https://arxiv.org/pdf/2402.13185&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/JianhongBai/UniEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://jianhongbai.github.io/UniEdit/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DragAnything:&lt;/strong&gt;&lt;/em&gt; Motion Control for Anything using Entity Representation &lt;a href=&#34;https://arxiv.org/pdf/2403.07420.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/showlab/DragAnything&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://weijiawu.github.io/draganything_page/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Style-A-Video:&lt;/strong&gt;&lt;/em&gt; Agile Diffusion for Arbitrary Text-based Video Style Transfer &lt;a href=&#34;https://arxiv.org/pdf/2305.05464.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[ICCV 2021] &lt;em&gt;&lt;strong&gt;WebVid-10M:&lt;/strong&gt;&lt;/em&gt; Frozen in Time: ï¸A Joint Video and Image Encoder for End to End Retrieval &lt;a href=&#34;https://arxiv.org/pdf/2104.00650.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://maxbain.com/webvid-dataset/&#34;&gt;[Dataset]&lt;/a&gt; &lt;a href=&#34;https://github.com/m-bain/webvid&#34;&gt;[GitHub]&lt;/a&gt; &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[ECCV 2022] &lt;strong&gt;ROS:&lt;/strong&gt; Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining &lt;a href=&#34;https://arxiv.org/pdf/2204.02393.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/metadriverse/ACO&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://mycuhk-my.sharepoint.com/personal/1155165194_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155165194%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2Fytb%5Fdriving%5Fvideos&amp;amp;ga=1&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[ICLR 2024] &lt;em&gt;&lt;strong&gt;InternVid:&lt;/strong&gt;&lt;/em&gt; A Large-scale Video-Text Dataset for Multimodal Understanding and Generation &lt;a href=&#34;https://arxiv.org/pdf/2307.06942&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[CVPR 2024] &lt;em&gt;&lt;strong&gt;Panda-70M:&lt;/strong&gt;&lt;/em&gt; Captioning 70M Videos with Multiple Cross-Modality Teachers &lt;a href=&#34;https://arxiv.org/pdf/2402.19479.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/Panda-70M&#34;&gt;[Dataset]&lt;/a&gt; &lt;a href=&#34;https://snap-research.github.io/Panda-70M&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[arXiv 2024] &lt;em&gt;&lt;strong&gt;VidProM:&lt;/strong&gt;&lt;/em&gt; A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2403.06098.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/WangWenhao0716/VidProM&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Q&amp;amp;A&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q: The conference sequence of this paper list?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This paper list is organized according to the following sequence: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CVPR&lt;/li&gt; &#xA;     &lt;li&gt;ICCV&lt;/li&gt; &#xA;     &lt;li&gt;ECCV&lt;/li&gt; &#xA;     &lt;li&gt;NeurIPS&lt;/li&gt; &#xA;     &lt;li&gt;ICLR&lt;/li&gt; &#xA;     &lt;li&gt;AAAI&lt;/li&gt; &#xA;     &lt;li&gt;ACM MM&lt;/li&gt; &#xA;     &lt;li&gt;SIGGRAPH&lt;/li&gt; &#xA;     &lt;li&gt;arXiv&lt;/li&gt; &#xA;     &lt;li&gt;Others&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q: What does &lt;code&gt;Others&lt;/code&gt; refers to?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Some of the following studies (e.g., &lt;code&gt;Sora&lt;/code&gt;) does not publish their technical report on arXiv. Instead, they tend to write a blog in their official websites. The &lt;code&gt;Others&lt;/code&gt; category refers to such kind of studies.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;reference.bib&lt;/code&gt; file summarizes bibtex references of up-to-date image inpainting papers, widely used datasets, and toolkits. Based on the original references, I have made the following modifications to make their results look nice in the &lt;code&gt;LaTeX&lt;/code&gt; manuscripts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refereces are normally constructed in the form of &lt;code&gt;author-etal-year-nickname&lt;/code&gt;. Particularly, references of datasets and toolkits are directly constructed as &lt;code&gt;nickname&lt;/code&gt;, e.g., &lt;code&gt;imagenet&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In each reference, all names of conferences/journals are converted into abbreviations, e.g., &lt;code&gt;Computer Vision and Pattern Recognition -&amp;gt; CVPR&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;doi&lt;/code&gt;, &lt;code&gt;publisher&lt;/code&gt;, &lt;code&gt;organization&lt;/code&gt;, &lt;code&gt;editor&lt;/code&gt;, &lt;code&gt;series&lt;/code&gt; in all references are removed.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;pages&lt;/code&gt; of all references are added if they are missing.&lt;/li&gt; &#xA; &lt;li&gt;All paper names are in title case. Besides, I have added an additional &lt;code&gt;{}&lt;/code&gt; to make sure that the title case would also work well in some particular templates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have other demands of reference formats, you may refer to the original references of papers by searching their names in &lt;a href=&#34;https://dblp.org/&#34;&gt;DBLP&lt;/a&gt; or &lt;a href=&#34;https://scholar.google.com/&#34;&gt;Google Scholar&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Star History&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://api.star-history.com/svg?repos=AlonzoLeeeooo/awesome-video-generation&amp;amp;type=Date&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://api.star-history.com/svg?repos=AlonzoLeeeooo/awesome-video-generation&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;ğŸ¯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>calculation-methods/M8O-303B-21</title>
    <updated>2024-03-20T01:36:24Z</updated>
    <id>tag:github.com,2024-03-20:/calculation-methods/M8O-303B-21</id>
    <link href="https://github.com/calculation-methods/M8O-303B-21" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ§Ğœ, Ğ²ĞµÑĞ½Ğ° 2024 Ğ³., 3 ĞºÑƒÑ€Ñ, Ğ³Ñ€. M8Ğ-303Ğ‘-21&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼&lt;/h2&gt; &#xA;&lt;h3&gt;Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-1.zip&#34;&gt;Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° â„–1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-2.zip&#34;&gt;Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° â„–2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-3.zip&#34;&gt;Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° â„–3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-4.zip&#34;&gt;Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° â„–4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ²&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Ğ¤Ğ˜Ğ&lt;/th&gt; &#xA;   &lt;th&gt;Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚&lt;/th&gt; &#xA;   &lt;th&gt;ĞĞ¸ĞºĞ½ĞµĞ¹Ğ¼&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ‘Ğ°Ñ‚ÑƒĞ»Ğ¸Ğ½ Ğ•Ğ²Ğ³ĞµĞ½Ğ¸Ğ¹ ĞĞ½Ğ´Ñ€ĞµĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;uggin-mai&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ‘ĞµĞ»Ğ¾Ğ² Ğ˜Ğ»ÑŒÑ ĞĞ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;xtempleZ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ‘ÑƒĞ´Ğ°Ğ¹Ñ‡Ğ¸ĞµĞ² Ğ“Ğ°Ğ´Ğ¶Ğ¸Ñ€Ğ°ÑÑƒĞ»&amp;nbsp;Ğ¡Ğ¸Ñ€Ğ°Ğ¶ÑƒÑ‚Ğ´Ğ¸Ğ½Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;nI1974In&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ“Ğ¾Ğ»Ğ¾ÑˆÑƒĞ¼Ğ¾Ğ² ĞœĞ¸Ñ…Ğ°Ğ¸Ğ» Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Goloshumovs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ“Ñ€ĞµĞ±Ğ½ĞµĞ²Ğ° ĞĞ»ÑŒĞ³Ğ° Ğ’Ğ»Ğ°Ğ´Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;thirteenames&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ•Ñ€Ğ¾Ñ„ĞµĞµĞ²Ğ° Ğ•ĞºĞ°Ñ‚ĞµÑ€Ğ¸Ğ½Ğ° Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;kmakovtsvet&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ•Ñ€ÑˆĞ¾Ğ² Ğ¡Ñ‚Ğ°Ğ½Ğ¸ÑĞ»Ğ°Ğ² Ğ“Ñ€Ğ¸Ğ³Ğ¾Ñ€ÑŒĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;stasOrel&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ–Ğ°Ğ»ÑĞ»ĞµÑ‚Ğ´Ğ¸Ğ½Ğ¾Ğ² ĞœÑƒÑ€Ğ°Ñ‚ Ğ Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;pokanepridymal&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ–Ğ¸Ğ»Ğ¸Ğ½ ĞœĞ¸Ñ…Ğ°Ğ¸Ğ» Ğ”ĞµĞ½Ğ¸ÑĞ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Krukrukruzhka&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ—Ğ²ĞµÑ€ĞµĞ²Ğ° Ğ•Ğ»Ğ¸Ğ·Ğ°Ğ²ĞµÑ‚Ğ° Ğ›ĞµĞ¾Ğ½Ğ¸Ğ´Ğ¾Ğ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;zxBan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞšĞ¾Ğ²Ğ°Ğ»ĞµĞ² ĞĞ½Ğ´Ñ€ĞµĞ¹ ĞĞ»ĞµĞ³Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;Andrew32516&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞšĞ¾Ğ½ Ğ®Ğ»Ğ¸Ñ Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ²Ğ¾Ğ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;liadiann&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞšÑƒĞ´Ñ€ÑĞ²Ñ†ĞµĞ² ĞĞ½Ğ´Ñ€ĞµĞ¹ Ğ“ĞµĞ¾Ñ€Ğ³Ğ¸ĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;averylongandhardtoreadnickname&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞšÑƒÑ‡Ğ¼Ğ¸ÑÑ‚Ğ¾Ğ² Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸Ğ¹ Ğ Ğ¾Ğ¼Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;Ketchounez&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞœĞ¾Ñ…Ğ½Ğ°Ñ‡ Ğ¢Ğ¸Ğ¼ÑƒÑ€ Ğ’Ğ°Ğ´Ğ¸Ğ¼Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;TypeOfFreak&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞĞ°ÑƒĞ¼Ğ¾Ğ² Ğ“ĞµÑ€Ğ¼Ğ°Ğ½ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ¸Ğ½Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;NaumovGerman&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ĞŸĞµÑ€Ğ²ÑƒÑ…Ğ¸Ğ½ ĞĞ»ĞµĞºÑĞµĞ¹ Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;aaaalioxa&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¡Ğ°Ğ±ÑƒÑ€Ğ¾Ğ²Ğ° Ğ¡ĞµÑ€Ğ°Ñ„Ğ¸Ğ¼Ğ° ĞŸĞ°Ğ²Ğ»Ğ¾Ğ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;SerafimaLil&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¡Ğ°Ğ¹Ñ„ÑƒĞ»Ğ»Ğ¸Ğ½ Ğ˜Ğ»ÑŒĞ´Ğ°Ñ€ ĞšĞ°Ğ¼Ğ¸Ğ»Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;IldarkoS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¡Ğ²Ğ¾ĞµĞ²Ğ¾Ğ»Ğ¸Ğ½ Ğ˜Ğ²Ğ°Ğ½ Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;Svoevolin&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¡ĞµĞ»ĞµĞ·Ğ½ĞµĞ² Ğ˜Ğ»ÑŒÑ Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;selfimprovementslander&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¡ÑƒĞ±Ğ±Ğ¾Ñ‚Ğ¸Ğ½Ğ° ĞœĞ°Ñ€Ğ¸Ñ ĞĞ»ĞµĞºÑĞµĞµĞ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;MarieSu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¡ÑƒĞ»ĞµĞ¹Ğ¼Ğ°Ğ½ÑĞ½ Ğ”ĞµĞ½Ğ¸Ñ Ğ’Ğ°Ğ´Ğ¸Ğ¼Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;DenisSuleymanyan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¢Ñ€Ğ¾Ñ„Ğ¸Ğ¼Ğ¾Ğ² Ğ’Ğ»Ğ°Ğ´Ğ¸ÑĞ»Ğ°Ğ² ĞĞ»ĞµĞ³Ğ¾Ğ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;vampsh00ta&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¢Ñ‹ÑÑÑ‡Ğ½Ñ‹Ğ¹ Ğ’Ğ»Ğ°Ğ´Ğ¸ÑĞ»Ğ°Ğ² Ğ’Ğ°Ğ»ĞµÑ€ÑŒĞµĞ²Ğ¸Ñ‡&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;Bradvurt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¥Ğ°Ğ¹Ñ€ÑƒĞ»Ğ»Ğ¸Ğ½Ğ° Ğ¯ÑĞ¼Ğ¸Ğ½ ĞĞ»Ğ¼Ğ°Ğ·Ğ¾Ğ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td&gt;ysmn-al&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¥Ñ€ÑƒÑˆĞºĞ¾Ğ²Ğ° Ğ’Ğ°Ğ»ĞµÑ€Ğ¸Ñ Ğ’Ğ¸Ñ‚Ğ°Ğ»ÑŒĞµĞ²Ğ½Ğ°&lt;/td&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;lera1kh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ğ¨ĞµÑ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² Ğ•Ğ³Ğ¾Ñ€ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸ĞµĞ²Ğ¸&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;HaPPyDutCHoGGG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ĞÑ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Fortran, C, C++.&lt;/li&gt; &#xA; &lt;li&gt;Ğ”Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².&lt;/li&gt; &#xA; &lt;li&gt;ĞŸĞ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ latex (Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ† Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ² Ğ¿Ğ°Ğ¿ĞºĞµ &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/report&#34;&gt;report&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚ (Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ PDF) Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ´Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Pull request (PR) Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ github.com (&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/#%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D0%BA-%D1%81%D0%B4%D0%B0%D1%87%D0%B8&#34;&gt;Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº ÑĞ´Ğ°Ñ‡Ğ¸&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ğ—Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/github.com&#34;&gt;github.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ğ’Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ´ ÑĞ²Ğ¾ĞµĞ¹ ÑƒÑ‡ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒÑ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/github.com&#34;&gt;github.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ğ¡Ñ‚ÑƒĞ´ĞµĞ½Ñ‚Ñƒ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ€Ğ¾ÑÑ‚Ğµ Ğ¸Ğ¼Ñ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ°. Ğ¡Ñ‚Ğ°Ñ€Ğ¾ÑÑ‚Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ğ¸Ğ· Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ github.&lt;/li&gt; &#xA; &lt;li&gt;Ğ¡Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ (Ğ½Ğ°Ğ¶Ğ°Ñ‚ÑŒ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ &lt;code&gt;Fork&lt;/code&gt; Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ).&lt;/li&gt; &#xA; &lt;li&gt;Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² Ğ¿Ğ°Ğ¿ĞºĞµ &lt;code&gt;stud&lt;/code&gt; ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ² ĞµĞµ Ñ‚Ñ€Ğ°Ğ½ÑĞ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ„Ğ°Ğ¼Ğ¸Ğ»Ğ¸Ğ¸ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ² Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğµ.&lt;/li&gt; &#xA; &lt;li&gt;Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ ÑĞ´Ğ°Ñ‡Ğ¸ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° README.md, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ² ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ Ğ¤Ğ˜Ğ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°, Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ½Ğ¾Ğ¼ĞµÑ€Ñƒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ² ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ. Ğ¤Ğ°Ğ¼Ğ¸Ğ»Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ² Ğ°Ğ»Ñ„Ğ°Ğ²Ğ¸Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ¸Ğ¼ĞµÑÑ‰ĞµĞ³Ğ¾ÑÑ ÑĞ¿Ğ¸ÑĞºĞ°.&lt;/li&gt; &#xA;   &lt;li&gt;Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¿. 3-7 &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/#%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D0%BA-%D1%81%D0%B4%D0%B0%D1%87%D0%B8&#34;&gt;Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº ÑĞ´Ğ°Ñ‡Ğ¸&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº ÑĞ´Ğ°Ñ‡Ğ¸&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ĞĞ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ.&lt;/li&gt; &#xA; &lt;li&gt;ĞŸĞ¾Ğ¼ĞµÑÑ‚Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ğ°Ğ¿ĞºÑƒ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸.&lt;/li&gt; &#xA; &lt;li&gt;Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ (Pull Request - PR) Ñ Ğ²ĞµÑ‚ĞºĞ¸, Ğ¸Ğ¼ĞµÑÑ‰ĞµĞ¹ Ğ¸Ğ¼Ñ, Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ Ğ¸Ğ¼ĞµĞ½Ğ¸ &lt;strong&gt;master&lt;/strong&gt;, Ğ½Ğ° Ğ²ĞµÑ‚ĞºÑƒ &lt;strong&gt;master&lt;/strong&gt; ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†ĞµĞ¹ Ğ¿Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñƒ &#34;Surname Name, lab 1, #1,...&#34;, Ğ³Ğ´Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ Ğ¿Ğ¸ÑˆÑƒÑ‚ÑÑ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ñ‹&lt;/li&gt; &#xA;   &lt;li&gt;ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Reviewer &lt;em&gt;pivovarov-mai&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Ğ¾Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ (Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¾ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ)&lt;/li&gt; &#xA;   &lt;li&gt;ÑĞ½Ğ°Ğ±Ğ´Ğ¸Ñ‚ÑŒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° (ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ›Ğ )&lt;/li&gt; &#xA;   &lt;li&gt;ÑĞ½Ğ°Ğ±Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ &lt;em&gt;milestone&lt;/em&gt; Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ°&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ĞĞ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ¿Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ĞµĞ»Ñ (ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸, Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½Ğ¸Ñ).&lt;/li&gt; &#xA; &lt;li&gt;Ğ£Ñ‡ĞµÑÑ‚ÑŒ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹.&lt;/li&gt; &#xA; &lt;li&gt;ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ¿Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞµĞµ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹.&lt;/li&gt; &#xA; &lt;li&gt;Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ ÑĞ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ² ÑÑ€Ğ¾Ğº, ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² &lt;em&gt;milestone&lt;/em&gt; ÑÑ€Ğ¾ĞºĞ°.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ğ¡Ñ€Ğ¾Ğº ÑĞ´Ğ°Ñ‡Ğ¸&lt;/li&gt; &#xA; &lt;li&gt;ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº ÑĞ´Ğ°Ñ‡Ğ¸&lt;/li&gt; &#xA; &lt;li&gt;ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ‚Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ&lt;/li&gt; &#xA; &lt;li&gt;Ğ¡Ğ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ&lt;/li&gt; &#xA; &lt;li&gt;ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°&lt;/li&gt; &#xA; &lt;li&gt;ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ&lt;/li&gt; &#xA; &lt;li&gt;ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ&lt;/li&gt; &#xA; &lt;li&gt;Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ git&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ&lt;/h3&gt; &#xA;&lt;p&gt;ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¸Ñ… PR ĞºĞ°ĞºĞ¸Ğµ-Ğ»Ğ¸Ğ±Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ² &lt;em&gt;Discussions&lt;/em&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>