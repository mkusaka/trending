<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-15T01:42:00Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction</title>
    <updated>2022-10-15T01:42:00Z</updated>
    <id>tag:github.com,2022-10-15:/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction</id>
    <link href="https://github.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A selection of state-of-the-art research materials on trajectory prediction&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Interaction-aware Behavior and Trajectory Prediction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Version-1.1-ff69b4.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/LastUpdated-2020.11-lightgrey.svg?sanitize=true&#34; alt=&#34;LastUpdated&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Topic-trajectory--prediction-yellow.svg?logo=github&#34; alt=&#34;Topic&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a checklist of state-of-the-art research materials (datasets, blogs, papers and public codes) related to trajectory prediction. Wish it could be helpful for both academia and industry. (Still updating)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maintainers&lt;/strong&gt;: &lt;a href=&#34;https://jiachenli94.github.io&#34;&gt;&lt;strong&gt;Jiachen Li&lt;/strong&gt;&lt;/a&gt;, Hengbo Ma, Jinning Li (University of California, Berkeley)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Emails&lt;/strong&gt;: {jiachen_li, hengbo_ma, jinning_li}@berkeley.edu&lt;/p&gt; &#xA;&lt;p&gt;Please feel free to pull request to add new resources or send emails to us for questions, discussion and collaborations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href=&#34;https://github.com/jiachenli94/Awesome-Decision-Making-Reinforcement-Learning&#34;&gt;&lt;strong&gt;Here&lt;/strong&gt;&lt;/a&gt; is also a collection of materials for reinforcement learning, decision making and motion planning.&lt;/p&gt; &#xA;&lt;p&gt;Please consider citing our work if you found this repo useful:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{li2020evolvegraph,&#xA;  title={EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning},&#xA;  author={Li, Jiachen and Yang, Fan and Tomizuka, Masayoshi and Choi, Chiho},&#xA;  booktitle={2020 Advances in Neural Information Processing Systems (NeurIPS)},&#xA;  year={2020}&#xA;}&#xA;&#xA;@inproceedings{li2019conditional,&#xA;  title={Conditional Generative Neural System for Probabilistic Trajectory Prediction},&#xA;  author={Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},&#xA;  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},&#xA;  pages={6150--6156},&#xA;  year={2019},&#xA;  organization={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#datasets&#34;&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#vehicles-and-traffic&#34;&gt;Vehicles and Traffic&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#pedestrians&#34;&gt;Pedestrians&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#sport-players&#34;&gt;Sport Players&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#literature-and-codes&#34;&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#survey-papers&#34;&gt;Survey Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#physics-systems-with-interaction&#34;&gt;Physics Systems with Interaction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#intelligent-vehicles-traffic&#34;&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#pedestrians-1&#34;&gt;Pedestrians&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#mobile-robots&#34;&gt;Mobile Robots&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#sport-players&#34;&gt;Sport Players&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#benchmark-and-evaluation-metrics&#34;&gt;Benchmark and Evaluation Metrics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiachenli94/Awesome-Interaction-Aware-Trajectory-Prediction/master/#others&#34;&gt;Others&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;!-- /TOC --&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Vehicles and Traffic&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Agents&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Scenarios&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sensors&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.interaction-dataset.com/&#34;&gt;INTERACTION&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Roundabout / intersection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/&#34;&gt;KITTI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highway / rural areas&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera / LiDAR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.highd-dataset.com/&#34;&gt;HighD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highway&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm&#34;&gt;NGSIM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highway&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html&#34;&gt;Cyclists&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cyclists&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.nuscenes.org/&#34;&gt;nuScenes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera / LiDAR / RADAR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://bdd-data.berkeley.edu/&#34;&gt;BDD100k&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists / people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highway / urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://apolloscape.auto/?source=post_page---------------------------&#34;&gt;Apolloscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists / people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/udacity/self-driving-car/tree/master/datasets&#34;&gt;Udacity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://cvgl.stanford.edu/projects/uav_data/&#34;&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.argoverse.org/&#34;&gt;Argoverse&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera / LiDAR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://gamma.umd.edu/researchdirections/autonomousdriving/trafdataset&#34;&gt;TRAF&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles/buses/cyclists/bikes / people/animals&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://level5.lyft.com/dataset/&#34;&gt;Lyft Level 5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles/cyclists/people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera/ LiDAR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.5724486&#34;&gt;Aschaffenburg Pose Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cyclists/people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pedestrians&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Agents&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Scenarios&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sensors&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data&#34;&gt;UCY&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Zara / students&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://icu.ee.ethz.ch/research/datsets.html&#34;&gt;ETH (ICCV09)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.viratdata.org/&#34;&gt;VIRAT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People / vehicles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/&#34;&gt;KITTI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highway / rural areas&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera / LiDAR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://irc.atr.jp/crest2010_HRI/ATC_dataset/&#34;&gt;ATC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Shopping center&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Range sensor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html&#34;&gt;Daimler&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;From moving vehicle&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html&#34;&gt;Central Station&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Inside station&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets&#34;&gt;Town Center&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban street&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/&#34;&gt;Edinburgh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/login/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.argoverse.org/&#34;&gt;Argoverse&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera / LiDAR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://cvgl.stanford.edu/projects/uav_data/&#34;&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vehicles / cyclists/ people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://trajnet.stanford.edu/&#34;&gt;TrajNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://data.nvision2.eecs.yorku.ca/PIE_dataset/&#34;&gt;PIE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://next.cs.cmu.edu/multiverse/index.html&#34;&gt;ForkingPaths&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban / Simulation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aicrowd.com/challenges/trajnet-a-trajectory-forecasting-challenge&#34;&gt;TrajNet++&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.5724486&#34;&gt;Aschaffenburg Pose Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cyclists/people&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Urban&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sport Players&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Agents&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Scenarios&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sensors&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://datahub.io/collections/football&#34;&gt;Football&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Football field&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/linouk23/NBA-Player-Movements&#34;&gt;NBA SportVU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Basketball Hall&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/a-vhadgar/Big-Data-Bowl&#34;&gt;NFL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;People&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;American Football&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Camera&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Survey Papers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modeling and Prediction of Human Driver Behavior: A Survey, 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.08832&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Human Motion Trajectory Prediction: A Survey, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1905.06113&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A literature review on the prediction of pedestrian behavior in urban scenarios, ITSC 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8569415&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Survey on Vision-Based Path Prediction. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-91131-1_4&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Autonomous vehicles that interact with pedestrians: A survey of theory and practice. [&lt;a href=&#34;https://arxiv.org/abs/1805.11773&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Trajectory data mining: an overview. [&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=2743025&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A survey on motion prediction and risk assessment for intelligent vehicles. [&lt;a href=&#34;https://robomechjournal.springeropen.com/articles/10.1186/s40648-014-0001-z&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Physics Systems with Interaction&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13924&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Interaction Templates for Multi-Robot Systems, IROS 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8737744/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Factorised Neural Relational Inference for Multi-Interaction Systems, ICML workshop 2019. [&lt;a href=&#34;https://arxiv.org/abs/1905.08721v1&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ekwebb/fNRI&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video, 2019. [&lt;a href=&#34;https://arxiv.org/pdf/1905.11169v1.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Neural Relational Inference for Interacting Systems, ICML 2018. [&lt;a href=&#34;https://arxiv.org/abs/1802.04687v2&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ethanfetaya/NRI&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks, UAI 2018. [&lt;a href=&#34;http://arxiv.org/abs/1807.09244v2&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Relational inductive biases, deep learning, and graph networks, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1806.01261v3&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions, ICLR 2018. [&lt;a href=&#34;http://arxiv.org/abs/1802.10353v1&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Graph networks as learnable physics engines for inference and control, ICML 2018. [&lt;a href=&#34;http://arxiv.org/abs/1806.01242v1&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Flexible Neural Representation for Physics Prediction, 2018. [&lt;a href=&#34;http://arxiv.org/abs/1806.08047v2&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A simple neural network module for relational reasoning, 2017. [&lt;a href=&#34;http://arxiv.org/abs/1706.01427v1&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;VAIN: Attentional Multi-agent Predictive Modeling, NIPS 2017. [&lt;a href=&#34;https://arxiv.org/pdf/1706.06122.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Visual Interaction Networks, 2017. [&lt;a href=&#34;http://arxiv.org/abs/1706.01433v1&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A Compositional Object-Based Approach to Learning Physical Dynamics, ICLR 2017. [&lt;a href=&#34;http://arxiv.org/abs/1612.00341v2&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Interaction Networks for Learning about Objects, Relations and Physics, 2016. [&lt;a href=&#34;https://arxiv.org/abs/1612.00222&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/higgsfield/interaction_network_pytorch&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13924&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;V2VNet- Vehicle-to-Vehicle Communication for Joint Perception and Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2008.07519&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;SMART- Simultaneous Multi-Agent Recurrent Trajectory Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.13078&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;SimAug- Learning Robust Representations from Simulation for Trajectory Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.02022&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Lane Graph Representations for Motion Forecasting, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.13732&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Implicit Latent Variable Model for Scene-Consistent Motion Forecasting, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.12036&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Diverse and Admissible Trajectory Forecasting through Multimodal Context Understanding, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03212&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Semantic Synthesis of Pedestrian Locomotion, ACCV 2020. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ACCV2020/html/Priisalu_Semantic_Synthesis_of_Pedestrian_Locomotion_ACCV_2020_paper.html&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Kernel Trajectory Maps for Multi-Modal Probabilistic Motion Prediction, CoRL 2019. [&lt;a href=&#34;https://arxiv.org/abs/1907.05127&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wzhi/KernelTrajectoryMaps&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein Graph Double-Attention Network, 2020. [&lt;a href=&#34;https://arxiv.org/abs/2002.06241&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs, 2019. [&lt;a href=&#34;https://arxiv.org/pdf/1912.01118.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://gamma.umd.edu/researchdirections/autonomousdriving/spectralcows/&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Looking to Relations for Future Trajectory Forecast, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Jointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles, IROS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.04586&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Sharing Is Caring: Socially-Compliant Autonomous Intersection Negotiation, IROS 2019. [&lt;a href=&#34;https://pdfs.semanticscholar.org/f4b2/021353bba52224eb33923b3b98956e2c9821.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;INFER: INtermediate Representations for FuturE PRediction, IROS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1903.10641&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/talsperre/INFER&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Deep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules, IROS 2019. [&lt;a href=&#34;http://rllab.snu.ac.kr/publications/papers/2019_iros_predstl.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;NeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles, IROS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1906.10971&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Urban Street Trajectory Prediction with Multi-Class LSTM Networks, IROS 2019. [N/A]&lt;/li&gt; &#xA; &lt;li&gt;Spatiotemporal Learning of Directional Uncertainty in Urban Environments with Kernel Recurrent Mixture Density Networks, IROS 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8772158&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Conditional generative neural system for probabilistic trajectory prediction, IROS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1905.01631&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Interaction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning, ICRA 2019. [&lt;a href=&#34;https://arxiv.org/abs/1904.02390&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving, IEEE Trans. Intell. Transport. Systems, 2019. [&lt;a href=&#34;https://www.researchgate.net/publication/334560415_Generic_Tracking_and_Probabilistic_Prediction_Framework_and_Its_Application_in_Autonomous_Driving&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Coordination and trajectory prediction for vehicle interactions via bayesian generative modeling, IV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1905.00587&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Wasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction, IV 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8813783&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;GRIP: Graph-based Interaction-aware Trajectory Prediction, ITSC 2019. [&lt;a href=&#34;https://arxiv.org/abs/1907.07792&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;AGen: Adaptable Generative Prediction Networks for Autonomous Driving, IV 2019. [&lt;a href=&#34;http://www.cs.cmu.edu/~cliu6/files/iv19-1.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/rohanchandra30/TrackNPred&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks, CVPR 2019. [&lt;a href=&#34;https://arxiv.org/pdf/1812.09395.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Argoverse: 3D Tracking and Forecasting With Rich Maps, CVPR 2019 [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Robust Aleatoric Modeling for Future Vehicle Localization, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrian occupancy prediction for autonomous vehicles, IRC 2019. [paper]&lt;/li&gt; &#xA; &lt;li&gt;Context-based path prediction for targets with switching dynamics, 2019.[&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-018-1104-4&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Deep Imitative Models for Flexible Inference, Planning, and Control, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1810.06544&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Infer: Intermediate representations for future prediction, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1903.10641&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/talsperre/INFER&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Multi-agent tensor fusion for contextual trajectory prediction, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1904.04776&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Context-Aware Pedestrian Motion Prediction In Urban Intersections, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1806.09453&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generic probabilistic interactive situation recognition and prediction: From virtual to real, ITSC 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8569780&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generic vehicle tracking framework capable of handling occlusions based on modified mixture particle filter, IV 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8500626&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1805.05499&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Sequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1802.06338&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;R2P2: A ReparameteRized Pushforward Policy for diverse, precise generative path forecasting, ECCV 2018. [&lt;a href=&#34;https://www.cs.cmu.edu/~nrhineha/R2P2.html&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Predicting trajectories of vehicles using large-scale motion priors, IV 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8500604&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Vehicle trajectory prediction by integrating physics-and maneuver based approaches using interactive multiple models, 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8186191&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1808.05819v1&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generative multi-agent behavioral cloning, 2018. [&lt;a href=&#34;https://www.semanticscholar.org/paper/Generative-Multi-Agent-Behavioral-Cloning-Zhan-Zheng/ccc196ada6ec9cad1e418d7321b0cd6813d9b261&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Deep Sequence Learning with Auxiliary Information for Traffic Prediction, KDD 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1806.07380.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/JingqingZ/BaiduTraffic&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction, 2017. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s10514-017-9619-z&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Probabilistic long-term prediction for autonomous vehicles, IV 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/7995726&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network, ITSC 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6632960&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Desire: Distant future prediction in dynamic scenes with interacting agents, CVPR 2017. [&lt;a href=&#34;https://arxiv.org/abs/1704.04394&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/yadrimz/DESIRE&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Imitating driver behavior with generative adversarial networks, 2017. [&lt;a href=&#34;https://arxiv.org/abs/1701.06699&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/sisl/gail-driver&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Infogail: Interpretable imitation learning from visual demonstrations, 2017. [&lt;a href=&#34;https://arxiv.org/abs/1703.08840&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/YunzhuLi/InfoGAIL&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Long-term planning by short-term prediction, 2017. [&lt;a href=&#34;https://arxiv.org/abs/1602.01580&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Long-term path prediction in urban scenarios using circular distributions, 2017. [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0262885617301853&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Deep learning driven visual path prediction from a single image, 2016. [&lt;a href=&#34;https://arxiv.org/abs/1601.07265&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Understanding interactions between traffic participants based on learned behaviors, 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7535554&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Visual path prediction in complex scenes with crowded moving objects, CVPR 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/7780661/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A game-theoretic approach to replanning-aware interactive scene prediction and planning, 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7353203&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Intention-aware online pomdp planning for autonomous driving in a crowd, ICRA 2015. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7139219&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6856480&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Patch to the future: Unsupervised visual prediction, CVPR 2014. [&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/6909818/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Mobile agent trajectory prediction using bayesian nonparametric reachability trees, 2011. [&lt;a href=&#34;https://dspace.mit.edu/handle/1721.1/114899&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pedestrians&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting, CVPR 2022, [&lt;a href=&#34;https://arxiv.org/abs/2203.04781&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pose and Semantic Map Based Probabilistic Forecast of Vulnerable Road Usersâ€™ Trajectories, 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.02598&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs, ICCV 2021 The ROAD Challenge Workshop. [&lt;a href=&#34;https://arxiv.org/pdf/2109.10257.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/abduallahmohamed/Skeleton-Graph&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Structured Representations of Spatial and Interactive Dynamics for Trajectory Prediction in Crowded Scenes, IEEE Robotics and Automation Letters 2021 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9309332&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/tdavchev/structured-trajectory-prediction&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social NCE: Contrastive Learning of Socially-aware Motion Representations. [&lt;a href=&#34;https://arxiv.org/abs/2012.11717&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/vita-epfl/social-nce&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pose Based Trajectory Forecast of Vulnerable Road Users Using Recurrent Neural Networks, ICPR International Workshops and Challenges 2020. [&lt;a href=&#34;https://www.springerprofessional.de/pose-based-trajectory-forecast-of-vulnerable-road-users-using-re/18885576&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13924&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.08514&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;It is not the Journey but the Destination- Endpoint Conditioned Trajectory Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.02025&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;How Can I See My Future? FvTraj: Using First-person View for Pedestrian Trajectory Prediction, ECCV 2020. [&lt;a href=&#34;http://graphics.cs.uh.edu/wp-content/papers/2020/2020-ECCV-PedestrianTrajPrediction.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Dynamic and Static Context-aware LSTM for Multi-agent Motion Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2008.00777&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Human Trajectory Forecasting in Crowds: A Deep Learning Perspective, 2020. [&lt;a href=&#34;https://arxiv.org/pdf/2007.03639.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/vita-epfl/trajnetplusplusbaselines&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;SimAug: Learning Robust Representations from 3D Simulation for Pedestrian Trajectory Prediction in Unseen Cameras, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/pdf/2004.02022&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/JunweiLiang/Multiverse&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting, ICPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.12661&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexmonti19/dagnet&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision, WACV 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.01138&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein Graph Double-Attention Network, 2020. [&lt;a href=&#34;https://arxiv.org/abs/2002.06241&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction, CVPR 2020. [&lt;a href=&#34;https://arxiv.org/pdf/2002.11927.pdf&#34;&gt;Paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/abduallahmohamed/Social-STGCNN/&#34;&gt;Code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction, CVPR 2020. [&lt;a href=&#34;https://arxiv.org/pdf/1912.06445.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://next.cs.cmu.edu/multiverse/index.html&#34;&gt;code/dataset&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision, WACV 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.01138&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pose Based Trajectory Forecast of Vulnerable Road Users, SSCI 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/9003023&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordASL/Trajectron&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/huang-xx/STGAT&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social and Scene-Aware Trajectory Prediction in Crowded Spaces, ICCV workshop 2019. [&lt;a href=&#34;https://arxiv.org/pdf/1909.08840.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Oghma/sns-lstm/&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Stochastic Sampling Simulation for Pedestrian Trajectory Prediction, IROS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1903.01860&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Long-Term Prediction of Motion Trajectories Using Path Homology Clusters, IROS 2019. [&lt;a href=&#34;http://www.csc.kth.se/~fpokorny/static/publications/carvalho2019a.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;StarNet: Pedestrian Trajectory Prediction Using Deep Neural Network in Star Topology, IROS 2019. [&lt;a href=&#34;https://arxiv.org/pdf/1906.01797.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Generative Socially-Aware Models of Pedestrian Motion, IROS 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8760356/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model, CVWW 2019. [&lt;a href=&#34;https://arxiv.org/pdf/1902.05437.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Path predictions using object attributes and semantic environment, VISIGRAPP 2019. [&lt;a href=&#34;http://mprg.jp/data/MPRG/C_group/C20190225_minoura.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Probabilistic Path Planning using Obstacle Trajectory Prediction, CoDS-COMAD 2019. [&lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3297006&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Human Trajectory Prediction using Adversarial Loss, hEART 2019. [&lt;a href=&#34;http://www.strc.ch/2019/Kothari_Alahi.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/vita-epfl/AdversarialLoss-SGAN&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs, CVPR 2019. [&lt;a href=&#34;https://sites.google.com/view/ieeecvf-cvpr2019-precognition&#34;&gt;&lt;em&gt;Precognition Workshop&lt;/em&gt;&lt;/a&gt;], [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/amiryanj/socialways&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Peeking into the Future: Predicting Future Person Activities and Locations in Videos, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPR_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/google/next-prediction&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning to Infer Relations for Future Trajectory Forecast, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Choi_Learning_to_Infer_Relations_for_Future_Trajectory_Forecast_CVPRW_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Which Way Are You Going? Imitative Decision Learning for Path Forecasting in Dynamic Scenes, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Which_Way_Are_You_Going_Imitative_Decision_Learning_for_Path_CVPR_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Makansi_Overcoming_Limitations_of_Mixture_Density_Networks_A_Sampling_and_Fitting_CVPR_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/lmb-freiburg/Multimodal-Future-Prediction&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Sophie: An attentive gan for predicting paths compliant to social and physical constraints, CVPR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1806.01482&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/hindupuravinash/the-gan-zoo/raw/master/README.md&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition, 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8370119/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1808.06887&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;The simpler the better: Constant velocity for pedestrian motion prediction, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1903.07933&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrian trajectory prediction in extremely crowded scenarios, 2019. [&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/30862018&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Srlstm: State refinement for lstm towards pedestrian trajectory prediction, 2019. [&lt;a href=&#34;https://arxiv.org/abs/1903.02793&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Location-velocity attention for pedestrian trajectory prediction, WACV 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8659060&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrian Trajectory Prediction in Extremely Crowded Scenarios, Sensors, 2019. [&lt;a href=&#34;https://www.mdpi.com/1424-8220/19/5/1223/pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A data-driven model for interaction-aware pedestrian motion prediction in object cluttered environments, ICRA 2018. [&lt;a href=&#34;https://arxiv.org/abs/1709.08528&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Move, Attend and Predict: An attention-based neural model for peopleâ€™s movement prediction, Pattern Recognition Letters 2018. [&lt;a href=&#34;https://reader.elsevier.com/reader/sd/pii/S016786551830182X?token=1EF2B664B70D2B0C3ECDD07B6D8B664F5113AEA7533CE5F0B564EF9F4EE90D3CC228CDEB348F79FEB4E8CDCD74D4BA31&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds, ACCV 2018, [&lt;a href=&#34;https://arxiv.org/pdf/1812.07667.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://www.youtube.com/watch?v=7cCIC_JIfms&#34;&gt;demo&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Ss-lstm: a hierarchical lstm model for pedestrian trajectory prediction, WACV 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8354239&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social Attention: Modeling Attention in Human Crowds, ICRA 2018. [&lt;a href=&#34;https://arxiv.org/abs/1710.04689&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/TNTant/social_lstm&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrian prediction by planning using deep neural networks, ICRA 2018. [&lt;a href=&#34;https://arxiv.org/abs/1706.05904&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Joint long-term prediction of human motion using a planning-based social force approach, ICRA 2018. [&lt;a href=&#34;https://iliad-project.eu/publications/2018-2/joint-long-term-prediction-of-human-motion-using-a-planning-based-social-force-approach/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Human motion prediction under social grouping constraints, IROS 2018. [&lt;a href=&#34;http://iliad-project.eu/publications/2018-2/human-motion-prediction-under-social-grouping-constraints/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Future Person Localization in First-Person Videos, CVPR 2018. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2018/papers/Yagi_Future_Person_Localization_CVPR_2018_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks, CVPR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1803.10892&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/agrimgupta92/sgan&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Group LSTM: Group Trajectory Prediction in Crowded Scenarios, ECCV 2018. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-11015-4_18&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses, CVPR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1805.00652&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Intent prediction of pedestrians via motion trajectories using stacked recurrent neural networks, 2018. [&lt;a href=&#34;http://ieeexplore.ieee.org/document/8481390/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Transferable pedestrian motion prediction models at intersections, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1804.00495&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Probabilistic map-based pedestrian motion prediction taking traffic participants into consideration, 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8500562&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A Computationally Efficient Model for Pedestrian Motion Prediction, ECC 2018. [&lt;a href=&#34;https://arxiv.org/abs/1803.04702&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Context-aware trajectory prediction, ICPR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1705.02503&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Set-based prediction of pedestrians in urban environments considering formalized traffic rules, ITSC 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8569434&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Building prior knowledge: A markov based pedestrian prediction model using urban environmental data, ICARCV 2018. [&lt;a href=&#34;https://arxiv.org/abs/1809.06045&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Depth Information Guided Crowd Counting for Complex Crowd Scenes, 2018. [&lt;a href=&#34;https://arxiv.org/abs/1803.02256&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Tracking by Prediction: A Deep Generative Model for Mutli-Person Localisation and Tracking, WACV 2018. [&lt;a href=&#34;https://arxiv.org/abs/1803.03347&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;â€œSeeing is Believingâ€: Pedestrian Trajectory Forecasting Using Visual Frustum of Attention, WACV 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8354238&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty, CVPR 2018. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/apratimbhattacharyya18/onboard_long_term_prediction&#34;&gt;code+data&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Encoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction, CVPR 2018. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.pdf&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/ShanghaiTechCVDL/CIDNN&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Walking Ahead: The Headed Social Force Model, 2017. [&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169734&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Real-time certified probabilistic pedestrian forecasting, 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7959047&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A multiple-predictor approach to human motion prediction, ICRA 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7989265&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Forecasting interactive dynamics of pedestrians with fictitious play, CVPR 2017. [&lt;a href=&#34;https://arxiv.org/abs/1604.01431&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Forecast the plausible paths in crowd scenes, IJCAI 2017. [&lt;a href=&#34;https://www.ijcai.org/proceedings/2017/386&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Bi-prediction: pedestrian trajectory prediction based on bidirectional lstm classification, DICTA 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8227412/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Aggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos, IJCAI 2017. [&lt;a href=&#34;https://www.ijcai.org/proceedings/2017/17&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Natural vision based method for predicting pedestrian behaviour in urban environments, ITSC 2017. [&lt;a href=&#34;http://ieeexplore.ieee.org/document/8317848/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Human Trajectory Prediction using Spatially aware Deep Attention Models, 2017. [&lt;a href=&#34;https://arxiv.org/pdf/1705.09436.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection, 2017. [&lt;a href=&#34;https://arxiv.org/pdf/1702.05552.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Forecasting Interactive Dynamics of Pedestrians with Fictitious Play, CVPR 2017. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_Forecasting_Interactive_Dynamics_CVPR_2017_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social LSTM: Human trajectory prediction in crowded spaces, CVPR 2016. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2016/html/Alahi_Social_LSTM_Human_CVPR_2016_paper.html&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/vita-epfl/trajnetplusplusbaselines&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Comparison and evaluation of pedestrian motion models for vehicle safety systems, ITSC 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7795912&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Age and Group-driven Pedestrian Behaviour: from Observations to Simulations, 2016. [&lt;a href=&#34;https://collective-dynamics.eu/index.php/cod/article/view/A3&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Structural-RNN: Deep learning on spatio-temporal graphs, CVPR 2016. [&lt;a href=&#34;https://arxiv.org/abs/1511.05298&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/asheshjain399/RNNexp&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Intent-aware long-term prediction of pedestrian motion, ICRA 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7487409&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Context-based detection of pedestrian crossing intention for autonomous driving in urban environments, IROS 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/7759351/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Novel planning-based algorithms for human motion prediction, ICRA 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7487505&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning social etiquette: Human trajectory understanding in crowded scenes, ECCV 2016. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/SajjadMzf/Pedestrian_Datasets_VIS&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;GLMP-realtime pedestrian path prediction using global and local movement patterns, ICRA 2016. [&lt;a href=&#34;http://ieeexplore.ieee.org/document/7487768/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Knowledge transfer for scene-specific motion prediction, ECCV 2016. [&lt;a href=&#34;https://arxiv.org/abs/1603.06987&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;STF-RNN: Space Time Features-based Recurrent Neural Network for predicting People Next Location, SSCI 2016. [&lt;a href=&#34;https://github.com/mhjabreel/STF-RNN&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Goal-directed pedestrian prediction, ICCV 2015. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7406377&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Trajectory analysis and prediction for improved pedestrian safety: Integrated framework and evaluations, 2015. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7225707&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Predicting and recognizing human interactions in public spaces, 2015. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11554-014-0428-8&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning collective crowd behaviors with dynamic pedestrian-agents, 2015. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-014-0735-3&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Modeling spatial-temporal dynamics of human movements for predicting future trajectories, AAAI 2015. [&lt;a href=&#34;https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10126&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised robot learning to predict person motion, ICRA 2015. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7139254&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;A controlled interactive multiple model filter for combined pedestrian intention recognition and path prediction, ITSC 2015. [&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7313129/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Real-Time Predictive Modeling and Robust Avoidance of Pedestrians with Uncertain, Changing Intentions, 2014. [&lt;a href=&#34;https://arxiv.org/abs/1405.5581&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Behavior estimation for a complete framework for human motion prediction in crowded environments, ICRA 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6907734&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrianâ€™s trajectory forecast in public traffic with artificial neural network, ICPR 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6977417&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Will the pedestrian cross? A study on pedestrian path prediction, 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6632960&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;BRVO: Predicting pedestrian trajectories using velocity-space reasoning, 2014. [&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0278364914555543&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Context-based pedestrian path prediction, ECCV 2014. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-10599-4_40&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Pedestrian path prediction using body language traits, 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6856498/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6856480&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning intentions for improved human motion prediction, 2013. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6766565&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Mobile Robots&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anticipatory Navigation in Crowds by Probabilistic Prediction of Pedestrian Future Movements, ICRA 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.06235&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Social NCE: Contrastive Learning of Socially-aware Motion Representations. [&lt;a href=&#34;https://arxiv.org/abs/2012.11717&#34;&gt;paper&lt;/a&gt;], [&lt;a href=&#34;https://github.com/vita-epfl/social-nce&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Multimodal probabilistic model-based planning for human-robot interaction, ICRA 2018. [&lt;a href=&#34;https://arxiv.org/abs/1710.09483&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/StanfordASL/TrafficWeavingCVAE&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning, ICRA 2017. [&lt;a href=&#34;https://arxiv.org/abs/1609.07845&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Augmented dictionary learning for motion prediction, ICRA 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7487407&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Predicting future agent motions for dynamic environments, ICMLA 2016. [&lt;a href=&#34;https://www.semanticscholar.org/paper/Predicting-Future-Agent-Motions-for-Dynamic-Previtali-Bordallo/2df8179ac7b819bad556b6d185fc2030c40f98fa&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Bayesian intention inference for trajectory prediction with an unknown goal destination, IROS 2015. [&lt;a href=&#34;http://ieeexplore.ieee.org/abstract/document/7354203/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning to predict trajectories of cooperatively navigating agents, ICRA 2014. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/6907442&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Sport Players&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13924&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Imitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation, CVPR 2020. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/html/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.html&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting, ICPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.12661&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexmonti19/dagnet&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Diverse Generation for Multi-Agent Sports Games, CVPR 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/html/Yeh_Diverse_Generation_for_Multi-Agent_Sports_Games_CVPR_2019_paper.html&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Stochastic Prediction of Multi-Agent Interactions from Partial Observations, ICLR 2019. [&lt;a href=&#34;http://arxiv.org/abs/1902.09641v1&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generating Multi-Agent Trajectories using Programmatic Weak Supervision, ICLR 2019. [&lt;a href=&#34;http://arxiv.org/abs/1803.07612v6&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generative Multi-Agent Behavioral Cloning, ICML 2018. [&lt;a href=&#34;http://www.stephanzheng.com/pdf/Zhan_Zheng_Lucey_Yue_Generative_Multi_Agent_Behavioral_Cloning.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Where Will They Go? Predicting Fine-Grained Adversarial Multi-Agent Motion using Conditional Variational Autoencoders, ECCV 2018. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Panna_Felsen_Where_Will_They_ECCV_2018_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Coordinated Multi-Agent Imitation Learning, ICML 2017. [&lt;a href=&#34;http://arxiv.org/abs/1703.03121v2&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generating long-term trajectories using deep hierarchical networks, 2017. [&lt;a href=&#34;https://arxiv.org/abs/1706.07138&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Learning Fine-Grained Spatial Models for Dynamic Sports Play Prediction, ICDM 2014. [&lt;a href=&#34;http://www.yisongyue.com/publications/icdm2014_bball_predict.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generative Modeling of Multimodal Multi-Human Behavior, 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1803.02015.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;What will Happen Next? Forecasting Player Moves in Sports Videos, ICCV 2017, [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2017/papers/Felsen_What_Will_Happen_ICCV_2017_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Benchmark and Evaluation Metrics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation, ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.03057&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/abduallahmohamed/Social-Implicit&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;OpenTraj: Assessing Prediction Complexity in Human Trajectories Datasets, ACCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2010.00890&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/crowdbotp/OpenTraj&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Testing the Safety of Self-driving Vehicles by Simulating Perception and Prediction, ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2008.06020&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction, ICCV 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Towards a fatality-aware benchmark of probabilistic reaction prediction in highly interactive driving scenarios, ITSC 2018. [&lt;a href=&#34;https://arxiv.org/abs/1809.03478&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;How good is my prediction? Finding a similarity measure for trajectory prediction evaluation, ITSC 2017. [&lt;a href=&#34;http://ieeexplore.ieee.org/document/8317825/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Trajnet: Towards a benchmark for human trajectory prediction. [&lt;a href=&#34;http://trajnet.epfl.ch/&#34;&gt;website&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pose Based Start Intention Detection of Cyclists, ITSC 2019. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8917215&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Cyclist trajectory prediction using bidirectional recurrent neural networks, AI 2018. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-03991-2_28&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Road infrastructure indicators for trajectory prediction, 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8500678&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Using road topology to improve cyclist path prediction, 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7995734/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Trajectory prediction of cyclists using a physical model and an artificial neural network, 2016. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/7535484/&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>vinayprabhu/X-is-all-you-need</title>
    <updated>2022-10-15T01:42:00Z</updated>
    <id>tag:github.com,2022-10-15:/vinayprabhu/X-is-all-you-need</id>
    <link href="https://github.com/vinayprabhu/X-is-all-you-need" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A survey of all the &#39;X is all you need&#39;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;X is all you need&lt;/h1&gt; &#xA;&lt;p&gt;Co-creators: &lt;a href=&#34;https://twitter.com/vinayprabhu&#34;&gt;Vinay Uday Prabhu&lt;/a&gt; and Ryan Teehan&lt;/p&gt; &#xA;&lt;p&gt;Citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{prabhu2020spices,&#xA;  title={SPICES: Survey Papers As Interactive Cheatsheet Embeddings},&#xA;  author={Prabhu, Uday Vinay and &#xA;McAteer, Matthew and Teehan, Ryan },&#xA;  booktitle={Rethinking ML Papers - ICLR 2021 Workshop},&#xA;  howpublished = {\url{https://openreview.net/pdf?id=1sysg9hi3KS}},&#xA;  month = {April},&#xA;  year = {2020},&#xA;  note = {(Accessed on 04/03/2021)}&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The success enjoyed by the &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is all you need&lt;/a&gt; spurred an avalanche of papers whose title or content banked off of the X-&lt;em&gt;is all you need&lt;/em&gt; quip &lt;span&gt;ðŸ˜…&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In Figure below, we present a survey of all these papers that allows the reader to summarize where and why these quips occur. &lt;img src=&#34;https://raw.githubusercontent.com/vinayprabhu/X-is-all-you-need/main/landscape/lanscape_iayn.png&#34; alt=&#34;Landscape_of_papers&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Authors&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Publication&lt;/th&gt; &#xA;   &lt;th&gt;Volume&lt;/th&gt; &#xA;   &lt;th&gt;Number&lt;/th&gt; &#xA;   &lt;th&gt;Pages&lt;/th&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Publisher&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kidambi, Rahul; Chang, Jonathan; Sun, Wen;&lt;/td&gt; &#xA;   &lt;td&gt;Optimism is All You Need: Model-Based Imitation Learning From Observation Alone&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2102.10769&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shen, Chao; Wu, Hau-Tieng;&lt;/td&gt; &#xA;   &lt;td&gt;Scalability and robustness of spectral embedding: landmark diffusion is all you need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2001.00801&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bertasius, Gedas; Wang, Heng; Torresani, Lorenzo;&lt;/td&gt; &#xA;   &lt;td&gt;Is Space-Time Attention All You Need for Video Understanding?&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2102.05095&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bach, Francis;&lt;/td&gt; &#xA;   &lt;td&gt;The sum of a geometric series is all you need!&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;de Witt, Christian Schroeder; Gupta, Tarun; Makoviichuk, Denys; Makoviychuk, Viktor; Torr, Philip HS; Sun, Mingfei; Whiteson, Shimon;&lt;/td&gt; &#xA;   &lt;td&gt;Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2011.09533&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prinzing, Michael;&lt;/td&gt; &#xA;   &lt;td&gt;Friendly superintelligent AI: all you need is love&lt;/td&gt; &#xA;   &lt;td&gt;3rd Conference on&#34; Philosophy and Theory of Artificial Intelligence&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;288-301&lt;/td&gt; &#xA;   &lt;td&gt;2017.0&lt;/td&gt; &#xA;   &lt;td&gt;Springer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Javed, Khurram; Yao, Hengshuai; White, Martha;&lt;/td&gt; &#xA;   &lt;td&gt;Is Fast Adaptation All You Need?&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1910.01705&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Abdu-Aguye, Mubarak G; Gomaa, Walid; Makihara, Yasushi; Yagi, Yasushi;&lt;/td&gt; &#xA;   &lt;td&gt;Adaptive Pooling Is All You Need: An Empirical Study on Hyperparameter-insensitive Human Action Recognition Using Wearable Sensors&lt;/td&gt; &#xA;   &lt;td&gt;2020 International Joint Conference on Neural Networks (IJCNN)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1-6&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;IEEE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cui, Xuefeng;&lt;/td&gt; &#xA;   &lt;td&gt;Attention is all you need for general-purpose protein structure embedding&lt;/td&gt; &#xA;   &lt;td&gt;bioRxiv&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;Cold Spring Harbor Laboratory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Xiao, Wencong;&lt;/td&gt; &#xA;   &lt;td&gt;All You Need to Know about Scheduling Deep Learning Jobs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Roberts, Nicholas; Prabhu, Vinay Uday; McAteer, Matthew;&lt;/td&gt; &#xA;   &lt;td&gt;Model weight theft with just noise inputs: The curious case of the petulant attacker&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1912.08987&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gao, Yang; Meyer, Christian M; Mesgar, Mohsen; Gurevych, Iryna;&lt;/td&gt; &#xA;   &lt;td&gt;Good Rewards Are All You Need: Reward Learning for Efficient Reinforcement Learning in Document Summarisation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ramsey, Norman;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is the monad.. what monad was that again&lt;/td&gt; &#xA;   &lt;td&gt;PPS Workshop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2016.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Le, James;&lt;/td&gt; &#xA;   &lt;td&gt;Meta-Learning Is All You Need MATH 789-Mathematics of Deep Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Disambiguation, Cross-lingual Visual Verb Sense;&lt;/td&gt; &#xA;   &lt;td&gt;Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Isola, Phillip;&lt;/td&gt; &#xA;   &lt;td&gt;Rethinking Few-Shot Image Classification: A Good Embedding is All You Need?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Springer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Silva, Francisco Alejandro Gallegos; AlvaracÃ­n-Paula, Mario Eduardo; GonzÃ¡lez, Hugo NicolÃ¡s Acosta;&lt;/td&gt; &#xA;   &lt;td&gt;Estimation of the causal effect of tourism advertising campaign All You Need is Ecuador on digital exposure&lt;/td&gt; &#xA;   &lt;td&gt;Communications in Statistics: Case Studies, Data Analysis and Applications&lt;/td&gt; &#xA;   &lt;td&gt;6.0&lt;/td&gt; &#xA;   &lt;td&gt;2.0&lt;/td&gt; &#xA;   &lt;td&gt;247-269&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;Taylor &amp;amp; Francis&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pedregosa, Fabian;&lt;/td&gt; &#xA;   &lt;td&gt;Sufficient decrease is all you need&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meinert, Judith;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is a (heuristic) cue?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Joshi, Anirudh;&lt;/td&gt; &#xA;   &lt;td&gt;QANet: Convolutions and Attention are All you Need&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ivanov, Andrei; Dryden, Nikoli; Ben-Nun, Tal; Li, Shigang; Hoefler, Torsten;&lt;/td&gt; &#xA;   &lt;td&gt;Data Movement Is All You Need: A Case Study of Transformer Networks&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2007.00072&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Namazifar, Mahdi; Papangelis, Alexandros; Tur, Gokhan; Hakkani-TÃ¼r, Dilek;&lt;/td&gt; &#xA;   &lt;td&gt;Language Model is All You Need: Natural Language Understanding as Question Answering&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2011.03023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kostrikov, Ilya; Yarats, Denis; Fergus, Rob;&lt;/td&gt; &#xA;   &lt;td&gt;Image augmentation is all you need: Regularizing deep reinforcement learning from pixels&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2004.13649&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Eikema, Bryan; Aziz, Wilker;&lt;/td&gt; &#xA;   &lt;td&gt;Is map decoding all you need? the inadequacy of the mode in neural machine translation&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2005.10283&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chen, Weijie; Xie, Di; Zhang, Yuan; Pu, Shiliang;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is a few shifts: Designing efficient convolutional neural networks for image classification&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7241-7250&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TrÃ¤uble, Frederik; Creager, Elliot; Kilbertus, Niki; Goyal, Anirudh; Locatello, Francesco; SchÃ¶lkopf, Bernhard; Bauer, Stefan;&lt;/td&gt; &#xA;   &lt;td&gt;Is independence all you need? on the generalization of representations learned from correlated data&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2006.07886&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tamborrino, Alexandre; Pellicano, Nicola; Pannier, Baptiste; Voitot, Pascal; Naudin, Louise;&lt;/td&gt; &#xA;   &lt;td&gt;Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2004.14074&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Barnes, Jeff;&lt;/td&gt; &#xA;   &lt;td&gt;Azure machine learning&lt;/td&gt; &#xA;   &lt;td&gt;Microsoft Azure Essentials. 1st ed, Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2015.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uhlich, Stefan; Mauch, Lukas; Cardinaux, Fabien; Yoshiyama, Kazuki; Garcia, Javier Alonso; Tiedemann, Stephen; Kemp, Thomas; Nakamura, Akira;&lt;/td&gt; &#xA;   &lt;td&gt;Mixed precision dnns: All you need is a good parametrization&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1905.11452&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kalavri, Vasiliki; Liagouris, John; Hoffmann, Moritz; Dimitrova, Desislava; Forshaw, Matthew; Roscoe, Timothy;&lt;/td&gt; &#xA;   &lt;td&gt;Three steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows&lt;/td&gt; &#xA;   &lt;td&gt;13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;783-798&lt;/td&gt; &#xA;   &lt;td&gt;2018.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Le, James;&lt;/td&gt; &#xA;   &lt;td&gt;Meta-Learning Is All You Need&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wu, Junru; Dai, Xiyang; Chen, Dongdong; Chen, Yinpeng; Liu, Mengchen; Yu, Ye; Wang, Zhangyang; Liu, Zicheng; Chen, Mei; Yuan, Lu;&lt;/td&gt; &#xA;   &lt;td&gt;Weak NAS Predictors Are All You Need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2102.10490&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zhu, Jing; Lu, Xingyu; Heimann, Mark; Koutra, Danai;&lt;/td&gt; &#xA;   &lt;td&gt;Node Proximity Is All You Need: Unified Structural and Positional Node and Graph Embedding&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2102.13582&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bilkhu, Manjot; Wang, Siyang; Dobhal, Tushar;&lt;/td&gt; &#xA;   &lt;td&gt;Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1906.02792&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prasad, Adarsh; Balakrishnan, Sivaraman; Ravikumar, Pradeep;&lt;/td&gt; &#xA;   &lt;td&gt;A Robust Univariate Mean Estimator is All You Need&lt;/td&gt; &#xA;   &lt;td&gt;International Conference on Artificial Intelligence and Statistics&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4034-4044&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;PMLR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kumar, Saurabh; Kumar, Aviral; Levine, Sergey; Finn, Chelsea;&lt;/td&gt; &#xA;   &lt;td&gt;One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2010.14484&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ivanov, Andrei; Dryden, Nikoli; Ben-Nun, Tal; Li, Shigang; Hoefler, Torsten;&lt;/td&gt; &#xA;   &lt;td&gt;Data Movement Is All You Need: A Case Study on Optimizing Transformers&lt;/td&gt; &#xA;   &lt;td&gt;arXiv e-prints&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;arXiv: 2007.00072&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ren, Xuanchi; Yang, Tao; Wang, Yuwang; Zeng, Wenjun;&lt;/td&gt; &#xA;   &lt;td&gt;Do Generative Models Know Disentanglement? Contrastive Learning is All You Need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2102.10543&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Boudiaf, Malik; Kervadec, Hoel; Masud, Ziko Imtiaz; Piantanida, Pablo; Ayed, Ismail Ben; Dolz, Jose;&lt;/td&gt; &#xA;   &lt;td&gt;Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2012.06166&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BlondÃ©, Lionel; Strasser, Pablo; Kalousis, Alexandros;&lt;/td&gt; &#xA;   &lt;td&gt;Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2006.16785&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bai, Shaojie; Kolter, J Zico; Koltun, Vladlen;&lt;/td&gt; &#xA;   &lt;td&gt;Deep equilibrium models&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1909.01377&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mishkin, Dmytro; Matas, Jiri;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is a good init&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1511.06422&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2015.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Lukasz; Polosukhin, Illia;&lt;/td&gt; &#xA;   &lt;td&gt;Attention is all you need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1706.03762&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2017.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chollet, Francois;&lt;/td&gt; &#xA;   &lt;td&gt;The limitations of deep learning&lt;/td&gt; &#xA;   &lt;td&gt;Deep Learning With Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2017.0&lt;/td&gt; &#xA;   &lt;td&gt;Manning Publications&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tran, Ba-Hien; Rossi, Simone; Milios, Dimitrios; Filippone, Maurizio;&lt;/td&gt; &#xA;   &lt;td&gt;All You Need is a Good Functional Prior for Bayesian Deep Learning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2011.12829&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chen, Qiming; Wu, Ren;&lt;/td&gt; &#xA;   &lt;td&gt;CNN is all you need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1712.09662&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2017.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bachlechner, Thomas; Majumder, Bodhisattwa Prasad; Mao, Huanru Henry; Cottrell, Garrison W; McAuley, Julian;&lt;/td&gt; &#xA;   &lt;td&gt;Rezero is all you need: Fast convergence at large depth&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2003.04887&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Xie, Di; Xiong, Jiang; Pu, Shiliang;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6176-6185&lt;/td&gt; &#xA;   &lt;td&gt;2017.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Guo, Yunhui; Li, Yandong; Wang, Liqiang; Rosing, Tajana;&lt;/td&gt; &#xA;   &lt;td&gt;Depthwise convolution is all you need for learning multiple visual domains&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/td&gt; &#xA;   &lt;td&gt;33.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;8368-8375&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Li, Guohao; Xiong, Chenxin; Thabet, Ali; Ghanem, Bernard;&lt;/td&gt; &#xA;   &lt;td&gt;Deepergcn: All you need to train deeper gcns&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2006.07739&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jeddi, Ahmadreza; Shafiee, Mohammad Javad; Wong, Alexander;&lt;/td&gt; &#xA;   &lt;td&gt;A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via Adversarial Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2012.13628&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GrÃ¶ndahl, Tommi; Pajola, Luca; Juuti, Mika; Conti, Mauro; Asokan, N;&lt;/td&gt; &#xA;   &lt;td&gt;All You Need is&#34; Love&#34; Evading Hate Speech Detection&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2-12&lt;/td&gt; &#xA;   &lt;td&gt;2018.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hu, Ronghang; Singh, Amanpreet;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2102.10772&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2021.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wang, Hao; Lu, Pu; Zhang, Hui; Yang, Mingkun; Bai, Xiang; Xu, Yongchao; He, Mengchao; Wang, Yongpan; Liu, Wenyu;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is boundary: Toward arbitrary-shaped text spotting&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the AAAI Conference on Artificial Intelligence&lt;/td&gt; &#xA;   &lt;td&gt;34.0&lt;/td&gt; &#xA;   &lt;td&gt;7.0&lt;/td&gt; &#xA;   &lt;td&gt;12160-12167&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Subakan, Cem; Ravanelli, Mirco; Cornell, Samuele; Bronzi, Mirko; Zhong, Jianyuan;&lt;/td&gt; &#xA;   &lt;td&gt;Attention is All You Need in Speech Separation&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2010.13154&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sharp, Nicholas; Attaiki, Souhaib; Crane, Keenan; Ovsjanikov, Maks;&lt;/td&gt; &#xA;   &lt;td&gt;Diffusion is All You Need for Learning on Surfaces&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2012.00888&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Oâ€™Mahony, Niall; Campbell, Sean; Carvalho, Anderson; Harapanahalli, Suman; Hernandez, Gustavo Velasco; Krpalkova, Lenka; Riordan, Daniel; Walsh, Joseph;&lt;/td&gt; &#xA;   &lt;td&gt;Deep learning vs. traditional computer vision&lt;/td&gt; &#xA;   &lt;td&gt;Science and Information Conference&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;128-144&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;Springer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chaudhury, Ayan; Boudon, FrÃ©dÃ©ric; Godin, Christophe;&lt;/td&gt; &#xA;   &lt;td&gt;3D Plant Phenotyping: All You Need is Labelled Point Cloud Data&lt;/td&gt; &#xA;   &lt;td&gt;European Conference on Computer Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;244-260&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;Springer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lu, Amy X; Lu, Alex X; Moses, Alan;&lt;/td&gt; &#xA;   &lt;td&gt;Evolution Is All You Need: Phylogenetic Augmentation for Contrastive Learning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2012.13475&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kato, Natsumi; Osone, Hiroyuki; Oomori, Kotaro; Ooi, Chun Wei; Ochiai, Yoichi;&lt;/td&gt; &#xA;   &lt;td&gt;Gans-based clothes design: Pattern maker is all you need to design clothing&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the 10th Augmented Human International Conference 2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1-7&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Klein, Tassilo; Nabi, Moin;&lt;/td&gt; &#xA;   &lt;td&gt;Attention is (not) all you need for commonsense reasoning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1905.13497&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Huang, Shaoli; Tao, Dacheng;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is a good representation: A multi-level and classifier-centric representation for few-shot learning&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1911.12476&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wood-Doughty, Zach; Andrews, Nicholas; Dredze, Mark;&lt;/td&gt; &#xA;   &lt;td&gt;Convolutions are all you need (for classifying character sequences)&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;208-213&lt;/td&gt; &#xA;   &lt;td&gt;2018.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Malach, Eran; Yehudai, Gilad; Shalev-Schwartz, Shai; Shamir, Ohad;&lt;/td&gt; &#xA;   &lt;td&gt;Proving the lottery ticket hypothesis: Pruning is all you need&lt;/td&gt; &#xA;   &lt;td&gt;International Conference on Machine Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6682-6691&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;PMLR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tian, Yonglong; Wang, Yue; Krishnan, Dilip; Tenenbaum, Joshua B; Isola, Phillip;&lt;/td&gt; &#xA;   &lt;td&gt;Rethinking few-shot image classification: a good embedding is all you need?&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2003.11539&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bojchevski, Aleksandar; Klicpera, Johannes; Perozzi, Bryan; Blais, Martin; Kapoor, Amol; Lukasik, Michal; GÃ¼nnemann, Stephan;&lt;/td&gt; &#xA;   &lt;td&gt;Is pagerank all you need for scalable graph neural networks?&lt;/td&gt; &#xA;   &lt;td&gt;ACM KDD, MLG Workshop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zhao, Chen; Ghanem, Bernard;&lt;/td&gt; &#xA;   &lt;td&gt;ThumbNet: One Thumbnail Image Contains All You Need for Recognition&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the 28th ACM International Conference on Multimedia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1506-1514&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zisserman, AP; Afouras, T; Chung, JS;&lt;/td&gt; &#xA;   &lt;td&gt;ASR is all you need: cross-modal distillation for lip reading&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the International Conference of Acoustics, Speech, and Signal Processing (ICASSP)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;IEEE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nizan, Ori; Tal, Ayellet;&lt;/td&gt; &#xA;   &lt;td&gt;Breaking the cycle-colleagues are all you need&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7860-7869&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prajwal, KR; Mukhopadhyay, Rudrabha; Namboodiri, Vinay P; Jawahar, CV;&lt;/td&gt; &#xA;   &lt;td&gt;A lip sync expert is all you need for speech to lip generation in the wild&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the 28th ACM International Conference on Multimedia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;484-492&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wu, Tsung-Han; Hsieh, Chun-Chen; Chen, Yen-Hao; Chi, Po-Han; Lee, Hung-yi;&lt;/td&gt; &#xA;   &lt;td&gt;Hand-crafted Attention is All You Need? A Study of Attention on Self-supervised Audio Transformer&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2006.05174&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cao, Meng; Zou, Yuexian;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is a second look: Towards Tighter Arbitrary shape text detection&lt;/td&gt; &#xA;   &lt;td&gt;ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2228-2232&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;IEEE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ayala, Daniel; Borrego, AgustÃ­n; HernÃ¡ndez, Inma; Rivero, Carlos R; Ruiz, David;&lt;/td&gt; &#xA;   &lt;td&gt;AYNEC: all you need for evaluating completion techniques in knowledge graphs&lt;/td&gt; &#xA;   &lt;td&gt;European Semantic Web Conference&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;397-411&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;Springer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Snower, Michael; Kadav, Asim; Lai, Farley; Graf, Hans Peter;&lt;/td&gt; &#xA;   &lt;td&gt;15 Keypoints Is All You Need&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6738-6748&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cheng, Zehua; Wu, Yuxiang; Xu, Zhenghua; Lukasiewicz, Thomas; Wang, Weiyang;&lt;/td&gt; &#xA;   &lt;td&gt;Segmentation is All You Need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1904.13300&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Afouras, Triantafyllos; Chung, Joon Son; Zisserman, Andrew;&lt;/td&gt; &#xA;   &lt;td&gt;Asr is all you need: Cross-modal distillation for lip reading&lt;/td&gt; &#xA;   &lt;td&gt;ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2143-2147&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;IEEE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Al-Dujaili, Abdullah; O&#39;Reilly, Una-May;&lt;/td&gt; &#xA;   &lt;td&gt;Sign bits are all you need for black-box attacks&lt;/td&gt; &#xA;   &lt;td&gt;International Conference on Learning Representations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Duan, Sufeng; Zhao, Hai;&lt;/td&gt; &#xA;   &lt;td&gt;Attention Is All You Need for Chinese Word Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1910.14537&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gelly, Sylvain; Kurach, Karol; Michalski, Marcin; Zhai, Xiaohua;&lt;/td&gt; &#xA;   &lt;td&gt;MemGEN: Memory is All You Need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1803.11203&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2018.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orseau, Laurent; Hutter, Marcus; Rivasplata, Omar;&lt;/td&gt; &#xA;   &lt;td&gt;Logarithmic pruning is all you need&lt;/td&gt; &#xA;   &lt;td&gt;Advances in Neural Information Processing Systems&lt;/td&gt; &#xA;   &lt;td&gt;33.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Komatsuzaki, Aran;&lt;/td&gt; &#xA;   &lt;td&gt;One epoch is all you need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:1906.06669&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2019.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Entezari, Negin; Al-Sayouri, Saba A; Darvishzadeh, Amirali; Papalexakis, Evangelos E;&lt;/td&gt; &#xA;   &lt;td&gt;All you need is low (rank) defending against adversarial attacks on graphs&lt;/td&gt; &#xA;   &lt;td&gt;Proceedings of the 13th International Conference on Web Search and Data Mining&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;169-177&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ramsauer, Hubert; SchÃ¤fl, Bernhard; Lehner, Johannes; Seidl, Philipp; Widrich, Michael; Gruber, Lukas; Holzleitner, Markus; PavloviÄ‡, Milena; Sandve, Geir Kjetil; Greiff, Victor;&lt;/td&gt; &#xA;   &lt;td&gt;Hopfield networks is all you need&lt;/td&gt; &#xA;   &lt;td&gt;arXiv preprint arXiv:2008.02217&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2020.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>