<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-22T01:48:39Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>criwits/missing-pdf</title>
    <updated>2024-04-22T01:48:39Z</updated>
    <id>tag:github.com,2024-04-22:/criwits/missing-pdf</id>
    <link href="https://github.com/criwits/missing-pdf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Your Missing Semester of Using Computer | 你缺失的那门计算机课（PDF 版，LaTeX）&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;《你缺失的那门计算机课》PDF 版本&lt;/h1&gt; &#xA;&lt;p&gt;这是《你缺失的那门计算机课》的 PDF 版本 LaTeX 源代码。按照以下步骤来生成 PDF：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;克隆本仓库到你喜欢的地方；&lt;/li&gt; &#xA; &lt;li&gt;将 &lt;code&gt;resource/quote.zip&lt;/code&gt; 中的两个标点映射文件放到&lt;strong&gt;用户 TEXMF 树&lt;/strong&gt;下的 &lt;code&gt;fonts/misc/xetex/fontmapping/xecjk&lt;/code&gt; 目录中；&lt;/li&gt; &#xA; &lt;li&gt;刷新 TeX 发行版的文件名数据库，TeX Live 用户执行 &lt;pre&gt;&lt;code&gt;mktexlsr&#xA;&lt;/code&gt;&lt;/pre&gt; MiKTeX 用户执行 &lt;pre&gt;&lt;code&gt;initexmf --update-fndb&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;重复执行两遍 &lt;pre&gt;&lt;code&gt;xelatex -output-driver=&#34;xdvipdfmx -i dvipdfmx-unsafe.cfg -q -E&#34; missing.tex&#xA;&lt;/code&gt;&lt;/pre&gt; 或者直接双击 &lt;code&gt;make.bat&lt;/code&gt; 来生成 PDF 文件。&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>sotopia-lab/awesome-social-agents</title>
    <updated>2024-04-22T01:48:39Z</updated>
    <id>tag:github.com,2024-04-22:/sotopia-lab/awesome-social-agents</id>
    <link href="https://github.com/sotopia-lab/awesome-social-agents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of works that investigate social agents, simulations and their real-world impact in text, embodied, and robotics contexts.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;🗣️👥 Awesome Social Agents&lt;/h1&gt; &#xA; &lt;a href=&#34;https://awesome.re&#34;&gt; &lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-Welcome-blue&#34; alt=&#34;PRs Welcome&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://img.shields.io/badge/arXiv-coming-b31b1b.svg&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/arXiv-Coming soon-b31b1b.svg&#34; alt=&#34;arXiv&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- The integration of AI systems into our daily lives has become increasingly ubiquitous, with these systems now capable of interacting with humans in more human-like behaviors than ever before. Specifically, t --&gt; &#xA;&lt;!-- 🎯 What is a **social agent**: A social agent is a human communicative agent that perceive environment, either social or physical, and act upon it with language, action, or non-verbal communication. --&gt; &#xA;&lt;p&gt;&lt;em&gt;For the best experience, we recommend reading this document on the &lt;a href=&#34;https://sotopia-lab.github.io/awesome-social-agents/&#34;&gt;website&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The rise of Large Language Models (LLMs)/foundational models presents new opportunities for simulating complex human &lt;strong&gt;social behaviors&lt;/strong&gt;. As a result, there is a rapidly growing body of work emerging in this domain. We hope to categorize and synergize recent efforts to provide a comprehensive guidebook of &lt;strong&gt;social agents&lt;/strong&gt; weaving together multiple domains, including language, embodiment, and robotics.&lt;/p&gt; &#xA;&lt;p&gt;Our goal is to offer insights crucial for understanding and harnessing &lt;strong&gt;social agents&#39;&lt;/strong&gt; potential impact on society. We strive to keep these updated regularly and continuously. &lt;strong&gt;We greatly appreciate any contributions via PRs, issues, emails, or other methods.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt; and &lt;strong&gt;Environment&lt;/strong&gt; (Sutton and Barto 2018): An agent is a goal-driven decision-maker that sense and act upon the state of the environment. An environment comprises the state outside the agent, including the other agents if any.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Social Agent&lt;/strong&gt;: An agent that interacts with a multi-agent environment.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Socially Intelligent Agent&lt;/strong&gt;: A social agent that interacts and communicates with other agents in a human-interpretable way.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;details&gt;&#xA;  &lt;summary&gt;more notes&lt;/summary&gt;&#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The social intelligence that we are focusing on is human-like, excluding the collective intelligence in a lot of social animals like ants, bees, fishes. &lt;/li&gt; &#xA;   &lt;li&gt;To understand whether an entity is a (social) agent, we have to situate it in an environment. It is not possible to discuss an agent outside of an environment. &lt;/li&gt; &#xA;   &lt;li&gt;We acknowledge there are many types of definitions for social agents. Our defitions here help narrow down the scope of our survey.&lt;/li&gt; &#xA;  &lt;/ol&gt;&#xA; &lt;/details&gt;&#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;🗂️ &lt;em&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/examples.md&#34;&gt;examples&lt;/a&gt; of social agents.&lt;/em&gt; 📚 &lt;em&gt;Check out the table format of the collected papers &lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/paper_table.md&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;📝 &lt;em&gt;We are currently working on a survey paper related to content of this repository. Stay tuned for updates!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This repo supports Python 3.9 and above. In one line, to use a virtual environment, e.g. with anaconda3:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;conda create -n awsome-social-agents python=3.9; conda activate awsome-social-agents; python -m install requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#surveys-and-overview&#34;&gt;Surveys and Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#environments&#34;&gt;Environments&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#text-and-speech-environments&#34;&gt;Text and Speech Environments&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#embodied-environments&#34;&gt;Embodied Environments&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#virtual-environments&#34;&gt;Virtual Environments&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#robotics&#34;&gt;Robotics&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#modeling&#34;&gt;Modeling&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#in-context-learning&#34;&gt;In-context Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#finetuning&#34;&gt;Finetuning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#reinforcement-learning&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#evaluating-social-agents&#34;&gt;Evaluating social agents&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#evaluating-text-social-agents&#34;&gt;Evaluating text social agents&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#evaluating-embodied-social-agents&#34;&gt;Evaluating embodied social agents&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#evaluating-virtual-social-agents&#34;&gt;Evaluating virtual social agents&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#evaluating-robotics-in-social-contexts&#34;&gt;Evaluating robotics in social contexts&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#interactions-with-humans&#34;&gt;Interactions with humans&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#human-chatbot-interaction&#34;&gt;Human-Chatbot Interaction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#human-embodied-agent-interaction&#34;&gt;Human-Embodied Agent Interaction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#human-robot-interaction&#34;&gt;Human Robot Interaction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#human-human-interaction&#34;&gt;Human-Human Interaction&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#challenges&#34;&gt;Challenges&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#theory-of-mind&#34;&gt;Theory of Mind&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#social-learning&#34;&gt;Social Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#simultaneous-interaction&#34;&gt;Simultaneous Interaction&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#applications&#34;&gt;Applications&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#health&#34;&gt;Health&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#policy&#34;&gt;Policy&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#education&#34;&gt;Education&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#concerns&#34;&gt;Concerns&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#risks&#34;&gt;Risks&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sotopia-lab/awesome-social-agents/main/#safety&#34;&gt;Safety&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;Surveys and Overview&lt;/h3&gt; &#xA;&lt;p&gt;[June, 2023] &lt;a href=&#34;https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0048&#34;&gt;Socially intelligent machines that learn from humans and help humans learn&lt;/a&gt;, Gweon et al., arXiv&lt;/p&gt; &#xA;&lt;h3&gt;Environments&lt;/h3&gt; &#xA;&lt;h4&gt;Text and Speech Environments&lt;/h4&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://openreview.net/forum?id=mM7VurbA4r&#34;&gt;SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents&lt;/a&gt;, Xuhui Zhou et al., ICLR&lt;/p&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2310.17512&#34;&gt;CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents&lt;/a&gt;, Qinlin Zhao et al., arXiv&lt;/p&gt; &#xA;&lt;h4&gt;Embodied Environments&lt;/h4&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2310.13724&#34;&gt;Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots&lt;/a&gt;, Puig et al., ICLR&lt;/p&gt; &#xA;&lt;p&gt;[September, 2020] &lt;a href=&#34;https://arxiv.org/pdf/2009.04300.pdf&#34;&gt;SEAN: Social Environment for Autonomous Navigation&lt;/a&gt;, Tsoi et al., HAI&lt;/p&gt; &#xA;&lt;h4&gt;Virtual Environments&lt;/h4&gt; &#xA;&lt;h4&gt;Robotics&lt;/h4&gt; &#xA;&lt;p&gt;[December, 2023] &lt;a href=&#34;https://proceedings.mlr.press/v205/xiong23a.html&#34;&gt;RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments&lt;/a&gt;, Haoyu Xiong et al., Proceedings of The 6th Conference on Robot Learning&lt;/p&gt; &#xA;&lt;p&gt;[August, 2022] &lt;a href=&#34;https://say-can.github.io/&#34;&gt;Do As I Can and Not As I Say: Grounding Language in Robotic Affordances&lt;/a&gt;, Michael Ahn et al., arXiv preprint arXiv:2204.01691&lt;/p&gt; &#xA;&lt;p&gt;[June, 2022] &lt;a href=&#34;https://arxiv.org/abs/2207.05608&#34;&gt;Inner Monologue: Embodied Reasoning through Planning with Language Models&lt;/a&gt;, Wenlong Huang et al., arXiv preprint arXiv:2207.05608&lt;/p&gt; &#xA;&lt;p&gt;[June, 2023] &lt;a href=&#34;https://arxiv.org/abs/2306.12372&#34;&gt;One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments&lt;/a&gt;, Yufei Wang et al., Robotics: Science and Systems (RSS)&lt;/p&gt; &#xA;&lt;p&gt;[August, 2023] &lt;a href=&#34;https://arxiv.org/abs/2108.06038&#34;&gt;Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration&lt;/a&gt;, Chen Wang et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.12910&#34;&gt;Yell At Your Robot: Improving On-the-Fly from Language Corrections&lt;/a&gt;, Lucy Xiaoyang Shi et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[April, 2016] &lt;a href=&#34;https://journals.sagepub.com/doi/10.1177/0018720816644364&#34;&gt;Human--robot interaction: status and challenges&lt;/a&gt;, Thomas B Sheridan et al., Human factors&lt;/p&gt; &#xA;&lt;p&gt;[June, 2021] &lt;a href=&#34;https://link.springer.com/article/10.1007/s12369-020-00666-5&#34;&gt;A taxonomy to structure and analyze human--robot interaction&lt;/a&gt;, Linda Onnasch et al., International Journal of Social Robotics&lt;/p&gt; &#xA;&lt;p&gt;[July, 2023] &lt;a href=&#34;https://arxiv.org/abs/2307.15363&#34;&gt;Robotic vision for human-robot interaction and collaboration: A survey and systematic review&lt;/a&gt;, Nicole Robinson et al., ACM Transactions on Human-Robot Interaction&lt;/p&gt; &#xA;&lt;p&gt;[October, 2022] &lt;a href=&#34;https://arxiv.org/abs/2212.05286&#34;&gt;A survey of multi-agent Human--Robot Interaction systems&lt;/a&gt;, Abhinav Dahiya et al., Robotics and Autonomous Systems&lt;/p&gt; &#xA;&lt;p&gt;[March, 2023] &lt;a href=&#34;https://doi.org/10.1145/3570169&#34;&gt;Nonverbal Cues in Human Robot Interaction: A Communication Studies Perspective&lt;/a&gt;, Jacqueline Urakami et al., J. Hum.-Robot Interact.&lt;/p&gt; &#xA;&lt;p&gt;[April, 2023] &lt;a href=&#34;https://doi.org/10.1145/3571718&#34;&gt;15 Years of (Who)man Robot Interaction: Reviewing the H in Human-Robot Interaction&lt;/a&gt;, Katie Winkle et al., J. Hum.-Robot Interact.&lt;/p&gt; &#xA;&lt;h3&gt;Modeling&lt;/h3&gt; &#xA;&lt;h4&gt;In-context Learning&lt;/h4&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2305.16291&#34;&gt;Voyager: An Open-Ended Embodied Agent with Large Language Models&lt;/a&gt;, Guanzhi Wang et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[March, 2023] &lt;a href=&#34;https://arxiv.org/abs/2303.17491&#34;&gt;Language Models can Solve Computer Tasks&lt;/a&gt;, Geunwoo Kim et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[September, 2024] &lt;a href=&#34;https://arxiv.org/abs/2309.08172&#34;&gt;LASER: LLM Agent with State-Space Exploration for Web Navigation&lt;/a&gt;, Kaixin Ma et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2305.14257&#34;&gt;Hierarchical Prompting Assists Large Language Model on Web Navigation&lt;/a&gt;, Abishek Sridhar et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[January, 2024] &lt;a href=&#34;https://openreview.net/forum?id=Pc8AU1aF5e&#34;&gt;Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control&lt;/a&gt;, Longtao Zheng et al., The Twelfth International Conference on Learning Representations&lt;/p&gt; &#xA;&lt;p&gt;[November, 2023] &lt;a href=&#34;https://openreview.net/forum?id=rnKgbKmelt&#34;&gt;AdaPlanner: Adaptive Planning from Feedback with Language Models&lt;/a&gt;, Haotian Sun et al., Thirty-seventh Conference on Neural Information Processing Systems&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2305.15486&#34;&gt;SPRING: Studying the Paper and Reasoning to Play Games&lt;/a&gt;, Yue Wu et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[March, 2023] &lt;a href=&#34;https://arxiv.org/abs/2303.17071&#34;&gt;DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents&lt;/a&gt;, Varun Nair et al., arXiv&lt;/p&gt; &#xA;&lt;h4&gt;Finetuning&lt;/h4&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2210.03945&#34;&gt;Understanding HTML with Large Language Models&lt;/a&gt;, Izzeddin Gur et al., arXiv&lt;br&gt; [ May, 2023] &lt;a href=&#34;https://openreview.net/forum?id=oLc9sGOBbc&#34;&gt;Instruction-Finetuned Foundation Models for Multimodal Web Navigation&lt;/a&gt;, Hiroki Furuta et al., ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models&lt;/p&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2210.03629&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;, Shunyu Yao et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[January, 2024] &lt;a href=&#34;https://openreview.net/forum?id=9JQtrumvg8&#34;&gt;A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis&lt;/a&gt;, Izzeddin Gur et al., The Twelfth International Conference on Learning Representations&lt;/p&gt; &#xA;&lt;p&gt;[November, 2023] &lt;a href=&#34;https://openreview.net/forum?id=3PjCt4kmRx&#34;&gt;From Pixels to {UI} Actions: Learning to Follow Instructions via Graphical User Interfaces&lt;/a&gt;, Peter Shaw et al., Thirty-seventh Conference on Neural Information Processing Systems&lt;/p&gt; &#xA;&lt;p&gt;[January, 2024] &lt;a href=&#34;https://arxiv.org/abs/2401.01614&#34;&gt;GPT-4V(ision) is a Generalist Web Agent, if Grounded&lt;/a&gt;, Boyuan Zheng et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.04476&#34;&gt;Dual-View Visual Contextualization for Web Navigation&lt;/a&gt;, Jihyung Kil et al., arXiv&lt;/p&gt; &#xA;&lt;h4&gt;Reinforcement learning&lt;/h4&gt; &#xA;&lt;h3&gt;Evaluating social agents&lt;/h3&gt; &#xA;&lt;h4&gt;Evaluating text social agents&lt;/h4&gt; &#xA;&lt;p&gt;[October, 2024] &lt;a href=&#34;https://openreview.net/forum?id=mM7VurbA4r&#34;&gt;SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents&lt;/a&gt;, Xuhui Zhou et al., ICLR&lt;/p&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2310.17512&#34;&gt;CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents&lt;/a&gt;, Qinlin Zhao et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.13679&#34;&gt;RoleInteract: Evaluating the Social Interaction of Role-Playing Agents&lt;/a&gt;, Hongzhan Chen et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[September, 2023] &lt;a href=&#34;https://aclanthology.org/2023.sigdial-1.25&#34;&gt;Approximating Online Human Evaluation of Social Chatbots with Prompting&lt;/a&gt;, Svikhnushina et al., Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue&lt;/p&gt; &#xA;&lt;p&gt;[December, 2023] &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/a3621ee907def47c1b952ade25c67698-Paper-Conference.pdf&#34;&gt;CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Language Model Society&lt;/a&gt;, Guohao Li et al., Advances in Neural Information Processing Systems&lt;/p&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/pdf/2310.14985.pdf&#34;&gt;Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay&lt;/a&gt;, Yihuai Lan et al., arXiv preprint arXiv:2310.14985&lt;/p&gt; &#xA;&lt;p&gt;[August, 2023] &lt;a href=&#34;https://arxiv.org/abs/2308.10278&#34;&gt;CharacterChat: Learning towards Conversational AI with Personalized Social Support&lt;/a&gt;, Quan Tu et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2310.09233&#34;&gt;AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems&lt;/a&gt;, Junjie Zhang et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.11807&#34;&gt;How Far Are We on the Decision-Making of LLMs? Evaluating LLMs&#39; Gaming Ability in Multi-Agent Environments&lt;/a&gt;, Jen-tse Huang et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[August, 2023] &lt;a href=&#34;https://arxiv.org/abs/2308.07201&#34;&gt;ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate&lt;/a&gt;, Chi-Min Chan et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.11958&#34;&gt;Automatic Evaluation for Mental Health Counseling using LLMs&lt;/a&gt;, Anqi Li et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.05863&#34;&gt;How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis&lt;/a&gt;, Federico Bianchi et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:268032940&#34;&gt;PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits&lt;/a&gt;, Hang Jiang et al., NAACL Findings&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:267523076&#34;&gt;Can Large Language Model Agents Simulate Human Trust Behaviors?&lt;/a&gt;, Chengxing Xie et al., ArXiv&lt;/p&gt; &#xA;&lt;p&gt;[January, 2024] &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:266725580&#34;&gt;LLM Harmony: Multi-Agent Communication for Problem Solving&lt;/a&gt;, Sumedh Rasal et al., ArXiv&lt;/p&gt; &#xA;&lt;p&gt;[November, 2021] &lt;a href=&#34;https://aclanthology.org/2021.eancs-1.3&#34;&gt;A Comprehensive Assessment of Dialog Evaluation Metrics&lt;/a&gt;, Yeh et al., The First Workshop on Evaluations and Assessments of Neural Conversation Systems&lt;/p&gt; &#xA;&lt;p&gt;[July, 2020] &lt;a href=&#34;https://aclanthology.org/2020.sigdial-1.8&#34;&gt;{C}onvo{K}it: A Toolkit for the Analysis of Conversations&lt;/a&gt;, Chang et al., Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2305.14757&#34;&gt;Psychological Metrics for Dialog System Evaluation&lt;/a&gt;, Salvatore Giorgi et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/pdf/2305.07797&#34;&gt;ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems&lt;/a&gt;, Sarik Ghazarian et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[November, 2020] &lt;a href=&#34;https://aclanthology.org/2020.emnlp-main.742&#34;&gt;{GRADE}: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems&lt;/a&gt;, Huang et al., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)&lt;/p&gt; &#xA;&lt;p&gt;[July, 2020] &lt;a href=&#34;https://aclanthology.org/2020.sigdial-1.28&#34;&gt;Unsupervised Evaluation of Interactive Dialog with {D}ialo{GPT}&lt;/a&gt;, Mehri et al., Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue&lt;/p&gt; &#xA;&lt;p&gt;[December, 2023] &lt;a href=&#34;https://aclanthology.org/2023.findings-emnlp.371&#34;&gt;x{D}ial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark&lt;/a&gt;, Zhang et al., Findings of the Association for Computational Linguistics: EMNLP 2023&lt;/p&gt; &#xA;&lt;p&gt;[July, 2023] &lt;a href=&#34;https://aclanthology.org/2023.acl-long.839&#34;&gt;Don{&#39;}t Forget Your {ABC}{&#39;}s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems&lt;/a&gt;, Finch et al., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/p&gt; &#xA;&lt;p&gt;[May, 2022] &lt;a href=&#34;https://aclanthology.org/2022.nlp4convai-1.8&#34;&gt;Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents&lt;/a&gt;, Smith et al., Proceedings of the 4th Workshop on NLP for Conversational AI&lt;/p&gt; &#xA;&lt;p&gt;[August, 2021] &lt;a href=&#34;https://aclanthology.org/2021.acl-long.441&#34;&gt;{D}yna{E}val: Unifying Turn and Dialogue Level Evaluation&lt;/a&gt;, Zhang et al., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)&lt;/p&gt; &#xA;&lt;p&gt;[January, 2021] &lt;a href=&#34;https://link.springer.com/article/10.1007/s10462-020-09866-x&#34;&gt;Survey on evaluation methods for dialogue systems&lt;/a&gt;, Jan Deriu et al., Artificial Intelligence Review&lt;/p&gt; &#xA;&lt;p&gt;[July, 2020] &lt;a href=&#34;https://aclanthology.org/2020.sigdial-1.29&#34;&gt;Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of Current Evaluation Protocols&lt;/a&gt;, Finch et al., Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue&lt;/p&gt; &#xA;&lt;p&gt;[July, 2020] &lt;a href=&#34;https://aclanthology.org/2020.acl-srw.27&#34;&gt;u{BLEU}: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems&lt;/a&gt;, Tsuta et al., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop&lt;/p&gt; &#xA;&lt;h4&gt;Evaluating embodied social agents&lt;/h4&gt; &#xA;&lt;p&gt;[December, 2022] &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.635/&#34;&gt;Don&#39;t Copy the Teacher: Data and Model Challenges in Embodied Dialogue&lt;/a&gt;, Min et al., EMNLP&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.12482&#34;&gt;Embodied LLM Agents Learn to Cooperate in Organized Teams&lt;/a&gt;, Xudong Guo et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[Februrary, 2021] &lt;a href=&#34;https://arxiv.org/abs/2103.00047&#34;&gt;SocNavBench: A Grounded Simulation Testing Framework for Evaluating Social Navigation&lt;/a&gt; Biswas et al., ACM Transactions on Human-Robot Interaction&lt;/p&gt; &#xA;&lt;p&gt;[January, 2021] &lt;a href=&#34;https://arxiv.org/abs/2101.05507&#34;&gt;Evaluating the Robustness of Collaborative Agents&lt;/a&gt; Knott et al., AAMAS &#39;21: Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems&lt;/p&gt; &#xA;&lt;h4&gt;Evaluating virtual social agents&lt;/h4&gt; &#xA;&lt;h4&gt;Evaluating robotics in social contexts&lt;/h4&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.10506&#34;&gt;HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation&lt;/a&gt;, Carmelo Sferrazza et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[December, 2020] &lt;a href=&#34;https://doi.org/10.1080/01691864.2019.1698462&#34;&gt;Optimization of criterion for objective evaluation of HRI performance that approximates subjective evaluation: a case study in robot competition&lt;/a&gt;, Y. Mizuchi et al., Advanced Robotics&lt;/p&gt; &#xA;&lt;p&gt;[July, 2020] &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0925753520300643&#34;&gt;Safety bounds in human robot interaction: A survey&lt;/a&gt;, Angeliki Zacharaki et al., Safety science&lt;/p&gt; &#xA;&lt;p&gt;[December, 2015] &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0004370215001174&#34;&gt;RoboCup@ Home: Analysis and results of evolving competitions for domestic and service robots&lt;/a&gt;, Luca Iocchi et al., Artificial Intelligence&lt;/p&gt; &#xA;&lt;p&gt;[October, 2011] &lt;a href=&#34;https://journals.sagepub.com/doi/10.1177/0018720811417254&#34;&gt;A meta-analysis of factors affecting trust in human-robot interaction&lt;/a&gt;, Peter A Hancock et al., Human factors&lt;/p&gt; &#xA;&lt;p&gt;[November, 2009] &lt;a href=&#34;https://link.springer.com/article/10.1007/s12369-008-0001-3&#34;&gt;Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots&lt;/a&gt;, Christoph Bartneck et al., International journal of social robotics&lt;/p&gt; &#xA;&lt;p&gt;[March, 2006] &lt;a href=&#34;https://doi.org/10.1145/1121241.1121249&#34;&gt;Common metrics for human-robot interaction&lt;/a&gt;, Aaron Steinfeld et al., Proceedings of the 1st ACM SIGCHI/SIGART Conference on Human-Robot Interaction&lt;/p&gt; &#xA;&lt;p&gt;[January, 2003] &lt;a href=&#34;https://ieeexplore.ieee.org/document/1174284&#34;&gt;Theory and evaluation of human robot interactions&lt;/a&gt;, J. Scholtz et al., 36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the&lt;/p&gt; &#xA;&lt;h3&gt;Interactions with humans&lt;/h3&gt; &#xA;&lt;h4&gt;Human-Chatbot Interaction&lt;/h4&gt; &#xA;&lt;p&gt;[April, 2023] &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3544548.3580995&#34;&gt;Collaborating with a Text-Based Chatbot: An Exploration of Real-World Collaboration Strategies Enacted during Human-Chatbot Interactions&lt;/a&gt;, Amon Rapp et al., Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3640543.3645213&#34;&gt;AI Comes Out of the Closet: Using AI-Generated Virtual Characters to Help Individuals Practice LGBTQIA+ Advocacy&lt;/a&gt;, Daniel Pillis et al., Proceedings of the 29th International Conference on Intelligent User Interfaces&lt;/p&gt; &#xA;&lt;p&gt;[April, 2023] &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3544548.3581384&#34;&gt;Exploring effects of chatbot-based social contact on reducing mental illness stigma&lt;/a&gt;, Yi-Chieh Lee et al., Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems&lt;/p&gt; &#xA;&lt;p&gt;[May, 2024] &lt;a href=&#34;https://arxiv.org/pdf/2403.03297.pdf&#34;&gt;&#34; It&#39;s the only thing I can trust&#34;: Envisioning Large Language Model Use by Autistic Workers for Communication Assistance&lt;/a&gt;, JiWoong Jang et al., Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems&lt;/p&gt; &#xA;&lt;p&gt;[April, 2022] &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3491102.3502058&#34;&gt;User perceptions of extraversion in chatbots after repeated use&lt;/a&gt;, Sarah Theres V{&#34;o}lkel et al., Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems&lt;/p&gt; &#xA;&lt;p&gt;[September, 2022] &lt;a href=&#34;https://www.mdpi.com/2227-9709/9/4/81&#34;&gt;Interacting with a chatbot-based advising system: Understanding the effect of chatbot personality and user gender on behavior&lt;/a&gt;, Mohammad Amin Kuhail et al., Informatics&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/pdf/2308.10385.pdf&#34;&gt;The Effects of Engaging and Affective Behaviors of Virtual Agents in Group Decision-Making&lt;/a&gt;, Hanseob Kim et al., Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3640543.3645198&#34;&gt;Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration&lt;/a&gt;, Crystal Qian et al., Proceedings of the 29th International Conference on Intelligent User Interfaces&lt;/p&gt; &#xA;&lt;h4&gt;Human-Embodied Agent Interaction&lt;/h4&gt; &#xA;&lt;p&gt;[January, 2023] &lt;a href=&#34;https://arxiv.org/pdf/2301.05223.pdf&#34;&gt;NOPA: Neurally-guided Online Probabilistic Assistance for Building Socially Intelligent Home Assistants&lt;/a&gt;, Puig et al., ICRA&lt;/p&gt; &#xA;&lt;p&gt;[Januaray, 2021] &lt;a href=&#34;https://arxiv.org/pdf/2010.09890.pdf&#34;&gt;WATCH-AND-HELP: A CHALLENGE FOR SOCIAL PERCEPTION AND HUMAN-AI COLLABORATION&lt;/a&gt;, Puig et al., ICLR&lt;/p&gt; &#xA;&lt;p&gt;[October, 2019] &lt;a href=&#34;https://arxiv.org/pdf/1910.05789.pdf&#34;&gt;On the utility of learning about humans for human-ai coordination&lt;/a&gt;, Carroll et al., Neurips&lt;/p&gt; &#xA;&lt;p&gt;[May, 2021] &lt;a href=&#34;https://escholarship.org/uc/item/9ks6n70q&#34;&gt;Interaction Flexibility in Artificial Agents Teaming with Human&lt;/a&gt;, Nalepka et al., Proceedings of the Annual Meeting of the Cognitive Science Society&lt;/p&gt; &#xA;&lt;p&gt;[December, 2023] &lt;a href=&#34;https://arxiv.org/abs/2312.15224&#34;&gt;LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination&lt;/a&gt;, Liu et al., arxiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2306.00087&#34;&gt;Adaptive coordination in social embodied rearrangement&lt;/a&gt;, Szot et al., ICML&lt;/p&gt; &#xA;&lt;p&gt;[April, 2023] &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;, Park et al., UIST&lt;/p&gt; &#xA;&lt;p&gt;[December, 2023] &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/4818263715b25dc137d393af8af6d2fc-Paper-Conference.pdf&#34;&gt;Diverse Conventions for Human-AI Collaboration&lt;/a&gt;, Bidipta Sarkar et al., Advances in Neural Information Processing Systems&lt;/p&gt; &#xA;&lt;h4&gt;Human Robot Interaction&lt;/h4&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2401.14673&#34;&gt;Generative expressive robot behaviors using large language models&lt;/a&gt;, Karthik Mahadevan et al., Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction&lt;/p&gt; &#xA;&lt;p&gt;[October, 2023] &lt;a href=&#34;https://arxiv.org/abs/2310.12931&#34;&gt;Eureka: Human-level reward design via coding large language models&lt;/a&gt;, Yecheng Jason Ma et al., arXiv preprint arXiv:2310.12931&lt;/p&gt; &#xA;&lt;p&gt;[August, 2023] &lt;a href=&#34;https://arxiv.org/abs/2309.02721&#34;&gt;Gesture-informed robot assistance via foundation models&lt;/a&gt;, Li-Heng Lin et al., 7th Annual Conference on Robot Learning&lt;/p&gt; &#xA;&lt;p&gt;[July, 2023] &lt;a href=&#34;https://arxiv.org/abs/2307.15217&#34;&gt;Open problems and fundamental limitations of reinforcement learning from human feedback&lt;/a&gt;, Stephen Casper et al., arXiv preprint arXiv:2307.15217&lt;/p&gt; &#xA;&lt;p&gt;[July, 2023] &lt;a href=&#34;https://arxiv.org/abs/2307.01928&#34;&gt;Robots that ask for help: Uncertainty alignment for large language model planners&lt;/a&gt;, Allen Z Ren et al., arXiv preprint arXiv:2307.01928&lt;/p&gt; &#xA;&lt;p&gt;[June, 2023] &lt;a href=&#34;https://arxiv.org/abs/2306.08647&#34;&gt;Language to rewards for robotic skill synthesis&lt;/a&gt;, Wenhao Yu et al., arXiv preprint arXiv:2306.08647&lt;/p&gt; &#xA;&lt;p&gt;[March, 2023] &lt;a href=&#34;https://arxiv.org/abs/2301.02555&#34;&gt;No, to the right: Online language corrections for robotic manipulation via shared autonomy&lt;/a&gt;, Yuchen Cui et al., Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction&lt;/p&gt; &#xA;&lt;p&gt;[March, 2023] &lt;a href=&#34;https://arxiv.org/abs/2211.12705&#34;&gt;In-Mouth Robotic Bite Transfer with Visual and Haptic Sensing&lt;/a&gt;, Lorenzo Shaikewitz et al., International Conference on Robotics and Automation (ICRA)&lt;/p&gt; &#xA;&lt;p&gt;[March, 2023] &lt;a href=&#34;https://arxiv.org/abs/2212.03363&#34;&gt;Few-shot preference learning for human-in-the-loop rl&lt;/a&gt;, Donald Joseph Hejna III et al., Conference on Robot Learning&lt;/p&gt; &#xA;&lt;p&gt;[August, 2021] &lt;a href=&#34;https://arxiv.org/abs/2006.16732&#34;&gt;Formalizing and guaranteeing human-robot interaction&lt;/a&gt;, Hadas Kress-Gazit et al., Communications of the ACM&lt;/p&gt; &#xA;&lt;h4&gt;Human-Human Interaction&lt;/h4&gt; &#xA;&lt;h3&gt;Challenges&lt;/h3&gt; &#xA;&lt;h4&gt;Theory of Mind&lt;/h4&gt; &#xA;&lt;h4&gt;Social Learning&lt;/h4&gt; &#xA;&lt;h4&gt;Simultaneous Interaction&lt;/h4&gt; &#xA;&lt;h3&gt;Applications&lt;/h3&gt; &#xA;&lt;h4&gt;Health&lt;/h4&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.13313&#34;&gt;Polaris: A Safety-focused LLM Constellation Architecture for Healthcare&lt;/a&gt;, Subhabrata Mukherjee et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[January, 2024] &lt;a href=&#34;https://arxiv.org/abs/2401.14589&#34;&gt;Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias&lt;/a&gt;, Yu He Ke et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.05547&#34;&gt;Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset&lt;/a&gt;, Hengguan Huang et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.09742&#34;&gt;AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis&lt;/a&gt;, Zhihao Fan et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.17546&#34;&gt;COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt&lt;/a&gt;, Suyeon Lee et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2305.08982&#34;&gt;Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback&lt;/a&gt;, Shang-Ling Hsu et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://arxiv.org/abs/2305.05138&#34;&gt;Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media&lt;/a&gt;, Wei Qin et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[May, 2023] &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/37152238/&#34;&gt;An artificial intelligence-based chatbot for prostate cancer education: Design and patient evaluation study&lt;/a&gt;, Magdalena Görtz et al., Digital Health&lt;/p&gt; &#xA;&lt;p&gt;[October, 2024] &lt;a href=&#34;https://arxiv.org/abs/2310.02374&#34;&gt;Conversational Health Agents: A Personalized LLM-Powered Agent Framework&lt;/a&gt;, Mahyar Abbasian et al., arXiv&lt;/p&gt; &#xA;&lt;h4&gt;Policy&lt;/h4&gt; &#xA;&lt;p&gt;[August, 2022] &lt;a href=&#34;https://doi.org/10.1145/3526113.3545616&#34;&gt;Social Simulacra: Creating Populated Prototypes for Social Computing Systems&lt;/a&gt;, Joon Sung Park et al., Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology&lt;/p&gt; &#xA;&lt;p&gt;[November, 2024] &lt;a href=&#34;https://arxiv.org/abs/2311.04076&#34;&gt;Do LLMs exhibit human-like response biases? A case study in survey design&lt;/a&gt;, Lindia Tjuatja et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.01908&#34;&gt;Large language models cannot replace human participants because they cannot portray identity groups&lt;/a&gt;, Angelina Wang et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[February, 2024] &lt;a href=&#34;https://arxiv.org/abs/2402.16333&#34;&gt;Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation&lt;/a&gt;, Xinyi Mou et al., arXiv&lt;/p&gt; &#xA;&lt;p&gt;[March, 2024] &lt;a href=&#34;https://arxiv.org/abs/2403.09498&#34;&gt;From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News&lt;/a&gt;, Yuhan Liu et al., arXiv&lt;/p&gt; &#xA;&lt;h4&gt;Education&lt;/h4&gt; &#xA;&lt;h3&gt;Concerns&lt;/h3&gt; &#xA;&lt;h4&gt;Risks&lt;/h4&gt; &#xA;&lt;h4&gt;Safety&lt;/h4&gt;</summary>
  </entry>
</feed>