<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-27T01:46:04Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>epfml/OptML_course</title>
    <updated>2023-02-27T01:46:04Z</updated>
    <id>tag:github.com,2023-02-27:/epfml/OptML_course</id>
    <link href="https://github.com/epfml/OptML_course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EPFL Course - Optimization for Machine Learning - CS-439&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EPFL Course - Optimization for Machine Learning - CS-439&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://edu.epfl.ch/coursebook/en/optimization-for-machine-learning-CS-439&#34;&gt;Official coursebook information&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Lectures:&lt;/code&gt; Fri 13:15-15:00 in &lt;a href=&#34;https://plan.epfl.ch/?room==CO%202&#34;&gt;CO2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Exercises:&lt;/code&gt; Fri 15:15-17:00 in &lt;a href=&#34;https://plan.epfl.ch/?room==BC%2001&#34;&gt;BC01&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This course teaches an overview of modern mathematical optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.&lt;/p&gt; &#xA;&lt;h3&gt;Team&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instructors: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Martin Jaggi &lt;a href=&#34;mailto:martin.jaggi@epfl.ch&#34;&gt;martin.jaggi@epfl.ch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Nicolas Flammarion &lt;a href=&#34;mailto:nicolas.flammarion@epfl.ch&#34;&gt;nicolas.flammarion@epfl.ch&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Assistants: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Aditya Varre &lt;a href=&#34;mailto:aditya.varre@epfl.ch&#34;&gt;aditya.varre@epfl.ch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Amirkeivan Mohtashami &lt;a href=&#34;mailto:amirkeivan.mohtashami@epfl.ch&#34;&gt;amirkeivan.mohtashami@epfl.ch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;YÃ¼ksel Oguz Kaan &lt;a href=&#34;mailto:oguz.yuksel@epfl.ch&#34;&gt;oguz.yuksel@epfl.ch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Chayti El Mahdi &lt;a href=&#34;mailto:el-mahdi.chayti@epfl.ch&#34;&gt;el-mahdi.chayti@epfl.ch&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Contents:&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Convexity, Gradient Methods, Proximal algorithms, Subgradient Methods, Stochastic and Online Variants of mentioned methods, Coordinate Descent, Frank-Wolfe, Accelerated Methods, Primal-Dual context and certificates, Lagrange and Fenchel Duality, Second-Order Methods including Quasi-Newton Methods, Derivative-Free Optimization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Advanced Contents:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Parallel and Distributed Optimization Algorithms&lt;/p&gt; &#xA;&lt;p&gt;Computational Trade-Offs (Time vs Data vs Accuracy), Lower Bounds&lt;/p&gt; &#xA;&lt;p&gt;Non-Convex Optimization: Convergence to Critical Points, Alternating minimization, Neural network training&lt;/p&gt; &#xA;&lt;h3&gt;Program:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Nr&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Topic&lt;/th&gt; &#xA;   &lt;th&gt;Materials&lt;/th&gt; &#xA;   &lt;th&gt;Exercises&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;24.2.&lt;/td&gt; &#xA;   &lt;td&gt;Introduction, Convexity&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture01.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex01/&#34;&gt;lab01&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;3.3.&lt;/td&gt; &#xA;   &lt;td&gt;Gradient Descent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture02.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex02/&#34;&gt;lab02&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;10.3.&lt;/td&gt; &#xA;   &lt;td&gt;Projected Gradient Descent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture03.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex03/&#34;&gt;lab03&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;17.3.&lt;/td&gt; &#xA;   &lt;td&gt;Proximal and Subgradient Descent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture04.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex04/&#34;&gt;lab04&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;24.3.&lt;/td&gt; &#xA;   &lt;td&gt;Stochastic Gradient Descent, Non-Convex Optimization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture05.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex05/&#34;&gt;lab05&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;31.3.&lt;/td&gt; &#xA;   &lt;td&gt;Non-Convex Optimization, Accelerated Gradient Descent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture06.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex06/&#34;&gt;lab06&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;.&lt;/td&gt; &#xA;   &lt;td&gt;7.4.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;easter vacation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;.&lt;/td&gt; &#xA;   &lt;td&gt;14.4.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;easter vacation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;21.4.&lt;/td&gt; &#xA;   &lt;td&gt;Newton&#39;s Method &amp;amp; Quasi-Newton&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture07.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex07/&#34;&gt;lab07&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;28.4.&lt;/td&gt; &#xA;   &lt;td&gt;Coordinate Descent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture08.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex08/&#34;&gt;lab08&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;5.5.&lt;/td&gt; &#xA;   &lt;td&gt;Frank-Wolfe&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/lecture_notes/lecture-notes.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture09.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex09/&#34;&gt;lab09&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;12.5.&lt;/td&gt; &#xA;   &lt;td&gt;Accelerated Gradient, Gradient-free, adaptive methods&lt;/td&gt; &#xA;   &lt;td&gt;notes, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture10.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/ex10/&#34;&gt;lab10&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;19.5.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Mini-Project week&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;26.5.&lt;/td&gt; &#xA;   &lt;td&gt;Opt for ML in Practice&lt;/td&gt; &#xA;   &lt;td&gt;notes, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture11.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Q&amp;amp;A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;2.6.&lt;/td&gt; &#xA;   &lt;td&gt;Opt for ML in Practice&lt;/td&gt; &#xA;   &lt;td&gt;notes, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/slides/lecture12.pdf&#34;&gt;slides&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Q&amp;amp;A Projects&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Videos:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL4O4bXkI-fAeYrsBqTUYn2xMjJAqlFQzX&#34;&gt;Public playlist of 2021 videos (youtube)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tube.switch.ch/switchcast/epfl.ch/series/4fab28ac-1c8f-4632-8d01-e128746b7a1d&#34;&gt;Playlist of 2022 videos (EPFL internal)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mediaspace.epfl.ch/channel/CS-439+Optimization+for+machine+learning/31980&#34;&gt;Playlist of 2023 videos (EPFL internal)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Exercises:&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/labs/&#34;&gt;weekly exercises&lt;/a&gt; consist of a mix of theoretical and practical &lt;code&gt;Python&lt;/code&gt; exercises for the corresponding topic each week (starting week 2). Solutions to theory exercises are available &lt;a href=&#34;https://raw.githubusercontent.com/epfml/tree/master/lecture_notes&#34;&gt;here&lt;/a&gt;, and for practicals in the lab folder.&lt;/p&gt; &#xA;&lt;h3&gt;Project:&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;mini-project&lt;/code&gt; will focus on the practical implementation: Here we encourage students to investigate the real-world performance of one of the studied optimization algorithms or variants, helping to provide solid empirical evidence for some behaviour aspects on a real machine-learning task. The project is mandatory and done in groups of 3 students. It will count 30% to the final grade. Project reports (3 page PDF) are due June 16th. Here is a &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/labs/mini-project/miniproject_description.pdf&#34;&gt;detailed project description&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Assessment:&lt;/h3&gt; &#xA;&lt;p&gt;Final written exam in exam session on July (t.b.d.) 2023 &lt;em&gt;Format: Closed book. Theoretical questions similar to exercises. You are allowed to bring one cheat sheet (A4 size paper, both sides can be used).&lt;/em&gt; For practice:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;exams &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2022.pdf&#34;&gt;2022&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2021.pdf&#34;&gt;2021&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2020.pdf&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2019.pdf&#34;&gt;2019&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2018.pdf&#34;&gt;2018&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;solutions &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2022solutions.pdf&#34;&gt;2022&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2021solutions.pdf&#34;&gt;2021&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2020solutions.pdf&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2019solutions.pdf&#34;&gt;2019&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/epfml/raw/master/exams/exam2018solutions.pdf&#34;&gt;2018&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Links to related courses and materials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.stat.cmu.edu/~ryantibs/convexopt-F18/&#34;&gt;CMU 10-725&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ee227c.github.io/&#34;&gt;Berkeley EE-227C&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recommended Books&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1405.4980.pdf&#34;&gt;Convex Optimization: Algorithms and Complexity&lt;/a&gt;, by SÃ©bastien Bubeck (free online)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://stanford.edu/~boyd/cvxbook/&#34;&gt;Convex Optimization&lt;/a&gt;, Stephen Boyd and Lieven Vandenberghe (free online)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Introductory Lectures on Convex Optimization&lt;/a&gt;, Yurii Nesterov (free online)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>shd/logic2023</title>
    <updated>2023-02-27T01:46:04Z</updated>
    <id>tag:github.com,2023-02-27:/shd/logic2023</id>
    <link href="https://github.com/shd/logic2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ÐÑÑÑ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾Ð¹ Ð»Ð¾Ð³Ð¸ÐºÐ¸, ÐÐ¢, Ð²ÐµÑÐ½Ð° 2023&lt;/h1&gt; &#xA;&lt;h2&gt;ÐÐ°ÑÐµÑÐ¸Ð°Ð»Ñ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/shd/logic2018/raw/master/conspect.pdf&#34;&gt;ÐÐ¾Ð½ÑÐ¿ÐµÐºÑ 2018 Ð³Ð¾Ð´Ð°&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/shd/logic2011/raw/master/conspect.pdf&#34;&gt;ÐÐ¾Ð½ÑÐ¿ÐµÐºÑ 2011 Ð³Ð¾Ð´Ð°&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/shd/logic2023/raw/master/hw-theory.pdf&#34;&gt;Ð¢ÐµÐ¾ÑÐµÑÐ¸ÑÐµÑÐºÐ¸Ðµ Ð´Ð¾Ð¼Ð°ÑÐ½Ð¸Ðµ Ð·Ð°Ð´Ð°Ð½Ð¸Ñ&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/shd/logic2018/raw/master/make.pdf&#34;&gt;ÐÑÐ°ÑÐºÐ°Ñ Ð¸Ð½ÑÑÑÑÐºÑÐ¸Ñ Ð¿Ð¾ ÑÑÐ¸Ð»Ð¸ÑÐµ make&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÐÐµÐºÑÐ¸Ñ 1&lt;/h2&gt; &#xA;&lt;h3&gt;ÐÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ÐÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¾Ð± Ð¸ÑÑÐ¾ÑÐ¸Ð¸ Ð²Ð¾Ð¿ÑÐ¾ÑÐ°&lt;/li&gt; &#xA; &lt;li&gt;Ð¯Ð·ÑÐº Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ñ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹&lt;/li&gt; &#xA; &lt;li&gt;Ð¢ÐµÐ¾ÑÐ¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð¾ÑÐµÐ½ÐºÐ° Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹&lt;/li&gt; &#xA; &lt;li&gt;Ð¢ÐµÐ¾ÑÐ¸Ñ Ð´Ð¾ÐºÐ°Ð·Ð°ÑÐµÐ»ÑÑÑÐ², Ð´Ð¾ÐºÐ°Ð·Ð°ÑÐµÐ»ÑÑÑÐ²Ð°, Ð²ÑÐ²Ð¾Ð´Ð¸Ð¼Ð¾ÑÑÑ&lt;/li&gt; &#xA; &lt;li&gt;Ð¢ÐµÐ¾ÑÐµÐ¼Ð° Ð¾ ÐºÐ¾ÑÑÐµÐºÑÐ½Ð¾ÑÑÐ¸ ÐºÐ»Ð°ÑÑÐ¸ÑÐµÑÐºÐ¾Ð³Ð¾ Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ñ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ÐÐ´Ðµ Ð¿Ð¾ÑÐ¸ÑÐ°ÑÑ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ð.Ð. ÐÐµÑÐµÑÐ°Ð³Ð¸Ð½, Ð. Ð¨ÐµÐ½Ñ. ÐÐµÐºÑÐ¸Ð¸ Ð¿Ð¾ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾Ð¹ Ð»Ð¾Ð³Ð¸ÐºÐµ Ð¸ ÑÐµÐ¾ÑÐ¸Ð¸ Ð°Ð»Ð³Ð¾ÑÐ¸ÑÐ¼Ð¾Ð². Ð¯Ð·ÑÐºÐ¸ Ð¸ Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ñ. &lt;a href=&#34;https://www.mccme.ru/free-books/shen/shen-logic-part2-2.pdf&#34;&gt;https://www.mccme.ru/free-books/shen/shen-logic-part2-2.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÐÐ¾Ð½ÑÐ¿ÐµÐºÑÑ 2011 Ð¸ 2018 Ð³Ð¾Ð´Ð° Ð¿Ð¾ Ð»Ð¾Ð³Ð¸ÐºÐµ.&lt;/li&gt; &#xA; &lt;li&gt;Ð Ð¿ÑÐ¾ÑÐ¸Ð²Ð¾ÑÐµÑÐ¸ÑÑ Ð² Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾Ð¼ Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ: ÐÐ¶Ð¾ÑÐ´Ð¶ ÐÐµÑÐºÐ»Ð¸, Â«ÐÐ½Ð°Ð»Ð¸ÑÐ¸Ðº. ÐÐµÑÐµÐ´Ð°, Ð°Ð´ÑÐµÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½ÐµÐ²ÐµÑÐ½Ð¾Ð¼Ñ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÐºÑ: Ð³Ð´Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ, ÑÐ²Ð»ÑÑÑÑÑ Ð»Ð¸ Ð¾Ð±ÑÐµÐºÑ, Ð¿ÑÐ¸Ð½ÑÐ¸Ð¿Ñ Ð¸ Ð²ÑÐ²Ð¾Ð´Ñ ÑÐ¾Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð±Ð¾Ð»ÐµÐµ Ð¾ÑÑÐµÑÐ»Ð¸Ð²Ð¾ Ð·Ð°Ð´ÑÐ¼Ð°Ð½Ñ Ð¸Ð»Ð¸ Ð±Ð¾Ð»ÐµÐµ ÑÐ²Ð½Ð¾ Ð²ÑÐ²ÐµÐ´ÐµÐ½Ñ, ÑÐµÐ¼ ÑÐµÐ»Ð¸Ð³Ð¸Ð¾Ð·Ð½ÑÐµ Ð¼Ð¸ÑÑÐµÑÐ¸Ð¸ Ð¸ ÑÐ¾ÑÐºÐ¸ Ð²ÐµÑÑÂ» --- Ð.: ÐÑÑÐ»Ñ, 1978&lt;/li&gt; &#xA; &lt;li&gt;Ð ÑÐ°Ð·Ð½ÑÑ Ð²Ð°ÑÐ¸Ð°Ð½ÑÐ°Ñ Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ñ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹ (Ð²ÐºÐ»ÑÑÐ°Ñ Ð²Ð°ÑÐ¸Ð°Ð½ÑÑ Ñ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑÐµÐ¼Ð¾Ð¹ Ð°ÐºÑÐ¸Ð¾Ð¼): &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Hilbert_systems&#34;&gt;https://en.wikipedia.org/wiki/List_of_Hilbert_systems&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÐÐµÐºÑÐ¸Ñ 2&lt;/h2&gt; &#xA;&lt;h3&gt;Ð¢ÐµÐ¾ÑÐµÐ¼Ñ Ð¾Ð± Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ð¸ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ð¢ÐµÐ¾ÑÐµÐ¼Ð° Ð¾ Ð´ÐµÐ´ÑÐºÑÐ¸Ð¸&lt;/li&gt; &#xA; &lt;li&gt;Ð¢ÐµÐ¾ÑÐµÐ¼Ð° Ð¾ Ð¿Ð¾Ð»Ð½Ð¾ÑÐµ ÐºÐ»Ð°ÑÑÐ¸ÑÐµÑÐºÐ¾Ðµ Ð.Ð.&lt;/li&gt; &#xA; &lt;li&gt;ÐÐ²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð² Ð¸Ð½ÑÑÐ¸ÑÐ¸Ð¾Ð½Ð¸ÑÑÑÐºÐ¾Ðµ Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹: Ð¸ÑÑÐ¾ÑÐ¸Ñ&lt;/li&gt; &#xA; &lt;li&gt;BHK-Ð¸Ð½ÑÐµÑÐ¿ÑÐµÑÐ°ÑÐ¸Ñ ÑÐ²ÑÐ·Ð¾Ðº&lt;/li&gt; &#xA; &lt;li&gt;Ð¢Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸ÑÐµÑÐºÐ°Ñ Ð¸Ð½ÑÐµÑÐ¿ÑÐµÑÐ°ÑÐ¸Ñ Ð¸Ð½ÑÑÐ¸ÑÐ¸Ð¾Ð½Ð¸ÑÑÑÐºÐ¾Ð³Ð¾ Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ñ Ð²ÑÑÐºÐ°Ð·ÑÐ²Ð°Ð½Ð¸Ð¹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ÐÐ´Ðµ Ð¿Ð¾ÑÐ¸ÑÐ°ÑÑ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ð.Ð. ÐÐµÑÐµÑÐ°Ð³Ð¸Ð½, Ð. Ð¨ÐµÐ½Ñ. ÐÐµÐºÑÐ¸Ð¸ Ð¿Ð¾ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾Ð¹ Ð»Ð¾Ð³Ð¸ÐºÐµ Ð¸ ÑÐµÐ¾ÑÐ¸Ð¸ Ð°Ð»Ð³Ð¾ÑÐ¸ÑÐ¼Ð¾Ð². Ð¯Ð·ÑÐºÐ¸ Ð¸ Ð¸ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ñ. &lt;a href=&#34;https://www.mccme.ru/free-books/shen/shen-logic-part2-2.pdf&#34;&gt;https://www.mccme.ru/free-books/shen/shen-logic-part2-2.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÐÐ¾Ð½ÑÐ¿ÐµÐºÑÑ 2011 Ð¸ 2018 Ð³Ð¾Ð´Ð° Ð¿Ð¾ Ð»Ð¾Ð³Ð¸ÐºÐµ.&lt;/li&gt; &#xA; &lt;li&gt;Morten Heine B. SÃ¸rensen, Pawel Urzyczyn: Lections on the Curry-Howard Isomorphism &lt;a href=&#34;https://disi.unitn.it/~bernardi/RSISE11/Papers/curry-howard.pdf&#34;&gt;https://disi.unitn.it/~bernardi/RSISE11/Papers/curry-howard.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ð.Ð.ÐÐ»Ð¸ÑÐºÐ¾, Ð.Ð¥.Ð¥Ð°ÑÐ°Ð½ÑÐ½, ÐÐ½ÑÑÐ¸ÑÐ¸Ð¾Ð½Ð¸ÑÑÑÐºÐ°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ°, ÐÐµÑ-ÐÐ°Ñ ÐÐÐ£ 2009 &lt;a href=&#34;http://logic.math.msu.ru/wp-content/uploads/plisko/intlog.pdf&#34;&gt;http://logic.math.msu.ru/wp-content/uploads/plisko/intlog.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>