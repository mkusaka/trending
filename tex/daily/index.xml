<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-17T01:46:41Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jxbz/agd</title>
    <updated>2023-04-17T01:46:41Z</updated>
    <id>tag:github.com,2023-04-17:/jxbz/agd</id>
    <link href="https://github.com/jxbz/agd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatic gradient descent&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; Automatic Gradient Descent &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jxbz/thesis/raw/main/img/art3.png&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://jeremybernste.in&#34;&gt;Jeremy&amp;nbsp;Bernstein*&lt;/a&gt;   &lt;b&gt;·&lt;/b&gt;   &lt;a href=&#34;https://c1510.github.io/&#34;&gt;Chris&amp;nbsp;Mingard*&lt;/a&gt;   &lt;b&gt;·&lt;/b&gt;   &lt;a href=&#34;https://kevinhuang8.github.io/&#34;&gt;Kevin&amp;nbsp;Huang&lt;/a&gt;   &lt;b&gt;·&lt;/b&gt;   &lt;a href=&#34;https://azizan.mit.edu&#34;&gt;Navid&amp;nbsp;Azizan&lt;/a&gt;   &lt;b&gt;·&lt;/b&gt;   &lt;a href=&#34;http://www.yisongyue.com&#34;&gt;Yisong&amp;nbsp;Yue&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Install PyTorch and a GPU, and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Command line arguments are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--arch           # options: fcn, vgg, resnet18, resnet50&#xA;--dataset        # options: cifar10, cifar100, mnist, imagenet&#xA;--train_bs       # training batch size&#xA;--test_bs        # testing batch size&#xA;--epochs         # number of training epochs&#xA;--depth          # number of layers for fcn&#xA;--width          # hidden layer width for fcn&#xA;--distribute     # train over multiple gpus (for imagenet)&#xA;--gain           # experimental acceleration of training&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;No training hyperparameters are neccessary. Optionally, you can try &lt;code&gt;--gain 10.0&lt;/code&gt; which we have found can accelerate training.&lt;/p&gt; &#xA;&lt;h2&gt;Repository structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── latex/                  # source code for the paper&#xA;├── supercloud/             # mit supercloud run files&#xA;├── util/                  &#xA;│   ├── util/data.py        # datasets and preprocessing&#xA;│   ├── util/models.py      # architecture definitions&#xA;├── agd.py                  # automatic gradient descent&#xA;├── main.py                 # entrypoint to training&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Description of the method&lt;/h2&gt; &#xA;&lt;p&gt;For the $k\text{th}$ weight matrix $W_k$ in $\mathbb{R}^{d_k \times d_{k-1}}$ and square or cross-entropy loss $\mathcal{L}$:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;initial weights are drawn from the uniform measure over orthogonal matrices, and then scaled by $\sqrt{d_k / d_{k-1}}$.&lt;/li&gt; &#xA; &lt;li&gt;weights are updated according to:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;W_k \gets W_k - \frac{\eta}{L} \cdot \sqrt{\tfrac{d_k}{d_{k-1}}} \cdot \frac{ \nabla_{W_k} \mathcal{L}}{\Vert{ \nabla_{W_k}\mathcal{L}(w)}\Vert _F}.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;$L$ measures the depth of the network, and the learning rate $\eta$ is set automatically via:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;$G \gets \frac{1}{L} \sum_{k\in{1...L}} \sqrt{\tfrac{d_k}{d_{k-1}}}\cdot \Vert\nabla_{W_k} \mathcal{L}\Vert_F$;&lt;/li&gt; &#xA; &lt;li&gt;$\eta \gets \log\Big( \tfrac{1+\sqrt{1+4G}}{2}\Big)$.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This procedure is slightly modified for convolutional layers.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find AGD helpful and you&#39;d like to cite the paper, we&#39;d appreciate it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{agd-2023,&#xA;  author  = {Jeremy Bernstein and Chris Mingard and Kevin Huang and Navid Azizan and Yisong Yue},&#xA;  title   = {{A}utomatic {G}radient {D}escent: {D}eep {L}earning without {H}yperparameters},&#xA;  journal = {arXiv:2304.05187},&#xA;  year    = 2023&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;Our paper titled &lt;code&gt;Automatic Gradient Descent: Deep Learning without Hyperparameters&lt;/code&gt; is available &lt;a href=&#34;https://arxiv.org/abs/2304.05187&#34;&gt;at this link&lt;/a&gt;. The derivation of AGD is a refined version of the majorise-minimise analysis given in my &lt;a href=&#34;https://arxiv.org/abs/2210.10101&#34;&gt;PhD thesis&lt;/a&gt; &lt;code&gt;Optimisation &amp;amp; Generalisation in Networks of Neurons&lt;/code&gt;, and was worked out in close collaboration with Chris and Kevin. In turn, this develops the perturbation analysis from &lt;a href=&#34;https://arxiv.org/abs/2002.03432&#34;&gt;our earlier paper&lt;/a&gt; &lt;code&gt;On the Distance between two Neural Networks and the Stability of Learning&lt;/code&gt; with a couple insights from &lt;a href=&#34;https://arxiv.org/abs/2011.14522&#34;&gt;Greg Yang and Edward Hu&#39;s&lt;/a&gt; &lt;code&gt;Feature Learning in Infinite-Width Neural Networks&lt;/code&gt; thrown in for good measure.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Some architecture definitions were adapted from &lt;a href=&#34;https://github.com/kuangliu/pytorch-cifar&#34;&gt;kuangliu/pytorch-cifar&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;We are making AGD available under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA 4.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
</feed>