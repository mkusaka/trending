<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-28T01:42:03Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fnzhan/Generative-AI</title>
    <updated>2023-08-28T01:42:03Z</updated>
    <id>tag:github.com,2023-08-28:/fnzhan/Generative-AI</id>
    <link href="https://github.com/fnzhan/Generative-AI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multimodal Image Synthesis and Editing: The Generative AI Era [TPAMI 2023]&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/title.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.13592&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2107.05399-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Survey&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Naereen/StrapDown.js/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/Naereen/Strapdown.js&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![made-with-Markdown](https://img.shields.io/badge/Made%20with-Markdown-1f425f.svg)](http://commonmark.org) --&gt; &#xA;&lt;!-- [![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](http://ansicolortags.readthedocs.io/?badge=latest) --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/teaser.gif&#34; align=&#34;center&#34;&gt; &#xA;&lt;!-- ![Teaser](teaser.gif) --&gt; &#xA;&lt;!-- ### TODO --&gt; &#xA;&lt;!-- - MISE Dataset for multimodel image synthesis and editing --&gt; &#xA;&lt;p&gt;This project is associated with our survey paper which comprehensively contextualizes the advance of the recent Multimodal Image Synthesis &amp;amp; Editing (MISE) and formulates taxonomies according to data modality and model architectures. The survey is featured on DeepAI and &lt;a href=&#34;https://mp.weixin.qq.com/s/DJXiydi6wHwk7s5Ad2ulaQ&#34;&gt;机器之心&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- For more details, please refer to: &lt;br&gt; --&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/logo.png&#34; align=&#34;center&#34; width=&#34;20&#34;&gt; &lt;strong&gt;Multimodal Image Synthesis and Editing: The Generative AI Era [&lt;a href=&#34;https://arxiv.org/abs/2112.13592&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fnzhan.com/Generative-AI/&#34;&gt;Project&lt;/a&gt;]&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://fnzhan.com/&#34;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&#34;https://yingchen001.github.io/&#34;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=SZkh3iAAAAAJ&amp;amp;hl=en&#34;&gt;Rongliang Wu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=DXpYbWkAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=uYmK-A0AAAAJ&amp;amp;hl=en&#34;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://generativevision.mpi-inf.mpg.de/&#34;&gt;Adam Kortylewsk&lt;/a&gt;, &lt;br&gt; &lt;a href=&#34;https://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~epxing/&#34;&gt;Eric Xing&lt;/a&gt; &lt;br&gt; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;!--[DeepAI](https://deepai.org/publication/multimodal-image-synthesis-and-editing-a-survey).**--&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; You are welcome to promote papers via pull request. &lt;br&gt; The process to submit a pull request:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a. Fork the project into your own repository.&lt;/li&gt; &#xA; &lt;li&gt;b. Add the Title, Author, Conference, Paper link, Project link, and Code link in &lt;code&gt;README.md&lt;/code&gt; with below format:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;**Title**&amp;lt;br&amp;gt;&#xA;*Author*&amp;lt;br&amp;gt;&#xA;Conference&#xA;[[Paper](Paper link)]&#xA;[[Code](Project link)]&#xA;[[Project](Code link)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c. Submit the pull request to this branch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Related Surveys &amp;amp; Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Text-to-Image Synthesis: A Review&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Stanislav Frolov, Tobias Hinz, Federico Raue, Jörn Hees, Andreas Dengel&lt;/em&gt;&lt;br&gt; Neural Networks 2021 [&lt;a href=&#34;https://arxiv.org/abs/2101.09983&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Inversion: A Survey&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang&lt;/em&gt;&lt;br&gt; TPAMI 2022 [&lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/awesome-gan-inversion&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Image Synthesis from Intuitive User Input: A Review and Perspectives&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuan Xue, Yuan-Chen Guo, Han Zhang, Tao Xu, Song-Hai Zhang, Xiaolei Huang&lt;/em&gt;&lt;br&gt; Computational Visual Media 2022 [&lt;a href=&#34;https://arxiv.org/abs/2107.04240&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Yutong-Zhou-cv/awesome-Text-to-Image&#34;&gt;Awesome-Text-to-Image&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of Contents (Work in Progress)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methods:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- ### Methods: --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#NeRF-based-Methods&#34;&gt;NeRF-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Diffusion-based-Methods&#34;&gt;Diffusion-based Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Autoregressive-Methods&#34;&gt;Autoregressive Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Image-Quantizer&#34;&gt;Image Quantizer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#GAN-based-Methods&#34;&gt;GAN-based Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#GAN-Inversion-Methods&#34;&gt;GAN-Inversion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Other-Methods&#34;&gt;Other Methods&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modalities &amp;amp; Datasets:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Text-Encoding&#34;&gt;Text Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Audio-Encoding&#34;&gt;Audio Encoding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fnzhan/Generative-AI/main/#Datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NeRF-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local 3D Editing via 3D Distillation of CLIP Knowledge&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.12570&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, You Li&lt;/em&gt;&lt;br&gt; IJCAI 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.05668&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.12422&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://itsallagi.com/dreamtime-a-new-way-to-create-3d-content-from-text/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.00547&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/AvatarStudio/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ori Gordon, Omri Avrahami, Dani Lischinski&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.12760&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.vision.huji.ac.il/blended-nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Youtan Yin, Zhoujie Fu, Fan Yang, Guosheng Lin&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.10503&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ornerf.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cuteyyt/or-nerf&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junzhe Zhu, Peiye Zhuang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hifa-team.github.io/HiFA-site/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://eckertzhang.github.io/Text2NeRF.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yukangcao.github.io/DreamAvatar/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.02827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://janeyeon.github.io/ditto-nerf/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/janeyeon/ditto-nerf-code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13843&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13450&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://danacohen95.github.io/Set-the-Scene/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DanaCohen95/Set-the-Scene&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instruct-nerf2nerf.github.io&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ayaanzhaque/instruct-nerf2nerf&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/3DFuse/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KU-CVLAB/3DFuse&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-To-4D Dynamic Scene Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, Yaniv Taigman&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2301.11280&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://make-a-video3d.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2211.10440&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deepimagination.cc/Magic3D/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gwanghyun Kim, Se Young Chun&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2211.16374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gwang-kim/DATID-3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://datid-3d.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://3ddesigner-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2209.14988&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/dreamfields&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/dreamfields&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2205.15517.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MrTornado24/IDE-3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mrtornado24.github.io/IDE-3D/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.10821&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/donydchen/sem2nerf&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://donydchen.github.io/sem2nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cassiePython/CLIPNeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://cassiepython.github.io/clipnerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CG-NeRF: Conditional Generative Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kyungmin Jo, Gyumin Shim, Sanghun Jung, Soyoung Yang, Jaegul Choo&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.03517&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01455&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/dreamfields&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, Juyong Zhang&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YudongGuo/AD-NeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://yudongguo.github.io/ADNeRF/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=TQO2EBYXLyU&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Diffusion-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dongxu Li, Junnan Li, Steven C.H. Hoi&lt;/em&gt;&lt;br&gt; Arxiv 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2305.14720.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dxli94.github.io/BLIP-Diffusion-website/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka&lt;/em&gt;&lt;br&gt; Arxiv 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2305.18047.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://qianwangx.github.io/InstructEdit/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qianwangx/instructedit&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani Yael, Pritch Michael, Rubinstein Kfir Aberman&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2208.12242.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/dreambooth&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Concept Customization of Text-to-Image Diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~custom-diffusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adobe-research/custom-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Diffusion for Multi-Modal Face Generation and Editing&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, Ziwei Liu&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2304.10530v1.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ziqihuangg.github.io/projects/collaborative-diffusion.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ziqihuangg/Collaborative-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pnp-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MichalGeyer/plug-and-play&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SINE: SINgle Image Editing with Text-to-Image Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zhang-zx.github.io/SINE/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhang-zx/SINE&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://null-text-inversion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Example: Exemplar-Based Image Editing With Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Fantasy-Studio/Paint-by-Example&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaText: Spatio-Textual Representation for Controllable Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2211.14305.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/spatext/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2304.08818.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructPix2Pix Learning to Follow Image Editing Instructions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tim Brooks, Aleksander Holynski, Alexei A. Efros&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix/&#34;&gt;Project&lt;/a&gt;] [[Code]https://github.com/timothybrooks/instruct-pix2pix)]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unite and Conquer: Plug &amp;amp; Play Multi-Modal Synthesis using Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Chaminda Bandara, Vishal M Patel&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nithin-gk.github.io/projectpages/Multidiff/index.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Nithin-GK/UniteandConquer&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffEdit: Diffusion-based semantic image editing with mask guidance&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/pdf/2210.11427.pdf&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu&lt;/em&gt;&lt;br&gt; Arxiv 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2211.01324.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/eDiff-I/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-to-Prompt Image Editing with Cross-Attention Control&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman1 Yael Pritch, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; Arxiv 2022 [&lt;a href=&#34;https://prompt-to-prompt.github.io/ptp_files/Prompt-to-Prompt_preprint.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://prompt-to-prompt.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/prompt-to-prompt&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; Arxiv 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2208.01618.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Human: Text-Driven Controllable Human Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022 [&lt;a href=&#34;https://arxiv.org/pdf/2205.15996.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yumingj.github.io/projects/Text2Human.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Text2Human&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DALL-E 2] Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;https://cdn.openai.com/papers/dall-e-2.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/DALLE2-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;v objective diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Katherine Crowson&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;https://github.com/crowsonkb/v-diffusion-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gwanghyun Kim, Jong Chul Ye&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.02711&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Diffusion for Text-driven Editing of Natural Images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omri Avrahami, Dani Lischinski, Ohad Fried&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2111.14818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Autoregressive-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MaskGIT: Masked Generative Image Transformer&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2202.04200&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Project](https://wenxin.baidu.com/wenxin/ernie-vilg)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.15283&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wenxin.baidu.com/wenxin/ernie-vilg&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.12417&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/NUWA&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/C9CTnZJ9ZE0&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L-Verse: Bidirectional Generation Between Image and Text&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Taehoon Kim, Gwangmo Song, Sihaeng Lee, Sangyun Kim, Yewon Seo, Soonyoung Lee, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.11133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tgisaturday/L-Verse&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Video](https://youtu.be/C9CTnZJ9ZE0)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, Hongxia Yang&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://arxiv.org/abs/2105.14211v3&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Project](https://compvis.github.io/imagebart/)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://openreview.net/pdf?id=-1AAgrS5FF&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/imagebart&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/imagebart/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yupan Huang, Bei Liu, Jianlong Fu, Yutong Lu&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.09756&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/generate-it&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Multimodal Transformer for Bi-directional Image and Text Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yupan Huang, Hongwei Xue, Bei Liu, Yutong Lu&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.09753&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/generate-it&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Transformers for High-Resolution Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/taming-transformers/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RuDOLPH: One Hyper-Modal Transformer can be creative as DALL-E and smart as CLIP&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Shonenkov and Michael Konstantinov&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://github.com/sberbank-ai/ru-dolph&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generate Images from Texts in Russian (ruDALL-E)&lt;/strong&gt;&lt;br&gt; [&lt;a href=&#34;https://github.com/sberbank-ai/ru-dalle&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://rudalle.ru/en/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Text-to-Image Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2102.12092&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/DALL-E&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://openai.com/blog/dall-e/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Transformers for Scene Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Drew A. Hudson, C. Lawrence Zitnick&lt;/em&gt;&lt;br&gt; NeurIPS 2021 [&lt;a href=&#34;https://openreview.net/pdf?id=YQeWoRnwTnE&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dorarad/gansformer&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi&lt;/em&gt;&lt;br&gt; EMNLP 2020 [&lt;a href=&#34;https://arxiv.org/abs/2009.11278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/allenai/x-lxmert&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Suzhen Wang, Lincheng Li, Yu Ding, Xin Yu&lt;/em&gt;&lt;br&gt; AAAI 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.02749&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Image-Quantizer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;[TE-VQGAN] Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, Edward Choi&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wcshin-git/TE-VQGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[ViT-VQGAN] Vector-quantized Image Modeling with Improved VQGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.04627&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/CompVis/taming-transformers)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;[PeCo] PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2111.12710&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/CompVis/taming-transformers)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-GAN] Taming Transformers for High-Resolution Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/taming-transformers&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Gumbel-VQ] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alexei Baevski, Steffen Schneider, Michael Auli&lt;/em&gt;&lt;br&gt; ICLR 2020 [&lt;a href=&#34;https://openreview.net/pdf?id=rylwJxrYDS&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pytorch/fairseq/raw/main/examples/wav2vec/README.md&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[EM VQ-VAE] Theory and Experiments on Vector Quantized Autoencoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aurko Roy, Ashish Vaswani, Arvind Neelakantan, Niki Parmar&lt;/em&gt;&lt;br&gt; arxiv 2018 [&lt;a href=&#34;https://arxiv.org/abs/1805.11063&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jaywalnut310/Vector-Quantized-Autoencoders&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-VAE] Neural Discrete Representation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu&lt;/em&gt;&lt;br&gt; NIPS 2017 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ritheshkumar95/pytorch-vqvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-VAE2 or EMA-VQ] Generating Diverse High-Fidelity Images with VQ-VAE-2&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Razavi, Aaron van den Oord, Oriol Vinyals&lt;/em&gt;&lt;br&gt; NIPS 2019 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/vq-vae-2-pytorch&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Discrete VAE] Discrete Variational Autoencoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jason Tyler Rolfe&lt;/em&gt;&lt;br&gt; ICLR 2017 [&lt;a href=&#34;https://arxiv.org/abs/1609.02200&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/DALL-E&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DVAE++] DVAE++: Discrete Variational Autoencoders with Overlapping Transformations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arash Vahdat, William G. Macready, Zhengbing Bian, Amir Khoshaman, Evgeny Andriyash&lt;/em&gt;&lt;br&gt; ICML 2018 [&lt;a href=&#34;https://arxiv.org/abs/1802.04920&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmax1/dvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DVAE#] DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arash Vahdat, Evgeny Andriyash, William G. Macready&lt;/em&gt;&lt;br&gt; NIPS 2018 [&lt;a href=&#34;https://arxiv.org/abs/1805.07445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmax1/dvae&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;GAN-based-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;GauGAN2&lt;/strong&gt;&lt;br&gt; &lt;em&gt;NVIDIA&lt;/em&gt;&lt;br&gt; [&lt;a href=&#34;http://gaugan.org/gaugan2/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=p9MAvRpT6Cg&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Conditional Image Synthesis with Product-of-Experts GANs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xun Huang, Arun Mallya, Ting-Chun Wang, Ming-Yu Liu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05130&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RiFeGAN2: Rich Feature Generation for Text-to-Image Synthesis from Constrained Prior Knowledge&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, Dapeng Tao&lt;/em&gt;&lt;br&gt; TCSVT 2021 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9656731/authors#authors&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TRGAN: Text to Image Generation Through Optimizing Initial Image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Liang Zhao, Xinwei Li, Pingda Huang, Zhikui Chen, Yanqi Dai, Tianyu Li&lt;/em&gt;&lt;br&gt; ICONIP 2021 [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-92307-5_76&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- **Image Synthesis From Layout With Locality-Aware Mask Adaption [Layout2Image]**&lt;br&gt;&#xA;*Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, Lingyun Sun*&lt;br&gt;&#xA;GCPR 2021&#xA;[[Paper](https://arxiv.org/pdf/2103.13722.pdf)]&#xA;[[Code](https://github.com/stanifrolov/AttrLostGAN)]&#xA;&#xA;**AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style [Layout2Image]**&lt;br&gt;&#xA;*Stanislav Frolov, Avneesh Sharma, Jörn Hees, Tushar Karayil, Federico Raue, Andreas Dengel*&lt;br&gt;&#xA;ICCV 2021&#xA;[[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Image_Synthesis_From_Layout_With_Locality-Aware_Mask_Adaption_ICCV_2021_paper.pdf)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio-Driven Emotional Video Portraits [Audio2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.07452&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jixinya/EVP/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://jixinya.github.io/projects/evp/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SketchyCOCO: Image Generation from Freehand Scene Sketches&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, Changqing Zou&lt;/em&gt;&lt;br&gt; CVPR 2020 [&lt;a href=&#34;https://arxiv.org/pdf/2003.02683.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sysu-imsl/SketchyCOCO&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mikexuq.github.io/test_building_pages/index.html&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Speech-to-Image Translation [Audio2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiguo Li, Xinfeng Zhang, Chuanmin Jia, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao&lt;/em&gt;&lt;br&gt; JSTSP 2020 [&lt;a href=&#34;https://ieeexplore.ieee.org/document/9067083/authors#authors&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/smallflyingpig/speech-to-image-translation-without-text&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://smallflyingpig.github.io/speech-to-image/main&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MirrorGAN: Learning Text-to-image Generation by Redescription [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao&lt;/em&gt;&lt;br&gt; CVPR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1903.05854&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qiaott/MirrorGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He&lt;/em&gt;&lt;br&gt; CVPR 2018 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/taoxugit/AttnGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug &amp;amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski&lt;/em&gt;&lt;br&gt; CVPR 2017 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Evolving-AI-Lab/ppgn&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas&lt;/em&gt;&lt;br&gt; TPAMI 2018 [&lt;a href=&#34;https://arxiv.org/abs/1710.10916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanzhanggit/StackGAN-v2&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks [Text2Image]&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas&lt;/em&gt;&lt;br&gt; ICCV 2017 [&lt;a href=&#34;https://arxiv.org/abs/1612.03242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanzhanggit/StackGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;GAN-Inversion-Methods&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, Christian Theobalt&lt;/em&gt;&lt;br&gt; SIGGRAPH 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.10973&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/DragGAN&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+ GAN Space Optimization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, Qiang Liu&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01573&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gnobitab/FuseDream&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Umut Kocasari, Alara Dirik, Mert Tiftikci, Pinar Yanardag&lt;/em&gt;&lt;br&gt; WACV 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.08493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/stylemc&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/stylemc/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cycle-Consistent Inverse GAN for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao&lt;/em&gt;&lt;br&gt; ACM MM 2021 [&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3474085.3475226&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Patashnik_StyleCLIP_Text-Driven_Manipulation_of_StyleGAN_Imagery_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/orpatashnik/StyleCLIP&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=PhR1gpXDu0w&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Talk-to-Edit: Fine-Grained Facial Editing via Dialog&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, Ziwei Liu&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Talk-To-Edit_Fine-Grained_Facial_Editing_via_Dialog_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Talk-to-Edit&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.mmlab-ntu.com/project/talkedit/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Face Image Generation and Manipulation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Xia_TediGAN_Text-Guided_Diverse_Face_Image_Generation_and_Manipulation_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IIGROUP/TediGAN&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=L8Na2f5viAM&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.01573&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Other-Methods&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Driven Image Style Transfer&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tsu-Jui Fu, Xin Eric Wang, William Yang Wang&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2106.00178&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIPstyler: Image Style Transfer with a Single Text Condition&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.00374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paper11667/CLIPstyler&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Text-Encoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLAVA: A Foundational Language And Vision Alignment Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2112.04482&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Code](https://github.com/paper11667/CLIPstyler)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Transferable Visual Models From Natural Language Supervision (CLIP)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever&lt;/em&gt;&lt;br&gt; arxiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OpenAI/CLIP&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Audio-Encoding&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wav2CLIP: Learning Robust Audio Representations From CLIP (Wav2CLIP)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, Juan Pablo Bello&lt;/em&gt;&lt;br&gt; ICASSP 2022 [&lt;a href=&#34;https://arxiv.org/abs/2110.11499&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/descriptinc/lyrebird-wav2clip&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Multimodal CelebA-HQ (&lt;a href=&#34;https://github.com/IIGROUP/MM-CelebA-HQ-Dataset&#34;&gt;https://github.com/IIGROUP/MM-CelebA-HQ-Dataset&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;DeepFashion MultiModal (&lt;a href=&#34;https://github.com/yumingj/DeepFashion-MultiModal&#34;&gt;https://github.com/yumingj/DeepFashion-MultiModal&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yyeboah/Awesome-Text-to-3D</title>
    <updated>2023-08-28T01:42:03Z</updated>
    <id>tag:github.com,2023-08-28:/yyeboah/Awesome-Text-to-3D</id>
    <link href="https://github.com/yyeboah/Awesome-Text-to-3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A growing curation of Text-to-3D, Diffusion-to-3D works.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Text-to-3D &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A growing curation of Text-to-3D, Diffusion-to-3D works. Heavily inspired by &lt;a href=&#34;https://github.com/awesome-NeRF/awesome-NeRF&#34;&gt;awesome-NeRF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Recent Updates &lt;span&gt;📰&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;05.08.2023&lt;/code&gt; - Provided citations in BibTeX&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;06.07.2023&lt;/code&gt; - Created initial list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers &lt;span&gt;📜&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.01455&#34;&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/a&gt;, Ajay Jain et al., CVPR 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L1-L6&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.02624&#34;&gt;CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation&lt;/a&gt;, Aditya Sanghi et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L8-L13&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields&lt;/a&gt;, Can Wang et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L15-L20&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.03517&#34;&gt;CG-NeRF: Conditional Generative Neural Radiance Fields&lt;/a&gt;, Kyungmin Jo et al., Arxiv 2021 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L22-L27&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.15172&#34;&gt;PureCLIPNERF: Understanding Pure CLIP Guidance for Voxel Grid NeRF Models&lt;/a&gt;, Han-Hung Lee et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L29-L34&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11277&#34;&gt;TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition&lt;/a&gt;, Yongwei Chen et al., NeurIPS 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L36-L41&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.04493&#34;&gt;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&lt;/a&gt;, Yen-Chi Cheng et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L43-L48&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/a&gt;, Gang Li et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L50-L55&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/a&gt;, Ben Poole et al., ICLR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L57-L62&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models&lt;/a&gt;, Jiale Xu et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L64-L69&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08070&#34;&gt;NeRF-Art: Text-Driven Neural Radiance Fields Stylization&lt;/a&gt;, Can Wang et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L71-L76&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.04628&#34;&gt;Novel View Synthesis with Diffusion Models&lt;/a&gt;, Daniel Watson et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L78-L83&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.16431&#34;&gt;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views&lt;/a&gt;, Dejia Xu et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L85-L90&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08751&#34;&gt;Point-E: A System for Generating 3D Point Clouds from Complex Prompts&lt;/a&gt;, Alex Nichol et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L92-L97&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.07600&#34;&gt;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/a&gt;, Gal Metzer et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L99-L104&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/magic3d/&#34;&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/a&gt;, Chen-Hsuan Linet et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L106-L111&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10663&#34;&gt;RealFusion: 360° Reconstruction of Any Object from a Single Image&lt;/a&gt;, Luke Melas-Kyriazi et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L113-L118&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.14816&#34;&gt;Monocular Depth Estimation using Diffusion Models&lt;/a&gt;, Saurabh Saxena et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L120-L125&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.00792&#34;&gt;SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction&lt;/a&gt;, Zhizhuo Zho et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L127-L132&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.10109&#34;&gt;NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion&lt;/a&gt;, Jiatao Gu et al., ICML 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L134-L139&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.00774&#34;&gt;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/a&gt;, Haochen Wang et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L141-L146&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.03302&#34;&gt;High-fidelity 3D Face Generation from Natural Language Descriptions&lt;/a&gt;, Menghua Wu et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L148-L153&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/a&gt;, Elad Richardson Chen et al., SIGGRAPH 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L155-L160&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.03267&#34;&gt;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/a&gt;, Congyue Deng et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L162-L167&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.12231&#34;&gt;DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models&lt;/a&gt;, Jamie Wynn et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L169-L174&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.10406&#34;&gt;3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process&lt;/a&gt;, Yuhan Li et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L540-L545&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gwang-kim.github.io/datid_3d/&#34;&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/a&gt;, Gwanghyun Kim et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L176-L181&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.04628&#34;&gt;Novel View Synthesis with Diffusion Models&lt;/a&gt;, Daniel Watson et al., ICLR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L183-L188&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/a&gt;, Zhengyi Wang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L190-L195&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://3d-avatar-diffusion.microsoft.com/&#34;&gt;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion&lt;/a&gt;, Tengfei Wang et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L197-L202&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17905&#34;&gt;3D-aware Image Generation using 2D Diffusion Models&lt;/a&gt;, Jianfeng Xiang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L204-L209&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior&lt;/a&gt;, Junshu Tang et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L211-L216&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/a&gt;, Mohammadreza Armandpour et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L218-L223&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.11280&#34;&gt;Text-To-4D Dynamic Scene Generation&lt;/a&gt;, Uriel Singer et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L225-L230&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.02602&#34;&gt;Generative Novel View Synthesis with 3D-Aware Diffusion Models&lt;/a&gt;, Eric R. Chan et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L232-L237&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/a&gt;, Jingbo Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L239-L244&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://guochengqian.github.io/project/magic123/&#34;&gt;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/a&gt;, Guocheng Qian et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L246-L251&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.13508/&#34;&gt;DreamBooth3D: Subject-Driven Text-to-3D Generation&lt;/a&gt;, Amit Raj et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L253-L258&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/a&gt;, Ruoshi Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L260-L265&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image&lt;/a&gt;, Zhenzhen Weng et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L267-L272&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.17606&#34;&gt;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/a&gt;, Ruixiang Jiang et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L274-L279&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.13348&#34;&gt;TextDeformer: Geometry Manipulation using Text Guidance&lt;/a&gt;, William Gao et al., Arxiv 2033 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L281-L286&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/ATT3D/&#34;&gt;ATT3D: Amortized Text-to-3D Object Synthesis&lt;/a&gt;, Jonathan Lorraine et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L288-L293&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://neuralcarver.github.io/michelangelo/&#34;&gt;Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation&lt;/a&gt;, Zibo Zhao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L295-L300&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://light.princeton.edu/publication/diffusion-sdf/&#34;&gt;Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions&lt;/a&gt;, Gene Chou et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L302-L307&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hifa-team.github.io/HiFA-site/&#34;&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/a&gt;, Junzhe Zhu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L309-L314&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.lerf.io/&#34;&gt;LERF: Language Embedded Radiance Fields&lt;/a&gt;, Justin Kerr et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L316-L321&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://instruct-nerf2nerf.github.io/&#34;&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/a&gt;, Ayaan Haque et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L323-L328&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://ku-cvlab.github.io/3DFuse/&#34;&gt;3DFuse: Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/a&gt;, Junyoung Seo et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L330-L335&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mvdiffusion.github.io/&#34;&gt;MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion&lt;/a&gt;, Shitao Tang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L337-L342&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://one-2-3-45.github.io/&#34;&gt;One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization&lt;/a&gt;, Minghua Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L344-L349&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/a&gt;, Christina Tsalicoglou Liu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L351-L356&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.16223&#34;&gt;Prompt-Free Diffusion: Taking &#34;Text&#34; out of Text-to-Image Diffusion Models&lt;/a&gt;, Xingqian Xu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L358-L363&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scenescape.github.io/&#34;&gt;SceneScape: Text-Driven Consistent Scene Generation&lt;/a&gt;, Rafail Fridman et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L365-L370&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.12570&#34;&gt;Local 3D Editing via 3D Distillation of CLIP Knowledge&lt;/a&gt;, Junha Hyung et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L372-L377&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.nasir.lol/clipmesh&#34;&gt;CLIP-Mesh: Generating textured meshes from text using pretrained image-text models&lt;/a&gt;, Nasir Khalid et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L379-L384&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lukashoel.github.io/text-to-room/&#34;&gt;Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models&lt;/a&gt;, Lukas Höllein et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L386-L391&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.06714&#34;&gt;Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction&lt;/a&gt;, Hansheng Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L393-L398&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01900&#34;&gt;PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion&lt;/a&gt;, Gwanghyun Kim et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L561-L566&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.11870&#34;&gt;Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models&lt;/a&gt;, Byungjun Kim et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L568-L573&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.02463&#34;&gt;Shap-E: Generating Conditional 3D Implicit Functions&lt;/a&gt;, Heewoo Jun et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L400-L405&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.03869&#34;&gt;Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation&lt;/a&gt;, Aditya Sanghi et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L407-L412&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.05668&#34;&gt;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models&lt;/a&gt;, Xingchen Zhou et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L414-L419&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://daveredrum.github.io/Text2Tex/&#34;&gt;Text2Tex: Text-driven Texture Synthesis via Diffusion Models&lt;/a&gt;, Dave Zhenyu Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L421-L426&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://snap-research.github.io/3DVADER/&#34;&gt;3D VADER - AutoDecoding Latent 3D Diffusion Models&lt;/a&gt;, Evangelos Ntavelis et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L428-L433&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://control4darxiv.github.io/&#34;&gt;Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor&lt;/a&gt;, Ruizhi Shao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L435-L440&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.03414&#34;&gt;DreamSparse: Escaping from Plato&#39;s Cave with 2D Frozen Diffusion Model Given Sparse Views&lt;/a&gt;, Paul Yoo et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L42-L447&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/a&gt;, Rui Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L449-L454&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03117&#34;&gt;DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance&lt;/a&gt;, Longwen Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L456-L461&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.13450&#34;&gt;Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes&lt;/a&gt;, Dana Cohen-Bar et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L463-L468&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.03038&#34;&gt;HeadSculpt: Crafting 3D Head Avatars with Text&lt;/a&gt;, Xiao Han et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L470-L475&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.07279&#34;&gt;Cap3D: Scalable 3D Captioning with Pretrained Models&lt;/a&gt;, Tiange Luo et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L477-L482&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.07154&#34;&gt;InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions&lt;/a&gt;, Jiale Xu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L484-L489&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.09329&#34;&gt;DreamHuman: Animatable 3D Avatars from Text&lt;/a&gt;, Nikos Kolotouros et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L554-L559&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.11418&#34;&gt;FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields&lt;/a&gt;, Sungwon Hwang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L491-L496&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.12981&#34;&gt;3D-LLM: Injecting the 3D World into Large Language Models&lt;/a&gt;, Yining Hong et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L498-L503&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.13908&#34;&gt;Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation&lt;/a&gt;, Chaohui Yu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L505-L510&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15988&#34;&gt;RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects&lt;/a&gt;, Sascha Kirch et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L512-L517&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.03610&#34;&gt;AvatarVerse: High-quality &amp;amp; Stable 3D Avatar Creation from Text and Pose&lt;/a&gt;, Huichao Zhang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L547-L552&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.08545&#34;&gt;TeCH: Text-guided Reconstruction of Lifelike Clothed Humans&lt;/a&gt;, Yangyi Huang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L575-L580&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://skhu101.github.io/HumanLiff/&#34;&gt;HumanLiff: Layer-wise 3D Human Generation with Diffusion Model&lt;/a&gt;, Hu Shoukang et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L582-L587&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://tada.is.tue.mpg.de&#34;&gt;TADA! Text to Animatable Digital Avatars&lt;/a&gt;, Tingting Liao et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L589-L594&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.09278&#34;&gt;MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR&lt;/a&gt;, Xudong Xu et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L596-L601&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.11473&#34;&gt;IT3D: Improved Text-to-3D Generation with Explicit View Synthesis&lt;/a&gt;, Yiwen Chen et al., Arxiv 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L603-L608&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.04909&#34;&gt;SATR: Zero-Shot Semantic Segmentation of 3D Shapes&lt;/a&gt;, Ahmed Abdelreheem et al., ICCV 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L610-L615&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets &lt;span&gt;💾&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08051&#34;&gt;Objaverse: A Universe of Annotated 3D Objects&lt;/a&gt;, Matt Deitke et al., Arxiv 2022 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L519-L524&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://objaverse.allenai.org/objaverse-xl-paper.pdf&#34;&gt;Objaverse-XL: A Universe of 10M+ 3D Objects&lt;/a&gt;, Matt Deitke et al., Preprint 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L526-L531&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.03302&#34;&gt;Describe3D: High-Fidelity 3D Face Generation from Natural Language Descriptions&lt;/a&gt;, Menghua Wu et al., CVPR 2023 | &lt;a href=&#34;https://raw.githubusercontent.com/yyeboah/Awesome-Text-to-3D/main/references/citations.bib#L533-L538&#34;&gt;citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Frameworks &lt;span&gt;🖥&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/threestudio-project/threestudio&#34;&gt;threestudio: A unified framework for 3D content generation&lt;/a&gt;, Yuan-Chen Guo et al., Github 2023&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/index.html&#34;&gt;Nerfstudio: A Modular Framework for Neural Radiance Field Development&lt;/a&gt;, Matthew Tancik et al., SIGGRAPH 2023&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/MirageML/Mirage3D&#34;&gt;Mirage3D: Open-Source Implementations of 3D Diffusion Models Optimized for GLB Output&lt;/a&gt;, Mirageml et al., Github 2023&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Initial List of the STOA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Provide citations in BibTeX&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sub-categorize based on input conditioning&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lintool/MapReduceAlgorithms</title>
    <updated>2023-08-28T01:42:03Z</updated>
    <id>tag:github.com,2023-08-28:/lintool/MapReduceAlgorithms</id>
    <link href="https://github.com/lintool/MapReduceAlgorithms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data-Intensive Text Processing with MapReduce&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>