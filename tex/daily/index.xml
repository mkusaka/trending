<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-17T01:36:13Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>srush/awesome-o1</title>
    <updated>2025-01-17T01:36:13Z</updated>
    <id>tag:github.com,2025-01-17:/srush/awesome-o1</id>
    <link href="https://github.com/srush/awesome-o1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A bibliography and survey of the papers surrounding o1&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;awesome-o1&lt;/h1&gt; &#xA;&lt;p&gt;This is a bibliography of papers that are presumed to be related to OpenAI’s &lt;a href=&#34;https://openai.com/index/learning-to-reason-with-llms/&#34;&gt;o1&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/6PEJ96k1kiw&#34;&gt;https://youtu.be/6PEJ96k1kiw&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/6PEJ96k1kiw&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/6c890bdf-8006-4d90-b822-84b413b0f248&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://srush.github.io/awesome-o1/o1-tutorial.pdf&#34;&gt;o1 Tutorial Slides&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process. We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute). The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them. … Similar to how a human may think for a long time before responding to a difficult question, o1 uses a chain of thought when attempting to solve a problem. Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isn’t working. This process dramatically improves the model’s ability to reason. To illustrate this leap forward, we showcase the chain of thought from o1-preview on several difficult problems below.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;What we would like to actually work?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Consistency&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Wang2022-px&#34;&gt;X. Wang et al. 2022&lt;/a&gt;) Majority voting of LLM output improves a bit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scratchpad&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Nye2021-bx&#34;&gt;Nye et al. 2021&lt;/a&gt;) / &lt;strong&gt;Chain-of-Thought&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Wei2022-uj&#34;&gt;Wei et al. 2022&lt;/a&gt;) Wouldn’t it be cool if an LLM could talk to itself and get better?&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tree-of-Thought&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Yao2023-nw&#34;&gt;Yao et al. 2023&lt;/a&gt;) Wouldn’t it be cool if you could scale this as a tree?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why might this be possible?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;AlphaGo&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Silver2016-ag&#34;&gt;Silver et al. 2016&lt;/a&gt;) Quantifies value of self-play training vs.&amp;nbsp;test search&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AlphaZero&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Silver2017-bn&#34;&gt;Silver et al. 2017&lt;/a&gt;) Shows training on guided self-trajectory can be generalized / scaled&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Libratus&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Brown2017-of&#34;&gt;N. Brown and Sandholm 2017&lt;/a&gt;) Poker bot built by scaling search&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scaling Laws for Board Games&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Jones2021-di&#34;&gt;Jones 2021&lt;/a&gt;) Clean experiments that compare train / test FLOPs in a controlled setting&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Noam Lecture&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Paul-G-Allen-School2024-da&#34;&gt;Paul G. Allen School 2024&lt;/a&gt;) Talk from Noam Brown about the power of search&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Can reasoning be a verifiable game?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;WebGPT&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Nakano2021-iz&#34;&gt;Nakano et al. 2021&lt;/a&gt;) Shows that test time rejection sampling against a reward model is a very strong model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GSM8K&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Cobbe2021-gt&#34;&gt;Cobbe et al. 2021&lt;/a&gt;) Considers why math reasoning is challenging and introduces ORM models for verification&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Process Reward&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Uesato2022-aw&#34;&gt;Uesato et al. 2022&lt;/a&gt;) Introduces distinction of a process reward / outcome reward model, and uses expert iteration RL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Let’s Verify&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Lightman2023-cr&#34;&gt;Lightman et al. 2023&lt;/a&gt;) Demonstrates that PRMs can be quite effective in efficacy of rejection sampling&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Math-Shepard&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Wang2023-ur&#34;&gt;P. Wang et al. 2023&lt;/a&gt;) Experiments with automatic value function learning with roll outs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Can a verifier make a better LLM?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Expert Iteration&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Anthony2017-dm&#34;&gt;Anthony, Tian, and Barber 2017&lt;/a&gt;) Search, collect, train. Method for self-improvement in RL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Training&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Yarowsky1995-tm&#34;&gt;Yarowsky 1995&lt;/a&gt;) Classic unsupervised method: generate, prune, retrain&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;STaR&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Zelikman2022-id&#34;&gt;Zelikman et al. 2022&lt;/a&gt;) Formulates LLM improvement as retraining on rationales that lead to correct answers. Justified as approximate policy gradient.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ReST&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Gulcehre2023-vk&#34;&gt;Gulcehre et al. 2023&lt;/a&gt;) Models improvement as offline-RL. Samples trajectories, grow corpus, retrain.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ReST-EM&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Singh2023-eb&#34;&gt;Singh et al. 2023&lt;/a&gt;) Formalizes similar methods as EM for RL. Applies to reasoning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Can LLMs learn to plan?&lt;/h2&gt; &#xA;&lt;p&gt;(This part is the most speculative)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stream of Search&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Gandhi2024-vs&#34;&gt;Gandhi et al. 2024&lt;/a&gt;) Training on linearized, non-optimal search trajectories induces better search.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DualFormer&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Su2024-us&#34;&gt;Su et al. 2024&lt;/a&gt;) Training on optimal reasoning traces with masked steps improves reasoning ability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AlphaZero-like&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Feng2023-sz&#34;&gt;Feng et al. 2023&lt;/a&gt;) / &lt;strong&gt;MCTS-DPO&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Xie2024-lp&#34;&gt;Xie et al. 2024&lt;/a&gt;) / &lt;strong&gt;Agent Q&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Putta2024-yy&#34;&gt;Putta et al. 2024&lt;/a&gt;) Sketches out MCTS-style expert iteration for LLM planning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PAVs&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Setlur2024-ax&#34;&gt;Setlur et al. 2024&lt;/a&gt;) Argues for advantage (PAV) function over value (PRM) for learning to search. Shows increase in search efficacy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SCoRE (Self-Correct)&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Kumar2024-fj&#34;&gt;Kumar et al. 2024&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Does this lead to test time scaling?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimal test scaling&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Snell2024-dx&#34;&gt;Snell et al. 2024&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Large Language Monkeys&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Brown2024-bs&#34;&gt;B. Brown et al. 2024&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Scaling&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/srush/awesome-o1/main/#ref-Wu2024-mt&#34;&gt;Y. Wu et al. 2024&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Full Bibliography.&lt;/h2&gt; &#xA;&lt;p&gt;Anthony, Thomas, Zheng Tian, and David Barber. 2017. “Thinking Fast and Slow with Deep Learning and Tree Search.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1705.08439&#34;&gt;http://arxiv.org/abs/1705.08439&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Brown, Bradley, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia Mirhoseini. 2024. “Large Language Monkeys: Scaling Inference Compute with Repeated Sampling.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2407.21787&#34;&gt;http://arxiv.org/abs/2407.21787&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Brown, Noam, and Tuomas Sandholm. 2017. “Libratus: The Superhuman AI for No-Limit Poker.” In &lt;em&gt;Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence&lt;/em&gt;. California: International Joint Conferences on Artificial Intelligence Organization. &lt;a href=&#34;https://www.onlinecasinoground.nl/wp-content/uploads/2018/10/Libratus-super-human-no-limit-poker-Sandholm-Brown.pdf&#34;&gt;https://www.onlinecasinoground.nl/wp-content/uploads/2018/10/Libratus-super-human-no-limit-poker-Sandholm-Brown.pdf&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Chen, Ziru, Michael White, Raymond Mooney, Ali Payani, Yu Su, and Huan Sun. 2024. “When Is Tree Search Useful for LLM Planning? It Depends on the Discriminator.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2402.10890&#34;&gt;http://arxiv.org/abs/2402.10890&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, et al. 2021. “Training Verifiers to Solve Math Word Problems.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2110.14168&#34;&gt;http://arxiv.org/abs/2110.14168&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Feng, Xidong, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. “Alphazero-Like Tree-Search Can Guide Large Language Model Decoding and Training.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2309.17179&#34;&gt;http://arxiv.org/abs/2309.17179&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Gandhi, Kanishk, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. 2024. “Stream of Search (SoS): Learning to Search in Language.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2404.03683&#34;&gt;http://arxiv.org/abs/2404.03683&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Gulcehre, Caglar, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, et al. 2023. “Reinforced Self-Training (ReST) for Language Modeling.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;citation_for_view=7hwJ2ckAAAAJ:evX43VCCuoAC&#34;&gt;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;citation_for_view=7hwJ2ckAAAAJ:evX43VCCuoAC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Jones, Andy L. 2021. “Scaling Scaling Laws with Board Games.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2104.03113&#34;&gt;http://arxiv.org/abs/2104.03113&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Kazemnejad, Amirhossein, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024. “VinePPO: Unlocking RL Potential for LLM Reasoning Through Refined Credit Assignment.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2410.01679&#34;&gt;http://arxiv.org/abs/2410.01679&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Kirchner, Jan Hendrik, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, and Yuri Burda. 2024. “Prover-Verifier Games Improve Legibility of LLM Outputs.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2407.13692&#34;&gt;http://arxiv.org/abs/2407.13692&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Kumar, Aviral, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, et al. 2024. “Training Language Models to Self-Correct via Reinforcement Learning.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2409.12917&#34;&gt;http://arxiv.org/abs/2409.12917&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. “Let’s Verify Step by Step.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2305.20050&#34;&gt;http://arxiv.org/abs/2305.20050&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Nakano, Reiichiro, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, et al. 2021. “WebGPT: Browser-Assisted Question-Answering with Human Feedback.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2112.09332&#34;&gt;http://arxiv.org/abs/2112.09332&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Nye, Maxwell, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, et al. 2021. “Show Your Work: Scratchpads for Intermediate Computation with Language Models.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2112.00114&#34;&gt;http://arxiv.org/abs/2112.00114&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Paul G. Allen School. 2024. “Parables on the Power of Planning in AI: From Poker to Diplomacy: Noam Brown (OpenAI).” Youtube. &lt;a href=&#34;https://www.youtube.com/watch?v=eaAonE58sLU&#34;&gt;https://www.youtube.com/watch?v=eaAonE58sLU&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Putta, Pranav, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. 2024. “Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2408.07199&#34;&gt;http://arxiv.org/abs/2408.07199&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Setlur, Amrith, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024. “Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2410.08146&#34;&gt;http://arxiv.org/abs/2410.08146&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, et al. 2016. “Mastering the Game of Go with Deep Neural Networks and Tree Search.” &lt;em&gt;Nature&lt;/em&gt; 529 (7587): 484–89. &lt;a href=&#34;https://www.nature.com/articles/nature16961&#34;&gt;https://www.nature.com/articles/nature16961&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/1712.01815&#34;&gt;http://arxiv.org/abs/1712.01815&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Singh, Avi, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, et al. 2023. “Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2312.06585&#34;&gt;http://arxiv.org/abs/2312.06585&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Snell, Charlie, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. “Scaling LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model Parameters.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2408.03314&#34;&gt;http://arxiv.org/abs/2408.03314&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Su, Dijia, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. 2024. “Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2410.09918&#34;&gt;http://arxiv.org/abs/2410.09918&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Uesato, Jonathan, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. “Solving Math Word Problems with Process- and Outcome-Based Feedback.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2211.14275&#34;&gt;http://arxiv.org/abs/2211.14275&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Wang, Junlin, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024. “Mixture-of-Agents Enhances Large Language Model Capabilities.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2406.04692&#34;&gt;http://arxiv.org/abs/2406.04692&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Wang, Peiyi, Lei Li, Zhihong Shao, R X Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui. 2023. “Math-Shepherd: Verify and Reinforce LLMs Step-by-Step Without Human Annotations.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2312.08935&#34;&gt;http://arxiv.org/abs/2312.08935&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2203.11171&#34;&gt;http://arxiv.org/abs/2203.11171&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” Edited by S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, and A Oh. &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;, 24824–37. &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf&#34;&gt;https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Welleck, Sean, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. “From Decoding to Meta-Generation: Inference-Time Algorithms for Large Language Models.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2406.16838&#34;&gt;http://arxiv.org/abs/2406.16838&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Wu, Tianhao, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024. “Thinking LLMs: General Instruction Following with Thought Generation.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2410.10630&#34;&gt;http://arxiv.org/abs/2410.10630&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Wu, Yangzhen, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2408.00724&#34;&gt;http://arxiv.org/abs/2408.00724&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Xie, Yuxi, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. “Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning.” &lt;em&gt;arXiv [Cs.AI]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2405.00451&#34;&gt;http://arxiv.org/abs/2405.00451&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Xie, Yuxi, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. “Self-Evaluation Guided Beam Search for Reasoning.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2305.00633&#34;&gt;http://arxiv.org/abs/2305.00633&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2305.10601&#34;&gt;http://arxiv.org/abs/2305.10601&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Yarowsky, David. 1995. “Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.” In &lt;em&gt;Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics -&lt;/em&gt;. Morristown, NJ, USA: Association for Computational Linguistics. &lt;a href=&#34;https://dl.acm.org/doi/10.3115/981658.981684&#34;&gt;https://dl.acm.org/doi/10.3115/981658.981684&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Yoshida, Davis, Kartik Goyal, and Kevin Gimpel. 2024. “&lt;span class=&#34;nocase&#34;&gt;MAP’s&lt;/span&gt; Not Dead yet: Uncovering True Language Model Modes by Conditioning Away Degeneracy.” In &lt;em&gt;Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 16164–215. Stroudsburg, PA, USA: Association for Computational Linguistics. &lt;a href=&#34;https://aclanthology.org/2024.acl-long.855.pdf&#34;&gt;https://aclanthology.org/2024.acl-long.855.pdf&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Zelikman, Eric, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. 2024. “Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.” &lt;em&gt;arXiv [Cs.CL]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2403.09629&#34;&gt;http://arxiv.org/abs/2403.09629&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Zelikman, Eric, Yuhuai Wu, Jesse Mu, and Noah D Goodman. 2022. “STaR: Bootstrapping Reasoning with Reasoning.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2203.14465&#34;&gt;http://arxiv.org/abs/2203.14465&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Zhao, Stephen, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. 2024. “Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo.” &lt;em&gt;arXiv [Cs.LG]&lt;/em&gt;. &lt;a href=&#34;http://arxiv.org/abs/2404.17546&#34;&gt;http://arxiv.org/abs/2404.17546&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>