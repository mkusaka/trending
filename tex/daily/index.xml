<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-23T01:46:17Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Lysxia/gradual-eff-paper</title>
    <updated>2023-07-23T01:46:17Z</updated>
    <id>tag:github.com,2023-07-23:/Lysxia/gradual-eff-paper</id>
    <link href="https://github.com/Lysxia/gradual-eff-paper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gradual Effect Handlers&lt;/h1&gt; &#xA;&lt;p&gt;This project is a formalization of a language with effect handlers and gradual types in Agda. It consists of a core calculus with effect handlers and explicit casts, and its metatheory. This development is written in Literate Agda; it can be read as a PDF document &lt;code&gt;doc.pdf&lt;/code&gt; included in the archive, which also illustrates the proof of gradual guarantee with simulation diagrams.&lt;/p&gt; &#xA;&lt;p&gt;This development was authored by&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://poisson.chat/&#34;&gt;Li-yao Xia&lt;/a&gt; &lt;a href=&#34;https://homepages.inf.ed.ac.uk/wadler/&#34;&gt;Philip Wadler&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;as part of the Huawei SAGE project at the University of Edinburgh. It is based on code originally written by&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wphomes.soic.indiana.edu/jsiek/&#34;&gt;Jeremy Siek&lt;/a&gt;, &lt;a href=&#34;http://www2.informatik.uni-freiburg.de/~thiemann/&#34;&gt;Peter Thiemann&lt;/a&gt;, and &lt;a href=&#34;https://homepages.inf.ed.ac.uk/wadler/&#34;&gt;Philip Wadler&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://github.com/Lysxia/gradual-eff-paper&#34;&gt;Link to the code repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;The definitions and proofs can be checked with the following command (it takes a couple of minutes):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make check&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The PDF document can be (re)compiled with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make doc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The main dependencies are (latest versions as of May 2023):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Agda 2.6.3&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Agda/agda-stdlib&#34;&gt;standard-library 1.7.2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pandoc 2.19.2&lt;/li&gt; &#xA; &lt;li&gt;Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chdemko/pandoc-latex-environment&#34;&gt;pandoc-latex-environment&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the syntax of types and the imprecision relation (&lt;code&gt;Types.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;the syntax of terms (intrinsically typed) (&lt;code&gt;Core.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;an operational semantics, and an evaluator, which doubles as a proof of progress (&lt;code&gt;Progress.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;the imprecision relation between terms (&lt;code&gt;Prec.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;the proof of the gradual guarantee, as a simulation proof (&lt;code&gt;SimAux.lagda.md&lt;/code&gt;, &lt;code&gt;Sim.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;some example terms (&lt;code&gt;Example.lagda.md&lt;/code&gt;, made more readable using an auxiliary module &lt;code&gt;Sugar.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;general-purpose definitions (&lt;code&gt;Utils.lagda.md&lt;/code&gt;, &lt;code&gt;VecPointwise2.lagda.md&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/amortized-optimization-tutorial</title>
    <updated>2023-07-23T01:46:17Z</updated>
    <id>tag:github.com,2023-07-23:/facebookresearch/amortized-optimization-tutorial</id>
    <link href="https://github.com/facebookresearch/amortized-optimization-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tutorial on amortized optimization for learning to optimize over continuous domains&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tutorial on Amortized Optimization&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the source code for the paper &lt;a href=&#34;https://arxiv.org/abs/2202.00665&#34;&gt;Tutorial on amortized optimization for learning to optimize over continuous domains&lt;/a&gt; by &lt;a href=&#34;http://bamos.github.io&#34;&gt;Brandon Amos&lt;/a&gt;. The main LaTeX source is in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper&#34;&gt;paper&lt;/a&gt; and the source code examples are in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code&#34;&gt;code&lt;/a&gt;. The code that generates the following plots is also in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures&#34;&gt;code/figures&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/main-example.py&#34;&gt;main-example.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/opt.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/learning-obj.png?raw=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/learning-reg.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/learning-rl.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/maxent-animation.py&#34;&gt;maxent-animation.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/maxent.gif?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/maxent.py&#34;&gt;maxent.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/maxent.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/ctrl.py&#34;&gt;ctrl.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/ctrl.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/imaml.py&#34;&gt;imaml.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/imaml.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/fixed-point.py&#34;&gt;fixed-point.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/fp.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/loss-comp.py&#34;&gt;loss-comp.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/loss-comp.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/code/figures/smoothed-loss.py&#34;&gt;smoothed-loss.py&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/amortized-optimization-tutorial/main/paper/fig/smoothed-loss.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;The source code for this tutorial, plots, and sphere experiment is licensed under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34;&gt;CC BY-NC 4.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>