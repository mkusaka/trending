<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-29T01:41:14Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jespercockx/agda-lecture-notes</title>
    <updated>2022-10-29T01:41:14Z</updated>
    <id>tag:github.com,2022-10-29:/jespercockx/agda-lecture-notes</id>
    <link href="https://github.com/jespercockx/agda-lecture-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agda lecture notes for the Functional Programming course at TU Delft&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>Simulation-Software-Engineering/Lecture-Material</title>
    <updated>2022-10-29T01:41:14Z</updated>
    <id>tag:github.com,2022-10-29:/Simulation-Software-Engineering/Lecture-Material</id>
    <link href="https://github.com/Simulation-Software-Engineering/Lecture-Material" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Material for the Simulation Software Engineering Lecture&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Simulation Software Engineering Lecture Material&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Simulation-Software-Engineering/lecture-materials/actions/workflows/markdownlint.yml/badge.svg?sanitize=true&#34; alt=&#34;markdownlint&#34;&gt; &lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/actions/workflows/create-pdfs-from-markdown.yml&#34;&gt;&lt;img src=&#34;https://github.com/Simulation-Software-Engineering/lecture-materials/actions/workflows/create-pdfs-from-markdown.yml/badge.svg?sanitize=true&#34; alt=&#34;PDFs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC%20BY%204.0-blue.svg?sanitize=true&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Material of the Simulation Software Engineering lecture. There are different way how to get an overview:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Look at &lt;code&gt;timetable.md&lt;/code&gt; of the &lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/raw/main/timetable.md&#34;&gt;current course&lt;/a&gt; or &lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/00_organization/wt2122/timetable.md&#34;&gt;previous courses&lt;/a&gt;, or&lt;/li&gt; &#xA; &lt;li&gt;Look at the &lt;code&gt;README.md&lt;/code&gt; files of each chapter / folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that we update the material over the course of each semester.&lt;/p&gt; &#xA;&lt;h2&gt;List of chapters&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/01_version_control&#34;&gt;Version Control&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/02_virtualization_and_containers&#34;&gt;Virtualization and Containers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/03_building_and_packaging&#34;&gt;Building and Packaging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/04_documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/05_testing_and_ci&#34;&gt;Testing and CI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Simulation-Software-Engineering/Lecture-Material/tree/main/06_miscellaneous&#34;&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Linting&lt;/h2&gt; &#xA;&lt;p&gt;The markdown files can be checked using &lt;a href=&#34;https://github.com/markdownlint/markdownlint/&#34;&gt;markdownlint&lt;/a&gt;. Once the linter is installed one can run it locally from the root of this repository using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mdl .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will automatically read the markdownlint configuration of this repository. The linter is configured in the files &lt;code&gt;.mdl.rb&lt;/code&gt; and &lt;code&gt;.mdlrc&lt;/code&gt;. The majority of the configuration is done in &lt;code&gt;.mdl.rb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Third-party content&lt;/h2&gt; &#xA;&lt;p&gt;In several parts of the material, we use content from&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Irving, Hertweck, Johnston, Ostblom, Wickham, and Wilson: &lt;a href=&#34;https://merely-useful.tech/py-rse&#34;&gt;Research Software Engineering with Python&lt;/a&gt;, 2021,&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;a book, which we also recommend to recap Git/Bash/Python basics.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This work is licensed under a &lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34; alt=&#34;CC BY 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xcfcode/Summarization-Papers</title>
    <updated>2022-10-29T01:41:14Z</updated>
    <id>tag:github.com,2022-10-29:/xcfcode/Summarization-Papers</id>
    <link href="https://github.com/xcfcode/Summarization-Papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Summarization Papers&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/pic/summary.png&#34; width=&#34;30&#34;&gt;Summarization Papers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/pic/collect.png&#34; width=&#34;30&#34;&gt;I am trying to collect 50 summarization papers before 2016.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Organized by &lt;a href=&#34;http://xcfeng.net/&#34;&gt;Xiachong Feng&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributor&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OrangeInSouth&#34;&gt;Yichong Huang&lt;/a&gt;, &lt;a href=&#34;https://github.com/hzyang95&#34;&gt;Haozheng Yang&lt;/a&gt;, &lt;a href=&#34;https://github.com/krystalan&#34;&gt;Jiaan Wang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Summarization Learning Route&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://xcfeng.net/res/summarization-route.pdf&#34;&gt;Summarization Learning Route (with link)&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/pic/route.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Trending&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/pic/trending.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Presentations &amp;amp;&amp;amp; Notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/Dialogue_Summarization_DAMO.pdf&#34;&gt;Dialogue Summarization (2022.1)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/Cross-lingual_Summarization.pdf&#34;&gt;Cross-lingual Summarization&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/GQQRRS3F7p4Zv6wSuDh0ng&#34;&gt;如何把DialoGPT用到对话摘要任务？@ ACL 2021&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/628OAOW1_-Yc_vQbeuY_uA&#34;&gt;对话摘要最新进展简述&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/Dialogue_Summarization.pdf&#34;&gt;Dialogue Summarization (2021.5)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/x3zqGc4pqh4x3q_uorNKcg&#34;&gt;融入常识知识的生成式对话摘要&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/Be7AYUPdux8NvAO4wo6_fg&#34;&gt;会议摘要有难度？快来引入对话篇章结构信息&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/tLdLGSFl229selxeogQk-w&#34;&gt;文本摘要论文列表(Chinese)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/Aye9FBwG-v2JO2MLoEjo0g&#34;&gt;事实感知的生成式文本摘要(Chinese)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/Ce6jtp-gTtqeh9lgi-kHtQ&#34;&gt;多模态摘要简述(Chinese)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/NGpDrYilAeuH6pQji0ujaA&#34;&gt;文本摘要简述&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-blog-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/Multi-modal-Summarization.pdf&#34;&gt;Multi-modal Summarization&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/acl2020-summarization.pdf&#34;&gt;ACL20 Summarization&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E7%AE%80%E8%BF%B0.pdf&#34;&gt;文本摘要简述 (Chinese)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/presentation/ACL19%20Summarization.pdf&#34;&gt;ACL19 Summarization&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-presentations-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/notes/Brief-intro-to-summarization.pdf&#34;&gt;Brief intro to summarization (Chinese)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-notes-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/notes/EMNLP19_Summarization.pdf&#34;&gt;EMNLP19 Summarization (Chinese)&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-notes-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/paper-slides/A%20Simple%20Theoretical%20Model%20of%20Importance%20for%20Summarization.pdf&#34;&gt;ACL19-A Simple Theoretical Model of Importance for Summarization&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-papers-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/slides/paper-slides/Multimodal%20Abstractive%20Summarization%20for%20How2%20Videos.pdf&#34;&gt;ACL19-Multimodal Abstractive Summarization for How2 Videos&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-papers-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MuLD: The Multitask Long Document Benchmark&lt;/strong&gt; &lt;em&gt;G Thomas Hudson, Noura Al Moubayed&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.07362&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ghomasHudson/muld&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EXPLAINABOARD: An Explainable Leaderboard for NLP&lt;/strong&gt; &lt;em&gt;Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, Graham Neubig&lt;/em&gt; &lt;a href=&#34;http://explainaboard.nlpedia.ai/ExplainaBoard.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;http://explainaboard.nlpedia.ai/leaderboard/task-summ/index.php&#34;&gt;[ExplainaBoard]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GLGE: A New General Language Generation Evaluation Benchmark&lt;/strong&gt; &lt;em&gt;Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou, Ming Gong, Pengcheng Wang, Jiusheng Chen, Daxin Jiang, Jiancheng Lv, Ruofei Zhang, Winnie Wu, Ming Zhou, Nan Duan&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.11928&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/glge&#34;&gt;[benchmark]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions&lt;/strong&gt; &lt;em&gt;Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.09894&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive dialogue summarization is to generate a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted great attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures and unclear topic boundaries. This survey provides a comprehensive investigation on existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks and using additional data.A list of datasets under different scenarios and widely-accepted evaluation metrics are summarized for completeness. After that, the trends of scenarios and techniques are summarized, together with deep insights on correlations between extensively exploited features and different scenarios. Based on these analyses, we recommend future directions including more controlled and complicated scenarios, technical innovations and comparisons, publicly available datasets in special domains, etc. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey of Automatic Text Summarization Using Graph Neural Networks&lt;/strong&gt; &lt;em&gt;Marco Ferdinand Salchner, Adam Jatowt&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.536/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Although automatic text summarization (ATS) has been researched for several decades, the application of graph neural networks (GNNs) to this task started relatively recently. In this survey we provide an overview on the rapidly evolving approach of using GNNs for the task of automatic text summarization. In particular we provide detailed information on the functionality of GNNs in the context of ATS, and a comprehensive overview of models utilizing this approach. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, Jie Zhou&lt;/em&gt; &lt;code&gt;TACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.12515&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Cross-lingual summarization is the task of generating a summary in one language (e.g., English) for the given document(s) in a different language (e.g., Chinese). Under the globalization background, this task has attracted increasing attention of the computational linguistics community. Nevertheless, there still remains a lack of comprehensive review for this task. Therefore, we present the first systematic critical review on the datasets, approaches, and challenges in this field. Specifically, we carefully organize existing datasets and approaches according to different construction methods and solution paradigms, respectively. For each type of datasets or approaches, we thoroughly introduce and summarize previous efforts and further compare them with each other to provide deeper analyses. In the end, we also discuss promising directions and offer our thoughts to facilitate future research. This survey is for both beginners and experts in cross-lingual summarization, and we hope it will serve as a starting point as well as a source of new ideas for researchers and engineers interested in this area. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics&lt;/strong&gt; &lt;em&gt;uan Yee Koh, Jiaxin Ju, Ming Liu, Shirui Pan&lt;/em&gt; &lt;code&gt;ACM Computing Surveys&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.00939&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Long documents such as academic articles and business reports have been the standard format to detail out important issues and complicated subjects that require extra attention. An automatic summarization system that can effectively condense long documents into short and concise texts to encapsulate the most important information would thus be significant in aiding the reader&#39;s comprehension. Recently, with the advent of neural architectures, significant research efforts have been made to advance automatic text summarization systems, and numerous studies on the challenges of extending these systems to the long document domain have emerged. In this survey, we provide a comprehensive overview of the research on long document summarization and a systematic evaluation across the three principal components of its research setting: benchmark datasets, summarization models, and evaluation metrics. For each component, we organize the literature within the context of long document summarization and conduct an empirical analysis to broaden the perspective on current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in this rapidly growing field. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-document Summarization via Deep Learning Techniques: A Survey&lt;/strong&gt; &lt;em&gt;Congbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, QUAN Z. Sheng&lt;/em&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3529754&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Embedding Knowledge for Document Summarization: A Survey&lt;/strong&gt; &lt;em&gt;Yutong Qu, Wei Emma Zhang, Jian Yang, Lingfei Wu, Jia Wu, Xindong Wu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.11190&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Dialogue Summarization: Recent Advances and New Frontiers&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Bing Qin&lt;/em&gt; &lt;code&gt;IJCAI 2022, Survey Track&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.03175&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Text Summarization Methods: A Comprehensive Review&lt;/strong&gt; &lt;em&gt;Divakar Yadav, Jalpa Desai, Arun Kumar Yadav&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.01849&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods&lt;/strong&gt; &lt;em&gt;Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, Hua Wu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.05227&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recent Advances in Neural Text Generation: A Task-Agnostic Survey&lt;/strong&gt; &lt;em&gt;Chen Tang, Frank Guerin, Yucheng Li, Chenghua Lin&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.03047&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Survey of Hallucination in Natural Language Generation&lt;/strong&gt; &lt;em&gt;Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.03629&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Retrieval-Augmented Text Generation&lt;/strong&gt; &lt;em&gt;Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.01110&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models&lt;/strong&gt; &lt;em&gt;Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.05337&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey of Pretrained Language Models Based Text Generation&lt;/strong&gt; &lt;em&gt;Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.05273&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Comprehensive Review on Summarizing Financial News Using Deep Learning&lt;/strong&gt; &lt;em&gt;Saurabh Kamal, Sahil Sharma&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.10118&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Multi-modal Summarization&lt;/strong&gt; &lt;em&gt;Anubhav Jangra, Adam Jatowt, Sriparna Saha, Mohammad Hasanuzzaman&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.05199&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing&lt;/strong&gt; &lt;em&gt;Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.13586&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pretrained Language Models for Text Generation: A Survey&lt;/strong&gt; &lt;em&gt;Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen&lt;/em&gt; &lt;code&gt;IJCAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.10311&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey of Recent Abstract Summarization Techniques&lt;/strong&gt; &lt;em&gt;Diyah Puspitaningrum&lt;/em&gt; &lt;code&gt;ICICT21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.00824&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey of the State-of-the-Art Models in Neural Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;AYESHA AYUB SYED, FORD LUMBAN GAOL, TOKURO MATSUO&lt;/em&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328413&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic summarization of scientific articles: A survey&lt;/strong&gt; &lt;em&gt;Nouf Ibrahim Altmami, Mohamed El Bachir Menai&lt;/em&gt; &lt;code&gt;Journal of King Saud University - Computer and Information Sciences&lt;/code&gt; &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1319157820303554&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-document Summarization via Deep Learning Techniques: A Survey&lt;/strong&gt; &lt;em&gt;Congbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, Quan Z. Sheng&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.04843&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Learning Based Abstractive Text Summarization: Approaches, Datasets, Evaluation Measures, and Challenges&lt;/strong&gt; &lt;em&gt;Dima Suleiman, Arafat A. Awajan&lt;/em&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/Deep-Learning-Based-Abstractive-Text-Summarization%3A-Suleiman-Awajan/b7da726c244287748575ef404009609afde45bea&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey of Knowledge-Enhanced Text Generation&lt;/strong&gt; &lt;em&gt;Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.04389&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;From Standard Summarization to New Tasks and Beyond: Summarization with Manifold Information&lt;/strong&gt; &lt;em&gt;Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao, Rui Yan&lt;/em&gt; &lt;code&gt;IJCAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.04684&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Abstractive Text Summarization with Sequence-to-Sequence Models&lt;/strong&gt; &lt;em&gt;Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, Chandan K. Reddy&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1812.02303&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Neural Network-Based Summarization Methods&lt;/strong&gt; &lt;em&gt;Yue Dong&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1804.04589&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automated text summarisation and evidence-based medicine: A survey of two domains&lt;/strong&gt; &lt;em&gt;Abeed Sarker, Diego Molla, Cecile Paris&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1706.08162&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Keyword Extraction for Text Summarization: A Survey&lt;/strong&gt; &lt;em&gt;Santosh Kumar Bharti, Korra Sathya Babu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1704.03242&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization Techniques: A Brief Survey&lt;/strong&gt; &lt;em&gt;Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D. Trippe, Juan B. Gutierrez, Krys Kochut&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1707.02268&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recent automatic text summarization techniques: a survey&lt;/strong&gt; &lt;em&gt;Mahak Gambhir, Vishal Gupta&lt;/em&gt; &lt;a href=&#34;https://link.springer.com/article/10.1007/s10462-016-9475-9&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Toolkit&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summary Workbench: Unifying Application and Evaluation of Text Summarization Models&lt;/strong&gt; &lt;em&gt;Shahbaz Syed, Dominik Schwabe, Martin Potthast&lt;/em&gt; &lt;code&gt;EMNLP 2022 Demo&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.09587&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://tldr.demo.webis.de/summarize&#34;&gt;[demo]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; This paper presents Summary Workbench, a new tool for developing and evaluating text summarization models. New models and evaluation measures can be easily integrated as Docker-based plugins, allowing to examine the quality of their summaries against any input and to evaluate them using various evaluation measures. Visual analyses combining multiple measures provide insights into the models&#39; strengths and weaknesses. The tool is hosted at \url{this https URL} and also supports local deployment for private resources. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration&lt;/strong&gt; &lt;em&gt;Eran Hirsch, Alon Eirew, Ori Shapira, Avi Caciularu, Arie Cattan, Ori Ernst, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, Ido Dagan&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.11621&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://biu-nlp.github.io/iFACETSUM/WebApp/client/&#34;&gt;[demo]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummerTime: Text Summarization Toolkit for Non-experts&lt;/strong&gt; &lt;em&gt;Ansong Ni, Zhangir Azerbayev, Mutethia Mutuma, Troy Feng, Yusen Zhang, Tao Yu, Ahmed Hassan Awadallah, Dragomir Radev&lt;/em&gt; &lt;code&gt;EMNLP 2021 Demo Track&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.12738&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/SummerTime&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summary Explorer: Visualizing the State of the Art in Text Summarization&lt;/strong&gt; &lt;em&gt;Shahbaz Syed, Tariq Yousef, Khalid Al-Khatib, Stefan Jänicke, Martin Potthast&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.01879&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://tldr.webis.de/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;fastnlp/fastSum&lt;/strong&gt; &lt;a href=&#34;https://github.com/fastnlp/fastSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graph4NLP&lt;/strong&gt; &lt;a href=&#34;https://github.com/graph4ai/graph4nlp&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://github.com/graph4ai/graph4nlp/tree/master/examples/pytorch/summarization&#34;&gt;[summarization]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CTRLsum: Towards Generic Controllable Text Summarization&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.04281&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hyunwoongko/summarizers&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenNMT-py: Open-Source Neural Machine Translation&lt;/strong&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W18-1817.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenNMT/OpenNMT-py&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit written in Python.&lt;/strong&gt; &lt;a href=&#34;https://github.com/pytorch/fairseq&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LeafNATS: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Tian Shi, Ping Wang, Chandan K. Reddy&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N19-4012/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tshi04/LeafNATS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TransformerSum&lt;/strong&gt; &lt;a href=&#34;https://github.com/HHousen/TransformerSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Analysis&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Analysis-analysis-red&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Meta%20Evaluation-evaluation-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Bias-bias-orange&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Architecture-architecture-blue&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analyzing Multi-Task Learning for Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Frederic Kirstein, Jan Philip Wahle, Terry Ruas, Bela Gipp&lt;/em&gt; `` &lt;a href=&#34;https://arxiv.org/abs/2210.14606&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite the recent success of multi-task learning and pre-finetuning for natural language understanding, few works have studied the effects of task families on abstractive text summarization. Task families are a form of task grouping during the pre-finetuning stage to learn common skills, such as reading comprehension. To close this gap, we analyze the influence of multi-task learning strategies using task families for the English abstractive text summarization task. We group tasks into one of three strategies, i.e., sequential, simultaneous, and continual multi-task learning, and evaluate trained models through two downstream tasks. We find that certain combinations of task families (e.g., advanced reading comprehension and natural language inference) positively impact downstream performance. Further, we find that choice and combinations of task families influence downstream performance more than the training scheme, supporting the use of task families for abstractive text summarization. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On Decoding Strategies for Neural Text Generators&lt;/strong&gt; &lt;em&gt;Gian Wiher, Clara Meister, Ryan Cotterell&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.15721&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training Dynamics for Text Summarization Models&lt;/strong&gt; &lt;em&gt;Tanya Goyal, Jiacheng Xu, Junyi Jessy Li, Greg Durrett&lt;/em&gt; [https://arxiv.org/abs/2110.08370]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Does Summary Evaluation Survive Translation to Other Languages?&lt;/strong&gt; &lt;em&gt;Neslihan Iskender, Oleg Vasilyev, Tim Polzehl, John Bohannon, Sebastian Möller&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.08129&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How well do you know your summarization datasets?&lt;/strong&gt; &lt;em&gt;Priyam Tejaswin, Dhruv Naik, Pengfei Liu&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.11388&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/priyamtejaswin/howwelldoyouknow&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution&lt;/strong&gt; &lt;em&gt;Jiacheng Xu, Greg Durrett&lt;/em&gt; &lt;code&gt;ACL2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.539/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jiacheng-xu/sum-interpret&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;To Point or Not to Point: Understanding How Abstractive Summarizers Paraphrase Text&lt;/strong&gt; &lt;em&gt;Matt Wilber, William Timkey, Marten Van Schijndel&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.01581&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mwilbz/pointer-generator-analysis&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What Makes a Good Summary? Reconsidering the Focus of Automatic Summarization&lt;/strong&gt; &lt;em&gt;Maartje ter Hoeve, Julia Kiseleva, Maarten de Rijke&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07619&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intrinsic Evaluation of Summarization Datasets&lt;/strong&gt; &lt;em&gt;Rishi Bommasani, Claire Cardie&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.649/&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics&lt;/strong&gt; &lt;em&gt;Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu&lt;/em&gt; &lt;code&gt;COLING20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.04096&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/manikbhandari/RevisitSummEvalMetrics&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;At Which Level Should We Extract? An Empirical Analysis on Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Qingyu Zhou, Furu Wei, Ming Zhou&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.02664&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Corpora Evaluation and System Bias detection in Multi Document Summarization&lt;/strong&gt; &lt;em&gt;Alvin Dey, Tanya Chowdhury, Yash Kumar, Tanmoy Chakraborty&lt;/em&gt; &lt;code&gt;Findings of EMNLP&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.254/&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries&lt;/strong&gt; &lt;em&gt;Daniel Deutsch, Dan Roth&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.12495&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/CogComp/content-analysis-experiments&#34;&gt;[code]&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Understanding Neural Abstractive Summarization Models via Uncertainty&lt;/strong&gt; &lt;em&gt;Jiacheng Xu, Shrey Desai, Greg Durrett&lt;/em&gt; &lt;code&gt;EMNLP20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.07882&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jiacheng-xu/text-sum-uncertainty&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Re-evaluating Evaluation in Text Summarization&lt;/strong&gt; &lt;em&gt;Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.07100&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/neulab/REALSumm&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems&lt;/strong&gt; &lt;em&gt;Yiran Chen, Pengfei Liu, Ming Zhong, Zi-Yi Dou, Danqing Wang, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.05139&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zide05/CDEvalSumm&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What Have We Achieved on Text Summarization?&lt;/strong&gt; &lt;em&gt;Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, Yue Zhang&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.04529&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Ke Shi, Nancy F. Chen&lt;/em&gt; &lt;code&gt;Findings of EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.13983&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-bias-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization as Text Matching&lt;/strong&gt; &lt;em&gt;Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.08795&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/maszhongming/MatchSum&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-architecture-blue&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-bias-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Text Summarization: A Critical Evaluation&lt;/strong&gt; &lt;em&gt;Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, Richard Socher&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-1051/&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Earlier Isn’t Always Better:Sub-aspect Analysis on Corpus and System Biases in Summarization&lt;/strong&gt; &lt;em&gt;Taehee Jung, Dongyeop Kang, Lucas Mentch, Eduard Hovy&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1908.11723&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/dykang/biassum&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-bias-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Closer Look at Data Bias in Neural Extractive Summarization Models&lt;/strong&gt; &lt;em&gt;Ming Zhong, Danqing Wang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;EMNLP19 Workshop&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.13705&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-bias-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses&lt;/strong&gt; &lt;em&gt;Matt Grenander, Yue Dong, Jackie Chi Kit Cheung, Annie Louis&lt;/em&gt; &lt;code&gt;EMNLP19 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.04028&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-bias-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Searching for Effective Neural Extractive Summarization: What Works and What&#39;s Next&lt;/strong&gt; &lt;em&gt;Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1907.03491&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/maszhongming/Effective_Extractive_Summarization&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-architecture-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Content Selection in Deep Learning Models of Summarization&lt;/strong&gt; &lt;em&gt;Chris Kedzie, Kathleen McKeown, Hal Daumé III&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1208/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kedz/nnsum/tree/emnlp18-release&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-architecture-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Thesis&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Principled Approaches to Automatic Text Summarization&lt;/strong&gt; &lt;em&gt;Maxime Peyrard&lt;/em&gt; &lt;a href=&#34;https://tuprints.ulb.tu-darmstadt.de/9012/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Text Summarization and Generation&lt;/strong&gt; &lt;em&gt;Piji Li&lt;/em&gt; &lt;a href=&#34;http://lipiji.com/docs/thesis.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Theory&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bayesian Active Summarization&lt;/strong&gt; &lt;em&gt;Alexios Gidiotis, Grigorios Tsoumakas&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.04480&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RefSum: Refactoring Neural Summarization&lt;/strong&gt; &lt;em&gt;Yixin Liu, Zi-Yi Dou, Pengfei Liu&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.07210&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yixinL7/Refactoring-Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Principled Approaches to Automatic Text Summarization&lt;/strong&gt; &lt;em&gt;Maxime Peyrard&lt;/em&gt; &lt;a href=&#34;https://tuprints.ulb.tu-darmstadt.de/9012/&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-thesis-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KLearn: Background Knowledge Inference from Summarization Data&lt;/strong&gt; &lt;em&gt;Maxime Peyrard, Robert West&lt;/em&gt; &lt;code&gt;Findings of EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.06213&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/epfl-dlab/KLearn&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Simple Theoretical Model of Importance for Summarization&lt;/strong&gt; &lt;em&gt;Maxime Peyrard&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1101/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle&lt;/strong&gt; &lt;em&gt;Peter West, Ari Holtzman, Jan Buys, Yejin Choi&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.07405&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/peterwestuw/BottleSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Conference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/harvardnlp/sent-summary&#34;&gt;CNN-DailyMail&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/K16-1028/&#34;&gt;Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SIGNLL16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2008T19&#34;&gt;New York Times&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2008T19&#34;&gt;The New York Times Annotated Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://duc.nist.gov/data.html&#34;&gt;DUC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/W04-1003/&#34;&gt;The Effects Of Human Variation In DUC Summarization Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/harvardnlp/sent-summary&#34;&gt;Gigaword&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1509.00685&#34;&gt;A Neural Attention Model For Abstractive Sentence Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP15&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://lil.nlp.cornell.edu/newsroom/&#34;&gt;Newsroom&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/N18-1065&#34;&gt;Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/EdinburghNLP/XSum&#34;&gt;Xsum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D18-1206/&#34;&gt;Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Alex-Fabbri/Multi-News&#34;&gt;Multi-News&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-document News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.01749&#34;&gt;Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.12237&#34;&gt;SAMSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-party conversation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.12237&#34;&gt;SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://groups.inf.ed.ac.uk/ami/download/&#34;&gt;AMI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meeting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://groups.inf.ed.ac.uk/ami/download/&#34;&gt;The AMI Meeting Corpus: A pre-announcement. &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://groups.inf.ed.ac.uk/ami/icsi/download/&#34;&gt;ICSI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meeting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://groups.inf.ed.ac.uk/ami/icsi/&#34;&gt;The ICSI Meeting Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.nlpr.ia.ac.cn/cip/jjzhang.htm&#34;&gt;MSMO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-modal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D18-1448/&#34;&gt;MSMO: Multimodal Summarization with Multimodal Output&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/srvk/how2-dataset&#34;&gt;How2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-modal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.00347&#34;&gt;How2: A Large-scale Dataset for Multimodal Language Understanding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NIPS18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://cs.stanford.edu/~myasu/projects/scisumm_net/&#34;&gt;ScisummNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.01716&#34;&gt;ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/armancohan/long-summarization&#34;&gt;PubMed, ArXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.05685&#34;&gt;A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/levguy/talksumm&#34;&gt;TALKSUMM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1204/&#34;&gt;TALKSUMM: A Dataset and Scalable Annotation Method for Scientiﬁc Paper Summarization Based on Conference Talks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/FiscalNote/BillSum&#34;&gt;BillSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Legal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D19-5406/&#34;&gt;BillSum: A Corpus for Automatic Summarization of US Legislation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://icrc.hitsz.edu.cn/Article/show/139.html&#34;&gt;LCSTS&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese Weibo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D15-1229/&#34;&gt;LCSTS: A Large Scale Chinese Short Text Summarization Dataset &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP15&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mahnazkoupaee/WikiHow-Dataset&#34;&gt;WikiHow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Online Knowledge Base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.09305&#34;&gt;WikiHow: A Large Scale Text Summarization Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/UKPLab/emnlp2017-cmapsum-corpus/&#34;&gt;Concept-map-based MDS Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Educational Multi-document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D17-1320/&#34;&gt;Bringing Structure into Summaries : Crowdsourcing a Benchmark Corpus of Concept Maps&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum&#34;&gt;WikiSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wikipedia Multi-document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.10198&#34;&gt;Generating Wikipedia By Summarizing Long Sequence&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ICLR18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Diego999/GameWikiSum&#34;&gt;GameWikiSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Game Multi-document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.06851&#34;&gt;GameWikiSum : a Novel Large Multi-Document Summarization Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LREC20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://www.nlpr.ia.ac.cn/cip/dataset.htm&#34;&gt;En2Zh CLS, Zh2En CLS&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cross-Lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.00156&#34;&gt;NCLS: Neural Cross-Lingual Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yingtaomj/Learning-towards-Abstractive-Timeline-Summarization&#34;&gt;Timeline Summarization Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Baidu timeline&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.ijcai.org/Proceedings/2019/686&#34;&gt;Learning towards Abstractive Timeline Summarization &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;IJCAI19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ctr4si/MMN&#34;&gt;Reddit TIFU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;online discussion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.00783&#34;&gt;Abstractive Summarization of Reddit Posts with Multi-level Memory Networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Junjieli0704/ASN&#34;&gt;TripAtt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Review&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D19-1297/&#34;&gt;Attribute-aware Sequence Network for Review Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1_YH5cBtvNnUNJjGj7kiTMjuHydBqWYQT/view?usp=drive_open&#34;&gt;Reader Comments Summarization Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Comments-based Weibo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.05407&#34;&gt;Abstractive Text Summarization by Incorporating Reader Comments &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://evasharma.github.io/bigpatent/&#34;&gt;BIGPATENT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Patent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.03741&#34;&gt;BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CurationCorp/curation-corpus&#34;&gt;Curation Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CurationCorp/curation-corpus&#34;&gt;Curation Corpus for Abstractive Text Summarisation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WHUIR/MATINF&#34;&gt;MATINF&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-task&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.12302&#34;&gt;MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/recitalAI/MLSUM&#34;&gt;MLSUM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-Lingual Summarization Dataset&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.14900&#34;&gt;MLSUM: The Multilingual Summarization Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue(Debate)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Argumentative Dialogue Summary Corpus&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/N15-1046/&#34;&gt;Using Summarization to Discover Argument Facets in Online Idealogical Dialog&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL15&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/complementizer/wcep-mds-dataset&#34;&gt;WCEP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News Multi-document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.10070&#34;&gt;A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL20 Short&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml&#34;&gt;ArgKP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Argument-to-key Point Mapping&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.01619&#34;&gt;From Arguments to Key Points: Towards Automatic Argument Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RevanthRameshkumar/CRD3&#34;&gt;CRD3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.459/&#34;&gt;Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/IlyaGusev/gazeta&#34;&gt;Gazeta&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Russian news&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11063&#34;&gt;Dataset for Automatic Summarization of Russian News&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://msnews.github.io/&#34;&gt;MIND&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;English news recommendation, Summarization, Classification, Entity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.331/&#34;&gt;MIND: A Large-scale Dataset for News Recommendation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/pltrdy/autoalign&#34;&gt;public_meetings&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;french meeting(test set)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.829&#34;&gt;Align then Summarize: Automatic Alignment Methods for Summarization Corpus Creation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LREC&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Enron&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Email&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/L14-1028/&#34;&gt;Building a Dataset for Summarization and Keyword Extraction from Emails&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2014&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Columbia&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Email&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xcfcode/Summarization-Papers/main/%5Bhttps://www.aclweb.org/anthology/N04-4027.pdf%5D(https://dl.acm.org/doi/10.5555/1613984.1614011)&#34;&gt;Summarizing Email Threads&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2004&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/bc3.html&#34;&gt;BC3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Email&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.ufv.ca/media/assets/computer-information-systems/gabriel-murray/publications/aaai08.pdf&#34;&gt;A publicly available annotated corpus for supervised email summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/esdurmus/Wikilingua&#34;&gt;WikiLingua&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cross-Lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.03093&#34;&gt;WikiLingua- A New Benchmark Dataset for Cross-Lingual Abstractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Findings of EMNLP20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://eie.usts.edu.cn/prj/NLPoSUST/LcsPIRT.htm&#34;&gt;LcsPIRT&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3407911&#34;&gt;Global Encoding for Long Chinese Text Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TALLIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lxj5957/CLTS-Dataset&#34;&gt;CLTS&lt;/a&gt;，&lt;a href=&#34;https://github.com/lxj5957/CLTS-plus-Dataset&#34;&gt;CLTS-plus&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-60450-9_42&#34;&gt;CLTS: A New Chinese Long Text Summarization Dataset&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.04253&#34;&gt;CLTS+: A New Chinese Long Text Summarization Dataset with Abstractive Summaries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NLPCC20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yingtaomj/VMSMO&#34;&gt;VMSMO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-modal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05406&#34;&gt;VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yaolu/Multi-XScience&#34;&gt;Multi-XScience&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.14235&#34;&gt;Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientiﬁc Articles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP20 short&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/allenai/scitldr&#34;&gt;SCITLDR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.15011&#34;&gt;TLDR: Extreme Summarization of Scientific Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Findings of EMNLP20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WING-NUS/scisumm-corpus&#34;&gt;scisumm-corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/sh/t2cp7ml1kb8ako0/AADmS2RMfJvLbukyQbb08CGGa?dl=0&#34;&gt;QBSUM&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Query-Based Chinese&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.14108&#34;&gt;QBSUM: a Large-Scale Query-Based Document Summarization Dataset from Real-world Applications&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Computer Speech &amp;amp; Language&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/aquamuse&#34;&gt;qMDS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Query-Based Multi-Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.12694&#34;&gt;AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fajri91/sum_liputan6&#34;&gt;Liputan6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Indonesian&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.00679.pdf&#34;&gt;Liputan6: A Large-scale Indonesian Dataset for Text Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AACL20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ej0cl6/SportsSum&#34;&gt;SportsSum&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sports Game&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://khhuang.me/docs/aacl2020sportssum.pdf&#34;&gt;Generating Sports News from Live Commentary: A Chinese Dataset for Sports Game Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AACL20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/neulab/wikiasp&#34;&gt;WikiAsp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Aspect-based&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.07832&#34;&gt;WikiAsp: A Dataset for Multi-domain Aspect-based Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Transaction of the ACL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Hellisotherpeople/DebateSum&#34;&gt;DebateSum&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Query%20Focused-purple&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;argument&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.07251&#34;&gt;DebateSum:A large-scale argument mining and summarization dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ARGMIN 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/amanpreet692/Open4Business&#34;&gt;Open4Business&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Business&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.07636&#34;&gt;Open4Business (O4B): An Open Access Dataset for Summarizing Business Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Workshop on Dataset Curation and Security-NeurIPS 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/moussaKam/OrangeSum&#34;&gt;OrangeSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;French&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.12321&#34;&gt;BARThez: a Skilled Pretrained French Sequence-to-Sequence Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cuhksz-nlp/HET-MC&#34;&gt;Medical Conversation&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;medical conversation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.63/&#34;&gt;Summarizing Medical Conversations via Identifying Important Utterances&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/huawei-noah/sumtitles&#34;&gt;SumTitles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;movie dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.503/&#34;&gt;SumTitles: a Summarization Dataset with Low Extractiveness&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.kaggle.com/datasets/prithwirajsust/bengali-news-summarization-dataset&#34;&gt;BANS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bengali news&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Bengali Abstractive News Summarization (BANS): A Neural Attention Approach&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TCCE-2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ypnlp/coling&#34;&gt;e-commerce&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;E-commerce&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.502/&#34;&gt;On the Faithfulness for E-commerce Product Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;TWEETSUM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Twitter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.504/&#34;&gt;TWEETSUM: Event-oriented Social Summarization Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/stangelid/qt&#34;&gt;SPACE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Opinion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.04443&#34;&gt;Extractive Opinion Summarization in Quantized Transformer Spaces&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TACL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hooshvare/pn-summary&#34;&gt;pn-summary&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Persian&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.11204&#34;&gt;Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;csicc2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RowitZou/topic-dialog-summ&#34;&gt;E-commerce1&lt;/a&gt;&lt;em&gt;desensitized&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.07311&#34;&gt;Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RowitZou/RankAE&#34;&gt;E-commerce2&lt;/a&gt;&lt;em&gt;desensitized&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.07300&#34;&gt;Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tafseer-nayeem/BengaliSummarization&#34;&gt;BengaliSummarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Bengali&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2102.04490&#34;&gt;Unsupervised Abstractive Summarization of Bengali Text Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zcgzcgzcg1/MediaSum&#34;&gt;MediaSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.06410&#34;&gt;MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/darsh10/Nutribullets&#34;&gt;Healthline and BreastCancer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;multi-document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.11921&#34;&gt;Nutri-bullets: Summarizing Health Studies by Composing Segments&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://gov-report-data.github.io/&#34;&gt;GOVREPORT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Long Government reports&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.02112&#34;&gt;Efficient Attentions for Long Document Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ChenxinAn-fdu/CGSum&#34;&gt;SSN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific Paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.03057&#34;&gt;Enhancing Scientific Papers Summarization with Citation Graph&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/babylonhealth/medical-note-summarisation&#34;&gt;MTSamples&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.04412&#34;&gt;Towards objectively evaluating the quality of generated medical summaries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Yale-LILY/QMSum&#34;&gt;QMSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meeting, Query&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.05938&#34;&gt;QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/allenai/ms2&#34;&gt;MS2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Medical, Multi-Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.06486&#34;&gt;MS2: Multi-Document Summarization of Medical Studies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mingdachen/SummScreen&#34;&gt;SummScreen&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Television Series&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.acl-long.589/&#34;&gt;SummScreen: A Dataset for Abstractive Screenplay Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/IBM/document2slides&#34;&gt;SciDuet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific Papers and Slides&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/IBM/document2slides&#34;&gt;D2S: Document-to-Slide Generation Via Query-Based Text Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://deephelp.zendesk.com/hc/en-us/sections/360011925552-MultiHumES&#34;&gt;MultiHumES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multilingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.146/&#34;&gt;MultiHumES: Multilingual Humanitarian Dataset for Extractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cylnlp/DialSumm&#34;&gt;DialSumm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.06762&#34;&gt;DialSumm: A Real-Life Scenario Dialogue Summarization Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Findings of ACL21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/salesforce/booksum&#34;&gt;BookSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Book, Long-form&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08209&#34;&gt;BookSum: A Collection of Datasets for Long-form Narrative Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://icrc.hitsz.edu.cn/xszy/yjzy.htm&#34;&gt;CLES&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese Weibo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9414946&#34;&gt;A Large-Scale Chinese Long-Text Extractive Summarization Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ICASSP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hfthair/emerald_crawler&#34;&gt;FacetSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scientific Paper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-short.137/&#34;&gt;Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL2021 short&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Yale-LILY/ConvoSumm&#34;&gt;ConvoSumm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.535/&#34;&gt;ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/AgreeSum&#34;&gt;AgreeSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-document with entailment annotations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02278&#34;&gt;AgreeSum: Agreement-Oriented Multi-Document Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Findings of ACL2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ybai-nlp/MCLAS&#34;&gt;En2De&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cross-Lingual En2De&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.13648&#34;&gt;Cross-Lingual Abstractive Summarization with Limited Parallel Resources&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;VT-SSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Spoken&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.05606&#34;&gt;VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ryanzhumich/AESLC&#34;&gt;AESLC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Email&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1043/&#34;&gt;This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/csebuetnlp/xl-sum&#34;&gt;XL-Sum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cross-lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://rifatshahriyar.github.io/files/XL-Sum.pdf&#34;&gt;XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Findings of ACL2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JoeBloggsIR/TSSuBERT&#34;&gt;TES 2012-2016&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tweet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.08770&#34;&gt;TSSuBERT: Tweet Stream Summarization Using BERT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://msnews.github.io/pens.html&#34;&gt;PENS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Personalized Headline&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2021/06/ACL2021_PENS_Camera_Ready_1862_Paper.pdf&#34;&gt;PENS: A Dataset and Generic Framework for Personalized News Headline Generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google-research-datasets/xsum_hallucination_annotations&#34;&gt;XSum Hallucination Annotations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Factuality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.00661&#34;&gt;On Faithfulness and Factuality in Abstractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tagoyal/factuality-datasets#factuality-datasets&#34;&gt;factuality-datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Factuality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.04302&#34;&gt;Annotating and Modeling Fine-grained Factuality in Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/artidoro/frank&#34;&gt;frank&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Factuality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.13346&#34;&gt;Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ppapalampidi/GraphTP&#34;&gt;TRIPOD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Movie&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.07536&#34;&gt;Movie Summarization via Sparse Graph Construction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TysonYu/AdaptSum&#34;&gt;AdaptSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Low-Resource&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.11332&#34;&gt;AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/FeiSun/ProductTitleSummarizationCorpus&#34;&gt;PTS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Product&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1808.06885&#34;&gt;Multi-Source Pointer Network for Product Title Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CIKM 2018&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;94&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lipiji/vae-salience-ramds&#34;&gt;RAMDS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Reader-Aware&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.01065&#34;&gt;Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2017 Workshop&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/gsh199449/proto-summ&#34;&gt;court judgment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;court judgment&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.08837&#34;&gt;How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MMLabTHUSZ/ADEGBTS&#34;&gt;ADEGBTS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gaze behaviors&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3339825.3394928&#34;&gt;A Dataset for Exploring Gaze Behaviors in Text Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACM MMSys&#39;20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/abachaa/MeQSum&#34;&gt;MeQSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1215/&#34;&gt;On the Summarization of Consumer Health Questions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;98&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/stangelid/oposum&#34;&gt;OpoSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Opinion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D18-1403/&#34;&gt;Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2018&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/xiyan524/MM-AVS&#34;&gt;MM-AVS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-modal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.08018&#34;&gt;Multi-modal Summarization for Video-containing Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lauhaide/WikiCatSum&#34;&gt;WikiCatSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;multi-doc&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.04687&#34;&gt;Generating Summaries with Topic Templates and Structured Convolutional Decoders&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;101&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MorenoLaQuatra/SDF-TLS&#34;&gt;SDF-TLS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Timeline&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3404835.3462954&#34;&gt;Summarize Dates First: A Paradigm Shift in Timeline Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SIGIR 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;102&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jingqiangchen/RWS-Cit&#34;&gt;RWS-Cit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/epdf/10.1002/cpe.4261&#34;&gt;*Automatic generation of related work through summarizing citations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2017&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;103&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://yiyualt.github.io/mtlsdata/&#34;&gt;MTLS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Timeline&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.32/&#34;&gt;Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;104&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZhangShiyue/EmailSum&#34;&gt;EMAILSUM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Email&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-long.537/&#34;&gt;EmailSum: Abstractive Email Thread Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;105&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://registry.opendata.aws/wikisum/&#34;&gt;WikiSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WikiHow&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-short.28/&#34;&gt;WikiSum: Coherent Summarization Dataset for Efficient Human-Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021 Short&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;106&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/vgupta123/sumpubmed&#34;&gt;SumPubMed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PubMed Scientific Article&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.acl-srw.30/&#34;&gt;SumPubMed: Summarization Dataset of PubMed Scientific Articles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021 Student Research Workshop&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;107&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/brxx122/CALMS&#34;&gt;MLGSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.findings-acl.242/&#34;&gt;Contrastive Aligned Joint Learning for Multilingual Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2021 Findings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;108&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/JD-AI-Research-NLP/CUSTOM&#34;&gt;SMARTPHONE,COMPUTER&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Product&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.08010&#34;&gt;CUSTOM: Aspect-Oriented Product Summarization for E-Commerce&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;109&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/xiaolinAndy/CSDS&#34;&gt;CSDS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Customer Service Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.13139&#34;&gt;CSDS: A Fine-grained Chinese Dataset for Customer Service Dialogue Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;110&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mohammadiahmad/persian-dataset&#34;&gt;persian-dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;persian&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.04098&#34;&gt;ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;111&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ucfnlp/streamhover&#34;&gt;StreamHover&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;spoken livestream&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.05160&#34;&gt;StreamHover: Livestream Transcript Summarization and Annotation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;112&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dqwang122.github.io/projects/CNewSum/&#34;&gt;CNewSum&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lileicc.github.io/pubs/wang2021cnewsum.pdf&#34;&gt;CNewSum: A Large-scale Chinese News Summarization Dataset with Human-annotated Adequacy and Deducibility Level&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NLPCC 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;113&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/XinnuoXu/MiRANews&#34;&gt;MiRANews&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;news, factual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.10650&#34;&gt;MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021 Findings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;114&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/odelliab/HowSumm&#34;&gt;HowSumm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;query multi-doc&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.03179&#34;&gt;HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;115&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/krystalan/SportsSum2.0&#34;&gt;SportsSum2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sports&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.05750&#34;&gt;SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;116&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/megagonlabs/cocosum&#34;&gt;CoCoSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;opinion multi-ref&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07520&#34;&gt;Comparative Opinion Summarization via Collaborative Decoding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;117&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Shen-Chenhui/MReD/&#34;&gt;MReD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Controllable&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07474&#34;&gt;MReD: A Meta-Review Dataset for Controllable Text Generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;118&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/allenai/ms2&#34;&gt;MSˆ2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-Document, Medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.594/&#34;&gt;MSˆ2: Multi-Document Summarization of Medical Studies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;119&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/danielvarab/massive-summ&#34;&gt;MassiveSumm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.797/&#34;&gt;MassiveSumm: a very large-scale, very multilingual, news summarisation dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;120&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lauhaide/clads&#34;&gt;XWikis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;multilingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.742/&#34;&gt;Models and Datasets for Cross-Lingual Summarisation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;121&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/afariha/SubSumE&#34;&gt;SUBSUME&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Intent, subjective&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.newsum-1.14/&#34;&gt;SUBSUME: A Dataset for Subjective Summary Extraction from Wikipedia Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021 newsum&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;122&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/sajastu/reddit_collector&#34;&gt;TLDR9+&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.newsum-1.15/&#34;&gt;TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021 newsum&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;123&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZurichNLP/20Minuten&#34;&gt;20 Minuten&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;German&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.newsum-1.16/&#34;&gt;A New Dataset and Efficient Baselines for Document-level Text Simplification in German&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021 newsum&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;124&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MehwishFatimah/wsd&#34;&gt;WSD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;multi-lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.newsum-1.5/&#34;&gt;A Novel Wikipedia based Dataset for Monolingual and Cross-Lingual Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021 newsum&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;125&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nttcslab-sp-admin/TEDSummary&#34;&gt;TEDSummary&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.08201&#34;&gt;Attention-based Multi-hypothesis Fusion for Speech Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;126&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tingofurro/summac/&#34;&gt;SummaC Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Factual, NLI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.09525&#34;&gt;SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;127&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/forumsum&#34;&gt;ForumSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Conversation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;ttps://aclanthology.org/2021.findings-emnlp.391/&#34;&gt;ForumSum: A Multi-Speaker Conversation Summarization Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021 Findings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/krystalan/K-SportsSum&#34;&gt;K-SportsSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sports&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.12535&#34;&gt;Knowledge Enhanced Sports Game Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WSDM 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;129&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/abrazinskas/Copycat-abstractive-opinion-summarizer&#34;&gt;Test-Amazon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Opinion, New test for Amazon reviews&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2020.acl-main.461/&#34;&gt;Unsupervised Opinion Summarization as Copycat-Review Generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;130&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/abrazinskas/FewSum&#34;&gt;Test-Amazon-Yelp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Opinion, New test for Amazon(180) and Yelp(300)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2020.emnlp-main.337/&#34;&gt;Few-Shot Learning for Opinion Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;131&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/abrazinskas/SelSum&#34;&gt;AmaSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Opinion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.743/&#34;&gt;Learning Opinion Summarizers by Selecting Informative Reviews&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;132&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/csebuetnlp/CrossSum&#34;&gt;CrossSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cross lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.08804&#34;&gt;CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;133&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/LitianD/HCSCL-MSDataset&#34;&gt;HCSCL-MSDataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-modal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.12072&#34;&gt;Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;134&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/dennlinger/klexikon&#34;&gt;Klexikon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;German&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.07198&#34;&gt;Klexikon: A German Dataset for Joint Summarization and Simplification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;135&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;TODSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Customer Service&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.12680&#34;&gt;TODSum: Task-Oriented Dialogue Summarization with State Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.24/&#34;&gt;TWEETSUMM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Customer Service&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.24/&#34;&gt;TWEETSUMM - A Dialog Summarization Dataset for Customer Service&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Findings of EMNLP 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;137&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/oaimli/PeerSum&#34;&gt;PeerSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi-document, Scientific&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.01769&#34;&gt;PeerSum: A Peer Review Dataset for Abstractive Multi-document Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;138&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/iriscxy/Unified-Timeline-Summarizer&#34;&gt;Celebrity TS, Event TS, Wiki TS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Timeline, person, event&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3517221&#34;&gt;Follow the Timeline! Generating Abstractive and Extractive Timeline Summary in Chronological Order&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TOSI 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;139&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/vis-nlp/Chart-to-text&#34;&gt;Chart-to-Text&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chart&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.06486&#34;&gt;Chart-to-Text: A Large-Scale Benchmark for Chart Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;140&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://gov-report-data.github.io/&#34;&gt;GovReport-QS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Long Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.10741&#34;&gt;HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;141&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zenodo.org/record/6359875&#34;&gt;EntSUM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Entity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bloomberg/entsum&#34;&gt;EntSUM: A Data Set for Entity-Centric Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;142&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HLTCHKUST/framing-bias-metric&#34;&gt;ALLSIDES&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Framing Bias&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.04902&#34;&gt;NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;143&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/maartjeth/summarization_with_graphical_elements&#34;&gt;GRAPHELSUMS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;graph&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.07551&#34;&gt;Summarization with Graphical Elements&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;144&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AshOlogn/Evaluating-Factuality-in-Text-Simplification&#34;&gt;Annotated-Wikilarge-Newsela&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Factuality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.07562&#34;&gt;Evaluating Factuality in Text Simplification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;145&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tikhonovpavel/wikimulti&#34;&gt;WikiMulti&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cross-lingual&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.11104&#34;&gt;WikiMulti: a Corpus for Cross-Lingual Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;146&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/UCREL/welsh-summarization-dataset&#34;&gt;Welsh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.02545&#34;&gt;Introducing the Welsh Text Summarisation Dataset and Baseline Systems&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;147&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://stonybrooknlp.github.io/SuMe/&#34;&gt;SuMe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Biomedical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.04652&#34;&gt;SuMe: A Dataset Towards Summarizing Biomedical Mechanisms&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LREC 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;148&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/morningmoni/CiteSum&#34;&gt;CiteSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.06207&#34;&gt;CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;148&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/xcfcode/MSAMSum&#34;&gt;MSAMSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dialogue&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.dialdoc-1.1/&#34;&gt;MSAMSum: Towards Benchmarking Multi-lingual Dialogue Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022 DialDoc&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;149&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nyu-mll/SQuALITY&#34;&gt;SQuALITY&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Long-Document&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.11465&#34;&gt;SQuALITY: Building a Long-Document Summarization Dataset the Hard Way&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;150&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/sobamchan/xscitldr&#34;&gt;X-SCITLDR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.15051&#34;&gt;X-SCITLDR: Cross-Lingual Extreme Summarization of Scholarly Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;JCDL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;151&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ali-bahrainian/NEWTS&#34;&gt;NEWTS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;News&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.15661&#34;&gt;NEWTS: A Corpus for News Topic-Focused Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;152&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bloomberg/entsum&#34;&gt;EntSUM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Entity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.acl-long.237/&#34;&gt;EntSUM: A Data Set for Entity-Centric Extractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;153&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/oja/aosumm&#34;&gt;ASPECTNEWS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.acl-long.449/&#34;&gt;ASPECTNEWS: Aspect-Oriented Summarization of News Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;154&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;RNSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Commit Logs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.acl-long.597/&#34;&gt;RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;155&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Alex-Fabbri/AnswerSumm&#34;&gt;AnswerSumm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;query multi-doc&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.06474&#34;&gt;AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;156&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shwetanlp/Yahoo-CHQ-Summ&#34;&gt;CHQ-Summ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.06581&#34;&gt;CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;157&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/multilexsum/dataset&#34;&gt;Multi-LexSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;multi-doc&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.10883&#34;&gt;Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;158&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://xarrador.dsic.upv.es/resources/dacsa&#34;&gt;DACSA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Catalan and Spanish&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.434/&#34;&gt;DACSA: A large-scale Dataset for Automatic summarization of Catalan and Spanish newspaper Articles&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NAACL 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;159&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/StevenLau6/BigSurvey&#34;&gt;BigSurvey&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Academic Multi-doc&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.ijcai.org/proceedings/2022/0591.pdf&#34;&gt;Generating a Structured Summary of Numerous Academic Papers: Dataset and Method&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;IJCAI 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ydli-ai/CSL&#34;&gt;CSL&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-Chinese-orange&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese, Academic&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.05034&#34;&gt;CSL: A Large-scale Chinese Scientific Literature Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;161&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fhewett/pcc-summaries&#34;&gt;PCC Summaries&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;German&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.63/&#34;&gt;Extractive Summarisation for German-language Data: A Text-level Approach with Discourse Features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;162&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/fajri91/LipKey&#34;&gt;LipKey&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;abstractive summaries, absent keyphrases, and titles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/2022.coling-1.303/&#34;&gt;LipKey: A Large-Scale News Dataset for Absent Keyphrases Generation and Abstractive Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COLING 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;163&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation&#34;&gt;PLOS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Lay summary of biomedical journal articles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.09932&#34;&gt;Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;164&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation&#34;&gt;eLife&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Lay summary of biomedical journal articles&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.09932&#34;&gt;Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;165&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/rajdeep345/ECTSum&#34;&gt;ECTSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Long Earnings Call Transcripts&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.12467&#34;&gt;ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;EMNLP 2022&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;166&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/achouhan93/eur-lex-sum&#34;&gt;EUR-Lex-Sum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multi- and Cross-lingual Legal&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.13448&#34;&gt;EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;167&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CrisisLTLSum/CrisisTimelines&#34;&gt;CrisisLTLSum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Timeline&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.14190&#34;&gt;CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;168&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LANS(&lt;code&gt;upon request&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Arabic&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.13600&#34;&gt;LANS: Large-scale Arabic News Summarization Corpus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Dialogue&lt;/h2&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts&lt;/strong&gt; &lt;em&gt;Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.12467&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rajdeep345/ECTSum&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial documents, including facts and figures, have largely been unexplored, majorly due to the unavailability of suitable datasets. In this work, we present ECTSum, a new dataset with transcripts of earnings calls (ECTs), hosted by publicly traded companies, as documents, and short experts-written telegram-style bullet point summaries derived from corresponding Reuters articles. ECTs are long unstructured documents without any prescribed length limit or format. We benchmark our dataset with state-of-the-art summarizers across various metrics evaluating the content quality and factual consistency of the generated summaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to generate a set of bullet points that precisely capture the important facts discussed in the calls.&#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TODSum: Task-Oriented Dialogue Summarization with State Tracking&lt;/strong&gt; &lt;em&gt;Lulu Zhao, Fujia Zheng, Keqing He, Weihao Zeng, Yuejie Lei, Huixing Jiang, Wei Wu, Weiran Xu, Jun Guo, Fanyu Meng&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.12680&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TWEETSUMM - A Dialog Summarization Dataset for Customer Service&lt;/strong&gt; &lt;em&gt;Guy Feigenblat, Chulaka Gunasekara, Benjamin Sznajder, Sachindra Joshi, David Konopnicki, Ranit Aharonov&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.24/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/guyfe/Tweetsumm&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ForumSum: A Multi-Speaker Conversation Summarization Dataset&lt;/strong&gt; &lt;em&gt;Misha Khalman, Yao Zhao, Mohammad Saleh&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.391/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/forumsum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CSDS: A Fine-grained Chinese Dataset for Customer Service Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Haitao Lin, Liqun Ma, Junnan Zhu, Lu Xiang, Yu Zhou, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.365/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xiaolinAndy/CSDS&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EmailSum: Abstractive Email Thread Summarization&lt;/strong&gt; &lt;em&gt;Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.537/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ZhangShiyue/EmailSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DialSumm: A Real-Life Scenario Dialogue Summarization Dataset&lt;/strong&gt; &lt;em&gt;Yulong Chen, Yang Liu, Liang Chen, Yue Zhang&lt;/em&gt; &lt;code&gt;Findings of ACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.06762&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/cylnlp/DialSumm&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining&lt;/strong&gt; &lt;em&gt;Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir Radev&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.535/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/ConvoSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.06410&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zcgzcgzcg1/MediaSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization&lt;/strong&gt; &lt;em&gt;Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir Radev&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.05938&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/QMSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset&lt;/strong&gt; &lt;em&gt;Revanth Rameshkumar, Peter Bailey&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.459/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/RevanthRameshkumar/CRD3&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SumTitles: a Summarization Dataset with Low Extractiveness&lt;/strong&gt; &lt;em&gt;Valentin Malykh, Konstantin Chernis, Ekaterina Artemova, Irina Piontkovskaya&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.503/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/huawei-noah/sumtitles&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Medical Conversations via Identifying Important Utterances&lt;/strong&gt; &lt;em&gt;Yan Song, Yuanhe Tian, Nan Wang, Fei Xia&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.63/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/cuhksz-nlp/HET-MC&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GupShup: Summarizing Open-Domain Code-Switched Conversations&lt;/strong&gt; &lt;em&gt;Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, Rajiv Ratn Shah&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.499/&#34;&gt;[pdf]&lt;/a&gt;&lt;a href=&#34;https://github.com/midas-research/gupshup&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummScreen: A Dataset for Abstractive Screenplay Summarization&lt;/strong&gt; &lt;em&gt;Mingda Chen, Zewei Chu, Sam Wiseman, Kevin Gimpel&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.589/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mingdachen/SummScreen&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, TV scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to TV series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1911.12237&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/src/1911.12237v2/anc/corpus.7z&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dial2Desc: End-to-end Dialogue Description Generation&lt;/strong&gt; &lt;em&gt;Haojie Pan, Junpei Zhou, Zhou Zhao, Yan Liu, Deng Cai, Min Yang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1811.00185&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The AMI meeting corpus: A pre-announcement&lt;/strong&gt; &lt;em&gt;Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and others&lt;/em&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/11677482_3&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The ICSI meeting corpus&lt;/strong&gt; &lt;em&gt;Janin, Adam and Baron, Don and Edwards, Jane and Ellis, Dan and Gelbart, David and Morgan, Nelson and Peskin, Barbara and Pfau, Thilo and Shriberg, Elizabeth and Stolcke, Andreas and others&lt;/em&gt; &lt;a href=&#34;https://www.researchgate.net/publication/4015071_The_ICSI_meeting_corpus&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Email Summarization&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization&lt;/strong&gt; &lt;em&gt;Kexun Zhang, Jiaao Chen, Diyi Yang&lt;/em&gt; &lt;code&gt;Findings of ACL 2022&lt;/code&gt; &lt;a href=&#34;https://faculty.cc.gatech.edu/~dyang888/docs/acl22_summarization.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EmailSum: Abstractive Email Thread Summarization&lt;/strong&gt; &lt;em&gt;Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.537/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ZhangShiyue/EmailSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart To-Do: Automatic Generation of To-Do Items from Emails&lt;/strong&gt; &lt;em&gt;Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed Hassan Awadallah, Ryen White&lt;/em&gt; &lt;code&gt;ACL 2020&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.767/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/MSR-LIT/SmartToDo&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.767.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Identifying Implicit Quotes for Unsupervised Extractive Summarization of Conversations&lt;/strong&gt; &lt;em&gt;Ryuji Kano, Yasuhide Miura, Tomoki Taniguchi, Tomoko Ohkuma&lt;/em&gt; &lt;code&gt;AACL20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.aacl-main.32/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation&lt;/strong&gt; &lt;em&gt;Rui Zhang, Joel Tetreault&lt;/em&gt; &lt;code&gt;ACL 2019&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1043/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ryanzhumich/AESLC&#34;&gt;[data]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1043.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Building a Dataset for Summarization and Keyword Extraction from Emails&lt;/strong&gt; &lt;em&gt;Vanessa Loza, Shibamouli Lahiri, Rada Mihalcea, Po-Hsiang Lai&lt;/em&gt; &lt;code&gt;LREC 2014&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/L14-1028/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Publicly Available Annotated Corpus for Supervised Email Summarization&lt;/strong&gt; &lt;em&gt;Jan Ulrich, Gabriel Murray, Giuseppe Carenini&lt;/em&gt; &lt;code&gt;AAAI 2008&lt;/code&gt; &lt;a href=&#34;https://www.aaai.org/Papers/Workshops/2008/WS-08-04/WS08-04-014.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Email Conversations with Clue Words&lt;/strong&gt; &lt;em&gt;Giuseppe Carenini, Raymond T. Ng, Xiaodong Zhou&lt;/em&gt; &lt;code&gt;WWW 2007&lt;/code&gt; &lt;a href=&#34;https://www2007.org/papers/paper631.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task-focused Summarization of Email&lt;/strong&gt; &lt;em&gt;Simon H. Corston-Oliver Eric Ringger Michael Gamon Richard Campbell&lt;/em&gt; &lt;code&gt;ACL 2004&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W04-1008.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing email threads&lt;/strong&gt; &lt;em&gt;Owen Rambow, Lokesh Shrestha, John Chen, Chirsty Lauridsen&lt;/em&gt; &lt;code&gt;NAACL 2004&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N04-4027/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N04-4027.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Facilitating email thread access by extractive summary generation&lt;/strong&gt; &lt;em&gt;Ani Nenkova&lt;/em&gt; &lt;code&gt;Recent advances in natural language processing III: selected papers from RANLP&lt;/code&gt; &lt;a href=&#34;https://www.academia.edu/21603342/Facilitating_email_thread_access_by_extractive_summary_generation&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Archived Discussions: A Beginning&lt;/strong&gt; &lt;em&gt;Paula S. Newman, John C. Blitzer&lt;/em&gt; &lt;code&gt;Proceedings of the 8th international conference on Intelligent user interfaces&lt;/code&gt; &lt;a href=&#34;http://john.blitzer.com/papers/iui.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Combining linguistic and machine learning techniques for email summarization&lt;/strong&gt; &lt;em&gt;Smaranda Muresan, Evelyne Tzoukermann, Judith L. Klavans&lt;/em&gt; &lt;code&gt;Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning (ConLL) 2001&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W01-0719/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W01-0719.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Meeting Summarization&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized Utterances&lt;/strong&gt; &lt;em&gt;Shumpei Inoue, Hy Nguyen, Pham Viet Hoang, Tsungwei Liu, Minh-Tien Nguyen&lt;/em&gt; &lt;code&gt;AACL-IJCNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.11374&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=TG1pJJo0Iqo&amp;amp;feature=youtu.be&#34;&gt;[demo]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Meetings are a universal process to make decisions in business and project collaboration. The capability to automatically itemize the decisions in daily meetings allows for extensive tracking of past discussions. To that end, we developed Meeting Decision Tracker, a prototype system to construct decision items comprising decision utterance detector (DUD) and decision utterance rewriter (DUR). We show that DUR makes a sizable contribution to improving the user experience by dealing with utterance collapse in natural conversation. An introduction video of our system is also available at this https URL. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ESSumm: Extractive Speech Summarization from Untranscribed Meeting&lt;/strong&gt; &lt;em&gt;Jun Wang&lt;/em&gt; &lt;code&gt;Interspeech 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.06913&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In this paper, we propose a novel architecture for direct extractive speech-to-speech summarization, ESSumm, which is an unsupervised model without dependence on intermediate transcribed text. Different from previous methods with text presentation, we are aimed at generating a summary directly from speech without transcription. First, a set of smaller speech segments are extracted based on speech signal&#39;s acoustic features. For each candidate speech segment, a distance-based summarization confidence score is designed for latent speech representation measure. Specifically, we leverage the off-the-shelf self-supervised convolutional neural network to extract the deep speech features from raw audio. Our approach automatically predicts the optimal sequence of speech segments that capture the key information with a target summary length. Extensive results on two well-known meeting datasets (AMI and ICSI corpora) show the effectiveness of our direct speech-based method to improve the summarization quality with untranscribed data. We also observe that our unsupervised speech-based method even performs on par with recent transcript-based summarization approaches, where extra speech recognition is required. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Meeting Summarization: A Survey&lt;/strong&gt; &lt;em&gt;Virgile Rennard, Guokan Shang, Julie Hunter, Michalis Vazirgiannis&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.04163&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Recent advances in deep learning, and especially the invention of encoder-decoder architectures, has significantly improved the performance of abstractive summarization systems. While the majority of research has focused on written documents, we have observed an increasing interest in the summarization of dialogues and multi-party conversation over the past few years. A system that could reliably transform the audio or transcript of a human conversation into an abridged version that homes in on the most important points of the discussion would be valuable in a wide variety of real-world contexts, from business meetings to medical consultations to customer service calls. This paper focuses on abstractive summarization for multi-party meetings, providing a survey of the challenges, datasets and systems relevant to this task and a discussion of promising directions for future study. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ALIGNMEET: A Comprehensive Tool for Meeting Annotation, Alignment, and Evaluation&lt;/strong&gt; &lt;em&gt;Peter Polák, Muskaan Singh, Anna Nedoluzhko, Ondřej Bojar&lt;/em&gt; &lt;code&gt;LREC 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.05433&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ELITR/alignmeet&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TANet: Thread-Aware Pretraining for Abstractive Conversational Summarization&lt;/strong&gt; &lt;em&gt;Ze Yang, Liran Wang, Zhoujin Tian, Wei Wu, Zhoujun Li&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.198/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Although pre-trained language models (PLMs) have achieved great success and become a milestone in NLP, abstractive conversational summarization remains a challenging but less studied task. The difficulty lies in two aspects. One is the lack of large-scale conversational summary data. Another is that applying the existing pre-trained models to this task is tricky because of the structural dependence within the conversation and its informal expression, etc. In this work, we first build a large-scale (11M) pretraining dataset called RCSum, based on the multi-person discussions in the Reddit community. We then present TANet, a thread-aware Transformer-based network. Unlike the existing pre-trained models that treat a conversation as a sequence of sentences, we argue that the inherent contextual dependency among the utterances plays an essential role in understanding the entire conversation and thus propose two new techniques to incorporate the structural information into our model. The first is thread-aware attention which is computed by taking into account the contextual dependency within utterances. Second, we apply thread prediction loss to predict the relations between utterances. We evaluate our model on four datasets of real conversations, covering types of meeting transcripts, customer-service records, and forum threads. Experimental results demonstrate that TANet achieves a new state-of-the-art in terms of both automatic evaluation and human judgment. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents&lt;/strong&gt; &lt;em&gt;Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed H. Awadallah, Dragomir Radev, Rui Zhang&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.112/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/psunlpgroup/Summ-N&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose SummN, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. SummN first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, SummN is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that SummN outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at &#xA;   &lt;a href=&#34;https://github.com/psunlpgroup/Summ-N&#34;&gt;https://github.com/psunlpgroup/Summ-N&lt;/a&gt;. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring Neural Models for Query-Focused Summarization&lt;/strong&gt; &lt;em&gt;Jesse Vig, Alexander R. Fabbri, Wojciech Kryściński&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.07637&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/query-focused-sum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment&lt;/strong&gt; &lt;em&gt;MengNan Qi, Hao Liu, YuZhuo Fu, Ting Liu&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.97/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meeting Summarization with Pre-training and Clustering Methods&lt;/strong&gt; &lt;em&gt;Andras Huebner, Wei Ji, Xiang Xiao&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.08210&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/wxj77/MeetingSummarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context or No Context? A preliminary exploration of human-in-the-loop approach for Incremental Temporal Summarization in meetings&lt;/strong&gt; &lt;em&gt;Nicole Beckage, Shachi H Kumar, Saurav Sahay, Ramesh Manuvinakurike&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.11/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Chenxin An, Ming Zhong, Zhichao Geng, Jianqiang Yang, Xipeng Qiu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.07943&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Exploratory Study on Long Dialogue Summarization: What Works and What&#39;s Next&lt;/strong&gt; &lt;em&gt;Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, Dragomir Radev&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.04609&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization&lt;/strong&gt; &lt;em&gt;Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.02492&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/DialogLM&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Sliding Window for Meeting Summarization&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Nancy F. Chen&lt;/em&gt; &lt;code&gt;SummDial@SIGDial 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.13629&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MeetSum: Transforming Meeting Transcript Summarization using Transformers!&lt;/strong&gt; &lt;em&gt;Nima Sadri, Bohan Zhang, Bihan Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.06310&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incremental temporal summarization in multiparty meetings&lt;/strong&gt; &lt;em&gt;Ramesh Manuvinakurike, Saurav Sahay, Wenda Chen, Lama Nachman&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.56.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Spoken Document Summarization using Hierarchical Model with Multi-stage Attention Diversity Optimization&lt;/strong&gt; &lt;em&gt;Potsawee Manakul, Mark J. F. Gales, Linlin Wang&lt;/em&gt; &lt;code&gt;INTERSPEECH 2020&lt;/code&gt; &lt;a href=&#34;http://www.interspeech2020.org/uploadfile/pdf/Thu-2-6-2.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/potsawee/spoken_summ_div&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What are meeting summaries? An analysis of human extractive summaries in meeting corpus&lt;/strong&gt; &lt;em&gt;Fei Liu, Yang Liu&lt;/em&gt; &lt;code&gt;SIGDIAL 2008&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W08-0112/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring Speaker Characteristics for Meeting Summarization&lt;/strong&gt; &lt;em&gt;Fei Liu, Yang Liu&lt;/em&gt; &lt;code&gt;INTERSPEECH 2010&lt;/code&gt; &lt;a href=&#34;https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_2518.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic meeting summarization and topic detection system&lt;/strong&gt; &lt;em&gt;Tai-Chia Huang, Chia-Hsuan Hsieh, Hei-Chia Wang&lt;/em&gt; &lt;a href=&#34;https://www.emerald.com/insight/content/doi/10.1108/DTA-09-2017-0062/full/html&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A keyphrase based approach to interactive meeting summarization&lt;/strong&gt; &lt;em&gt;Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-T¨ur&lt;/em&gt; &lt;code&gt;2008 IEEE Spoken Language Technology Workshop&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/4777863&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A global optimization framework for meeting summarization&lt;/strong&gt; &lt;em&gt;Dan Gillick, Korbinian Riedhammerm, Benoit Favre, Dilek Hakkani-Tur&lt;/em&gt; &lt;code&gt;2009 IEEE International Conference on Acoustics, Speech and Signal Processing&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/4960697&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluating the effectiveness of features and sampling in extractive meeting summarization&lt;/strong&gt; &lt;em&gt;Shasha Xie, Yang Liu, Hui Lin&lt;/em&gt; &lt;code&gt;SLT 2008&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/4777864&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Meeting Summarization Using Dependency Graph Fusion&lt;/strong&gt; &lt;em&gt;Siddhartha Banerjee, Prasenjit Mitra, Kazunari Sugiyama&lt;/em&gt; &lt;code&gt;WWW 2015&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1609.07035&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Community Creation for Abstractive Spoken Conversation Summarization&lt;/strong&gt; &lt;em&gt;Karan Singla, Evgeny Stepanov, Ali Orkan Bayer, Giuseppe Carenini, Giuseppe Riccardi&lt;/em&gt; &lt;code&gt;ACL 2017 workshop&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W17-4506/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W17-4506.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization&lt;/strong&gt; &lt;em&gt;Guokan Shang, Wensi Ding, Zekun Zhang, Antoine Jean-Pierre Tixier, Polykarpos Meladianos, Michalis Vazirgiannis, Jean-Pierre Lorré&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1805.05271&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://bitbucket.org/dascim/acl2018_abssumm/src&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive meeting summarization based on an attentional neural model&lt;/strong&gt; &lt;em&gt;Nouha Dammak, Yassine BenAyed&lt;/em&gt; &lt;a href=&#34;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11605/1160504/Abstractive-meeting-summarization-based-on-an-attentional-neural-model/10.1117/12.2587172.full&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Study of Text Summarization Techniques for Generating Meeting Minutes&lt;/strong&gt; &lt;em&gt;Tu My Doan, Francois Jacquenet, Christine Largeron, Marc Bernard&lt;/em&gt; &lt;code&gt;RCIS 2020&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-50316-1_33&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meeting Summarization, A Challenge for Deep Learning&lt;/strong&gt; &lt;em&gt;Francois Jacquenet, Marc Bernard, Christine Largeron&lt;/em&gt; &lt;code&gt;IWANN 2019&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-20521-8_53&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Abstractive Summaries from Meeting Transcripts&lt;/strong&gt; &lt;em&gt;Siddhartha Banerjee, Prasenjit Mitra, Kazunari Sugiyama&lt;/em&gt; &lt;code&gt;Proceedings of the 2015 ACM Symposium on Document Engineering, DocEng&#39; 2015&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1609.07033&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Align then Summarize: Automatic Alignment Methods for Summarization Corpus Creation&lt;/strong&gt; &lt;em&gt;Paul Tardy, David Janiszek, Yannick Estève, Vincent Nguyen&lt;/em&gt; &lt;code&gt;LREC 2020&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.829&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.829.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dialogue Discourse-Aware Graph Model and Data Augmentation for Meeting Summarization&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng&lt;/em&gt; &lt;code&gt;IJCAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.03502&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xcfcode/DDAMS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How Domain Terminology Affects Meeting Summarization Performance&lt;/strong&gt; &lt;em&gt;Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Dillon Burns, Alec Kerrigan, Fei Liu&lt;/em&gt; &lt;code&gt;COLING20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.00692&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/meeting-domain-terminology&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to Interact and Change? Abstractive Dialogue Summarization with Dialogue Act Weight and Topic Change Info&lt;/strong&gt; &lt;em&gt;Jiasheng Di, Xiao Wei, Zhenyu Zhang&lt;/em&gt; &lt;code&gt;KSEM 2020&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/content/pdf/10.1007/978-3-030-55393-7_22.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/d1jiasheng/DialogueSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts&lt;/strong&gt; &lt;em&gt;Chih-Wen Goo, Yun-Nung Chen&lt;/em&gt; &lt;code&gt;SLT18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1809.05715&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/MiuLab/DialSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Sliding-Window Approach to Automatic Creation of Meeting Minutes&lt;/strong&gt; &lt;em&gt;Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Fei Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.12324&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Learning for Generation with Long Source Sequences&lt;/strong&gt; &lt;em&gt;Tobias Rohde, Xiaoxia Wu, Yinhan Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.07545&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/birch-research/hierarchical-learning&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining&lt;/strong&gt; &lt;em&gt;Chenguang Zhu, Ruochen Xu, Michael Zeng, Xuedong Huang&lt;/em&gt; &lt;code&gt;Findings of EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.02016&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/HMNet&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://github.com/JudeLee19/HMNet-End-to-End-Abstractive-Summarization-for-Meetings&#34;&gt;[unofficial-code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Meeting Summarization via Hierarchical Adaptive Segmental Network Learning&lt;/strong&gt; &lt;em&gt;Zhou Zhao, Haojie Pan, Changjie Fan, Yan Liu, Linlin Li, Min Yang&lt;/em&gt; &lt;code&gt;WWW19&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3308558.3313619&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Restructuring Conversations using Discourse Relations for Zero-shot Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Prakhar Ganesh, Saket Dingliwal&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1902.01615&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization&lt;/strong&gt; &lt;em&gt;Manling Li, Lingyu Zhang, Heng Ji, Richard J. Radke&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1210/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic analysis of multiparty meetings&lt;/strong&gt; &lt;em&gt;STEVE RENALS&lt;/em&gt; &lt;a href=&#34;https://link.springer.com/article/10.1007/s12046-011-0051-3&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Multimodal Meeting Browser that Implements an Important Utterance Detection Model based on Multimodal Information&lt;/strong&gt; &lt;em&gt;Fumio Nihei, Yukiko I. Nakano&lt;/em&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3379336.3381491&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring Methods for Predicting Important Utterances Contributing to Meeting Summarization&lt;/strong&gt; &lt;em&gt;Fumio Nihei, Yukiko I. Nakano&lt;/em&gt; &lt;a href=&#34;https://www.mdpi.com/2414-4088/3/3/50&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fusing Verbal and Nonverbal Information for Extractive Meeting Summarization&lt;/strong&gt; &lt;em&gt;Fumio Nihei, Yukiko I. Nakano, Yutaka Takase&lt;/em&gt; &lt;code&gt;GIFT18&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3279981.3279987&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meeting Extracts for Discussion Summarization Based on Multimodal Nonverbal Information&lt;/strong&gt; &lt;em&gt;Fumio Nihei, Yukiko I. Nakano, Yutaka Takase&lt;/em&gt; &lt;code&gt;ICMI16&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/2993148.2993160&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization of Meeting Recordings&lt;/strong&gt; &lt;em&gt;Gabriel Murray, Steve Renals, Jean Carletta&lt;/em&gt; &lt;a href=&#34;https://www.cstr.ed.ac.uk/downloads/publications/2005/murray-eurospeech05.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Summarization of Meeting Recordings&lt;/strong&gt; &lt;em&gt;Bema Erol, Dar-Shyang Lee, Jonathan Hull&lt;/em&gt; &lt;code&gt;ICME 2003&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/1221239&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data&lt;/strong&gt; &lt;em&gt;Sanjeev Kumar Karn, Francine Chen, Yan-Ying Chen, Ulli Waltinger, Hinrich Schütze&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.adaptnlp-1.24/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leverage Unlabeled Data for Abstractive Speech Summarization with Self-Supervised Learning and Back-Summarization&lt;/strong&gt; &lt;em&gt;SPECOM 2020&lt;/em&gt; &lt;code&gt;SPECOM 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.15296&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focused Meeting Summarization via Unsupervised Relation Extraction&lt;/strong&gt; &lt;em&gt;Lu Wang, Claire Cardie&lt;/em&gt; &lt;code&gt;SIGDIAL 2012&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W12-1642.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization&lt;/strong&gt; &lt;em&gt;Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir Radev&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.05938&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/QMSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain-Independent Abstract Generation for Focused Meeting Summarization&lt;/strong&gt; &lt;em&gt;Lu Wang, Claire Cardie&lt;/em&gt; &lt;code&gt;ACL 2013&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P13-1137.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Decisions in Spoken Meetings&lt;/strong&gt; &lt;em&gt;Lu Wang, Claire Cardie&lt;/em&gt; &lt;code&gt;ACL 2011&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1606.07965&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extracting Decisions from Multi-Party Dialogue Using Directed Graphical Models and Semantic Similarity&lt;/strong&gt; &lt;em&gt;Trung Bui, Matthew Frampton, John Dowding, Stanley Peters&lt;/em&gt; &lt;code&gt;SIGDIAL 2009&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W09-3934/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W09-3934.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining&lt;/strong&gt; &lt;em&gt;Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir Radev&lt;/em&gt; &lt;code&gt;ACL2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.00829&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/ConvoSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Chat Summarization&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Seungone Kim, Se June Joo, Hyungjoo Chae, Chaehyeong Kim, Seung-won Hwang, Jinyoung Yeo&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.548/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In this paper, we propose to leverage the unique characteristics of dialogues sharing commonsense knowledge across participants, to resolve the difficulties in summarizing them. We present SICK, a framework that uses commonsense inferences as additional context. Compared to previous work that solely relies on the input dialogue, SICK uses an external knowledge model to generate a rich set of commonsense inferences and selects the most probable one with a similarity-based selection method. Built upon SICK, SICK++ utilizes commonsense as supervision, where the task of generating commonsense inferences is added upon summarizing the dialogue in a multi-task learning setting. Experimental results show that with injected commonsense knowledge, our framework generates more informative and consistent summaries than existing methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Finer-grain Universal Dialogue Semantic Structures based Model For Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Yuejie Lei, Fujia Zheng, Yuanmeng Yan, Keqing He, Weiran Xu&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.117/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/apexmeister/FINDS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Dongyub Lee, Jungwoo Lim, Taesun Whang, Chanhee Lee, Seungwoo Cho, Mingun Park, Heuiseok Lim&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.8/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Who says like a style of Vitamin: Towards Syntax-Aware DialogueSummarization using Multi-task Learning&lt;/strong&gt; &lt;em&gt;Seolhwa Lee, Kisu Yang, Chanjun Park, João Sedoc, Heuiseok Lim&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.14199&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controllable Neural Dialogue Summarization with Personal Named Entity Planning&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Nancy F. Chen&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.13070&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GupShup: Summarizing Open-Domain Code-Switched Conversations&lt;/strong&gt; &lt;em&gt;Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, Rajiv Ratn Shah&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.499/&#34;&gt;[pdf]&lt;/a&gt;&lt;a href=&#34;https://github.com/midas-research/gupshup&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, Xiaojie Wang&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.04994&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Junpliu/ConDigSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Give the Truth: Incorporate Semantic Slot into Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Lulu Zhao, Weihao Zeng, Weiran Xu, Jun Guo&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://www.researchgate.net/publication/354162497_Give_the_Truth_Incorporate_Semantic_Slot_into_Abstractive_Dialogue_Summarization&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining&lt;/strong&gt; &lt;em&gt;Yicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, Qi Zhang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.04080&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/RowitZou/DAMS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancing Semantic Understanding with Self-Supervised Methods for Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Hyunjae Lee, Jaewoong Yun, Hyunjin Choi, Seongho Joe, Youngjune L. Gwon&lt;/em&gt; &lt;code&gt;Interspeech 2021&lt;/code&gt; &lt;a href=&#34;https://www.isca-speech.org/archive/interspeech_2021/lee21_interspeech.html&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dialogue summarization with supporting utterance flow modeling and fact regularization&lt;/strong&gt; &lt;em&gt;Wang Chen, Piji Li, Hou PongChan, Irwin King&lt;/em&gt; &lt;code&gt;Knowledge-Based Systems&lt;/code&gt; &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0950705121005906&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Situation-Based Multiparticipant Chat Summarization: a Concept, an Exploration-Annotation Tool and an Example Collection&lt;/strong&gt; &lt;em&gt;Anna Smirnova, Evgeniy Slobodkin, George Chernishev&lt;/em&gt; &lt;code&gt;ACL 2021 Student Research Workshop&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-srw.14/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mechanicpanic/Chat-Corpora-Annotator&#34;&gt;[tool]&lt;/a&gt; &lt;a href=&#34;https://github.com/mechanicpanic/Situation_Dataset&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Coreference-Aware Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Ke Shi, Nancy F. Chen&lt;/em&gt; &lt;code&gt;SIGDIAL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.08556&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incorporating Commonsense Knowledge into Abstractive Dialogue Summarization via Heterogeneous Graph Networks&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Bing Qin&lt;/em&gt; &lt;code&gt;CCL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.10044&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Speaker-Aware Sequence-to-Sequence Model for Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Yuejie Lei, Yuanmeng Yan, Zhiyuan Zeng, Keqing He, Ximing Zhang, Weiran Xu&lt;/em&gt; &lt;code&gt;ICASSP21&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/9414547&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summary Grounded Conversation Generation&lt;/strong&gt; &lt;em&gt;Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Sachindra Joshi, David Konopnicki&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.03337&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controllable Abstractive Dialogue Summarization with Sketch Supervision&lt;/strong&gt; &lt;em&gt;Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, Caiming Xiong&lt;/em&gt; &lt;code&gt;ACL-Findings 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.14064&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/ConvSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs&lt;/strong&gt; &lt;em&gt;Jiaao Chen, Diyi Yang&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.08400&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/GT-SALT/Structure-Aware-BART&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Planning with Learned Entity Prompts for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simoes, Ryan McDonald&lt;/em&gt; &lt;code&gt;TACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.tacl-1.88/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Abstractive Dialogue Summarization with Graph Structures and Topic Words&lt;/strong&gt; &lt;em&gt;Lulu Zhao, Weiran Xu, Jun Guo&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.39/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Jiaao Chen, Diyi Yang&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.01672&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/GT-SALT/Multi-View-Seq2Seq&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1911.12237&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/src/1911.12237v2/anc/corpus.7z&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Medical Dialogue Summarization&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;COSSUM: Towards Conversation-Oriented Structured Summarization for Automatic Medical Insurance Assessment&lt;/strong&gt; &lt;em&gt;Sheng Xu, Xiaojun Wan, Sen Hu, Mengdi Zhou, Teng Xu, Hongbin Wang, Haitao Mi&lt;/em&gt; &lt;code&gt;KDD 2022&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3534678.3539116&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In medical insurance industry, a lot of human labor is required to collect information of claimants. Human assessors need to converse with claimants in order to record key information and organize it into a structured summary. With the purpose of helping save human labor, we propose the task of conversation-oriented structured summarization which aims to automatically produce the desired structured summary from a conversation automatically. One major challenge of the task is that the structured summary contains multiple fields of different types. To tackle this problem, we propose a unified approach COSSUM based on prompting to generate the values of all fields simultaneously. By learning all fields together, our approach can capture the inherent relationship between them. Moreover, we propose a specially designed curriculum learning strategy for model training. Both automatic and human evaluations are performed, and the results show the effectiveness of our proposed approach. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Counseling Summarization using Mental Health Knowledge Guided Utterance Filtering&lt;/strong&gt; &lt;em&gt;Aseem Srivastava, Tharun Suresh, Sarah Peregrine (Grin)Lord, Md. Shad Akhtar, Tanmoy Chakraborty&lt;/em&gt; &lt;code&gt;KDD 2022 ADS Track&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.03886&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient. Unlike general clinical discussions, psychotherapy&#39;s core components (viz. symptoms) are hard to distinguish, thus becoming a complex problem to summarize later. A structured counseling conversation may contain discussions about symptoms, history of mental health issues, or the discovery of the patient&#39;s behavior. It may also contain discussion filler words irrelevant to a clinical summary. We refer to these elements of structured psychotherapy as counseling components. In this paper, the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning. We create a new dataset after annotating 12.9K utterances of counseling components and reference summaries for each dialogue. Further, we propose ConSum, a novel counseling-component guided summarization model. ConSum undergoes three independent modules. First, to assess the presence of depressive symptoms, it filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while the second and third modules aim to classify counseling components. At last, we propose a problem-specific Mental Health Information Capture (MHIC) evaluation metric for counseling summaries. Our comparative study shows that we improve on performance and generate cohesive, semantic, and coherent summaries. We comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements. Human and clinical evaluations on the summary show that ConSum generates quality summary. Further, mental health experts validate the clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of &#xA;   &lt;a href=&#34;http://mpathic.ai/&#34;&gt;http://mpathic.ai/&lt;/a&gt; &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adding more data does not always help: A study in medical conversation summarization with PEGASUS&lt;/strong&gt; &lt;em&gt;Varun Nair, Namit Katariya, Xavier Amatriain, Ilya Valmianski, Anitha Kannan&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.07564&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations&lt;/strong&gt; &lt;em&gt;Longxiang Zhang, Renato Negrinho, Arindam Ghosh, Vasudevan Jagannathan, Hamid Reza Hassanzadeh, Thomas Schaaf, and Matthew R. Gormley&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~mgormley/papers/zhang+al.emnlp.2021.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Bharath Chintagunta, Namit Katariya, Xavier Amatriain, Anitha Kannan&lt;/em&gt; &lt;code&gt;NAACL | NLPMC 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.nlpmc-1.9/&#34;&gt;[pdf1]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.07356&#34;&gt;[pdf2]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques&lt;/strong&gt; &lt;em&gt;Kundan Krishna, Sopan Khosla, Jeffrey P. Bigham, Zachary C. Lipton&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.384/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/acmi-lab/modular-summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Medical Conversations via Identifying Important Utterances&lt;/strong&gt; &lt;em&gt;Yan Song, Yuanhe Tian, Nan Wang, Fei Xia&lt;/em&gt; &lt;code&gt;COLING 2020&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.63/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/cuhksz-nlp/HET-MC&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.63.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dr.Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures&lt;/strong&gt; &lt;em&gt;Anirudh Joshi, Namit Katariya, Xavier Amatriain, Anitha Kannan&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2009.08666&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.335.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Medical Dialogue Summarization for Automated Reporting in Healthcare&lt;/strong&gt; &lt;em&gt;Sabine Molenaar, Lientje Maas, Verónica Burriel, Fabiano Dalpiaz,Sjaak Brinkkemper&lt;/em&gt; &lt;code&gt;Advanced Information Systems Engineering Workshops 2020&lt;/code&gt; &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7225507/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Medical Reports from Patient-Doctor Conversations using Sequence-to-Sequence Models&lt;/strong&gt; &lt;em&gt;Seppo Enarvi, Marilisa Amoia, Miguel Del-Agua Teba, Brian Delaney, Frank Diehl, Stefan Hahn, Kristina Harris, Liam McGrath, Yue Pan, Joel Pinto, Luca Rubini, Miguel Ruiz, Gagandeep Singh, Fabian Stemmer, Weiyi Sun, Paul Vozila, Thomas Lin, Ranjani Ramamurthy&lt;/em&gt; &lt;code&gt;ACL 2020 Short&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.nlpmc-1.4/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.nlpmc-1.4.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatically Generating Psychiatric Case Notes From Digital Transcripts of Doctor-Patient Conversations&lt;/strong&gt; &lt;em&gt;Nazmul Kazi, Indika Kahanda&lt;/em&gt; &lt;code&gt;NAACL 2019&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W19-1918/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W19-1918.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Alignment Annotation for Clinic Visit Dialogue to Clinical Note Sentence Language Generation&lt;/strong&gt; &lt;em&gt;Wen-wai Yim, Meliha Yetisgen, Jenny Huang, Micah Grossman&lt;/em&gt; &lt;code&gt;LREC 2020&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.52/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.52.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic-aware Pointer-Generator Networks for Summarizing Spoken Conversations&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Angela Ng, Sheldon Lee, Ai Ti Aw, Nancy F. Chen&lt;/em&gt; &lt;code&gt; ASRU 2019&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.01335&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Customer Service Summarization&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions&lt;/strong&gt; &lt;em&gt;Haitao Lin, Junnan Zhu, Lu Xiang, Yu Zhou, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.182/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xiaolinandy/rods&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers. Existing methods handle this task by summarizing each role’s content separately and thus are prone to ignore the information from other roles. However, we believe that other roles’ content could benefit the quality of summaries, such as the omitted information mentioned by other roles. Therefore, we propose a novel role interaction enhanced method for role-oriented dialogue summarization. It adopts cross attention and decoder self-attention interactions to interactively acquire other roles’ critical information. The cross attention interaction aims to select other roles’ critical dialogue utterances, while the decoder self-attention interaction aims to obtain key information from other roles’ summaries. Experimental results have shown that our proposed method significantly outperforms strong baselines on two public role-oriented dialogue summarization datasets. Extensive analyses have demonstrated that other roles’ content could help generate summaries with more complete semantics and correct topic structures. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An End-to-End Dialogue Summarization System for Sales Calls&lt;/strong&gt; &lt;em&gt;Abedelkadir Asi, Song Wang, Roy Eisenstadt, Dean Geckt, Yarin Kuper, Yi Mao, Royi Ronen&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.12951&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Heuristic-based Inter-training to Improve Few-shot Multi-perspective Dialog Summarization&lt;/strong&gt; &lt;em&gt;Benjamin Sznajder, Chulaka Gunasekara, Guy Lev, Sachin Joshi, Eyal Shnarch, Noam Slonim&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.15590&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking&lt;/strong&gt; &lt;em&gt;Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea Madotto, Juneyoung Park&lt;/em&gt; &lt;code&gt;Findings of ACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.01552&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jshin49/ds2&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TWEETSUMM - A Dialog Summarization Dataset for Customer Service&lt;/strong&gt; &lt;em&gt;Guy Feigenblat, Chulaka Gunasekara, Benjamin Sznajder, Sachindra Joshi, David Konopnicki, Ranit Aharonov&lt;/em&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.24/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/guyfe/Tweetsumm&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Dialogue Summarization Without Annotation Based on Distantly Supervised Machine Reading Comprehension in Customer Service&lt;/strong&gt; &lt;em&gt;Bing Ma, Haifeng Sun , Jingyu Wang , Qi Qi, and Jianxin Liao&lt;/em&gt; &lt;code&gt;TASLP&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/9645319/authors#authors&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TODSum: Task-Oriented Dialogue Summarization with State Tracking&lt;/strong&gt; &lt;em&gt;Lulu Zhao, Fujia Zheng, Keqing He, Weihao Zeng, Yuejie Lei, Huixing Jiang, Wei Wu, Weiran Xu, Jun Guo, Fanyu Meng&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.12680&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CSDS: A Fine-grained Chinese Dataset for Customer Service Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Haitao Lin, Liqun Ma, Junnan Zhu, Lu Xiang, Yu Zhou, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.13139&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xiaolinAndy/CSDS&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distant Supervision based Machine Reading Comprehension for Extractive Summarization in Customer Service&lt;/strong&gt; &lt;em&gt;Bing Ma, Cao Liu, Jingyu Wang, Shujie Hu, Fan Yang, Xunliang Cai, Guanglu Wan, Jiansong Chen, Jianxin Liao&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3404835.3463046&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Abstractive Dialogue Summarization for Tete-a-Tetes&lt;/strong&gt; &lt;em&gt;Xinyuan Zhang, Ruiyi Zhang, Manzil Zaheer, Amr Ahmed&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2009.06851&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling&lt;/strong&gt; &lt;em&gt;Yicheng Zou, Lujun Zhao, Yangyang Kang, Jun Lin, Minlong Peng, Zhuoren Jiang, Changlong Sun, Qi Zhang, Xuanjing Huang, Xiaozhong Liu&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07311&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/RowitZou/topic-dialog-summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders&lt;/strong&gt; &lt;em&gt;Yicheng Zou, Jun Lin, Lujun Zhao, Yangyang Kang, Zhuoren Jiang, Changlong Sun, Qi Zhang, Xuanjing Huang, Xiaozhong Liu&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07300&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/RowitZou/RankAE&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Dialog Summarization with Semantic Scaffolds&lt;/strong&gt; &lt;em&gt;Lin Yuan, Zhou Yu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.00825&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Dialogue Summary Generation for Customer Service&lt;/strong&gt; &lt;em&gt;Chunyi Liu, Peng Wang, Jiang Xu, Zang Li and Jieping Ye&lt;/em&gt; &lt;code&gt;KDD19&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3292500.3330683&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Domain Adaption&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Lulu Zhao, Fujia Zheng, Weihao Zeng, Keqing He, Weiran Xu, Huixing Jiang, Wei Wu, Yanan Wu&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.04362&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Zeng-WH/DOP-Tuning&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Tiezheng Yu, Zihan Liu, Pascale Fung&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.11332&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/TysonYu/AdaptSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain Adaptation to Summarize Human Conversations&lt;/strong&gt; &lt;em&gt;Oana Sandu, Giuseppe Carenini, Gabriel Murray, Raymond Ng&lt;/em&gt; &lt;code&gt;ACL2010 Workshop&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W10-2603/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analyzing and Evaluating Faithfulness in Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, Haizhou Li&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.11777&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/BinWang28/FacEval&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications. Many efforts have been made to improve faithfulness in text summarization. However, there is a lack of systematic study on dialogue summarization systems. In this work, we first perform the fine-grained human analysis on the faithfulness of dialogue summaries and observe that over 35% of generated summaries are faithfully inconsistent respective the source dialogues. Furthermore, we present a new model-level faithfulness evaluation method. It examines generation models with multi-choice questions created by rule-based transformations. Experimental results show that our evaluation schema is a strong proxy for the factual correctness of summarization models. The human-annotated faithfulness samples and the evaluation toolkit are released to facilitate future research toward faithful dialogue summarization. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions&lt;/strong&gt; &lt;em&gt;Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.09894&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive dialogue summarization is to generate a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted great attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures and unclear topic boundaries. This survey provides a comprehensive investigation on existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks and using additional data.A list of datasets under different scenarios and widely-accepted evaluation metrics are summarized for completeness. After that, the trends of scenarios and techniques are summarized, together with deep insights on correlations between extensively exploited features and different scenarios. Based on these analyses, we recommend future directions including more controlled and complicated scenarios, technical innovations and comparisons, publicly available datasets in special domains, etc. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leveraging Non-dialogue Summaries for Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Seongmin Park, Dongchan Shin, Jihwa Lee&lt;/em&gt; &lt;code&gt;Transcript Understanding Workshop at COLING 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.09474&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; To mitigate the lack of diverse dialogue summarization datasets in academia, we present methods to utilize non-dialogue summarization data for enhancing dialogue summarization systems. We apply transformations to document summarization data pairs to create training data that better befit dialogue summarization. The suggested transformations also retain desirable properties of non-dialogue datasets, such as improved faithfulness to the source text. We conduct extensive experiments across both English and Korean to verify our approach. Although absolute gains in ROUGE naturally plateau as more dialogue summarization samples are introduced, utilizing non-dialogue data for training significantly improves summarization performance in zero- and few-shot settings and enhances faithfulness across all training regimes. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning&lt;/strong&gt; &lt;em&gt;Zhichao Geng, Ming Zhong, Zhangyue Yin, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.569/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Pre-trained models have brought remarkable success on the text summarization task. For dialogue summarization, the subdomain of text summarization, utterances are concatenated to flat text before being processed. As a result, existing summarization systems based on pre-trained models are unable to recognize the unique format of the speaker-utterance pair well in the dialogue. To investigate this issue, we conduct probing tests and manual analysis, and find that the powerful pre-trained model can not identify different speakers well in the conversation, which leads to various factual errors. Moreover, we propose three speaker-aware supervised contrastive learning (SCL) tasks: Token-level SCL, Turn-level SCL, and Global-level SCL. Comprehensive experiments demonstrate that our methods achieve significant performance improvement on two mainstream dialogue summarization datasets. According to detailed human evaluations, pre-trained models equipped with SCL tasks effectively generate summaries with better factual consistency. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;View Dialogue in 2D: A Two-stream Model in Time-speaker Perspective for Dialogue Summarization and beyond&lt;/strong&gt; &lt;em&gt;Keli Xie, Dongchen He, Jiaxin Zhuang, Siyuan Lu, Zhongfeng Wang&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.531/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/shakeley/View2dSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Existing works on dialogue summarization often follow the common practice in document summarization and view the dialogue, which comprises utterances of different speakers, as a single utterance stream ordered by time. However, this single-stream approach without specific attention to the speaker-centered points has limitations in fully understanding the dialogue. To better capture the dialogue information, we propose a 2D view of dialogue based on a time-speaker perspective, where the time and speaker streams of dialogue can be obtained as strengthened input. Based on this 2D view, we present an effective two-stream model called ATM to combine the two streams. Extensive experiments on various summarization datasets demonstrate that ATM significantly surpasses other models regarding diverse metrics and beats the state-of-the-art models on the QMSum dataset in ROUGE scores. Besides, ATM achieves great improvements in summary faithfulness and human evaluation. Moreover, results on machine reading comprehension datasets show the generalization ability of the proposed methods and shed light on other dialogue-based tasks. Our code will be publicly available online. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Dialogues with Negative Cues&lt;/strong&gt; &lt;em&gt;Junpeng Liu, Yanyan Zou, Yuxuan Xi, Shengjie Li, Mian Ma, Zhuoye Ding&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.528/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive dialogue summarization aims to convert a long dialogue content into its short form where the salient information is preserved while the redundant pieces are ignored. Different from the well-structured text, such as news and scientific articles, dialogues often consist of utterances coming from two or more interlocutors, where the conversations are often informal, verbose, and repetitive, sprinkled with false-starts, backchanneling, reconfirmations, hesitations, speaker interruptions and the salient information is often scattered across the whole chat. The above properties of conversations make it difficult to directly concentrate on scattered outstanding utterances and thus present new challenges of summarizing dialogues. In this work, rather than directly forcing a summarization system to merely pay more attention to the salient pieces, we propose to explicitly have the model perceive the redundant parts of an input dialogue history during the training phase. To be specific, we design two strategies to construct examples without salient pieces as negative cues. Then, the sequence-to-sequence likelihood loss is cooperated with the unlikelihood objective to drive the model to focus less on the unimportant information and also pay more attention to the salient pieces. Extensive experiments on the benchmark dataset demonstrate that our simple method significantly outperforms the baselines with regard to both semantic matching and factual consistent based metrics. The human evaluation also proves the performance gains. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, Jie Zhou&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.05599&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/krystalan/ClidSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We present ClidSum, a benchmark dataset for building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated summaries in different target languages. Based on the proposed ClidSum, we introduce two benchmark settings for supervised and semi-supervised scenarios, respectively. We then build various baseline systems in different paradigms (pipeline and end-to-end) and conduct extensive experiments on ClidSum to provide deeper analyses. Furthermore, we propose mDialBART which extends mBART-50 (a multi-lingual BART) via further pre-training. The multiple objectives used in the further pre-training stage help the pre-trained model capture the structural characteristics as well as important content in dialogues and the transformation from source to the target language. Experimental results show the superiority of mDialBART, as an end-to-end model, outperforms strong pipeline models on ClidSum. Finally, we discuss specific challenges that current approaches faced with this task and give multiple promising directions for future research. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Focused Study on Sequence Length for Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Bin Wang, Chen Zhang, Chengwei Wei, Haizhou Li&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.11910&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Output length is critical to dialogue summarization systems. The dialogue summary length is determined by multiple factors, including dialogue complexity, summary objective, and personal preferences. In this work, we approach dialogue summary length from three perspectives. First, we analyze the length differences between existing models&#39; outputs and the corresponding human references and find that summarization models tend to produce more verbose summaries due to their pretraining objectives. Second, we identify salient features for summary length prediction by comparing different model settings. Third, we experiment with a length-aware summarizer and show notable improvement on existing models if summary length can be well incorporated. Analysis and experiments are conducted on popular DialogSum and SAMSum datasets to validate our findings. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DialogSum Challenge: Results of the Dialogue Summarization Shared Task&lt;/strong&gt; &lt;em&gt;Yulong Chen, Naihao Deng, Yang Liu, Yue Zhang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.03898&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We report the results of DialogSum Challenge, the shared task on summarizing real-life scenario dialogues at INLG 2022. Four teams participate in this shared task and three submit their system reports, exploring different methods to improve the performance of dialogue summarization. Although there is a great improvement over the baseline models regarding automatic evaluation metrics, such as Rouge scores, we find that there is a salient gap between model generated outputs and human annotated summaries by human evaluation from multiple aspects. These findings demonstrate the difficulty of dialogue summarization and suggest that more fine-grained evaluatuion metrics are in need. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Effectiveness of French Language Models on Abstractive Dialogue Summarization Task&lt;/strong&gt; &lt;em&gt;Yongxin Zhou, François Portet, Fabien Ringeval&lt;/em&gt; &lt;code&gt;LREC 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.08305&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Pre-trained language models have established the state-of-the-art on various natural language processing tasks, including dialogue summarization, which allows the reader to quickly access key information from long conversations in meetings, interviews or phone calls. However, such dialogues are still difficult to handle with current models because the spontaneity of the language involves expressions that are rarely present in the corpora used for pre-training the language models. Moreover, the vast majority of the work accomplished in this field has been focused on English. In this work, we present a study on the summarization of spontaneous oral dialogues in French using several language specific pre-trained models: BARThez, and BelGPT-2, as well as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments were performed on the DECODA (Call Center) dialogue corpus whose task is to generate abstractive synopses from call center conversations between a caller and one or several agents depending on the situation. Results show that the BARThez models offer the best performance far above the previous state-of-the-art on DECODA. We further discuss the limits of such pre-trained models and the challenges that must be addressed for summarizing spontaneous dialogues. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Augmentation for Low-Resource Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Yongtai Liu, Joshua Maynez, Gonçalo Simões, Shashi Narayan&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.53/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We present DADS, a novel Data Augmentation technique for low-resource Dialogue Summarization. Our method generates synthetic examples by replacing sections of text from both the input dialogue and summary while preserving the augmented summary to correspond to a viable summary for the augmented dialogue. We utilize pretrained language models that produce highly likely dialogue alternatives while still being free to generate diverse alternatives. We applied our data augmentation method to the SAMSum dataset in low resource scenarios, mimicking real world problems such as chat, thread, and meeting summarization where large scale supervised datasets with human-written summaries are scarce. Through both automatic and human evaluations, we show that DADS shows strong improvements for low resource scenarios while generating topically diverse summaries without introducing additional hallucinations to the summaries. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An End-to-End Dialogue Summarization System for Sales Calls&lt;/strong&gt; &lt;em&gt;Abedelkadir Asi, Song Wang, Roy Eisenstadt, Dean Geckt, Yarin Kuper, Yi Mao, Royi Ronen&lt;/em&gt; &lt;code&gt;NAACL 2022 Industry Track&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-industry.6/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Summarizing sales calls is a routine task performed manually by salespeople. We present a production system which combines generative models fine-tuned for customer-agent setting, with a human-in-the-loop user experience for an interactive summary curation process. We address challenging aspects of dialogue summarization task in a real-world setting including long input dialogues, content validation, lack of labeled data and quality evaluation. We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting. Experiments show significant improvements by our models in tackling the summarization and content validation tasks on public datasets. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Few-shot fine-tuning SOTA summarization models for medical dialogues&lt;/strong&gt; &lt;em&gt;David Fraile Navarro, Mark Dras, Shlomo Berkovsky&lt;/em&gt; &lt;code&gt;NAACL 2022 Student Research Workshop&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-srw.32/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/dafraile/Clinical-Dialogue-Summarization&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization of medical dialogues presents a challenge for standard training approaches, given the paucity of suitable datasets. We explore the performance of state-of-the-art models with zero-shot and few-shot learning strategies and measure the impact of pretraining with general domain and dialogue-specific text on the summarization performance. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DialSummEval: Revisiting Summarization Evaluation for Dialogues&lt;/strong&gt; &lt;em&gt;Mingqi Gao, Xiaojun Wan&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.418/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kite99520/DialSummEval&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy trends which are different from the conventional summarization tasks are identified. We will release DialSummEval, a multi-faceted dataset of human judgments containing the outputs of 14 models on SAMSum. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Lulu Zhao, Fujia Zheng, Weihao Zeng, Keqing He, Weiran Xu, Huixing Jiang, Wei Wu, Yanan Wu&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.357/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Zeng-WH/DOP-Tuning&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The most advanced abstractive dialogue summarizers lack generalization ability on new domains and the existing researches for domain adaptation in summarization generally rely on large-scale pre-trainings. To explore the lightweight fine-tuning methods for domain adaptation of dialogue summarization, in this paper, we propose an efficient and generalizable Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized prefix module to alleviate domain entanglement and adopts discrete prompts to guide the model to focus on key contents of dialogues and enhance model generalization. We conduct zero-shot experiments and build domain adaptation benchmarks on two multi-domain dialogue summarization datasets, TODSum and QMSum. Adequate experiments and qualitative analysis prove the effectiveness of our methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;From spoken dialogue to formal summary: An utterance rewriting for dialogue summarization&lt;/strong&gt; &lt;em&gt;Yue Fang, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Bo Long, Yanyan Lan, Yanquan Zhou&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.283/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Due to the dialogue characteristics of unstructured contexts and multi-parties with first-person perspective, many successful text summarization works have failed when dealing with dialogue summarization. In dialogue summarization task, the input dialogue is usually spoken style with ellipsis and co-references but the output summaries are more formal and complete. Therefore, the dialogue summarization model should be able to complete the ellipsis content and co-reference information and then produce a suitable summary accordingly. However, the current state-of-the-art models pay more attention on the topic or structure of summary, rather than the consistency of dialogue summary with its input dialogue context, which may suffer from the personal and logical inconsistency problem. In this paper, we propose a new model, named ReWriteSum, to tackle this problem. Firstly, an utterance rewriter is conducted to complete the ellipsis content of dialogue content and then obtain the rewriting utterances. Then, the co-reference data augmentation mechanism is utilized to replace the referential person name with its specific name to enhance the personal information. Finally, the rewriting utterances and the co-reference replacement data are used in the standard BART model. Experimental results on both SAMSum and DialSum datasets show that our ReWriteSum significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on multi-speakers also shows that ReWriteSum can obtain relatively higher improvement with more speakers, validating the correctness and property of ReWriteSum. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion&lt;/strong&gt; &lt;em&gt;Seongmin Park, Jihwa Lee&lt;/em&gt; &lt;code&gt;WIT Workshop @ ACL2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.13108&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/seongminp/graph-dialogue-summary&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MSAMSum: Towards Benchmarking Multi-lingual Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Bing Qin&lt;/em&gt; &lt;code&gt;ACL 2022 DialDoc Workshop&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.dialdoc-1.1/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xcfcode/MSAMSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Cross-lingual Conversation Summarization Challenge&lt;/strong&gt; &lt;em&gt;Yulong Chen, Ming Zhong, Xuefeng Bai, Naihao Deng, Jing Li, Xianchao Zhu, Yue Zhang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.00379&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Post-Training Dialogue Summarization using Pseudo-Paraphrasing&lt;/strong&gt; &lt;em&gt;Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.125/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/JiaQiSJTU/DialSent-PGG&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Previous dialogue summarization techniques adapt large language models pretrained on the narrative text by injecting dialogue-specific features into the models. These features either require additional knowledge to recognize or make the resulting models harder to tune. To bridge the format gap between dialogues and narrative summaries in dialogue summarization tasks, we propose to post-train pretrained language models (PLMs) to rephrase from dialogue to narratives. After that, the model is fine-tuned for dialogue summarization as usual. Comprehensive experiments show that our approach significantly improves vanilla PLMs on dialogue summarization and outperforms other SOTA models by the summary quality and implementation costs. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning&lt;/strong&gt; &lt;em&gt;Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, Dragomir Radev&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.08713&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Are We Summarizing the Right Way? A Survey of Dialogue Summarization Data Sets&lt;/strong&gt; &lt;em&gt;Don Tuggener, Margot Mieskes, Jan Deriu, Mark Cieliebak&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.12/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dialogue Inspectional Summarization with Factual Inconsistency Awareness&lt;/strong&gt; &lt;em&gt;Leilei Gan, Yating Zhang, Kun Kuang, Lin Yuan, Shuo Li, Changlong Sun, Xiaozhong Liu, Fei Wu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.03284&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Do Boat and Ocean Suggest Beach? Dialogue Summarization with External Knowledge&lt;/strong&gt; &lt;em&gt;Tianqing Fang, Haojie Pan, Hongming Zhang, Yangqiu Song, Kun Xu, Dong Yu&lt;/em&gt; &lt;code&gt;AKBC 2021&lt;/code&gt; &lt;a href=&#34;https://www.akbc.ws/2021/papers/AJKd0iIFMDc&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/HKUST-KnowComp/CODC-Dialogue-Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt scoring system for dialogue summarization using GPT3&lt;/strong&gt; &lt;em&gt;Prodan, George; Pelican, Elena&lt;/em&gt; &lt;a href=&#34;https://www.techrxiv.org/articles/preprint/Prompt_scoring_system_for_dialogue_summarization_using_GPT-3/16652392&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue SummarizationJiaao&lt;/strong&gt; &lt;em&gt;Jiaao Chen, Diyi Yang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://www.cc.gatech.edu/~dyang888/docs/emnlp21_chen_coda.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/GT-SALT/CODA&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Bag of Tricks for Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Muhammad Khalifa, Miguel Ballesteros, Kathleen McKeown&lt;/em&gt; &lt;code&gt;EMNLP 2021 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.08232&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Summarization for Longform Spoken Dialog&lt;/strong&gt; &lt;em&gt;Daniel Li, Thomas Chen, Albert Tung, Lydia Chilton&lt;/em&gt; &lt;code&gt;UIST 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.09597&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy&lt;/strong&gt; &lt;em&gt;Xiyan Fu, Yating Zhang, Tianyi Wang, Xiaozhong Liu, Changlong Sun, Zhenglu Yang&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.471/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xiyan524/RepSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.117/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xcfcode/PLM_annotator&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Two-Phase Approach for Abstractive Podcast Summarization&lt;/strong&gt; &lt;em&gt;Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, Ling Fan&lt;/em&gt; &lt;code&gt;TREC 2020 Podcasts Track&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.08291&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Learning for Generation with Long Source Sequences&lt;/strong&gt; &lt;em&gt;Tobias Rohde, Xiaoxia Wu, Yinhan Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.07545&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/birch-research/hierarchical-learning&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Online Forums Summarization via Unifying Hierarchical Attention Networks with Convolutional Neural Networks&lt;/strong&gt; &lt;em&gt;Sansiri Tarnpradab, Fereshteh Jafariakinabad, Kien A. Hua&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.13587&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sansiri20/forums_summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization of Call Transcripts&lt;/strong&gt; &lt;em&gt;Pratik K. Biswas, Aleksandr Iakubovich&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.10599&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Legal Summarization for Multi-role Debate Dialogue via Controversy Focus Mining and Multi-task Learning&lt;/strong&gt; &lt;em&gt;Xinyu Duan, Yating Zhang, Lin Yuan, Xin Zhou, Xiaozhong Liu, Tianyi Wang, Ruocheng Wang, Qiong Zhang, Changlong Sun, Fei Wu&lt;/em&gt; &lt;code&gt;CIKM 2019&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3357384.3357940&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Collabot: Personalized Group Chat Summarization&lt;/strong&gt; &lt;em&gt;Naama Tepper, Anat Hashavit, Maya Barnea, Inbal Ronen, Lior Leiba&lt;/em&gt; &lt;code&gt;WSDM 2018&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3159652.3160588&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Dialogic Arguments from Social Media&lt;/strong&gt; &lt;em&gt;Amita Misra, Shereen Oraby, Shubhangi Tandon, Sharath TS, Pranav Anand, Marilyn Walker&lt;/em&gt; &lt;code&gt;SemDial 2017&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1711.00092&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The SENSEI Annotated Corpus: Human Summaries of Reader Comment Conversations in On-line News&lt;/strong&gt; &lt;em&gt;Emma Barker, Monica Lestari Paramita, Ahmet Aker, Emina Kurtic, Mark Hepple, Robert Gaizauskas&lt;/em&gt; &lt;code&gt;SIGDIAL 2016&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W16-3605/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic Similarity Applied to Spoken Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Iryna Gurevych, Michael Strube&lt;/em&gt; &lt;code&gt;COLING 2004&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/C04-1110/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/C04-1110.bib&#34;&gt;[bib]&lt;/a&gt; Switchboard dialogues&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Long Document&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;HeterGraphLongSum: Heterogeneous Graph Neural Network with Passage Aggregation for Extractive Long Document Summarization&lt;/strong&gt; &lt;em&gt;Tuan-Anh Phan, Ngoc-Dung Ngoc Nguyen, Khac-Hoai Nam Bui&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.545/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tuananhphan97vn/HeterGraphLongSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Graph Neural Network (GNN)-based models have proven effective in various Natural Language Processing (NLP) tasks in recent years. Specifically, in the case of the Extractive Document Summarization (EDS) task, modeling documents under graph structure is able to analyze the complex relations between semantic units (e.g., word-to-word, word-to-sentence, sentence-to-sentence) and enrich sentence representations via valuable information from their neighbors. However, long-form document summarization using graph-based methods is still an open research issue. The main challenge is to represent long documents in a graph structure in an effective way. In this regard, this paper proposes a new heterogeneous graph neural network (HeterGNN) model to improve the performance of long document summarization (HeterGraphLongSum). Specifically, the main idea is to add the passage nodes into the heterogeneous graph structure of word and sentence nodes for enriching the final representation of sentences. In this regard, HeterGraphLongSum is designed with three types of semantic units such as word, sentence, and passage. Experiments on two benchmark datasets for long documents such as Pubmed and Arxiv indicate promising results of the proposed model for the extractive long document summarization problem. Especially, HeterGraphLongSum is able to achieve state-of-the-art performance without relying on any pre-trained language models (e.g., BERT). The source code is available for further exploitation on the Github. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi Graph Neural Network for Extractive Long Document Summarization&lt;/strong&gt; &lt;em&gt;Xuan-Dung Doan, Le-Minh Nguyen, Khac-Hoai Nam Bui&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.512/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/dungdx34/MTGNN-SUM&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Heterogeneous Graph Neural Networks (HeterGNN) have been recently introduced as an emergent approach for extracting document summarization (EDS) by exploiting the cross-relations between words and sentences. However, applying HeterGNN for long documents is still an open research issue. One of the main majors is the lacking of inter-sentence connections. In this regard, this paper exploits how to apply HeterGNN for long documents by building a graph on sentence-level nodes (homogeneous graph) and combine with HeterGNN for capturing the semantic information in terms of both inter and intra-sentence connections. Experiments on two benchmark datasets of long documents such as PubMed and ArXiv show that our method is able to achieve state-of-the-art results in this research field. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HEGEL: Hypergraph Transformer for Long Document Summarization&lt;/strong&gt; &lt;em&gt;Haopeng Zhang, Xiao Liu, Jiawei Zhang&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.04126&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization&lt;/strong&gt; &lt;em&gt;Qianqian Xie, Jimin Huang, Tulika Saha, Sophia Ananiadou&lt;/em&gt; &lt;code&gt;COLING2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.546/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xashely/GRETEL_extractive&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Recently, neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs), to capture the global semantic information for text summarization. However, in these methods, there remain limitations in the way they capture and integrate the global semantic information. In this paper, we propose a novel model, the graph contrastive topic enhanced language model (GRETEL), that incorporates the graph contrastive topic model with the pre-trained language model, to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs, the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics. Experimental results on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sparse Optimization for Unsupervised Extractive Summarization of Long Documents with the Frank-Wolfe Algorithm&lt;/strong&gt; &lt;em&gt;Alicia Y. Tsai, Laurent El Ghaoui&lt;/em&gt; &lt;code&gt;SustaiNLP at EMNLP 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.09454&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We address the problem of unsupervised extractive document summarization, especially for long documents. We model the unsupervised problem as a sparse auto-regression one and approximate the resulting combinatorial problem via a convex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe algorithm. To generate a summary with k sentences, the algorithm only needs to execute ≈k iterations, making it very efficient. We explain how to avoid explicit calculation of the full gradient and how to include sentence embedding information. We evaluate our approach against two other unsupervised methods using both lexical (standard) ROUGE scores, as well as semantic (embedding-based) ones. Our method achieves better results with both datasets and works especially well when combined with embeddings for highly paraphrased summaries. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization Framework based on Semantic Blocks&lt;/strong&gt; &lt;em&gt;Xinnian Liang, Jing Li, Shuangzhi Wu, Jiali Zeng, Yufan Jiang, Mu Li, Zhoujun Li&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.558/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xnliang98/c2f-far&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Unsupervised summarization methods have achieved remarkable results by incorporating representations from pre-trained language models. However, existing methods fail to consider efficiency and effectiveness at the same time when the input document is extremely long. To tackle this problem, in this paper, we proposed an efficient Coarse-to-Fine Facet-Aware Ranking (C2F-FAR) framework for unsupervised long document summarization, which is based on the semantic block. The semantic block refers to continuous sentences in the document that describe the same facet. Specifically, we address this problem by converting the one-step ranking method into the hierarchical multi-granularity two-stage ranking. In the coarse-level stage, we propose a new segment algorithm to split the document into facet-aware semantic blocks and then filter insignificant blocks. In the fine-level stage, we select salient sentences in each block and then extract the final summary from selected sentences. We evaluate our framework on four long document summarization datasets: Gov-Report, BillSum, arXiv, and PubMed. Our C2F-FAR can achieve new state-of-the-art unsupervised summarization results on Gov-Report and BillSum. In addition, our method speeds up 4-28 times more than previous methods.\footnote{\url{this https URL}} &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Investigating Efficiently Extending Transformers for Long Input Summarization&lt;/strong&gt; &lt;em&gt;Jason Phang, Yao Zhao, Peter J. Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.04347&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/pegasus/tree/main/pegasus/flax&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs continues to be a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most pretrained models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms can most efficiently adapt a pretrained Transformer for long input summarization. We find that a staggered, block-local Transformer with global encoder tokens strikes a good balance of performance and efficiency, and that an additional pretraining phase on long sequences meaningfully improves downstream summarization performance. Based on our findings, we introduce PEGASUS-X, an extension of the PEGASUS model with additional long input pretraining to handle inputs of up to 16K tokens. PEGASUS-X achieves strong performance on long input summarization tasks comparable with much larger models while adding few additional parameters and not requiring model parallelism to train. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics&lt;/strong&gt; &lt;em&gt;uan Yee Koh, Jiaxin Ju, Ming Liu, Shirui Pan&lt;/em&gt; &lt;code&gt;ACM Computing Surveys&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.00939&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Long documents such as academic articles and business reports have been the standard format to detail out important issues and complicated subjects that require extra attention. An automatic summarization system that can effectively condense long documents into short and concise texts to encapsulate the most important information would thus be significant in aiding the reader&#39;s comprehension. Recently, with the advent of neural architectures, significant research efforts have been made to advance automatic text summarization systems, and numerous studies on the challenges of extending these systems to the long document domain have emerged. In this survey, we provide a comprehensive overview of the research on long document summarization and a systematic evaluation across the three principal components of its research setting: benchmark datasets, summarization models, and evaluation metrics. For each component, we organize the literature within the context of long document summarization and conduct an empirical analysis to broaden the perspective on current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in this rapidly growing field. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes&lt;/strong&gt; &lt;em&gt;Nianlong Gu, Elliott Ash, Richard Hahnloser&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.450/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nianlonggu/memsum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum’s awareness of extraction history. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic Self-Segmentation for Abstractive Summarization of Long Legal Documents in Low-Resource Regimes&lt;/strong&gt; &lt;em&gt;Gianluca Moro, Luca Ragazzi&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-3882.MoroG.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents by Sampling Summary Views&lt;/strong&gt; &lt;em&gt;Marcio Fonseca, Yftah Ziser, Shay B. Cohen&lt;/em&gt; `` &lt;a href=&#34;https://arxiv.org/abs/2205.12486&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leveraging Locality in Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb, Chenguang Zhu, Ahmed H. Awadallah, Dragomir Radev&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.12476&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SNaC: Coherence Error Detection for Narrative Summarization&lt;/strong&gt; &lt;em&gt;Tanya Goyal, Junyi Jessy Li, Greg Durrett&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.09641&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequence-Based Extractive Summarisation for Scientific Articles&lt;/strong&gt; &lt;em&gt;Daniel Kershaw, Rob Koeling&lt;/em&gt; `` &lt;a href=&#34;https://arxiv.org/abs/2204.03301&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LDKP: A Dataset for Identifying Keyphrases from Long Scientific Documents&lt;/strong&gt; &lt;em&gt;Debanjan Mahata, Naveen Agarwal, Dibya Gautam, Amardeep Kumar, Swapnil Parekh, Yaman Kumar Singla, Anish Acharya, Rajiv Ratn Shah&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.15349&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/midas/ldkp3k&#34;&gt;[data1]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/midas/ldkp10k&#34;&gt;[data2]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization&lt;/strong&gt; &lt;em&gt;Shuyang Cao, Lu Wang&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.58/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ShuyangCao/hibrids_summ&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://gov-report-data.github.io/&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Document structure is critical for efficient information consumption. However, it is challenging to encode it efficiently into the modern Transformer architecture. In this work, we present HIBRIDS, which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation. We further present a new task, hierarchical question-summary generation, for summarizing salient content in the source document into a hierarchy of questions and summaries, where each follow-up question inquires about the content of its parent question-summary pair. We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports. Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage, a finding also echoed by human judges. Additionally, our model improves the generation of long-form summaries from long government reports and Wikipedia articles, as measured by ROUGE scores. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information&lt;/strong&gt; &lt;em&gt;Qian Ruan, Malte Ostendorff, Georg Rehm&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.09629&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/QianRuan/histruct&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long Document Summarization with Top-down and Bottom-up Inference&lt;/strong&gt; &lt;em&gt;Bo Pang, Erik Nijkamp, Wojciech Kryściński, Silvio Savarese, Yingbo Zhou, Caiming Xiong&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.07586&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents&lt;/strong&gt; &lt;em&gt;Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed H. Awadallah, Dragomir Radev, Rui Zhang&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.10150&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization&lt;/strong&gt; &lt;em&gt;Ziming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang, Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang Zhu, Ahmed H. Awadallah, Dragomir Radev&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.118/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/DYLE&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization. DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding. To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator. We evaluate our method on different long-document and long-dialogue summarization tasks: GovReport, QMSum, and arXiv. Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum, with gains up to 6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that the proposed dynamic weights provide interpretability of our generation process. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SciBERTSUM: Extractive Summarization for Scientific Documents&lt;/strong&gt; &lt;em&gt;Athar Sefid, C Lee Giles&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.08495&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/atharsefid/SciBERTSUM&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Content Extraction for Poster Generation of Scientific Papers&lt;/strong&gt; &lt;em&gt;Sheng Xu, Xiaojun Wan&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.08550&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LongT5: Efficient Text-To-Text Transformer for Long Sequences&lt;/strong&gt; &lt;em&gt;Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.07916&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Influence of Data Pre-processing and Post-processing on Long Document Summarization&lt;/strong&gt; &lt;em&gt;Xinwei Du, Kailun Dong, Yuchen Zhang, Yongsheng Li, Ruei-Yu Tsay&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.01660&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;End-to-End Segmentation-based News Summarization&lt;/strong&gt; &lt;em&gt;Yang Liu, Chenguang Zhu, Michael Zeng&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.07850&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leveraging Information Bottleneck for Scientific Document Summarization&lt;/strong&gt; &lt;em&gt;Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, Shirui Pan&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.01280&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Summaries for Scientific Paper Review&lt;/strong&gt; &lt;em&gt;Ana Sabina Uban, Cornelia Caragea&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.14059&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems&lt;/strong&gt; &lt;em&gt;Potsawee Manakul, Mark J. F. Gales&lt;/em&gt; &lt;code&gt;EMNLP 2021 short paper&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.03888&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/potsawee/encdec_attn_sparse&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents&lt;/strong&gt; &lt;em&gt;Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He&lt;/em&gt; &lt;code&gt;ACL 2021 short&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-short.137/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hfthair/emerald_crawler&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents&lt;/strong&gt; &lt;em&gt;Peng Cui, Le Hu&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.470/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/pcui-nlp/SSN_DM&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long-Span Summarization via Local Attention and Content Selection&lt;/strong&gt; &lt;em&gt;Potsawee Manakul, Mark J. F. Gales&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.470/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Globalizing BERT-based Transformer Architectures for Long Document Summarization&lt;/strong&gt; &lt;em&gt;Quentin Grail, Julien Perez, Eric Gaussier&lt;/em&gt; &lt;code&gt;EACL 2021&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.154/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discourse-Aware Unsupervised Summarization for Long Scientific Documents&lt;/strong&gt; &lt;em&gt;Yue Dong, Andrei Mircea Romascanu, Jackie Chi Kit Cheung&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.93/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mirandrom/HipoRank&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancing Scientific Papers Summarization with Citation Graph&lt;/strong&gt; &lt;em&gt;Chenxin An, Ming Zhong, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;AAAI 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.03057&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenxinAn-fdu/CGSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient Attentions for Long Document Summarization&lt;/strong&gt; &lt;em&gt;Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang&lt;/em&gt; &lt;code&gt;NAACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.02112&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/luyang-huang96/LongDocSum&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://gov-report-data.github.io/&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Can We Automate Scientific Reviewing?&lt;/strong&gt; &lt;em&gt;Weizhe Yuan, Pengfei Liu, and Graham Neubig&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.00176&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/neulab/ReviewAdvisor&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long Document Summarization in a Low Resource Setting using Pretrained Language Models&lt;/strong&gt; &lt;em&gt;Ahsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Pradhiksha Ashok Kumar, Rheeya Uppaal, Bradford Windsor, Eliot Brenner, Dominic Dotterrer, Rajarshi Das, Andrew McCallum&lt;/em&gt; &lt;code&gt;ACL 2021 Student Research Workshop&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-srw.7/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summaformers @ LaySumm 20, LongSumm 20&lt;/strong&gt; &lt;em&gt;Sayar Ghosh Roy, Nikhil Pinnaparaju, Risubh Jain, Manish Gupta, Vasudeva Varma&lt;/em&gt; &lt;code&gt;SDP EMNLP 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2101.03553&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On Generating Extended Summaries of Long Documents&lt;/strong&gt; &lt;em&gt;Sajad Sotudeh, Arman Cohan, Nazli Goharian&lt;/em&gt; &lt;code&gt;SDU21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.14136&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Georgetown-IR-Lab/ExtendedSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised Learning for Visual Summary Identification in Scientific Publications&lt;/strong&gt; &lt;em&gt;Shintaro Yamamoto, Anne Lauscher, Simone Paolo Ponzetto, Goran Glavaš, Shigeo Morishima&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.11213&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Systematically Exploring Redundancy Reduction in Summarizing Long Documents&lt;/strong&gt; &lt;em&gt;Wen Xiao, Giuseppe Carenini&lt;/em&gt; &lt;code&gt;AACL20&lt;/code&gt; [&lt;a href=&#34;https://www.aclweb.org/anthology/2020.aacl-main.51/&#34;&gt;pdf&lt;/a&gt; &lt;a href=&#34;http://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On Extractive and Abstractive Neural Document Summarization with Transformer Language Models&lt;/strong&gt; &lt;em&gt;Sandeep Subramanian, Raymond Li, Jonathan Pilault, Christopher Pal&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.03186&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dimsum @LaySumm 20: BART-based Approach for Scientific Document Summarization&lt;/strong&gt; &lt;em&gt;Tiezheng Yu, Dan Su, Wenliang Dai, Pascale Fung&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.09252&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/TysonYu/Laysumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline&lt;/strong&gt; &lt;em&gt;Jiaxin Ju, Ming Liu, Longxiang Gao, Shirui Pan&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.09190&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mingzi151/SummPip&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks&lt;/strong&gt; &lt;em&gt;Peng Cui, Le Hu, Yuanchao Liu&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.06253&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientiﬁc Articles&lt;/strong&gt; &lt;em&gt;Yao Lu, Yue Dong, Laurent Charlin&lt;/em&gt; &lt;code&gt;EMNLP20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.14235&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yaolu/Multi-XScience&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Divide-and-Conquer Approach to the Summarization of Long Documents&lt;/strong&gt; &lt;em&gt;Alexios Gidiotis, Grigorios Tsoumakas&lt;/em&gt; &lt;code&gt;IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.06190&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TLDR: Extreme Summarization of Scientific Documents&lt;/strong&gt; &lt;em&gt;Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld&lt;/em&gt; &lt;code&gt;Findings of EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.15011&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/scitldr&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization of Long Documents by Combining Global and Local Context&lt;/strong&gt; &lt;em&gt;Wen Xiao, Giuseppe Carenini&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.08089&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Wendy-Xiao/Extsumm_local_global_context&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks&lt;/strong&gt; &lt;em&gt;Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R. Fabbri, Irene Li, Dan Friedman, Dragomir R. Radev&lt;/em&gt; &lt;code&gt;AAAI19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.01716&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://cs.stanford.edu/~myasu/projects/scisumm_net/&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks&lt;/strong&gt; &lt;em&gt;Guy Lev, Michal Shmueli-Scheuer, Jonathan Herzig, Achiya Jerbi, David Konopnicki&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1204/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/levguy/talksumm&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents&lt;/strong&gt; &lt;em&gt;Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, Nazli Goharian&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1804.05685&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/armancohan/long-summarization&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Factual Consistency&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/How%20to%20evaluate%20factual%20consistency%20of%20summary-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/badge/How%20to%20improve%20factual%20consistency%20of%20summary-improve-orange&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/badge/analysis%20about%20factual%20consistency%20of%20summary-analysis-blue&#34; alt=&#34;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/badge/How%20to%20correct%20factual%20errors%20in%20summary-correct-red&#34; alt=&#34;&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Toolkit: &lt;a href=&#34;https://github.com/Huffon/factsumm&#34;&gt;factsumm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual Information Alleviates Hallucinations in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Liam van der Poel, Ryan Cotterell, Clara Meister&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.13210&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/VanderpoelLiam/CPMI&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite significant progress in the quality of language generated from abstractive summarization models, these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document. A number of works have tried to fix--or at least uncover the source of--the problem with limited success. In this paper, we identify a simple criterion under which models are significantly more likely to assign more probability to hallucinated content during generation: high model uncertainty. This finding offers a potential explanation for hallucinations: models default to favoring text with high marginal probability, i.e., high-frequency occurrences in the training set, when uncertain about a continuation. It also motivates possible routes for real-time intervention during decoding to prevent such hallucinations. We propose a decoding strategy that switches to optimizing for pointwise mutual information of the source and target token--rather than purely the probability of the target token--when the model exhibits uncertainty. Experiments on the XSum dataset show that our method decreases the probability of hallucinated tokens while maintaining the Rouge and BertS scores of top-performing decoding strategies. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling&lt;/strong&gt; &lt;em&gt;Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, Yulia Tsvetkov&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.12378&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/vidhishanair/FactEdit&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets -- CNN/DM and XSum -- we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model -- FactEdit -- improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Phrase-Level Localization of Inconsistency Errors in Summarization by Weak Supervision&lt;/strong&gt; &lt;em&gt;Masato Takatsuka, Tetsunori Kobayashi, Yoshihiko Hayashi&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.537/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/taka2946/sumphrase&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Although the fluency of automatically generated abstractive summaries has improved significantly with advanced methods, the inconsistency that remains in summarization is recognized as an issue to be addressed. In this study, we propose a methodology for localizing inconsistency errors in summarization. A synthetic dataset that contains a variety of factual errors likely to be produced by a common summarizer is created by applying sentence fusion, compression, and paraphrasing operations. In creating the dataset, we automatically label erroneous phrases and the dependency relations between them as “inconsistent,” which can contribute to detecting errors more adequately than existing models that rely only on dependency arc-level labels. Subsequently, this synthetic dataset is employed as weak supervision to train a model called SumPhrase, which jointly localizes errors in a summary and their corresponding sentences in the source document. The empirical results demonstrate that our SumPhrase model can detect factual errors in summarization more effectively than existing weakly supervised methods owing to the phrase-level labeling. Moreover, the joint identification of error-corresponding original sentences is proven to be effective in improving error detection accuracy. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Just ClozE! A Fast and Simple Method for Evaluating the Factual Consistency in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yiyang Li, Lei Li, Qing Yang, Marina Litvak, Natalia Vanetik, Dingxin Hu, Yuze Li, Yanquan Zhou, Dongliang Xu, Xuanyu Zhang&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.02804&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The issue of factual consistency in abstractive summarization has attracted much attention in recent years, and the evaluation of factual consistency between summary and document has become an important and urgent task. Most of the current evaluation metrics are adopted from the question answering (QA). However, the application of QA-based metrics is extremely time-consuming in practice, causing the iteration cycle of abstractive summarization research to be severely prolonged. In this paper, we propose a new method called ClozE to evaluate factual consistency by cloze model, instantiated based on masked language model(MLM), with strong interpretability and substantially higher speed. We demonstrate that ClozE can reduce the evaluation time by nearly 96% relative to QA-based metrics while retaining their interpretability and performance through experiments on six human-annotated datasets and a meta-evaluation benchmark GO FIGURE \citep{gabriel2020go}. We also implement experiments to further demonstrate more characteristics of ClozE in terms of performance and speed. In addition, we conduct an experimental analysis of the limitations of ClozE, which suggests future research directions. The code and models for ClozE will be released upon the paper acceptance. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization&lt;/strong&gt; &lt;em&gt;Shiyue Zhang, David Wan, Mohit Bansal&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.03549&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ZhangShiyue/extractive_is_not_faithful&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1500 English summaries produced by 15 diverse extractive systems. We find that 33% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that is designed for detecting unfaithful extractive summaries and is shown to have the best performance. We hope our work can increase the awareness of unfaithfulness problems in extractive summarization and help future work to evaluate and resolve these issues. Our data and code are publicly available at this https URL &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Entity-based SpanCopy for Abstractive Summarization to Improve the Factual Consistency&lt;/strong&gt; &lt;em&gt;Wen Xiao, Giuseppe Carenini&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.03479&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Wendy-Xiao/Entity-based-SpanCopy&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite the success of recent abstractive summarizers on automatic evaluation metrics, the generated summaries still present factual inconsistencies with the source document. In this paper, we focus on entity-level factual inconsistency, i.e. reducing the mismatched entities between the generated summaries and the source documents. We therefore propose a novel entity-based SpanCopy mechanism, and explore its extension with a Global Relevance component. Experiment results on four summarization datasets show that SpanCopy can effectively improve the entity-level factual consistency with essentially no change in the word-level and entity-level saliency. The code is available at this https URL &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jointly Learning Guidance Induction and Faithful Summary Generation via Conditional Variational Autoencoders&lt;/strong&gt; &lt;em&gt;Wang Xu, Tiejun Zhao&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.180/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization can generate high quality results with the development of the neural network. However, generating factual consistency summaries is a challenging task for abstractive summarization. Recent studies extract the additional information with off-the-shelf tools from the source document as a clue to guide the summary generation, which shows effectiveness to improve the faithfulness. Unlike these work, we present a novel framework based on conditional variational autoencoders, which induces the guidance information and generates the summary equipment with the guidance synchronously. Experiments on XSUM and CNNDM dataset show that our approach can generate relevant and fluent summaries which is more faithful than the existing state-of-the-art approaches, according to multiple factual consistency metrics. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking&lt;/strong&gt; &lt;em&gt;Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.76/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hwanheelee1993/MFMA&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite the recent advances in abstractive summarization systems, it is still difficult to determine whether a generated summary is factual consistent with the source text. To this end, the latest approach is to train a factual consistency classifier on factually consistent and inconsistent summaries. Luckily, the former is readily available as reference summaries in existing summarization datasets. However, generating the latter remains a challenge, as they need to be factually inconsistent, yet closely relevant to the source text to be effective. In this paper, we propose to generate factually inconsistent summaries using source texts and reference summaries with key information masked. Experiments on seven benchmark datasets demonstrate that factual consistency classifiers trained on summaries generated using our method generally outperform existing models and show a competitive correlation with human judgments. We also analyze the characteristics of the summaries generated using our method. We will release the pre-trained model and the code at &#xA;   &lt;a href=&#34;https://github.com/hwanheelee1993/MFMA&#34;&gt;https://github.com/hwanheelee1993/MFMA&lt;/a&gt;. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control&lt;/strong&gt; &lt;em&gt;Haopeng Zhang, Semih Yavuz, Wojciech Kryscinski, Kazuma Hashimoto, Yingbo Zhou&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.40/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;David Wan, Mohit Bansal&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.74/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/meetdavidwan/factpegasus&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We present FactPEGASUS, an abstractive summarization model that addresses the problem of factuality during pre-training and fine-tuning: (1) We augment the sentence selection strategy of PEGASUS’s (Zhang et al., 2019) pre-training objective to create pseudo-summaries that are both important and factual; (2) We introduce three complementary components for fine-tuning. The corrector removes hallucinations present in the reference summary, the contrastor uses contrastive learning to better differentiate nonfactual summaries from factual ones, and the connector bridges the gap between the pre-training and fine-tuning for better transfer of knowledge. Experiments on three downstream tasks demonstrate that FactPEGASUS substantially improves factuality evaluated by multiple automatic metrics and humans. Our thorough analysis suggests that FactPEGASUS is more factual than using the original pre-training objective in zero-shot and few-shot settings, retains factual behavior more robustly than strong baselines, and does not rely entirely on becoming more extractive to improve factuality. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control&lt;/strong&gt; &lt;em&gt;Haopeng Zhang, Semih Yavuz, Wojciech Kryscinski, Kazuma Hashimoto, Yingbo Zhou&lt;/em&gt; &lt;code&gt;NAACL 2022 findings&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.02263&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization&lt;/strong&gt; &lt;em&gt;Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst&lt;/em&gt; &lt;code&gt;TACL 2022 Volume 10&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.tacl-1.10/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tingofurro/summac/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Meng Cao, Yue Dong, Jackie Cheung&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.236/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mcao516/entfa&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text. Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations. Including these factual hallucinations in a summary can be beneficial because they provide useful background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method is based on an entity’s prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively. Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks.Furthermore, we use our method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors&lt;/strong&gt; &lt;em&gt;Liyan Tang, Tanya Goyal, Alexander R. Fabbri, Philippe Laban, Jiacheng Xu, Semih Yahvuz, Wojciech Kryściński, Justin F. Rousseau, Greg Durrett&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.12854&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Liyan06/AggreFact&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization&lt;/strong&gt; &lt;em&gt;Prasetya Ajie Utama, Joshua Bambrick, Nafise Sadat Moosavi, Iryna Gurevych&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.199/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/joshbambrick/Falsesum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Neural abstractive summarization models are prone to generate summaries that are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However, state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that NLI models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce Falsesum, a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries, introducing varying types of factual inconsistencies. Unlike previously introduced document-level NLI datasets, our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a Falsesum-augmented NLI dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking&lt;/strong&gt; &lt;em&gt;Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung&lt;/em&gt; &lt;code&gt;NAACL 2022 Findings&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.02035&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hwanheelee1993/MFMA&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yue Dong, John Wieting, Pat Verga&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.13761&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to Revise References for Faithful Summarization&lt;/strong&gt; &lt;em&gt;Griffin Adams, Han-Chin Shing, Qing Sun, Christopher Winestock, Kathleen McKeown, Noémie Elhadad&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.10290&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-research/summary-reference-revision&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Factual Error Correction for Abstractive Summaries Using Entity Retrieval&lt;/strong&gt; &lt;em&gt;Hwanhee Lee, Cheoneum Park, Seunghyun Yoon, Trung Bui, Franck Dernoncourt, Juae Kim, Kyomin Jung&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.08263&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluating Factuality in Text Simplification&lt;/strong&gt; &lt;em&gt;Ashwin Devaraj, William Sheffield, Byron C. Wallace, Junyi Jessy Li&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.07562&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/AshOlogn/Evaluating-Factuality-in-Text-Simplification&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations&lt;/strong&gt; &lt;em&gt;Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, Mohit Bansal&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.236/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-research/fact-graph&#34;&gt;[code]&lt;/a&gt;&#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Don&#39;t Say What You Don&#39;t Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search&lt;/strong&gt; &lt;em&gt;Daniel King, Zejiang Shen, Nishant Subramani, Daniel S. Weld, Iz Beltagy, Doug Downey&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.08436&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/pinocchio&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning&lt;/strong&gt; &lt;em&gt;Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, Dragomir Radev&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.415/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we first devised a typology of factual errors to better understand the types of hallucinations generated by current models and conducted human evaluation on popular dialog summarization dataset. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual errors from our annotation, we introduce additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. We show that our model significantly reduces all kinds of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using both automatic metrics, ROUGE and BARTScore, and human evaluation. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization&lt;/strong&gt; &lt;em&gt;Alexander R. Fabbri, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.187/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/QAFactEval&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CO2Sum:Contrastive Learning for Factual-Consistent Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Wei Liu, Huanqin Wu, Wenjing Mu, Zhen Li, Tao Chen, Dan Nie&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.01147&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Are Factuality Checkers Reliable? Adversarial Meta-evaluation of Factuality in Summarization&lt;/strong&gt; &lt;em&gt;Yiran Chen, Pengfei Liu, Xipeng Qiu&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.179/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zide05/AdvFact&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization&lt;/strong&gt; &lt;em&gt;Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.09525&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tingofurro/summac/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dialogue Inspectional Summarization with Factual Inconsistency Awareness&lt;/strong&gt; &lt;em&gt;Leilei Gan, Yating Zhang, Kun Kuang, Lin Yuan, Shuo Li, Changlong Sun, Xiaozhong Liu, Fei Wu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.03284&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fine-grained Factual Consistency Assessment for Abstractive Summarization Models&lt;/strong&gt; &lt;em&gt;Sen Zhang, Jianwei Niu, Chuyuan Wei&lt;/em&gt; `` &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.9/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Prafulla Kumar Choubey, Jesse Vig, Wenhao Liu, Nazneen Fatema Rajani&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.07166&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries&lt;/strong&gt; &lt;em&gt;Xiangru Tang, Alexander R. Fabbri, Ziming Mao, Griffin Adams, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir Radev&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.417/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Current pre-trained models applied for summarization are prone to factual inconsistencies that misrepresent the source text. Evaluating the factual consistency of summaries is thus necessary to develop better models. However, the human evaluation setup for evaluating factual consistency has not been standardized. To determine the factors that affect the reliability of the human evaluation, we crowdsource evaluations for factual consistency across state-of-the-art models on two news summarization datasets using the rating-based Likert Scale and ranking-based Best-Worst Scaling. Our analysis reveals that the ranking-based Best-Worst Scaling offers a more reliable measure of summary quality across datasets and that the reliability of Likert ratings highly depends on the target dataset and the evaluation design. To improve crowdsourcing reliability, we extend the scale of the Likert rating and present a scoring algorithm for Best-Worst Scaling that we call value learning. Our crowdsourcing guidelines will be publicly available to facilitate future work on factual consistency in summarization. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization&lt;/strong&gt; &lt;em&gt;Xinnuo Xu, Ondřej Dušek, Shashi Narayan, Verena Rieser, Ioannis Konstas&lt;/em&gt; &lt;code&gt;EMNLP2021 Findings&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.10650&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/XinnuoXu/MiRANews&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Meng Cao, Yue Dong, Jackie Chi Kit Cheung&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.09784&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Shuyang Cao, Lu Wang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.09209&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://shuyangcao.github.io/projects/cliff_summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Faisal Ladhak, Esin Durmus, He He, Claire Cardie, Kathleen McKeown&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.100/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/fladhak/effective-faithfulness&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors. While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive. In this work, we present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulness-abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as well as recently proposed methods for improving faithfulness, fail to consistently improve over the control at the same level of abstractiveness. Finally, we learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets. Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation&lt;/strong&gt; &lt;em&gt;Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.13134&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xieyxclack/factual_coco&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Factual Consistency of Abstractive Summarization on Customer Feedback&lt;/strong&gt; &lt;em&gt;Yang Liu, Yifei Sun, Vincent Gao&lt;/em&gt; &lt;code&gt;ACL 2021 Proceedings of The 4th Workshop on e-Commerce and NLP&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.ecnlp-1.19/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AgreeSum: Agreement-Oriented Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Richard Yuanzhe Pang, Adam D. Lelkes, Vinh Q. Tran, Cong Yu&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.02278&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/AgreeSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focus Attention: Promoting Faithfulness and Diversity in Summarization&lt;/strong&gt; &lt;em&gt;Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, Ryan McDonald&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.474/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Factual Consistency of Abstractive Summarization via Question Answering&lt;/strong&gt; &lt;em&gt;Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.536/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-research/fact-check-summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discourse Understanding and Factual Consistency in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Saadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys, Kyle Lo, Asli Celikyilmaz, Yejin Choi&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.34/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/skgabriel/coopnet&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection&lt;/strong&gt; &lt;em&gt;Sihao Chen, Fan Zhang, Kazoo Sone and Dan Roth&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.09061&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/CogComp/faithful_summarization&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics&lt;/strong&gt; &lt;em&gt;Artidoro Pagnoni, Vidhisha Balachandran and Yulia Tsvetkov&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.13346&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/artidoro/frank&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Annotating and Modeling Fine-grained Factuality in Summarization&lt;/strong&gt; &lt;em&gt;Tanya Goyal, Greg Durrett&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.04302&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tagoyal/factuality-datasets&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SAFEval: Summarization Asks for Fact-based Evaluation&lt;/strong&gt; &lt;em&gt;Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.12693&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/recitalAI/QuestEval&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancing Factual Consistency of Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, Meng Jiang&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2003.08612&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Entity-level Factual Consistency of Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen McKeown, Bing Xiang&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.235/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-research/fact-check-summarization&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On the Faithfulness for E-commerce Product Summarization&lt;/strong&gt; &lt;em&gt;Peng Yuan, Haoran Li, Song Xu, Youzheng Wu, Xiaodong He, Bowen Zhou&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.502/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ypnlp/coling&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FFCI: A Framework for Interpretable Automatic Evaluation of Summarization&lt;/strong&gt; &lt;em&gt;Fajri Koto, Jey Han Lau, Timothy Baldwin&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.13662&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/fajri91/ffci&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GSum: A General Framework for Guided Neural Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.08014&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/neulab/guided_summarization&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Truth or Error? Towards systematic analysis of factual errors in abstractive summaries&lt;/strong&gt; &lt;em&gt;Klaus-Michael Lux, Maya Sappelli, Martha Larson&lt;/em&gt; &lt;code&gt;EMNLP | Eval4NLP 20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.eval4nlp-1.1/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Detecting Hallucinated Content in Conditional Neural Sequence Generation&lt;/strong&gt; &lt;em&gt;Chunting Zhou, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, Marjan Ghazvininejad&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.02593&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/violet-zct/fairseq-detect-hallucination&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Go Figure! A Meta Evaluation of Factuality in Summarization&lt;/strong&gt; &lt;em&gt;Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.12834&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Constrained Abstractive Summarization: Preserving Factual Consistency with Constrained Generation&lt;/strong&gt; &lt;em&gt;Yuning Mao, Xiang Ren, Heng Ji, Jiawei Han&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.12723&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Factual Error Correction for Abstractive Summarization Models&lt;/strong&gt; &lt;em&gt;Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung&lt;/em&gt; &lt;code&gt;EMNLP20 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.08712&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mcao610/Factual-Error-Correction&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-correct-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Fact Correction in Abstractive Text Summarization.&lt;/strong&gt; &lt;em&gt;Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, Jingjing Liu&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.02443&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-correct-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Factual Error Correction for Abstractive Summarization Models&lt;/strong&gt; &lt;em&gt;Cao Meng, Yue Cheung Dong, Jiapeng Wu, and Jackie Chi Kit&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-correct-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluating the Factual Consistency of Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.12840&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/factCC&#34;&gt;[code]&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reducing Quantity Hallucinations in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Zheng Zhao, Shay B. Cohen, Bonnie Webber&lt;/em&gt; &lt;code&gt;Findings of EMNLP&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2009.13312&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On Faithfulness and Factuality in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald&lt;/em&gt;&lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.00661&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/xsum_hallucination_annotations&#34;&gt;[data]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-analysis-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Truthfulness of Headline Generation&lt;/strong&gt; &lt;em&gt;Kazuki Matsumaru, Sho Takase, Naoaki Okazaki&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.00882&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports&lt;/strong&gt; &lt;em&gt;Yuhao Zhang, Derek Merck, Emily Bao Tsai, Christopher D. Manning, Curtis P. Langlotz&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.02541&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Esin Durmus, He He, Mona Diab&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.03754&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/esdurmus/feqa&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asking and Answering Questions to Evaluate the Factual Consistency of Summaries&lt;/strong&gt; &lt;em&gt;Alex Wang, Kyunghyun Cho, Mike Lewis&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.04228&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/W4ngatang/qags&#34;&gt;[code]&lt;/a&gt;&lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward&lt;/strong&gt; &lt;em&gt;Luyang Huang, Lingfei Wu, Lu Wang&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.01159&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mind The Facts: Knowledge-Boosted Coherent Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Beliz Gunel, Chenguang Zhu, Michael Zeng, Xuedong Huang&lt;/em&gt; &lt;code&gt;NIPS19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.15435&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Assessing The Factual Accuracy of Generated Text&lt;/strong&gt; &lt;em&gt;Ben Goodrich, Vinay Rao, Mohammad Saleh, Peter J Liu&lt;/em&gt; &lt;code&gt;KDD19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.13322&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference&lt;/strong&gt; &lt;em&gt;Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, Iryna Gurevych&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1213/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2002&#34;&gt;[data]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;COLING18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/C18-1121/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hrlinlp/entail_sum&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faithful to the Original: Fact-Aware Neural Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Ziqiang Cao, Furu Wei, Wenjie Li, Sujian Li&lt;/em&gt; &lt;code&gt;AAAI18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1711.04434&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FAR-ASS：Fact-aware reinforced abstractive sentence summarization&lt;/strong&gt; &lt;em&gt;MengLi Zhanga, Gang Zhoua, Wanting Yua, Wenfen Liub&lt;/em&gt; &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0306457320309675&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contrastive Learning&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization&lt;/strong&gt; &lt;em&gt;COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.508/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenxinAn-fdu/CoLo&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called COLO. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that COLO boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3~8 speed-up ratio during inference while maintaining comparable results.&#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Shuyang Cao, Lu Wang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.09209&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://shuyangcao.github.io/projects/cliff_summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequence Level Contrastive Learning for Text Summarization&lt;/strong&gt; &lt;em&gt;Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; [pdf]](&lt;a href=&#34;https://arxiv.org/abs/2109.03481&#34;&gt;https://arxiv.org/abs/2109.03481&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced Seq2Seq Autoencoder via Contrastive Learning for Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, Ling Fan, Zhe Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.11992&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/chz816/esacl&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Constructing Contrastive samples via Summarization for Text Classification with limited annotations&lt;/strong&gt; &lt;em&gt;Yangkai Du, Tengfei Ma, Lingfei Wu, Fangli Xu, Xuhong Zhang, Shouling Ji&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.05094&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Alleviating Exposure Bias via Contrastive Learning for Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Shichao Sun, Wenjie Li&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.11846&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ShichaoSun/ConAbsSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yixin Liu, Pengfei Liu&lt;/em&gt; &lt;code&gt;ACL 2021 short&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-short.135/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yixinL7/SimCLS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contrastive Learning with Adversarial Perturbations for Conditional Text Generation&lt;/strong&gt; &lt;em&gt;Seanie Lee, Dong Bok Lee, Sung Ju Hwang&lt;/em&gt; &lt;code&gt;ICLR 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07280&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DeepChannel: Salience Estimation by Contrastive Learning for Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Jiaxin Shi, Chen Liang, Lei Hou, Juanzi Li, Zhiyuan Liu, Hanwang Zhang&lt;/em&gt; &lt;code&gt;AAAI 2019&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1811.02394&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lliangchenc/DeepChannel&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning&lt;/strong&gt; &lt;em&gt;Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, Shouling Ji&lt;/em&gt; &lt;code&gt;EMNLP 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.01781&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/whl97/LS-Score&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contrastive Attention Mechanism for Abstractive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Xiangyu Duan, Hongfei Yu, Mingming Yin, Min Zhang, Weihua Luo, Yue Zhang&lt;/em&gt; &lt;code&gt;EMNLP 2019&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-1301/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/travel-go/Abstractive-Text-Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Universal Evasion Attacks on Summarization Scoring&lt;/strong&gt; &lt;em&gt;Wenchuan Mu, Kwan Hui Lim&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.14260&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The automatic scoring of summaries is important as it guides the development of summarizers. Scoring is also complex, as it involves multiple aspects such as fluency, grammar, and even textual entailment with the source text. However, summary scoring has not been considered a machine learning task to study its accuracy and robustness. In this study, we place automatic scoring in the context of regression machine learning tasks and perform evasion attacks to explore its robustness. Attack systems predict a non-summary string from each input, and these non-summary strings achieve competitive scores with good summarizers on the most popular metrics: ROUGE, METEOR, and BERTScore. Attack systems also &#34;outperform&#34; state-of-the-art summarization methods on ROUGE-1 and ROUGE-L, and score the second-highest on METEOR. Furthermore, a BERTScore backdoor is observed: a simple trigger can score higher than any automatic summarization method. The evasion attacks in this work indicate the low robustness of current scoring systems at the system level. We hope that our highlighting of these proposed attacks will facilitate the development of summary scores. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Repetition in Abstractive Neural Summarizers&lt;/strong&gt; &lt;em&gt;Nikita Salkar, Thomas Trikalinos, Byron C. Wallace, Ani Nenkova&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.08145&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We provide a quantitative and qualitative analysis of self-repetition in the output of neural summarizers. We measure self-repetition as the number of n-grams of length four or longer that appear in multiple outputs of the same system. We analyze the behavior of three popular architectures (BART, T5, and Pegasus), fine-tuned on five datasets. In a regression analysis, we find that the three architectures have different propensities for repeating content across output summaries for inputs, with BART being particularly prone to self-repetition. Fine-tuning on more abstractive data, and on data featuring formulaic language, is associated with a higher rate of self-repetition. In qualitative analysis we find systems produce artefacts such as ads and disclaimers unrelated to the content being summarized, as well as formulaic phrases common in the fine-tuning domain. Our approach to corpus-level analysis of self-repetition may help practitioners clean up training data for summarizers and ultimately support methods for minimizing the amount of self-repetition. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation&lt;/strong&gt; &lt;em&gt;Julius Steen, Katja Markert&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.527/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/julmaxi/summary_coherence_evaluation&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatically evaluating the coherence of summaries is of great significance both to enable cost-efficient summarizer evaluation and as a tool for improving coherence by selecting high-scoring candidate summaries. While many different approaches have been suggested to model summary coherence, they are often evaluated using disparate datasets and metrics. This makes it difficult to understand their relative performance and identify ways forward towards better summary coherence modelling. In this work, we conduct a large-scale investigation of various methods for summary coherence modelling on an even playing field. Additionally, we introduce two novel analysis measures, &#xA;   &lt;em&gt;intra-system correlation&lt;/em&gt; and &#xA;   &lt;em&gt;bias matrices&lt;/em&gt;, that help identify biases in coherence measures and provide robustness against system-level confounders. While none of the currently available automatic coherence measures are able to assign reliable coherence scores to system summaries across all evaluation metrics, large-scale language models fine-tuned on self-supervised tasks show promising results, as long as fine-tuning takes into account that they need to generalize across different summary lengths. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PrefScore: Pairwise Preference Learning for Reference-free Summarization Quality Assessment&lt;/strong&gt; &lt;em&gt;Ge Luo, Hebi Li, Youbiao He, Forrest Sheng Bao&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.515/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/NKWBTB/PrefScore&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Evaluating machine-generated summaries without a human-written reference summary has been a need for a long time. Inspired by preference labeling in existing work of summarization evaluation, we propose to judge summary quality by learning the preference rank of summaries using the Bradley-Terry power ranking model from inferior summaries generated by corrupting base summaries. Extensive experiments on several datasets show that our weakly supervised scheme can produce scores highly correlated with human ratings. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to Find Strong Summary Coherence Measures? A Toolbox and a Comparative Study for Summary Coherence Measure Evaluation&lt;/strong&gt; &lt;em&gt;Julius Steen, Katja Markert&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.06517&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/julmaxi/summary_coherence_evaluation&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatically evaluating the coherence of summaries is of great significance both to enable cost-efficient summarizer evaluation and as a tool for improving coherence by selecting high-scoring candidate summaries. While many different approaches have been suggested to model summary coherence, they are often evaluated using disparate datasets and metrics. This makes it difficult to understand their relative performance and identify ways forward towards better summary coherence modelling. In this work, we conduct a large-scale investigation of various methods for summary coherence modelling on an even playing field. Additionally, we introduce two novel analysis measures, intra-system correlation and bias matrices, that help identify biases in coherence measures and provide robustness against system-level confounders. While none of the currently available automatic coherence measures are able to assign reliable coherence scores to system summaries across all evaluation metrics, large-scale language models fine-tuned on self-supervised tasks show promising results, as long as fine-tuning takes into account that they need to generalize across different summary lengths.&#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummScore: A Comprehensive Evaluation Metric for Summary Quality Based on Cross-Encoder&lt;/strong&gt; &lt;em&gt;Wuhang Lin, Shasha Li, Chen Zhang, Bin Ji, Jie Yu, Jun Ma, Zibo Yi&lt;/em&gt; &lt;code&gt;APWeb-WAIM2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.04660&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Text summarization models are often trained to produce summaries that meet human quality requirements. However, the existing evaluation metrics for summary text are only rough proxies for summary quality, suffering from low correlation with human scoring and inhibition of summary diversity. To solve these problems, we propose SummScore, a comprehensive metric for summary quality evaluation based on CrossEncoder. Firstly, by adopting the original-summary measurement mode and comparing the semantics of the original text, SummScore gets rid of the inhibition of summary diversity. With the help of the text-matching pre-training Cross-Encoder, SummScore can effectively capture the subtle differences between the semantics of summaries. Secondly, to improve the comprehensiveness and interpretability, SummScore consists of four fine-grained submodels, which measure Coherence, Consistency, Fluency, and Relevance separately. We use semi-supervised multi-rounds of training to improve the performance of our model on extremely limited annotated data. Extensive experiments show that SummScore significantly outperforms existing evaluation metrics in the above four dimensions in correlation with human scoring. We also provide the quality evaluation results of SummScore on 16 mainstream summarization models for later research. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Does Summary Evaluation Survive Translation to Other Languages?&lt;/strong&gt; &lt;em&gt;Spencer Braun, Oleg Vasilyev, Neslihan Iskender, John Bohannon&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.173/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/PrimerAI/primer-research/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at &#xA;   &lt;a href=&#34;https://github.com/PrimerAI/primer-research/&#34;&gt;https://github.com/PrimerAI/primer-research/&lt;/a&gt;. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics&lt;/strong&gt; &lt;em&gt;Daniel Deutsch, Rotem Dror, Dan Roth&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.442/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://cogcomp.seas.upenn.edu/page/publication_view/973&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SueNes: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling&lt;/strong&gt; &lt;em&gt;Forrest Bao, Ge Luo, Hebi Li, Minghui Qiu, Yinfei Yang, Youbiao He, Cen Chen&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.175/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/forrestbao/SueNes/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Canonical automatic summary evaluation metrics, such as ROUGE, focus on lexical similarity which cannot well capture semantics nor linguistic quality and require a reference summary which is costly to obtain. Recently, there have been a growing number of efforts to alleviate either or both of the two drawbacks. In this paper, we present a proof-of-concept study to a weakly supervised summary evaluation approach without the presence of reference summaries. Massive data in existing summarization datasets are transformed for training by pairing documents with corrupted reference summaries. In cross-domain tests, our strategy outperforms baselines with promising improvements, and show a great advantage in gauging linguistic qualities over all metrics. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reference-free Summarization Evaluation via Semantic Correlation and Compression Ratio&lt;/strong&gt; &lt;em&gt;Yizhu Liu, Qi Jia, Kenny Zhu&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.153/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/YizhuLiu/summeval&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; A document can be summarized in a number of ways. Reference-based evaluation of summarization has been criticized for its inflexibility. The more sufficient the number of abstracts, the more accurate the evaluation results. However, it is difficult to collect sufficient reference summaries. In this paper, we propose a new automatic reference-free evaluation metric that compares semantic distribution between source document and summary by pretrained language models and considers summary compression ratio. The experiments show that this metric is more consistent with human evaluation in terms of coherence, consistency, relevance and fluency. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MaskEval: Weighted MLM-Based Evaluation for Text Summarization and Simplification&lt;/strong&gt; &lt;em&gt;Yu Lu Liu, Rachel Bawden, Thomas Scaliom, Benoît Sagot, Jackie Chi Kit Cheung&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.12394&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/YuLuLiu/MaskEval&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TRUE: Re-evaluating Factual Consistency Evaluation&lt;/strong&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.04991&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation&lt;/strong&gt; &lt;em&gt;Nicholas Egan, Oleg Vasilyev, John Bohannon&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.10918&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/PrimerAI/blanc/tree/master/shannon&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Differentiable N-gram Objective on Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yunqi Zhu, Wensheng Zhang, Mingjin Zhu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.04003&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhuyunqi96/ngramObj&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence&lt;/strong&gt; &lt;em&gt;Wei Zhao, Michael Strube, Steffen Eger&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.11176&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/AIPHES/DiscoScore&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WIDAR -- Weighted Input Document Augmented ROUGE&lt;/strong&gt; &lt;em&gt;Raghav Jain, Vaibhav Mavi, Anubhav Jangra, Sriparna Saha&lt;/em&gt; &lt;code&gt;ECIR 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.09282&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Raghav10j/WIDAR&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;InfoLM: A New Metric to Evaluate Summarization &amp;amp; Data2Text Generation&lt;/strong&gt; &lt;em&gt;Pierre Colombo, Chloe Clave, Pablo Piantanida&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.01589&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation of Summarization Systems across Gender, Age, and Race&lt;/strong&gt; &lt;em&gt;Anna Jørgensen, Anders Søgaard&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.6/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes&lt;/strong&gt; &lt;em&gt;M. Arana-Catania, Rob Procter, Yulan He, Maria Liakata&lt;/em&gt; &lt;code&gt;EMNLP 2021 New Frontiers in Summarization Workshop&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.05847&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation of Summarization Systems across Gender, Age, and Race&lt;/strong&gt; &lt;em&gt;Anna Jørgensen, Anders Søgaard&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.04384&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Finding a Balanced Degree of Automation for Summary Evaluation&lt;/strong&gt; &lt;em&gt;Shiyue Zhang, Mohit Bansal&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.11503&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ZhangShiyue/Lite2-3Pyramid&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;QuestEval: Summarization Asks for Fact-based Evaluation&lt;/strong&gt; &lt;em&gt;Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.12693&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/recitalAI/QuestEval&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BARTScore: Evaluating Generated Text as Text Generation&lt;/strong&gt; &lt;em&gt;Weizhe Yuan, Graham Neubig, Pengfei Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.11520&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/neulab/BARTScore&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy&lt;/strong&gt; &lt;em&gt;Wang Chen, Piji Li, Irwin King&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.34/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Chen-Wang-CUHK/Training-Free-and-Ref-Free-Summ-Evaluation&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluating the Efficacy of Summarization Evaluation across Languages&lt;/strong&gt; &lt;em&gt;Fajri Koto, Jey Han Lau, Timothy Baldwin&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.01478&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Question-aware Transformer Models for Consumer Health Question Summarization&lt;/strong&gt; &lt;em&gt;Shweta Yadav, Deepak Gupta, Asma Ben Abacha, Dina Demner-Fushman&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.00219&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Human-Free Automatic Quality Evaluation of German Summarization&lt;/strong&gt; &lt;em&gt;Neslihan Iskender, Oleg Vasilyev, Tim Polzehl, John Bohannon, Sebastian Möller&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.06027&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reliability of Human Evaluation for Text Summarization: Lessons Learned and Challenges Ahead&lt;/strong&gt; &lt;em&gt;Neslihan Iskender, Tim Polzehl, Sebastian Möller&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.humeval-1.10/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nesliskender/reliability_humeval_summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for Text Summarization&lt;/strong&gt; &lt;em&gt;Jesse Vig, Wojciech Kryscinski, Karan Goel, Nazneen Fatema Rajani&lt;/em&gt; &lt;code&gt;ACL 2021 demo&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-demo.18/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/robustness-gym/summvis&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Is human scoring the best criteria for summary evaluation?&lt;/strong&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;em&gt;Oleg Vasilyev, John Bohannon&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.14602&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to Evaluate a Summarizer: Study Design and Statistical Analysis for Manual Linguistic Quality Evaluation&lt;/strong&gt; &lt;em&gt;Julius Steen, Katja Markert&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.160/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/julmaxi/summary_lq_analysis&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HOLMS: Alternative Summary Evaluation with Large Language Models&lt;/strong&gt; &lt;em&gt;Yassine Mrabet, Dina Demner-Fushman&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.498/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.498.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FFCI: A Framework for Interpretable Automatic Evaluation of Summarization&lt;/strong&gt; &lt;em&gt;Fajri Koto, Jey Han Lau, Timothy Baldwin&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.13662&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/fajri91/ffci&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-evaluation-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning&lt;/strong&gt; &lt;em&gt;Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, Shouling Ji&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.01781&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/whl97/LS-Score&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics&lt;/strong&gt; &lt;em&gt;Daniel Deutsch, Dan Roth&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.05374&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/danieldeutsch/sacrerouge&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummEval: Re-evaluating Summarization Evaluation&lt;/strong&gt; &lt;em&gt;Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, Dragomir Radev&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.12626&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Yale-LILY/SummEval&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HIGHRES: Highlight-based Reference-less Evaluation of Summarization&lt;/strong&gt; &lt;em&gt;Hardy, Shashi Narayan, Andreas Vlachos&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.01361&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sheffieldnlp/highres&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Multi-Document&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;How &#34;Multi&#34; is Multi-Document Summarization?&lt;/strong&gt; &lt;em&gt;Ruben Wolhandler, Arie Cattan, Ori Ernst, Ido Dagan&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.12688&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ariecattan/multi_mds&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The task of multi-document summarization (MDS) aims at models that, given multiple documents as input, are able to generate a summary that combines disperse information, originally spread across these documents. Accordingly, it is expected that both reference summaries in MDS datasets, as well as system summaries, would indeed be based on such dispersed information. In this paper, we argue for quantifying and assessing this expectation. To that end, we propose an automated measure for evaluating the degree to which a summary is ``disperse&#39;&#39;, in the sense of the number of source documents needed to cover its content. We apply our measure to empirically analyze several popular MDS datasets, with respect to their reference summaries, as well as the output of state-of-the-art systems. Our results show that certain MDS datasets barely require combining information from multiple documents, where a single document often covers the full summary content. Overall, we advocate using our metric for assessing and improving the degree to which summarization datasets require combining multi-document information, and similarly how summarization models actually meet this challenge. Our code is available in this https URL. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analyzing the Dialect Diversity in Multi-document Summaries&lt;/strong&gt; &lt;em&gt;Olubusayo Olabisi, Aaron Hudson, Antonie Jetter, Ameeta Agrawal&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.542/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/PortNLP/DivSumm&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Social media posts provide a compelling, yet challenging source of data of diverse perspectives from many socially salient groups. Automatic text summarization algorithms make this data accessible at scale by compressing large collections of documents into short summaries that preserve salient information from the source text. In this work, we take a complementary approach to analyzing and improving the quality of summaries generated from social media data in terms of their ability to represent salient as well as diverse perspectives. We introduce a novel dataset, DivSumm, of dialect diverse tweets and human-written extractive and abstractive summaries. Then, we study the extent of dialect diversity reflected in human-written reference summaries as well as system-generated summaries. The results of our extensive experiments suggest that humans annotate fairly well-balanced dialect diverse summaries, and that cluster-based pre-processing approaches seem beneficial in improving the overall quality of the system-generated summaries without loss in diversity. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document-aware Positional Encoding and Linguistic-guided Encoding for Abstractive Multi-document Summarization&lt;/strong&gt; &lt;em&gt;Congbo Ma, Wei Emma Zhang, Pitawelayalage Dasun Dileepa Pitawela, Yutong Qu, Haojie Zhuang, Hu Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.05929&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; One key challenge in multi-document summarization is to capture the relations among input documents that distinguish between single document summarization (SDS) and multi-document summarization (MDS). Few existing MDS works address this issue. One effective way is to encode document positional information to assist models in capturing cross-document relations. However, existing MDS models, such as Transformer-based models, only consider token-level positional information. Moreover, these models fail to capture sentences&#39; linguistic structure, which inevitably causes confusions in the generated summaries. Therefore, in this paper, we propose document-aware positional encoding and linguistic-guided encoding that can be fused with Transformer architecture for MDS. For document-aware positional encoding, we introduce a general protocol to guide the selection of document encoding functions. For linguistic-guided encoding, we propose to embed syntactic dependency relations into the dependency relation mask with a simple but effective non-linear encoding learner for feature learning. Extensive experiments show the proposed model can generate summaries with high quality. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Document Scientific Summarization from a Knowledge Graph-Centric View&lt;/strong&gt; &lt;em&gt;Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.543/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/muguruzawang/KGSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating a Structured Summary of Numerous Academic Papers: Dataset and Method&lt;/strong&gt; &lt;em&gt;Shuaiqi LIU, Jiannong Cao, Ruosong Yang, Zhiyuan Wen&lt;/em&gt; &lt;code&gt;IJCAI 2022&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/proceedings/2022/591&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/StevenLau6/BigSurvey&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Writing a survey paper on one research topic usually needs to cover the salient content from numerous related papers, which can be modeled as a multi-document summarization (MDS) task. Existing MDS datasets usually focus on producing the structureless summary covering a few input documents. Meanwhile, previous structured summary generation works focus on summarizing a single document into a multi-section summary. These existing datasets and methods cannot meet the requirements of summarizing numerous academic papers into a structured summary. To deal with the scarcity of available data, we propose BigSurvey, the first large-scale dataset for generating comprehensive summaries of numerous academic papers on each topic. We collect target summaries from more than seven thousand survey papers and utilize their 430 thousand reference papers’ abstracts as input documents. To organize the diverse content from dozens of input documents and ensure the efficiency of processing long text sequences, we propose a summarization method named category-based alignment and sparse transformer (CAST). The experimental results show that our CAST method outperforms various advanced summarization methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Proposition-Level Clustering for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Ori Ernst, Avi Caciularu, Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Jacob Goldberger, Ido Dagan&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.128/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/oriern/ProCluster&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Text clustering methods were traditionally incorporated into multi-document summarization (MDS) as a means for coping with considerable information repetition. Particularly, clusters were leveraged to indicate information saliency as well as to avoid redundancy. Such prior methods focused on clustering sentences, even though closely related sentences usually contain also non-aligned parts. In this work, we revisit the clustering approach, grouping together sub-sentential propositions, aiming at more precise information alignment. Specifically, our method detects salient propositions, clusters them into paraphrastic clusters, and generates a representative sentence for each cluster via text fusion.Our summarization method improves over the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011 datasets, both in automatic ROUGE scores and human preference. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities&lt;/strong&gt; &lt;em&gt;Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, Doug Downey&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.10883&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/multilexsum/dataset&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC) (this https URL),which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence &#34;extreme&#34; summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further research in summarization methods as well as to facilitate development of applications to assist in the CRLC&#39;s mission at this https URL. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization&lt;/strong&gt; &lt;em&gt;Alexander R. Fabbri, Xiaojian Wu, Srini Iyer, Haoran Li, Mona Diab&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.180/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Alex-Fabbri/AnswerSumm&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Community Question Answering (CQA) fora such as Stack Overflow and Yahoo! Answers contain a rich resource of answers to a wide range of community-based questions. Each question thread can receive a large number of answers with different perspectives. One goal of answer summarization is to produce a summary that reflects the range of answer perspectives. A major obstacle for this task is the absence of a dataset to provide supervision for producing such summaries. Recent works propose heuristics to create such data, but these are often noisy and do not cover all answer perspectives present. This work introduces a novel dataset of 4,631 CQA threads for answer summarization curated by professional linguists. Our pipeline gathers annotations for all subtasks of answer summarization, including relevant answer sentence selection, grouping these sentences based on perspectives, summarizing each perspective, and producing an overall summary. We analyze and benchmark state-of-the-art models on these subtasks and introduce a novel unsupervised approach for multi-perspective data augmentation that boosts summarization performance according to automatic evaluation. Finally, we propose reinforcement learning rewards to improve factual consistency and answer coverage and analyze areas for improvement. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature&lt;/strong&gt; &lt;em&gt;Yulia Otmakhova, Karin Verspoor, Timothy Baldwin, Jey Han Lau&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.350/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Although multi-document summarisation (MDS) of the biomedical literature is a highly valuable task that has recently attracted substantial interest, evaluation of the quality of biomedical summaries lacks consistency and transparency. In this paper, we examine the summaries generated by two current models in order to understand the deficiencies of existing evaluation approaches in the context of the challenges that arise in the MDS task. Based on this analysis, we propose a new approach to human evaluation and identify several challenges that must be overcome to develop effective biomedical MDS systems. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Georgios Katsimpras, Georgios Paliouras&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.137/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge. However, the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks. In this study, we propose a new method to predict the effectiveness of an intervention in a clinical trial. Our method relies on generating an informative summary from multiple documents available in the literature about the intervention under study. Specifically, our method first gathers all the abstracts of PubMed articles related to the intervention. Then, an evidence sentence, which conveys information about the effectiveness of the intervention, is extracted automatically from each abstract. Based on the set of evidence sentences extracted from the abstracts, a short summary about the intervention is constructed. Finally, the produced summaries are used to train a BERT-based classifier, in order to infer the effectiveness of an intervention. To evaluate our proposed method, we introduce a new dataset which is a collection of clinical trials together with their associated PubMed articles. Our experiments, demonstrate the effectiveness of producing short informative summaries and using them to predict the effectiveness of an intervention. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature&lt;/strong&gt; &lt;em&gt;Gianluca Moro, Luca Ragazzi, Lorenzo Valgimigli, Davide Freddi&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.15/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://disi-unibo-nlp.github.io/projects/damen/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can be vital. Others leverage linear model approximations to apply multi-input concatenation, worsening the results because all information is considered, even if it is conflicting or noisy with respect to a shared background. Despite the importance and social impact of medicine, there are no ad-hoc solutions for multi-document summarization. For this reason, we propose a novel discriminative marginalized probabilistic method (DAMEN) trained to discriminate critical information from a cluster of topic-related medical documents and generate a multi-document summary via token probability marginalization. Results prove we outperform the previous state-of-the-art on a biomedical dataset for multi-document summarization of systematic literature reviews. Moreover, we perform extensive ablation studies to motivate the design choices and prove the importance of each module of our method. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ACM -- Attribute Conditioning for Abstractive Multi Document Summarization&lt;/strong&gt; &lt;em&gt;Aiswarya Sankar, Ankit Chadha&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.03978&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness&lt;/strong&gt; &lt;em&gt;Yun-Zhu Song, Yi-Syuan Chen, Hong-Han Shuai&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.120/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yunzhusong/NAACL2022-REFLECT&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; A notable challenge in Multi-Document Summarization (MDS) is the extremely-long length of the input. In this paper, we present an extract-then-abstract Transformer framework to overcome the problem. Specifically, we leverage pre-trained language models to construct a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries. However, learning such a framework is challenging since the optimal contents for the abstractor are generally unknown. Previous works typically create pseudo extraction oracle to enable the supervised learning for both the extractor and the abstractor. Nevertheless, we argue that the performance of such methods could be restricted due to the insufficient information for prediction and inconsistent objectives between training and testing. To this end, we propose a loss weighting mechanism that makes the model aware of the unequal importance for the sentences not in the pseudo extraction oracle, and leverage the fine-tuned abstractor to generate summary references as auxiliary signals for learning the extractor. Moreover, we propose a reinforcement learning method that can efficiently apply to the extractor for harmonizing the optimization between training and testing. Experiment results show that our framework substantially outperforms strong baselines with comparable model sizes and achieves the best results on the Multi-News, Multi-XScience, and WikiCatSum corpora. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias&lt;/strong&gt; &lt;em&gt;Nayeon Lee, Yejin Bang, Tiezheng Yu, Andrea Madotto, Pascale Fung&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.228/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/HLTCHKUST/framing-bias-metric&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Media news framing bias can increase political polarization and undermine civil society. The need for automatic mitigation methods is therefore growing. We propose a new task, a neutral summary generation from multiple news articles of the varying political leaningsto facilitate balanced and unbiased news reading.In this paper, we first collect a new dataset, illustrate insights about framing bias through a case study, and propose a new effective metric and model (NeuS-Title) for the task. Based on our discovery that title provides a good signal for framing bias, we present NeuS-Title that learns to neutralize news content in hierarchical order from title to article. Our hierarchical multi-task learning is achieved by formatting our hierarchical data pair (title, article) sequentially with identifier-tokens (“TITLE=&amp;gt;”, “ARTICLE=&amp;gt;”) and fine-tuning the auto-regressive decoder with the standard negative log-likelihood objective.We then analyze and point out the remaining challenges and future directions. One of the most interesting observations is that neural NLG models can hallucinate not only factually inaccurate or unverifiable content but also politically biased content. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Read Top News First: A Document Reordering Approach for Multi-Document News Summarization&lt;/strong&gt; &lt;em&gt;Chao Zhao, Tenghao Huang, Somnath Basu Roy Chowdhury, Muthu Kumar Chandrasekaran, Kathleen McKeown, Snigdha Chaturvedi&lt;/em&gt; &lt;code&gt;Findings of ACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.10254&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhaochaocs/MDS-DR&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.351/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jacob-parnell-rozetta/longformer_coverage/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models. However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks, reinforcement learning (RL) offers the potential to improve the training of MDS models; yet, it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents. For this reason, in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents. To implement the approach, we utilize RELAX (Grathwohl et al., 2018), a contemporary gradient estimator which is both low-variance and unbiased, and we fine-tune the baseline in a few-shot style for both stability and computational efficiency. Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline, and competitive results with the literature. In addition, they show that the coverage of the input documents is increased, and evenly across all documents. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization&lt;/strong&gt; &lt;em&gt;Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.360/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/PRIMER&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PeerSum: A Peer Review Dataset for Abstractive Multi-document Summarization&lt;/strong&gt; &lt;em&gt;Miao Li, Jianzhong Qi, Jey Han Lau&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.01769&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/oaimli/PeerSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Proposition-Level Clustering Approach for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Ori Ernst, Avi Caciularu, Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Jacob Goldberger, Ido Dagan&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.08770&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/oriern/ClusterProp&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MSˆ2: Multi-Document Summarization of Medical Studies&lt;/strong&gt; &lt;em&gt;Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, Lucy Wang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.594/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/ms2&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SgSum: Transforming Multi-document Summarization into Sub-graph Selection&lt;/strong&gt; &lt;em&gt;Moye Chen, Wei Li, Jiachen Liu, Xinyan Xiao, Hua Wu, Haifeng Wang&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.12645&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/Research/tree/master/NLP/EMNLP2021-SgSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic-Guided Abstractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Peng Cui, Le Hu&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.11207&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modeling Endorsement for Multi-Document Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Logan Lebanoff, Bingqing Wang, Zhe Feng, Fei Liu&lt;/em&gt; &lt;code&gt;EMNLP 2021|newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.13/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization&lt;/strong&gt; &lt;em&gt;Congbo Ma, Wei Emma Zhang, Hu Wang, Shubham Gupta, Mingyu Guo&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.11199&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation&lt;/strong&gt; &lt;em&gt;Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.473/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/iriscxy/relatedworkgeneration&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highlight-Transformer: Leveraging Key Phrase Aware Attention to Improve Abstractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen&lt;/em&gt; &lt;code&gt;ACL 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-acl.445/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Entity-Aware Abstractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Hao Zhou, Weidong Ren, Gongshen Liu, Bo Su, Wei Lu&lt;/em&gt; &lt;code&gt;ACL 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-acl.30/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Oceandam/EMSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TWAG: A Topic-Guided Wikipedia Abstract Generator&lt;/strong&gt; &lt;em&gt;Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.356/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-KEG/TWAG&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AgreeSum: Agreement-Oriented Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Richard Yuanzhe Pang, Adam D. Lelkes, Vinh Q. Tran, Cong Yu&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.02278&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/AgreeSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analysis of GraphSum&#39;s Attention Weights to Improve the Explainability of Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;M. Lautaro Hickmann, Fabian Wurzberger, Megi Hoxhalli, Arne Lochner, Jessica Töllich, Ansgar Scherp&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.11908&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extending Multi-Document Summarization Evaluation to the Interactive Setting&lt;/strong&gt; &lt;em&gt;Ori Shapira, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, Yael Amsterdamer, Ido Dagan&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.54/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/OriShapira/InterExp&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters&lt;/strong&gt; &lt;em&gt;Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, Sujith Ravi, Markus Dreyer&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.380/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-research/BartGraphSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised and Controlled Multi-Document Opinion Summarization&lt;/strong&gt; &lt;em&gt;Hady Elsahar, Maximin Coavoux, Jos Rozen, Matthias Gallé&lt;/em&gt; &lt;code&gt;EACL 2021&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.141/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MS2: Multi-Document Summarization of Medical Studies&lt;/strong&gt; &lt;em&gt;Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Keuhl, Lucy Lu Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.06486&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/ms2&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nutri-bullets: Summarizing Health Studies by Composing Segments&lt;/strong&gt; &lt;em&gt;Darsh J Shah, Lili Yu, Tao Lei, Regina Barzilay&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.11921&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/darsh10/Nutribullets&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-document Summarization using Semantic Role Labeling and Semantic Graph for Indonesian News Article&lt;/strong&gt; &lt;em&gt;Yuly Haruka Berliana Gunawan, Masayu Leylia Khodra&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.03736&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flight of the PEGASUS? Comparing Transformers on Few-Shot and Zero-Shot Multi-document Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Travis Goodwin, Max Savery, Dina Demner-Fushman&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.494/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization&lt;/strong&gt; &lt;em&gt;Hanqi Jin, Xiaojun Wan&lt;/em&gt; &lt;code&gt;Findings of EMNLP&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.231/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhongxia96/MDS-and-SDS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Coarse-to-Fine Query Focused Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Yumo Xu, Mirella Lapata&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.296/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yumoxu/querysum&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://github.com/yumoxu/querysum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WSL-DS: Weakly Supervised Learning with Distant Supervision for Query Focused Multi-Document Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Xiangji Huang&lt;/em&gt; &lt;code&gt;COLING20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.01421&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tahmedge/WSL-DS-COLING-2020&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, Eugene Ie&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.12694&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/aquamuse&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning&lt;/strong&gt; &lt;em&gt;Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, Jiawei Han&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.00117&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/morningmoni/RL-MMR.git&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks for Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.12393v1&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/brxx122/HeterSUMGraph&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Hanqi Jin, Tianming Wang, Xiaojun Wan&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.556/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Yang Gao, Wei Zhao, Steffen Eger&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.03724&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yg211/acl20-ref-free-eval.git&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leveraging Graph to Improve Abstractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.10043&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2020-GraphSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Representative Headlines for News Stories&lt;/strong&gt; &lt;em&gt;Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, Hongkun Yu, You Wu, Cong Yu, Daniel Finnie, Jiaqi Zhai, Nicholas Zukoski&lt;/em&gt; &lt;code&gt;WWW20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2001.09386&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/NewSHead.git&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Diego Antognini, Boi Faltings&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.12231&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, Fei Liu&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.00072&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/summarization-dpp-capsnet&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Transformers for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Yang Liu, Mirella Lapata&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.13164&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nlpyang/hiersumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Eric Chu, Peter J. Liu&lt;/em&gt; &lt;code&gt;ICML19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1810.05739&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sosuperic/MeanSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Wikipedia By Summarizing Long Sequence&lt;/strong&gt; &lt;em&gt;Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam Shazeer&lt;/em&gt; &lt;code&gt;ICLR18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1801.10198&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/memory-compressed-attention.git&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Logan Lebanoff, Kaiqiang Song, Fei Liu&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1446/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/multidoc_summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graph-based Neural Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, Dragomir Radev&lt;/em&gt; &lt;code&gt;CoNLL17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/K17-1045/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Multi-Document Summarization via Text Classification&lt;/strong&gt; &lt;em&gt;Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei&lt;/em&gt; &lt;code&gt;AAAI17&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1611.09238&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic generation of related work through summarizing citations&lt;/strong&gt; &lt;em&gt;Jingqiang Chen, Hai Zhuge&lt;/em&gt; &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/epdf/10.1002/cpe.4261&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jingqiangchen/RWS-Cit&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Unsupervised Multi-Document Summarization Framework Based on Neural Document Model&lt;/strong&gt; &lt;em&gt;Shulei Ma, Zhi-Hong Deng, Yunlun Yang&lt;/em&gt; &lt;code&gt;COLING16&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/C16-1143/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Event-Centric Summary Generation&lt;/strong&gt; &lt;em&gt;Lucy Vanderwende Michele Banko Arul Menezes&lt;/em&gt; &lt;code&gt;ACL04&lt;/code&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/event-centric-summary-generation/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Cross-Lingual&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, Jie Zhou&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.05599&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/krystalan/ClidSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, Jie Zhou&lt;/em&gt; &lt;code&gt;TACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.12515&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation&lt;/strong&gt; &lt;em&gt;Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, Noah Constant&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.12647&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MSAMSum: Towards Benchmarking Multi-lingual Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Bing Qin&lt;/em&gt; &lt;code&gt;ACL 2022 DialDoc Workshop&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.dialdoc-1.1/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xcfcode/MSAMSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Cross-lingual Conversation Summarization Challenge&lt;/strong&gt; &lt;em&gt;Yulong Chen, Ming Zhong, Xuefeng Bai, Naihao Deng, Jing Li, Xianchao Zhu, Yue Zhang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.00379&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization&lt;/strong&gt; &lt;em&gt;Ruipeng Jia, Xingxing Zhang, Yanan Cao, Shi Wang, Zheng Lin, Furu Wei&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.42/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages. Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics. However, these monolingual labels created on English datasets may not be optimal on datasets of other languages, for that there is the syntactic or semantic discrepancy between different languages. In this way, it is possible to translate the English dataset to other languages and obtain different sets of labels again using heuristics. To fully leverage the information of these different sets of labels, we propose NLSSum (Neural Label Search for Summarization), which jointly learns hierarchical weights for these different sets of labels together with our summarization model. We conduct multilingual zero-shot summarization experiments on MLSUM and WikiLingua datasets, and we achieve state-of-the-art results using both human and automatic evaluations across these two datasets. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bridging Cross-Lingual Gaps During Leveraging the Multilingual Sequence-to-Sequence Pretraining for Text Generation&lt;/strong&gt; &lt;em&gt;Changtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng Liu, Dacheng Tao&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.07834&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Variational Hierarchical Model for Neural Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Yunlong Liang, Fandong Meng, Chulun Zhou, Jinan Xu, Yufeng Chen, Jinsong Su, Jie Zhou&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.148/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/XL2248/VHM&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT&amp;amp;MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CptGraphSum: Let key clues guide the cross-lingual abstractive summarization&lt;/strong&gt; &lt;em&gt;Shuyu Jiang, Dengbiao Tu, Xingshu Chen, Rui Tang, Wenxian Wang, Haizhou Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.02797&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs&lt;/strong&gt; &lt;em&gt;Tahmid Hasan, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yuan-Fang Li, Yong-Bin Kang, Rifat Shahriyar&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.08804&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/csebuetnlp/CrossSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Neural Cross-Lingual Summarization via Employing Optimal Transport Distance for Knowledge Distillation&lt;/strong&gt; &lt;em&gt;Thong Nguyen, Luu Anh Tuan&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.03473&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nguyentthong/CrossSummOptimalTransport&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes&lt;/strong&gt; &lt;em&gt;Miguel Arana-Catania, Rob Procter, Yulan He, Maria Liakata&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.7/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Models and Datasets for Cross-Lingual Summarisation&lt;/strong&gt; &lt;em&gt;Laura Perez-Beltrachini, Mirella Lapata&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.742/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lauhaide/clads&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MassiveSumm: a very large-scale, very multilingual, news summarisation dataset&lt;/strong&gt; &lt;em&gt;Daniel Varab, Natalie Schluter&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.797/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/danielvarab/massive-summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bridging the Gap: Cross-Lingual Summarization with Compression Rate&lt;/strong&gt; &lt;em&gt;Yu Bai, Heyan Huang, Kai Fan, Yang Gao, Zewen Chi, Boxing Chen&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.07936&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contrastive Aligned Joint Learning for Multilingual Summarization&lt;/strong&gt; &lt;em&gt;Danqing Wang, Jiaze Chen, Hao Zhou, Xipeng Qiu, Lei Li&lt;/em&gt; &lt;code&gt;ACL 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-acl.242/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/brxx122/CALMS&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages&lt;/strong&gt; &lt;em&gt;T. Hasan, A. Bhattacharjee, M. S. Islam, K. Samin, Y. Li, Y. Kang, M. S. Rahman, R. Shahriyar&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.13822&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/csebuetnlp/xl-sum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation&lt;/strong&gt; &lt;em&gt;Kaushal Kumar Maurya, Maunendra Sankar Desarkar, Yoshinobu Kano, Kumari Deepshikha&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.01597&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kaushal0494/ZmBART&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs&lt;/strong&gt; &lt;em&gt;Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang Xian-Ling Mao, Heyan Huang, Furu Wei&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.08692&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluating the Efficacy of Summarization Evaluation across Languages&lt;/strong&gt; &lt;em&gt;Fajri Koto, Jey Han Lau, Timothy Baldwin&lt;/em&gt; &lt;code&gt;Findings of ACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.01478&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Lingual Abstractive Summarization with Limited Parallel Resources&lt;/strong&gt; &lt;em&gt;Yu Bai, Yang Gao, Heyan Huang&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.538/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/WoodenWhite/MCLAS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Approach to Multilingual User Comments Summarization&lt;/strong&gt; &lt;em&gt;Aleš Žagar, Marko Robnik-Šikonja&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.hackashop-1.13/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/12wUDg64k4oK24rNSd4DRZL9xywNMiPil?usp=sharing&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MultiHumES: Multilingual Humanitarian Dataset for Extractive Summarization&lt;/strong&gt; &lt;em&gt;Jenny Paola Yela-Bello, Ewan Oglethorpe, Navid Rekabsaz&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.146/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://deephelp.zendesk.com/hc/en-us/sections/360011925552-MultiHumES&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-lingual Approach to Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Aleš Žagar, Marko Robnik-Šikonja&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.04307&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mixed-Lingual Pre-training for Cross-lingual Summarization&lt;/strong&gt; &lt;em&gt;Ruochen Xu, Chenguang Zhu, Yu Shi, Michael Zeng, Xuedong Huang&lt;/em&gt; &lt;code&gt;AACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.08892&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Task Learning for Cross-Lingual Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Sho Takase, Naoaki Okazaki&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.07503&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen McKeown&lt;/em&gt; &lt;code&gt;Findings of EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.03093&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/esdurmus/Wikilingua&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with Bilingual Semantic Similarity Rewards&lt;/strong&gt; &lt;em&gt;Zi-Yi Dou, Sachin Kumar, Yulia Tsvetkov&lt;/em&gt; &lt;code&gt;ACL20 workshop&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.ngt-1.7/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zdou0830/crosslingual_summarization_semantic&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Yue Cao, Hui Liu, Xiaojun Wan&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.554/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Junnan Zhu, Yu Zhou, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.121/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ZNLP/ATSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MultiSumm: Towards a Unified Model for Multi-Lingual Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yue Cao, Xiaojun Wan, Jinge Yao, Dian Yu&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://aaai.org/ojs/index.php/AAAI/article/view/5328&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ycao1996/Multi-Lingual-Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Lingual Natural Language Generation via Pre-Training&lt;/strong&gt; &lt;em&gt;Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, Heyan Huang&lt;/em&gt; &lt;code&gt;AAAI 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.10481&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/CZWin32768/XNLG&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Global Voices: Crossing Borders in Automatic News Summarization&lt;/strong&gt; &lt;em&gt;Khanh Nguyen, Hal Daumé III&lt;/em&gt; &lt;code&gt;EMNLP19 workshop &lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.00421&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://forms.gle/gpkJDT6RJWHM1Ztz9&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;NCLS: Neural Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Junnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Jiajun Zhang, Shaonan Wang, Chengqing Zong&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.00156&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;http://www.nlpr.ia.ac.cn/cip/dataset.htm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention&lt;/strong&gt; &lt;em&gt;Xiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, Weihua Luo&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1305/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/KelleyYin/Cross-lingual-Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Robust Abstractive System for Cross-Lingual Summarization&lt;/strong&gt; &lt;em&gt;Jessica Ouyang, Boya Song, Kathy McKeown&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1204/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Lingual Korean Speech-to-Text Summarization&lt;/strong&gt; &lt;em&gt;HyoJeon Yoon, Dinh Tuyen Hoang, Ngoc Thanh Nguyen, Dosam Hwang&lt;/em&gt; &lt;code&gt;ACIIDS19&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-14799-0_17&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-language document summarization via extraction and ranking of multiple summaries&lt;/strong&gt; &lt;em&gt;Xiaojun Wan, Fuli Luo, Xue Sun, Songfang Huang &amp;amp; Jin-ge Yao&lt;/em&gt; &lt;a href=&#34;https://link.springer.com/article/10.1007/s10115-018-1152-7&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-Shot Cross-Lingual Neural Headline Generation&lt;/strong&gt; &lt;em&gt;Shi-qi Shen, Yun Chen, Cheng Yang, Zhi-yuan Liu, Mao-song Sun&lt;/em&gt; &lt;code&gt;TASLP18&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1109/TASLP.2018.2842432&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Language Text Summarization using Sentence and Multi-Sentence Compression&lt;/strong&gt; &lt;em&gt;Elvys Linhares Pontes, Stéphane Huet, Juan-Manuel Torres-Moreno, Andréa Carneiro Linhares&lt;/em&gt; &lt;code&gt;NLDB18&lt;/code&gt; &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01779465/document&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Cross-Language Summarization via Translation Model Enhanced Predicate Argument Structure Fusing&lt;/strong&gt; &lt;em&gt;Jiajun Zhang, Yu Zhou, Chengqing Zong&lt;/em&gt; &lt;code&gt;TASLP16&lt;/code&gt; &lt;a href=&#34;http://www.nlpr.ia.ac.cn/cip/ZhangPublications/zhang-taslp-2016.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Phrase-based Compressive Cross-Language Summarization&lt;/strong&gt; &lt;em&gt;Jin-ge Yao ,Xiaojun Wan ,Jianguo Xiao&lt;/em&gt; &lt;code&gt;EMNLP15&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D15-1012.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multilingual Single-Document Summarization with MUSE&lt;/strong&gt; &lt;em&gt;Marina Litvak, Mark Last&lt;/em&gt; &lt;code&gt;MultiLing13&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W13-3111/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Using bilingual information for cross-language document summarization&lt;/strong&gt; &lt;em&gt;Xiaojun Wan&lt;/em&gt; &lt;code&gt;ACL11&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P11-1155.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Graph-based Approach to Cross-language Multi-document Summarization&lt;/strong&gt; &lt;em&gt;Florian Boudin, Stéphane Huet, Juan-Manuel Torres-Moreno&lt;/em&gt; &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-02021891/file/Polibits11.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-language document summarization based on machine translation quality prediction&lt;/strong&gt; &lt;em&gt;Xiaojun Wan, Huiying Li, Jianguo Xiao&lt;/em&gt; &lt;code&gt;ACL10&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P10-1094/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation of a Cross-lingual Romanian-English Multi-document Summariser&lt;/strong&gt; &lt;em&gt;Constantin Orasan, Oana Andreea Chiorean&lt;/em&gt; &lt;code&gt;LREC08&lt;/code&gt; &lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2008/pdf/539_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-lingual C*ST*RD: English access to Hindi information&lt;/strong&gt; &lt;em&gt;Anton Leuski, Chin-Yew Lin, Liang Zhou, Ulrich Germann, Franz Josef Och, Eduard Hovy&lt;/em&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/979872.979877&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Multi-modal&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;TLDW: Extreme Multimodal Summarisation of News Videos&lt;/strong&gt; &lt;em&gt;Peggy Tang, Kun Hu, Lei Zhang, Jiebo Luo, Zhiyong Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.08481&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Multimodal summarisation with multimodal output is drawing increasing attention due to the rapid growth of multimedia data. While several methods have been proposed to summarise visual-text contents, their multimodal outputs are not succinct enough at an extreme level to address the information overload issue. To the end of extreme multimodal summarisation, we introduce a new task, eXtreme Multimodal Summarisation with Multimodal Output (XMSMO) for the scenario of TL;DW - Too Long; Didn&#39;t Watch, akin to TL;DR. XMSMO aims to summarise a video-document pair into a summary with an extremely short length, which consists of one cover frame as the visual summary and one sentence as the textual summary. We propose a novel unsupervised Hierarchical Optimal Transport Network (HOT-Net) consisting of three components: hierarchical multimodal encoders, hierarchical multimodal fusion decoders, and optimal transport solvers. Our method is trained, without using reference summaries, by optimising the visual and textual coverage from the perspectives of the distance between the semantic distributions under optimal transport plans. To facilitate the study on this task, we collect a large-scale dataset XMSMO-News by harvesting 4,891 video-document pairs. The experimental results show that our method achieves promising performance in terms of ROUGE and IoU metrics. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical3D Adapters for Long Video-to-text Summarization&lt;/strong&gt; &lt;em&gt;Pinelopi Papalampidi, Mirella Lapata&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.04829&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In this paper, we focus on video-to-text summarization and investigate how to best utilize multimodal information for summarizing long inputs (e.g., an hour-long TV show) into long outputs (e.g., a multi-sentence summary). We extend SummScreen (Chen et al., 2021), a dialogue summarization dataset consisting of transcripts of TV episodes with reference summaries, and create a multimodal variant by collecting corresponding full-length videos. We incorporate multimodal information into a pre-trained textual summarizer efficiently using adapter modules augmented with a hierarchical structure while tuning only 3.8% of model parameters. Our experiments demonstrate that multimodal information offers superior performance over more memory-heavy and fully fine-tuned textual summarization methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization&lt;/strong&gt; &lt;em&gt;Xinnian Liang, Chenhao Cui, Shuangzhi Wu, Jiali Zeng, Yufan Jiang, Zhoujun Li&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.11303&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features, then these features are fused with language representations to generate the summary with an encoder-decoder model. The cascaded way cannot capture the semantic alignments between images and paragraphs, which are crucial to a precise summary. In this paper, we propose ViL-Sum to jointly model paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment and Multi-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal encoder with two well-designed tasks, image reordering and image selection. The joint multi-modal encoder captures the interactions between modalities, where the reordering task guides the model to learn paragraph-level semantic alignment and the selection task guides the model to selected summary-related images in the final summary. Experimental results show that our proposed ViL-Sum significantly outperforms current state-of-the-art methods. In further analysis, we find that two well-designed tasks and joint multi-modal encoder can effectively guide the model to learn reasonable paragraphs-images and summary-images relations. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MHMS: Multimodal Hierarchical Multimedia Summarization&lt;/strong&gt; &lt;em&gt;Jielin Qiu, Jiacheng Zhu, Mengdi Xu, Franck Dernoncourt, Trung Bui, Zhaowen Wang, Bo Li, Ding Zhao, Hailin Jin&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.03734&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Video Summarization Based on Video-text Representation&lt;/strong&gt; &lt;em&gt;Li Haopeng, Ke Qiuhong, Gong Mingming, Zhang Rui&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.02494&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation&lt;/strong&gt; &lt;em&gt;Zhengkun Zhang, Xiaojun Meng, Yasheng Wang, Xin Jiang, Qun Liu, Zhenglu Yang&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.05812&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization&lt;/strong&gt; &lt;em&gt;Litian Zhang, Xiaoming Zhang, Junshu Pan, Feiran Huang&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.12072&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/LitianD/HCSCL-MSDataset&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention-based Multi-hypothesis Fusion for Speech Summarization&lt;/strong&gt; &lt;em&gt;Takatomo Kano, Atsunori Ogawa, Marc Delcroix, Shinji Watanabe&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.08201&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.02401&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/HLTCHKUST/VG-GPLMs&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Modal Supplementary-Complementary Summarization using Multi-Objective Optimization&lt;/strong&gt; &lt;em&gt;Anubhav Jangra, Sriparna Saha, Adam Jatowt, Mohammad Hasanuzzaman&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3404835.3462877&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised Multimodal Opinion Summarization&lt;/strong&gt; &lt;em&gt;Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, Sehee Chung&lt;/em&gt; &lt;code&gt;ACL21&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.33/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nc-ai/knowledge/tree/master/publications/MultimodalSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video Summarization&lt;/strong&gt; &lt;em&gt;Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring&lt;/em&gt; &lt;code&gt;ICMR21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.12465&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Sentence Summarization via Multimodal Selective Encoding&lt;/strong&gt; &lt;em&gt;Haoran Li, Junnan Zhu, Jiajun Zhang, Xiaodong He, Chengqing Zong&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.496/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos&lt;/strong&gt; &lt;em&gt;Nayu Liu, Xian Sun, Hongfeng Yu, Wenkai Zhang, Guangluan Xu&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.144/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention&lt;/strong&gt; &lt;em&gt;Aman Khullar, Udit Arora&lt;/em&gt; &lt;code&gt;EMNLP20 Workshop&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.08021&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amankhullar/mast&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles&lt;/strong&gt; &lt;em&gt;Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, Rui Yan&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.05406&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yingtaomj/VMSMO&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-modal Summarization for Video-containing Documents&lt;/strong&gt; &lt;em&gt;Xiyan Fu, Jun Wang, Zhenglu Yang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2009.08018&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xiyan524/MM-AVS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text-Image-Video Summary Generation Using Joint Integer Linear Programming&lt;/strong&gt; &lt;em&gt;Anubhav Jangra, Adam Jatowt, Mohammad Hasanuzzaman, Sriparna Saha&lt;/em&gt; &lt;code&gt;ECIR20&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-45442-5_24&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products&lt;/strong&gt; &lt;em&gt;Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, Bowen Zhou&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://aaai.org/ojs/index.php/AAAI/article/view/6332/6188&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hrlinlp/cepsum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional Hierarchical Attention Network for Query-Focused Video Summarization&lt;/strong&gt; &lt;em&gt;Shuwen Xiao, Zhou Zhao, Zijian Zhang, Xiaohui Yan, Min Yang&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.03740&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Summarization with Guidance of Multimodal Reference&lt;/strong&gt; &lt;em&gt;Junnan Zhu, Yu Zhou, Jiajun Zhang, Haoran Li, Chengqing Zong, Changliang Li&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://aaai.org/ojs/index.php/AAAI/article/view/6525/6381&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos&lt;/strong&gt; &lt;em&gt;Haipeng Zeng, Xinhuan Shu, Yanbang Wang, Yong Wang, Liguo Zhang, Ting-Chuen Pong, Huamin Qu&lt;/em&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/8948010&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Survey on Automatic Summarization Using Multi-Modal Summarization System for Asynchronous Collections&lt;/strong&gt; &lt;em&gt;Shilpadevi Vasant Bhagwat, Sheetal .S. Thokal&lt;/em&gt; &lt;a href=&#34;http://www.ijirset.com/upload/2019/february/4_shilpa_IEEE.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive summarization of documents with images based on multi-modal RNN&lt;/strong&gt; &lt;em&gt;Jingqiang Chen, Hai Zhuge&lt;/em&gt; &lt;a href=&#34;https://research.aston.ac.uk/en/publications/extractive-summarization-of-documents-with-images-based-on-multi-&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization&lt;/strong&gt; &lt;em&gt;Manling Li, Lingyu Zhang, Heng Ji, Richard J. Radke&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1210/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Abstractive Summarization for How2 Videos&lt;/strong&gt; &lt;em&gt;Shruti Palaskar, Jindřich Libovický, Spandana Gella, Florian Metze&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1659/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MSMO: Multimodal Summarization with Multimodal Output&lt;/strong&gt; &lt;em&gt;Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1448/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;http://www.nlpr.ia.ac.cn/cip/jjzhang.htm&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN&lt;/strong&gt; &lt;em&gt;Jingqiang Chen, Hai Zhuge&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1438/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-modal Sentence Summarization with Modality Attention and Image Filtering&lt;/strong&gt; &lt;em&gt;Haoran Li, Junnan Zhu, Tianshang Liu, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;IJCAI18&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/Proceedings/2018/0577.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Abstractive Summarization for Open-Domain Videos&lt;/strong&gt; &lt;em&gt;Jindrich Libovický, Shruti Palaskar, Spandana Gella, Florian Metze&lt;/em&gt; &lt;code&gt;NIPS18&lt;/code&gt; &lt;a href=&#34;https://nips2018vigil.github.io/static/papers/accepted/8.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/srvk/how2-dataset&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Read, Watch, Listen, and Summarize: Multi-Modal Summarization for Asynchronous Text, Image, Audio and Video&lt;/strong&gt; &lt;em&gt;Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/8387512&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fusing Verbal and Nonverbal Information for Extractive Meeting Summarization&lt;/strong&gt; &lt;em&gt;Fumio Nihei, Yukiko Nakano, Yukiko I. Nakano, Yutaka Takase, Yutaka Takase&lt;/em&gt; &lt;code&gt;GIFT18&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3279981.3279987&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video&lt;/strong&gt; &lt;em&gt;Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, Chengqing Zong&lt;/em&gt; &lt;code&gt;EMNLP17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D17-1114/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meeting Extracts for Discussion Summarization Based on Multimodal Nonverbal Information&lt;/strong&gt; &lt;em&gt;Fumio Nihei, Yukiko Nakano, Yukiko I. Nakano, Yutaka Takase, Yutaka Takase&lt;/em&gt; &lt;code&gt;ICMI16&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/2993148.2993160&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing a multimodal set of documents in a Smart Room&lt;/strong&gt; &lt;em&gt;Maria Fuentes, Horacio Rodríguez, Jordi Turmo&lt;/em&gt; &lt;code&gt;LREC12&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/L12-1524/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-modal summarization of key events and top players in sports tournament videos&lt;/strong&gt; &lt;em&gt;Dian Tjondronegoro, Xiaohui Tao, Johannes Sasongko and Cher Han Lau&lt;/em&gt; &lt;a href=&#34;https://eprints.qut.edu.au/43479/1/WACV_266_%281%29.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Summarization of Complex Sentences&lt;/strong&gt; &lt;em&gt;Naushad UzZaman, Jeffrey P. Bigham, James F. Allen&lt;/em&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~jbigham/pubs/pdfs/2011/multimodal_summarization.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarization of Multimodal Information&lt;/strong&gt; &lt;em&gt;Saif Ahmad, Paulo C F de Oliveira, Khurshid Ahmad&lt;/em&gt; &lt;code&gt;LREC04&lt;/code&gt; &lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2004/pdf/502.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multimodal Summarization of Meeting Recordings&lt;/strong&gt; &lt;em&gt;Berna Erol, Dar-Shyang Lee, and Jonathan Hull&lt;/em&gt; &lt;code&gt;ICME03&lt;/code&gt; &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.862.6509&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Sentiment Related&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Making the Best Use of Review Summary for Sentiment Analysis&lt;/strong&gt; &lt;em&gt;Sen Yang, Leyang Cui, Jun Xie, Yue Zhang&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.15/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/RingoS/sentiment-review-summary&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.15.bib&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Unified Dual-view Model for Review Summarization and Sentiment Classification with Inconsistency Loss&lt;/strong&gt; &lt;em&gt;Hou Pong Chan, Wang Chen, Irwin King&lt;/em&gt; &lt;code&gt;SIGIR20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2006.01592&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kenchan0226/dual_view_review_sum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification&lt;/strong&gt; &lt;em&gt;Shuming Ma, Xu Sun, Junyang Lin, Xuancheng Ren&lt;/em&gt; &lt;code&gt;IJCAI18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1805.01089&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Two-level Text Summarization from Online News Sources with Sentiment Analysis&lt;/strong&gt; &lt;em&gt;Tarun B. Mirani, Sreela Sasi&lt;/em&gt; &lt;code&gt;IEEE17&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/8076735&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Creating Video Summarization From Emotion Perspective&lt;/strong&gt; &lt;em&gt;Yijie Lan, Shikui Wei, Ruoyu Liu, Yao Zhao&lt;/em&gt; &lt;code&gt;ICSP16&lt;/code&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/document/7878001/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Pre-trained Language Model Based&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, Xuedong Huang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.09770&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model&#39;s performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MVP: Multi-task Supervised Pre-training for Natural Language Generation&lt;/strong&gt; &lt;em&gt;Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.12131&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/RUCAIBox/MVP&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Pre-trained language models (PLMs) have achieved notable success in natural language generation (NLG) tasks. Up to now, most of the PLMs are pre-trained in an unsupervised manner using large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with less labeled data showcase superior performance compared to unsupervised models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. For pre-training the text generation model MVP, we collect a labeled pre-training corpus from 45 datasets over seven generation tasks. For each task, we further pre-train specific soft prompts to stimulate the model capacity in performing a specific task. Extensive experiments have demonstrated the effectiveness of our supervised pre-training in a number of NLG tasks, and our general methods achieve state-of-the-art performance on 12 of 17 datasets. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation&lt;/strong&gt; &lt;em&gt;Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.14912&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Does Pretraining for Summarization Require Knowledge Transfer?&lt;/strong&gt; &lt;em&gt;Kundan Krishna, Jeffrey Bigham, Zachary C. Lipton&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.273/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/acmi-lab/pretraining-with-nonsense&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Alireza Salemi, Emad Kebriaei, Ghazal Neisi Minaei, Azadeh Shakery&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.741/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/alirezasalemi7/ARMAN&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leveraging Lead Bias for Zero-shot Abstractive News Summarization&lt;/strong&gt; &lt;em&gt;Chenguang Zhu, Ziyi Yang, Robert Gmyr, Michael Zeng, Xuedong Huang&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3404835.3462846&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation&lt;/strong&gt; &lt;em&gt;Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, Haifeng Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.02137&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining&lt;/strong&gt; &lt;em&gt;Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, Ming Zhou, Nan Duan&lt;/em&gt; &lt;code&gt;ICML 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.15525&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/BANG&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT&lt;/strong&gt; &lt;em&gt;Ruifeng Yuan, Zili Wang, Wenjie Li&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.09739&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Ruifeng-paper/FactExsum-coling2020&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Zero-Shot Conditional Summarization with Adaptive Multi-Task Fine-Tuning&lt;/strong&gt; &lt;em&gt;Travis Goodwin, Max Savery, Dina Demner-Fushman&lt;/em&gt; &lt;code&gt;Findings of EMNLP&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.289/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/h4ste/mtft_zsl&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation&lt;/strong&gt; &lt;em&gt;Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.12836&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-trained Summarization Distillation&lt;/strong&gt; &lt;em&gt;Sam Shleifer, Alexander M. Rush&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.13002&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-training for Abstractive Document Summarization by Reinstating Source Text&lt;/strong&gt; &lt;em&gt;Yanyan Zou, Xingxing Zhang, Wei Lu, Furu Wei, Ming Zhou&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.01853v3&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zoezou2015/abs_pretraining&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PALM: Pre-training an Autoencoding&amp;amp;Autoregressive Language Model for Context-conditioned Generation&lt;/strong&gt; &lt;em&gt;Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.07159&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising&lt;/strong&gt; &lt;em&gt;Ziyi Yang Chenguang Zhu Robert Gmyr Michael Zeng Xuedong Huang Eric Darve&lt;/em&gt; &lt;code&gt;Findings of EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2001.00725&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;QURIOUS: Question Generation Pretraining for Text Generation&lt;/strong&gt; &lt;em&gt;Shashi Narayan, Gonçalo Simoes, Ji Ma, Hannah Craighead, Ryan Mcdonald&lt;/em&gt; &lt;code&gt;ACL20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.11026&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu&lt;/em&gt; &lt;code&gt;ICML20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1912.08777&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/pegasus&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Text Summarization based on Language Model Conditioning and Locality Modeling&lt;/strong&gt; &lt;em&gt;Dmitrii Aksenov, Julián Moreno-Schneider, Peter Bourgonje, Robert Schwarzenberg, Leonhard Hennig, Georg Rehm&lt;/em&gt; &lt;code&gt;LREC20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2003.13027&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models&lt;/strong&gt; &lt;em&gt;Dmitrii Aksenov, Julián Moreno-Schneider, Peter Bourgonje, Robert Schwarzenberg, Leonhard Hennig, Georg Rehm&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2003.13028&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning by Semantic Similarity Makes Abstractive Summarization Better&lt;/strong&gt; &lt;em&gt;Wonjin Yoon, Yoon Sun Yeo, Minbyul Jeong, Bong-Jun Yi, Jaewoo Kang&lt;/em&gt; &lt;code&gt;ICML20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.07767&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/icml-2020-nlp/semsim&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization with Pretrained Encoders&lt;/strong&gt; &lt;em&gt;Yang Liu, Mirella Lapata&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1908.08345&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nlpyang/PreSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization&lt;/strong&gt; &lt;em&gt;Xingxing Zhang, Furu Wei, Ming Zhou&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1499/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MASS: Masked Sequence to Sequence Pre-training for Language Generation&lt;/strong&gt; &lt;em&gt;Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu&lt;/em&gt; &lt;code&gt;ICML19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.02450&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/MASS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pretraining-Based Natural Language Generation for Text Summarization&lt;/strong&gt; &lt;em&gt;Haoyu Zhang, Jianjun Xu, Ji Wang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1902.09243&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fine-tune BERT for Extractive Summarization&lt;/strong&gt; &lt;em&gt;Yang Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1903.10318&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nlpyang/BertSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/strong&gt; &lt;em&gt;Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon&lt;/em&gt; &lt;code&gt;NIPS19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.03197&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised Learning for Contextualized Extractive Summarization&lt;/strong&gt; &lt;em&gt;Hong Wang, Xin Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Shiyu Chang, William Yang Wang&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.04466&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hongwang600/Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient Adaptation of Pretrained Transformers for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, Yejin Choi&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.00138&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Andrew03/transformer-abstractive-summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Controllable&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;SentBS: Sentence-level Beam Search for Controllable Summarization&lt;/strong&gt; &lt;em&gt;Chenhui Shen, Liying Cheng, Lidong Bing, Yang You, Luo Si&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.14502&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Shen-Chenhui/SentBS&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; A wide range of control perspectives have been explored in controllable text generation. Structure-controlled summarization is recently proposed as a useful and interesting research direction. However, current structure-controlling methods have limited effectiveness in enforcing the desired structure. To address this limitation, we propose a sentence-level beam search generation method (SentBS), where evaluation is conducted throughout the generation process to select suitable sentences for subsequent generations. We experiment with different combinations of decoding methods to be used as subcomponents by SentBS and evaluate results on the structure-controlled dataset MReD. Experiments show that all explored combinations for SentBS can improve the agreement between the generated text and the desired structure, with the best method significantly reducing the structural discrepancies suffered by the existing model, by approximately 68%. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Readability Controllable Biomedical Document Summarization&lt;/strong&gt; &lt;em&gt;Readability Controllable Biomedical Document Summarization&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.04705&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers&#39; domain knowledge. However, existing biomedical document summarization systems have paid little attention to readability control, leaving users with summaries that are incompatible with their levels of expertise. In recognition of this urgent demand, we introduce a new task of readability controllable summarization for biomedical documents, which aims to recognise users&#39; readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen. To establish this task, we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors, and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques. Moreover, we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries. Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EDU-level Extractive Summarization with Varying Summary Lengths&lt;/strong&gt; &lt;em&gt;Yuping Wu, Ching-Hsun Tseng, Jiayu Shang, Shengzhong Mao, Goran Nenadic, Xiao-Jun Zeng&lt;/em&gt; `` &lt;a href=&#34;https://arxiv.org/abs/2210.04029&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Extractive models usually formulate text summarization as extracting top-k important sentences from document as summary. Few work exploited extracting finer-grained Elementary Discourse Unit (EDU) and there is little analysis and justification for the extractive unit selection. To fill such a gap, this paper firstly conducts oracle analysis to compare the upper bound of performance for models based on EDUs and sentences. The analysis provides evidences from both theoretical and experimental perspectives to justify that EDUs make more concise and precise summary than sentences without losing salient information. Then, considering this merit of EDUs, this paper further proposes EDU-level extractive model with Varying summary Lengths (EDU-VL) and develops the corresponding learning algorithm. EDU-VL learns to encode and predict probabilities of EDUs in document, and encode EDU-level candidate summaries with different lengths based on various k values and select the best candidate summary in an end-to-end training manner. Finally, the proposed and developed approach is experimented on single and multi-document benchmark datasets and shows the improved performances in comparison with the state-of-the-art models. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic-Aware Evaluation and Transformer Methods for Topic-Controllable Summarization&lt;/strong&gt; &lt;em&gt;Tatiana Passali, Grigorios Tsoumakas&lt;/em&gt; `` &lt;a href=&#34;https://arxiv.org/abs/2206.04317&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Topic-controllable summarization is an emerging research area with a wide range of potential applications. However, existing approaches suffer from significant limitations. First, there is currently no established evaluation metric for this task. Furthermore, existing methods built upon recurrent architectures, which can significantly limit their performance compared to more recent Transformer-based architectures, while they also require modifications to the model&#39;s architecture for controlling the topic. In this work, we propose a new topic-oriented evaluation measure to automatically evaluate the generated summaries based on the topic affinity between the generated summary and the desired topic. We also conducted a user study that validates the reliability of this measure. Finally, we propose simple, yet powerful methods for topic-controllable summarization either incorporating topic embeddings into the model&#39;s architecture or employing control tokens to guide the summary generation. Experimental results show that control tokens can achieve better performance compared to more complicated embedding-based approaches while being at the same time significantly faster. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Length Control in Abstractive Summarization by Pretraining Information Selection&lt;/strong&gt; &lt;em&gt;Yizhu Liu, Qi Jia, Kenny Zhu&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.474/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yizhuliu/lengthcontrol&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length. They also tend to generate summaries as long as those in the training data. In this paper, we propose a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on the desired length. Our approach works by training LAAM on a summary length balanced dataset built from the original training data, and then fine-tuning as usual. Results show that this approach is effective in generating high-quality summaries with desired lengths and even those short lengths never seen in the original training set.&#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Puyuan Liu, Xiang Zhang, Lili Mou&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.14522&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/MANGA-UOFA/NACC&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EntSUM: A Data Set for Entity-Centric Summarization&lt;/strong&gt; &lt;em&gt;Mounica Maddela, Mayank Kulkarni, Daniel Preotiuc-Pietro&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.02213&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/bloomberg/entsum&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/record/6359875&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reinforced Abstractive Summarization with Adaptive Length Controlling&lt;/strong&gt; &lt;em&gt;Mingyang Song, Yi Feng, Liping Jing&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.07534&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models&lt;/strong&gt; &lt;em&gt;Tanya Goyal, Nazneen Fatema Rajani, Wenhao Liu, Wojciech Kryściński&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.04400&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Chenxin An, Ming Zhong, Zhichao Geng, Jianqiang Yang, Xipeng Qiu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.07943&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aspect-Controllable Opinion Summarization&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Stefanos Angelidis, Mirella Lapata&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.03171&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rktamplayo/AceSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extract, Denoise, and Enforce: Evaluating and Predicting Lexical Constraints for Conditional Text Generation&lt;/strong&gt; &lt;em&gt;Yuning Mao, Wenchang Ma, Deren Lei, Xiang Ren&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.08724&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/morningmoni/LCGen-eval&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Planning with Learned Entity Prompts for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simoes, Ryan McDonald&lt;/em&gt; &lt;code&gt;TACL&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.07606&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GSum: A General Framework for Guided Neural Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.08014&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/neulab/guided_summarization&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-keywords-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-sentence-red&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-triples-orange&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-summaries-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive summarization with combination of pre-trained sequence-to-sequence and saliency models&lt;/strong&gt; &lt;em&gt;Itsumi Saito, Kyosuke Nishida, Kosuke Nishida, Junji Tomita&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2003.13028&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-keywords-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-sentence-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised and Controlled Multi-Document Opinion Summarization&lt;/strong&gt; &lt;em&gt;Hady Elsahar, Maximin Coavoux, Jos Rozen, Matthias Gallé&lt;/em&gt; &lt;code&gt;EACL 2021&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.141/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controllable Summarization with Constrained Markov Decision Process&lt;/strong&gt; &lt;em&gt;Hou Pong Chan, Lu Wang, Irwin King&lt;/em&gt; &lt;code&gt;TACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.03405&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kenchan0226/control-sum-cmdp&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LenAtten: An Effective Length Controlling Unit For Text Summarization&lt;/strong&gt; &lt;em&gt;Zhongyi Yu, Zhenghao Wu, Hao Zheng, Zhe XuanYuan, Jefferson Fong, Weifeng Su&lt;/em&gt; &lt;code&gt; Findings of ACL 2021 (short)&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.00316&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/X-AISIG/LenAtten&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controllable Abstractive Dialogue Summarization with Sketch Supervision&lt;/strong&gt; &lt;em&gt;Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, Caiming Xiong&lt;/em&gt; &lt;code&gt;ACL-Findings 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.14064&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/ConvSumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancing Factual Consistency of Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, Meng Jiang&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2003.08612&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inference Time Style Control for Summarization&lt;/strong&gt; &lt;em&gt;Shuyang Cao, Lu Wang&lt;/em&gt; &lt;code&gt;NAACL21 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.01724&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://shuyangcao.github.io/projects/inference_style_control/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CTRLsum: Towards Generic Controllable Text Summarization&lt;/strong&gt; &lt;em&gt;Junxian He, Wojciech Kryściński, Bryan McCann, Nazneen Rajani, Caiming Xiong&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.04281&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/ctrl-sum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Constrained Abstractive Summarization: Preserving Factual Consistency with Constrained Generation&lt;/strong&gt; &lt;em&gt;Yuning Mao, Xiang Ren, Heng Ji, Jiawei Han&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.12723&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-improve-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Keywords-Guided Abstractive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Haoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong, Xiaodong He&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6333&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-keywords-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SemSUM: Semantic Dependency Guided Neural Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Hanqi Jin, Tianming Wang, Xiaojun Wan&lt;/em&gt; &lt;code&gt;AAAI2020&lt;/code&gt; &lt;a href=&#34;https://ojs.aaai.org//index.php/AAAI/article/view/6312&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhongxia96/SemSUM&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-triples-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interpretable Multi-Headed Attention for Abstractive Summarization at Controllable Lengths&lt;/strong&gt; &lt;em&gt;Ritesh Sarkhel, Moniba Keymanesh, Arnab Nandi, Srinivasan Parthasarathy&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.606/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controllable Abstractive Sentence Summarization with Guiding Entities&lt;/strong&gt; &lt;em&gt;Changmeng Zheng, Yi Cai, Guanjie Zhang, Qing Li&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.497/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/thecharm/Abs-LRModel&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-keywords-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach&lt;/strong&gt; &lt;em&gt;Bowen Tan, Lianhui Qin, Eric P. Xing, Zhiting Hu&lt;/em&gt; &lt;code&gt;EMNLP20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.06792&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tanyuqian/aspect-based-summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Length-controllable Abstractive Summarization by Guiding with Summary Prototype&lt;/strong&gt; &lt;em&gt;Itsumi Saito, Kyosuke Nishida, Kosuke Nishida, Atsushi Otsuka, Hisako Asano, Junji Tomita, Hiroyuki Shindo, Yuji Matsumoto&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2001.07331&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Summary Loop: Learning to Write Abstractive Summaries Without Examples&lt;/strong&gt; &lt;em&gt;Philippe Laban, Andrew Hsi, John Canny, Marti A. Hearst&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.460/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hooks in the Headline: Learning to Generate Headlines with Controlled Styles&lt;/strong&gt; &lt;em&gt;Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.01980&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jind11/TitleStylist&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Kai Wang, Xiaojun Quan, Rui Wang&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1207/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/InitialBug/BiSET&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-summaries-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Abstractive Document Summarization with Salient Information Modeling&lt;/strong&gt; &lt;em&gt;Yongjian You, Weijia Jia, Tianyi Liu, Wenmian Yang&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1205/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/StevenWD/ETADS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Positional Encoding to Control Output Sequence Length&lt;/strong&gt; &lt;em&gt;Sho Takase, Naoaki Okazaki&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1401/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/takase/control-length&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query Focused Abstractive Summarization: Incorporating Query Relevance, Multi-Document Coverage, and Summary Length Constraints into seq2seq Models&lt;/strong&gt; &lt;em&gt;Tal Baumel, Matan Eyal, Michael Elhadad&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1801.07704&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network&lt;/strong&gt; &lt;em&gt;Chenliang Li, Weiran Xu, Si Li, Sheng Gao&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N18-2009/&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-keywords-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controllable Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Angela Fan, David Grangier, Michael Auli&lt;/em&gt; &lt;code&gt;ACL2018 Workshop&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1711.05217&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization&lt;/strong&gt; &lt;em&gt;Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P18-1015/&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-summaries-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controlling Length in Abstractive Summarization Using a Convolutional Neural Network&lt;/strong&gt; &lt;em&gt;Yizhu Liu, Zhiyi Luo, Kenny Zhu&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1444/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;http://202.120.38.146/sumlen&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Wikipedia By Summarizing Long Sequence&lt;/strong&gt; &lt;em&gt;Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam Shazeer&lt;/em&gt; &lt;code&gt;ICLR18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1801.10198&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/memory-compressed-attention.git&#34;&gt;[code]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-sentence-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controlling Output Length in Neural Encoder-Decoders&lt;/strong&gt; &lt;em&gt;Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, Manabu Okumura&lt;/em&gt; &lt;code&gt;EMNLP16&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D16-1140/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kiyukuta/lencon&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Abstractive&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving abstractive summarization with energy-based re-ranking&lt;/strong&gt; &lt;em&gt;Diogo Pernes, Afonso Mendes, André F.T. Martins&lt;/em&gt; &lt;code&gt;GEM at EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.15553&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Priberam/SummEBR&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Current abstractive summarization systems present important weaknesses which prevent their deployment in real-world applications, such as the omission of relevant information and the generation of factual inconsistencies (also known as hallucinations). At the same time, automatic evaluation metrics such as CTC scores have been recently proposed that exhibit a higher correlation with human judgments than traditional lexical-overlap metrics such as ROUGE. In this work, we intend to close the loop by leveraging the recent advances in summarization metrics to create quality-aware abstractive summarizers. Namely, we propose an energy-based model that learns to re-rank summaries according to one or a combination of these metrics. We experiment using several metrics to train our energy-based re-ranker and show that it consistently improves the scores achieved by the predicted summaries. Nonetheless, human evaluation results show that the re-ranking approach should be used with care for highly abstractive summaries, as the available metrics are not yet sufficiently reliable for this purpose. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Salience Allocation as Guidance for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Fei Wang, Kaiqiang Song, Hongming Zhang, Lifeng Jin, Sangwoo Cho, Wenlin Yao, Xiaoyang Wang, Muhao Chen, Dong Yu&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.12330&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tencent-ailab/season&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization models typically learn to capture the salient information from scratch implicitly. Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. Furthermore, it cannot easily adapt to documents with various abstractiveness. As the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Summary Candidates Fusion&lt;/strong&gt; &lt;em&gt;Mathieu Ravaut, Shafiq Joty, Nancy F. Chen&lt;/em&gt; &lt;code&gt;EMNLP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.08779&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ntunlp/SummaFusion/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam. Recently, re-ranking methods have been proposed, to learn to select a better summary candidate. However, such methods are limited by the summary quality aspects captured by the first-stage candidates. To bypass this limitation, we propose a new paradigm in second-stage abstractive summarization called SummaFusion that fuses several summary candidates to produce a novel abstractive second-stage summary. Our method works well on several summarization datasets, improving both the ROUGE scores and qualitative properties of fused summaries. It is especially good when the candidates to fuse are worse, such as in the few-shot setup where we set a new state-of-the-art. We will make our code and checkpoints available at this https URL. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generation of Patient After-Visit Summaries to Support Physicians&lt;/strong&gt; &lt;em&gt;Pengshan Cai, Fei Liu, Adarsha Bajracharya, Joe Sills, Alok Kapoor, Weisong Liu, Dan Berlowitz, David Levy, Richeek Pradhan, Hong Yu&lt;/em&gt; `` &lt;a href=&#34;https://aclanthology.org/2022.coling-1.544/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/pengshancai/AVS_gen&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients’ disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic Overlap Summarization among Multiple Alternative Narratives: An Exploratory Study&lt;/strong&gt; &lt;em&gt;Naman Bansal, Mousumi Akter, Shubhra Kanti Karmaker&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.541/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://karmake2.github.io/publications/&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In this paper, we introduce an important yet relatively unexplored NLP task called Semantic Overlap Summarization (SOS), which entails generating a single summary from multiple alternative narratives which can convey the common information provided by those narratives. As no benchmark dataset is readily available for this task, we created one by collecting 2,925 alternative narrative pairs from the web and then, went through the tedious process of manually creating 411 different reference summaries by engaging human annotators. As a way to evaluate this novel task, we first conducted a systematic study by borrowing the popular ROUGE metric from text-summarization literature and discovered that ROUGE is not suitable for our task. Subsequently, we conducted further human annotations to create 200 document-level and 1,518 sentence-level ground-truth overlap labels. Our experiments show that the sentence-wise annotation technique with three overlap labels, i.e., Absent (A), Partially-Present (PP), and Present (P), yields a higher correlation with human judgment and higher inter-rater agreement compared to the ROUGE metric. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ArgLegalSumm: Improving Abstractive Summarization of Legal Documents with Argument Mining&lt;/strong&gt; &lt;em&gt;Mohamed Elaraby, Diane Litman&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.540/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/EngSalem/arglegalsumm&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Source-summary Entity Aggregation in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;José Ángel González, Annie Louis, Jackie Chi Kit Cheung&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.526/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In a text, entities mentioned earlier can be referred to in later discourse by a more general description. For example, Celine Dion and Justin Bieber can be referred to by Canadian singers or celebrities. In this work, we study this phenomenon in the context of summarization, where entities from a source text are generalized in the summary. We call such instances source-summary entity aggregations. We categorize these aggregations into two types and analyze them in the Cnn/Dailymail corpus, showing that they are reasonably frequent. We then examine how well three state-of-the-art summarization systems can generate such aggregations within summaries. We also develop techniques to encourage them to generate more aggregations. Our results show that there is significant room for improvement in producing semantically correct aggregations. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models&lt;/strong&gt; &lt;em&gt;Yanjun Gao, Dmitry Dligach, Timothy Miller, Dongfang Xu, Matthew M. Churpek, Majid Afshar&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.264/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatically summarizing patients&#39; main problems from daily progress notes using natural language processing methods helps to battle against information and cognitive overload in hospital settings and potentially assists providers with computerized diagnostic decision support. Problem list summarization requires a model to understand, abstract, and generate clinical documentation. In this work, we propose a new NLP task that aims to generate a list of problems in a patient&#39;s daily care plan using input from the provider&#39;s progress notes during hospitalization. We investigate the performance of T5 and BART, two state-of-the-art seq2seq transformer architectures, in solving this problem. We provide a corpus built on top of progress notes from publicly available electronic health record progress notes in the Medical Information Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain text, and we experiment with a data augmentation method and a domain adaptation pre-training method to increase exposure to medical vocabulary and knowledge. Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence embedding, and F-score on medical concepts. Results show that T5 with domain adaptive pre-training achieves significant performance gains compared to a rule-based system and general domain pre-trained language models, indicating a promising direction for tackling the problem summarization task. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic-Preserving Abstractive Text Summarization with Siamese Generative Adversarial Net&lt;/strong&gt; &lt;em&gt;Xin Sheng, Linli Xu, Yinlong Xu, Deqiang Jiang, Bo Ren&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.163/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We propose a novel siamese generative adversarial net for abstractive text summarization (SSPGAN), which can preserve the main semantics of the source text. Different from previous generative adversarial net based methods, SSPGAN is equipped with a siamese semantic-preserving discriminator, which can not only be trained to discriminate the machine-generated summaries from the human-summarized ones, but also ensure the semantic consistency between the source text and target summary. As a consequence of the min-max game between the generator and the siamese semantic-preserving discriminator, the generator can generate a summary that conveys the key content of the source text more accurately. Extensive experiments on several text summarization benchmarks in different languages demonstrate that the proposed model can achieve significant improvements over the state-of-the-art methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ExtraPhrase: Efficient Data Augmentation for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Mengsay Loem, Sho Takase, Masahiro Kaneko, Naoaki Okazaki&lt;/em&gt; &lt;code&gt;NAACL 2022 Student Research Workshop&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-srw.3/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/loem-ms/ExtraPhrase&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; TNeural models trained with large amount of parallel data have achieved impressive performance in abstractive summarization tasks. However, large-scale parallel corpora are expensive and challenging to construct. In this work, we introduce a low-cost and effective strategy, ExtraPhrase, to augment training data for abstractive summarization tasks. ExtraPhrase constructs pseudo training data in two steps: extractive summarization and paraphrasing. We extract major parts of an input text in the extractive summarization step and obtain its diverse expressions with the paraphrasing step. Through experiments, we show that ExtraPhrase improves the performance of abstractive summarization tasks by more than 0.50 points in ROUGE scores compared to the setting without data augmentation. ExtraPhrase also outperforms existing methods such as back-translation and self-training. We also show that ExtraPhrase is significantly effective when the amount of genuine training data is remarkably small, i.e., a low-resource setting. Moreover, ExtraPhrase is more cost-efficient than the existing approaches &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BRIO: Bringing Order to Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yixin Liu, Pengfei Liu, Dragomir Radev, Graham Neubig&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.207/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yixinL7/BRIO&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Mathieu Ravaut, Shafiq Joty, Nancy F. Chen&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.309/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ntunlp/SummaReranker&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN- DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at &#xA;   &lt;a href=&#34;https://github.com/ntunlp/SummaReranker&#34;&gt;https://github.com/ntunlp/SummaReranker&lt;/a&gt;. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adaptive Beam Search to Enhance On-device Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Harichandana B S S, Sumit Kumar&lt;/em&gt; &lt;code&gt;IEEE INDICON 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.02739&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PLSUM: Generating PT-BR Wikipedia by Summarizing Multiple Websites&lt;/strong&gt; &lt;em&gt;André Seidel Oliveira, Anna Helena Reali Costa&lt;/em&gt; &lt;code&gt;ENIAC 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.01591&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pointer over Attention: An Improved Bangla Text Summarization Approach Using Hybrid Pointer Generator Network&lt;/strong&gt; &lt;em&gt;Nobel Dhar, Gaurob Saha, Prithwiraj Bhattacharjee, Avi Mallick, Md Saiful Islam&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.10269&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Template-aware Attention Model for Earnings Call Report Generation&lt;/strong&gt; &lt;em&gt;Yangchen Huang, Prashant K. Dhingra, Seyed Danial Mohseni Taheri&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.2/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Khalil Mrini, Can Liu, Markus Dreyer&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.4/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge and Keywords Augmented Abstractive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Shuo Guan, Ping Zhu, Zhihua Wei&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.3.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/SeanG-325/KAS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sentence-level Planning for Especially Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Andreas Marfurt, James Henderson&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.1/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/idiap/sentence-planner&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learn to Copy from the Copying History: Correlational Copy Network for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Haoran Li, Song Xu, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Bowen Zhou&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.336/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hrlinlp/coconet&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhance Long Text Understanding via Distilled Gist Detector from Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yan Liu, Yazheng Yang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.04741&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VieSum: How Robust Are Transformer-based Models on Vietnamese Summarization?&lt;/strong&gt; &lt;em&gt;Hieu Nguyen, Long Phan, James Anibal, Alec Peltekian, Hieu Tran&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.04257&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enriching and Controlling Global Semantics for Text Summarization&lt;/strong&gt; &lt;em&gt;Thong Nguyen, Anh Tuan Luu, Truc Lu, Tho Quan&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.10616&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Augmented Abstractive Summarization With Document-LevelSemantic Graph&lt;/strong&gt; &lt;em&gt;Qiwei Bi, Haoyuan Li, Kun Lu, Hanfang Yang&lt;/em&gt; &lt;code&gt;Journal of Data Science&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.06046&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Alireza Salemi, Emad Kebriaei, Ghazal Neisi Minaei, Azadeh Shakery&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.04098&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mohammadiahmad/persian-dataset&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Subjective Bias in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Lei Li, Wei Liu, Marina Litvak, Natalia Vanetik, Jiacheng Pei, Yinan Liu, Siya Qi&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.10084&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/thinkwee/SubjectiveBiasABS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Abstractive Unsupervised Summarization of Online News Discussions&lt;/strong&gt; &lt;em&gt;Ignacio Tampe Palma, Marcelo Mendoza, Evangelos Milios&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.03953&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention Temperature Matters in Abstractive Summarization Distillation&lt;/strong&gt; &lt;em&gt;Shengqiang Zhang, Xingxing Zhang, Hangbo Bao, Furu Wei&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.11/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Shengqiang-Zhang/plate&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive. This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss. Pseudo-labeling based methods are popular in sequence-to-sequence model distillation. In this paper, we find simply manipulating attention temperatures in Transformers can make pseudo labels easier to learn for student models. Our experiments on three summarization datasets show our proposed method consistently improves vanilla pseudo-labeling based methods. Further empirical analysis shows that both pseudo labels and summaries produced by our students are shorter and more abstractive. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BASS: Boosting Abstractive Summarization with Unified Semantic Graph&lt;/strong&gt; &lt;em&gt;Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, Haifeng Wang&lt;/em&gt; &lt;code&gt;ACL21&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.472/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, Jianfeng Gao&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.naacl-main.381/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jiangycTarheel/TPT-Summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Uncertainty-Aware Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Alexios Gidiotis, Grigorios Tsoumakas&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.10155&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What&#39;s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization&lt;/strong&gt; &lt;em&gt;Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, Noémie Elhadad&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.00816&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating abstractive summaries of Lithuanian news articles using a transformer model&lt;/strong&gt; &lt;em&gt;Lukas Stankevičius, Mantas Lukoševičius&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.03279&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarization, Simplification, and Generation: The Case of Patents&lt;/strong&gt; &lt;em&gt;Silvia Casola, Alberto Lavelli&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.14860&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quantifying Appropriateness of Summarization Data for Curriculum Learning&lt;/strong&gt; &lt;em&gt;Ryuji Kano, Takumi Takahashi, Toru Nishino, Motoki Taniguchi, Tomoki Taniguchi, Tomoko Ohkuma&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.119/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization of Czech News Articles Using Named Entities&lt;/strong&gt; &lt;em&gt;Petr Marek, Štěpán Müller, Jakub Konrád, Petr Lorenc, Jan Pichl, Jan Šedivý&lt;/em&gt; &lt;code&gt;Journal&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.10454&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Planning with Entity Chains for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simoes, Ryan McDonald&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.07606&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention Head Masking for Inference Time Content Selection in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Shuyang Cao, Lu Wang&lt;/em&gt; &lt;code&gt;NAACL21 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.02205&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://shuyangcao.github.io/projects/inference_head_masking/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A New Approach to Overgenerating and Scoring Abstractive Summaries&lt;/strong&gt; &lt;em&gt;Kaiqiang Song, Bingqing Wang, Zhe Feng, Fei Liu&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.01726&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/varying-length-summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring Explainable Selection to Control Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Wang Haonan, Gao Yang, Bai Yu, Mirella Lapata, Huang Heyan&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.11779&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Wanghn95/Esca_Code&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Friendly Topic Assistant for Transformer Based Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Zhengjue Wang, Zhibin Duan, Hao Zhang, Chaojie Wang, Long Tian, Bo Chen, Mingyuan Zhou&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.35/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/BoChenGroup/TA&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Abstractive Text Summarizer for Telugu Language&lt;/strong&gt; &lt;em&gt;Mohan Bharath B, Aravindh Gowtham B, Akhil M&lt;/em&gt; &lt;code&gt;ICSCSP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2101.07120&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic-Aware Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, Ling Fan&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.10323&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/taas-www21/taas&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-hop Inference for Question-driven Summarization&lt;/strong&gt; &lt;em&gt;Yang Deng, Wenxuan Zhang, Wai Lam&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.03738&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quantitative Argument Summarization and Beyond-Cross-Domain Key Point Analysis&lt;/strong&gt; &lt;em&gt;Roy Bar-Haim, Yoav Kantor, Lilach Eden, Roni Friedman, Dan Lahav, Noam Slonim&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.05369&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to Fuse Sentences with Transformers for Summarization&lt;/strong&gt; &lt;em&gt;Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, Fei Liu&lt;/em&gt; &lt;code&gt;EMNLP20 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.03726&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/sent-fusion-transformers&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Cascade Approach to Neural Abstractive Summarization with Content Selection and Fusion&lt;/strong&gt; &lt;em&gt;Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Walter Chang, Fei Liu&lt;/em&gt; &lt;code&gt;AACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.03722&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/cascaded-summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AutoSurvey: Automatic Survey Generation based on a Research Draft&lt;/strong&gt; &lt;em&gt;Hen-Hsen Huang&lt;/em&gt; &lt;code&gt;IJCAI20&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/Proceedings/2020/0761.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;http://www.cs.nccu.edu.tw/~hhhuang/auto_survey/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Abstractive Summarization with Structural Attention&lt;/strong&gt; &lt;em&gt;Tanya Chowdhury, Sachin Kumar, Tanmoy Chakraborty&lt;/em&gt; &lt;code&gt;IJCAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.09739&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Unified Model for Financial Event Classification, Detection and Summarization&lt;/strong&gt; &lt;em&gt;Quanzhi Li, Qiong Zhang&lt;/em&gt; &lt;code&gt;IJCAI20 Special Track on AI in FinTech&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/Proceedings/2020/644&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discriminative Adversarial Search for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano&lt;/em&gt; &lt;code&gt;ICML20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.10375&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Controlling the Amount of Verbatim Copying in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Kaiqiang Song, Bingqing Wang, Zhe Feng, Liu Ren, Fei Liu&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1911.10390&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/control-over-copying&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GRET：Global Representation Enhanced Transformer&lt;/strong&gt; &lt;em&gt;Rongxiang Weng, Haoran Wei, Shujian Huang, Heng Yu, Lidong Bing, Weihua Luo, Jiajun Chen&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2002.10101&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Summarization of Spoken and Written Instructions with BERT&lt;/strong&gt; &lt;em&gt;Alexandra Savelieva, Bryan Au-Yeung, Vasanth Ramani&lt;/em&gt; &lt;code&gt;KDD Converse 2020&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2008.09676&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concept Pointer Network for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Wang Wenbo, Gao Yang, Huang Heyan, Zhou Yuxiang&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.08486&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/wprojectsn/codes&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Co-opNet: Cooperative Generator–Discriminator Networks for Abstractive Summarization with Narrative Flow&lt;/strong&gt; &lt;em&gt;Saadia Gabriel, Antoine Bosselut, Ari Holtzman, Kyle Lo, Asli Celikyilmaz, Yejin Choi&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1907.01272&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contrastive Attention Mechanism for Abstractive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Xiangyu Duan, Hongfei Yu, Mingming Yin, Min Zhang, Weihua Luo, Yue Zhang&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-1301/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/travel-go/Abstractive-Text-Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Entity-Driven Framework for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Eva Sharma, Luyang Huang, Zhe Hu, Lu Wang&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.02059&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://evasharma.github.io/SENECA/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstract Text Summarization: A Low Resource Challenge&lt;/strong&gt; &lt;em&gt;Shantipriya Parida, Petr Motlicek&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-1616/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Attention Optimization for Abstractive Document Summarization&lt;/strong&gt; &lt;em&gt;Min Gui, Junfeng Tian, Rui Wang, Zhenglu Yang&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.11491&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scoring Sentence Singletons and Pairs for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, Fei Liu&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1209/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/summarization-sing-pair-mix&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inducing Document Structure for Aspect-based Summarization&lt;/strong&gt; &lt;em&gt;Lea Frermann, Alexandre Klementiev&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1630/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ColiLea/aspect_based_summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Summaries with Topic Templates and Structured Convolutional Decoders&lt;/strong&gt; &lt;em&gt;Laura Perez-Beltrachini, Yang Liu, Mirella Lapata&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.04687&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lauhaide/WikiCatSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summary Refinement through Denoising&lt;/strong&gt; &lt;em&gt;Nikola I. Nikolov, Alessandro Calmanovici, Richard H.R. Hahnloser&lt;/em&gt; &lt;code&gt;RANLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1907.10873&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ninikolov/summary-denoising&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Closed-Book Training to Improve Summarization Encoder Memory&lt;/strong&gt; &lt;em&gt;Yichen Jiang, Mohit Bansal&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1809.04585&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Neural Abstractive Document Summarization with Structural Regularization&lt;/strong&gt; &lt;em&gt;Wei Li, Xinyan Xiao, Yajuan Lyu, Yuanzhuo Wang&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1441/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bottom-Up Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1808.10792&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sebastianGehrmann/bottom-up-summary&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss&lt;/strong&gt; &lt;em&gt;Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, Min Sun&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P18-1013/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation&lt;/strong&gt; &lt;em&gt;Han Guo, Ramakanth Pasunuru, Mohit Bansal&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P18-1064/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Document Summarization via Bidirectional Decoder&lt;/strong&gt; &lt;em&gt;Xin WanChen LiRuijia WangDing XiaoChuan Shi&lt;/em&gt; &lt;code&gt;ADMA18&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-05090-0_31&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Entity Commonsense Representation for Neural Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Seonjae Lim, Seung-won Hwang&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N18-1064/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Get To The Point: Summarization with Pointer-Generator Networks&lt;/strong&gt; &lt;em&gt;Abigail See, Peter J. Liu, Christopher D. Manning&lt;/em&gt; &lt;code&gt;ACL17&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1704.04368&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/abisee/pointer-generator&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Selective Encoding for Abstractive Sentence Summarization&lt;/strong&gt; &lt;em&gt;Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou&lt;/em&gt; &lt;code&gt;ACL17&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1704.07073&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Document Summarization with a Graph-Based Attentional Neural Model&lt;/strong&gt; &lt;em&gt;Jiwei Tan, Xiaojun Wan, Jianguo Xiao&lt;/em&gt; &lt;code&gt;ACL17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P17-1108/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Toward Abstractive Summarization Using Semantic Representations&lt;/strong&gt; &lt;em&gt;Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, Noah A. Smith&lt;/em&gt; &lt;code&gt;NAACL15&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N15-1114/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Meeting Summarization with Entailment and Fusion&lt;/strong&gt; &lt;em&gt;Yashar Mehdad, Giuseppe Carenini, Frank Tompa, Raymond T. Ng&lt;/em&gt; &lt;code&gt;ENLG13&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/W13-2117/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Graph-Based&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Heterogeneous Graph Attention Network for Syntax-Aware Summarization&lt;/strong&gt; &lt;em&gt;Zixing Song, Irwin King&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-6812.SongZ.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarization with Graphical Elements&lt;/strong&gt; &lt;em&gt;Maartje ter Hoeve, Julia Kiseleva, Maarten de Rijke&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.07551&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/maartjeth/summarization_with_graphical_elements&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization&lt;/strong&gt; &lt;em&gt;Ye Liu, Jian-Guo Zhang, Yao Wan, Congying Xia, Lifang He, Philip S. Yu&lt;/em&gt; &lt;code&gt;EMNLP 2021 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.06388&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization&lt;/strong&gt; &lt;em&gt;Haopeng Zhang, Jiawei Zhang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.15327&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network&lt;/strong&gt; &lt;em&gt;Ruipeng Jia, Yanan Cao, Hengzhu Tang, Fang Fang, Cong Cao, Shi Wang&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.295/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/coder352/HAHSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks&lt;/strong&gt; &lt;em&gt;Peng Cui, Le Hu, Yuanchao Liu&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.06253&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks for Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.12393&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/brxx122/HeterSUMGraph&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structured Neural Summarization&lt;/strong&gt; &lt;em&gt;Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt&lt;/em&gt; &lt;code&gt;ICLR19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1811.01824&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/CoderPat/structured-neural-summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical Transformers for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Yang Liu, Mirella Lapata&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1905.13164&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nlpyang/hiersumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Diego Antognini, Boi Faltings&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.12231&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graph-based Neural Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, Dragomir Radev&lt;/em&gt; &lt;code&gt;CoNLL17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/K17-1045/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Document Summarization with a Graph-Based Attentional Neural Model&lt;/strong&gt; &lt;em&gt;Jiwei Tan, Xiaojun Wan, Jianguo Xiao&lt;/em&gt; &lt;code&gt;ACL17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P17-1108/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Unsupervised&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor&lt;/strong&gt; &lt;em&gt;Shangqing Tu, Jifan Yu, Fangwei Zhu, Juanzi Li, Lei Hou, Jian-Yun Nie&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.550/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-KEG/UPER&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm, which first extracts a relatively short meta-document, then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents. However, the trained scoring model is prone to under-fitting for low-resource settings, as it relies on the training data. To extract documents effectively, we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword’s perplexity, which can assess the document’s semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document’s salience, thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets, 2 data settings, and 2 abstractive backbone models, showing our method’s effectiveness. Our code is available at &#xA;   &lt;a href=&#34;https://github.com/THU-KEG/UPER&#34;&gt;https://github.com/THU-KEG/UPER&lt;/a&gt; &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization&lt;/strong&gt; &lt;em&gt;Puyuan Liu, Chenyang Huang, Lili Mou&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; [&lt;a href=&#34;https://aclanthology.org/2022.acl-long.545/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/manga-uofa/naus&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Text summarization aims to generate a short summary for an input text. In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach, which does not require parallel data for training. Our NAUS first performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth. Then, we train an encoder-only non-autoregressive Transformer based on the search result. We also propose a dynamic programming approach for length-control decoding, which is important for the summarization task. Experiments on two datasets show that NAUS achieves state-of-the-art performance for unsupervised summarization, yet largely improving inference efficiency. Further, our algorithm is able to perform explicit length-transfer summary generation. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Extractive Opinion Summarization Using Sparse Coding&lt;/strong&gt; &lt;em&gt;Somnath Basu Roy Chowdhury, Chao Zhao, Snigdha Chaturvedi&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.86/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/brcsomnath/SemAE&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews. We present Semantic Autoencoder (SemAE) to perform extractive opinion summarization in an unsupervised manner. SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent representation of each sentence over semantic units. Our extractive summarization algorithm leverages the representations to identify representative opinions among hundreds of reviews. SemAE is also able to perform controllable summarization to generate aspect-specific summaries using only a few samples. We report strong performance on SPACE and AMAZON datasets and perform experiments to investigate the functioning of our model. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Want To Reduce Labeling Cost? GPT-3 Can Help&lt;/strong&gt; &lt;em&gt;Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.13487&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Unsupervised Extractive Summarization with Facet-Aware Modeling&lt;/strong&gt; &lt;em&gt;Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li&lt;/em&gt; &lt;code&gt;ACL 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-acl.147/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MRCBert: A Machine Reading ComprehensionApproach for Unsupervised Summarization&lt;/strong&gt; &lt;em&gt;Saurabh Jain, Guokai Tang, Lim Sze Chi&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.00239&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/saurabhhssaurabh/reviews_summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization&lt;/strong&gt; &lt;em&gt;Haopeng Zhang, Jiawei Zhang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.15327&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Opinion Summarization with Content Planning&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Stefanos Angelidis, Mirella Lapata&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07808&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rktamplayo/PlanSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Biased TextRank: Unsupervised Graph-Based Content Extraction&lt;/strong&gt; &lt;em&gt;Ashkan Kazemi, Verónica Pérez-Rosas, Rada Mihalcea&lt;/em&gt; &lt;code&gt;COLING20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2011.01026&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://lit.eecs.umich.edu/downloads.html&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers&lt;/strong&gt; &lt;em&gt;Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei, Ming Zhou&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.08242&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/xssstory/STAS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q-learning with Language Model for Edit-based Unsupervised Summarization&lt;/strong&gt; &lt;em&gt;Ryosuke Kohita, Akifumi Wachi, Yang Zhao, Ryuki Tachibana&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.04379&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/kohilin/ealm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive Document Summarization without Parallel Data&lt;/strong&gt; &lt;em&gt;Nikola I. Nikolov, Richard H.R. Hahnloser&lt;/em&gt; &lt;code&gt;LREC20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1907.12951&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ninikolov/low_resource_summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking&lt;/strong&gt; &lt;em&gt;Masaru Isonuma, Junichiro Mori, Ichiro Sakata&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.05691&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/misonuma/strsum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sentence Centrality Revisited for Unsupervised Summarization&lt;/strong&gt; &lt;em&gt;Hao Zheng, Mirella Lapata&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1628/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mswellhao/PacSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction&lt;/strong&gt; &lt;em&gt;Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, Katja Markert&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.01791&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/raphael-sch/HC_Sentence_Summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummAE: Zero-Shot Abstractive Text Summarization using Length-Agnostic Auto-Encoders&lt;/strong&gt; &lt;em&gt;Peter J. Liu, Yu-An Chung, Jie Ren&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.00998&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/summae&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Eric Chu, Peter J. Liu&lt;/em&gt; &lt;code&gt;ICML19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1810.05739&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sosuperic/MeanSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression&lt;/strong&gt; &lt;em&gt;Christos Baziotis, Ion Androutsopoulos, Ioannis Konstas, Alexandros Potamianos&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1904.03651&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/cbaziotis/seq3&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to Encode Text as Human-Readable Summaries usingGenerative Adversarial Networks&lt;/strong&gt; &lt;em&gt;Yaushian Wang, Hung-Yi Lee&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1451/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yaushian/Unparalleled-Text-Summarization-using-GAN&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization&lt;/strong&gt; &lt;em&gt;Guokan Shang, Wensi Ding, Zekun Zhang, Antoine Tixier, Polykarpos Meladianos, Michalis Vazirgiannis, Jean-Pierre Lorré&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1805.05271&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://bitbucket.org/dascim/acl2018_abssumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Concept-map-based&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Concept Mention Grouping for Concept Map–based Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Tobias Falke, Iryna Gurevych&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1074/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/UKPLab/naacl2019-cmaps-lshcw&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bringing Structure into Summaries : Crowdsourcing a Benchmark Corpus of Concept Maps&lt;/strong&gt; &lt;em&gt;Tobias Falke, Iryna Gurevych&lt;/em&gt; &lt;code&gt;EMNLP17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D17-1320/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/UKPLab/emnlp2017-cmapsum-corpus/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Timeline&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization&lt;/strong&gt; &lt;em&gt;Hossein Rajaby Faghihi, Bashar Alhafni, Ke Zhang, Shihao Ran, Joel Tetreault, Alejandro Jaimes&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.14190&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/CrisisLTLSum/CrisisTimelines&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Social media has increasingly played a key role in emergency response: first responders can use public posts to better react to ongoing crisis events and deploy the necessary resources where they are most needed. Timeline extraction and abstractive summarization are critical technical tasks to leverage large numbers of social media posts about events. Unfortunately, there are few datasets for benchmarking technical approaches for those tasks. This paper presents CrisisLTLSum, the largest dataset of local crisis event timelines available to date. CrisisLTLSum contains 1,000 crisis event timelines across four domains: wildfires, local fires, traffic, and storms. We built CrisisLTLSum using a semi-automated cluster-then-refine approach to collect data from the public Twitter stream. Our initial experiments indicate a significant gap between the performance of strong baselines compared to the human performance on both tasks. Our dataset, code, and models are publicly available. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Joint Learning-based Heterogeneous Graph Attention Network for Timeline Summarization&lt;/strong&gt; &lt;em&gt;Jingyi You, Dongyuan Li, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.301/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/MorenoLaQuatra/SDF-TLS&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a joint learning-based heterogeneous graph attention network for TLS (HeterTls), in which date selection and event detection are combined into a unified framework to improve the extraction accuracy and remove redundant sentences simultaneously. Our heterogeneous graph involves multiple types of nodes, the representations of which are iteratively learned across the heterogeneous graph attention layer. We evaluated our model on four datasets, and found that it significantly outperformed the current state-of-the-art baselines with regard to ROUGE scores and date selection metrics. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Updated Headline Generation: Creating Updated Summaries for Evolving News Stories&lt;/strong&gt; &lt;em&gt;Sheena Panthaplackel, Adrian Benton, Mark Dredze&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.446/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/panthap2/updated-headline-generation&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We propose the task of updated headline generation, in which a system generates a headline for an updated article, considering both the previous article and headline. The system must identify the novel information in the article update, and modify the existing headline accordingly. We create data for this task using the NewsEdits corpus by automatically identifying contiguous article versions that are likely to require a substantive headline update. We find that models conditioned on the prior headline and body revisions produce headlines judged by humans to be as factual as gold headlines while making fewer unnecessary edits compared to a standard headline generation model. Our experiments establish benchmarks for this new contextual summarization task. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Abstractive summarization of hospitalisation histories with transformer networks&lt;/strong&gt; &lt;em&gt;Alexander Yalunin, Dmitriy Umerenkov, Vladimir Kokh&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.02208&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Follow the Timeline! Generating Abstractive and Extractive Timeline Summary in Chronological Order&lt;/strong&gt; &lt;em&gt;Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan&lt;/em&gt; &lt;code&gt;TOIS&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3517221&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/iriscxy/Unified-Timeline-Summarizer&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries&lt;/strong&gt; &lt;em&gt;Yi Yu, Adam Jatowt, Antoine Doucet, Kazunari Sugiyama, Masatoshi Yoshikawa&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.32/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://yiyualt.github.io/mtlsdata/&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarize Dates First: A Paradigm Shift in Timeline Summarization&lt;/strong&gt; &lt;em&gt;Moreno La Quatra, Luca Cagliero, Elena Baralis, Alberto Messina, Maurizio Montagnuolo&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3404835.3462954&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/MorenoLaQuatra/SDF-TLS&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Examining the State-of-the-Art in News Timeline Summarization&lt;/strong&gt; &lt;em&gt;Demian Gholipour Ghalandari, Georgiana Ifrim&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.10107&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/complementizer/news-tls&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning towards Abstractive Timeline Summarization&lt;/strong&gt; &lt;em&gt;Xiuying Chen, Zhangming Chan, Shen Gao, Meng-Hsuan Yu, Dongyan Zhao, Rui Yan&lt;/em&gt; &lt;code&gt;IJCAI19&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/Proceedings/2019/686&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yingtaomj/Learning-towards-Abstractive-Timeline-Summarization&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Opinion&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Opinion Summarization Using Approximate Geodesics&lt;/strong&gt; &lt;em&gt;Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.07496&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Opinion summarization is the task of creating summaries capturing popular opinions from user reviews. In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm involves an encoder-decoder based representation learning model, that generates representations of text as a distribution over latent semantic units. GeoSumm generates these representations by performing dictionary learning over pre-trained text representations at multiple decoder layers. We then use these representations to quantify the relevance of review sentences using a novel approximate geodesic distance based scoring mechanism. We use the relevance scores to identify popular opinions in order to compose general and aspect-specific summaries. Our proposed model, GeoSumm, achieves state-of-the-art performance on three opinion summarization datasets. We perform additional experiments to analyze the functioning of our model and showcase the generalization ability of {\X} across different domains. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Template-based Abstractive Microblog Opinion Summarisation&lt;/strong&gt; &lt;em&gt;Iman Munire Bilal, Bo Wang, Adam Tsakalidis, Dong Nguyen, Rob Procter, Maria Liakata&lt;/em&gt; &lt;code&gt;TACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.04083&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We introduce the task of microblog opinion summarisation (MOS) and share a dataset of 3100 gold-standard opinion summaries to facilitate research in this domain. The dataset contains summaries of tweets spanning a 2-year period and covers more topics than any other public Twitter summarisation dataset. Summaries are abstractive in nature and have been created by journalists skilled in summarising news articles following a template separating factual information (main story) from author opinions. Our method differs from previous work on generating gold-standard summaries from social media, which usually involves selecting representative posts and thus favours extractive summarisation models. To showcase the dataset&#39;s utility and challenges, we benchmark a range of abstractive and extractive state-of-the-art summarisation models and achieve good performance, with the former outperforming the latter. We also show that fine-tuning is necessary to improve performance and investigate the benefits of using different sample sizes. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient Few-Shot Fine-Tuning for Opinion Summarization&lt;/strong&gt; &lt;em&gt;Arthur Bražinskas, Ramesh Nallapati, Mohit Bansal, Markus Dreyer&lt;/em&gt; &lt;code&gt;Findings of NAACL 202&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.113/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-research/adasum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Abstractive summarization models are typically pre-trained on large amounts of generic texts, then fine-tuned on tens or hundreds of thousands of annotated samples. However, in opinion summarization, large annotated datasets of reviews paired with reference summaries are not available and would be expensive to create. This calls for fine-tuning methods robust to overfitting on small datasets. In addition, generically pre-trained models are often not accustomed to the specifics of customer reviews and, after fine-tuning, yield summaries with disfluencies and semantic mistakes. To address these problems, we utilize an efficient few-shot method based on adapters which, as we show, can easily store in-domain knowledge. Instead of fine-tuning the entire model, we add adapters and pre-train them in a task-specific way on a large corpus of unannotated customer reviews, using held-out reviews as pseudo summaries. Then, fine-tune the adapters on the small available human-annotated dataset. We show that this self-supervised adapter pre-training improves summary quality over standard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp datasets, respectively. Finally, for summary personalization, we condition on aspect keyword queries, automatically created from generic datasets. In the same vein, we pre-train the adapters in a query-based manner on customer reviews and then fine-tune them on annotated datasets. This results in better-organized summary content reflected in improved coherence and fewer redundancies. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text Generation in E-commerce Title and Review Summarization&lt;/strong&gt; &lt;em&gt;Xueying Zhang, Yunjiang Jiang, Yue Shang, Zhaomeng Cheng, Chi Zhang, Xiaochuan Fan, Yun Xiao, Bo Long&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.08414&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convex Aggregation for Opinion Summarization&lt;/strong&gt; &lt;em&gt;Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis, Wang-Chiew Tan&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.328/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/megagonlabs/coop&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Measuring Similarity of Opinion-bearing Sentences&lt;/strong&gt; &lt;em&gt;Wenyi Tay, Xiuzhen Zhang, Stephen Wan, Sarvnaz Karimi&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.9/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/wenyi-tay/sos&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comparative Opinion Summarization via Collaborative Decoding&lt;/strong&gt; &lt;em&gt;Hayate Iso, Xiaolan Wang, Yoshihiko Suhara&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.07520&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/megagonlabs/cocosum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning Opinion Summarizers by Selecting Informative Reviews&lt;/strong&gt; &lt;em&gt;Arthur Bražinskas, Mirella Lapata, Ivan Titov&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.04325&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/abrazinskas/SelSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aspect-Controllable Opinion Summarization&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Stefanos Angelidis, Mirella Lapata&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.03171&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rktamplayo/AceSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CUSTOM: Aspect-Oriented Product Summarization for E-Commerce&lt;/strong&gt; &lt;em&gt;Jiahui Liang, Junwei Bao, Yifan Wang, Youzheng Wu, Xiaodong He, Bowen Zhou&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.08010&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/JD-AI-Research-NLP/CUSTOM&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TransSum: Translating Aspect and Sentiment Embeddings for Self-Supervised Opinion Summarization&lt;/strong&gt; &lt;em&gt;Ke Wang, Xiaojun Wan&lt;/em&gt; &lt;code&gt;ACL 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-acl.65/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance&lt;/strong&gt; &lt;em&gt;Masaru Isonuma, Junichiro Mori, Danushka Bollegala, Ichiro Sakata&lt;/em&gt; &lt;code&gt;TACL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.08007&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PASS: Perturb-and-Select Summarizer for Product Reviews&lt;/strong&gt; &lt;em&gt;Nadav Oved, Ran Levy&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.30/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised Multimodal Opinion Summarization&lt;/strong&gt; &lt;em&gt;Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, Sehee Chung&lt;/em&gt; &lt;code&gt;ACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.13135&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nc-ai/knowledge/tree/master/publications/MultimodalSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MRCBert: A Machine Reading Comprehension Approach for Unsupervised Summarization&lt;/strong&gt; &lt;em&gt;Saurabh Jain, Guokai Tang, Lim Sze Chi&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.00239&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/saurabhhssaurabh/reviews_summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Informative and Controllable Opinion Summarization&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Mirella Lapata&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.229/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rktamplayo/CondaSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-Supervised and Controlled Multi-Document Opinion Summarization&lt;/strong&gt; &lt;em&gt;Hady Elsahar, Maximin Coavoux, Jos Rozen, Matthias Gallé&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.141/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Opinion Summarization with Content Planning&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Stefanos Angelidis, Mirella Lapata&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07808&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rktamplayo/PlanSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Opinion Summarization in Quantized Transformer Spaces&lt;/strong&gt; &lt;em&gt;Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, Mirella Lapata&lt;/em&gt; &lt;code&gt;TACL&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.04443&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/stangelid/qt&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Few-Shot Learning for Opinion Summarization&lt;/strong&gt; &lt;em&gt;Arthur Bražinskas, Mirella Lapata, Ivan Titov&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.14884&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/abrazinskas/FewSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Opinion Summarization as Copycat-Review Generation&lt;/strong&gt; &lt;em&gt;Arthur Bražinskas, Mirella Lapata, Ivan Titov&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1911.02247&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/abrazinskas/Copycat-abstractive-opinion-summarizer&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Opinion Summarization with Noising and Denoising&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Mirella Lapata&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2004.10150&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/rktamplayo/DenoiseSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OPINIONDIGEST: A Simple Framework for Opinion Summarization&lt;/strong&gt; &lt;em&gt;Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis, Wang-Chiew Tan&lt;/em&gt; &lt;code&gt;ACL20 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.01901&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/megagonlabs/opiniondigest&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Weakly-Supervised Opinion Summarization by Leveraging External Information&lt;/strong&gt; &lt;em&gt;Chao Zhao, Snigdha Chaturvedi&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1911.09844&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhaochaocs/AspMem&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Eric Chu, Peter J. Liu&lt;/em&gt; &lt;code&gt;ICML19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1810.05739&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sosuperic/MeanSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Reinforcement Learning&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards&lt;/strong&gt; &lt;em&gt;Shweta Yadav, Deepak Gupta, Asma Ben Abacha, Dina Demner-Fushman&lt;/em&gt; &lt;code&gt;ACL 2021 short&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-short.33/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/shwetanlp/CHQ-Summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation&lt;/strong&gt; &lt;em&gt;Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi&lt;/em&gt; &lt;code&gt;5th Workshop on Structured Prediction for NLP ACL-IJCNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.04080&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reinforced Generative Adversarial Network for Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Tianyang Xu, Chunyun Zhang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.15176&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Answers Unite! Unsupervised Metrics for Reinforced Summarization Models&lt;/strong&gt; &lt;em&gt;Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.01610&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Siyao Li, Deren Lei, Pengda Qin, William Yang Wang&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.00141&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reinforced Extractive Summarization with Question-Focused Rewards&lt;/strong&gt; &lt;em&gt;Kristjan Arumae, Fei Liu&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P18-3015/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting&lt;/strong&gt; &lt;em&gt;Yen-Chun Chen, Mohit Bansal&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1805.11080&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenRocks/fast_abs_rl&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Reward Reinforced Summarization with Saliency and Entailmen&lt;/strong&gt; &lt;em&gt;Ramakanth Pasunuru, Mohit Bansal&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N18-2102/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Communicating Agents for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, Yejin Choi&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1803.10357&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ranking Sentences for Extractive Summarization with Reinforcement Learning&lt;/strong&gt; &lt;em&gt;Shashi Narayan, Shay B. Cohen, Mirella Lapata&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N18-1158/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/EdinburghNLP/Refresh&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Deep Reinforced Model For Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Romain Paulus, Caiming Xiong, Richard Socher&lt;/em&gt; &lt;code&gt;ICLR18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1705.04304&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Reward Learning&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recursively Summarizing Books with Human Feedback&lt;/strong&gt; &lt;em&gt;Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nissan Stiennon, Ryan Lowe, Jan Leike, Paul Christiano&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.10862&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://openaipublic.blob.core.windows.net/recursive-book-summ/website/index.html#/&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to summarize from human feedback&lt;/strong&gt; &lt;em&gt;Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2009.01325&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/openai/summarize-from-feedback&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better Rewards Yield Better Summaries: Learning to Summarise Without References&lt;/strong&gt; &lt;em&gt;Florian Böhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, Iryna Gurevych&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1909.01214&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yg211/summary-reward-no-reference&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Extractive&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Noise-injected Consistency Training and Entropy-constrained Pseudo Labeling for Semi-supervised Extractive Summarization&lt;/strong&gt; &lt;em&gt;Yiming Wang, Qianren Mao, Junnan Liu, Weifeng Jiang, Hongdong Zhu, Jianxin Li&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.561/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenSUM/CPSUM&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Labeling large amounts of extractive summarization data is often prohibitive expensive due to time, financial, and expertise constraints, which poses great challenges to incorporating summarization system in practical applications. This limitation can be overcome by semi-supervised approaches: consistency-training and pseudo-labeling to make full use of unlabeled data. Researches on the two, however, are conducted independently, and very few works try to connect them. In this paper, we first use the noise-injected consistency training paradigm to regularize model predictions. Subsequently, we propose a novel entropy-constrained pseudo labeling strategy to obtain high-confidence labels from unlabeled predictions, which can obtain high-confidence labels from unlabeled predictions by comparing the entropy of supervised and unsupervised predictions. By combining consistency training and pseudo-labeling, this framework enforce a low-density separation between classes, which decently improves the performance of supervised learning over an insufficient labeled extractive summarization dataset. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries&lt;/strong&gt; &lt;em&gt;Xiaofei Sun, Chun Fan, Zijun Sun, Yuxian Meng, Fei Wu, Jiwei Li&lt;/em&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.556/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The difficulty of generating coherent long texts lies in the fact that existing models overwhelmingly focus on the tasks of local word prediction, and cannot make high level plans on what to generate or capture the high-level discourse dependencies between chunks of texts. Inspired by how humans write, where a list of bullet points or a catalog is first outlined, and then each bullet point is expanded to form the whole article, we propose SOE, a pipelined system that involves of summarizing, outlining and elaborating for long text generation: the model first outlines the summaries for different segments of long texts, and then elaborates on each bullet point to generate the corresponding segment. To avoid the labor-intensive process of summary soliciting, we propose the reconstruction strategy, which extracts segment summaries in an unsupervised manner by selecting its most informative part to reconstruct the segment. The proposed generation system comes with the following merits: (1) the summary provides high-level guidance for text generation and avoids the local minimum of individual word predictions; (2) the high-level discourse dependencies are captured in the conditional dependencies between summaries and are preserved during the summary expansion process and (3) additionally, we are able to consider significantly more contexts by representing contexts as concise summaries. Extensive experiments demonstrate that SOE produces long texts with significantly better quality, along with faster convergence speed. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarisation for German-language Data: A Text-level Approach with Discourse Features&lt;/strong&gt; &lt;em&gt;Freya Hewett, Manfred Stede&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.63/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/fhewett/pcc-summaries&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We examine the link between facets of Rhetorical Structure Theory (RST) and the selection of content for extractive summarisation, for German-language texts. For this purpose, we produce a set of extractive summaries for a dataset of German-language newspaper commentaries, a corpus which already has several layers of annotation. We provide an in-depth analysis of the connection between summary sentences and several RST-based features and transfer these insights to various automated summarisation models. Our results show that RST features are informative for the task of extractive summarisation, particularly nuclearity and relations at sentence-level. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization with Oracle Expectation&lt;/strong&gt; &lt;em&gt;Yumo Xu, Mirella Lapata&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.12714&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yumoxu/oreo&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Extractive summarization produces summaries by identifying and concatenating the most important sentences in a document. Since most summarization datasets do not come with gold labels indicating whether document sentences are summary-worthy, different labeling algorithms have been proposed to extrapolate oracle extracts for model training. In this work, we identify two flaws with the widely used greedy labeling approach: it delivers suboptimal and deterministic oracles. To alleviate both issues, we propose a simple yet effective labeling algorithm that creates soft, expectation-based sentence labels. We define a new learning objective for extractive summarization which incorporates learning signals from multiple oracle summaries and prove it is equivalent to estimating the oracle expectation for each document sentence. Without any architectural modifications, the proposed labeling scheme achieves superior performance on a variety of summarization benchmarks across domains and languages, in both supervised and zero-shot settings. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OTExtSum: Extractive Text Summarisation with Optimal Transport&lt;/strong&gt; &lt;em&gt;Peggy Tang, Kun Hu, Rui Yan, Lei Zhang, Junbin Gao, Zhiyong Wang&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.85/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/peggypytang/OTExtSum/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Extractive text summarisation aims to select salient sentences from a document to form a short yet informative summary. While learning-based methods have achieved promising results, they have several limitations, such as dependence on expensive training and lack of interpretability. Therefore, in this paper, we propose a novel non-learning-based method by for the first time formulating text summarisation as an Optimal Transport (OT) problem, namely Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction is conceptualised as obtaining an optimal summary that minimises the transportation cost to a given document regarding their semantic distributions. Such a cost is defined by the Wasserstein distance and used to measure the summary’s semantic coverage of the original document. Comprehensive experiments on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and CNN/DM demonstrate that our proposed method outperforms the state-of-the-art non-learning-based methods and several recent learning-based methods in terms of the ROUGE metric. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Post-Editing Extractive Summaries by Definiteness Prediction&lt;/strong&gt; &lt;em&gt;Jad Kabbara, Jackie Chi Kit Cheung&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.312/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decision-Focused Summarization&lt;/strong&gt; &lt;em&gt;Chao-Chun Hsu, Chenhao Tan&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.06896&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ChicagoHAI/decsum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization&lt;/strong&gt; &lt;em&gt;Huy To Quoc, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen, Anh Gia-Tuan Nguyen&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.13741&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiplex Graph Neural Network for Extractive Text Summarization&lt;/strong&gt; &lt;em&gt;Baoyu Jing, Zeyu You, Tao Yang, Wei Fan, Hanghang Tong&lt;/em&gt; &lt;code&gt;EMNLP 2021 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.12870&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering&lt;/strong&gt; &lt;em&gt;Reinald Adrian Pugoy, Hung-Yu Kao&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.232/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Differential Amplifier for Extractive Summarization&lt;/strong&gt; &lt;em&gt;Ruipeng Jia, Yanan Cao, Fang Fang, Yuchen Zhou, Zheng Fang, Yanbing Liu, Shi Wang&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.31/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents&lt;/strong&gt; &lt;em&gt;Paheli Bhattacharya, Soham Poddar, Koustav Rudra, Kripabandhu Ghosh, Saptarshi Ghosh&lt;/em&gt; &lt;code&gt;ICAIL 2021&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.15876&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Topic Modeling Based Extractive Text Summarization&lt;/strong&gt; &lt;em&gt;Kalliath Abdul Rasheed Issam, Shivam Patel, Subalalitha C. N&lt;/em&gt; &lt;code&gt;Journal&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.15313&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning&lt;/strong&gt; &lt;em&gt;Linzi Xing, Wen Xiao, Giuseppe Carenini&lt;/em&gt; &lt;code&gt;ACL2021-short&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-short.119/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lxing532/Debiasing&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Genetic Algorithms For Extractive Summarization&lt;/strong&gt; &lt;em&gt;William Chen, Kensal Ramos, Kalyan Naidu Mullaguri&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.02365&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph&lt;/strong&gt; &lt;em&gt;Yin Jou Huang, Sadao Kurohashi&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.265/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Keping Bi, Rahul Jha, Bruce Croft, Asli Celikyilmaz&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.22/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Extractive Summarization using Pointwise Mutual Information&lt;/strong&gt; &lt;em&gt;Vishakh Padmakumar, He He&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.213/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/vishakhpk/mi-unsup-summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better Highlighting: Creating Sub-Sentence Summary Highlights&lt;/strong&gt; &lt;em&gt;Sangwoo Cho, Kaiqiang Song, Chen Li, Dong Yu, Hassan Foroosh, Fei Liu&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.10566&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/better-highlighting&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SupMMD: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy&lt;/strong&gt; &lt;em&gt;Umanga Bista, Alexander Patrick Mathews, Aditya Krishna Menon, Lexing Xie&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.02568&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/computationalmedia/supmmd&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stepwise Extractive Summarization and Planning with Structured Transformers&lt;/strong&gt; &lt;em&gt;Shashi Narayan, Joshua Maynez, Jakub Adamek, Daniele Pighin, Blaž Bratanič, Ryan McDonald&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.02744&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/etcsum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Discourse-Aware Neural Extractive Model for Text Summarization&lt;/strong&gt; &lt;em&gt;Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu&lt;/em&gt; &lt;code&gt;ACL20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1910.14142&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jiacheng-xu/DiscoBERT&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reading Like HER: Human Reading Inspired Extractive Summarization&lt;/strong&gt; &lt;em&gt;Ling Luo, Xiang Ao, Yan Song, Feiyang Pan, Min Yang, Qing He&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-1300/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploiting Discourse-Level Segmentation for Extractive Summarization&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Nancy Chen&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-5415/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DeepChannel: Salience Estimation by Contrastive Learning for Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Jiaxin Shi, Chen Liang, Lei Hou, Juanzi Li, Zhiyuan Liu, Hanwang Zhang&lt;/em&gt; &lt;code&gt;AAAI19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1811.02394&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/lliangchenc/DeepChannel&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks&lt;/strong&gt; &lt;em&gt;Aishwarya Jadhav, Vaibhav Rajan&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P18-1014/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Document Summarization by Jointly Learning to Score and Select Sentences&lt;/strong&gt; &lt;em&gt;Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, Tiejun Zhao&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/P18-1061/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Latent Extractive Document Summarization&lt;/strong&gt; &lt;em&gt;Xingxing Zhang, Mirella Lapata, Furu Wei, Ming Zhou&lt;/em&gt; &lt;code&gt;ACL18&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D18-1088/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative Adversarial Network for Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, Hongyan Li&lt;/em&gt; &lt;code&gt;AAAI18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1711.09357&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/iwangjian/textsum-gan&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling&lt;/strong&gt; &lt;em&gt;Wei Li, Xinyan Xiao, Yajuan Lyu, Yuanzhuo Wang&lt;/em&gt; &lt;code&gt;EMNLP18&lt;/code&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D18-1205/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extractive Summarization Using Multi-Task Learning with Document Classification&lt;/strong&gt; &lt;em&gt;Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka Matsuo, Ichiro Sakata&lt;/em&gt; &lt;code&gt;EMNLP17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D17-1223/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents&lt;/strong&gt; &lt;em&gt;Ramesh Nallapati, Feifei Zhai, Bowen Zhou&lt;/em&gt; &lt;code&gt;AAAI17&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1611.04230&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/hpzhao/SummaRuNNer&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization through Entailment-based Minimum Vertex Cover&lt;/strong&gt; &lt;em&gt;Anand Gupta, Manpreet Kaur, Shachar Mirkin, Adarsh Singh, Aseem Goyal&lt;/em&gt; &lt;code&gt;ENLG13&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/S14-1010/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Extractive-Abstractive&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;EASE: Extractive-Abstractive Summarization with Explanations&lt;/strong&gt; &lt;em&gt;Haoran Li, Arash Einolghozati, Srinivasan Iyer, Bhargavi Paranjape, Yashar Mehdad, Sonal Gupta, Marjan Ghazvininejad&lt;/em&gt; &lt;code&gt;EMNLP 2021| newsum&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.newsum-1.10/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic Extractor-Paraphraser based Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Anubhav Jangra, Raghav Jain, Vaibhav Mavi, Sriparna Saha, Pushpak Bhattacharyya&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.01296&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contextualized Rewriting for Text Summarization&lt;/strong&gt; &lt;em&gt;Guangsheng Bao, Yue Zhang&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.00385&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jointly Extracting and Compressing Documents with Summary State Representations&lt;/strong&gt; &lt;em&gt;Afonso Mendes, Shashi Narayan, Sebastião Miranda, Zita Marinho, André F. T. Martins, Shay B. Cohen&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1904.02020&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Priberam/exconsumm&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;VAE&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Recurrent Generative Decoder for Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Piji Li, Wai Lam, Lidong Bing, Zihao Wang&lt;/em&gt; &lt;code&gt;EMNLP17&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D17-1222/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document Summarization with VHTM: Variational Hierarchical Topic-Aware Mechanism&lt;/strong&gt; &lt;em&gt;Xiyan Fu, Jun Wang, Jinghan Zhang, Jinmao Wei, Zhenglu Yang&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://ojs.aaai.org//index.php/AAAI/article/view/6277&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Syntactic&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compressive Summarization with Plausibility and Salience Modeling&lt;/strong&gt; &lt;em&gt;Shrey Desai, Jiacheng Xu, Greg Durrett&lt;/em&gt; &lt;code&gt;EMNLP20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.07886&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/shreydesai/cups&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;StructSum: Incorporating Latent and Explicit Sentence Dependencies for Single Document Summarization&lt;/strong&gt; &lt;em&gt;Vidhisha Balachandran, Artidoro Pagnoni, Jay Yoon Lee, Dheeraj Rajagopal, Jaime Carbonell, Yulia Tsvetkov&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/events/eacl-2021/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/vidhishanair/structured_summarizer&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Joint Parsing and Generation for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Kaiqiang Song, Logan Lebanoff, Qipeng Guo, Xipeng Qiu, Xiangyang Xue, Chen Li, Dong Yu, Fei Liu&lt;/em&gt; &lt;code&gt;AAAI20&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1911.10389&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/KaiQiangSong/joint_parse_summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Extractive Text Summarization with Syntactic Compression&lt;/strong&gt; &lt;em&gt;Jiacheng Xu, Greg Durrett&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1902.00863&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jiacheng-xu/neu-compression-sum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Single Document Summarization as Tree Induction&lt;/strong&gt; &lt;em&gt;Yang Liu, Ivan Titov, Mirella Lapata&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1173/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/nlpyang/SUMO&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;QA Related&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focus-Driven Contrastive Learning for Medical Question Summarization&lt;/strong&gt; &lt;em&gt;Ming Zhang, Shuai Dou, Ziyang Wang, Yunfang Wu&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.539/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatic medical question summarization can significantly help the system to understand consumer health questions and retrieve correct answers. The Seq2Seq model based on maximum likelihood estimation (MLE) has been applied in this task, which faces two general problems: the model can not capture well question focus and and the traditional MLE strategy lacks the ability to understand sentence-level semantics. To alleviate these problems, we propose a novel question focus-driven contrastive learning framework (QFCL). Specially, we propose an easy and effective approach to generate hard negative samples based on the question focus, and exploit contrastive learning at both encoder and decoder to obtain better sentence level representations. On three medical benchmark datasets, our proposed model achieves new state-of-the-art results, and obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline BART model on three datasets respectively. Further human judgement and detailed analysis prove that our QFCL model learns better sentence representations with the ability to distinguish different sentence meanings, and generates high-quality summaries by capturing question focus. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization&lt;/strong&gt; &lt;em&gt;Zhenjie Zhao, Yufang Hou, Dakuo Wang, Mo Yu, Chengzhong Liu, Xiaojuan Ma&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.348/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/zhaozj89/Educational-Question-Generation&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Generating educational questions of fairytales or storybooks is vital for improving children’s literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution of an input story paragraph, and then summarizes salient events which can be used to generate high-cognitive-demand questions. To train the event-centric summarizer, we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs. On a newly proposed educational question-answering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics. Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics&lt;/strong&gt; &lt;em&gt;Daniel Deutsch, Dan Roth&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.10206&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Using Question Answering Rewards to Improve Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Ranit Aharonov, Sachindra Joshi&lt;/em&gt; &lt;code&gt;EMNLP 2021 Findings&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.47/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Question-Based Salient Span Selection for More Controllable Text Summarization&lt;/strong&gt; &lt;em&gt;Daniel Deutsch, Dan Roth&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.07935&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Summarization with Latent Queries&lt;/strong&gt; &lt;em&gt;Yumo Xu, Mirella Lapata&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.00104&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarizing Chinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention&lt;/strong&gt; &lt;em&gt;Ningyu Zhang, Shumin Deng, Juan Li, Xi Chen, Wei Zhang, Huajun Chen&lt;/em&gt; &lt;code&gt;Findings of EMNLP&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.2/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary&lt;/strong&gt; &lt;em&gt;Daniel Deutsch, Tania Bedrax-Weiss, Dan Roth&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2010.00490&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/CogComp/qaeval-experiments&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Guiding Extractive Summarization with Question-Answering Rewards&lt;/strong&gt; &lt;em&gt;Kristjan Arumae, Fei Liu&lt;/em&gt; &lt;code&gt;NAACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1904.02321&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ucfnlp/summ_qa_rewards&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Semantic QA-Based Approach for Text Summarization Evaluation&lt;/strong&gt; &lt;em&gt;Ping Chen, Fei Wu, Tong Wang, Wei Ding&lt;/em&gt; &lt;code&gt;AAAI18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1704.06259&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Query&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Constrained Regeneration for Cross-Lingual Query-Focused Extractive Summarization&lt;/strong&gt; &lt;em&gt;Elsbeth Turcan, David Wan, Faisal Ladhak, Petra Galuscakova, Sukanta Sen, Svetlana Tchistiakova, Weijia Xu, Marine Carpuat, Kenneth Heafield, Douglas Oard, Kathleen McKeown&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.236/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Query-focused summaries of foreign-language, retrieved documents can help a user understand whether a document is actually relevant to the query term. A standard approach to this problem is to first translate the source documents and then perform extractive summarization to find relevant snippets. However, in a cross-lingual setting, the query term does not necessarily appear in the translations of relevant documents. In this work, we show that constrained machine translation and constrained post-editing can improve human relevance judgments by including a query term in a summary when its translation appears in the source document. We also present several strategies for selecting only certain documents for regeneration which yield further improvements &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focus-Driven Contrastive Learniang for Medical Question Summarization&lt;/strong&gt; &lt;em&gt;Ming Zhang, Shuai Dou, Ziyang Wang, Yunfang Wu&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.00484&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatic medical question summarization can significantly help the system to understand consumer health questions and retrieve correct answers. The Seq2Seq model based on maximum likelihood estimation (MLE) has been applied in this task, which faces two general problems: the model can not capture well question focus and and the traditional MLE strategy lacks the ability to understand sentence-level semantics. To alleviate these problems, we propose a novel question focus-driven contrastive learning framework (QFCL). Specially, we propose an easy and effective approach to generate hard negative samples based on the question focus, and exploit contrastive learning at both encoder and decoder to obtain better sentence level representations. On three medical benchmark datasets, our proposed model achieves new state-of-the-art results, and obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline BART model on three datasets respectively. Further human judgement and detailed analysis prove that our QFCL model learns better sentence representations with the ability to distinguish different sentence meanings, and generates high-quality summaries by capturing question focus. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization&lt;/strong&gt; &lt;em&gt;Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Xiangji Huang&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.11670&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tahmedge/PreQFAS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring Neural Models for Query-Focused Summarization&lt;/strong&gt; &lt;em&gt;Jesse Vig, Alexander R. Fabbri, Wojciech Kryściński&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.109/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/query-focused-sum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. While recently released datasets, such as QMSum or AQuaMuSe, facilitate research efforts in QFS, the field lacks a comprehensive study of the broad space of applicable modeling methods. In this paper we conduct a systematic exploration of neural approaches to QFS, considering two general classes of methods: two-stage extractive-abstractive solutions and end-to-end models. Within those categories, we investigate existing models and explore strategies for transfer learning. We also present two modeling extensions that achieve state-of-the-art performance on the QMSum dataset, up to a margin of 3.38 ROUGE-1, 3.72 ROUGE2, and 3.28 ROUGE-L when combined with transfer learning strategies. Results from human evaluation suggest that the best models produce more comprehensive and factually consistent summaries compared to a baseline model. Code and checkpoints are made publicly available: &#xA;   &lt;a href=&#34;https://github.com/salesforce/query-focused-sum&#34;&gt;https://github.com/salesforce/query-focused-sum&lt;/a&gt;. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aspect-Oriented Summarization through Query-Focused Extraction&lt;/strong&gt; &lt;em&gt;Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, Greg Durrett&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.08296&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions&lt;/strong&gt; &lt;em&gt;Diego Mollá (1 and 2), Urvashi Khanna (1), Dima Galat (1), Vincent Nguyen (2 and 3)Maciej Rybinski (3) ( (1) Macquarie University, (2) CSIRO Data61, (3) Australian National University)&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.12189&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summary-Oriented Question Generation for Informational Queries&lt;/strong&gt; &lt;em&gt;Xusen Yin, Li Zhou, Kevin Small, Jonathan May&lt;/em&gt; &lt;code&gt;Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.dialdoc-1.11/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards&lt;/strong&gt; &lt;em&gt;Shweta Yadav, Deepak Gupta, Asma Ben Abacha, Dina Demner-Fushman&lt;/em&gt; &lt;code&gt;ACL 2021 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2107.00176&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/shwetanlp/CHQ-Summ&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Query Focused Summaries from Query-Free Resources&lt;/strong&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;em&gt;Yumo Xu, Mirella Lapata&lt;/em&gt; &lt;a href=&#34;https://aclanthology.org/2021.acl-long.475/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/yumoxu/margesum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance&lt;/strong&gt; &lt;em&gt;Dan Su, Tiezheng Yu, Pascale Fung&lt;/em&gt; &lt;code&gt;ACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.12969&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/HLTCHKUST/QFS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;D2S: Document-to-Slide Generation Via Query-Based Text Summarization&lt;/strong&gt; &lt;em&gt;Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, Nancy X.R. Wang&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.03664&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/IBM/document2slides&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;EncoderFusion&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning&lt;/strong&gt; &lt;em&gt;Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Zhaopeng Tu&lt;/em&gt; &lt;code&gt;ICLR21&lt;/code&gt; &lt;a href=&#34;https://openreview.net/pdf?id=n1HD8M6WGn&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving Abstractive Text Summarization with History Aggregation&lt;/strong&gt; &lt;em&gt;Pengcheng Liao, Chuang Zhang, Xiaojun Chen, Xiaofei Zhou&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1912.11046&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Pc-liao/Transformer_agg&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Discourse&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discourse-Aware Unsupervised Summarization for Long Scientific Documents&lt;/strong&gt; &lt;em&gt;Yue Dong, Andrei Mircea Romascanu, Jackie Chi Kit Cheung&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.93/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/mirandrom/HipoRank&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discourse Understanding and Factual Consistency in Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Saadia Gabriel, Antoine Bosselut, Jeff Da, Ari Holtzman, Jan Buys, Kyle Lo, Asli Celikyilmaz, Yejin Choi&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.eacl-main.34/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/skgabriel/coopnet&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Predicting Discourse Trees from Transformer-based Neural Summarizers&lt;/strong&gt; &lt;em&gt;Wen Xiao, Patrick Huber, Giuseppe Carenini&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.07058&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Wendy-Xiao/summ_guided_disco_parser&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !&lt;/strong&gt; &lt;em&gt;Wen Xiao, Patrick Huber, Giuseppe Carenini&lt;/em&gt; &lt;code&gt;EMNLP20 Workshop&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.02144&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dialogue Discourse-Aware Graph Convolutional Networks for Abstractive Meeting Summarization&lt;/strong&gt; &lt;em&gt;Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, Ting Liu&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.03502&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-meeting-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Restructuring Conversations using Discourse Relations for Zero-shot Abstractive Dialogue Summarization&lt;/strong&gt; &lt;em&gt;Prakhar Ganesh, Saket Dingliwal&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/1902.01615&#34;&gt;[pdf]&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/-meeting-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking&lt;/strong&gt; &lt;em&gt;Masaru Isonuma, Junichiro Mori, Ichiro Sakata&lt;/em&gt; &lt;code&gt;ACL19&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.05691&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/misonuma/strsum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploiting Discourse-Level Segmentation for Extractive Summarization&lt;/strong&gt; &lt;em&gt;Zhengyuan Liu, Nancy Chen&lt;/em&gt; &lt;code&gt;EMNLP19&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/D19-5415/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents&lt;/strong&gt; &lt;em&gt;Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, Nazli Goharian&lt;/em&gt; &lt;code&gt;NAACL18&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/1804.05685&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/armancohan/long-summarization&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Movie&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Movie Summarization via Sparse Graph Construction&lt;/strong&gt; &lt;em&gt;Pinelopi Papalampidi, Frank Keller, Mirella Lapata&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07536&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ppapalampidi/GraphTP&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Low Resource&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Xiaochen Liu, Yu Bai, Jiawei Li, Yinan Hu, Yang Gao&lt;/em&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.553/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Few-shot abstractive summarization has become a challenging task in natural language generation. To support it, we developed a novel soft prompts architecture coupled with a prompt pre-training plus prompt fine-tuning paradigm, which is effective and tunes only extremely light parameters. To meet the structure of the generation models, the soft prompts comprise continuous input embeddings across an encoder and a decoder. Importantly, a new inner-prompt placed in the text is introduced to capture document-level information. The aim is to devote attention to understanding the document that better prompts the model to generate document-related content. In the training process, the prompt pre-training with self-supervised pseudo-data firstly teaches the model basic summarizing capability. Then, with few-shot examples, only the designed lightweight soft prompts are fine-tuned. Experimental results on the CNN/DailyMail and XSum datasets show that our method, with only 0.1% of the parameters, outperforms full-model tuning where all model parameters are tuned. It also surpasses Prompt Tuning by a large margin and delivers competitive results against Prefix-Tuning with 3% of the parameters. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Summarizing Healthcare Questions in Low-Resource Setting&lt;/strong&gt; &lt;em&gt;Shweta Yadav, Cornelia Caragea&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.255/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The current advancement in abstractive document summarization depends to a large extent on a considerable amount of human-annotated datasets. However, the creation of large-scale datasets is often not feasible in closed domains, such as medical and healthcare domains, where human annotation requires domain expertise. This paper presents a novel data selection strategy to generate diverse and semantic questions in a low-resource setting with the aim to summarize healthcare questions. Our method exploits the concept of guided semantic-overlap and diversity-based objective functions to optimally select the informative and diverse set of synthetic samples for data augmentation. Our extensive experiments on benchmark healthcare question summarization datasets demonstrate the effectiveness of our proposed data selection strategy by achieving new state-of-the-art results. Our human evaluation shows that our method generates diverse, fluent, and informative summarized questions. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Summarization of Russian Texts: Comparison of Extractive and Abstractive Methods&lt;/strong&gt; &lt;em&gt;Valeriya Goloviznina, Evgeny Kotelnikov&lt;/em&gt; &lt;code&gt;Dialogue-2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.09253&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The development of large and super-large language models, such as GPT-3, T5, Switch Transformer, ERNIE, etc., has significantly improved the performance of text generation. One of the important research directions in this area is the generation of texts with arguments. The solution of this problem can be used in business meetings, political debates, dialogue systems, for preparation of student essays. One of the main domains for these applications is the economic sphere. The key problem of the argument text generation for the Russian language is the lack of annotated argumentation corpora. In this paper, we use translated versions of the Argumentative Microtext, Persuasive Essays and UKP Sentential corpora to fine-tune RuBERT model. Further, this model is used to annotate the corpus of economic news by argumentation. Then the annotated corpus is employed to fine-tune the ruGPT-3 model, which generates argument texts. The results show that this approach improves the accuracy of the argument generation by more than 20 percentage points (63.2% vs. 42.5%) compared to the original ruGPT-3 model. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Indian Legal Text Summarization: A Text Normalisation-based Approach&lt;/strong&gt; &lt;em&gt;Satyajit Ghosh, Mousumi Dutta, Tanaya Das&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.06238&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; In the Indian court system, pending cases have long been a problem. There are more than 4 crore cases outstanding. Manually summarising hundreds of documents is a time-consuming and tedious task for legal stakeholders. Many state-of-the-art models for text summarization have emerged as machine learning has progressed. Domain-independent models don&#39;t do well with legal texts, and fine-tuning those models for the Indian Legal System is problematic due to a lack of publicly available datasets. To improve the performance of domain-independent models, the authors have proposed a methodology for normalising legal texts in the Indian context. The authors experimented with two state-of-the-art domain-independent models for legal text summarization, namely BART and PEGASUS. BART and PEGASUS are put through their paces in terms of extractive and abstractive summarization to understand the effectiveness of the text normalisation approach. Summarised texts are evaluated by domain experts on multiple parameters and using ROUGE metrics. It shows the proposed text normalisation approach is effective in legal texts with domain-independent models. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain Specific Fine-tuning of Denoising Sequence-to-Sequence Models for Natural Language Summarization&lt;/strong&gt; &lt;em&gt;Brydon Parker, Alik Sokolov, Mahtab Ahmed, Matt Kalebic, Sedef Akinli Kocak, Ofer Shai&lt;/em&gt; `` &lt;a href=&#34;https://arxiv.org/abs/2204.09716&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/VectorInstitute/Vector_NLP_Domain-Summ&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://www.kaggle.com/datasets/vectorinstitute/domainspecific-reddit-data-medical-and-financial&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Overview of Indian Language Datasets used for Text Summarization&lt;/strong&gt; &lt;em&gt;Shagun Sinha, Girish Nath Jha&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.16127&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AraBART: a Pretrained Arabic Sequence-to-Sequence Model for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, Michalis Vazirgiannis&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.10945&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/moussaKam/AraBART&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ExtraPhrase: Efficient Data Augmentation for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Mengsay Loem, Sho Takase, Masahiro Kaneko, Naoaki Okazaki&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.05313&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Ahmed Magooda, Diane Litman&lt;/em&gt; &lt;code&gt;Findings of EMNLP 2021 Short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.08569&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring Multitask Learning for Low-Resource Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Ahmed Magooda, Mohamed Elaraby, Diane Litman&lt;/em&gt; &lt;code&gt;EMNLP 2021 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2109.08565&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data&lt;/strong&gt; &lt;em&gt;Sanjeev Kumar Karn, Francine Chen, Yan-Ying Chen, Ulli Waltinger, Hinrich Schütze&lt;/em&gt; &lt;code&gt;EACL21&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/2021.adaptnlp-1.24/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Tiezheng Yu, Zihan Liu, Pascale Fung&lt;/em&gt; &lt;code&gt;NAACL21&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.11332&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/TysonYu/AdaptSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meta-Transfer Learning for Low-Resource Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Yi-Syuan Chen, Hong-Han Shuai&lt;/em&gt; &lt;code&gt;AAAI21&lt;/code&gt; &lt;a href=&#34;https://basiclab.nctu.edu.tw/assets/LowResourceSummarization.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/YiSyuanChen/MTL-ABS&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Personalized&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised Summarization with Customized Granularities&lt;/strong&gt; &lt;em&gt;Ming Zhong, Yang Liu, Suyu Ge, Yuning Mao, Yizhu Jiao, Xingxing Zhang, Yichong Xu, Chenguang Zhu, Michael Zeng, Jiawei Han&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.12502&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transformer Reasoning Network for Personalized Review Summarization&lt;/strong&gt; &lt;em&gt;Hongyan Xu, Hongtao Liu, Pengfei Jiao, Wenjun Wang&lt;/em&gt; &lt;code&gt;SIGIR 2021&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3404835.3462854&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PENS: A Dataset and Generic Framework for Personalized News Headline Generation&lt;/strong&gt; &lt;em&gt;Xiang Ao Xiting Wang Ling Luo Ying Qiao Qing He Xing Xie&lt;/em&gt; &lt;code&gt;ACL 2021&lt;/code&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2021/06/ACL2021_PENS_Camera_Ready_1862_Paper.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://msnews.github.io/pens.html&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Collabot: Personalized Group Chat Summarization&lt;/strong&gt; &lt;em&gt;Naama Tepper, Anat Hashavit, Maya Barnea, Inbal Ronen, Lior Leiba&lt;/em&gt; &lt;code&gt;WSDM 2018&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3159652.3160588&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback&lt;/strong&gt; &lt;em&gt;Avinesh P.V.S, Christian M. Meyer&lt;/em&gt; &lt;code&gt;ACL 2017&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/P17-1124/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/UKPLab/acl2017-interactive_summarizer&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context Enhanced Personalized Social Summarization&lt;/strong&gt; &lt;em&gt;Po Hu, Donghong Ji, Chong Teng, Yujing Guo&lt;/em&gt; &lt;code&gt;COLING12&lt;/code&gt; &lt;a href=&#34;https://www.aclweb.org/anthology/C12-1075.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization&lt;/strong&gt; &lt;em&gt;Rui Yan, Jian-Yun Nie, Xiaoming Li&lt;/em&gt; &lt;code&gt;EMNLP 2011&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/D11-1124/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;In-Browser Summarisation: Generating Elaborative Summaries Biased Towards the Reading Context&lt;/strong&gt; &lt;em&gt;Stephen Wan, Cécile Paris&lt;/em&gt; &lt;code&gt;ACL 2008&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/P08-2033/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Personalized Summarization Agent Using Non-negative Matrix Factorization&lt;/strong&gt; &lt;em&gt;Sun Park&lt;/em&gt; &lt;code&gt;PRICAI 2008&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-540-89197-0_103&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aspect-Based Personalized Text Summarization&lt;/strong&gt; &lt;em&gt;Shlomo Berkovsky, Timothy Baldwin, Ingrid Zukerman&lt;/em&gt; &lt;code&gt;AH 2008&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-540-70987-9_31&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-model based personalized summarization&lt;/strong&gt; &lt;em&gt;Alberto Díaz, Pablo Gervás&lt;/em&gt; &lt;a href=&#34;https://doi.org/10.1016/j.ipm.2007.01.009&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Machine Learning of Generic and User-Focused Summarization&lt;/strong&gt; &lt;em&gt;Inderjeet Mani, Eric Bloedorn&lt;/em&gt; &lt;code&gt;AAAI 1998&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/cs/9811006&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Interactive&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback&lt;/strong&gt; &lt;em&gt;Duy-Hung Nguyen, Nguyen Viet Dung Nghiem, Bao-Sinh Nguyen, Dung Tien Tien Le, Shahab Sabahi, Minh-Tien Nguyen, Hung Le&lt;/em&gt; &lt;code&gt;Findings of NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.findings-naacl.147/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; For summarization, human preferences is critical to tame outputs of the summarizer in favor of human interests, as ground-truth summaries are scarce and ambiguous. Practical settings require dynamic exchanges between humans and AI agents wherein feedback is provided in an online manner, a few at a time. In this paper, we introduce a new framework to train summarization models with preference feedback interactively. By properly leveraging offline data and a novel reward model, we improve the performance regarding ROUGE scores and sample-efficiency. Our experiments on three various datasets confirm the benefit of the proposed framework in active, few-shot and online settings of preference learning. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interactive Query-Assisted Summarization via Deep Reinforcement Learning&lt;/strong&gt; &lt;em&gt;Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Ido Dagan, Yael Amsterdamer&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.184/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/OriShapira/InterExp_DeepRL&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hone as You Read: A Practical Type of Interactive Summarization&lt;/strong&gt; &lt;em&gt;Tanner Bohn, Charles X. Ling&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.02923&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Speech&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech Summarization using Restricted Self-Attention&lt;/strong&gt; &lt;em&gt;Roshan Sharma, Shruti Palaskar, Alan W Black, Florian Metze&lt;/em&gt; &lt;code&gt;ICASSP 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.06263&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Prompt&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;News Summarization and Evaluation in the Era of GPT-3&lt;/strong&gt; &lt;em&gt;Tanya Goyal, Junyi Jessy Li, Greg Durrett&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.12356&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://tagoyal.github.io/zeroshot-news-annotations.html&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The recent success of zero- and few-shot prompting with models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how zero-shot GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics, e.g. recently proposed QA- or entailment-based factuality approaches, cannot reliably evaluate zero-shot summaries. Finally, we discuss future research challenges beyond generic summarization, specifically, keyword- and aspect-based summarization, showing how dominant fine-tuning approaches compare to zero-shot prompting. To support further research, we release: (a) a corpus of 10K generated summaries from fine-tuned and zero-shot models across 4 standard summarization benchmarks, (b) 1K human preference judgments and rationales comparing different systems for generic- and keyword-based summarization. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;To Adapt or to Fine-tune: A Case Study on Abstractive Summarization&lt;/strong&gt; &lt;em&gt;Zheng Zhao, Pinzhen Chen&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.14559&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Recent advances in the field of abstractive summarization leverage pre-trained language models rather than train a model from scratch. However, such models are sluggish to train and accompanied by a massive overhead. Researchers have proposed a few lightweight alternatives such as smaller adapters to mitigate the drawbacks. Nonetheless, it remains uncertain whether using adapters benefits the task of summarization, in terms of improved efficiency without an unpleasant sacrifice in performance. In this work, we carry out multifaceted investigations on fine-tuning and adapters for summarization tasks with varying complexity: language, domain, and task transfer. In our experiments, fine-tuning a pre-trained language model generally attains a better performance than using adapters; the performance gap positively correlates with the amount of training data used. Notably, adapters exceed fine-tuning under extremely low-resource conditions. We further provide insights on multilinguality, model convergence, and robustness, hoping to shed light on the pragmatic choice of fine-tuning or adapters in abstractive summarization. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discourse-Aware Prompt Design for Text Generation&lt;/strong&gt; &lt;em&gt;Marjan Ghazvininejad, Vladimir Karpukhin, Asli Celikyilmaz&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.05717&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Temp&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Harnessing Abstractive Summarization for Fact-Checked Claim Detection&lt;/strong&gt; &lt;em&gt;Harnessing Abstractive Summarization for Fact-Checked Claim Detection&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.04612&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/varadhbhatnagar/FC-Claim-Det/&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Social media platforms have become new battlegrounds for anti-social elements, with misinformation being the weapon of choice. Fact-checking organizations try to debunk as many claims as possible while staying true to their journalistic processes but cannot cope with its rapid dissemination. We believe that the solution lies in partial automation of the fact-checking life cycle, saving human time for tasks which require high cognition. We propose a new workflow for efficiently detecting previously fact-checked claims that uses abstractive summarization to generate crisp queries. These queries can then be executed on a general-purpose retrieval system associated with a collection of previously fact-checked claims. We curate an abstractive text summarization dataset comprising noisy claims from Twitter and their gold summaries. It is shown that retrieval performance improves 2x by using popular out-of-the-box summarization models and 3x by fine-tuning them on the accompanying dataset compared to verbatim querying. Our approach achieves Recall@5 and MRR of 35% and 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset, code, and models are available publicly: this https URL &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stage-wise Stylistic Headline Generation: Style Generation and Summarized Content Insertion&lt;/strong&gt; &lt;em&gt;Jiaao Zhan, Yang Gao∗, Yu Bai, Qianhui Liu&lt;/em&gt; &lt;code&gt;IJCAI 2022&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/proceedings/2022/0623.pdf&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; A quality headline with a high click-rate should notonly summarize the content of an article, but alsorefect a style that attracts users. Such demand hasdrawn rising attention to the task of stylistic headline generation (SHG). An intuitive method is to frstgenerate plain headlines leveraged by documentheadline parallel data then transfer them to a targetstyle. However, this inevitably suffers from errorpropagation. Therefore, to unify the two sub-tasksand explicitly decompose style-relevant attributesand summarize content, we propose an end-to-endstage-wise SHG model containing the style generation component and the content insertion component, where the former generates stylistic-relevantintermediate outputs and the latter receives theseoutputs then inserts the summarized content. The intermediate outputs are observable, making the stylegeneration easy to control. Our system is comprehensively evaluated by both quantitative and qualitative metrics, and it achieves state-of-the-art resultsin SHG over three different stylistic datasets. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries&lt;/strong&gt; &lt;em&gt;Hai Dang, Karim Benharrak, Florian Lehmann, Daniel Buschek&lt;/em&gt; &lt;code&gt;ACM UIST 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.09323&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; We propose a text editor to help users plan, structure and reflect on their writing process. It provides continuously updated paragraph-wise summaries as margin annotations, using automatic text summarization. Summary levels range from full text, to selected (central) sentences, down to a collection of keywords. To understand how users interact with this system during writing, we conducted two user studies (N=4 and N=8) in which people wrote analytic essays about a given topic and article. As a key finding, the summaries gave users an external perspective on their writing and helped them to revise the content and scope of their drafted paragraphs. People further used the tool to quickly gain an overview of the text and developed strategies to integrate insights from the automated summaries. More broadly, this work explores and highlights the value of designing AI tools for writers, with Natural Language Processing (NLP) capabilities that go beyond direct text generation and correction. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SETSum: Summarization and Visualization of Student Evaluations of Teaching&lt;/strong&gt; &lt;em&gt;Yinuo Hu, Shiyue Zhang, Viji Sathy, Abigail Panter, Mohit Bansal&lt;/em&gt; &lt;code&gt;NAACL 2022 Demo&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-demo.9/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/evahuyn/SETSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Student Evaluations of Teaching (SETs) are widely used in colleges and universities. Typically SET results are summarized for instructors in a static PDF report. The report often includes summary statistics for quantitative ratings and an unsorted list of open-ended student comments. The lack of organization and summarization of the raw comments hinders those interpreting the reports from fully utilizing informative feedback, making accurate inferences, and designing appropriate instructional improvements. In this work, we introduce a novel system, SETSUM, that leverages sentiment analysis, aspect extraction, summarization, and visualization techniques to provide organized illustrations of SET findings to instructors and other reviewers. Ten university professors from diverse departments serve as evaluators of the system and all agree that SETSUM help them interpret SET results more efficiently; and 6 out of 10 instructors prefer our system over the standard static PDF report (while the remaining 4 would like to have both). This demonstrates that our work holds the potential of reforming the SET reporting conventions in the future. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ASPECTNEWS: Aspect-Oriented Summarization of News Documents&lt;/strong&gt; &lt;em&gt;Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, Greg Durrett&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.449/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/oja/aosumm&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions. But real users’ needs often fall in between these extremes and correspond to aspects, high-level topics discussed among similar types of documents. In this paper, we collect a dataset of realistic aspect-oriented summaries, AspectNews, which covers different subtopics about articles in news sub-domains. We annotate data across two domains of articles, earthquakes and fraud investigations, where each article is annotated with two distinct summaries focusing on different aspects for each domain. A system producing a single generic summary cannot concisely satisfy both aspects. Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data, so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work. We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted. Our evaluation shows that our final approach yields (a) focused summaries, better than those from a generic summarization system or from keyword matching; (b) a system sensitive to the choice of keywords. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Triangle-Densest-k-Subgraph Problem: Hardness, Lovász Extension, and Application to Document Summarization&lt;/strong&gt; &lt;em&gt;Aritra Konar, Nicholas D. Sidiropoulos&lt;/em&gt; &lt;code&gt;AAAI 2022&lt;/code&gt; &lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-4653.KonarA.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Applying Automatic Text Summarization for Fake News Detection&lt;/strong&gt; &lt;em&gt;Philipp Hartl, Udo Kruschwitz&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.01841&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/phHartl/lrec_2022&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graph Enhanced Contrastive Learning for Radiology Findings Summarization&lt;/strong&gt; &lt;em&gt;Jinpeng Hu, Zhuo Li, Zhihong Chen, Zhen Li, Xiang Wan, Tsung-Hui Chang&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.320/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/jinpeng01/AIG_CL&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention. With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information). Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings. To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation. In detail, for each input findings, it is encoded by a text encoder and a graph is constructed through its entities and dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph. Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words). The experimental results on two datasets, OpenI and MIMIC-CXR, confirm the effectiveness of our proposed method, where the state-of-the-art results are achieved. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization&lt;/strong&gt; &lt;em&gt;Sanjeev Kumar Karn, Ning Liu, Hinrich Schuetze, Oladimeji Farri&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.109/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist’s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models – which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on English radiology reports from two clinical sites show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score of 3-4%. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AUTOSUMM: Automatic Model Creation for Text Summarization&lt;/strong&gt; &lt;em&gt;Sharmila Reddy Nangi, Atharv Tyagi, Jay Mundra, Sagnik Mukherjee, Raj Snehal, Niyati Chhaya, Aparna Garimella&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.798.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Extend&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map&lt;/strong&gt; &lt;em&gt;Kangli Zi, Shi Wang, Yu Liu, Jicun Li, Yanan Cao, Cungen Cao&lt;/em&gt; &lt;code&gt;EMNLP 2021&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.33/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Zikangli/SOM-NCSCM&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Retrieve-augmented&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data&lt;/strong&gt; &lt;em&gt;Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, Michael Zeng&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/list/cs.CL/pastweek?skip=0&amp;amp;show=25&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/REINA&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Chart-to-text&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chart-to-Text: A Large-Scale Benchmark for Chart Summarization&lt;/strong&gt; &lt;em&gt;Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq Joty&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.277/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/vis-nlp/Chart-to-text&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Podcast&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Podcast Summary Assessment: A Resource for Evaluating Summary Assessment Methods&lt;/strong&gt; &lt;em&gt;Potsawee Manakul, Mark J. F. Gales&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.13265&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/potsawee/podcast_summary_assessment&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatic summary assessment is useful for both machine-generated and human-produced summaries. Automatically evaluating the summary text given the document enables, for example, summary generation system development and detection of inappropriate summaries. Summary assessment can be run in a number of modes: ranking summary generation systems; ranking summaries of a particular document; and estimating the quality of a document-summary pair on an absolute scale. Existing datasets with annotation for summary assessment are usually based on news summarization datasets such as CNN/DailyMail or XSum. In this work, we describe a new dataset, the podcast summary assessment corpus, a collection of podcast summaries that were evaluated by human experts at TREC2020. Compared to existing summary assessment data, this dataset has two unique aspects: (i) long-input, speech podcast based, documents; and (ii) an opportunity to detect inappropriate reference summaries in podcast corpus. First, we examine existing assessment methods, including model-free and model-based methods, and provide benchmark results for this long-input summary assessment dataset. Second, with the aim of filtering reference summary-document pairings for training, we apply summary assessment for data selection. The experimental results on these two aspects provide interesting insights on the summary assessment and generation tasks. The podcast summary assessment data is available. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Abstractive Grounded Summarization of Podcast Transcripts&lt;/strong&gt; &lt;em&gt;Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu, Fei Liu&lt;/em&gt; &lt;code&gt;ACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.acl-long.302/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/tencent-ailab/GrndPodcastSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Podcasts have shown a recent rise in popularity. Summarization of podcasts is of practical benefit to both content providers and consumers. It helps people quickly decide whether they will listen to a podcast and/or reduces the cognitive load of content providers to write summaries. Nevertheless, podcast summarization faces significant challenges including factual inconsistencies of summaries with respect to the inputs. The problem is exacerbated by speech disfluencies and recognition errors in transcripts of spoken language. In this paper, we explore a novel abstractive summarization method to alleviate these issues. Our approach learns to produce an abstractive summary while grounding summary segments in specific regions of the transcript to allow for full inspection of summary details. We conduct a series of analyses of the proposed approach on a large podcast dataset and show that the approach can achieve promising results. Grounded summaries bring clear benefits in locating the summary and transcript segments that contain inconsistent information, and hence improve summarization quality in terms of automatic and human evaluation. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Sports&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Soccer Game Summarization using Audio Commentary, Metadata, and Captions&lt;/strong&gt; &lt;em&gt;Sushant Gautam, Cise Midoglu, Saeed Shafiee Sabet, Dinesh Baniya Kshatri, Pål Halvorsen&lt;/em&gt; &lt;code&gt;NarSUM 2022&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3552463.3557019&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/simula/soccer-summarization&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Enhanced Sports Game Summarization&lt;/strong&gt; &lt;em&gt;Jiaan Wang, Zhixu Li, Tingyi Zhang, Duo Zheng, Jianfeng Qu, An Liu, Lei Zhao, Zhigang Chen&lt;/em&gt; &lt;code&gt;WSDM 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.12535&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/krystalan/K-SportsSum&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary&lt;/strong&gt; &lt;em&gt;Jiaan Wang, Zhixu Li, Qiang Yang, Jianfeng Qu, Zhigang Chen, Qingsheng Liu, Guoping Hu&lt;/em&gt; &lt;code&gt;CIKM 2021 short&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.05750&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/krystalan/SportsSum2.0&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating Sports News from Live Commentary: A Chinese Dataset for Sports Game Summarization&lt;/strong&gt; &lt;em&gt;Kuan-Hao Huang, Chen Li, Kai-Wei Chang&lt;/em&gt; &lt;code&gt;AACL 2020&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2020.aacl-main.61/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ej0cl6/SportsSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate Football News from Live Webcast Scripts Based on Character-CNN with Five Strokes&lt;/strong&gt; &lt;em&gt;Xue-Qiang Lv, Xin-Dong You, Wen-Chao Wang, Jian-She Zhou&lt;/em&gt; &lt;a href=&#34;http://csroc.org.tw/journal/JOC31-1/JOC3101-21.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Content Selection for Real-time Sports News Construction from Commentary Texts&lt;/strong&gt; &lt;em&gt;Jin-ge Yao, Jianmin Zhang, Xiaojun Wan, Jianguo Xiao&lt;/em&gt; &lt;code&gt;INLG 2017&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/W17-3504/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Constructing Sports News from Live Text Commentary&lt;/strong&gt; &lt;em&gt;Jianmin Zhang, Jin-ge Yao, Xiaojun Wan&lt;/em&gt; &lt;code&gt;ACL 2016&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/P16-1129/&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sports News Generation from Live Webcast Scripts Based on Rules and Templates&lt;/strong&gt; &lt;em&gt;Maofu Liu, Qiaosong Qi, Huijun Hu, Han Ren&lt;/em&gt; &lt;code&gt;NLPCC 2016&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-50496-4_81&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research on Summary Sentences Extraction Oriented to Live Sports Text&lt;/strong&gt; &lt;em&gt;Liya Zhu, Wenchao Wang, Yujing Chen, Xueqiang Lv, Jianshe Zhou&lt;/em&gt; &lt;code&gt;NLPCC 2016&lt;/code&gt; &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-50496-4_72&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Scientific [TBD]&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comparative Graph-based Summarization of Scientific Papers Guided by Comparative Citations&lt;/strong&gt; &lt;em&gt;Jingqiang Chen, Chaoxiang Cai, Xiaorui Jiang, Kejia Chen&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.coling-1.522/&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; With the rapid growth of scientific papers, understanding the changes and trends in a research area is rather time-consuming. The first challenge is to find related and comparable articles for the research. Comparative citations compare co-cited papers in a citation sentence and can serve as good guidance for researchers to track a research area. We thus go through comparative citations to find comparable objects and build a comparative scientific summarization corpus (CSSC). And then, we propose the comparative graph-based summarization (CGSUM) method to create comparative summaries using citations as guidance. The comparative graph is constructed using sentences as nodes and three different relationships of sentences as edges. The relationship that sentences occur in the same paper is used to calculate the salience of sentences, the relationship that sentences occur in two different papers is used to calculate the difference between sentences, and the relationship that sentences are related to citations is used to calculate the commonality of sentences. Experiments show that CGSUM outperforms comparative baselines on CSSC and performs well on DUC2006 and DUC2007. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CSL: A Large-scale Chinese Scientific Literature Dataset&lt;/strong&gt; &lt;em&gt;Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao, Hui Zhang&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.05034&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/ydli-ai/CSL&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Scientific literature serves as a high-quality corpus, supporting a lot of Natural Language Processing (NLP) research. However, existing datasets are centered around the English language, which restricts the development of Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese Scientific Literature dataset, which contains the titles, abstracts, keywords and academic fields of 396k papers. To our knowledge, CSL is the first scientific document dataset in Chinese. The CSL can serve as a Chinese corpus. Also, this semi-structured data is a natural annotation that can constitute many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the performance of models across scientific domain tasks, i.e., summarization, keyword generation and text classification. We analyze the behavior of existing text-to-text models on the evaluation tasks and reveal the challenges for Chinese scientific NLP tasks, which provides a valuable reference for future research. Data and code are available at this https URL &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Document Scientific Summarization from a Knowledge Graph-Centric View&lt;/strong&gt; &lt;em&gt;Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang&lt;/em&gt; &lt;code&gt;COLING 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.04319&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/muguruzawang/KGSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;On Extractive Summarization for Profile-centric Neural Expert Search in Academia&lt;/strong&gt; &lt;em&gt;Rennan C. Lima, Rodrygo L. T. Santos&lt;/em&gt; &lt;code&gt;SIGIR 2022 Short&lt;/code&gt; &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3477495.3531713&#34;&gt;[pdf]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Identifying academic experts is crucial for the progress of science, enabling researchers to connect, form networks, and collaborate on the most pressing research problems. A key challenge for ranking experts in response to a query is how to infer their expertise from the publications they coauthored. Profile-centric approaches represent candidate experts by concatenating all their publications into a text-based profile. Despite offering a complete picture of each candidate&#39;s scientific output, such lengthy profiles make it inefficient to leverage state-of-the-art neural architectures for inferring expertise. To overcome this limitation, we investigate the suitability of extractive summarization as a mechanism to reduce candidate profiles for semantic encoding using Transformers. Our thorough experiments with a representative academic search test collection demonstrate the benefits of encoding summarized profiles for an improved expertise inference. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating a Structured Summary of Numerous Academic Papers: Dataset and Method&lt;/strong&gt; &lt;em&gt;Shuaiqi LIU, Jiannong Cao, Ruosong Yang, Zhiyuan Wen&lt;/em&gt; &lt;code&gt;IJCAI 2022&lt;/code&gt; &lt;a href=&#34;https://www.ijcai.org/proceedings/2022/591&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/StevenLau6/BigSurvey&#34;&gt;[data]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Writing a survey paper on one research topic usually needs to cover the salient content from numerous related papers, which can be modeled as a multi-document summarization (MDS) task. Existing MDS datasets usually focus on producing the structureless summary covering a few input documents. Meanwhile, previous structured summary generation works focus on summarizing a single document into a multi-section summary. These existing datasets and methods cannot meet the requirements of summarizing numerous academic papers into a structured summary. To deal with the scarcity of available data, we propose BigSurvey, the first large-scale dataset for generating comprehensive summaries of numerous academic papers on each topic. We collect target summaries from more than seven thousand survey papers and utilize their 430 thousand reference papers’ abstracts as input documents. To organize the diverse content from dozens of input documents and ensure the efficiency of processing long text sequences, we propose a summarization method named category-based alignment and sparse transformer (CAST). The experimental results show that our CAST method outperforms various advanced summarization methods. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TSTR: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation&lt;/strong&gt; &lt;em&gt;Sajad Sotudeh, Nazli Goharian&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.25/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/Georgetown-IR-Lab/TSTRSum&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Many scientific papers such as those in arXiv and PubMed data collections have abstracts with varying lengths of 50-1000 words and average length of approximately 200 words, where longer abstracts typically convey more information about the source paper. Up to recently, scientific summarization research has typically focused on generating short, abstract-like summaries following the existing datasets used for scientific summarization. In domains where the source text is relatively long-form, such as in scientific documents, such summary is not able to go beyond the general and coarse overview and provide salient information from the source document. The recent interest to tackle this problem motivated curation of scientific datasets, arXiv-Long and PubMed-Long, containing human-written summaries of 400-600 words, hence, providing a venue for research in generating long/extended summaries. Extended summaries facilitate a faster read while providing details beyond coarse information. In this paper, we propose TSTR, an extractive summarizer that utilizes the introductory information of documents as pointers to their salient information. The evaluations on two existing large-scale extended summarization datasets indicate statistically significant improvement in terms of Rouge and average Rouge (F1) scores (except in one case) as compared to strong baselines and state-of-the-art. Comprehensive human evaluations favor our generated extended summaries in terms of cohesion and completeness. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;X-SCITLDR: Cross-Lingual Extreme Summarization of Scholarly Documents&lt;/strong&gt; &lt;em&gt;Sotaro Takeshita, Tommaso Green, Niklas Friedrich, Kai Eckert, Simone Paolo Ponzetto&lt;/em&gt; &lt;code&gt;JCDL 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.15051&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/sobamchan/xscitldr&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Target-aware Abstractive Related Work Generation with Contrastive Learning&lt;/strong&gt; &lt;em&gt;Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang&lt;/em&gt; &lt;code&gt;SIGIR 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.13339&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/iriscxy/Target-aware-RWG&#34;&gt;[code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CiteSum: Citation Text-guided Scientific Extreme Summarization and Low-resource Domain Adaptation&lt;/strong&gt; &lt;em&gt;Yuning Mao, Ming Zhong, Jiawei Han&lt;/em&gt; &lt;a href=&#34;https://arxiv.org/pdf/2205.06207.pdf&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/morningmoni/CiteSum&#34;&gt;[data]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Post-Editing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;An Exploration of Post-Editing Effectiveness in Text Summarization&lt;/strong&gt; &lt;em&gt;Vivian Lai, Alison Smith-Renner, Ke Zhang, Ruijia Cheng, Wenjuan Zhang, Joel Tetreault, Alejandro Jaimes&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.35/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/vivlai/post-editing-effectiveness-summarization&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatic summarization methods are efficient but can suffer from low quality. In comparison, manual summarization is expensive but produces higher quality. Can humans and AI collaborate to improve summarization performance? In similar text generation tasks (e.g., machine translation), human-AI collaboration in the form of &#34;post-editing&#34; AI-generated text reduces human workload and improves the quality of AI output. Therefore, we explored whether post-editing offers advantages in text summarization. Specifically, we conducted an experiment with 72 participants, comparing post-editing provided summaries with manual summarization for summary quality, human efficiency, and user experience on formal (XSum news) and informal (Reddit posts) text. This study sheds valuable insights on when post-editing is useful for text summarization: it helped in some cases (e.g., when participants lacked domain knowledge) but not in others (e.g., when provided summaries include inaccurate information). Participants&#39; different editing strategies and needs for assistance offer implications for future human-AI summarization systems. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Human&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;What Makes a Good and Useful Summary? Incorporating Users in Automatic Summarization Research&lt;/strong&gt; &lt;em&gt;Maartje Ter Hoeve, Julia Kiseleva, Maarten Rijke&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.4/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://github.com/maartjeth/survey_useful_summarization&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatic text summarization has enjoyed great progress over the years and is used in numerous applications, impacting the lives of many. Despite this development, there is little research that meaningfully investigates how the current research focus in automatic summarization aligns with users’ needs. To bridge this gap, we propose a survey methodology that can be used to investigate the needs of users of automatically generated summaries. Importantly, these needs are dependent on the target group. Hence, we design our survey in such a way that it can be easily adjusted to investigate different user groups. In this work we focus on university students, who make extensive use of summaries during their studies. We find that the current research directions of the automatic summarization community do not fully align with students’ needs. Motivated by our findings, we present ways to mitigate this mismatch in future research on automatic summarization: we propose research directions that impact the design, the development and the evaluation of automatically generated summaries. &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mapping the Design Space of Human-AI Interaction in Text Summarization&lt;/strong&gt; &lt;em&gt;Ruijia Cheng, Alison Smith-Renner, Ke Zhang, Joel Tetreault, Alejandro Jaimes-Larrarte&lt;/em&gt; &lt;code&gt;NAACL 2022&lt;/code&gt; &lt;a href=&#34;https://aclanthology.org/2022.naacl-main.33/&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;&#34;&gt;[code]&lt;/a&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;[Abs]&lt;/summary&gt; Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans’ roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.&#xA;  &lt;/details&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;  &#xA;&lt;h2&gt;Tutorial&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Beyond Opinion Mining: Summarizing Opinions of Customer Reviews&lt;/strong&gt; &lt;em&gt;Reinald Kim Amplayo, Arthur Bražinskas, Yoshi Suhara, Xiaolan Wang, Bing Liu&lt;/em&gt; &lt;code&gt;SIGIR Tutorial 2022&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2206.01543&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>