<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-08T01:47:10Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>optimass/continual_learning_papers</title>
    <updated>2023-02-08T01:47:10Z</updated>
    <id>tag:github.com,2023-02-08:/optimass/continual_learning_papers</id>
    <link href="https://github.com/optimass/continual_learning_papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Relevant papers in Continual Learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Continual Learning Literature&lt;/h1&gt; &#xA;&lt;p&gt;This repository is maintained by Massimo Caccia and Timoth√©e Lesort don&#39;t hesitate to send us an email to collaborate or fix some entries ({massimo.p.caccia , t.lesort} at gmail.com). The automation script of this repo is adapted from &lt;a href=&#34;https://github.com/TLESORT/Automatic_Awesome_Bibliography&#34;&gt;Automatic_Awesome_Bibliography&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For contributing to the repository please follow the process &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/scripts/README.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can directly use our bib.tex in overleaf &lt;a href=&#34;https://www.overleaf.com/project/606f5acf8bf59dcda3e66f9e&#34;&gt;with this link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#classics&#34;&gt;Classics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#empirical-study&#34;&gt;Empirical Study&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#surveys&#34;&gt;Surveys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#influentials&#34;&gt;Influentials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#new-settings-or-metrics&#34;&gt;New Settings or Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#general-continual-learning-methods-(sl-and-rl)&#34;&gt;General Continual Learning Methods (SL and RL)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#task-agnostic-continual-learning&#34;&gt;Task-Agnostic Continual Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#regularization-methods&#34;&gt;Regularization Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#distillation-methods&#34;&gt;Distillation Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#rehearsal-methods&#34;&gt;Rehearsal Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#generative-replay-methods&#34;&gt;Generative Replay Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#dynamic-architectures-or-routing-methods&#34;&gt;Dynamic Architectures or Routing Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#hybrid-methods&#34;&gt;Hybrid Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#continual-few-shot-learning&#34;&gt;Continual Few-Shot Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#meta-continual-learning&#34;&gt;Meta-Continual Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#lifelong-reinforcement-learning&#34;&gt;Lifelong Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#task-agnostic-lifelong-reinforcement-learning&#34;&gt;Task-Agnostic Lifelong Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#continual-generative-modeling&#34;&gt;Continual Generative Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#biologically-inspired&#34;&gt;Biologically-Inspired&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#miscellaneous&#34;&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#thesis&#34;&gt;Thesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#libraries&#34;&gt;Libraries&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#workshops&#34;&gt;Workshops&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Classics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942&#34;&gt;&lt;strong&gt;Catastrophic forgetting in connectionist networks&lt;/strong&gt;&lt;/a&gt; , (1999) by &lt;em&gt;French, Robert M.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1335-L1349&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.3723&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;&lt;strong&gt;Lifelong robot learning&lt;/strong&gt;&lt;/a&gt; , (1995) by &lt;em&gt;Thrun, Sebastian and Mitchell, Tom M&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L423-L432&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Argues knowledge transfer is essential if robots are to learn control with moderate learning times&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1080/09540099550039318&#34;&gt;&lt;strong&gt;Catastrophic Forgetting, Rehearsal and Pseudorehearsal&lt;/strong&gt;&lt;/a&gt; , (1995) by * Anthony Robins * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2075-L2088&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0079742108605368&#34;&gt;&lt;strong&gt;Catastrophic interference in connectionist networks: The sequential learning problem&lt;/strong&gt;&lt;/a&gt; , (1989) by &lt;em&gt;McCloskey, Michael and Cohen, Neal J&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1015-L1025&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces CL and reveals the catastrophic forgetting problem&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Empirical Study&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02577&#34;&gt;&lt;strong&gt;Effects of Auxiliary Knowledge on Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2022) by &lt;em&gt;Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Bonicelli, Lorenzo, Boschini, Matteo, Calderara, Simone and Spampinato, Concetto&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L158-L165&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9412614&#34;&gt;&lt;strong&gt;Rethinking Experience Replay: a Bag of Tricks for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2021) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L257-L268&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0893608020304202&#34;&gt;&lt;strong&gt;A comprehensive study of class incremental learning algorithms for visual tasks&lt;/strong&gt;&lt;/a&gt; , (Neural Networks 2021) by &lt;em&gt;Eden Belouadah, Adrian Popescu and Ioannis Kanellos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2808-L2820&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Online Continual Learning in Image Classification: An Empirical Survey&lt;/strong&gt;, (2021) by &lt;em&gt;Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim and Scott Sanner&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2822-L2830&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GDumb: A simple approach that questions our progress in continual learning&lt;/strong&gt;, (ECCV 2020) by &lt;em&gt;Prabhu, Ameya, Torr, Philip HS and Dokania, Puneet K&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L282-L290&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introduces a super simple methods that outperforms almost all methods in all of the CL benchmarks. We need new better benchamrks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.08383&#34;&gt;&lt;strong&gt;Continual learning: A comparative study on how to defy forgetting in classification tasks&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh and Tinne Tuytelaars&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L508-L517&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive empirical study of CL methods (in the multi-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.07734&#34;&gt;&lt;strong&gt;Three scenarios for continual learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;van de Ven, Gido M and Tolias, Andreas S&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1047-L1054&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;An extensive review of CL methods in three different scenarios (task-, domain-, and class-incremental learning)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous learning in single-incremental-task scenarios&lt;/strong&gt;, (Neural Networks 2019) by &lt;em&gt;Maltoni, Davide and Lomonaco, Vincenzo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1362-L1371&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.09733&#34;&gt;&lt;strong&gt;Towards Robust Evaluations of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2018) by &lt;em&gt;Farquhar, Sebastian and Gal, Yarin&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L435-L442&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes desideratas and reexamines the evaluation protocol&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Catastrophic forgetting: still a problem for DNNs&lt;/strong&gt;, (ICANN 2018) by &lt;em&gt;Pf&#34;ulb, B, Gepperth, A, Abdullah, S and Krawczyk, A&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2226-L2232&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Measuring Catastrophic Forgetting in Neural Networks&lt;/strong&gt;, (2017) by &lt;em&gt;Kemker, R., McClure, M., Abitino, A. and Hayes, T. and Kanan, C.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1374-L1387&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v78/lomonaco17a.html&#34;&gt;&lt;strong&gt;CORe50: a New Dataset and Benchmark for Continuous Object Recognition&lt;/strong&gt;&lt;/a&gt; , (CoRL 2017) by &lt;em&gt;Vincenzo Lomonaco and Davide Maltoni&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1389-L1404&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6211&#34;&gt;&lt;strong&gt;An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2013) by &lt;em&gt;Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A. and Bengio, Y.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L492-L505&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Investigates CF in neural networks&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Surveys&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06758&#34;&gt;&lt;strong&gt;An Investigation of Replay-based Approaches for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCNN 2021) by &lt;em&gt;Bagus, Benedikt and Gepperth, Alexander&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3295-L3304&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Embracing Change: Continual Learning in Deep Neural Networks&lt;/strong&gt;, (2020) by &lt;em&gt;Hadsell, Raia, Rao, Dushyant, Rusu, Andrei and Pascanu, Razvan&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L167-L177&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Continual Reinforcement Learning: A Review and Perspectives&lt;/strong&gt;, (2020) by &lt;em&gt;Khimya Khetarpal, Matthew Riemer, Irina Rish and Doina Precup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L293-L301&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A review on continual reinforcement learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1566253519307377&#34;&gt;&lt;strong&gt;Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges&lt;/strong&gt;&lt;/a&gt; , (Information Fusion 2020) by &lt;em&gt;Timoth√©e Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia D√≠az-Rodr√≠guez&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1321-L1332&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.01797&#34;&gt;&lt;strong&gt;A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Mundt, Martin, Hong, Yong Won, Pliushch, Iuliia and Ramesh, Visvanathan&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2595-L2602&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;propose a consolidated view to bridge continual learning, active learning and open set recognition in DNNs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.574&#34;&gt;&lt;strong&gt;Continual Lifelong Learning in Natural Language Processing: A Survey&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Magdalena Biesialska, Katarzyna Biesialska, Marta R. Costa-juss√†&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2773-L2785&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;An extensive review of CL in Natural Language Processing (NLP)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0893608019300231&#34;&gt;&lt;strong&gt;Continual lifelong learning with neural networks: A review&lt;/strong&gt;&lt;/a&gt; , (Neural Networks 2019) by &lt;em&gt;German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan and Stefan Wermter&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L520-L531&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;An extensive review of CL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01418129&#34;&gt;&lt;strong&gt;Incremental learning algorithms and applications&lt;/strong&gt;&lt;/a&gt; , (2016) by &lt;em&gt;Gepperth, Alexander and Hammer, Barbara&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1178-L1189&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A survey on incremental learning and the various applications fields&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Influentials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.00420&#34;&gt;&lt;strong&gt;Efficient Lifelong Learning with A-GEM&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Chaudhry, Arslan, Ranzato, Marc‚ÄôAurelio, Rohrbach, Marcus and Elhoseiny, Mohamed&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L445-L452&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;More efficient GEM; Introduces online continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.09733&#34;&gt;&lt;strong&gt;Towards Robust Evaluations of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2018) by &lt;em&gt;Farquhar, Sebastian and Gal, Yarin&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L435-L442&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes desideratas and reexamines the evaluation protocol&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.05202&#34;&gt;&lt;strong&gt;Continual Learning in Practice&lt;/strong&gt;&lt;/a&gt; , (NeurIPS Workshop 2018) by &lt;em&gt;Diethe, Tom, Borchert, Tom, Thereska, Eno, Pigem, Borja de Balle and Lawrence, Neil&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2327-L2334&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a reference architecture for a continual learning system&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34;&gt;&lt;strong&gt;Overcoming catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (PNAS 2017) by &lt;em&gt;Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L455-L463&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf&#34;&gt;&lt;strong&gt;Gradient Episodic Memory for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Lopez-Paz, David and Ranzato, Marc-Aurelio&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L467-L477&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A model that alliviates CF via constrained optimization&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08690&#34;&gt;&lt;strong&gt;Continual learning with deep generative replay&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Shin, Hanul, Lee, Jung Kwon, Kim, Jaehong and Kim, Jiwon&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L481-L489&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces generative replay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6211&#34;&gt;&lt;strong&gt;An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2013) by &lt;em&gt;Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A. and Bengio, Y.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L492-L505&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Investigates CF in neural networks&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;New Settings or Metrics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chandar-lab.github.io/IIRC/&#34;&gt;&lt;strong&gt;IIRC: Incremental Implicitly-Refined Classification&lt;/strong&gt;&lt;/a&gt; , (CVPR 2021) by &lt;em&gt;Mohamed Abdelsalam, Mojtaba Faramarzi, Shagun Sodhani and Sarath Chandar&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2761-L2769&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A setup and benchmark to evaluate lifelong learning models in more real-life aligned scenarios.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lebrice/Sequoia&#34;&gt;&lt;strong&gt;Sequoia - Towards a Systematic Organization of Continual Learning Research&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Matthew Riemer, Pau Rodriguez, Julio Hurtado, Khimya Khetarpal, Timoth√©e Lesort, Laurent Charlin, Irina Rish and Massimo Caccia&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2951-L2960&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library that unifies Continual Supervised and Continual Reinforcement Learning research&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.04546&#34;&gt;&lt;strong&gt;Wandering Within a World: Online Contextualized Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer and Richard S. Zemel&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L325-L333&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;proposes a new continual few-shot setting where spacial and temporal context can be leveraged to and unseen classes need to be predicted&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.11967&#34;&gt;&lt;strong&gt;Defining Benchmarks for Continual Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Antoniou, Antreas, Patacchiola, Massimiliano, Ochal, Mateusz and Storkey, Amos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L367-L374&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;(title is a good enough summary)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05856&#34;&gt;&lt;strong&gt;Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Caccia, Massimo, Rodriguez, Pau, Ostapenko, Oleksiy, Normandin, Fabrice, Lin, Min, Caccia, Lucas, Laradji, Issam, Rish, Irina, Lacoste, Alexandre, Vazquez, David and Charlin, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1473-L1480&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a new approach to CL evaluation more aligned with real-life applications, bringing CL closer to Online Learning and Open-World learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=rklnDgHtDS&#34;&gt;&lt;strong&gt;Compositional Language Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Yuanpeng Li, Liang Zhao, Kenneth Church and Mohamed Elhoseiny&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1551-L1558&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;method for compositional continual learning of sequence-to-sequence models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.01797&#34;&gt;&lt;strong&gt;A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Mundt, Martin, Hong, Yong Won, Pliushch, Iuliia and Ramesh, Visvanathan&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2595-L2602&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;propose a consolidated view to bridge continual learning, active learning and open set recognition in DNNs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Don&#39;t forget, there is more than forgetting: new metrics for Continual Learning&lt;/strong&gt;, (arXiv 2018) by &lt;em&gt;D{&#39;\i}az-Rodr{&#39;\i}guez, Natalia, Lomonaco, Vincenzo, Filliat, David and Maltoni, Davide&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2942-L2948&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introduces a CL score that takes more than just forgetting into account&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;General Continual Learning Methods (SL and RL)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34;&gt;&lt;strong&gt;Overcoming catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (PNAS 2017) by &lt;em&gt;Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L455-L463&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1701.08734&#34;&gt;&lt;strong&gt;PathNet: Evolution Channels Gradient Descent in Super Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1796-L1816&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Task-Agnostic Continual Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12772&#34;&gt;&lt;strong&gt;Task-agnostic Continual Learning with Hybrid Probabilistic Models&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Polina Kirichenko, Mehrdad Farajtabar, Dushyant Rao, Balaji Lakshminarayanan, Nir Levine, Ang Li, Huiyi Hu, Andrew Gordon Wilson and Razvan Pascanu&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L226-L235&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html&#34;&gt;&lt;strong&gt;Learning where to learn: Gradient sparsity in meta and continual learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Von Oswald, Johannes, Zhao, Dominic, Kobayashi, Seijin, Schug, Simon, Caccia, Massimo, Zucchet, Nicolas and Sacramento, Jo{~a}o&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L247-L255&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HklUCCVKDB&#34;&gt;&lt;strong&gt;Uncertainty-guided Continual Learning with Bayesian Neural Networks&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell and Marcus Rohrbach&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L557-L564&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Uses Bayes by Backprop for variational Continual Learning.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05856&#34;&gt;&lt;strong&gt;Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Caccia, Massimo, Rodriguez, Pau, Ostapenko, Oleksiy, Normandin, Fabrice, Lin, Min, Caccia, Lucas, Laradji, Issam, Rish, Irina, Lacoste, Alexandre, Vazquez, David and Charlin, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1473-L1480&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a new approach to CL evaluation more aligned with real-life applications, bringing CL closer to Online Learning and Open-World learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;iTAML: An Incremental Task-Agnostic Meta-learning Approach&lt;/strong&gt;, (CVPR 2020) by &lt;em&gt;Rajasegaran, Jathushan, Khan, Salman, Hayat, Munawar, Khan, Fahad Shahbaz and Shah, Mubarak&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2411-L2418&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.14481.pdf&#34;&gt;&lt;strong&gt;Continual Unsupervised Representation Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu and Raia Hadsell&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L942-L951&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces unsupervised continual learning (no task label and no task boundaries)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.00689.pdf&#34;&gt;&lt;strong&gt;A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Lee, Soochan, Ha, Junsoo, Zhang, Dongsu and Kim, Gunhee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1540-L1547&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;This paper introduces expansion-based approach for task-free continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.10123.pdf&#34;&gt;&lt;strong&gt;Task Agnostic Continual Learning Using Online Variational Bayes&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Chen Zeno, Itay Golan, Elad Hoffer and Daniel Soudry&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L580-L589&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces an optimizer for CL that relies on closed form updates of mu and sigma of BNN; introduce label trick for class learning (single-head) but warning: it isn&#39;t really task-agnostic&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Regularization Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.01834&#34;&gt;&lt;strong&gt;Continual Learning in Deep Networks: an Analysis of the Last Layer&lt;/strong&gt;&lt;/a&gt; , (arXiv 2021) by &lt;em&gt;Lesort, Timoth{&#39;e}e, George, Thomas and Rish, Irina&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3245-L3252&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SJlsFpVtDB&#34;&gt;&lt;strong&gt;Continual Learning with Bayesian Neural Networks for Non-Stationary Data&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt and Stephan G√ºnnemann&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L888-L895&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;continual learning for non-stationary data using Bayesian neural networks and memory-based online variational Bayes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.02099&#34;&gt;&lt;strong&gt;Improving and Understanding Variational Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Siddharth Swaroop, Cuong V. Nguyen, Thang D. Bui and Richard E. Turner&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L545-L553&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Improved results and interpretation of VCL.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/8690-uncertainty-based-continual-learning-with-adaptive-regularization.pdf&#34;&gt;&lt;strong&gt;Uncertainty-based Continual Learning with Adaptive Regularization&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Ahn, Hongjoon, Cha, Sungmin, Lee, Donggyu and Moon, Taesup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L567-L577&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces VCL with uncertainty measured for neurons instead of weights.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.11356&#34;&gt;&lt;strong&gt;Functional Regularisation for Continual Learning with Gaussian Processes&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Titsias, Michalis K, Schwarz, Jonathan, Matthews, Alexander G de G, Pascanu, Razvan and Teh, Yee Whye&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1351-L1358&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;functional regularisation for Continual Learning: avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.10123.pdf&#34;&gt;&lt;strong&gt;Task Agnostic Continual Learning Using Online Variational Bayes&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Chen Zeno, Itay Golan, Elad Hoffer and Daniel Soudry&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L580-L589&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces an optimizer for CL that relies on closed form updates of mu and sigma of BNN; introduce label trick for class learning (single-head) but warning: it isn&#39;t really task-agnostic&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=B1al7jg0b&#34;&gt;&lt;strong&gt;Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation&lt;/strong&gt;&lt;/a&gt; , (ICLR 2018) by &lt;em&gt;Xu He and Herbert Jaeger&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L621-L628&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Conceptor-Aided Backprop (CAB): gradients are shielded by conceptors against degradation of previously learned tasks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v80/serra18a.html&#34;&gt;&lt;strong&gt;Overcoming Catastrophic Forgetting with Hard Attention to the Task&lt;/strong&gt;&lt;/a&gt; , (ICML 2018) by &lt;em&gt;Serra, Joan, Suris, Didac, Miron, Marius and Karatzoglou, Alexandros&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L641-L657&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing a hard attention idea with binary masks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.10112&#34;&gt;&lt;strong&gt;Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence&lt;/strong&gt;&lt;/a&gt; , (ECCV 2018) by &lt;em&gt;Chaudhry, Arslan, Dokania, Puneet K, Ajanthan, Thalaiyasingam and Torr, Philip HS&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L660-L667&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formalizes the shortcomings of multi-head evaluation, as well as the importance of replay in single-head setup. Presenting an improved version of EWC.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.10628&#34;&gt;&lt;strong&gt;Variational Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2018) by &lt;em&gt;Cuong V. Nguyen, Yingzhen Li, Thang D. Bui and Richard E. Turner&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L691-L698&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.06370&#34;&gt;&lt;strong&gt;Progress &amp;amp; compress: A scalable framework for continual learning&lt;/strong&gt;&lt;/a&gt; , (ICML 2018) by &lt;em&gt;Schwarz, Jonathan, Luketina, Jelena, Czarnecki, Wojciech M, Grabska-Barwinska, Agnieszka, Teh, Yee Whye, Pascanu, Razvan and Hadsell, Raia&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L702-L709&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A new P\&amp;amp;C architecture; online EWC for keeping the knowledge about the previous task, knowledge for keeping the knowledge about the current task (Multi-head setting, RL)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Online structured laplace approximations for overcoming catastrophic forgetting&lt;/strong&gt;, (NeurIPS 2018) by &lt;em&gt;Ritter, Hippolyt, Botev, Aleksandar and Barber, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2056-L2063&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.10644&#34;&gt;&lt;strong&gt;Facilitating Bayesian Continual Learning by Natural Gradients and Stein Gradients&lt;/strong&gt;&lt;/a&gt; , (NeurIPS Workshop 2018) by &lt;em&gt;Chen, Yu, Diethe, Tom and Lawrence, Neil&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2337-L2344&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Improves on VCL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34;&gt;&lt;strong&gt;Overcoming catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (PNAS 2017) by &lt;em&gt;Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L455-L463&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1711.09601&#34;&gt;&lt;strong&gt;Memory Aware Synapses: Learning what (not) to forget&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach and Tinne Tuytelaars&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L673-L686&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Importance of parameter measured based on their contribution to change in the learned prediction function&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v70/zenke17a.html&#34;&gt;&lt;strong&gt;Continual Learning Through Synaptic Intelligence&lt;/strong&gt;&lt;/a&gt; , (ICML 2017) by *Zenke, Friedeman, Poole, Ben and Ganguli, Surya * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L712-L727&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Synaptic Intelligence (SI). Importance of parameter measured based on their contribution to change in the loss. &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overcoming catastrophic forgetting by incremental moment matching&lt;/strong&gt;, (NeurIPS 2017) by &lt;em&gt;Lee, Sang-Woo, Kim, Jin-Hwa, Jun, Jaehyun, Ha, Jung-Woo and Zhang, Byoung-Tak&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1925-L1932&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Distillation Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.00766&#34;&gt;&lt;strong&gt;Class-Incremental Continual Learning into the eXtended DER-verse&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Buzzega, Pietro, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L10-L19&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.00388&#34;&gt;&lt;strong&gt;Transfer without Forgetting&lt;/strong&gt;&lt;/a&gt; , (ECCV 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Porrello, Angelo, Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Spampinato, Concetto and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L46-L53&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04215&#34;&gt;&lt;strong&gt;Self-Supervised Models are Continual Learners&lt;/strong&gt;&lt;/a&gt; , (CVPR 2022) by &lt;em&gt;Fini, Enrico, da Costa, Victor G Turrisi, Alameda-Pineda, Xavier, Ricci, Elisa, Alahari, Karteek and Mairal, Julien&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3430-L3437&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Explores Continual Self-Supervised Learning and proposes a simple and effective feature distillation method&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.14098&#34;&gt;&lt;strong&gt;Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2022) by &lt;em&gt;Yang, Guanglei, Fini, Enrico, Xu, Dan, Rota, Paolo, Ding, Mingli, Nabi, Moin, Alameda-Pineda, Xavier and Ricci, Elisa&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3440-L3448&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.00432&#34;&gt;&lt;strong&gt;Continual Attentive Fusion for Incremental Learning in Semantic Segmentation&lt;/strong&gt;&lt;/a&gt; , (TMM 2022) by &lt;em&gt;Yang, Guanglei, Fini, Enrico, Xu, Dan, Rota, Paolo, Ding, Mingli, Hao, Tang, Alameda-Pineda, Xavier and Ricci, Elisa&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3450-L3458&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf&#34;&gt;&lt;strong&gt;Dark Experience for General Continual Learning: a Strong, Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2020) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo, Abati, Davide and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L270-L280&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730715.pdf&#34;&gt;&lt;strong&gt;Online Continual Learning under Extreme Memory Constraints&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Fini, Enrico, Lathuili√®re, St√®phane, Sangineto, Enver, Nabi, Moin and Ricci, Elisa&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2497-L2504&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces Memory-Constrained Online Continual Learning, a setting where no information can be transferred between tasks, and proposes a distillation-based solution (Batch-level Distillation)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650086.pdf&#34;&gt;&lt;strong&gt;PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Douillard, Arthur, Cord, Matthieu, Ollion, Charles, Robert, Thomas and Valle, Eduardo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2566-L2573&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Novel knowledge distillation that trades efficiently rigidity and plasticity to learn large amount of small tasks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.12648&#34;&gt;&lt;strong&gt;Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild&lt;/strong&gt;&lt;/a&gt; , (ICCV 2019) by &lt;em&gt;Lee, Kibok, Lee, Kimin, Shin, Jinwoo and Lee, Honglak&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1057-L1065&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing global distillation loss and balanced finetuning; leveraging unlabeled data in the open world setting (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34;&gt;&lt;strong&gt;Large scale incremental learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2019) by &lt;em&gt;Wu, Yue, Chen, Yinpeng, Wang, Lijuan, Ye, Yuancheng, Liu, Zicheng, Guo, Yandong and Fu, Yun&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1068-L1076&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing bias parameters to the last fully connected layer to resolve the data imbalance issue (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continual Reinforcement Learning deployed in Real-life using PolicyDistillation and Sim2Real Transfer&lt;/strong&gt;, (ICML Workshop 2019) by *Kalifou, Ren√© Traor√©, Caselles-Dupr√©, Hugo, Lesort, Timoth√©e, Sun, Te, Diaz-Rodriguez, Natalia and Filliat, David * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1253-L1259&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34;&gt;&lt;strong&gt;Lifelong learning via progressive distillation and retrospection&lt;/strong&gt;&lt;/a&gt; , (ECCV 2018) by &lt;em&gt;Hou, Saihui, Pan, Xinyu, Change Loy, Chen, Wang, Zilei and Lin, Dahua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1079-L1087&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing an expert of the current task in the knowledge distillation method (Multi-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.09536&#34;&gt;&lt;strong&gt;End-to-end incremental learning&lt;/strong&gt;&lt;/a&gt; , (ECCV 2018) by &lt;em&gt;Castro, Francisco M, Marin-Jimenez, Manuel J, Guil, Nicolas, Schmid, Cordelia and Alahari, Karteek&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1090-L1098&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Finetuning the last fully connected layer with a balanced dataset to resolve the data imbalance issue (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.09282&#34;&gt;&lt;strong&gt;Learning without forgetting&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2017) by &lt;em&gt;Li, Zhizhong and Hoiem, Derek&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L730-L738&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Functional regularization through distillation (keeping the output of the updated network on the new data close to the output of the old network on the new data)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34;&gt;&lt;strong&gt;icarl: Incremental classifier and representation learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2017) by &lt;em&gt;Rebuffi, Sylvestre-Alvise, Kolesnikov, Alexander, Sperl, Georg and Lampert, Christoph H&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1101-L1109&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Binary cross-entropy loss for representation learning \&amp;amp; exemplar memory (or coreset) for replay (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Rehearsal Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.06443&#34;&gt;&lt;strong&gt;On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2022) by &lt;em&gt;Bonicelli, Lorenzo, Boschini, Matteo, Porrello, Angelo, Spampinato, Concetto and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1-L8&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.00766&#34;&gt;&lt;strong&gt;Class-Incremental Continual Learning into the eXtended DER-verse&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Buzzega, Pietro, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L10-L19&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06552&#34;&gt;&lt;strong&gt;Continual semi-supervised learning through contrastive interpolation consistency&lt;/strong&gt;&lt;/a&gt; , (PRL 2022) by &lt;em&gt;Boschini, Matteo, Buzzega, Pietro, Bonicelli, Lorenzo, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L33-L44&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.00388&#34;&gt;&lt;strong&gt;Transfer without Forgetting&lt;/strong&gt;&lt;/a&gt; , (ECCV 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Porrello, Angelo, Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Spampinato, Concetto and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L46-L53&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02577&#34;&gt;&lt;strong&gt;Effects of Auxiliary Knowledge on Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2022) by &lt;em&gt;Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Bonicelli, Lorenzo, Boschini, Matteo, Calderara, Simone and Spampinato, Concetto&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L158-L165&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9412614&#34;&gt;&lt;strong&gt;Rethinking Experience Replay: a Bag of Tricks for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2021) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L257-L268&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HHSEKOnPvaO&#34;&gt;&lt;strong&gt;Graph-Based Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2021) by &lt;em&gt;Binh Tang and David S. Matteson&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2641-L2648&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Use graphs to link saved samples and improve the memory quality.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.00093.pdf&#34;&gt;&lt;strong&gt;Online Class-Incremental Continual Learning with Adversarial Shapley Value&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Dongsub Shim, Zheda Mai, Jihwan Jeong*, Scott Sanner, Hyunwoo Kim and Jongseong Jang&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2798-L2805&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Use Shapley Value adversarially to select which samples to relay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf&#34;&gt;&lt;strong&gt;Dark Experience for General Continual Learning: a Strong, Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2020) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo, Abati, Davide and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L270-L280&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GDumb: A simple approach that questions our progress in continual learning&lt;/strong&gt;, (ECCV 2020) by &lt;em&gt;Prabhu, Ameya, Torr, Philip HS and Dokania, Puneet K&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L282-L290&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introduces a super simple methods that outperforms almost all methods in all of the CL benchmarks. We need new better benchamrks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00487&#34;&gt;&lt;strong&gt;Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Timoth√©e Lesort&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L337-L346&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580409.pdf&#34;&gt;&lt;strong&gt;Imbalanced Continual Learning with Partitioning Reservoir Sampling&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Kim, Chris Dongjoo, Jeong, Jinseo and Kim, Gunhee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2470-L2477&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650086.pdf&#34;&gt;&lt;strong&gt;PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Douillard, Arthur, Cord, Matthieu, Ollion, Charles, Robert, Thomas and Valle, Eduardo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2566-L2573&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Novel knowledge distillation that trades efficiently rigidity and plasticity to learn large amount of small tasks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650681.pdf&#34;&gt;&lt;strong&gt;{REMIND Your Neural Network to Prevent Catastrophic Forgetting}&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Hayes, Tyler L., Kafle, Kushal, Shrestha, Robik and Acharya, Manoj and Kanan, Christopher&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2585-L2593&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.00420&#34;&gt;&lt;strong&gt;Efficient Lifelong Learning with A-GEM&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Chaudhry, Arslan, Ranzato, Marc‚ÄôAurelio, Rohrbach, Marcus and Elhoseiny, Mohamed&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L445-L452&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;More efficient GEM; Introduces online continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.07104&#34;&gt;&lt;strong&gt;Orthogonal Gradient Descent for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Mehrdad Farajtabar, Navid Azizan, Alex Mott and Ang Li&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L799-L808&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf&#34;&gt;&lt;strong&gt;Gradient based sample selection for online continual learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Aljundi, Rahaf, Lin, Min, Goujaud, Baptiste and Bengio, Yoshua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L812-L822&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;sample selection as a constraint reduction problem based on the constrained optimization view of continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf&#34;&gt;&lt;strong&gt;Online Continual Learning with Maximal Interfered Retrieval&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Aljundi, Rahaf and , Lucas, Belilovsky, Eugene, Caccia, Massimo, Lin, Min, Charlin, Laurent and Tuytelaars, Tinne&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L826-L837&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Controlled sampling of memories for replay to automatically rehearse on tasks currently undergoing the most forgetting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.08019&#34;&gt;&lt;strong&gt;Online Learned Continual Compression with Adaptative Quantization Module&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;Caccia, Lucas, Belilovsky, Eugene, Caccia, Massimo and Pineau, Joelle&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L841-L848&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Uses stacks of VQ-VAE modules to progressively compress the data stream, enabling better rehearsal&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34;&gt;&lt;strong&gt;Large scale incremental learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2019) by &lt;em&gt;Wu, Yue, Chen, Yinpeng, Wang, Lijuan, Ye, Yuancheng, Liu, Zicheng, Guo, Yandong and Fu, Yun&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1068-L1076&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing bias parameters to the last fully connected layer to resolve the data imbalance issue (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning a Unified Classifier Incrementally via Rebalancing&lt;/strong&gt;, (CVPR 2019) by &lt;em&gt;Hou, Saihui, Pan, Xinyu, Loy, Chen Change, Wang, Zilei and Lin, Dahua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1113-L1120&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continual Reinforcement Learning deployed in Real-life using PolicyDistillation and Sim2Real Transfer&lt;/strong&gt;, (ICML Workshop 2019) by *Kalifou, Ren√© Traor√©, Caselles-Dupr√©, Hugo, Lesort, Timoth√©e, Sun, Te, Diaz-Rodriguez, Natalia and Filliat, David * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1253-L1259&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.11682&#34;&gt;&lt;strong&gt;Experience replay for continual learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Rolnick, David, Ahuja, Arun, Schwarz, Jonathan, Lillicrap, Timothy and Wayne, Gregory&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1461-L1469&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf&#34;&gt;&lt;strong&gt;Gradient Episodic Memory for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Lopez-Paz, David and Ranzato, Marc-Aurelio&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L467-L477&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A model that alliviates CF via constrained optimization&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34;&gt;&lt;strong&gt;icarl: Incremental classifier and representation learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2017) by &lt;em&gt;Rebuffi, Sylvestre-Alvise, Kolesnikov, Alexander, Sperl, Georg and Lampert, Christoph H&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1101-L1109&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Binary cross-entropy loss for representation learning \&amp;amp; exemplar memory (or coreset) for replay (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1080/09540099550039318&#34;&gt;&lt;strong&gt;Catastrophic Forgetting, Rehearsal and Pseudorehearsal&lt;/strong&gt;&lt;/a&gt; , (1995) by * Anthony Robins * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2075-L2088&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generative Replay Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00487&#34;&gt;&lt;strong&gt;Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Timoth√©e Lesort&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L337-L346&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baicsworkshop.github.io/pdf/BAICS_8.pdf&#34;&gt;&lt;strong&gt;Brain-Like Replay For Continual Learning With Artificial Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;van de Ven, Gido M, Siegelmann, Hava T and Tolias, Andreas S&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L415-L421&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html&#34;&gt;&lt;strong&gt;Learning to remember: A synaptic plasticity driven framework for continual learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2019) by &lt;em&gt;Ostapenko, Oleksiy, Puscas, Mihai, Klein, Tassilo, Jahnichen, Patrick and Nabi, Moin&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L305-L313&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introdudes Dynamic generative memory (DGM) which relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01951954&#34;&gt;&lt;strong&gt;Generative Models from the perspective of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCNN 2019) by &lt;em&gt;Lesort, Timoth√©e, Caselles-Dupr√©, Hugo, Garcia-Ortiz, Michael, Goudou, Jean-Fran{\c c}ois and Filliat, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L954-L966&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive evaluation of CL methods for generative modeling&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=3367471.3367504&#34;&gt;&lt;strong&gt;Closed-loop Memory GAN for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCAI 2019) by &lt;em&gt;Rios, Amanda and Itti, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1261-L1275&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.12069&#34;&gt;&lt;strong&gt;Marginal replay vs conditional replay for continual learning&lt;/strong&gt;&lt;/a&gt; , (ICANN 2019) by &lt;em&gt;Lesort, Timoth√©e, Gepperth, Alexander, Stoian, Andrei and Filliat, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1406-L1415&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive evaluation of generative replay methods&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1809.10635&#34;&gt;&lt;strong&gt;Generative replay with feedback connections as a general strategy for continual learning&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Michiel van der Ven and Andreas S. Tolias&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L852-L860&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;smarter Generative Replay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08690&#34;&gt;&lt;strong&gt;Continual learning with deep generative replay&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Shin, Hanul, Lee, Jung Kwon, Kim, Jaehong and Kim, Jiwon&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L481-L489&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces generative replay&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dynamic Architectures or Routing Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnbot.2022.846219/full&#34;&gt;&lt;strong&gt;Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Iyer, Abhiram, Grewal, Karan, Velu, Akash, Souza, Lucas Oliveira, Forest, Jeremy and Ahmad, Subutai&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L77-L86&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;bio-inspired method which dynamically restrict and route information in a context-specific manner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11326&#34;&gt;&lt;strong&gt;DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion&lt;/strong&gt;&lt;/a&gt; , (arXiv 2021) by &lt;em&gt;Douillard, Arthur, Ram{&#39;e}, Alexandre, Couairon, Guillaume and Cord, Matthieu&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3313-L3320&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.14769&#34;&gt;&lt;strong&gt;Supermasks in superposition&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Wortsman, Mitchell, Ramanujan, Vivek, Liu, Rosanne, Kembhavi, Aniruddha, Rastegari, Mohammad, Yosinski, Jason and Farhadi, Ali&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L65-L74&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a binary mask over the network is inferred based on the input, and only the masked part of the network is used to train/infer&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1902.09432&#34;&gt;&lt;strong&gt;ORACLE: Order Robust Adaptive Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Jaehong Yoon and Saehoon Kim and Eunho Yang and Sung Ju Hwang&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L592-L608&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1904.00310&#34;&gt;&lt;strong&gt;Learn to Grow: {A} Continual Structure Learning Framework for Overcoming Catastrophic Forgetting&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Xilai Li and Yingbo Zhou and Tianfu Wu and Richard Socher and Caiming Xiong&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2206-L2224&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/doi/pdf/10.1073/pnas.1803839115&#34;&gt;&lt;strong&gt;Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Masse, Nicolas Y, Grant, Gregory D and Freedman, David J&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L90-L101&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a network trained to do CL where select subnetworks are used to learn each task; these subnetworks are chosen a priori&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=ryj0790hb&#34;&gt;&lt;strong&gt;Incremental Learning through Deep Adaptation&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Amir Rosenfeld and John K. Tsotsos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L753-L759&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Packnet: Adding multiple tasks to a single network by iterative pruning&lt;/strong&gt;, (CVPR 2018) by &lt;em&gt;Mallya, Arun and Lazebnik, Svetlana&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1123-L1130&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Piggyback: Adapting a single network to multiple tasks by learning to mask weights&lt;/strong&gt;, (ECCV 2018) by &lt;em&gt;Mallya, Arun, Davis, Dillon and Lazebnik, Svetlana&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1132-L1139&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.05202&#34;&gt;&lt;strong&gt;Continual Learning in Practice&lt;/strong&gt;&lt;/a&gt; , (NeurIPS Workshop 2018) by &lt;em&gt;Diethe, Tom, Borchert, Tom, Thereska, Eno, Pigem, Borja de Balle and Lawrence, Neil&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2327-L2334&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a reference architecture for a continual learning system&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Growing a brain: Fine-tuning by increasing model capacity&lt;/strong&gt;, (CVPR 2017) by &lt;em&gt;Wang, Yu-Xiong, Ramanan, Deva and Hebert, Martial&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1141-L1148&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1701.08734&#34;&gt;&lt;strong&gt;PathNet: Evolution Channels Gradient Descent in Super Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1796-L1816&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lifelong learning with dynamically expandable networks&lt;/strong&gt;, (arXiv 2017) by &lt;em&gt;Yoon, Jaehong, Yang, Eunho, Lee, Jeongtae and Hwang, Sung Ju&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2066-L2072&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.04671&#34;&gt;&lt;strong&gt;Progressive Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2016) by &lt;em&gt;Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G. and Soyer, H., Kirkpatrick, J., Kavukcuoglu, K. and Pascanu, R. and Hadsell, R.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L761-L776&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Each task have a specific model connected to the previous ones&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hybrid Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SJgwNerKvB&#34;&gt;&lt;strong&gt;Continual learning with hypernetworks&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Johannes von Oswald, Christian Henning, Jo√£o Sacramento and Benjamin F. Grewe&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1005-L1012&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Learning task-conditioned hypernetworks for continual learning as well as task embeddings; hypernetwors offers good model compression.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.06562&#34;&gt;&lt;strong&gt;Compacting, Picking and Growing for Unforgetting Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Hung, Ching-Yi, Tu, Cheng-Hao, Wu, Cheng-En, Chen, Chien-Hung, Chan, Yi-Ming and Chen, Chu-Song&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L534-L542&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. All enforced in an iterative manner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.00689.pdf&#34;&gt;&lt;strong&gt;A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Lee, Soochan, Ha, Junsoo, Zhang, Dongsu and Kim, Gunhee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1540-L1547&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;This paper introduces expansion-based approach for task-free continual learning&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Continual Few-Shot Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html&#34;&gt;&lt;strong&gt;Learning where to learn: Gradient sparsity in meta and continual learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Von Oswald, Johannes, Zhao, Dominic, Kobayashi, Seijin, Schug, Simon, Caccia, Massimo, Zucchet, Nicolas and Sacramento, Jo{~a}o&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L247-L255&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.04546&#34;&gt;&lt;strong&gt;Wandering Within a World: Online Contextualized Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer and Richard S. Zemel&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L325-L333&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;proposes a new continual few-shot setting where spacial and temporal context can be leveraged to and unseen classes need to be predicted&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.11967&#34;&gt;&lt;strong&gt;Defining Benchmarks for Continual Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Antoniou, Antreas, Patacchiola, Massimiliano, Ochal, Mateusz and Storkey, Amos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L367-L374&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;(title is a good enough summary)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05856&#34;&gt;&lt;strong&gt;Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Caccia, Massimo, Rodriguez, Pau, Ostapenko, Oleksiy, Normandin, Fabrice, Lin, Min, Caccia, Lucas, Laradji, Issam, Rish, Irina, Lacoste, Alexandre, Vazquez, David and Charlin, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1473-L1480&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a new approach to CL evaluation more aligned with real-life applications, bringing CL closer to Online Learning and Open-World learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.04695&#34;&gt;&lt;strong&gt;Learning from the Past: Continual Meta-Learning via Bayesian Graph Modeling&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Yadan Luo, Zi Huang, Zheng Zhang, Ziwei Wang, Mahsa Baktashmotlagh and Yang Yang&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L876-L885&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v97/finn19a.html&#34;&gt;&lt;strong&gt;Online Meta-Learning&lt;/strong&gt;&lt;/a&gt; , (ICML 2019) by &lt;em&gt;Finn, Chelsea, Rajeswaran, Aravind, Kakade, Sham and Levine, Sergey&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L898-L913&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;defines Online Meta-learning; propsoses Follow the Meta Leader (FTML) (~ Online MAML)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/9112-reconciling-meta-learning-and-continual-learning-with-online-mixtures-of-tasks.pdf&#34;&gt;&lt;strong&gt;Reconciling meta-learning and continual learning with online mixtures of tasks&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Jerfel, Ghassen, Grant, Erin, Griffiths, Tom and Heller, Katherine A&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L917-L927&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Meta-learns a tasks structure; continual adaptation via non-parametric prior&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HyxAfnA5tm&#34;&gt;&lt;strong&gt;Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Anusha Nagabandi, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L932-L939&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formulates an online learning procedure that uses SGD to update model parameters, and an EM with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distribution&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.05201&#34;&gt;&lt;strong&gt;Task Agnostic Continual Learning via Meta Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A. Rusu, Yee Whye Teh and Razvan Pascanu&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1028-L1036&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces What \&amp;amp; How framework; enables Task Agnostic CL with meta learned task inference&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Meta-Continual Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html&#34;&gt;&lt;strong&gt;Learning where to learn: Gradient sparsity in meta and continual learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Von Oswald, Johannes, Zhao, Dominic, Kobayashi, Seijin, Schug, Simon, Caccia, Massimo, Zucchet, Nicolas and Sacramento, Jo{~a}o&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L247-L255&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.13904&#34;&gt;&lt;strong&gt;La-MAML: Look-ahead Meta Learning for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Gunshi Gupta, Karmesh Yadav and Liam Paull&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L316-L322&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes an online replay-based meta-continual learning algorithm with learning-rate modulation to mitigate catastrophic forgetting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.09571&#34;&gt;&lt;strong&gt;Learning to Continually Learn&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Beaulieu, Shawn, Frati, Lapo, Miconi, Thomas, Lehman, Joel, Stanley, Kenneth O, Clune, Jeff and Cheney, Nick&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1431-L1438&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Follow-up of OML. Meta-learns an activation-gating function instead.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/8458-meta-learning-representations-for-continual-learning.pdf&#34;&gt;&lt;strong&gt;Meta-Learning Representations for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Javed, Khurram and White, Martha&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L863-L873&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces Learns how to continually learn (OML) i.e. learns how to do online updates without forgetting.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.04170.pdf&#34;&gt;&lt;strong&gt;Meta-learnt priors slow down catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;Spigler, Giacomo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1441-L1448&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Learning MAML in a Meta continual learning way slows down forgetting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=B1gTShAct7&#34;&gt;&lt;strong&gt;Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu and and Gerald Tesauro&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1452-L1459&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lifelong Reinforcement Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.03934&#34;&gt;&lt;strong&gt;A Study of Continual Learning Methods for Q-Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2022) by &lt;em&gt;Bagus, Benedikt and Gepperth, Alexander&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L55-L62&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Studies Q-Learning methods in CRL environments. When there&#39;s no task interference, (A-)GEM can outperform Experience Replay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=PVJ6j87gOHz&#34;&gt;&lt;strong&gt;Co{MPS}: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (ICLR 2022) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L126-L133&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Co{MPS} is a novel meta-policy search algorithm for task-agnostic continual RL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.14495&#34;&gt;&lt;strong&gt;Task-Agnostic Continual Reinforcement Learning: In Praise of a Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (arXiv 2022) by &lt;em&gt;Caccia, Massimo, Mueller, Jonas, Kim, Taesup, Charlin, Laurent and Fakoor, Rasool&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L136-L143&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;combines replay and an RNN to set a simple baseline for TACRL: shows that the baselines matches and surpasses previously thought upper bounds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=5XmLzdslFNN&#34;&gt;&lt;strong&gt;Modular Lifelong Reinforcement Learning via Neural Composition&lt;/strong&gt;&lt;/a&gt; , (ICLR 2022) by &lt;em&gt;Jorge A Mendez, Harm van Seijen and ERIC EATON&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L180-L187&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05742&#34;&gt;&lt;strong&gt;Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Steinparz, Christian, Schmied, Thomas, Paischer, Fabian, Dinu, Marius-Constantin, Patil, Vihang, Bitto-Nemling, Angela, Eghbal-zadeh, Hamid and Hochreiter, Sepp&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3460-L3467&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Detects changes and explores when and where they happen to recover from non-stationarity.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02940&#34;&gt;&lt;strong&gt;Same State, Different Task: Continual Reinforcement Learning without Interference&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Samuel Kessler, Jack Parker-Holder, Philip J. Ball, Stefan Zohren and Stephen J. Roberts&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L114-L122&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;learns multiple policies and cast policy-retrieval as a multi-arm bandit problem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04467&#34;&gt;&lt;strong&gt;CoMPS: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L189-L198&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HIGSa_3kOx3&#34;&gt;&lt;strong&gt;Reset-Free Lifelong Learning with Skill-Space Planning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2021) by &lt;em&gt;Kevin Lu, Aditya Grover, Pieter Abbeel and Igor Mordatch&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2725-L2732&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11441&#34;&gt;&lt;strong&gt;Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengdi Xu, Wenhao Ding, Jiacheng Zhu, Zuxin Liu, Baiming Chen and Ding Zhao&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L146-L154&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;uses an infinite mixture of Gaussian Processes to learn a task-agnostic policy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.07011&#34;&gt;&lt;strong&gt;Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Jorge A. Mendez, Boyu Wang and Eric Eaton&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L200-L210&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Continual Reinforcement Learning: A Review and Perspectives&lt;/strong&gt;, (2020) by &lt;em&gt;Khimya Khetarpal, Matthew Riemer, Irina Rish and Doina Precup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L293-L301&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A review on continual reinforcement learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1566253519307377&#34;&gt;&lt;strong&gt;Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges&lt;/strong&gt;&lt;/a&gt; , (Information Fusion 2020) by &lt;em&gt;Timoth√©e Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia D√≠az-Rodr√≠guez&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1321-L1332&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HyxAfnA5tm&#34;&gt;&lt;strong&gt;Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Anusha Nagabandi, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L932-L939&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formulates an online learning procedure that uses SGD to update model parameters, and an EM with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distribution&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continual Reinforcement Learning deployed in Real-life using PolicyDistillation and Sim2Real Transfer&lt;/strong&gt;, (ICML Workshop 2019) by *Kalifou, Ren√© Traor√©, Caselles-Dupr√©, Hugo, Lesort, Timoth√©e, Sun, Te, Diaz-Rodriguez, Natalia and Filliat, David * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1253-L1259&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.11682&#34;&gt;&lt;strong&gt;Experience replay for continual learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Rolnick, David, Ahuja, Arun, Schwarz, Jonathan, Lillicrap, Timothy and Wayne, Gregory&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1461-L1469&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1701.08734&#34;&gt;&lt;strong&gt;PathNet: Evolution Channels Gradient Descent in Super Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1796-L1816&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incremental robot learning of new objects with fixed update time&lt;/strong&gt;, (2017) by &lt;em&gt;R. {Camoriano}, G. {Pasquale}, C. {Ciliberto}, L. {Natale}, L. {Rosasco} and G. {Metta}&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1934-L1946&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Task-Agnostic Lifelong Reinforcement Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=PVJ6j87gOHz&#34;&gt;&lt;strong&gt;Co{MPS}: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (ICLR 2022) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L126-L133&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Co{MPS} is a novel meta-policy search algorithm for task-agnostic continual RL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.14495&#34;&gt;&lt;strong&gt;Task-Agnostic Continual Reinforcement Learning: In Praise of a Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (arXiv 2022) by &lt;em&gt;Caccia, Massimo, Mueller, Jonas, Kim, Taesup, Charlin, Laurent and Fakoor, Rasool&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L136-L143&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;combines replay and an RNN to set a simple baseline for TACRL: shows that the baselines matches and surpasses previously thought upper bounds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05742&#34;&gt;&lt;strong&gt;Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Steinparz, Christian, Schmied, Thomas, Paischer, Fabian, Dinu, Marius-Constantin, Patil, Vihang, Bitto-Nemling, Angela, Eghbal-zadeh, Hamid and Hochreiter, Sepp&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3460-L3467&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Detects changes and explores when and where they happen to recover from non-stationarity.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02940&#34;&gt;&lt;strong&gt;Same State, Different Task: Continual Reinforcement Learning without Interference&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Samuel Kessler, Jack Parker-Holder, Philip J. Ball, Stefan Zohren and Stephen J. Roberts&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L114-L122&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;learns multiple policies and cast policy-retrieval as a multi-arm bandit problem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04467&#34;&gt;&lt;strong&gt;CoMPS: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L189-L198&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11441&#34;&gt;&lt;strong&gt;Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengdi Xu, Wenhao Ding, Jiacheng Zhu, Zuxin Liu, Baiming Chen and Ding Zhao&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L146-L154&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;uses an infinite mixture of Gaussian Processes to learn a task-agnostic policy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HyxAfnA5tm&#34;&gt;&lt;strong&gt;Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Anusha Nagabandi, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L932-L939&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formulates an online learning procedure that uses SGD to update model parameters, and an EM with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distribution&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Continual Generative Modeling&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.14481.pdf&#34;&gt;&lt;strong&gt;Continual Unsupervised Representation Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu and Raia Hadsell&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L942-L951&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces unsupervised continual learning (no task label and no task boundaries)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01951954&#34;&gt;&lt;strong&gt;Generative Models from the perspective of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCNN 2019) by &lt;em&gt;Lesort, Timoth√©e, Caselles-Dupr√©, Hugo, Garcia-Ortiz, Michael, Goudou, Jean-Fran{\c c}ois and Filliat, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L954-L966&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive evaluation of CL methods for generative modeling&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=3367471.3367504&#34;&gt;&lt;strong&gt;Closed-loop Memory GAN for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCAI 2019) by &lt;em&gt;Rios, Amanda and Itti, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1261-L1275&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.09847&#34;&gt;&lt;strong&gt;Lifelong Generative Modeling&lt;/strong&gt;&lt;/a&gt; , (arXiv 2017) by &lt;em&gt;Ramapuram, Jason, Gregorova, Magda and Kalousis, Alexandros&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L969-L976&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Biologically-Inspired&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnbot.2022.846219/full&#34;&gt;&lt;strong&gt;Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Iyer, Abhiram, Grewal, Karan, Velu, Akash, Souza, Lucas Oliveira, Forest, Jeremy and Ahmad, Subutai&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L77-L86&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;bio-inspired method which dynamically restrict and route information in a context-specific manner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1.full.pdf&#34;&gt;&lt;strong&gt;A rapid and efficient learning rule for biological neural circuits&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Eren Sezener, Agnieszka Grabska-Barwinska, Dimitar Kostadinov, Maxime Beau, Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew M. Botvinick, Claudia Clopath, Michael H{&#34;a}usser and Peter E. Latham&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L104-L111&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/doi/pdf/10.1073/pnas.1803839115&#34;&gt;&lt;strong&gt;Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Masse, Nicolas Y, Grant, Gregory D and Freedman, David J&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L90-L101&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a network trained to do CL where select subnetworks are used to learn each task; these subnetworks are chosen a priori&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Miscellaneous&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.07461&#34;&gt;&lt;strong&gt;Learning causal models online&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Javed, Khurram, White, Martha and Bengio, Yoshua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L237-L244&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.06568&#34;&gt;&lt;strong&gt;On the Limitations of Continual Learning for Malware Classification&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Rahman, Mohammad Saidur, Coull, Scott E and Wright, Matthew&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L21-L28&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;This paper investigates overcoming catastrophic forgetting for malware classification&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.09578&#34;&gt;&lt;strong&gt;CLOPS: Continual Learning of Physiological Signals&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Kiyasseh, Dani, Zhu, Tingting and Clifton, David A&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L404-L411&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a healthcare-specific replay-based method to mitigate destructive interference during continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=Skgxcn4YDS&#34;&gt;&lt;strong&gt;LAMAL: LAnguage Modeling Is All You Need for Lifelong Language Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Fan-Keng Sun, Cheng-Hao Ho and Hung-Yi Lee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1522-L1529&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=rklnDgHtDS&#34;&gt;&lt;strong&gt;Compositional Language Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Yuanpeng Li, Liang Zhao, Kenneth Church and Mohamed Elhoseiny&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1551-L1558&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;method for compositional continual learning of sequence-to-sequence models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8569992&#34;&gt;&lt;strong&gt;Incremental Lifelong Deep Learning for Autonomous Vehicles&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Pierre, John M.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L212-L223&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0925231217309864&#34;&gt;&lt;strong&gt;Unsupervised real-time anomaly detection for streaming data&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Ahmad, Subutai, Lavin, Alexander, Purdy, Scott and Agha, Zuha&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L377-L387&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;HTM applied to real-world anomaly detection problem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.05463&#34;&gt;&lt;strong&gt;Continuous online sequence learning with an unsupervised neural network model&lt;/strong&gt;&lt;/a&gt; , (2016) by &lt;em&gt;Cui, Yuwei, Ahmad, Subutai and Hawkins, Jeff&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L390-L401&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;HTM applied to a prediction problem of taxi passenger demand&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Thesis&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00487&#34;&gt;&lt;strong&gt;Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Timoth√©e Lesort&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L337-L346&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://amsdottorato.unibo.it/9073/1/vincenzo_lomonaco_thesis.pdf&#34;&gt;&lt;strong&gt;Continual Learning with Deep Architectures&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Vincenzo Lomonaco&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1688-L1694&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.02718&#34;&gt;&lt;strong&gt;Continual Learning in Neural Networks&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;Aljundi, Rahaf&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2160-L2167&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.utexas.edu/~ring/Ring-dissertation.pdf&#34;&gt;&lt;strong&gt;Continual learning in reinforcement environments&lt;/strong&gt;&lt;/a&gt; , (1994) by &lt;em&gt;Ring, Mark Bishop&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1761-L1768&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Libraries&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/renate&#34;&gt;&lt;strong&gt;Renate: a library for real-world continual learning&lt;/strong&gt;&lt;/a&gt; , (2022) by ** &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3470-L3478&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for real-world continual learning with integrated hyperparameter tuning.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lebrice/Sequoia&#34;&gt;&lt;strong&gt;Sequoia - Towards a Systematic Organization of Continual Learning Research&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Matthew Riemer, Pau Rodriguez, Julio Hurtado, Khimya Khetarpal, Timoth√©e Lesort, Laurent Charlin, Irina Rish and Massimo Caccia&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2951-L2960&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library that unifies Continual Supervised and Continual Reinforcement Learning research&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://avalanche.continualai.org/&#34;&gt;&lt;strong&gt;Avalanche: an End-to-End Library for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Gabriele Graffieti and Antonio Carta&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2973-L2981&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for Continual Supervised Learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chandar-lab/Lifelong-Hanabi&#34;&gt;&lt;strong&gt;Continuous Coordination As a Realistic Scenario for Lifelong Learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Hadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville and Sarath Chandar&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2984-L2993&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;River: machine learning for streaming data in Python&lt;/strong&gt;, (2020) by &lt;em&gt;Jacob Montiel, Max Halford, Saulo Martiello Mastelini and Geoffrey Bolmier, Raphael Sourty, Robin Vaysse and Adil Zouitine, Heitor Murilo Gomes, Jesse Read and Talel Abdessalem and Albert Bifet&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2606-L2617&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for online learning.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuum, Data Loaders for Continual Learning&lt;/strong&gt;, (2020) by &lt;em&gt;Douillard, Arthur and Lesort, Timoth√©e&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2620-L2628&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library proposing continual learning scenarios and metrics.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mmasana/FACIL&#34;&gt;&lt;strong&gt;Framework for Analysis of Class-Incremental Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Masana, Marc, Liu, Xialei, Twardowski, Bartlomiej, Menta, Mikel, Bagdanov, Andrew D and van de Weijer, Joost&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2963-L2970&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for Continual Class-Incremental Learning&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Workshops&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/cl-icml/organizers?authuser=0&#34;&gt;&lt;strong&gt;Workshop on Continual Learning at ICML 2020&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Rahaf Aljundi, Haytham Fayek, Eugene Belilovsky, David Lopez-Paz, Arslan Chaudhry, Marc Pickett, Puneet Dokania, Jonathan Schwarz and Sayna Ebrahimi&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L348-L355&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/group?id=ICML.cc/2020/Workshop/LifelongML#accept&#34;&gt;&lt;strong&gt;4th Lifelong Machine Learning Workshop at ICML 2020&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Shagun Sodhani, Sarath Chandar, Balaraman Ravindran and Doina Precup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L358-L365&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CVPR 2020 Continual Learning in Computer Vision Competition: Approaches, Results, Current Challenges and Future Directions&lt;/strong&gt;, (arXiv 2020) by &lt;em&gt;Lomonaco, Vincenzo, Pellegrini, Lorenzo, Rodriguez, Pau, Caccia, Massimo, She, Qi, Chen, Yu, Jodelet, Quentin, Wang, Ruiping, Mai, Zheda, Vazquez, David and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1562-L1568&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;surveys the results of the first CL competition at CVPR&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.statmt.org/wmt20/lifelong-learning-task.html&#34;&gt;&lt;strong&gt;1st Lifelong Learning for Machine Translation Shared Task at WMT20 (EMNLP 2020)&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Lo√Øc Barrault, Magdalena Biesialska, Marta R. Costa-juss√†, Fethi Bougares and Olivier Galibert&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2789-L2791&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>llsoftsec/llsoftsecbook</title>
    <updated>2023-02-08T01:47:10Z</updated>
    <id>tag:github.com,2023-02-08:/llsoftsec/llsoftsecbook</id>
    <link href="https://github.com/llsoftsec/llsoftsecbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Low-Level Software Security for Compiler Developers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llsoftsecbook: a book on Low-Level Software Security for Compiler Developers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg?sanitize=true&#34; alt=&#34;License: CC BY 4.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/llsoftsec/llsoftsecbook/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Build book with docker container CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#contributors-&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/all_contributors-12-orange.svg?style=flat-square&#34; alt=&#34;All Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-BADGE:END --&gt; &#xA;&lt;p&gt;This book aims to provide a structured, broad overview of all attacks and security hardening techniques relevant for code generation tools.&lt;/p&gt; &#xA;&lt;h2&gt;Purpose&lt;/h2&gt; &#xA;&lt;p&gt;Compilers, assemblers and similar tools generate all the binary code that processors execute. Therefore, they play a crucial role in hardening binaries against security threats.&lt;/p&gt; &#xA;&lt;p&gt;The variety of attacks and hardening techniques has been rising sharply, and it is becoming difficult to maintain a good broad basic understanding of all of them.&lt;/p&gt; &#xA;&lt;p&gt;The purpose of this book is to help every compiler developer that needs to learn about software security relevant to compilers. It aims to achieve that by providing a description of all relevant high-level aspects of attacks, vulnerabilities, mitigations and hardening techniques. For further details, this book provides pointers to material on specific techniques.&lt;/p&gt; &#xA;&lt;p&gt;Even though the focus is on compiler developers, we expect that this book will also be useful to other people working on low-level software.&lt;/p&gt; &#xA;&lt;h2&gt;Why an open source book?&lt;/h2&gt; &#xA;&lt;p&gt;The idea for this book emerged out of a frustration of not finding a good overview on this topic. Kristof Beyls and Georgia Kouveli, compiler engineers working on security features from time to time, wished a book like this would exist. After not finding such a book, we decided to try and write one ourselves. We immediately realized that we do not have all necessary expertise ourselves to complete such a daunting task. So we decided to try and create this book in an open source style, seeking contributions from many experts.&lt;/p&gt; &#xA;&lt;p&gt;As you read this, the book remains unfinished. This book may well never be finished, as new vulnerabilities continue to be discovered regularly. Our hope is that developing the book as an open source project will allow it to continue to evolve and improve. It being open source increases the likelihood that it remains relevant as new vulnerabilities and mitigations emerge.&lt;/p&gt; &#xA;&lt;p&gt;Kristof and Georgia are far from experts on all possible vulnerabilities. So what is the plan to get high quality content to cover all relevant topics? It is two-fold.&lt;/p&gt; &#xA;&lt;p&gt;First, by studying specific topics, we hope to gain enough knowledge to write up a good summary for this book.&lt;/p&gt; &#xA;&lt;p&gt;Second, we very much invite and welcome contributions. If you&#39;re interested in potentially contributing content, please let us know.&lt;/p&gt; &#xA;&lt;p&gt;As a reader, you can also contribute to making this book better. We highly encourage feedback, both positive and constructive criticisms. We prefer feedback to be received through the GitHub communication channels &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues&#34;&gt;Issues&lt;/a&gt; and &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Live version&lt;/h2&gt; &#xA;&lt;p&gt;A live top-of-main version of the book is available as a webpage at &lt;a href=&#34;https://llsoftsec.github.io/llsoftsecbook&#34;&gt;https://llsoftsec.github.io/llsoftsecbook&lt;/a&gt;. A &lt;a href=&#34;https://llsoftsec.github.io/llsoftsecbook/book.pdf&#34;&gt;PDF&lt;/a&gt; is also available.&lt;/p&gt; &#xA;&lt;h2&gt;Build instructions&lt;/h2&gt; &#xA;&lt;p&gt;You can build the book by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ make all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This requires pandoc, latex and necessary latex packages to be installed. The easiest way to make sure you build the book with the right versions of those tools is to use the script build_with_docker.sh:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ ./build_with_docker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This builds a docker container with the exact versions of pandoc, latex and necessary extra packages; and builds the book using that container.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll find the PDF and HTML versions of the book in build/book.pdf and build/book.html if the build finishes successfully.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please find contribution guidelines in &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/raw/main/contributing.md&#34;&gt;https://github.com/llsoftsec/llsoftsecbook/blob/main/contributing.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors ‚ú®&lt;/h2&gt; &#xA;&lt;p&gt;Thanks goes to these wonderful people (&lt;a href=&#34;https://allcontributors.org/docs/en/emoji-key&#34;&gt;emoji key&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/kbeyls&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/19591946?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Kristof Beyls&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Kristof Beyls&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=kbeyls&#34; title=&#34;Tests&#34;&gt;‚ö†Ô∏è&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=kbeyls&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-kbeyls&#34; title=&#34;Content&#34;&gt;üñã&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=kbeyls&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-kbeyls&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#infra-kbeyls&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;üöá&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Akbeyls&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://tubafranz.me/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/25690309?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Francesco Petrogalli&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Francesco Petrogalli&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Afpetrogalli&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=fpetrogalli&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#infra-fpetrogalli&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;üöá&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/g-kouv&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6901396?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;g-kouv&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;g-kouv&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Ag-kouv&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-g-kouv&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=g-kouv&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-g-kouv&#34; title=&#34;Content&#34;&gt;üñã&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/statham-arm&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/54840944?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Simon Tatham&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Simon Tatham&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Astatham-arm&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-statham-arm&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=statham-arm&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-statham-arm&#34; title=&#34;Content&#34;&gt;üñã&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/sam-ellis&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6695726?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Sam Ellis&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Sam Ellis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=sam-ellis&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-sam-ellis&#34; title=&#34;Content&#34;&gt;üñã&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3Asam-ellis&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-sam-ellis&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Asam-ellis&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://www.lyndonfawcett.com&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5150703?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Lyndon Fawcett&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Lyndon Fawcett&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3Alyndon160&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-lyndon160&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/JLouisKaplan-Arm&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/90251161?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Jonathan Louis Kaplan&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Jonathan Louis Kaplan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3AJLouisKaplan-Arm&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-JLouisKaplan-Arm&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=JLouisKaplan-Arm&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3AJLouisKaplan-Arm&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/jacobbramley&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5206553?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Jacob Bramley&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Jacob Bramley&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-jacobbramley&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/joseph-yiu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/77114984?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Joseph Yiu&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Joseph Yiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=joseph-yiu&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-joseph-yiu&#34; title=&#34;Content&#34;&gt;üñã&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Arnaud-de-Grandmaison-ARM&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24225823?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Arnaud de Grandmaison&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Arnaud de Grandmaison&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3AArnaud-de-Grandmaison-ARM&#34; title=&#34;Reviewed Pull Requests&#34;&gt;üëÄ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Fare9&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/13202760?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Fare9&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Fare9&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-Fare9&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://homepages.dcc.ufmg.br/~fernando/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/367846?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Fernando Magno Quint√£o Pereira&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Fernando Magno Quint√£o Pereira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3Apronesto&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;p&gt;This project follows the &lt;a href=&#34;https://github.com/all-contributors/all-contributors&#34;&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind welcome!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;This book is licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mengchaoheng/SCUT_thesis</title>
    <updated>2023-02-08T01:47:10Z</updated>
    <id>tag:github.com,2023-02-08:/mengchaoheng/SCUT_thesis</id>
    <link href="https://github.com/mengchaoheng/SCUT_thesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ÂçéÂçóÁêÜÂ∑•Â§ßÂ≠¶Á°ïÂçöÂ£´Â≠¶‰ΩçËÆ∫ÊñáLaTeXÊ®°ÊùøÔºåÂßã‰∫é2020Âπ¥ÁöÑÊúÄÊñ∞È°πÁõÆÔºå2023Âπ¥ÊåÅÁª≠Êõ¥Êñ∞‰∏≠„ÄÇLatex templates for the thesis of South China University of Technology&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ÂçéÂçóÁêÜÂ∑•Â§ßÂ≠¶Á°ï/ÂçöÂ£´Â≠¶‰ΩçËÆ∫ÊñáLaTeXÊ®°Êùø&lt;/h1&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆÂßã‰∫é2020Âπ¥ÔºåÂèØÁî®‰∫éÊí∞ÂÜôÂçéÂçóÁêÜÂ∑•Â§ßÂ≠¶Á°ï/ÂçöÂ£´Â≠¶‰ΩçËÆ∫Êñá„ÄÇÊ®°ÊùøÁî±&lt;a href=&#34;https://github.com/alwintsui/scutthesis&#34;&gt;alwintsui&lt;/a&gt;‰ª•Âèä&lt;a href=&#34;https://github.com/yecfly/scut-thesis&#34;&gt;yecfly&lt;/a&gt;ÁöÑÊ®°Êùø‰øÆÊîπËÄåÊù•ÔºåÂÆûÊµãÂèØ‰ª•ÈÄöËøáÊâÄÊúâÊ†ºÂºèÂÆ°Ê†∏ÔºåÂú®Ê≠§Âêëalwintsui„ÄÅyecflyËá¥‰ª•Â¥áÈ´òÁöÑÊï¨ÊÑèÔºÅ&lt;/p&gt; &#xA;&lt;p&gt;2022Êõ¥Êñ∞Ôºö&lt;/p&gt; &#xA;&lt;p&gt;Ê†πÊçÆ2021Â±ä„ÄÅ2022Â±äÁ°ï„ÄÅÂçöÂ£´ÊØï‰∏öÁîüÁöÑÂ§ßÈáèÂèçÈ¶àÊÑèËßÅÔºåÊú¨È°πÁõÆÂ∑≤ÁªèÊåÅÁª≠Êõ¥Êñ∞‰∏§Âπ¥„ÄÇÊú¨‰∫∫Â∑≤‰∫é2022Âπ¥ÂõûÊ†°ËØªÂçöÔºå‰ºö‰∏ÄÁõ¥Áª¥Êä§Ëøô‰∏™È°πÁõÆÔºåÂ∏åÊúõÂêéÁª≠ÂêåÂ≠¶Âú®‰ΩøÁî®ËøáÁ®ã‰∏≠ÈÅáÂà∞ÈóÆÈ¢òÁßØÊûÅÂèçÈ¶àÔºåËã•ËÉΩÊé®ÈÄÅÊõ¥Êñ∞ÂèÇ‰∏éÁª¥Êä§Êõ¥Â•ΩÔºÅ&lt;/p&gt; &#xA;&lt;h2&gt;Âø´ÈÄü‰ΩøÁî®Ôºö&lt;/h2&gt; &#xA;&lt;p&gt;‰ΩøÁî®‰πãÂâçÂèØÂÖàÈòÖËØªÊú¨ÊñáÊ®°ÊùøÁºñËØëÂêéÁîüÊàêÁöÑ‰ΩøÁî®ËØ¥Êòé&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/raw/master/scutthesis.pdf&#34;&gt;scutthesis.pdf&lt;/a&gt;Êñá‰ª∂„ÄÅÂπ∂ÊúâÈÄâÊã©ÊÄßÂú∞ÈòÖËØª&lt;a href=&#34;https://github.com/CTeX-org/lshort-zh-cn.git&#34;&gt;„Ää‰∏Ä‰ªΩÔºà‰∏çÂ§™ÔºâÁÆÄÁü≠ÁöÑLATEX 2Œµ ‰ªãÁªç„Äã&lt;/a&gt; ÂÖ•Èó®„ÄÇËøòÂèØ‰ª•ÈòÖËØª&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/tree/master/tutorial&#34;&gt;tutorial&lt;/a&gt;ÈáåÁöÑ&lt;code&gt;scutthesisËØ¥Êòé.pdf&lt;/code&gt;Êñá‰ª∂ÔºåËøôÊòØÊóßÊ®°ÁâàÁöÑÊñá‰ª∂ÔºåÂèØ‰ª•ÂèÇËÄÉÊóßÊ®°ÁâàÁöÑ‰∏úË•ø„ÄÇ‰ΩøÁî®Ê°à‰æã‰∏∫&lt;a href=&#34;https://github.com/mengchaoheng/SCUTthesis-mengchaoheng.git&#34;&gt;Êú¨‰∫∫ÁöÑÂ≠¶‰ΩçËÆ∫Êñá&lt;/a&gt;(Ê≥®ÊÑèËØ•ËÆ∫ÊñáÊòØÂü∫‰∫éÂàù‰ª£Ê®°ÁâàÔºåËÄåÁé∞Âú®ÁöÑÊ®°ÁâàÂ∑≤Áªè‰∏çÊñ≠Êõ¥Êñ∞)„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÁºñËØë‰πãÂâçÈ¶ñÂÖàÂÆâË£Ö&lt;a href=&#34;https://www.tug.org/texlive/&#34;&gt;texlive&lt;/a&gt;ÔºåÊâæÂà∞ÂØπÂ∫îÁ≥ªÁªüÔºàLinuxÔºåwinÔºåmacOSÔºâÁöÑÁâàÊú¨„ÄÇÊ≥®ÊÑèmacOSÊòØMacTeX„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÁºñËØëÊúâ‰∏âÁßçÊñπÊ≥ïÔºö&lt;/p&gt; &#xA;&lt;p&gt;1.‰ΩøÁî®VSCode, ÂÆâË£ÖLaTeX WorkshopÊèí‰ª∂ÔºåÂú®&lt;code&gt;settings.json&lt;/code&gt;‰∏≠‰øÆÊîπÁõ∏ÂÖ≥Â≠óÊÆµ‰∏∫&lt;code&gt;settings_files/settings.json&lt;/code&gt;‰∏≠ÁöÑÂÄº„ÄÇ‰ª•ÂâçÂ∞±ÊúâÂú®‰ΩøÁî®vscodeÁöÑÂêåÂ≠¶ÈúÄË¶ÅÊääËØ•&lt;code&gt;settings.json&lt;/code&gt;Êñá‰ª∂ÁöÑÈÉ®ÂàÜÂÜÖÂÆπÊ∑ªÂä†Âà∞Ëá™Â∑±ÁöÑ.jsonÊñá‰ª∂„ÄÇÁ¨¨‰∏ÄÊ¨°‰ΩøÁî®vscodeÁöÑÂêåÂ≠¶Áõ¥Êé•Ë¶ÜÁõñÂ∞±Ë°å„ÄÇÂú®vscodeÈÖçÁΩÆ‰ΩøÁî®xelatexmkËøõË°åÁºñËØë„ÄÇËØ¶ÊÉÖÂèÇËÄÉ&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions&#34;&gt;ËÆ®ËÆ∫Âå∫&lt;/a&gt;„ÄÇÔºà2022Âπ¥Ëµ∑Êé®ËçêÁöÑÊñπÊ≥ïÔºåÂ∏åÊúõÊõ¥Â§öÂêåÂ≠¶ÂèÇ‰∏éÂÆåÂñÑÂπ∂ÊääÊõ¥Êñ∞Êé®ÈÄÅÁªôÊàëÔºåÁé∞Âú®ÁöÑ.jsonÊñá‰ª∂ËøòÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊîπËøõÔºâ&lt;/p&gt; &#xA;&lt;p&gt;2.‰ΩøÁî®ÁºñËØëËÑöÊú¨&lt;code&gt;all.bat&lt;/code&gt;ÔºåËÆ∞ÂæóÂÖ≥ÊéâÁîüÊàêÁöÑpdfÊñáÊ°£ÂÜçÂèåÂáª&lt;code&gt;all.bat&lt;/code&gt;„ÄÇÂèåÂáª&lt;code&gt;clean.bat&lt;/code&gt;Âà†Èô§‰∏¥Êó∂Êñá‰ª∂„ÄÇÔºàÊ≠§ÊñπÊ≥ï‰ªÖÊîØÊåÅwinÔºâ&lt;/p&gt; &#xA;&lt;p&gt;3.‰ΩøÁî®TeXstudioÔºåÈ¶ñÊ¨°ÁºñËØëÂª∫ËÆÆ‰ªé‰∏ªÊñá‰ª∂&lt;code&gt;scutthesis.tex&lt;/code&gt;ÂºÄÂßãÁºñËØëÔºåÈ¶ñÂÖàÂú®&lt;code&gt;TeXstudioÁöÑOptions-&amp;gt;Configure TeXstudio-&amp;gt;build&lt;/code&gt;‰∏≠ÔºåÁºñËØëÂô®(Dufault Compiler)ÈÄâÊã©&lt;code&gt;XeLaTeX&lt;/code&gt;ÔºåÈªòËÆ§ÊñáÁåÆÂ∑•ÂÖ∑(Default Bibliography Tool)ÈÄâ&lt;code&gt;Biber&lt;/code&gt;ÔºåÊûÑÂª∫Âπ∂Êü•ÁúãÔºàbuild &amp;amp; viewÔºâ ÊåâÁÖß&lt;code&gt;scutthesis.pdf&lt;/code&gt;‰∏≠ÁöÑÂõæ2-1ËøõË°åËÆæÁΩÆÔºàÁÇπÂáªÂè≥‰æßÊâ≥ÊâãÁ¨¶Âè∑ËøõË°åËÆæÁΩÆÔºåÂàÜÂà´ÈÄâÊã©&lt;code&gt;recompile-bibliography&lt;/code&gt;„ÄÅ&lt;code&gt;Defualt compiler&lt;/code&gt;„ÄÅ&lt;code&gt;Defualt Viewer&lt;/code&gt;Âπ∂ÁÇπaddÂà∞Âè≥‰æßÊ†è‰∏≠ÔºâÔºå‰πüÂèØ‰ª•‰ΩøÁî®ÈªòËÆ§ÁöÑCompile &amp;amp; ViewÔºàÂè™‰∏çËøáÊ≠§Êó∂‰∏ç‰ºöËá™Âä®Êõ¥Êñ∞ÂèÇËÄÉÊñáÁåÆËëóÂΩïÔºâ„ÄÇ(20Âπ¥-21Âπ¥ÊúüÈó¥‰∏ªË¶ÅÁöÑ‰ΩøÁî®ÊñπÊ≥ï)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; Êú¨Ê®°Êùø‰∏çÂÜçÂà©Áî®LyxÔºåÁõ¥Êé•‰ΩøÁî®TeXstudioÊàñËÄÖvscodeËøõË°åÁºñËæëÔºåËøôÁ±ª‰ººIDE„ÄÇvscodeÁöÑ‰∏Ä‰∫õ‰ΩøÁî®ÊäÄÂ∑ß‰ª•ÂèäÂø´Êç∑ÈîÆÂèØÂèÇËÄÉÂÖ∂‰ªñÊïôÁ®ã(ÁôæÂ∫¶vscode latex)ÔºåËøôÈáå‰ªÖ‰ªÖÁªôÂá∫ÈÖçÁΩÆÊñá‰ª∂&lt;code&gt;settings_files/settings.json&lt;/code&gt;„ÄÇËØ¶ÊÉÖÁßªÊ≠•&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions&#34;&gt;ËÆ®ËÆ∫Âå∫&lt;/a&gt;ÁöÑ&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions/6&#34;&gt;vscodeÈÖçÁΩÆ&lt;/a&gt;„ÄÇÊúâ‰ªÄ‰πàÈóÆÈ¢òÈÉΩÂèØ‰ª•Âú®ËÆ®ËÆ∫Âå∫‰∫§ÊµÅ„ÄÇ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ÂèÇËÄÉÊñáÁåÆÁÆ°ÁêÜ&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨Ê®°ÊùøÊúÄ‰∏ªË¶ÅÁöÑÊîπÂä®ÊòØÂèÇËÄÉÊñáÁåÆ‰ΩøÁî®biblatexÔºàbiberÔºâÔºåËÄå‰∏çÊòØÂéüÊù•ÁöÑBibTeXÔºåÂõ†Ê≠§‰∏çÂÜçÈúÄË¶Å.bstÊñá‰ª∂„ÄÇÂú®ËøôÈ°πÊîπÂä®‰πãÂêéÔºåÂèÇËÄÉÊñáÁåÆÈÉ®ÂàÜÁöÑÊ†ºÂºèÁ¨¶ÂêàÂõΩÊ†áÔºåÂΩìÁÑ∂Ë¶ÅÊ≥®ÊÑèËøõË°åÈÄÇÂΩìÁöÑËÆæÁΩÆÔºåÂõ†‰∏∫Ê†πÊçÆÂ≠¶Ê†°ÁöÑËÆ∫ÊñáÊí∞ÂÜôËßÑËåÉÔºåÊúâÁöÑ‰ø°ÊÅØÊòØ‰∏çÈúÄË¶ÅÊòæÁ§∫Âú®ÂèÇËÄÉÊñáÁåÆËëóÂΩïÈáåÁöÑ„ÄÇÂèØ‰ª•Êü•ÁúãbiblatexÂåÖÁöÑ&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/raw/master/settings_files/package_Documentation/biblatex-gb7714-2015.pdf&#34;&gt;‰ΩøÁî®ËØ¥Êòé&lt;/a&gt;Ôºå&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/tree/master/settings_files/package_Documentation&#34;&gt;package_Documentation&lt;/a&gt;Êñá‰ª∂Â§πËøòÊúâÂÖ∂‰ªñÂåÖÁöÑ‰ΩøÁî®ËØ¥Êòé„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Âè¶Â§ñÂèÇËÄÉÊñáÁåÆÁÆ°ÁêÜËΩØ‰ª∂Êé®Ëçê‰ΩøÁî®ÂºÄÊ∫êÁöÑzoteroÔºåËøôÊòØÁîüÊàêÊú¨Ê®°ÊùøÁöÑbibÊñá‰ª∂ÁöÑËΩØ‰ª∂ÔºàÊúÄÊñ∞ÂÆûÊµãÂÖ∂‰ªñÊñáÁåÆÁÆ°ÁêÜËΩØ‰ª∂Â¶Çendnote‰πüÂèØ‰ª•‰ΩøÁî®Êú¨Ê®°ÊùøÔºåÈáçÁÇπÊòØÂØºÂá∫bibÊñá‰ª∂Ôºâ„ÄÇÂΩìÁÑ∂Ôºå‰πüÂèØ‰ª•ÊâãÂä®Êñ∞Âª∫‰∏Ä‰∏™ÂêéÁºÄÂêç‰∏∫.bibÁöÑÊñá‰ª∂ÔºåÁÑ∂ÂêéÁõ¥Êé•Âú®ÊñáÁåÆÈ°µÈù¢ÔºàÊàñË∞∑Ê≠å„ÄÅÁôæÂ∫¶Â≠¶ÊúØÈ°µÈù¢ÔºâÂ§çÂà∂BibTeXÊï∞ÊçÆÂà∞ËØ•.bibÊñá‰ª∂ÔºåÊúÄÂêéÂú®&lt;code&gt;scutthesis.tex&lt;/code&gt;Êñá‰ª∂Èáå‰ΩøÁî®Â∞±Ë°å„ÄÇzotero‰∏ç‰ªÖÊúâÂº∫Â§ßÁöÑPCÁ´ØÔºàÊîØÊåÅmac„ÄÅwin„ÄÅlinuxÔºâÔºåÂèØ‰ª•‰ΩøÁî®chromeÁ≠âÊµèËßàÂô®ÊêúÁ¥¢Âà∞ËÆ∫ÊñáÂêéÂà©Áî®Êèí‰ª∂‰∏ÄÈîÆÊçïËé∑ÊñáÁåÆ‰ø°ÊÅØÂà∞zotero„ÄÇÁÑ∂ÂêéÂØπzoteroÊî∂ÈõÜÂ•ΩÁöÑÊñáÁåÆÔºåÈÄâ‰∏≠ÊÉ≥Ë¶ÅÂºïÁî®ÁöÑËÆ∫ÊñáÁÑ∂ÂêéÊåâÂø´Êç∑ÈîÆÂ§çÂà∂ÔºàÈúÄË¶ÅÈÄÇÂΩìËá™ÂÆö‰πâËÆæÁΩÆÔºâÔºåÂÜçÂà∞Êí∞ÂÜôËÆ∫ÊñáÁöÑtexÊñá‰ª∂Á≤òË¥¥Âç≥ÂèØÔºàword‰πüÂèØ‰ª•‰ΩøÁî®zoteroÔºå‰πüÂæàÊñπ‰æøÔºåÊú¨È°πÁõÆÊàë‰ª¨‰ªÖ‰ªÖËÆ®ËÆ∫latexÊ®°ÊùøÔºâ„ÄÇzoteroËøòÊúâÁßªÂä®Á´ØÔºàÂÆâÂçì„ÄÅiosÔºâÔºåÊâÄÊúâÊñáÁåÆ‰ø°ÊÅØÈÉΩÊòØÂêåÊ≠•ÁöÑÔºåÊñáÁåÆÁöÑpdfÊñá‰ª∂ÂèØ‰ª•‰ΩøÁî®Á¨¨‰∏âÊñπÂ≠òÂÇ®Â∑•ÂÖ∑ÂêåÊ≠•„ÄÇÊÄª‰πãÂ§öÁ´ØÂêåÊ≠•ÔºåÂºÄÊ∫êÂÖçË¥πÔºåÊñπ‰æøÈöèÊó∂ÈöèÂú∞ÁúãËÆ∫ÊñáÔºåÁúÅÂéªÁÆ°ÁêÜÊñáÁåÆÁöÑÈ∫ªÁÉ¶ÔºåÁõ∏ËßÅÊÅ®ÊôöÔºÅÂú®ÂÜôÊØï‰∏öËÆ∫ÊñáÁöÑÂêÑ‰ΩçÂèØ‰ª•Êé®ËçêÁªôÁ†î‰∏ÄÁöÑÂ∏àÂºüÂ∏àÂ¶πÔºåÊó©Áî®Êó©‰∫´ÂèóÔºåÊñπ‰æøÂêéÊúüÂÜôËÆ∫Êñá„ÄÇ&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; ‰ΩøÁî®zoteroÔºåÁßëÂ≠¶‰∏äÁΩëÂæàÈáçË¶ÅÔºåÈÄöÂ∏∏Êàë‰ª¨‰ΩøÁî®Ë∞∑Ê≠åÂ≠¶ÊúØÊêúÁ¥¢ÊñáÁåÆÂπ∂Âà©Áî®chromeÁöÑzoteroÊèí‰ª∂Áõ¥Êé•ÊçïËé∑ÊñáÁåÆËëóÂΩï‰ø°ÊÅØ„ÄÇ‰ΩÜÊàë‰ΩøÁî®ÂêÑÁßçÊñπÊ≥ïÂùáÈÅáÂà∞ËøáË¢´Ë∞∑Ê≠åÂ≠¶ÊúØÂ∞ÅÈîÅÁöÑÊÉÖÂÜµÔºåÂêéÊù•Âè™ËÉΩÊç¢ÁßëÂ≠¶‰∏äÁΩëÊñπÊ≥ïÔºåËøôÊñπÈù¢ÂèØ‰ª•ÁôæÂ∫¶Ëá™Ë°åÂ≠¶‰π†„ÄÇÂº∫ÁÉàÂª∫ËÆÆ‰∏çË¶ÅÂú®Ë∞∑Ê≠åÂ≠¶ÊúØÊêúÁ¥¢ÁïåÈù¢Áî®zoteroÊèí‰ª∂‰∏ÄÈîÆÊçïËé∑Â§ßÈáèÊñáÁ´†Ôºå‰∏ÄÊ¨°Âè™ÊçïËé∑‰∏Ä‰∏§ÁØáÊúÄ‰Ω≥„ÄÇËøõÂÖ•ÊñáÁ´†È°µÈù¢ÔºàÂ¶ÇIEEEÈ°µÔºâ‰∏ÄÁØá‰∏ÄÁØáÂú∞ÊçïËé∑‰πüÂèØ‰ª•„ÄÇÂà©Áî®Ë∞∑Ê≠åÂ≠¶ÊúØ‰ªéÊêúÁ¥¢ÁªìÊûúÂ§ßÈáèÊçïËé∑ÊñáÁåÆ‰ø°ÊÅØÂÆπÊòìË¢´Ë∞∑Ê≠åÂ∞ÅipÔºå‰∏ÄÊó¶Ë¢´Â∞ÅÂè™ËÉΩÊç¢ipÔºåÁõ∏ÂΩìÈ∫ªÁÉ¶ÔºÅ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ÈîôËØØÊéíÈô§Ôºö&lt;/h2&gt; &#xA;&lt;p&gt;ÁºñËØëÊó∂Â¶ÇÊûúÊèêÁ§∫Áº∫‰πèÂ≠ó‰ΩìÔºåËØ∑ÂÆâË£Ö&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/tree/master/settings_files&#34;&gt;settings_files&lt;/a&gt;ÈáåÈù¢ÁöÑÂ≠ó‰ΩìÔºàwinÂè≥ÈîÆÂÆâË£ÖÂç≥ÂèØÔºõmacÂèÇËÄÉÂÆòÊñπÂ≠ó‰ΩìÂÆâË£ÖÊïôÁ®ãÔºõlinux‰∏ãÁ±ª‰ººÔºåÈúÄË¶ÅËá™Ë°åÁôæÂ∫¶Ôºâ„ÄÇÊ≠§Â§ñÔºåÁõÆÂâçÈÅáÂà∞ÁöÑÂØºËá¥ÈîôËØØÁöÑÂéüÂõ†Â§ßÈÉΩÊòØ‰ª•‰∏ã‰∏âÁßçÔºö&lt;/p&gt; &#xA;&lt;p&gt;1.ËØ≠Ê≥ïÈîôËØØÔºåËøôÁßçÂÄüÂä©ÁôæÂ∫¶„ÄÅË∞∑Ê≠åÂæàÂÆπÊòìËß£ÂÜ≥ÔºåÊåâÁÖßlatexÁöÑËßÑËåÉÂéªÂÜôÂ∞±Â•Ω„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;2.ÂíåÂèÇËÄÉÊñáÁåÆÊúâÂÖ≥ÁöÑÈîôËØØÔºåËÆ∞‰Ωè‰∏ÄÂÆöÊ£ÄÊü•Â•ΩbibÊñá‰ª∂ÔºåËÄåbibÊñá‰ª∂Êù•Ê∫ê‰∫éÂèÇËÄÉÊñáÁåÆÁÆ°ÁêÜËΩØ‰ª∂Â¶ÇÊú¨ÊñáÊé®ËçêÁöÑzoteroÔºåÂèàÊàñËÄÖÊù•Ê∫ê‰∫éËá™Â∑±ÊâãÂä®Âª∫Á´ãÁöÑÊñá‰ª∂„ÄÇ‰∏ÄÂÆöË¶ÅËÆæÁΩÆÂ•ΩÂØºÂá∫ÈÄâÈ°π‰ª•ÂèäÊ†ºÂºèÔºåÂêåÊó∂Ë¶ÅËÆæÁΩÆÂ•ΩÁºñËØëÈÄâÈ°πbiber(Âå∫Âà´‰∫éÂ∏∏ËßÅÁöÑbibtex)„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;3.ÁºñËØëÂô®ÈóÆÈ¢òÔºåÂíåÁ¨¨‰∫åÊù°‰∏ÄÊ†∑Ôºå‰ΩøÁî®TeXstudioÁöÑËØùÔºå‰∏ÄÂÆöË¶ÅÂú®&lt;code&gt;Options-&amp;gt;Configure TeXstudio-&amp;gt;build&lt;/code&gt;‰∏≠ËÆæÁΩÆÂ•ΩÔºõ‰ΩøÁî®vscodeÁöÑËØùÔºåËÆ∞ÂæóÊåâÁÖß&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions&#34;&gt;ËÆ®ËÆ∫Âå∫&lt;/a&gt;ËÆæÁΩÆÂ•ΩvscodeÁºñËØëÁöÑÈÖçÁΩÆÊñá‰ª∂&lt;code&gt;settings.json&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;4.ÂÖ∂‰ªñÈóÆÈ¢òÔºåÂÖ≥ÊéâÁîüÊàêÁöÑPDFÔºåÊ∏ÖÈô§ÊâÄÊúâ‰∏≠Èó¥Êñá‰ª∂ÂÜçÁºñËØë‰∏ÄÊ¨°ÁúãÁúã„ÄÇÂ¶ÇwinÁ≥ªÁªüÂèåÂáª&lt;code&gt;clean.bat&lt;/code&gt;Êñá‰ª∂„ÄÇÊàñ‰ΩøÁî®TeXstudioÁöÑËØùÔºåÁÇπÂáª&lt;code&gt;Tools-&amp;gt;Clean Auxiliary files&lt;/code&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Á°Æ‰øù‰ª•‰∏äÂá†ÊñπÈù¢Ê≤°ÈóÆÈ¢òÁöÑËØùÔºåÂü∫Êú¨Â∞±ÂèØ‰ª•ÊÑâÂø´ÁöÑÂÜôËÆ∫Êñá‰∫Ü„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Ë¥°ÁåÆËÄÖ&lt;/h2&gt; &#xA;&lt;p&gt;ÊåâÂä†ÂÖ•Êó∂Èó¥È°∫Â∫èÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mengchaoheng&#34;&gt;mengchaoheng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Komari-Koshigaya&#34;&gt;Komari-Koshigaya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhuohoudeputao&#34;&gt;zhuohoudeputao&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ê¨¢ËøéÂ§ßÂÆ∂Âä†ÂÖ•Áª¥Êä§Âõ¢ÈòüÔºÅÔºÅÔºÅ&lt;/p&gt;</summary>
  </entry>
</feed>