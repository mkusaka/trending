<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-08T01:47:10Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>optimass/continual_learning_papers</title>
    <updated>2023-02-08T01:47:10Z</updated>
    <id>tag:github.com,2023-02-08:/optimass/continual_learning_papers</id>
    <link href="https://github.com/optimass/continual_learning_papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Relevant papers in Continual Learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Continual Learning Literature&lt;/h1&gt; &#xA;&lt;p&gt;This repository is maintained by Massimo Caccia and Timothée Lesort don&#39;t hesitate to send us an email to collaborate or fix some entries ({massimo.p.caccia , t.lesort} at gmail.com). The automation script of this repo is adapted from &lt;a href=&#34;https://github.com/TLESORT/Automatic_Awesome_Bibliography&#34;&gt;Automatic_Awesome_Bibliography&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For contributing to the repository please follow the process &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/scripts/README.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can directly use our bib.tex in overleaf &lt;a href=&#34;https://www.overleaf.com/project/606f5acf8bf59dcda3e66f9e&#34;&gt;with this link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#classics&#34;&gt;Classics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#empirical-study&#34;&gt;Empirical Study&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#surveys&#34;&gt;Surveys&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#influentials&#34;&gt;Influentials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#new-settings-or-metrics&#34;&gt;New Settings or Metrics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#general-continual-learning-methods-(sl-and-rl)&#34;&gt;General Continual Learning Methods (SL and RL)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#task-agnostic-continual-learning&#34;&gt;Task-Agnostic Continual Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#regularization-methods&#34;&gt;Regularization Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#distillation-methods&#34;&gt;Distillation Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#rehearsal-methods&#34;&gt;Rehearsal Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#generative-replay-methods&#34;&gt;Generative Replay Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#dynamic-architectures-or-routing-methods&#34;&gt;Dynamic Architectures or Routing Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#hybrid-methods&#34;&gt;Hybrid Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#continual-few-shot-learning&#34;&gt;Continual Few-Shot Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#meta-continual-learning&#34;&gt;Meta-Continual Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#lifelong-reinforcement-learning&#34;&gt;Lifelong Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#task-agnostic-lifelong-reinforcement-learning&#34;&gt;Task-Agnostic Lifelong Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#continual-generative-modeling&#34;&gt;Continual Generative Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#biologically-inspired&#34;&gt;Biologically-Inspired&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#miscellaneous&#34;&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#thesis&#34;&gt;Thesis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#libraries&#34;&gt;Libraries&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/README.md#workshops&#34;&gt;Workshops&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Classics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942&#34;&gt;&lt;strong&gt;Catastrophic forgetting in connectionist networks&lt;/strong&gt;&lt;/a&gt; , (1999) by &lt;em&gt;French, Robert M.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1335-L1349&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.3723&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;&lt;strong&gt;Lifelong robot learning&lt;/strong&gt;&lt;/a&gt; , (1995) by &lt;em&gt;Thrun, Sebastian and Mitchell, Tom M&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L423-L432&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Argues knowledge transfer is essential if robots are to learn control with moderate learning times&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1080/09540099550039318&#34;&gt;&lt;strong&gt;Catastrophic Forgetting, Rehearsal and Pseudorehearsal&lt;/strong&gt;&lt;/a&gt; , (1995) by * Anthony Robins * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2075-L2088&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0079742108605368&#34;&gt;&lt;strong&gt;Catastrophic interference in connectionist networks: The sequential learning problem&lt;/strong&gt;&lt;/a&gt; , (1989) by &lt;em&gt;McCloskey, Michael and Cohen, Neal J&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1015-L1025&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces CL and reveals the catastrophic forgetting problem&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Empirical Study&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02577&#34;&gt;&lt;strong&gt;Effects of Auxiliary Knowledge on Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2022) by &lt;em&gt;Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Bonicelli, Lorenzo, Boschini, Matteo, Calderara, Simone and Spampinato, Concetto&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L158-L165&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9412614&#34;&gt;&lt;strong&gt;Rethinking Experience Replay: a Bag of Tricks for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2021) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L257-L268&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0893608020304202&#34;&gt;&lt;strong&gt;A comprehensive study of class incremental learning algorithms for visual tasks&lt;/strong&gt;&lt;/a&gt; , (Neural Networks 2021) by &lt;em&gt;Eden Belouadah, Adrian Popescu and Ioannis Kanellos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2808-L2820&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Online Continual Learning in Image Classification: An Empirical Survey&lt;/strong&gt;, (2021) by &lt;em&gt;Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim and Scott Sanner&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2822-L2830&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GDumb: A simple approach that questions our progress in continual learning&lt;/strong&gt;, (ECCV 2020) by &lt;em&gt;Prabhu, Ameya, Torr, Philip HS and Dokania, Puneet K&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L282-L290&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introduces a super simple methods that outperforms almost all methods in all of the CL benchmarks. We need new better benchamrks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.08383&#34;&gt;&lt;strong&gt;Continual learning: A comparative study on how to defy forgetting in classification tasks&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh and Tinne Tuytelaars&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L508-L517&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive empirical study of CL methods (in the multi-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.07734&#34;&gt;&lt;strong&gt;Three scenarios for continual learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;van de Ven, Gido M and Tolias, Andreas S&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1047-L1054&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;An extensive review of CL methods in three different scenarios (task-, domain-, and class-incremental learning)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous learning in single-incremental-task scenarios&lt;/strong&gt;, (Neural Networks 2019) by &lt;em&gt;Maltoni, Davide and Lomonaco, Vincenzo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1362-L1371&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.09733&#34;&gt;&lt;strong&gt;Towards Robust Evaluations of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2018) by &lt;em&gt;Farquhar, Sebastian and Gal, Yarin&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L435-L442&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes desideratas and reexamines the evaluation protocol&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Catastrophic forgetting: still a problem for DNNs&lt;/strong&gt;, (ICANN 2018) by &lt;em&gt;Pf&#34;ulb, B, Gepperth, A, Abdullah, S and Krawczyk, A&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2226-L2232&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Measuring Catastrophic Forgetting in Neural Networks&lt;/strong&gt;, (2017) by &lt;em&gt;Kemker, R., McClure, M., Abitino, A. and Hayes, T. and Kanan, C.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1374-L1387&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v78/lomonaco17a.html&#34;&gt;&lt;strong&gt;CORe50: a New Dataset and Benchmark for Continuous Object Recognition&lt;/strong&gt;&lt;/a&gt; , (CoRL 2017) by &lt;em&gt;Vincenzo Lomonaco and Davide Maltoni&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1389-L1404&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6211&#34;&gt;&lt;strong&gt;An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2013) by &lt;em&gt;Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A. and Bengio, Y.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L492-L505&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Investigates CF in neural networks&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Surveys&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06758&#34;&gt;&lt;strong&gt;An Investigation of Replay-based Approaches for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCNN 2021) by &lt;em&gt;Bagus, Benedikt and Gepperth, Alexander&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3295-L3304&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Embracing Change: Continual Learning in Deep Neural Networks&lt;/strong&gt;, (2020) by &lt;em&gt;Hadsell, Raia, Rao, Dushyant, Rusu, Andrei and Pascanu, Razvan&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L167-L177&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Continual Reinforcement Learning: A Review and Perspectives&lt;/strong&gt;, (2020) by &lt;em&gt;Khimya Khetarpal, Matthew Riemer, Irina Rish and Doina Precup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L293-L301&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A review on continual reinforcement learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1566253519307377&#34;&gt;&lt;strong&gt;Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges&lt;/strong&gt;&lt;/a&gt; , (Information Fusion 2020) by &lt;em&gt;Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia Díaz-Rodríguez&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1321-L1332&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.01797&#34;&gt;&lt;strong&gt;A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Mundt, Martin, Hong, Yong Won, Pliushch, Iuliia and Ramesh, Visvanathan&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2595-L2602&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;propose a consolidated view to bridge continual learning, active learning and open set recognition in DNNs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.coling-main.574&#34;&gt;&lt;strong&gt;Continual Lifelong Learning in Natural Language Processing: A Survey&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Magdalena Biesialska, Katarzyna Biesialska, Marta R. Costa-jussà&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2773-L2785&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;An extensive review of CL in Natural Language Processing (NLP)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0893608019300231&#34;&gt;&lt;strong&gt;Continual lifelong learning with neural networks: A review&lt;/strong&gt;&lt;/a&gt; , (Neural Networks 2019) by &lt;em&gt;German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan and Stefan Wermter&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L520-L531&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;An extensive review of CL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01418129&#34;&gt;&lt;strong&gt;Incremental learning algorithms and applications&lt;/strong&gt;&lt;/a&gt; , (2016) by &lt;em&gt;Gepperth, Alexander and Hammer, Barbara&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1178-L1189&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A survey on incremental learning and the various applications fields&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Influentials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.00420&#34;&gt;&lt;strong&gt;Efficient Lifelong Learning with A-GEM&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Chaudhry, Arslan, Ranzato, Marc’Aurelio, Rohrbach, Marcus and Elhoseiny, Mohamed&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L445-L452&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;More efficient GEM; Introduces online continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.09733&#34;&gt;&lt;strong&gt;Towards Robust Evaluations of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2018) by &lt;em&gt;Farquhar, Sebastian and Gal, Yarin&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L435-L442&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes desideratas and reexamines the evaluation protocol&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.05202&#34;&gt;&lt;strong&gt;Continual Learning in Practice&lt;/strong&gt;&lt;/a&gt; , (NeurIPS Workshop 2018) by &lt;em&gt;Diethe, Tom, Borchert, Tom, Thereska, Eno, Pigem, Borja de Balle and Lawrence, Neil&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2327-L2334&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a reference architecture for a continual learning system&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34;&gt;&lt;strong&gt;Overcoming catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (PNAS 2017) by &lt;em&gt;Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L455-L463&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf&#34;&gt;&lt;strong&gt;Gradient Episodic Memory for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Lopez-Paz, David and Ranzato, Marc-Aurelio&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L467-L477&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A model that alliviates CF via constrained optimization&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08690&#34;&gt;&lt;strong&gt;Continual learning with deep generative replay&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Shin, Hanul, Lee, Jung Kwon, Kim, Jaehong and Kim, Jiwon&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L481-L489&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces generative replay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6211&#34;&gt;&lt;strong&gt;An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2013) by &lt;em&gt;Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A. and Bengio, Y.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L492-L505&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Investigates CF in neural networks&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;New Settings or Metrics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chandar-lab.github.io/IIRC/&#34;&gt;&lt;strong&gt;IIRC: Incremental Implicitly-Refined Classification&lt;/strong&gt;&lt;/a&gt; , (CVPR 2021) by &lt;em&gt;Mohamed Abdelsalam, Mojtaba Faramarzi, Shagun Sodhani and Sarath Chandar&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2761-L2769&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A setup and benchmark to evaluate lifelong learning models in more real-life aligned scenarios.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lebrice/Sequoia&#34;&gt;&lt;strong&gt;Sequoia - Towards a Systematic Organization of Continual Learning Research&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Matthew Riemer, Pau Rodriguez, Julio Hurtado, Khimya Khetarpal, Timothée Lesort, Laurent Charlin, Irina Rish and Massimo Caccia&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2951-L2960&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library that unifies Continual Supervised and Continual Reinforcement Learning research&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.04546&#34;&gt;&lt;strong&gt;Wandering Within a World: Online Contextualized Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer and Richard S. Zemel&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L325-L333&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;proposes a new continual few-shot setting where spacial and temporal context can be leveraged to and unseen classes need to be predicted&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.11967&#34;&gt;&lt;strong&gt;Defining Benchmarks for Continual Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Antoniou, Antreas, Patacchiola, Massimiliano, Ochal, Mateusz and Storkey, Amos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L367-L374&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;(title is a good enough summary)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05856&#34;&gt;&lt;strong&gt;Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Caccia, Massimo, Rodriguez, Pau, Ostapenko, Oleksiy, Normandin, Fabrice, Lin, Min, Caccia, Lucas, Laradji, Issam, Rish, Irina, Lacoste, Alexandre, Vazquez, David and Charlin, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1473-L1480&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a new approach to CL evaluation more aligned with real-life applications, bringing CL closer to Online Learning and Open-World learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=rklnDgHtDS&#34;&gt;&lt;strong&gt;Compositional Language Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Yuanpeng Li, Liang Zhao, Kenneth Church and Mohamed Elhoseiny&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1551-L1558&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;method for compositional continual learning of sequence-to-sequence models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.01797&#34;&gt;&lt;strong&gt;A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Mundt, Martin, Hong, Yong Won, Pliushch, Iuliia and Ramesh, Visvanathan&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2595-L2602&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;propose a consolidated view to bridge continual learning, active learning and open set recognition in DNNs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Don&#39;t forget, there is more than forgetting: new metrics for Continual Learning&lt;/strong&gt;, (arXiv 2018) by &lt;em&gt;D{&#39;\i}az-Rodr{&#39;\i}guez, Natalia, Lomonaco, Vincenzo, Filliat, David and Maltoni, Davide&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2942-L2948&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introduces a CL score that takes more than just forgetting into account&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;General Continual Learning Methods (SL and RL)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34;&gt;&lt;strong&gt;Overcoming catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (PNAS 2017) by &lt;em&gt;Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L455-L463&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1701.08734&#34;&gt;&lt;strong&gt;PathNet: Evolution Channels Gradient Descent in Super Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1796-L1816&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Task-Agnostic Continual Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12772&#34;&gt;&lt;strong&gt;Task-agnostic Continual Learning with Hybrid Probabilistic Models&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Polina Kirichenko, Mehrdad Farajtabar, Dushyant Rao, Balaji Lakshminarayanan, Nir Levine, Ang Li, Huiyi Hu, Andrew Gordon Wilson and Razvan Pascanu&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L226-L235&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html&#34;&gt;&lt;strong&gt;Learning where to learn: Gradient sparsity in meta and continual learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Von Oswald, Johannes, Zhao, Dominic, Kobayashi, Seijin, Schug, Simon, Caccia, Massimo, Zucchet, Nicolas and Sacramento, Jo{~a}o&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L247-L255&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HklUCCVKDB&#34;&gt;&lt;strong&gt;Uncertainty-guided Continual Learning with Bayesian Neural Networks&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell and Marcus Rohrbach&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L557-L564&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Uses Bayes by Backprop for variational Continual Learning.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05856&#34;&gt;&lt;strong&gt;Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Caccia, Massimo, Rodriguez, Pau, Ostapenko, Oleksiy, Normandin, Fabrice, Lin, Min, Caccia, Lucas, Laradji, Issam, Rish, Irina, Lacoste, Alexandre, Vazquez, David and Charlin, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1473-L1480&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a new approach to CL evaluation more aligned with real-life applications, bringing CL closer to Online Learning and Open-World learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;iTAML: An Incremental Task-Agnostic Meta-learning Approach&lt;/strong&gt;, (CVPR 2020) by &lt;em&gt;Rajasegaran, Jathushan, Khan, Salman, Hayat, Munawar, Khan, Fahad Shahbaz and Shah, Mubarak&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2411-L2418&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.14481.pdf&#34;&gt;&lt;strong&gt;Continual Unsupervised Representation Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu and Raia Hadsell&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L942-L951&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces unsupervised continual learning (no task label and no task boundaries)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.00689.pdf&#34;&gt;&lt;strong&gt;A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Lee, Soochan, Ha, Junsoo, Zhang, Dongsu and Kim, Gunhee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1540-L1547&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;This paper introduces expansion-based approach for task-free continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.10123.pdf&#34;&gt;&lt;strong&gt;Task Agnostic Continual Learning Using Online Variational Bayes&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Chen Zeno, Itay Golan, Elad Hoffer and Daniel Soudry&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L580-L589&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces an optimizer for CL that relies on closed form updates of mu and sigma of BNN; introduce label trick for class learning (single-head) but warning: it isn&#39;t really task-agnostic&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Regularization Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.01834&#34;&gt;&lt;strong&gt;Continual Learning in Deep Networks: an Analysis of the Last Layer&lt;/strong&gt;&lt;/a&gt; , (arXiv 2021) by &lt;em&gt;Lesort, Timoth{&#39;e}e, George, Thomas and Rish, Irina&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3245-L3252&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SJlsFpVtDB&#34;&gt;&lt;strong&gt;Continual Learning with Bayesian Neural Networks for Non-Stationary Data&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt and Stephan Günnemann&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L888-L895&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;continual learning for non-stationary data using Bayesian neural networks and memory-based online variational Bayes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.02099&#34;&gt;&lt;strong&gt;Improving and Understanding Variational Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Siddharth Swaroop, Cuong V. Nguyen, Thang D. Bui and Richard E. Turner&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L545-L553&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Improved results and interpretation of VCL.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/8690-uncertainty-based-continual-learning-with-adaptive-regularization.pdf&#34;&gt;&lt;strong&gt;Uncertainty-based Continual Learning with Adaptive Regularization&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Ahn, Hongjoon, Cha, Sungmin, Lee, Donggyu and Moon, Taesup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L567-L577&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces VCL with uncertainty measured for neurons instead of weights.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.11356&#34;&gt;&lt;strong&gt;Functional Regularisation for Continual Learning with Gaussian Processes&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Titsias, Michalis K, Schwarz, Jonathan, Matthews, Alexander G de G, Pascanu, Razvan and Teh, Yee Whye&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1351-L1358&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;functional regularisation for Continual Learning: avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.10123.pdf&#34;&gt;&lt;strong&gt;Task Agnostic Continual Learning Using Online Variational Bayes&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Chen Zeno, Itay Golan, Elad Hoffer and Daniel Soudry&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L580-L589&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces an optimizer for CL that relies on closed form updates of mu and sigma of BNN; introduce label trick for class learning (single-head) but warning: it isn&#39;t really task-agnostic&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=B1al7jg0b&#34;&gt;&lt;strong&gt;Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation&lt;/strong&gt;&lt;/a&gt; , (ICLR 2018) by &lt;em&gt;Xu He and Herbert Jaeger&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L621-L628&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Conceptor-Aided Backprop (CAB): gradients are shielded by conceptors against degradation of previously learned tasks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v80/serra18a.html&#34;&gt;&lt;strong&gt;Overcoming Catastrophic Forgetting with Hard Attention to the Task&lt;/strong&gt;&lt;/a&gt; , (ICML 2018) by &lt;em&gt;Serra, Joan, Suris, Didac, Miron, Marius and Karatzoglou, Alexandros&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L641-L657&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing a hard attention idea with binary masks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.10112&#34;&gt;&lt;strong&gt;Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence&lt;/strong&gt;&lt;/a&gt; , (ECCV 2018) by &lt;em&gt;Chaudhry, Arslan, Dokania, Puneet K, Ajanthan, Thalaiyasingam and Torr, Philip HS&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L660-L667&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formalizes the shortcomings of multi-head evaluation, as well as the importance of replay in single-head setup. Presenting an improved version of EWC.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.10628&#34;&gt;&lt;strong&gt;Variational Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2018) by &lt;em&gt;Cuong V. Nguyen, Yingzhen Li, Thang D. Bui and Richard E. Turner&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L691-L698&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.06370&#34;&gt;&lt;strong&gt;Progress &amp;amp; compress: A scalable framework for continual learning&lt;/strong&gt;&lt;/a&gt; , (ICML 2018) by &lt;em&gt;Schwarz, Jonathan, Luketina, Jelena, Czarnecki, Wojciech M, Grabska-Barwinska, Agnieszka, Teh, Yee Whye, Pascanu, Razvan and Hadsell, Raia&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L702-L709&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A new P\&amp;amp;C architecture; online EWC for keeping the knowledge about the previous task, knowledge for keeping the knowledge about the current task (Multi-head setting, RL)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Online structured laplace approximations for overcoming catastrophic forgetting&lt;/strong&gt;, (NeurIPS 2018) by &lt;em&gt;Ritter, Hippolyt, Botev, Aleksandar and Barber, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2056-L2063&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.10644&#34;&gt;&lt;strong&gt;Facilitating Bayesian Continual Learning by Natural Gradients and Stein Gradients&lt;/strong&gt;&lt;/a&gt; , (NeurIPS Workshop 2018) by &lt;em&gt;Chen, Yu, Diethe, Tom and Lawrence, Neil&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2337-L2344&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Improves on VCL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34;&gt;&lt;strong&gt;Overcoming catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (PNAS 2017) by &lt;em&gt;Kirkpatrick, James, Pascanu, Razvan, Rabinowitz, Neil, Veness, Joel, Desjardins, Guillaume, Rusu, Andrei A, Milan, Kieran, Quan, John, Ramalho, Tiago, Grabska-Barwinska, Agnieszka and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L455-L463&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1711.09601&#34;&gt;&lt;strong&gt;Memory Aware Synapses: Learning what (not) to forget&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach and Tinne Tuytelaars&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L673-L686&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Importance of parameter measured based on their contribution to change in the learned prediction function&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v70/zenke17a.html&#34;&gt;&lt;strong&gt;Continual Learning Through Synaptic Intelligence&lt;/strong&gt;&lt;/a&gt; , (ICML 2017) by *Zenke, Friedeman, Poole, Ben and Ganguli, Surya * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L712-L727&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Synaptic Intelligence (SI). Importance of parameter measured based on their contribution to change in the loss. &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overcoming catastrophic forgetting by incremental moment matching&lt;/strong&gt;, (NeurIPS 2017) by &lt;em&gt;Lee, Sang-Woo, Kim, Jin-Hwa, Jun, Jaehyun, Ha, Jung-Woo and Zhang, Byoung-Tak&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1925-L1932&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Distillation Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.00766&#34;&gt;&lt;strong&gt;Class-Incremental Continual Learning into the eXtended DER-verse&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Buzzega, Pietro, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L10-L19&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.00388&#34;&gt;&lt;strong&gt;Transfer without Forgetting&lt;/strong&gt;&lt;/a&gt; , (ECCV 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Porrello, Angelo, Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Spampinato, Concetto and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L46-L53&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04215&#34;&gt;&lt;strong&gt;Self-Supervised Models are Continual Learners&lt;/strong&gt;&lt;/a&gt; , (CVPR 2022) by &lt;em&gt;Fini, Enrico, da Costa, Victor G Turrisi, Alameda-Pineda, Xavier, Ricci, Elisa, Alahari, Karteek and Mairal, Julien&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3430-L3437&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Explores Continual Self-Supervised Learning and proposes a simple and effective feature distillation method&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.14098&#34;&gt;&lt;strong&gt;Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2022) by &lt;em&gt;Yang, Guanglei, Fini, Enrico, Xu, Dan, Rota, Paolo, Ding, Mingli, Nabi, Moin, Alameda-Pineda, Xavier and Ricci, Elisa&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3440-L3448&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.00432&#34;&gt;&lt;strong&gt;Continual Attentive Fusion for Incremental Learning in Semantic Segmentation&lt;/strong&gt;&lt;/a&gt; , (TMM 2022) by &lt;em&gt;Yang, Guanglei, Fini, Enrico, Xu, Dan, Rota, Paolo, Ding, Mingli, Hao, Tang, Alameda-Pineda, Xavier and Ricci, Elisa&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3450-L3458&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf&#34;&gt;&lt;strong&gt;Dark Experience for General Continual Learning: a Strong, Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2020) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo, Abati, Davide and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L270-L280&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730715.pdf&#34;&gt;&lt;strong&gt;Online Continual Learning under Extreme Memory Constraints&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Fini, Enrico, Lathuilière, Stèphane, Sangineto, Enver, Nabi, Moin and Ricci, Elisa&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2497-L2504&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces Memory-Constrained Online Continual Learning, a setting where no information can be transferred between tasks, and proposes a distillation-based solution (Batch-level Distillation)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650086.pdf&#34;&gt;&lt;strong&gt;PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Douillard, Arthur, Cord, Matthieu, Ollion, Charles, Robert, Thomas and Valle, Eduardo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2566-L2573&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Novel knowledge distillation that trades efficiently rigidity and plasticity to learn large amount of small tasks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.12648&#34;&gt;&lt;strong&gt;Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild&lt;/strong&gt;&lt;/a&gt; , (ICCV 2019) by &lt;em&gt;Lee, Kibok, Lee, Kimin, Shin, Jinwoo and Lee, Honglak&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1057-L1065&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing global distillation loss and balanced finetuning; leveraging unlabeled data in the open world setting (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34;&gt;&lt;strong&gt;Large scale incremental learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2019) by &lt;em&gt;Wu, Yue, Chen, Yinpeng, Wang, Lijuan, Ye, Yuancheng, Liu, Zicheng, Guo, Yandong and Fu, Yun&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1068-L1076&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing bias parameters to the last fully connected layer to resolve the data imbalance issue (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continual Reinforcement Learning deployed in Real-life using PolicyDistillation and Sim2Real Transfer&lt;/strong&gt;, (ICML Workshop 2019) by *Kalifou, René Traoré, Caselles-Dupré, Hugo, Lesort, Timothée, Sun, Te, Diaz-Rodriguez, Natalia and Filliat, David * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1253-L1259&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34;&gt;&lt;strong&gt;Lifelong learning via progressive distillation and retrospection&lt;/strong&gt;&lt;/a&gt; , (ECCV 2018) by &lt;em&gt;Hou, Saihui, Pan, Xinyu, Change Loy, Chen, Wang, Zilei and Lin, Dahua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1079-L1087&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing an expert of the current task in the knowledge distillation method (Multi-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.09536&#34;&gt;&lt;strong&gt;End-to-end incremental learning&lt;/strong&gt;&lt;/a&gt; , (ECCV 2018) by &lt;em&gt;Castro, Francisco M, Marin-Jimenez, Manuel J, Guil, Nicolas, Schmid, Cordelia and Alahari, Karteek&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1090-L1098&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Finetuning the last fully connected layer with a balanced dataset to resolve the data imbalance issue (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.09282&#34;&gt;&lt;strong&gt;Learning without forgetting&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2017) by &lt;em&gt;Li, Zhizhong and Hoiem, Derek&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L730-L738&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Functional regularization through distillation (keeping the output of the updated network on the new data close to the output of the old network on the new data)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34;&gt;&lt;strong&gt;icarl: Incremental classifier and representation learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2017) by &lt;em&gt;Rebuffi, Sylvestre-Alvise, Kolesnikov, Alexander, Sperl, Georg and Lampert, Christoph H&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1101-L1109&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Binary cross-entropy loss for representation learning \&amp;amp; exemplar memory (or coreset) for replay (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Rehearsal Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.06443&#34;&gt;&lt;strong&gt;On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2022) by &lt;em&gt;Bonicelli, Lorenzo, Boschini, Matteo, Porrello, Angelo, Spampinato, Concetto and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1-L8&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.00766&#34;&gt;&lt;strong&gt;Class-Incremental Continual Learning into the eXtended DER-verse&lt;/strong&gt;&lt;/a&gt; , (TPAMI 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Buzzega, Pietro, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L10-L19&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06552&#34;&gt;&lt;strong&gt;Continual semi-supervised learning through contrastive interpolation consistency&lt;/strong&gt;&lt;/a&gt; , (PRL 2022) by &lt;em&gt;Boschini, Matteo, Buzzega, Pietro, Bonicelli, Lorenzo, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L33-L44&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.00388&#34;&gt;&lt;strong&gt;Transfer without Forgetting&lt;/strong&gt;&lt;/a&gt; , (ECCV 2022) by &lt;em&gt;Boschini, Matteo, Bonicelli, Lorenzo, Porrello, Angelo, Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Spampinato, Concetto and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L46-L53&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02577&#34;&gt;&lt;strong&gt;Effects of Auxiliary Knowledge on Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2022) by &lt;em&gt;Bellitto, Giovanni, Pennisi, Matteo, Palazzo, Simone, Bonicelli, Lorenzo, Boschini, Matteo, Calderara, Simone and Spampinato, Concetto&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L158-L165&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9412614&#34;&gt;&lt;strong&gt;Rethinking Experience Replay: a Bag of Tricks for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICPR 2021) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L257-L268&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HHSEKOnPvaO&#34;&gt;&lt;strong&gt;Graph-Based Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2021) by &lt;em&gt;Binh Tang and David S. Matteson&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2641-L2648&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Use graphs to link saved samples and improve the memory quality.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2009.00093.pdf&#34;&gt;&lt;strong&gt;Online Class-Incremental Continual Learning with Adversarial Shapley Value&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Dongsub Shim, Zheda Mai, Jihwan Jeong*, Scott Sanner, Hyunwoo Kim and Jongseong Jang&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2798-L2805&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Use Shapley Value adversarially to select which samples to relay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/b704ea2c39778f07c617f6b7ce480e9e-Paper.pdf&#34;&gt;&lt;strong&gt;Dark Experience for General Continual Learning: a Strong, Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2020) by &lt;em&gt;Buzzega, Pietro, Boschini, Matteo, Porrello, Angelo, Abati, Davide and Calderara, Simone&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L270-L280&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GDumb: A simple approach that questions our progress in continual learning&lt;/strong&gt;, (ECCV 2020) by &lt;em&gt;Prabhu, Ameya, Torr, Philip HS and Dokania, Puneet K&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L282-L290&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introduces a super simple methods that outperforms almost all methods in all of the CL benchmarks. We need new better benchamrks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00487&#34;&gt;&lt;strong&gt;Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Timothée Lesort&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L337-L346&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580409.pdf&#34;&gt;&lt;strong&gt;Imbalanced Continual Learning with Partitioning Reservoir Sampling&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Kim, Chris Dongjoo, Jeong, Jinseo and Kim, Gunhee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2470-L2477&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650086.pdf&#34;&gt;&lt;strong&gt;PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Douillard, Arthur, Cord, Matthieu, Ollion, Charles, Robert, Thomas and Valle, Eduardo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2566-L2573&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Novel knowledge distillation that trades efficiently rigidity and plasticity to learn large amount of small tasks&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650681.pdf&#34;&gt;&lt;strong&gt;{REMIND Your Neural Network to Prevent Catastrophic Forgetting}&lt;/strong&gt;&lt;/a&gt; , (ECCV 2020) by &lt;em&gt;Hayes, Tyler L., Kafle, Kushal, Shrestha, Robik and Acharya, Manoj and Kanan, Christopher&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2585-L2593&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.00420&#34;&gt;&lt;strong&gt;Efficient Lifelong Learning with A-GEM&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Chaudhry, Arslan, Ranzato, Marc’Aurelio, Rohrbach, Marcus and Elhoseiny, Mohamed&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L445-L452&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;More efficient GEM; Introduces online continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.07104&#34;&gt;&lt;strong&gt;Orthogonal Gradient Descent for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Mehrdad Farajtabar, Navid Azizan, Alex Mott and Ang Li&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L799-L808&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf&#34;&gt;&lt;strong&gt;Gradient based sample selection for online continual learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Aljundi, Rahaf, Lin, Min, Goujaud, Baptiste and Bengio, Yoshua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L812-L822&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;sample selection as a constraint reduction problem based on the constrained optimization view of continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf&#34;&gt;&lt;strong&gt;Online Continual Learning with Maximal Interfered Retrieval&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Aljundi, Rahaf and , Lucas, Belilovsky, Eugene, Caccia, Massimo, Lin, Min, Charlin, Laurent and Tuytelaars, Tinne&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L826-L837&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Controlled sampling of memories for replay to automatically rehearse on tasks currently undergoing the most forgetting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.08019&#34;&gt;&lt;strong&gt;Online Learned Continual Compression with Adaptative Quantization Module&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;Caccia, Lucas, Belilovsky, Eugene, Caccia, Massimo and Pineau, Joelle&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L841-L848&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Uses stacks of VQ-VAE modules to progressively compress the data stream, enabling better rehearsal&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34;&gt;&lt;strong&gt;Large scale incremental learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2019) by &lt;em&gt;Wu, Yue, Chen, Yinpeng, Wang, Lijuan, Ye, Yuancheng, Liu, Zicheng, Guo, Yandong and Fu, Yun&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1068-L1076&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introducing bias parameters to the last fully connected layer to resolve the data imbalance issue (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning a Unified Classifier Incrementally via Rebalancing&lt;/strong&gt;, (CVPR 2019) by &lt;em&gt;Hou, Saihui, Pan, Xinyu, Loy, Chen Change, Wang, Zilei and Lin, Dahua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1113-L1120&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continual Reinforcement Learning deployed in Real-life using PolicyDistillation and Sim2Real Transfer&lt;/strong&gt;, (ICML Workshop 2019) by *Kalifou, René Traoré, Caselles-Dupré, Hugo, Lesort, Timothée, Sun, Te, Diaz-Rodriguez, Natalia and Filliat, David * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1253-L1259&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.11682&#34;&gt;&lt;strong&gt;Experience replay for continual learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Rolnick, David, Ahuja, Arun, Schwarz, Jonathan, Lillicrap, Timothy and Wayne, Gregory&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1461-L1469&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/7225-gradient-episodic-memory-for-continual-learning.pdf&#34;&gt;&lt;strong&gt;Gradient Episodic Memory for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Lopez-Paz, David and Ranzato, Marc-Aurelio&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L467-L477&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A model that alliviates CF via constrained optimization&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34;&gt;&lt;strong&gt;icarl: Incremental classifier and representation learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2017) by &lt;em&gt;Rebuffi, Sylvestre-Alvise, Kolesnikov, Alexander, Sperl, Georg and Lampert, Christoph H&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1101-L1109&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Binary cross-entropy loss for representation learning \&amp;amp; exemplar memory (or coreset) for replay (Single-head setting)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doi.org/10.1080/09540099550039318&#34;&gt;&lt;strong&gt;Catastrophic Forgetting, Rehearsal and Pseudorehearsal&lt;/strong&gt;&lt;/a&gt; , (1995) by * Anthony Robins * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2075-L2088&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generative Replay Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00487&#34;&gt;&lt;strong&gt;Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Timothée Lesort&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L337-L346&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baicsworkshop.github.io/pdf/BAICS_8.pdf&#34;&gt;&lt;strong&gt;Brain-Like Replay For Continual Learning With Artificial Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;van de Ven, Gido M, Siegelmann, Hava T and Tolias, Andreas S&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L415-L421&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html&#34;&gt;&lt;strong&gt;Learning to remember: A synaptic plasticity driven framework for continual learning&lt;/strong&gt;&lt;/a&gt; , (CVPR 2019) by &lt;em&gt;Ostapenko, Oleksiy, Puscas, Mihai, Klein, Tassilo, Jahnichen, Patrick and Nabi, Moin&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L305-L313&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;introdudes Dynamic generative memory (DGM) which relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01951954&#34;&gt;&lt;strong&gt;Generative Models from the perspective of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCNN 2019) by &lt;em&gt;Lesort, Timothée, Caselles-Dupré, Hugo, Garcia-Ortiz, Michael, Goudou, Jean-Fran{\c c}ois and Filliat, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L954-L966&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive evaluation of CL methods for generative modeling&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=3367471.3367504&#34;&gt;&lt;strong&gt;Closed-loop Memory GAN for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCAI 2019) by &lt;em&gt;Rios, Amanda and Itti, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1261-L1275&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.12069&#34;&gt;&lt;strong&gt;Marginal replay vs conditional replay for continual learning&lt;/strong&gt;&lt;/a&gt; , (ICANN 2019) by &lt;em&gt;Lesort, Timothée, Gepperth, Alexander, Stoian, Andrei and Filliat, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1406-L1415&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive evaluation of generative replay methods&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1809.10635&#34;&gt;&lt;strong&gt;Generative replay with feedback connections as a general strategy for continual learning&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Michiel van der Ven and Andreas S. Tolias&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L852-L860&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;smarter Generative Replay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.08690&#34;&gt;&lt;strong&gt;Continual learning with deep generative replay&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2017) by &lt;em&gt;Shin, Hanul, Lee, Jung Kwon, Kim, Jaehong and Kim, Jiwon&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L481-L489&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces generative replay&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dynamic Architectures or Routing Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnbot.2022.846219/full&#34;&gt;&lt;strong&gt;Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Iyer, Abhiram, Grewal, Karan, Velu, Akash, Souza, Lucas Oliveira, Forest, Jeremy and Ahmad, Subutai&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L77-L86&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;bio-inspired method which dynamically restrict and route information in a context-specific manner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11326&#34;&gt;&lt;strong&gt;DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion&lt;/strong&gt;&lt;/a&gt; , (arXiv 2021) by &lt;em&gt;Douillard, Arthur, Ram{&#39;e}, Alexandre, Couairon, Guillaume and Cord, Matthieu&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3313-L3320&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.14769&#34;&gt;&lt;strong&gt;Supermasks in superposition&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Wortsman, Mitchell, Ramanujan, Vivek, Liu, Rosanne, Kembhavi, Aniruddha, Rastegari, Mohammad, Yosinski, Jason and Farhadi, Ali&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L65-L74&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a binary mask over the network is inferred based on the input, and only the masked part of the network is used to train/infer&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1902.09432&#34;&gt;&lt;strong&gt;ORACLE: Order Robust Adaptive Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Jaehong Yoon and Saehoon Kim and Eunho Yang and Sung Ju Hwang&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L592-L608&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1904.00310&#34;&gt;&lt;strong&gt;Learn to Grow: {A} Continual Structure Learning Framework for Overcoming Catastrophic Forgetting&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Xilai Li and Yingbo Zhou and Tianfu Wu and Richard Socher and Caiming Xiong&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2206-L2224&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/doi/pdf/10.1073/pnas.1803839115&#34;&gt;&lt;strong&gt;Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Masse, Nicolas Y, Grant, Gregory D and Freedman, David J&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L90-L101&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a network trained to do CL where select subnetworks are used to learn each task; these subnetworks are chosen a priori&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=ryj0790hb&#34;&gt;&lt;strong&gt;Incremental Learning through Deep Adaptation&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Amir Rosenfeld and John K. Tsotsos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L753-L759&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Packnet: Adding multiple tasks to a single network by iterative pruning&lt;/strong&gt;, (CVPR 2018) by &lt;em&gt;Mallya, Arun and Lazebnik, Svetlana&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1123-L1130&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Piggyback: Adapting a single network to multiple tasks by learning to mask weights&lt;/strong&gt;, (ECCV 2018) by &lt;em&gt;Mallya, Arun, Davis, Dillon and Lazebnik, Svetlana&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1132-L1139&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.05202&#34;&gt;&lt;strong&gt;Continual Learning in Practice&lt;/strong&gt;&lt;/a&gt; , (NeurIPS Workshop 2018) by &lt;em&gt;Diethe, Tom, Borchert, Tom, Thereska, Eno, Pigem, Borja de Balle and Lawrence, Neil&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2327-L2334&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a reference architecture for a continual learning system&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Growing a brain: Fine-tuning by increasing model capacity&lt;/strong&gt;, (CVPR 2017) by &lt;em&gt;Wang, Yu-Xiong, Ramanan, Deva and Hebert, Martial&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1141-L1148&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1701.08734&#34;&gt;&lt;strong&gt;PathNet: Evolution Channels Gradient Descent in Super Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1796-L1816&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lifelong learning with dynamically expandable networks&lt;/strong&gt;, (arXiv 2017) by &lt;em&gt;Yoon, Jaehong, Yang, Eunho, Lee, Jeongtae and Hwang, Sung Ju&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2066-L2072&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1606.04671&#34;&gt;&lt;strong&gt;Progressive Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2016) by &lt;em&gt;Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G. and Soyer, H., Kirkpatrick, J., Kavukcuoglu, K. and Pascanu, R. and Hadsell, R.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L761-L776&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Each task have a specific model connected to the previous ones&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hybrid Methods&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=SJgwNerKvB&#34;&gt;&lt;strong&gt;Continual learning with hypernetworks&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Johannes von Oswald, Christian Henning, João Sacramento and Benjamin F. Grewe&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1005-L1012&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Learning task-conditioned hypernetworks for continual learning as well as task embeddings; hypernetwors offers good model compression.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.06562&#34;&gt;&lt;strong&gt;Compacting, Picking and Growing for Unforgetting Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Hung, Ching-Yi, Tu, Cheng-Hao, Wu, Cheng-En, Chen, Chien-Hung, Chan, Yi-Ming and Chen, Chu-Song&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L534-L542&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. All enforced in an iterative manner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.00689.pdf&#34;&gt;&lt;strong&gt;A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Lee, Soochan, Ha, Junsoo, Zhang, Dongsu and Kim, Gunhee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1540-L1547&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;This paper introduces expansion-based approach for task-free continual learning&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Continual Few-Shot Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html&#34;&gt;&lt;strong&gt;Learning where to learn: Gradient sparsity in meta and continual learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Von Oswald, Johannes, Zhao, Dominic, Kobayashi, Seijin, Schug, Simon, Caccia, Massimo, Zucchet, Nicolas and Sacramento, Jo{~a}o&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L247-L255&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.04546&#34;&gt;&lt;strong&gt;Wandering Within a World: Online Contextualized Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer and Richard S. Zemel&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L325-L333&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;proposes a new continual few-shot setting where spacial and temporal context can be leveraged to and unseen classes need to be predicted&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.11967&#34;&gt;&lt;strong&gt;Defining Benchmarks for Continual Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Antoniou, Antreas, Patacchiola, Massimiliano, Ochal, Mateusz and Storkey, Amos&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L367-L374&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;(title is a good enough summary)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05856&#34;&gt;&lt;strong&gt;Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Caccia, Massimo, Rodriguez, Pau, Ostapenko, Oleksiy, Normandin, Fabrice, Lin, Min, Caccia, Lucas, Laradji, Issam, Rish, Irina, Lacoste, Alexandre, Vazquez, David and Charlin, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1473-L1480&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes a new approach to CL evaluation more aligned with real-life applications, bringing CL closer to Online Learning and Open-World learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.04695&#34;&gt;&lt;strong&gt;Learning from the Past: Continual Meta-Learning via Bayesian Graph Modeling&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Yadan Luo, Zi Huang, Zheng Zhang, Ziwei Wang, Mahsa Baktashmotlagh and Yang Yang&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L876-L885&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v97/finn19a.html&#34;&gt;&lt;strong&gt;Online Meta-Learning&lt;/strong&gt;&lt;/a&gt; , (ICML 2019) by &lt;em&gt;Finn, Chelsea, Rajeswaran, Aravind, Kakade, Sham and Levine, Sergey&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L898-L913&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;defines Online Meta-learning; propsoses Follow the Meta Leader (FTML) (~ Online MAML)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/9112-reconciling-meta-learning-and-continual-learning-with-online-mixtures-of-tasks.pdf&#34;&gt;&lt;strong&gt;Reconciling meta-learning and continual learning with online mixtures of tasks&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Jerfel, Ghassen, Grant, Erin, Griffiths, Tom and Heller, Katherine A&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L917-L927&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Meta-learns a tasks structure; continual adaptation via non-parametric prior&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HyxAfnA5tm&#34;&gt;&lt;strong&gt;Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Anusha Nagabandi, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L932-L939&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formulates an online learning procedure that uses SGD to update model parameters, and an EM with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distribution&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.05201&#34;&gt;&lt;strong&gt;Task Agnostic Continual Learning via Meta Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A. Rusu, Yee Whye Teh and Razvan Pascanu&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1028-L1036&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces What \&amp;amp; How framework; enables Task Agnostic CL with meta learned task inference&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Meta-Continual Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html&#34;&gt;&lt;strong&gt;Learning where to learn: Gradient sparsity in meta and continual learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Von Oswald, Johannes, Zhao, Dominic, Kobayashi, Seijin, Schug, Simon, Caccia, Massimo, Zucchet, Nicolas and Sacramento, Jo{~a}o&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L247-L255&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.13904&#34;&gt;&lt;strong&gt;La-MAML: Look-ahead Meta Learning for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Gunshi Gupta, Karmesh Yadav and Liam Paull&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L316-L322&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Proposes an online replay-based meta-continual learning algorithm with learning-rate modulation to mitigate catastrophic forgetting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.09571&#34;&gt;&lt;strong&gt;Learning to Continually Learn&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Beaulieu, Shawn, Frati, Lapo, Miconi, Thomas, Lehman, Joel, Stanley, Kenneth O, Clune, Jeff and Cheney, Nick&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1431-L1438&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Follow-up of OML. Meta-learns an activation-gating function instead.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/8458-meta-learning-representations-for-continual-learning.pdf&#34;&gt;&lt;strong&gt;Meta-Learning Representations for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Javed, Khurram and White, Martha&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L863-L873&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces Learns how to continually learn (OML) i.e. learns how to do online updates without forgetting.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1909.04170.pdf&#34;&gt;&lt;strong&gt;Meta-learnt priors slow down catastrophic forgetting in neural networks&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;Spigler, Giacomo&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1441-L1448&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Learning MAML in a Meta continual learning way slows down forgetting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=B1gTShAct7&#34;&gt;&lt;strong&gt;Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu and and Gerald Tesauro&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1452-L1459&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lifelong Reinforcement Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.03934&#34;&gt;&lt;strong&gt;A Study of Continual Learning Methods for Q-Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2022) by &lt;em&gt;Bagus, Benedikt and Gepperth, Alexander&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L55-L62&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Studies Q-Learning methods in CRL environments. When there&#39;s no task interference, (A-)GEM can outperform Experience Replay&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=PVJ6j87gOHz&#34;&gt;&lt;strong&gt;Co{MPS}: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (ICLR 2022) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L126-L133&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Co{MPS} is a novel meta-policy search algorithm for task-agnostic continual RL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.14495&#34;&gt;&lt;strong&gt;Task-Agnostic Continual Reinforcement Learning: In Praise of a Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (arXiv 2022) by &lt;em&gt;Caccia, Massimo, Mueller, Jonas, Kim, Taesup, Charlin, Laurent and Fakoor, Rasool&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L136-L143&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;combines replay and an RNN to set a simple baseline for TACRL: shows that the baselines matches and surpasses previously thought upper bounds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=5XmLzdslFNN&#34;&gt;&lt;strong&gt;Modular Lifelong Reinforcement Learning via Neural Composition&lt;/strong&gt;&lt;/a&gt; , (ICLR 2022) by &lt;em&gt;Jorge A Mendez, Harm van Seijen and ERIC EATON&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L180-L187&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05742&#34;&gt;&lt;strong&gt;Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Steinparz, Christian, Schmied, Thomas, Paischer, Fabian, Dinu, Marius-Constantin, Patil, Vihang, Bitto-Nemling, Angela, Eghbal-zadeh, Hamid and Hochreiter, Sepp&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3460-L3467&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Detects changes and explores when and where they happen to recover from non-stationarity.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02940&#34;&gt;&lt;strong&gt;Same State, Different Task: Continual Reinforcement Learning without Interference&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Samuel Kessler, Jack Parker-Holder, Philip J. Ball, Stefan Zohren and Stephen J. Roberts&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L114-L122&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;learns multiple policies and cast policy-retrieval as a multi-arm bandit problem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04467&#34;&gt;&lt;strong&gt;CoMPS: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L189-L198&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HIGSa_3kOx3&#34;&gt;&lt;strong&gt;Reset-Free Lifelong Learning with Skill-Space Planning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2021) by &lt;em&gt;Kevin Lu, Aditya Grover, Pieter Abbeel and Igor Mordatch&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2725-L2732&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11441&#34;&gt;&lt;strong&gt;Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengdi Xu, Wenhao Ding, Jiacheng Zhu, Zuxin Liu, Baiming Chen and Ding Zhao&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L146-L154&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;uses an infinite mixture of Gaussian Processes to learn a task-agnostic policy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.07011&#34;&gt;&lt;strong&gt;Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Jorge A. Mendez, Boyu Wang and Eric Eaton&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L200-L210&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Towards Continual Reinforcement Learning: A Review and Perspectives&lt;/strong&gt;, (2020) by &lt;em&gt;Khimya Khetarpal, Matthew Riemer, Irina Rish and Doina Precup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L293-L301&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A review on continual reinforcement learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S1566253519307377&#34;&gt;&lt;strong&gt;Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges&lt;/strong&gt;&lt;/a&gt; , (Information Fusion 2020) by &lt;em&gt;Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia Díaz-Rodríguez&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1321-L1332&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HyxAfnA5tm&#34;&gt;&lt;strong&gt;Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Anusha Nagabandi, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L932-L939&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formulates an online learning procedure that uses SGD to update model parameters, and an EM with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distribution&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continual Reinforcement Learning deployed in Real-life using PolicyDistillation and Sim2Real Transfer&lt;/strong&gt;, (ICML Workshop 2019) by *Kalifou, René Traoré, Caselles-Dupré, Hugo, Lesort, Timothée, Sun, Te, Diaz-Rodriguez, Natalia and Filliat, David * &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1253-L1259&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.11682&#34;&gt;&lt;strong&gt;Experience replay for continual learning&lt;/strong&gt;&lt;/a&gt; , (NeurIPS 2019) by &lt;em&gt;Rolnick, David, Ahuja, Arun, Schwarz, Jonathan, Lillicrap, Timothy and Wayne, Gregory&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1461-L1469&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/1701.08734&#34;&gt;&lt;strong&gt;PathNet: Evolution Channels Gradient Descent in Super Neural Networks&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1796-L1816&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incremental robot learning of new objects with fixed update time&lt;/strong&gt;, (2017) by &lt;em&gt;R. {Camoriano}, G. {Pasquale}, C. {Ciliberto}, L. {Natale}, L. {Rosasco} and G. {Metta}&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1934-L1946&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Task-Agnostic Lifelong Reinforcement Learning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=PVJ6j87gOHz&#34;&gt;&lt;strong&gt;Co{MPS}: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (ICLR 2022) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L126-L133&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Co{MPS} is a novel meta-policy search algorithm for task-agnostic continual RL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.14495&#34;&gt;&lt;strong&gt;Task-Agnostic Continual Reinforcement Learning: In Praise of a Simple Baseline&lt;/strong&gt;&lt;/a&gt; , (arXiv 2022) by &lt;em&gt;Caccia, Massimo, Mueller, Jonas, Kim, Taesup, Charlin, Laurent and Fakoor, Rasool&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L136-L143&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;combines replay and an RNN to set a simple baseline for TACRL: shows that the baselines matches and surpasses previously thought upper bounds&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05742&#34;&gt;&lt;strong&gt;Reactive Exploration to Cope with Non-Stationarity in Lifelong Reinforcement Learning&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Steinparz, Christian, Schmied, Thomas, Paischer, Fabian, Dinu, Marius-Constantin, Patil, Vihang, Bitto-Nemling, Angela, Eghbal-zadeh, Hamid and Hochreiter, Sepp&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3460-L3467&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Detects changes and explores when and where they happen to recover from non-stationarity.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02940&#34;&gt;&lt;strong&gt;Same State, Different Task: Continual Reinforcement Learning without Interference&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Samuel Kessler, Jack Parker-Holder, Philip J. Ball, Stefan Zohren and Stephen J. Roberts&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L114-L122&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;learns multiple policies and cast policy-retrieval as a multi-arm bandit problem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04467&#34;&gt;&lt;strong&gt;CoMPS: Continual Meta Policy Search&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L189-L198&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11441&#34;&gt;&lt;strong&gt;Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Mengdi Xu, Wenhao Ding, Jiacheng Zhu, Zuxin Liu, Baiming Chen and Ding Zhao&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L146-L154&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;uses an infinite mixture of Gaussian Processes to learn a task-agnostic policy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=HyxAfnA5tm&#34;&gt;&lt;strong&gt;Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL&lt;/strong&gt;&lt;/a&gt; , (ICLR 2019) by &lt;em&gt;Anusha Nagabandi, Chelsea Finn and Sergey Levine&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L932-L939&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Formulates an online learning procedure that uses SGD to update model parameters, and an EM with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distribution&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Continual Generative Modeling&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.14481.pdf&#34;&gt;&lt;strong&gt;Continual Unsupervised Representation Learning&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu and Raia Hadsell&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L942-L951&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Introduces unsupervised continual learning (no task label and no task boundaries)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01951954&#34;&gt;&lt;strong&gt;Generative Models from the perspective of Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCNN 2019) by &lt;em&gt;Lesort, Timothée, Caselles-Dupré, Hugo, Garcia-Ortiz, Michael, Goudou, Jean-Fran{\c c}ois and Filliat, David&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L954-L966&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;Extensive evaluation of CL methods for generative modeling&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=3367471.3367504&#34;&gt;&lt;strong&gt;Closed-loop Memory GAN for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (IJCAI 2019) by &lt;em&gt;Rios, Amanda and Itti, Laurent&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1261-L1275&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1705.09847&#34;&gt;&lt;strong&gt;Lifelong Generative Modeling&lt;/strong&gt;&lt;/a&gt; , (arXiv 2017) by &lt;em&gt;Ramapuram, Jason, Gregorova, Magda and Kalousis, Alexandros&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L969-L976&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Biologically-Inspired&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnbot.2022.846219/full&#34;&gt;&lt;strong&gt;Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Iyer, Abhiram, Grewal, Karan, Velu, Akash, Souza, Lucas Oliveira, Forest, Jeremy and Ahmad, Subutai&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L77-L86&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;bio-inspired method which dynamically restrict and route information in a context-specific manner&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1.full.pdf&#34;&gt;&lt;strong&gt;A rapid and efficient learning rule for biological neural circuits&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Eren Sezener, Agnieszka Grabska-Barwinska, Dimitar Kostadinov, Maxime Beau, Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew M. Botvinick, Claudia Clopath, Michael H{&#34;a}usser and Peter E. Latham&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L104-L111&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pnas.org/doi/pdf/10.1073/pnas.1803839115&#34;&gt;&lt;strong&gt;Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Masse, Nicolas Y, Grant, Gregory D and Freedman, David J&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L90-L101&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a network trained to do CL where select subnetworks are used to learn each task; these subnetworks are chosen a priori&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Miscellaneous&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.07461&#34;&gt;&lt;strong&gt;Learning causal models online&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Javed, Khurram, White, Martha and Bengio, Yoshua&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L237-L244&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.06568&#34;&gt;&lt;strong&gt;On the Limitations of Continual Learning for Malware Classification&lt;/strong&gt;&lt;/a&gt; , (2022) by &lt;em&gt;Rahman, Mohammad Saidur, Coull, Scott E and Wright, Matthew&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L21-L28&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;This paper investigates overcoming catastrophic forgetting for malware classification&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.09578&#34;&gt;&lt;strong&gt;CLOPS: Continual Learning of Physiological Signals&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Kiyasseh, Dani, Zhu, Tingting and Clifton, David A&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L404-L411&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a healthcare-specific replay-based method to mitigate destructive interference during continual learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=Skgxcn4YDS&#34;&gt;&lt;strong&gt;LAMAL: LAnguage Modeling Is All You Need for Lifelong Language Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Fan-Keng Sun, Cheng-Hao Ho and Hung-Yi Lee&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1522-L1529&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=rklnDgHtDS&#34;&gt;&lt;strong&gt;Compositional Language Continual Learning&lt;/strong&gt;&lt;/a&gt; , (ICLR 2020) by &lt;em&gt;Yuanpeng Li, Liang Zhao, Kenneth Church and Mohamed Elhoseiny&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1551-L1558&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;method for compositional continual learning of sequence-to-sequence models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8569992&#34;&gt;&lt;strong&gt;Incremental Lifelong Deep Learning for Autonomous Vehicles&lt;/strong&gt;&lt;/a&gt; , (2018) by &lt;em&gt;Pierre, John M.&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L212-L223&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0925231217309864&#34;&gt;&lt;strong&gt;Unsupervised real-time anomaly detection for streaming data&lt;/strong&gt;&lt;/a&gt; , (2017) by &lt;em&gt;Ahmad, Subutai, Lavin, Alexander, Purdy, Scott and Agha, Zuha&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L377-L387&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;HTM applied to real-world anomaly detection problem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.05463&#34;&gt;&lt;strong&gt;Continuous online sequence learning with an unsupervised neural network model&lt;/strong&gt;&lt;/a&gt; , (2016) by &lt;em&gt;Cui, Yuwei, Ahmad, Subutai and Hawkins, Jeff&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L390-L401&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;HTM applied to a prediction problem of taxi passenger demand&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Thesis&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.00487&#34;&gt;&lt;strong&gt;Continual Learning: Tackling Catastrophic Forgetting in Deep Neural Networks with Replay Processes&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Timothée Lesort&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L337-L346&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://amsdottorato.unibo.it/9073/1/vincenzo_lomonaco_thesis.pdf&#34;&gt;&lt;strong&gt;Continual Learning with Deep Architectures&lt;/strong&gt;&lt;/a&gt; , (2019) by &lt;em&gt;Vincenzo Lomonaco&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1688-L1694&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.02718&#34;&gt;&lt;strong&gt;Continual Learning in Neural Networks&lt;/strong&gt;&lt;/a&gt; , (arXiv 2019) by &lt;em&gt;Aljundi, Rahaf&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2160-L2167&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.utexas.edu/~ring/Ring-dissertation.pdf&#34;&gt;&lt;strong&gt;Continual learning in reinforcement environments&lt;/strong&gt;&lt;/a&gt; , (1994) by &lt;em&gt;Ring, Mark Bishop&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1761-L1768&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Libraries&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/renate&#34;&gt;&lt;strong&gt;Renate: a library for real-world continual learning&lt;/strong&gt;&lt;/a&gt; , (2022) by ** &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L3470-L3478&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for real-world continual learning with integrated hyperparameter tuning.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lebrice/Sequoia&#34;&gt;&lt;strong&gt;Sequoia - Towards a Systematic Organization of Continual Learning Research&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Matthew Riemer, Pau Rodriguez, Julio Hurtado, Khimya Khetarpal, Timothée Lesort, Laurent Charlin, Irina Rish and Massimo Caccia&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2951-L2960&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library that unifies Continual Supervised and Continual Reinforcement Learning research&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://avalanche.continualai.org/&#34;&gt;&lt;strong&gt;Avalanche: an End-to-End Library for Continual Learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Gabriele Graffieti and Antonio Carta&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2973-L2981&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for Continual Supervised Learning&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chandar-lab/Lifelong-Hanabi&#34;&gt;&lt;strong&gt;Continuous Coordination As a Realistic Scenario for Lifelong Learning&lt;/strong&gt;&lt;/a&gt; , (2021) by &lt;em&gt;Hadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville and Sarath Chandar&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2984-L2993&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;River: machine learning for streaming data in Python&lt;/strong&gt;, (2020) by &lt;em&gt;Jacob Montiel, Max Halford, Saulo Martiello Mastelini and Geoffrey Bolmier, Raphael Sourty, Robin Vaysse and Adil Zouitine, Heitor Murilo Gomes, Jesse Read and Talel Abdessalem and Albert Bifet&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2606-L2617&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for online learning.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuum, Data Loaders for Continual Learning&lt;/strong&gt;, (2020) by &lt;em&gt;Douillard, Arthur and Lesort, Timothée&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2620-L2628&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library proposing continual learning scenarios and metrics.&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mmasana/FACIL&#34;&gt;&lt;strong&gt;Framework for Analysis of Class-Incremental Learning&lt;/strong&gt;&lt;/a&gt; , (arXiv 2020) by &lt;em&gt;Masana, Marc, Liu, Xialei, Twardowski, Bartlomiej, Menta, Mikel, Bagdanov, Andrew D and van de Weijer, Joost&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2963-L2970&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;A library for Continual Class-Incremental Learning&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Workshops&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/cl-icml/organizers?authuser=0&#34;&gt;&lt;strong&gt;Workshop on Continual Learning at ICML 2020&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Rahaf Aljundi, Haytham Fayek, Eugene Belilovsky, David Lopez-Paz, Arslan Chaudhry, Marc Pickett, Puneet Dokania, Jonathan Schwarz and Sayna Ebrahimi&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L348-L355&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openreview.net/group?id=ICML.cc/2020/Workshop/LifelongML#accept&#34;&gt;&lt;strong&gt;4th Lifelong Machine Learning Workshop at ICML 2020&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Shagun Sodhani, Sarath Chandar, Balaraman Ravindran and Doina Precup&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L358-L365&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CVPR 2020 Continual Learning in Computer Vision Competition: Approaches, Results, Current Challenges and Future Directions&lt;/strong&gt;, (arXiv 2020) by &lt;em&gt;Lomonaco, Vincenzo, Pellegrini, Lorenzo, Rodriguez, Pau, Caccia, Massimo, She, Qi, Chen, Yu, Jodelet, Quentin, Wang, Ruiping, Mai, Zheda, Vazquez, David and others&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L1562-L1568&#34;&gt;[bib]&lt;/a&gt; &lt;code&gt;surveys the results of the first CL competition at CVPR&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.statmt.org/wmt20/lifelong-learning-task.html&#34;&gt;&lt;strong&gt;1st Lifelong Learning for Machine Translation Shared Task at WMT20 (EMNLP 2020)&lt;/strong&gt;&lt;/a&gt; , (2020) by &lt;em&gt;Loïc Barrault, Magdalena Biesialska, Marta R. Costa-jussà, Fethi Bougares and Olivier Galibert&lt;/em&gt; &lt;a href=&#34;https://github.com/optimass/continual_learning_papers/raw/master/bibtex.bib#L2789-L2791&#34;&gt;[bib]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>llsoftsec/llsoftsecbook</title>
    <updated>2023-02-08T01:47:10Z</updated>
    <id>tag:github.com,2023-02-08:/llsoftsec/llsoftsecbook</id>
    <link href="https://github.com/llsoftsec/llsoftsecbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Low-Level Software Security for Compiler Developers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llsoftsecbook: a book on Low-Level Software Security for Compiler Developers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg?sanitize=true&#34; alt=&#34;License: CC BY 4.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/llsoftsec/llsoftsecbook/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Build book with docker container CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#contributors-&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/all_contributors-12-orange.svg?style=flat-square&#34; alt=&#34;All Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-BADGE:END --&gt; &#xA;&lt;p&gt;This book aims to provide a structured, broad overview of all attacks and security hardening techniques relevant for code generation tools.&lt;/p&gt; &#xA;&lt;h2&gt;Purpose&lt;/h2&gt; &#xA;&lt;p&gt;Compilers, assemblers and similar tools generate all the binary code that processors execute. Therefore, they play a crucial role in hardening binaries against security threats.&lt;/p&gt; &#xA;&lt;p&gt;The variety of attacks and hardening techniques has been rising sharply, and it is becoming difficult to maintain a good broad basic understanding of all of them.&lt;/p&gt; &#xA;&lt;p&gt;The purpose of this book is to help every compiler developer that needs to learn about software security relevant to compilers. It aims to achieve that by providing a description of all relevant high-level aspects of attacks, vulnerabilities, mitigations and hardening techniques. For further details, this book provides pointers to material on specific techniques.&lt;/p&gt; &#xA;&lt;p&gt;Even though the focus is on compiler developers, we expect that this book will also be useful to other people working on low-level software.&lt;/p&gt; &#xA;&lt;h2&gt;Why an open source book?&lt;/h2&gt; &#xA;&lt;p&gt;The idea for this book emerged out of a frustration of not finding a good overview on this topic. Kristof Beyls and Georgia Kouveli, compiler engineers working on security features from time to time, wished a book like this would exist. After not finding such a book, we decided to try and write one ourselves. We immediately realized that we do not have all necessary expertise ourselves to complete such a daunting task. So we decided to try and create this book in an open source style, seeking contributions from many experts.&lt;/p&gt; &#xA;&lt;p&gt;As you read this, the book remains unfinished. This book may well never be finished, as new vulnerabilities continue to be discovered regularly. Our hope is that developing the book as an open source project will allow it to continue to evolve and improve. It being open source increases the likelihood that it remains relevant as new vulnerabilities and mitigations emerge.&lt;/p&gt; &#xA;&lt;p&gt;Kristof and Georgia are far from experts on all possible vulnerabilities. So what is the plan to get high quality content to cover all relevant topics? It is two-fold.&lt;/p&gt; &#xA;&lt;p&gt;First, by studying specific topics, we hope to gain enough knowledge to write up a good summary for this book.&lt;/p&gt; &#xA;&lt;p&gt;Second, we very much invite and welcome contributions. If you&#39;re interested in potentially contributing content, please let us know.&lt;/p&gt; &#xA;&lt;p&gt;As a reader, you can also contribute to making this book better. We highly encourage feedback, both positive and constructive criticisms. We prefer feedback to be received through the GitHub communication channels &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues&#34;&gt;Issues&lt;/a&gt; and &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Live version&lt;/h2&gt; &#xA;&lt;p&gt;A live top-of-main version of the book is available as a webpage at &lt;a href=&#34;https://llsoftsec.github.io/llsoftsecbook&#34;&gt;https://llsoftsec.github.io/llsoftsecbook&lt;/a&gt;. A &lt;a href=&#34;https://llsoftsec.github.io/llsoftsecbook/book.pdf&#34;&gt;PDF&lt;/a&gt; is also available.&lt;/p&gt; &#xA;&lt;h2&gt;Build instructions&lt;/h2&gt; &#xA;&lt;p&gt;You can build the book by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ make all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This requires pandoc, latex and necessary latex packages to be installed. The easiest way to make sure you build the book with the right versions of those tools is to use the script build_with_docker.sh:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ ./build_with_docker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This builds a docker container with the exact versions of pandoc, latex and necessary extra packages; and builds the book using that container.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll find the PDF and HTML versions of the book in build/book.pdf and build/book.html if the build finishes successfully.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please find contribution guidelines in &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/raw/main/contributing.md&#34;&gt;https://github.com/llsoftsec/llsoftsecbook/blob/main/contributing.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors ✨&lt;/h2&gt; &#xA;&lt;p&gt;Thanks goes to these wonderful people (&lt;a href=&#34;https://allcontributors.org/docs/en/emoji-key&#34;&gt;emoji key&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/kbeyls&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/19591946?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Kristof Beyls&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Kristof Beyls&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=kbeyls&#34; title=&#34;Tests&#34;&gt;⚠️&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=kbeyls&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-kbeyls&#34; title=&#34;Content&#34;&gt;🖋&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=kbeyls&#34; title=&#34;Documentation&#34;&gt;📖&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-kbeyls&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#infra-kbeyls&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;🚇&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Akbeyls&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;http://tubafranz.me/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/25690309?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Francesco Petrogalli&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Francesco Petrogalli&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Afpetrogalli&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=fpetrogalli&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#infra-fpetrogalli&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;🚇&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/g-kouv&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6901396?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;g-kouv&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;g-kouv&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Ag-kouv&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-g-kouv&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=g-kouv&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-g-kouv&#34; title=&#34;Content&#34;&gt;🖋&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/statham-arm&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/54840944?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Simon Tatham&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Simon Tatham&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Astatham-arm&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-statham-arm&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=statham-arm&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-statham-arm&#34; title=&#34;Content&#34;&gt;🖋&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/sam-ellis&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6695726?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Sam Ellis&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Sam Ellis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=sam-ellis&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-sam-ellis&#34; title=&#34;Content&#34;&gt;🖋&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3Asam-ellis&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-sam-ellis&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3Asam-ellis&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://www.lyndonfawcett.com&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5150703?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Lyndon Fawcett&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Lyndon Fawcett&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3Alyndon160&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-lyndon160&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/JLouisKaplan-Arm&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/90251161?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Jonathan Louis Kaplan&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Jonathan Louis Kaplan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3AJLouisKaplan-Arm&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-JLouisKaplan-Arm&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=JLouisKaplan-Arm&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3AJLouisKaplan-Arm&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/jacobbramley&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5206553?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Jacob Bramley&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Jacob Bramley&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-jacobbramley&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/joseph-yiu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/77114984?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Joseph Yiu&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Joseph Yiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/commits?author=joseph-yiu&#34; title=&#34;Code&#34;&gt;💻&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#content-joseph-yiu&#34; title=&#34;Content&#34;&gt;🖋&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Arnaud-de-Grandmaison-ARM&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24225823?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Arnaud de Grandmaison&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Arnaud de Grandmaison&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/pulls?q=is%3Apr+reviewed-by%3AArnaud-de-Grandmaison-ARM&#34; title=&#34;Reviewed Pull Requests&#34;&gt;👀&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Fare9&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/13202760?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Fare9&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Fare9&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/llsoftsec/llsoftsecbook/main/#ideas-Fare9&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;🤔&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://homepages.dcc.ufmg.br/~fernando/&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/367846?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Fernando Magno Quintão Pereira&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Fernando Magno Quintão Pereira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/llsoftsec/llsoftsecbook/issues?q=author%3Apronesto&#34; title=&#34;Bug reports&#34;&gt;🐛&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;p&gt;This project follows the &lt;a href=&#34;https://github.com/all-contributors/all-contributors&#34;&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind welcome!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;This book is licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mengchaoheng/SCUT_thesis</title>
    <updated>2023-02-08T01:47:10Z</updated>
    <id>tag:github.com,2023-02-08:/mengchaoheng/SCUT_thesis</id>
    <link href="https://github.com/mengchaoheng/SCUT_thesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;华南理工大学硕博士学位论文LaTeX模板，始于2020年的最新项目，2023年持续更新中。Latex templates for the thesis of South China University of Technology&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;华南理工大学硕/博士学位论文LaTeX模板&lt;/h1&gt; &#xA;&lt;p&gt;本项目始于2020年，可用于撰写华南理工大学硕/博士学位论文。模板由&lt;a href=&#34;https://github.com/alwintsui/scutthesis&#34;&gt;alwintsui&lt;/a&gt;以及&lt;a href=&#34;https://github.com/yecfly/scut-thesis&#34;&gt;yecfly&lt;/a&gt;的模板修改而来，实测可以通过所有格式审核，在此向alwintsui、yecfly致以崇高的敬意！&lt;/p&gt; &#xA;&lt;p&gt;2022更新：&lt;/p&gt; &#xA;&lt;p&gt;根据2021届、2022届硕、博士毕业生的大量反馈意见，本项目已经持续更新两年。本人已于2022年回校读博，会一直维护这个项目，希望后续同学在使用过程中遇到问题积极反馈，若能推送更新参与维护更好！&lt;/p&gt; &#xA;&lt;h2&gt;快速使用：&lt;/h2&gt; &#xA;&lt;p&gt;使用之前可先阅读本文模板编译后生成的使用说明&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/raw/master/scutthesis.pdf&#34;&gt;scutthesis.pdf&lt;/a&gt;文件、并有选择性地阅读&lt;a href=&#34;https://github.com/CTeX-org/lshort-zh-cn.git&#34;&gt;《一份（不太）简短的LATEX 2ε 介绍》&lt;/a&gt; 入门。还可以阅读&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/tree/master/tutorial&#34;&gt;tutorial&lt;/a&gt;里的&lt;code&gt;scutthesis说明.pdf&lt;/code&gt;文件，这是旧模版的文件，可以参考旧模版的东西。使用案例为&lt;a href=&#34;https://github.com/mengchaoheng/SCUTthesis-mengchaoheng.git&#34;&gt;本人的学位论文&lt;/a&gt;(注意该论文是基于初代模版，而现在的模版已经不断更新)。&lt;/p&gt; &#xA;&lt;p&gt;编译之前首先安装&lt;a href=&#34;https://www.tug.org/texlive/&#34;&gt;texlive&lt;/a&gt;，找到对应系统（Linux，win，macOS）的版本。注意macOS是MacTeX。&lt;/p&gt; &#xA;&lt;p&gt;编译有三种方法：&lt;/p&gt; &#xA;&lt;p&gt;1.使用VSCode, 安装LaTeX Workshop插件，在&lt;code&gt;settings.json&lt;/code&gt;中修改相关字段为&lt;code&gt;settings_files/settings.json&lt;/code&gt;中的值。以前就有在使用vscode的同学需要把该&lt;code&gt;settings.json&lt;/code&gt;文件的部分内容添加到自己的.json文件。第一次使用vscode的同学直接覆盖就行。在vscode配置使用xelatexmk进行编译。详情参考&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions&#34;&gt;讨论区&lt;/a&gt;。（2022年起推荐的方法，希望更多同学参与完善并把更新推送给我，现在的.json文件还可以进一步改进）&lt;/p&gt; &#xA;&lt;p&gt;2.使用编译脚本&lt;code&gt;all.bat&lt;/code&gt;，记得关掉生成的pdf文档再双击&lt;code&gt;all.bat&lt;/code&gt;。双击&lt;code&gt;clean.bat&lt;/code&gt;删除临时文件。（此方法仅支持win）&lt;/p&gt; &#xA;&lt;p&gt;3.使用TeXstudio，首次编译建议从主文件&lt;code&gt;scutthesis.tex&lt;/code&gt;开始编译，首先在&lt;code&gt;TeXstudio的Options-&amp;gt;Configure TeXstudio-&amp;gt;build&lt;/code&gt;中，编译器(Dufault Compiler)选择&lt;code&gt;XeLaTeX&lt;/code&gt;，默认文献工具(Default Bibliography Tool)选&lt;code&gt;Biber&lt;/code&gt;，构建并查看（build &amp;amp; view） 按照&lt;code&gt;scutthesis.pdf&lt;/code&gt;中的图2-1进行设置（点击右侧扳手符号进行设置，分别选择&lt;code&gt;recompile-bibliography&lt;/code&gt;、&lt;code&gt;Defualt compiler&lt;/code&gt;、&lt;code&gt;Defualt Viewer&lt;/code&gt;并点add到右侧栏中），也可以使用默认的Compile &amp;amp; View（只不过此时不会自动更新参考文献著录）。(20年-21年期间主要的使用方法)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; 本模板不再利用Lyx，直接使用TeXstudio或者vscode进行编辑，这类似IDE。vscode的一些使用技巧以及快捷键可参考其他教程(百度vscode latex)，这里仅仅给出配置文件&lt;code&gt;settings_files/settings.json&lt;/code&gt;。详情移步&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions&#34;&gt;讨论区&lt;/a&gt;的&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions/6&#34;&gt;vscode配置&lt;/a&gt;。有什么问题都可以在讨论区交流。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;参考文献管理&lt;/h2&gt; &#xA;&lt;p&gt;本模板最主要的改动是参考文献使用biblatex（biber），而不是原来的BibTeX，因此不再需要.bst文件。在这项改动之后，参考文献部分的格式符合国标，当然要注意进行适当的设置，因为根据学校的论文撰写规范，有的信息是不需要显示在参考文献著录里的。可以查看biblatex包的&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/raw/master/settings_files/package_Documentation/biblatex-gb7714-2015.pdf&#34;&gt;使用说明&lt;/a&gt;，&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/tree/master/settings_files/package_Documentation&#34;&gt;package_Documentation&lt;/a&gt;文件夹还有其他包的使用说明。&lt;/p&gt; &#xA;&lt;p&gt;另外参考文献管理软件推荐使用开源的zotero，这是生成本模板的bib文件的软件（最新实测其他文献管理软件如endnote也可以使用本模板，重点是导出bib文件）。当然，也可以手动新建一个后缀名为.bib的文件，然后直接在文献页面（或谷歌、百度学术页面）复制BibTeX数据到该.bib文件，最后在&lt;code&gt;scutthesis.tex&lt;/code&gt;文件里使用就行。zotero不仅有强大的PC端（支持mac、win、linux），可以使用chrome等浏览器搜索到论文后利用插件一键捕获文献信息到zotero。然后对zotero收集好的文献，选中想要引用的论文然后按快捷键复制（需要适当自定义设置），再到撰写论文的tex文件粘贴即可（word也可以使用zotero，也很方便，本项目我们仅仅讨论latex模板）。zotero还有移动端（安卓、ios），所有文献信息都是同步的，文献的pdf文件可以使用第三方存储工具同步。总之多端同步，开源免费，方便随时随地看论文，省去管理文献的麻烦，相见恨晚！在写毕业论文的各位可以推荐给研一的师弟师妹，早用早享受，方便后期写论文。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; 使用zotero，科学上网很重要，通常我们使用谷歌学术搜索文献并利用chrome的zotero插件直接捕获文献著录信息。但我使用各种方法均遇到过被谷歌学术封锁的情况，后来只能换科学上网方法，这方面可以百度自行学习。强烈建议不要在谷歌学术搜索界面用zotero插件一键捕获大量文章，一次只捕获一两篇最佳。进入文章页面（如IEEE页）一篇一篇地捕获也可以。利用谷歌学术从搜索结果大量捕获文献信息容易被谷歌封ip，一旦被封只能换ip，相当麻烦！&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;错误排除：&lt;/h2&gt; &#xA;&lt;p&gt;编译时如果提示缺乏字体，请安装&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/tree/master/settings_files&#34;&gt;settings_files&lt;/a&gt;里面的字体（win右键安装即可；mac参考官方字体安装教程；linux下类似，需要自行百度）。此外，目前遇到的导致错误的原因大都是以下三种：&lt;/p&gt; &#xA;&lt;p&gt;1.语法错误，这种借助百度、谷歌很容易解决，按照latex的规范去写就好。&lt;/p&gt; &#xA;&lt;p&gt;2.和参考文献有关的错误，记住一定检查好bib文件，而bib文件来源于参考文献管理软件如本文推荐的zotero，又或者来源于自己手动建立的文件。一定要设置好导出选项以及格式，同时要设置好编译选项biber(区别于常见的bibtex)。&lt;/p&gt; &#xA;&lt;p&gt;3.编译器问题，和第二条一样，使用TeXstudio的话，一定要在&lt;code&gt;Options-&amp;gt;Configure TeXstudio-&amp;gt;build&lt;/code&gt;中设置好；使用vscode的话，记得按照&lt;a href=&#34;https://github.com/mengchaoheng/SCUT_thesis/discussions&#34;&gt;讨论区&lt;/a&gt;设置好vscode编译的配置文件&lt;code&gt;settings.json&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;4.其他问题，关掉生成的PDF，清除所有中间文件再编译一次看看。如win系统双击&lt;code&gt;clean.bat&lt;/code&gt;文件。或使用TeXstudio的话，点击&lt;code&gt;Tools-&amp;gt;Clean Auxiliary files&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;确保以上几方面没问题的话，基本就可以愉快的写论文了。&lt;/p&gt; &#xA;&lt;h2&gt;贡献者&lt;/h2&gt; &#xA;&lt;p&gt;按加入时间顺序：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mengchaoheng&#34;&gt;mengchaoheng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Komari-Koshigaya&#34;&gt;Komari-Koshigaya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhuohoudeputao&#34;&gt;zhuohoudeputao&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;欢迎大家加入维护团队！！！&lt;/p&gt;</summary>
  </entry>
</feed>