<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-26T01:45:31Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>edoliberty/vector-search-class-notes</title>
    <updated>2023-05-26T01:45:31Z</updated>
    <id>tag:github.com,2023-05-26:/edoliberty/vector-search-class-notes</id>
    <link href="https://github.com/edoliberty/vector-search-class-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Class notes for the course &#34;Long Term Memory in AI - Vector Search and Databases&#34; COS 495 @ Princeton Fall 2023&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Long Term Memory in AI - Vector Search and Databases&lt;/h1&gt; &#xA;&lt;p&gt;COS 495 is given at Princeton during the Fall semester 2023 by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=QHS_pZAAAAAJ&amp;amp;hl=en&#34;&gt;Edo Liberty&lt;/a&gt;, the Founder and CEO of &lt;a href=&#34;https://www.pinecone.io&#34;&gt;Pinecone&lt;/a&gt;, the world&#39;s leading Vector Database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=0eFZtREAAAAJ&amp;amp;hl=en%5D&#34;&gt;Matthijs Douze&lt;/a&gt; the architect and main developer of &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt; the most popular and advanced open source library for vector search.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The course covers the core concepts, algorithms, and data structures used for modern vector search systems and platforms. An advanced undergraduate or graduate student with some hands-on experience in linear algebra, probability, algorithms, and data structures should be able to follow this course.&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Long Term Memory is a fundamental capability in the modern AI Stack. At their core, these systems are using Vector search. Vector search is also a fundamental tool for systems that manipulate large collections of media like search engines, knowledge bases, content moderation tools, recommendation systems, etc. As such, the discipline lays at the intersection of Artificial Intelligence and Database Management Systems. This course will cover the scientific foundations and practical implementation of vector search applications, algorithms, and systems. The course will be evaluated with project and in-class presentation.&lt;/p&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The class contents below are tentative.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Introduction to Vector Search [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Embeddings as an information bottleneck. Instead of learning end-to-end, use embeddings as an intermediate representation&lt;/li&gt; &#xA;   &lt;li&gt;Advantages: scalability, instant updates, and explainability&lt;/li&gt; &#xA;   &lt;li&gt;Typical volumes of data and scalability. Embeddings are the only way to manage / access large databases&lt;/li&gt; &#xA;   &lt;li&gt;The embedding contract: the embedding extractor and embedding indexer agree on the meaning of the distance. Separation of concerns.&lt;/li&gt; &#xA;   &lt;li&gt;The vector space model in information retrieval&lt;/li&gt; &#xA;   &lt;li&gt;Vector embeddings in machine learning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Text embeddings [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2-layer word embeddings. Word2vec and fastText, obtained via a factorization of a co-occurrence matrix. Embedding arithmetic: king + woman - man = queen, (already based on similarity search)&lt;/li&gt; &#xA;   &lt;li&gt;Sentence embeddings: How to train, masked LM. Properties of sentence embeddings.&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models: reasoning as an emerging property of a LM. What happens when the training set = the whole web&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Image embeddings [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pixel structures of images. Early works on direct pixel indexing&lt;/li&gt; &#xA;   &lt;li&gt;Traditional CV models. Global descriptors (GIST). Local descriptors (SIFT and friends)Direct indexing of local descriptors for image matching, local descriptor pooling (Fisher, VLAD)&lt;/li&gt; &#xA;   &lt;li&gt;Convolutional Neural Nets. Off-the-shelf models. Trained specifically (contrastive learning, self-supervised learning)&lt;/li&gt; &#xA;   &lt;li&gt;Modern Computer Vision models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Practical indexing [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;How an index works: basic functionalities (search, add). Optional functionalities: snapshot, incremental add, remove&lt;/li&gt; &#xA;   &lt;li&gt;k-NN search vs. range search&lt;/li&gt; &#xA;   &lt;li&gt;maintaining top-k results&lt;/li&gt; &#xA;   &lt;li&gt;Criteria: Speed / accuracy / memory usage / updateability / index construction time&lt;/li&gt; &#xA;   &lt;li&gt;Early works on bag-of-visual-words inspiration, based on quantization&lt;/li&gt; &#xA;   &lt;li&gt;Voronoi diagram with search buckets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Low Dimensional Vector Search [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;k-d tree, space partitioning based algorithms, proof, structures, and asymptotic behavior&lt;/li&gt; &#xA;   &lt;li&gt;Probabilistic inequalities. Recap of basic inequalities: Markov, Chernoof, Hoeffding&lt;/li&gt; &#xA;   &lt;li&gt;Concentration Of Measure phenomena. Orthogonality of random vectors&lt;/li&gt; &#xA;   &lt;li&gt;Curse of dimensionality. The failure of space partitioning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dimensionality Reduction [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Principal Components Analysis, optimal dimension reduction in the sum of squared distance measure&lt;/li&gt; &#xA;   &lt;li&gt;Fast PCA algorithms&lt;/li&gt; &#xA;   &lt;li&gt;Random Projections. Gaussian random i.i.d. dimension reduction&lt;/li&gt; &#xA;   &lt;li&gt;Fast Random Projections. Accelerating the above to near linear time&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Locality Sensitive Hashing [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Definition of Approximate Nearest Neighbor Search (ANNS)&lt;/li&gt; &#xA;   &lt;li&gt;Criteria: Speed / accuracy / memory usage / updateability / index construction time&lt;/li&gt; &#xA;   &lt;li&gt;Definition of Locality Sensitive Hashing and examples&lt;/li&gt; &#xA;   &lt;li&gt;The LSH Algorithm&lt;/li&gt; &#xA;   &lt;li&gt;LSH Analysis, proof of correctness, and asymptotics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clustering [Edo]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Semantic clustering: properties (purity, as aid for annotation)&lt;/li&gt; &#xA;   &lt;li&gt;Clustering from a similarity graph (spectral clustering, agglomerative clustering)&lt;/li&gt; &#xA;   &lt;li&gt;Vector clustering: mean squared error criterion. Tradeoff with number of clusters&lt;/li&gt; &#xA;   &lt;li&gt;Relationship between vector clustering and quantization (OOD extension)&lt;/li&gt; &#xA;   &lt;li&gt;The k-means clustering measure and Lloyd&#39;s algorithm&lt;/li&gt; &#xA;   &lt;li&gt;Lloyd&#39;s optimality conditions&lt;/li&gt; &#xA;   &lt;li&gt;Initialization strategies (kmeans++, progressive dimensions with PCA)&lt;/li&gt; &#xA;   &lt;li&gt;The inverted file model. Relationship with sparse matrices&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Quantization for lossy vector compression [Matthijs]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vector quantization is a topline (directly optimizes the objective)&lt;/li&gt; &#xA;   &lt;li&gt;Binary quantization and hamming comparison&lt;/li&gt; &#xA;   &lt;li&gt;Product quantization. Chunked vector quantization. Optimized vector quantization&lt;/li&gt; &#xA;   &lt;li&gt;Additive quantization. Extension of product quantization. Difficulty in training approximations (Residual quantization, CQ, TQ, LSQ, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;Cost of coarse quantization vs. inverted list scanning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graph based indexes [Guest lecture]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Early works: hierarchical k-means&lt;/li&gt; &#xA;   &lt;li&gt;Neighborhood graphs. How to construct them. Nearest Neighbor Descent&lt;/li&gt; &#xA;   &lt;li&gt;Greedy search in Neighborhood graphs. That does not work -- need long jumps&lt;/li&gt; &#xA;   &lt;li&gt;HNSW. A practical hierarchical graph-based index&lt;/li&gt; &#xA;   &lt;li&gt;NSG. Evolving a graph k-NN graph&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Computing Hardware and Vector Search [Guest lecture]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Computing platform: local vs. service / CPU vs. GPU&lt;/li&gt; &#xA;   &lt;li&gt;efficient implementation of brute force search&lt;/li&gt; &#xA;   &lt;li&gt;distance computations for product quantization -- tradeoffs. SIMD implementation&lt;/li&gt; &#xA;   &lt;li&gt;Parallelization and distribution -- sharding vs. inverted list distribution&lt;/li&gt; &#xA;   &lt;li&gt;Using co-processors (GPUs)&lt;/li&gt; &#xA;   &lt;li&gt;Using a hierarchy of memory types (RAM + SSD or RAM + GPU RAM)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced topics -- articles to be presented by students [all]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vectors in Generative AI&lt;/li&gt; &#xA;   &lt;li&gt;Neural quantization&lt;/li&gt; &#xA;   &lt;li&gt;Vector Databases&lt;/li&gt; &#xA;   &lt;li&gt;Beyond feature extraction and indexing: neural indexing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In class project presentations&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Selected literature&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Product quantization (PQ) and inverted file: &lt;a href=&#34;https://hal.inria.fr/inria-00514462v2/document&#34;&gt;“Product quantization for nearest neighbor search”&lt;/a&gt;, Jégou &amp;amp; al., PAMI 2011.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fast k-selection on wide SIMD architectures: &lt;a href=&#34;https://arxiv.org/abs/1702.08734&#34;&gt;“Billion-scale similarity search with GPUs”&lt;/a&gt;, Johnson &amp;amp; al, ArXiv 1702.08734, 2017&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HNSW indexing method: &lt;a href=&#34;https://arxiv.org/abs/1603.09320&#34;&gt;&#34;Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs&#34;&lt;/a&gt;, Malkov &amp;amp; al., ArXiv 1603.09320, 2016&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In-register vector comparisons: &lt;a href=&#34;https://arxiv.org/abs/1812.09162&#34;&gt;&#34;Quicker ADC : Unlocking the Hidden Potential of Product Quantization with SIMD&#34;&lt;/a&gt;, André et al, PAMI&#39;19, also used in &lt;a href=&#34;https://arxiv.org/abs/1908.10396&#34;&gt;&#34;Accelerating Large-Scale Inference with Anisotropic Vector Quantization&#34;&lt;/a&gt;, Guo, Sun et al, ICML&#39;20.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Graph-based indexing method NSG: &lt;a href=&#34;https://arxiv.org/abs/1707.00143&#34;&gt;&#34;Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph&#34;&lt;/a&gt;, Cong Fu et al, VLDB 2019.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Local search quantization (an additive quantization method): &lt;a href=&#34;https://drive.google.com/file/d/1dDuv6fQozLQFS2AJoNNFGTH499QIp_vO/view&#34;&gt;&#34;Revisiting additive quantization&#34;&lt;/a&gt;, Julieta Martinez, et al. ECCV 2016 and &lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/html/Julieta_Martinez_LSQ_lower_runtime_ECCV_2018_paper.html&#34;&gt;&#34;LSQ++: Lower running time and higher recall in multi-codebook quantization&#34;&lt;/a&gt;, Julieta Martinez, et al. ECCV 2018.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Learning-based quantizer: &lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/html/Morozov_Unsupervised_Neural_Quantization_for_Compressed-Domain_Similarity_Search_ICCV_2019_paper.html&#34;&gt;&#34;Unsupervised Neural Quantization for Compressed-Domain Similarity Search&#34;&lt;/a&gt;, Morozov and Banenko, ICCV&#39;19&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Neural indexing: &lt;a href=&#34;https://arxiv.org/abs/2202.06991&#34;&gt;&#34;Transformer Memory as a Differentiable Search Index&#34;&lt;/a&gt;, Tan &amp;amp; al. ArXiV&#39;22&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The hybrid RAM-disk index: &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/hash/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Abstract.html&#34;&gt;&#34;Diskann: Fast accurate billion-point nearest neighbor search on a single node&#34;&lt;/a&gt;, Subramanya &amp;amp; al. NeurIPS&#39;19&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The nearest neighbor descent method: &lt;a href=&#34;https://www.ambuehler.ethz.ch/CDstore/www2011/proceedings/p577.pdf&#34;&gt;Efficient k-nearest neighbor graph construction for generic similarity measures&lt;/a&gt; from Dong et al., WWW&#39;11&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;On unix-like systems with the bibtex and pdflatex available you should be able to do this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:edoliberty/vector-search-class-notes.git&#xA;cd vector-search-class-notes&#xA;./build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;These class notes are intended to be used freely by academics anywhere, students and professors alike. Please feel free to contribute in the form of pull requests or opening issues.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>being24/latex-template-ja</title>
    <updated>2023-05-26T01:45:31Z</updated>
    <id>tag:github.com,2023-05-26:/being24/latex-template-ja</id>
    <link href="https://github.com/being24/latex-template-ja" rel="alternate"></link>
    <summary type="html">&lt;p&gt;できるだけ簡単にLaTeX環境を構築できるようにするテンプレート&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;日本語論文をLaTeXで書いて、textlintをするためのテンプレート&lt;/h1&gt; &#xA;&lt;h2&gt;機能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;個人環境にLaTeX workshopを構築せず、dockerでビルドします&lt;/li&gt; &#xA; &lt;li&gt;GitHub Actionsを使用してtextlintを実行します&lt;/li&gt; &#xA; &lt;li&gt;github上にreleaseします&lt;/li&gt; &#xA; &lt;li&gt;レジュメや論文用のテンプレートを持ちますが、あくまで個人の環境用に構築したものです&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;環境&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows 10 or later&lt;/li&gt; &#xA; &lt;li&gt;macOS 10.14 or later&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 18.04 LTS or later&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Docker環境が必要ですが、clsファイルについては多少弄ればCloud LaTeX等でも使用できます&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker Desktop for Mac 2.1 or later&lt;/li&gt; &#xA; &lt;li&gt;Docker 18.06 or later&lt;/li&gt; &#xA; &lt;li&gt;Docker Desktop for Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ubuntu 20.04 LTS上の TeX Live 2022を使用します&lt;br&gt; ビルド用のdocker imageは&lt;a href=&#34;https://github.com/being24/latex-docker&#34;&gt;こちらのリポジトリ&lt;/a&gt;を参照してください&lt;/p&gt; &#xA;&lt;p&gt;また、VSCodeが必要です&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/being24/latex-template-ja/master/example/figures/screenshot.png&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;使い方&lt;/h2&gt; &#xA;&lt;p&gt;使い方やFAQはこの&lt;a href=&#34;https://zenn.dev/being/articles/how-to-use-my-latex&#34;&gt;記事&lt;/a&gt;にまとめています&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;CC0&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;Being&lt;/p&gt; &#xA;&lt;h2&gt;config&lt;/h2&gt; &#xA;&lt;p&gt;VSCode上での設定例は&lt;a href=&#34;https://raw.githubusercontent.com/being24/latex-template-ja/master/.vscode/settings.json&#34;&gt;settings.json&lt;/a&gt;を参照してください&lt;/p&gt; &#xA;&lt;h2&gt;テンプレートについて&lt;/h2&gt; &#xA;&lt;p&gt;できるだけモダナイズを意識して作成したテンプレートですが、LaTeXに詳しいわけではないので誤りがあった場合は教えていただけると幸いです&lt;br&gt; 実際の使用時はexample等必要のないファイルは消してください&lt;/p&gt; &#xA;&lt;p&gt;jlistingの代わりにmintedを使用し、参考文献はbiblatexを使用します&lt;/p&gt; &#xA;&lt;h3&gt;resume.cls&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/being24/latex-template-ja/master/classes/resume.cls&#34;&gt;resume.cls&lt;/a&gt;は2段組みのレジュメを作成するためのクラスファイルです&lt;br&gt; 使用方法は&lt;a href=&#34;https://raw.githubusercontent.com/being24/latex-template-ja/master/example/tex/resume_template.tex&#34;&gt;例&lt;/a&gt;を参照してください&lt;/p&gt; &#xA;&lt;h3&gt;report.cls&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/being24/latex-template-ja/master/classes/report.cls&#34;&gt;report.cls&lt;/a&gt;は論文を作成するためのクラスファイルです&lt;br&gt; 使用方法は&lt;a href=&#34;https://raw.githubusercontent.com/being24/latex-template-ja/master/example/tex/report_template.tex&#34;&gt;例&lt;/a&gt;を参照してください&lt;/p&gt; &#xA;&lt;h3&gt;.vscode/settings.jsonについて&lt;/h3&gt; &#xA;&lt;p&gt;使用しやすい設定を参考程度ですが上げておきます。&lt;br&gt; VSCodeであればこの設定を読み込んでくれるため、設定を変更する必要はありません&lt;/p&gt; &#xA;&lt;h2&gt;参考URL&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://poyo.hatenablog.jp/entry/2020/12/05/110000&#34;&gt;https://poyo.hatenablog.jp/entry/2020/12/05/110000&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>