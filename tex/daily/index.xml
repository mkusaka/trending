<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-09T01:33:02Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dccuchile/CC6205</title>
    <updated>2024-03-09T01:33:02Z</updated>
    <id>tag:github.com,2024-03-09:/dccuchile/CC6205</id>
    <link href="https://github.com/dccuchile/CC6205" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CC6205 - Natural Language Processing&lt;/h1&gt; &#xA;&lt;p&gt;This is a course on natural language processing.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Lecturer: &lt;a href=&#34;https://felipebravom.com/&#34;&gt;Felipe Bravo-Marquez&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TAs: &lt;a href=&#34;https://giturra.cl/&#34;&gt;Gabriel Iturra-Bocaz&lt;/a&gt;, &lt;a href=&#34;https://www.ortizfuentes.com/&#34;&gt;Jorge Ortiz&lt;/a&gt;, Consuelo Rojas, SebastiÃ¡n Tinoco and &lt;a href=&#34;http://www.dim.uchile.cl/~furrutia/&#34;&gt;Felipe Urrutia&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/apunte/apunte.pdf&#34;&gt;Course Notes (in Spanish)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lectures: Tuesday 14:30 - 16:00, Thursday 14:30 - 16:00&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.google.com/document/d/1DNja7nf0b26aRWF_gMNJf9L6SLtvtyFpucDhfcgG4d0/edit?usp=sharing&#34;&gt;Course Program&lt;/a&gt; (in Spanish)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/calendar.md&#34;&gt;Course Calendar&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&#34;&gt;Youtube Playlist with lectures&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Info&lt;/h3&gt; &#xA;&lt;p&gt;This course aims to provide a comprehensive introduction to Natural Language Processing (NLP) by covering essential concepts. We strive to strike a balance between traditional techniques, such as N-gram language models, Naive Bayes, and Hidden Markov Models (HMMs), and modern deep neural networks, including word embeddings, recurrent neural networks (RNNs), and transformers.&lt;/p&gt; &#xA;&lt;p&gt;The course material draws from various sources. In many instances, sentences from these sources are directly incorporated into the slides. The neural network topics primarily rely on the book &lt;a href=&#34;https://link.springer.com/book/10.1007/978-3-031-02165-7&#34;&gt;Neural Network Methods for Natural Language Processing&lt;/a&gt; by Goldberg. Non-neural network topics, such as Probabilistic Language Models, Naive Bayes, and HMMs, are sourced from &lt;a href=&#34;http://www.cs.columbia.edu/~mcollins/&#34;&gt;Michael Collins&#39; course&lt;/a&gt; and &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/&#34;&gt;Dan Jurafsky&#39;s book&lt;/a&gt;. Additionally, some slides are adapted from online tutorials and other courses, such as &lt;a href=&#34;http://web.stanford.edu/class/cs224n/&#34;&gt;Manning&#39;s Stanford course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Slides&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-introduction.pdf&#34;&gt;Introduction to Natural Language Processing&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-introduction.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/HEKTNOttGvU&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/P8cwnI-f-Kg&#34;&gt;video 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-IR.pdf&#34;&gt;Vector Space Model and Information Retrieval&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-IR.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://www.youtube.com/watch?v=FXIVClF370w&amp;amp;list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&amp;amp;index=2&amp;amp;t=0s&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=f8nG1EMmPZk&amp;amp;list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&amp;amp;index=2&#34;&gt;video 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-PLM.pdf&#34;&gt;Probabilistic Language Models&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-PLM.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=9E2jJ6kcb4Y&amp;amp;list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&amp;amp;index=3&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=ZWqbEQXLra0&amp;amp;list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&amp;amp;index=4&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=tsumFqwFlaA&amp;amp;list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&amp;amp;index=5&#34;&gt;video 3&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=s3TWdv4sqkg&amp;amp;list=PLppKo85eGXiXIh54H_qz48yHPHeNVJqBi&amp;amp;index=6&#34;&gt;video 4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-NB.pdf&#34;&gt;Text Classification and Naive Bayes&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-NB.tex&#34;&gt;tex source file&lt;/a&gt;) , &lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/4.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/kG9BK9Oy1hU&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/Iqte5kKHvzE&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/TSJg0_X3Abk&#34;&gt;video 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-linear.pdf&#34;&gt;Linear Models&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-linear.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/zhBxDsNLZEA&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/Fooua_uaWSE&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/DqbzhdQa1eQ&#34;&gt;video 3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/1nfWWXqfAzA&#34;&gt;video 4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-neural.pdf&#34;&gt;Neural Networks&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-neural.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/oHZHA8h2xN0&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/2lXank0W6G4&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/BUDIi9qItzY&#34;&gt;video 3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/KKN2Ipy-vGk&#34;&gt;video 4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-wordvectors.pdf&#34;&gt;Word Vectors&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-wordvectors.tex&#34;&gt;tex source file&lt;/a&gt;) &lt;a href=&#34;https://youtu.be/wtwUsJMC9CA&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/XDxzQ7JU95U&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/Ikyc3DRVodk&#34;&gt;video 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-HMM.pdf&#34;&gt;Sequence Labeling and Hidden Markov Models &lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-HMM.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf&#34;&gt;notes&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/-ngfOZz8yK0&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/Tjgb-yQOg54&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/aaa5Qoi8Vco&#34;&gt;video 3&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/4pKWIDkF_6Y&#34;&gt;video 4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-CRF.pdf&#34;&gt;MEMMs and CRFs&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-CRF.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;http://www.cs.columbia.edu/~mcollins/crf.pdf&#34;&gt;notes 1&lt;/a&gt;, &lt;a href=&#34;http://www.cs.columbia.edu/~mcollins/fb.pdf&#34;&gt;notes 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/qlI-4lSUDkg&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/PLoLKQwkONw&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/ZpUwDy6o28Y&#34;&gt;video 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-CNN.pdf&#34;&gt;Convolutional Neural Networks&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-CNN.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/lLZW5Fn40r8&#34;&gt;video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-RNN.pdf&#34;&gt;Recurrent Neural Networks&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-RNN.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/BmhjUkzz3nk&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/z43YFR1iIvk&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/7L5JxQdwNJk&#34;&gt;video 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-seq2seq.pdf&#34;&gt;Sequence to Sequence Models and Attention&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-seq2seq.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/OpKxRjISqmM&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/WQ7ihm5voB0&#34;&gt;video 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-transformer.pdf&#34;&gt;Transformer Architecture&lt;/a&gt; | (&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-seq2seq.tex&#34;&gt;tex source file&lt;/a&gt;), &lt;a href=&#34;https://youtu.be/8RE23Uq8rU0&#34;&gt;video 1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-LLM.pdf&#34;&gt;Contextualized Embeddings and Large Language Models&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/sSGbgZpHymI&#34;&gt;video 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/C-QfzWU6eUE&#34;&gt;video 2&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/5j4Mgl3GuVY&#34;&gt;video 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/NLP-LLMpatterns.pdf&#34;&gt;Large Language Models Usage and Evaluation Patterns&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;NLP Libraries and Tools&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nltk.org/&#34;&gt;NLTK: Natural Language Toolkit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://radimrehurek.com/gensim/&#34;&gt;Gensim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spacy.io/&#34;&gt;spaCy: Industrial-strength NLP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://torchtext.readthedocs.io/en/latest/&#34;&gt;Torchtext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://allennlp.org/&#34;&gt;AllenNLP: Open source project for designing deep leaning-based NLP models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;HuggingFace Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat.openai.com/&#34;&gt;ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bard.google.com&#34;&gt;Google Bard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stanfordnlp.github.io/stanza/&#34;&gt;Stanza - A Python NLP Library for Many Human Languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/flairNLP/flair&#34;&gt;FlairNLP: A very simple framework for state-of-the-art Natural Language Processing (NLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wefe.readthedocs.io/en/latest/&#34;&gt;WEFE: The Word Embeddings Fairness Evaluation Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rasahq.github.io/whatlies/&#34;&gt;WhatLies: A library that tries help you to understand. &#34;What lies in word embeddings?&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/LASER&#34;&gt;LASER:a library to calculate and use multilingual sentence embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UKPLab/sentence-transformers&#34;&gt;Sentence Transformers: Multilingual Sentence Embeddings using BERT / RoBERTa / XLM-RoBERTa &amp;amp; Co. with PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Datasets: a lightweight library with one-line dataloaders for many public datasets in NLP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dccuchile.github.io/rivertext/&#34;&gt;RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Notes and Books&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/&#34;&gt;Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cs.columbia.edu/~mcollins/&#34;&gt;Michael Collins&#39; NLP notes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://u.cs.biu.ac.il/~yogo/nnlp.pdf&#34;&gt;A Primer on Neural Network Models for Natural Language Processing by Joav Goldberg&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.07916&#34;&gt;Natural Language Understanding with Distributed Representation by Kyunghyun Cho&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.18223&#34;&gt;A Survey of Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jacobeisenstein/gt-nlp-class/raw/master/notes/eisenstein-nlp-notes.pdf&#34;&gt;Natural Language Processing Book by Jacob Eisenstein&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.nltk.org/book/&#34;&gt;NLTK book&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://josecamachocollados.com/book_embNLP_draft.pdf&#34;&gt;Embeddings in Natural Language Processing by Mohammad Taher Pilehvar and Jose Camacho-Collados&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://d2l.ai/&#34;&gt;Dive into Deep Learning Book&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.06006.pdf&#34;&gt;Contextual Word Representations: A Contextual Introduction by Noah A. Smith&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Other NLP Courses&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224n/&#34;&gt;CS224n: Natural Language Processing with Deep Learning, Stanford course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.upc.edu/~horacio/ahlt/DeepLearning02.pdf&#34;&gt;Deep Learning in NLP: slides by Horacio RodrÃ­guez&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://people.ischool.berkeley.edu/~dbamman/nlp18.html&#34;&gt;David Bamman NLP Slides @Berkley&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.natalieparde.com/teaching/cs521_spring2020.html&#34;&gt;CS 521: Statistical Natural Language Processing by Natalie Parde, University of Illinois&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kdnuggets.com/2019/10/10-free-top-notch-courses-natural-language-processing.html&#34;&gt;10 Free Top Notch Natural Language Processing Courses&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Videos&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&amp;amp;disable_polymer=true&#34;&gt;Natural Language Processing MOOC videos by Dan Jurafsky and Chris Manning, 2012&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCB_JX4jH3QQmp69rmkWpl1A/playlists?shelf_id=3&amp;amp;view=50&amp;amp;sort=dd&#34;&gt;Natural Language Processing MOOC videos by Michael Collins, 2013&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&#34;&gt;Natural Language Processing with Deep Learning by Chris Manning and Richard Socher, 2017&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&#34;&gt;CS224N: Natural Language Processing with Deep Learning | Winter 2019&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLegWUnz91WfuPebLI97-WueAP90JO-15i&#34;&gt;Computational Linguistics I by Jordan Boyd-Graber University of Maryland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks&#34;&gt;Visualizing and Understanding Recurrent Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6&#34;&gt;BERT Research Series by Chris McCormick&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=jfwqRMdTmLo&#34;&gt;Successes and Challenges in Neural Models for Speech and Language - Michael Collins&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tv.vera.com.uy/video/55388&#34;&gt;More on Transforemers: BERT and Friends by Jorge PÃ©rez&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Other Resources&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/portal/&#34;&gt;ACL Portal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/keon/awesome-nlp&#34;&gt;Awesome-nlp: A curated list of resources dedicated to Natural Language Processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nlpprogress.com/&#34;&gt;NLP-progress: Repository to track the progress in Natural Language Processing (NLP)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mailman.uib.no/listinfo/corpora&#34;&gt;Corpora Mailing List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;ðŸ¤— Open LLM Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.realworldnlpbook.com/&#34;&gt;Real World NLP Book: AllenNLP tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer: a very illustrative blog post about the Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;Better Language Models and Their Implications OpenAI Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@gitlostmurali/understanding-lora-and-qlora-the-powerhouses-of-efficient-finetuning-in-large-language-models-7ac1adf6c0cf&#34;&gt;Understanding LoRA and QLoRA â€” The Powerhouses of Efficient Finetuning in Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;RNN effectiveness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://super.gluebenchmark.com/&#34;&gt;SuperGLUE: an benchmark of Natural Language Understanding Tasks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://decanlp.com/&#34;&gt;decaNLP The Natural Language Decathlon: a benchmark for studying general NLP models that can perform a variety of complex, natural language tasks&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images&#34;&gt;Chatbot and Related Research Paper Notes with Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bentrevett/&#34;&gt;Ben Trevett&#39;s torchtext tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/PLMpapers&#34;&gt;PLMpapers: a collection of papers about Pre-Trained Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-gpt2/&#34;&gt;The Illustrated GPT-2 (Visualizing Transformer Language Models)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@emilymenonbender/linguistics-nlp-and-interdisciplinarity-or-look-at-your-data-e49e03d37c9c&#34;&gt;Linguistics, NLP, and Interdisciplinarity Or: Look at Your Data, by Emily M. Bender&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@nlpscholar/state-of-nlp-cbf768492f90&#34;&gt;The State of NLP Literature: Part I, by Saif Mohammad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1805.04032.pdf&#34;&gt;From Word to Sense Embeddings:A Survey on Vector Representations of Meaning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ruder.io/research-highlights-2019/index.html&#34;&gt;10 ML &amp;amp; NLP Research Highlights of 2019 by Sebastian Ruder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html?m=1&#34;&gt;Towards a Conversational Agent that Can Chat Aboutâ€¦Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://notebooks.quantumstat.com/&#34;&gt;The Super Duper NLP Repo: a collection of Colab notebooks covering a wide array of NLP task implementations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datasets.quantumstat.com/&#34;&gt;The Big Bad NLP Database, a collection of nearly 300 well-organized, sortable, and searchable natural language processing datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.12327&#34;&gt;A Primer in BERTology: What we know about how BERT works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.medium.com/wFxx3d96f7&#34;&gt;How Self-Attention with Relative Position Representations works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.03705.pdf&#34;&gt;Deep Learning Based Text Classification: A Comprehensive Review&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/yoavgo/status/1318567498653061122&#34;&gt;Teaching NLP is quite depressing, and I don&#39;t know how to do it well by Yoav Goldberg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://index.quantumstat.com/&#34;&gt;The NLP index&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/amanchadha/100-nlp-papers&#34;&gt;100 Must-Read NLP Papers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>