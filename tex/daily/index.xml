<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-31T01:40:44Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>roaldarbol/LaPreprint</title>
    <updated>2022-10-31T01:40:44Z</updated>
    <id>tag:github.com,2022-10-31:/roaldarbol/LaPreprint</id>
    <link href="https://github.com/roaldarbol/LaPreprint" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üìù A nicely formatted LaTeX preprint template&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;LaPreprint&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/roaldarbol/LaPreprint/main/#&#34;&gt;&lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/roaldarbol/lapreprint?style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/roaldarbol/LaPreprint/main/#&#34;&gt;&lt;img alt=&#34;Stars&#34; src=&#34;https://img.shields.io/github/stars/roaldarbol/lapreprint?style=social&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/roaldarbol/LaPreprint/main/#&#34;&gt;&lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/roaldarbol?style=social&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;297&#34; height=&#34;420&#34; src=&#34;https://user-images.githubusercontent.com/25629697/186181395-85edc43b-c573-4032-85da-a9227ba7721d.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;A template for easily creating pretty, nicely formatted preprints in LaTeX.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;p&gt;With simple options you can enable/disable:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bioRxiv&lt;/code&gt;, &lt;code&gt;medRxiv&lt;/code&gt;, &lt;code&gt;arXiv&lt;/code&gt; and &lt;code&gt;chemRxiv&lt;/code&gt; logos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;blue&lt;/code&gt; or &lt;code&gt;red&lt;/code&gt; colour schemes&lt;/li&gt; &#xA; &lt;li&gt;Figures at the end&lt;/li&gt; &#xA; &lt;li&gt;Line numbers&lt;/li&gt; &#xA; &lt;li&gt;Change line spacing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See more in the &lt;a href=&#34;https://github.com/roaldarbol/LaPreprint/wiki&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Use this template&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open the document in your preferred environment (&lt;a href=&#34;https://github.com/roaldarbol/LaPreprint/wiki/Working-environment&#34;&gt;e.g. Overleaf or VSCode&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;main.tex&lt;/code&gt;, edit &lt;code&gt;Article setup&lt;/code&gt; to have the correct information&lt;/li&gt; &#xA; &lt;li&gt;Start writing! The sections are found in the &lt;code&gt;main&lt;/code&gt; folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you want different sections you can easily rename them - just make sure also to edit the names in &lt;code&gt;main.tex&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;LaPreprint is inspired by the style of eLife and PLoS, and is based on the eLife template. Additionally, the fancy footer is modified from the &lt;a href=&#34;https://www.overleaf.com/latex/templates/henriqueslab-biorxiv-template/nyprsybwffws&#34;&gt;the Henriques Lab template&lt;/a&gt;. Without all their work, this template wouldn&#39;t exist!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chaofengc/Awesome-Image-Quality-Assessment</title>
    <updated>2022-10-31T01:40:44Z</updated>
    <id>tag:github.com,2022-10-31:/chaofengc/Awesome-Image-Quality-Assessment</id>
    <link href="https://github.com/chaofengc/Awesome-Image-Quality-Assessment" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A comprehensive collection of IQA papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Image Quality Assessment (IQA)&lt;/h1&gt; &#xA;&lt;p&gt;A comprehensive collection of IQA papers, datasets and codes. We also provide PyTorch implementations of mainstream metrics in &lt;a href=&#34;https://github.com/chaofengc/IQA-PyTorch&#34;&gt;IQA-PyTorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chaofengc/IQA-PyTorch&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Toolbox-IQA--PyTorch-critical&#34; alt=&#34;toolbox&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyiqa/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pyiqa&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=chaofengc/Awesome-Image-Quality-Assessment&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#awesome-image-quality-assessment-iqa&#34;&gt;Awesome Image Quality Assessment (IQA)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#no-reference-nr&#34;&gt;No Reference (NR)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#full-reference-fr&#34;&gt;Full Reference (FR)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#others&#34;&gt;Others&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#datasets&#34;&gt;Datasets&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#iqa-datasets&#34;&gt;IQA datasets&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/#perceptual-similarity-datasets&#34;&gt;Perceptual similarity datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;No Reference (NR)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[TMM2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.07666&#34;&gt;GraphIQA: Learning Distortion Graph Representations for Blind Image Quality Assessment&lt;/a&gt;, Sun et al. &lt;a href=&#34;https://github.com/geekyutao/GraphIQA&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L701-L707&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[CVPR2021]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2105.06747&#34;&gt;Troubleshooting Blind Image Quality Models in the Wild&lt;/a&gt;, Wang et al. &lt;a href=&#34;https://github.com/wangzhihua520/troubleshooting_BIQA&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L716-L722&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Link&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Keywords&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.08958&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MANIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPRW2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IIGROUP/MANIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer, multi-dimension attention, dual branch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06858&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TReS&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;WACV2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/isalirezag/TReS&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer, relative ranking, self-consistency&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bmvc2021-virtualconference.com/assets/papers/0868.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KonIQ++&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;BMVC2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SSL92/koniqplusplus&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-task with distortion prediction&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.05997&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MUSIQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ICCV2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/musiq&#34;&gt;Official&lt;/a&gt; / &lt;a href=&#34;https://github.com/anse3832/MUSIQ&#34;&gt;Pytorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-scale, transformer, Aspect Ratio Preserved (ARP) resizing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.07948&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CKDN&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ICCV2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/researchmm/CKDN&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Degraded reference, Conditional knowledge distillation (related to HIQA)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HyperIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SSL92/hyperIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-aware hyper network&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.05508&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta-IQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhuhancheng/MetaIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta-learning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.08932&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ECCV2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cientgu/GIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generated image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1809.07517&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PI&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;2018 PIRM Challenge&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/roimehrez/PIRM2018&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1/2 * (NIQE + (10 - NRQM)).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.01681&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://kwanyeelin.github.io/projects/HIQA/HIQA.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Hallucinated reference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1805.08493v1.pdf&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BPSQM&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pixel-wise quality map&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.08347&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RankIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;ICCV2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xialeiliu/RankIQA&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pretrain on synthetically ranked data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Convolutional_Neural_Networks_2014_CVPR_paper.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CNNIQA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2014&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/CNNIQA&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;First CNN-based NR-IQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.13983&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;UNIQUE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zwx8981/UNIQUE&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Combine synthetic and authentic image pairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.02665.pdf&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DBCNN&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TCSVT2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zwx8981/DBCNN-PyTorch&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Two branches for synthetic and authentic distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.jdl.link/doc/2011/20191226_08489929.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SFA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TMM2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/SFA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Aggregate ResNet50 features of multiple cropped patches&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tMjcllKP8SzTn-dWVmogxaCLpzL1L7nO/view&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;https://arxiv.org/abs/1708.08190&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PQR&lt;/td&gt; &#xA;   &lt;td&gt;NR/Aesthetic&lt;/td&gt; &#xA;   &lt;td&gt;TIP2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HuiZeng/Unified_IAA&#34;&gt;Official1&lt;/a&gt;/&lt;a href=&#34;https://github.com/HuiZeng/BIQA_Toolbox&#34;&gt;Official2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Unify different type of aesthetic labels&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.01697&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaDIQaM (deepIQA)&lt;/td&gt; &#xA;   &lt;td&gt;NR/FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/WaDIQaM&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Weighted average of patch qualities, shared FR/NR models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/ielx7/83/8347140/08352823.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NIMA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kentsyx/Neural-IMage-Assessment&#34;&gt;PyTorch&lt;/a&gt;/&lt;a href=&#34;https://github.com/idealo/image-quality-assessment&#34;&gt;Tensorflow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Squared EMD loss&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/publications/TIP_E2E_BIQA.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MEON&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-task: distortion learning and quality prediction&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.06505&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;dipIQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~k29ma/codes/dipIQ.rar&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Similar to RankIQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.05890&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NRQM (Ma)&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVIU2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.google.com/site/chaoma99/sr-metric&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional, Super resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.04757&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FRIQUEE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;JoV2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/FRIQUEE&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Authentically Distorted, Bag of Features&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7501619&#34;&gt;IEEE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HOSA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7501619&#34;&gt;Matlab download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2015/zhang2015feature.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ILNIQE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www4.comp.polyu.edu.hk/~cslzhang/IQA/ILNIQE/ILNIQE.htm&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2012/TIP%20BRISQUE.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BRISQUE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/BRISQUE&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2012/saad_2012_tip.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BLIINDS-II&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/BLIINDS2&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.359.7510&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CORNIA&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/HuiZeng/BIQA_Toolbox&#34;&gt;Matlab download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Codebook Representation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2013/mittal2013.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NIQE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;SPL2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/niqe&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.imaging.utk.edu/research/wcho/references/2011%20TIP%20BLINDS2.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DIIVINE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/utlive/DIIVINE&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | []() | | NR | | []() |  --&gt; &#xA;&lt;h3&gt;Full Reference (FR)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[ACM MM2022]&lt;/code&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.08689&#34;&gt;Quality Assessment of Image Super-Resolution: Balancing Deterministic and Statistical Fidelity&lt;/a&gt;, Zhou et al. &lt;a href=&#34;https://github.com/weizhou-geek/SRIF&#34;&gt;Github&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaofengc/Awesome-Image-Quality-Assessment/main/iqa_ref.bib#L709-L714&#34;&gt;Bibtex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Link&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Keywords&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.10485&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AHIQ&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2022 NTIRE workshop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IIGROUP/AHIQ&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Attention, Transformer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.08763&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JSPL&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/happycaoyue/JSPL&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;semi-supervised and positive-unlabeled (PU) learning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.13123&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CVRKD&lt;/td&gt; &#xA;   &lt;td&gt;NAR&lt;/td&gt; &#xA;   &lt;td&gt;AAAI2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/guanghaoyin/CVRKD-IQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Non-Aligned content reference, knowledge distillation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.14730&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IQT&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPRW2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/anse3832/IQT&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Transformer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.08521&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A-DISTS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;ACMM2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dingkeyan93/A-DISTS&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.07728&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DISTS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TPAMI2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dingkeyan93/DISTS&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.03924&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LPIPS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://richzhang.github.io/PerceptualSimilarity/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Perceptual similarity, Pairwise Preference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.02067&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PieAPP&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://civc.ucsb.edu/graphics/Papers/CVPR2018_PieAPP/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Perceptual similarity, Pairwise Preference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.01697&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaDIQaM&lt;/td&gt; &#xA;   &lt;td&gt;NR/FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/WaDIQaM&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1902.05316&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JND-SalCAR&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TCSVT2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JND (Just-Noticeable-Difference)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nottingham-repository.worktribe.com/preview/1589753/Visual%20IEEE-TIP-2019.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;QADS&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.vista.ac.cn/super-resolution/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Super-resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sse.tongji.edu.cn/linzhang/IQA/FSIM/Files/Fsim%20a%20feature%20similarity%20index%20for%20image%20quality%20assessment.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FSIM&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sse.tongji.edu.cn/linzhang/IQA/FSIM/FSIM.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2004/hrs_ieeetip_2004_imginfo.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;VIF/IFC&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2006&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/Quality/VIF.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/publications/msssim.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MS-SSIM&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/research/ssim/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SSIM&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2004&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~z70wang/research/ssim/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PSNR&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Traditional&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | []() | | FR | | []() |  --&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Keywords&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.03889&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;NiNLoss&lt;/td&gt; &#xA;   &lt;td&gt;ACMM2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lidq92/LinearityIQA&#34;&gt;Official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Norm-in-Norm Loss&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;h3&gt;IQA datasets&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Link&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Images&lt;/th&gt; &#xA;   &lt;th&gt;Annotations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.10088&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PaQ-2-PiQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baidut/PaQ-2-PiQ&#34;&gt;Official github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;40k, 120k patches&lt;/td&gt; &#xA;   &lt;td&gt;4M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_Perceptual_Quality_Assessment_of_Smartphone_Photography_CVPR_2020_paper.html&#34;&gt;CVF&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SPAQ&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/h4nwei/SPAQ&#34;&gt;Offical github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11k (smartphone)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.06180&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KonIQ-10k&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://database.mmsp-kn.de/koniq-10k-database.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;10k from &lt;a href=&#34;http://projects.dfki.uni-kl.de/yfcc100m/&#34;&gt;YFCC100M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.2M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1511.02919&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CLIVE&lt;/td&gt; &#xA;   &lt;td&gt;NR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/ChallengeDB/index.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1200&lt;/td&gt; &#xA;   &lt;td&gt;350k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://refbase.cvc.uab.es/files/MMP2012a.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AVA&lt;/td&gt; &#xA;   &lt;td&gt;NR / Aesthentic&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mtobeiyf/ava_downloader&#34;&gt;Github&lt;/a&gt;/&lt;a href=&#34;http://www.lucamarchesotti.com/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;250k (60 categories)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.12142&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PIPAL&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;ECCV2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.jasongt.com/projectpages/pipal.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;250&lt;/td&gt; &#xA;   &lt;td&gt;1.13M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2001.08113&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KADIS-700k&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;arXiv&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://database.mmsp-kn.de/kadid-10k-database.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;140k pristine / 700k distorted&lt;/td&gt; &#xA;   &lt;td&gt;30 ratings (DCRs) per image.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8743252&#34;&gt;IEEE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;KADID-10k&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;QoMEX2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://database.mmsp-kn.de/kadid-10k-database.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;81&lt;/td&gt; &#xA;   &lt;td&gt;10k distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~k29ma/papers/17_TIP_EXPLORATION.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Waterloo-Exp&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ece.uwaterloo.ca/~k29ma/exploration/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4744&lt;/td&gt; &#xA;   &lt;td&gt;94k distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://daneshyari.com/article/preview/533080.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MDID&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;PR2017&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;1600 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/papers/euvip_tid2013.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TID2013&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;SP2015&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/tid2013.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;3000 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.298.9133&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LIVEMD&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;ACSSC2012&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/Quality/live_multidistortedimage.html&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15 pristine images&lt;/td&gt; &#xA;   &lt;td&gt;two successive distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Damon-Chandler/publication/220050520_Most_apparent_distortion_Full-reference_image_quality_assessment_and_the_role_of_strategy/links/5629cd1c08ae518e347e1445/Most-apparent-distortion-Full-reference-image-quality-assessment-and-the-role-of-strategy.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CSIQ&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;Journal of Electronic Imaging 2010&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td&gt;866 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/papers/mre2009tid.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TID2008&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;2009&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.ponomarenko.info/tid2008.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;1700 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/publications/2006/hrs-transIP-06.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LIVE IQA&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;TIP2006&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://live.ece.utexas.edu/research/Quality/subjective.htm&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;29 images, 780 synthetic distortions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://hal.univ-nantes.fr/hal-00580755/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IVC&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;2005&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;185 distortions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Perceptual similarity datasets&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Paper Title&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Name&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Published&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Images&lt;/th&gt; &#xA;   &lt;th&gt;Annotations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.03924&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BAPPS(LPIPS)&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://richzhang.github.io/PerceptualSimilarity/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;187.7k&lt;/td&gt; &#xA;   &lt;td&gt;484k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.02067&#34;&gt;arXiv&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PieAPP&lt;/td&gt; &#xA;   &lt;td&gt;FR&lt;/td&gt; &#xA;   &lt;td&gt;CVPR2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://civc.ucsb.edu/graphics/Papers/CVPR2018_PieAPP/&#34;&gt;Project&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200 images&lt;/td&gt; &#xA;   &lt;td&gt;2.3M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>