<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-07T01:41:31Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>guidoism/tex-oberon</title>
    <updated>2024-01-07T01:41:31Z</updated>
    <id>tag:github.com,2024-01-07:/guidoism/tex-oberon</id>
    <link href="https://github.com/guidoism/tex-oberon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make Project Oberon Pretty Again&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Project Oberon TeX Typesetting Project&lt;/h1&gt; &#xA;&lt;p&gt;Project Oberon is an amazing piece of computer science and amazing tool for teaching. The book and the code demonstrate, without any tiny amount of doubt, that it is possible to build a usable computing system small enough to fit in the head of a normal programmer. That, in my not-so-humble opinion, is a truly great achievement and I am in awe of Niklaus Wirth and Jürg Gutknecht for it.&lt;/p&gt; &#xA;&lt;p&gt;The last edition -- from 2013 -- could use some tender loving care. It should look beautiful.&lt;/p&gt; &#xA;&lt;p&gt;This project is an attempt to:&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Liberate the text from the un-editable PDF sources.&lt;/li&gt; &#xA; &lt;li&gt;Typeset the book using (plain) TeX and Knuth&#39;s own &lt;code&gt;taocpmac.tex&lt;/code&gt; macros.&lt;/li&gt; &#xA; &lt;li&gt;Add the full (typeset) source code to the book for those of us who like reading code in bed.&lt;/li&gt; &#xA; &lt;li&gt;Convert it into a Literate Programming project where code is more liberally scattered amongst the prose and &#34;tangled&#34; into the final product.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Being a document with TeX sources, instead of a dead PDF, future Project Oberon engineers will be able to modify the text to keep it up-to-date with the running source code.&lt;/p&gt; &#xA;&lt;p&gt;I have a dream that we -- as the Oberon community -- can edit and publish this book. I would love for this to be printed as a nice hardcover that can sit right next to my Art of Computer Programming books.&lt;/p&gt; &#xA;&lt;p&gt;PDF and code taken from &lt;a href=&#34;http://www.projectoberon.com&#34;&gt;http://www.projectoberon.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Progress&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Copy text / basic typesetting&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; First proofreading pass&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Second proofreading pass&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;luatex oberon.tex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A recent in-progress PDF can be found in the Releases&lt;/p&gt; &#xA;&lt;h2&gt;Quotes&lt;/h2&gt; &#xA;&lt;p&gt;The requirement of many megabytes of store for an operating system is, albeit commonly tolerated, absurd and another hallmark of user-unfriendliness, or perhaps manufacturer friendliness.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MichaelMaiii/AIGC-Brain</title>
    <updated>2024-01-07T01:41:31Z</updated>
    <id>tag:github.com,2024-01-07:/MichaelMaiii/AIGC-Brain</id>
    <link href="https://github.com/MichaelMaiii/AIGC-Brain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIGC-Brain&lt;/h1&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.00430&#34;&gt;Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This project is associated with our survey paper which comprehensively examines the emerging field of AIGC-based brain-conditional multimodal synthesis, termed AIGC-Brain, to delineate the current landscape and future directions. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.00430&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2401.00430-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Naereen/StrapDown.js/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/Naereen/Strapdown.js&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/AIGC-Brain.png&#34; align=&#34;center&#34;&gt; Brain-Conditional Multimodal Synthesis via AIGC-Brain Decoder. Sensory stimuli comprising visual stimuli (Image (I), Video (V)) and audio stimuli (Music (M), Speech/Sound (S)) from the external world are first encoded to non-invasive brain signals (EEG, fMRI, or MEG) and then decoded back to perceptual experience via the AIGC-Brain decoder. This survey focuses on passive brain-conditional multimodal synthesis tasks including Image-Brain-Image (IBI), Video-Brain-Video (VBV), Sound-Brain-Sound (SBS), Music-Brain-Music (MBM), Image-Brain-Text (IBT), Video-Brain-Text (VBT), and Speech-Brain-Text (SBT), where IBI refers to image synthesis tasks conditioned on brain signals evoked by image stimuli. &lt;br&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/Result.png&#34; align=&#34;center&#34;&gt; Qualitative Results for AIGC-Brain Tasks. A: IBI results on GOD (left) and EEG-VOA (right) datasets; B: IBI and IBT results on NSD dataset; C: VBV results on DNV dataset; D: VBT (top) and SBT (bottom) results based on CLSR; E: MBM results based on Brain2Music; F: SBS results based on BSR. &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of Contents (Work in Progress)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;AIGC-Brain Tasks &amp;amp; Implementations:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- ### Tasks: --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#image-brain-image&#34;&gt;Image-Brain-Image (Image Reconstruction)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#ibi-fmri&#34;&gt;IBI-fMRI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#ibi-eeg&#34;&gt;IBI-EEG&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#ibi-meg&#34;&gt;IBI-MEG&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#image-brain-imagetext&#34;&gt;Image-Brain-Image&amp;amp;Text (Image&amp;amp;Semantic Reconstruction)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#video-brain-video&#34;&gt;Video-Brain-Video (Video Reconstruction)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#sound-brain-sound&#34;&gt;Sound-Brain-Sound (Sound Reconstruction)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#music-brain-music&#34;&gt;Music-Brain-Music (Music Reconstruction)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#imagevideospeech-brain-text&#34;&gt;Image&amp;amp;Video&amp;amp;Speech-Brain-Text (Semantic Reconstruction)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#image-brain-text&#34;&gt;Image-Brain-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#speech-brain-text&#34;&gt;Speech-Brain-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#videospeech-brain-text&#34;&gt;Video&amp;amp;Speech-Brain-Text&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#active-tasks&#34;&gt;Active Tasks&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#brain-to-image&#34;&gt;Brain-to-Image (Visual-Imagery)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#brain-to-text&#34;&gt;Brain-to-Text (Silent-Reading)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#invasive-tasks&#34;&gt;Invasive Tasks&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#brain-to-speech&#34;&gt;Brain-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#brain-to-music&#34;&gt;Brain-to-music&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#neuroimaging-dataset&#34;&gt;Neuroimaging Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Image-Brain-Image&lt;/h2&gt; &#xA;&lt;h3&gt;IBI-fMRI&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DREAM: Visual Decoding from Reversing Human Visual System&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Raoul de Charette, Cengiz Öztireli, Jing-Hao Xue&lt;/em&gt;&lt;br&gt; arXiv 2023.10 [&lt;a href=&#34;https://arxiv.org/abs/2310.02265&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/DREAM&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://weihaox.github.io/DREAM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainSD) High-resolution image reconstruction with latent diffusion models from human brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu Takagi, Shinji Nishimoto&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.11.18.517004v3&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yu-takagi/StableDiffusionReconstruction&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stablediffusion-with-brain/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainSD-TGD) Improving visual image reconstruction from human brain activity using latent diffusion models via multiple decoded inputs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu Takagi, Shinji Nishimoto&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.11536&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yu-takagi/StableDiffusionReconstruction&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stablediffusion-with-brain/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainDiffuser) Natural scene reconstruction from fMRI signals using generative latent diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Furkan Ozcelik, Rufin VanRullen&lt;/em&gt;&lt;br&gt; Scientific Reports 2023 [&lt;a href=&#34;https://www.nature.com/articles/s41598-023-42891-8&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://github.com/ozcelikfu/brain-diffuser&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yizhuo Lu, Changde Du, Dianpeng Wang, Huiguang He&lt;/em&gt;&lt;br&gt; ACM MM 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.14139&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ReedOnePeck/MindDiffuser&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mind Reader: Reconstructing complex images from brain activities&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sikun Lin, Thomas Sprague, Ambuj K Singh&lt;/em&gt;&lt;br&gt; NeurIPS 2022 [&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/hash/bee5125b773414d3d6eeb4334fbc5453-Abstract-Conference.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sklin93/mind-reader&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MindEye) Reconstructing the Mind&#39;s Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, Tanishq Mathew Abraham&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.18274&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MedARC-AI/fMRI-reconstruction-NSD&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://medarc-ai.github.io/mindeye/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yulong Liu, Yongqiang Ma, Wei Zhou, Guibo Zhu, Nanning Zheng&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.12971&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YulongBonjour/BrainCLIP&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainSCN) Decoding natural image stimuli from fMRI data with a surface-based convolutional network&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zijin Gu, Keith Jamison, Amy Kuceyeski, Mert Sabuncu&lt;/em&gt;&lt;br&gt; MIDL 2023 [&lt;a href=&#34;https://arxiv.org/abs/2212.02409&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zijin-gu/meshconv-decoding&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MindVis) Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, Juan Helen Zhou&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2211.06956&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zjc062/mind-vis&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mind-vis.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-pretrain&#34;&gt;HCP&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BLOD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(LEA) Joint fMRI Decoding and Encoding with Latent Embedding Alignment&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xuelin Qian, Yikai Wang, Yanwei Fu, Xinwei Sun, Xiangyang Xue, Jianfeng Feng&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.14730&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-pretrain&#34;&gt;HCP&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BLOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(CMVDM) Controllable Mind Visual Diffusion Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu, Baochang Zhang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.10135&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-pretrain&#34;&gt;HCP&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BLOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(CAD) Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jingyuan Sun, Mingxiao Li, Zijiao Chen, Yunhao Zhang, Shaonan Wang, Marie-Francine Moens&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.17214&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-pretrain&#34;&gt;HCP&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BLOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainICG) Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, Rufin VanRullen&lt;/em&gt;&lt;br&gt; IJCNN 2022 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9892673&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ozcelikfu/IC-GAN_fMRI_Reconstruction&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(DBDM) Dual-Guided Brain Diffusion Model: Natural Image Reconstruction from Human Visual Stimulus fMRI&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lu Meng, Chuanhao Yang&lt;/em&gt;&lt;br&gt; Bioengineering 2023 [&lt;a href=&#34;https://www.mdpi.com/2306-5354/10/10/1117&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(VQ-fMRI) Rethinking Visual Reconstruction: Experience-Based Content Completion Guided by Visual Cues&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiaxuan Chen, Yu Qi, Gang Pan&lt;/em&gt;&lt;br&gt; ICML 2023 [&lt;a href=&#34;https://openreview.net/forum?id=l3sdNQdmQh&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainHVAE) Generative Decoding of Visual Stimuli&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Eleni Miliotou, Panagiotis Kyriakis, Jason D Hinman, Andrei Irimia, Paul Bogdan&lt;/em&gt;&lt;br&gt; ICML 2023 [&lt;a href=&#34;https://openreview.net/forum?id=57OuafQmu8&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainHSG) Semantics-guided hierarchical feature encoding generative adversarial network for natural image reconstruction from brain activities&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lu Meng, Chuanhao Yang&lt;/em&gt;&lt;br&gt; IJCNN 2023 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10191903&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(SBD) Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Matteo Ferrante, Tommaso Boccato, Nicola Toschi&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2212.06726&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/matteoferrante/semantic-brain-decoding&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(SSNIR) From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, Michal Irani&lt;/em&gt;&lt;br&gt; NeurIPS 2019 [&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2019/hash/7d2be41b1bde6ff8fe45150c37488ebb-Abstract.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.wisdom.weizmann.ac.il/~vision/ssfmri2im/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://github.com/WeizmannVision/ssfmri2im&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;Vim-1&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(SSNIR-SC) Self-supervised Natural Image Reconstruction and Large-scale Semantic Classification from Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guy Gaziv, Roman Beliy, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, Michal Irani&lt;/em&gt;&lt;br&gt; NeuroImage 2022 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S105381192200249X&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WeizmannVision/SelfSuperReconst&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;Vim-1&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(SSDR) More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guy Gaziv, Michal Irani&lt;/em&gt;&lt;br&gt; arXiv 2021 [&lt;a href=&#34;https://arxiv.org/abs/2106.05113&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WeizmannVision/SelfSuperReconst&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;Vim-1&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainSSG) Reconstructing Perceptive Images from Brain Activity by Shape-Semantic GAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Fang, Yu Qi, Gang Pan&lt;/em&gt;&lt;br&gt; NeurIPS 2020 [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/9813b270ed0288e7c0388f0fd4ec68f5-Abstract.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/duolala1/Reconstructing-Perceptive-Images-from-Brain-Activity-by-Shape-Semantic-GAN&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;]]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainBBG) Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Milad Mozafari, Leila Reddy, Rufin VanRullen&lt;/em&gt;&lt;br&gt; IJCNN 2020 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9206960&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainDVG) Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, Xinbo Gao&lt;/em&gt;&lt;br&gt; NeuroImage 2021 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811920310879&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BRAINS&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BCP&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;6-9&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainDCG) Generative adversarial networks for reconstructing natural images from brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;K. Seeliger, U. Güçlü, L. Ambrogioni, Y. Güçlütürk, M.A.J. van Gerven&lt;/em&gt;&lt;br&gt; NeuroImage 2018 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S105381191830658X&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/seelikat/ganrecon&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;Vim-1&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;BRAINS&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;(DIR) Deep image reconstruction from human brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani&lt;/em&gt;&lt;br&gt; PLOS Computational Biology 2019 [&lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KamitaniLab/DeepImageReconstruction&#34;&gt;Code&lt;/a&gt;]&lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;DIR&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(E-DIR) End-to-End Deep Image Reconstruction From Human Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, Yukiyasu Kamitani&lt;/em&gt;&lt;br&gt; Frontiers in Computational Neuroscience 2019 [&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2019.00021/full&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;DIR&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;BigGAN-based Bayesian Reconstruction of Natural Images from Human Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kai Qiao, Jian Chen, Linyuan Wang, Chi Zhang, Li Tong, Bin Yan&lt;/em&gt;&lt;br&gt; Neuroscience 2020 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0306452220304814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;Vim-1&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Faces) Reconstructing faces from fMRI patterns using deep generative neural networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rufin VanRullen, Leila Reddy&lt;/em&gt;&lt;br&gt; Communications Biology 2019 [&lt;a href=&#34;https://www.nature.com/articles/s42003-019-0438-y&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rufinv/VAE-GAN-celebA&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;Faces&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;IBI-EEG&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamDiffusion: Generating High-Quality Images from Brain EEG Signals&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yunpeng Bai, Xintao Wang, Yan-pei Cao, Yixiao Ge, Chun Yuan, Ying Shan&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.16934&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bbaaii/DreamDiffusion&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(NeuroImagen) Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, Lili Qiu&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.02510&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DM-RE2I: A framework based on diffusion model for the reconstruction from EEG to image&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hong Zeng, Nianzhang Xia, Dongguan Qian, Motonobu Hattori, Chu Wang, Wanzeng Kong&lt;/em&gt;&lt;br&gt; BSPC 2023 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S174680942300558X&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain2Image: Converting Brain Signals into Images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Isaak Kavasidis, Simone Palazzo, Concetto Spampinato, Daniela Giordano, Mubarak Shah&lt;/em&gt;&lt;br&gt; ACM MM 2017 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S174680942300558X&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuroVision: perceived image regeneration using cProGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sanchita Khare, Rajiv Nayan Choubey, Loveleen Amar, Venkanna Udutalapalli&lt;/em&gt;&lt;br&gt; Neural Computing and Applications 2022 [&lt;a href=&#34;https://link.springer.com/article/10.1007/s00521-021-06774-1&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(EEG-VGD) Decoding EEG by Visual-guided Deep Neural Networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhicheng Jiao, Haoxuan You, Fan Yang, Xin Li, Han Zhang, Dinggang Shen&lt;/em&gt;&lt;br&gt; IJCAI 2019 [&lt;a href=&#34;https://www.ijcai.org/proceedings/2019/192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(EEG-GAN) Generative Adversarial Networks Conditioned by Brain Signals&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Mubarak Shah&lt;/em&gt;&lt;br&gt; ICCV 2017 [&lt;a href=&#34;https://openaccess.thecvf.com/content_iccv_2017/html/Palazzo_Generative_Adversarial_Networks_ICCV_2017_paper.html&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-VOA&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;IBI-MEG&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MEG-BD) Brain decoding: toward real-time reconstruction of visual perception&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yohann Benchetrit1, Hubert Banville1, Jean-Remi King&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2310.19812&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#meg-image&#34;&gt;MEG-Things&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Image-Brain-Image&amp;amp;Text&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weijian Mai, Zhijun Zhang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.07428&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Captioning: Decoding human brain activity into images and text&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, Nicola Toschi&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.11560&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Video-Brain-Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Mind-Video) Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zijiao Chen, Jiaxin Qing, Juan Helen Zhou&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.11675&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jqin4749/MindVideo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://mind-video.com/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;DNV&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;HCP&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(SSRNM) A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ganit Kupershmidt, Roman Beliy, Guy Gaziv, Michal Irani&lt;/em&gt;&lt;br&gt; arxiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2206.03544&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.wisdom.weizmann.ac.il/~vision/VideoReconstFromFMRI/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;DNV&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(f-CVGAN) Reconstructing rapid natural vision with fMRI-conditional video generative adversarial network&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chong Wang, Hongmei Yan, Wei Huang, Jiyi Li, Yuting Wang, Yun-Shuang Fan, Wei Sheng, Tao Liu, Rong Li, Huafu Chen&lt;/em&gt;&lt;br&gt; Cerebral Cortex 2022 [&lt;a href=&#34;https://academic.oup.com/cercor/article-abstract/32/20/4502/6515038&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;DNV&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-pretrain&#34;&gt;HCP&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BrainViVAE) Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kuan Han, Haiguang Wen, Junxing Shi, Kun-Han Lu, Yizhen Zhang, Di Fu, Zhongming Liu&lt;/em&gt;&lt;br&gt; NeuroImage 2019 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811919304318&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;DNV&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(DNV) Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, Zhongming Liu&lt;/em&gt;&lt;br&gt; Cerebral Cortex 2018 [&lt;a href=&#34;https://academic.oup.com/cercor/article/28/12/4136/4560155&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://purr.purdue.edu/publications/2809/1&#34;&gt;Data&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;DNV&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain2Pix: Fully convolutional naturalistic video reconstruction from brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lynn Le, Luca Ambrogioni, Katja Seeliger, Yağmur Güçlütürk, Marcel van Gerven, Umut Güçlü&lt;/em&gt;&lt;br&gt; Frontiers in Neuroscience 2022 [&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2022.940972/full&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/neuralcodinglab/brain2pix&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;STNS&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Sound-Brain-Sound&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BSR) Sound reconstruction from human brain activity via a generative model with brain-like auditory features&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jong-Yun Park, Mitsuaki Tsukamoto, Misato Tanaka, Yukiyasu Kamitani&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.11629&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-soundspeech&#34;&gt;BSR&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(ETCAS) End-to-end translation of human neural activity to speech with a dual–dual generative adversarial network&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yina Guo, Ting Liu, Xiaofei Zhang, Anhong Wang, Wenwu Wang&lt;/em&gt;&lt;br&gt; Knowledge-Based Systems 2023 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0950705123005877&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qwe1218088/dual-dualgan&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-soundspeech&#34;&gt;ETCAS&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Music-Brain-Music&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain2Music: Reconstructing Music from Human Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Timo I. Denk, Yu Takagi, Takuya Matsuyama, Andrea Agostinelli, Tomoya Nakai, Christian Frank, Shinji Nishimoto&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.11078&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://google-research.github.io/seanet/brain2music/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://www.kaggle.com/datasets/nishimotolab/music-caption-brain2music&#34;&gt;Data-MusicCaption&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-music&#34;&gt;MusicGenre&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(NDMusic) Neural decoding of music from the EEG&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ian Daly&lt;/em&gt;&lt;br&gt; Scientific Reports 2023 [&lt;a href=&#34;https://www.nature.com/articles/s41598-022-27361-x&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmrieeg-music&#34;&gt;MusicAffect&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Image&amp;amp;Video&amp;amp;Speech-Brain-Text&lt;/h2&gt; &#xA;&lt;h3&gt;Image-Brain-Text&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Subhrasankar Chatterjee, Debasis Samanta&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.10082&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(GIC-RL) Generation of Viewed Image Captions From Human Brain Activity Via Unsupervised Text Latent Space&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Saya Takada, Ren Togo, Takahiro Ogawa, Miki Haseyama&lt;/em&gt;&lt;br&gt; ICIP 2020 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9191262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;GOD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(GIC-PTL) A neural decoding algorithm that generates language from visual activity evoked by natural images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wei Huang, Hongmei Yan, Kaiwen Cheng et al.&lt;/em&gt;&lt;br&gt; Neural Networks 2021 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0893608021003117&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;OCD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(GIC-CT) A CNN-transformer hybrid approach for decoding visual neural activity into text&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiang Zhang, Chen Li, Ganwanming Liu et al.&lt;/em&gt;&lt;br&gt; Computer Methods and Programs in Biomedicine 2022 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S016926072100660X&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;OCD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(DSR) Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Subhrasankar Chatterjee, Debasis Samanta&lt;/em&gt;&lt;br&gt; ICSMC 2018 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8616103&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;VER&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(GNLD) Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida, Hideki Asoh&lt;/em&gt;&lt;br&gt; ACL 2016 student research workshop 2016 [&lt;a href=&#34;https://aclanthology.org/P16-3004/&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video&#34;&gt;VER&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Speech-Brain-Text&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nuwa Xi, Sendong Zhao, Haochun Wang, Chi Liu, Bing Qin, Ting Liu&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.05355&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-speech&#34;&gt;Narratives&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Video&amp;amp;Speech-Brain-Text&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(CLSR) Semantic reconstruction of continuous language from non-invasive brain recordings&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jerry Tang, Amanda LeBel, Shailee Jain, Alexander G. Huth&lt;/em&gt;&lt;br&gt; Nature Neuroscience 2023 [&lt;a href=&#34;https://www.nature.com/articles/s41593-023-01304-9&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HuthLab/semantic-decoding&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-video-speech&#34;&gt;CLSR&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Active Tasks&lt;/h2&gt; &#xA;&lt;h3&gt;Brain-to-Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Praveen Tirupattur, Yogesh Singh Rawat, Concetto Spampinato, Mubarak Shah&lt;/em&gt;&lt;br&gt; ACM MM 2018 [&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3240508.3240641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-Imagery&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuroGAN: image reconstruction from EEG signals via an attention-based GAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rahul Mishra, Krishan Sharma, R. R. Jha, Arnav Bhavsar&lt;/em&gt;&lt;br&gt; Neural Computing and Applications 2023 [&lt;a href=&#34;https://link.springer.com/article/10.1007/s00521-022-08178-1&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-Imagery&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EEG2IMAGE: Image Reconstruction from EEG Brain Signals&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Prajwal Singh, Pankaj Pandey, Krishna Miyapuram, Shanmuganathan Raman&lt;/em&gt;&lt;br&gt; ICASSP 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.10121&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/prajwalsingh/EEG2Image&#34;&gt;Code&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#eeg-image&#34;&gt;EEG-Imagery&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Brain-to-Text&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dewave: Discrete eeg waves encoding for brain dynamics to text translation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqun Duan, Jinzhao Zhou, Zhen Wang, Yu-Kai Wang, Chin-Teng Lin&lt;/em&gt;&lt;br&gt; NeurIPS 2023 [&lt;a href=&#34;https://arxiv.org/abs/2309.14030&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Invasive Tasks&lt;/h2&gt; &#xA;&lt;h3&gt;Brain-to-Speech&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech synthesis from neural decoding of spoken sentences&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gopala K. Anumanchipalli, Josh Chartier &amp;amp; Edward F. Chang&lt;/em&gt;&lt;br&gt; Nature 2019 [&lt;a href=&#34;https://www.nature.com/articles/s41586-019-1119-1.&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Brain-to-Music&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Music can be reconstructed from human auditory cortex activity using nonlinear decoding models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ludovic Bellier, Anaïs Llorens, et al.&lt;/em&gt;&lt;br&gt; PLOS Biology 2023 [&lt;a href=&#34;https://journals.plos.org/plosbiology/&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Neuroimaging Dataset&lt;/h2&gt; &#xA;&lt;h3&gt;fMRI-Pretrain&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(HCP) The WU-Minn Human Connectome Project: An overview&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David C. Van Essen a, Stephen M. Smith b, Deanna M. Barch c, Timothy E.J. Behrens b, Essa Yacoub d, Kamil Ugurbil d, for the WU-Minn HCP Consortium&lt;/em&gt;&lt;br&gt; NeuroImage 2013 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811913005351&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MOABB) MOABB: trustworthy algorithm benchmarking for BCIs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Vinay Jayaram, Alexandre Barachant&lt;/em&gt;&lt;br&gt; Journal of Neural Engineering 2018 [&lt;a href=&#34;https://iopscience.iop.org/article/10.1088/1741-2552/aadea0/meta&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NeuroTechX/moabb&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;fMRI-Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(NSD) A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Emily J. Allen, Ghislain St-Yves, Yihan Wu, Jesse L. Breedlove, Jacob S. Prince, Logan T. Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, J. Benjamin Hutchinson, Thomas Naselaris &amp;amp; Kendrick Kay&lt;/em&gt;&lt;br&gt; Nature Neuroscience 2021 [&lt;a href=&#34;https://www.nature.com/articles/s41593-021-00962-x&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://naturalscenesdataset.org/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(GOD) Generic decoding of seen and imagined objects using hierarchical visual features&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tomoyasu Horikawa, Yukiyasu Kamitani&lt;/em&gt;&lt;br&gt; Nature Communications 2017 [&lt;a href=&#34;https://www.nature.com/articles/ncomms15037&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KamitaniLab/GenericObjectDecoding&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BOLD) BOLD5000, a public fMRI dataset while viewing 5000 visual images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nadine Chang, John A. Pyles, Austin Marcus, Abhinav Gupta, Michael J. Tarr &amp;amp; Elissa M. Aminoff&lt;/em&gt;&lt;br&gt; Scientific Data 2019 [&lt;a href=&#34;https://www.nature.com/articles/s41597-019-0052-3&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bold5000-dataset.github.io/website/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(DIR) Deep image reconstruction from human brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani&lt;/em&gt;&lt;br&gt; PLOS Computational Biology 2019 [&lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KamitaniLab/DeepImageReconstruction&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Vim-1) Identifying natural images from human brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kendrick N. Kay, Thomas Naselaris, Ryan J. Prenger, Jack L. Gallant&lt;/em&gt;&lt;br&gt; Nature 2008 [&lt;a href=&#34;https://www.nature.com/articles/nature06713&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Faces) Reconstructing faces from fMRI patterns using deep generative neural networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rufin VanRullen, Leila Reddy&lt;/em&gt;&lt;br&gt; Communications Biology 2019 [&lt;a href=&#34;https://www.nature.com/articles/s42003-019-0438-y&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rufinv/VAE-GAN-celebA&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://openneuro.org/datasets/ds001761/versions/2.0.1&#34;&gt;Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(OCD) Long short-term memory-based neural decoding of object categories evoked by natural images&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wei Huang, Hongmei Yan, Chong Wang et al.&lt;/em&gt;&lt;br&gt; Human Brain Mapping 2020 [&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.25136&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://www.neuro.uestc.edu.cn/vccl/data/Huang2020_Article_Perception-to-ImageReconstruct.html&#34;&gt;Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BRAINS) Linear reconstruction of perceived images from human brain activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sanne Schoenmakers, Markus Barth, Tom Heskes, Marcel van Gerven&lt;/em&gt;&lt;br&gt; NeuroImage 2013 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811913007994&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BCP) Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yoichi Miyawaki, Hajime Uchida, Okito Yamashita et al.&lt;/em&gt;&lt;br&gt; Neuron 2008 [&lt;a href=&#34;https://www.cell.com/fulltext/S0896-6273(08)00958-6&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(6-9) Neural Decoding with Hierarchical Generative Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Marcel A. J. van Gerven, Floris P. de Lange, Tom Heskes&lt;/em&gt;&lt;br&gt; Neural Computation 2010 [&lt;a href=&#34;https://direct.mit.edu/neco/article-abstract/22/12/3127/7597/Neural-Decoding-with-Hierarchical-Generative&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;EEG-Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(EEG-VOA) Deep Learning Human Mind for Automated Visual Classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, Mubarak Shah&lt;/em&gt;&lt;br&gt; CVPR 2017 [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/html/Spampinato_Deep_Learning_Human_CVPR_2017_paper.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/perceivelab/eeg_visual_classification&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(EEG-Things) A large and rich EEG dataset for modeling human visual object recognition&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alessandro T. Gifford, Kshitij Dwivedi, Gemma Roig, Radoslaw M. Cichy&lt;/em&gt;&lt;br&gt; NeuroImage 2022 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811922008758?via%3Dihub&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gifale95/eeg_encoding&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://osf.io/3jk45/&#34;&gt;Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(EEG-Imagery) Envisioned speech recognition using EEG sensors&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Pradeep Kumar, Rajkumar Saini, Partha Pratim Roy, Pawan Kumar Sahu, Debi Prosad Dogra&lt;/em&gt;&lt;br&gt; Personal and Ubiquitous Computing 2018 [&lt;a href=&#34;https://link.springer.com/article/10.1007/s00779-017-1083-4&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;MEG-Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MEG-Things) THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Martin N Hebart, Oliver ContierLina, TeichmannAdam et al.&lt;/em&gt;&lt;br&gt; Elife 2023 [&lt;a href=&#34;https://elifesciences.org/articles/82580#&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://things-initiative.org/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;fMRI-Video&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(DNV) Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, Zhongming Liu&lt;/em&gt;&lt;br&gt; Cerebral Cortex 2018 [&lt;a href=&#34;https://academic.oup.com/cercor/article/28/12/4136/4560155&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://purr.purdue.edu/publications/2809/1&#34;&gt;Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(VER) Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shinji Nishimoto, An T. Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, Jack L. Gallant&lt;/em&gt;&lt;br&gt; Current Biology 2011 [&lt;a href=&#34;https://www.cell.com/fulltext/S0960-9822(11)00937-7&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;fMRI-Video&amp;amp;Speech&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(STNS) A large single-participant fMRI dataset for probing brain responses to naturalistic stimuli in space and time&lt;/strong&gt;&lt;br&gt; &lt;em&gt;K.Seeliger, R.P.Sommers, U.Güçlü, S.E.Bosch, M.A.J.van Gerven&lt;/em&gt;&lt;br&gt; bioRxiv 2019 [&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/687681v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://data.donders.ru.nl/collections/di/dcc/DSC_2018.00082_134?2&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(CLSR) Semantic reconstruction of continuous language from non-invasive brain recordings&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jerry Tang, Amanda LeBel, Shailee Jain, Alexander G. Huth&lt;/em&gt;&lt;br&gt; Nature Neuroscience 2023 [&lt;a href=&#34;https://www.nature.com/articles/s41593-023-01304-9&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HuthLab/semantic-decoding&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://openneuro.org/datasets/ds003020/versions/2.0.3&#34;&gt;Data-Train-Speech&lt;/a&gt;] [&lt;a href=&#34;https://openneuro.org/datasets/ds004510/versions/1.1.0&#34;&gt;Data-Test-SpeechVideo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;fMRI-Sound&amp;amp;Speech&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(BSR) Sound reconstruction from human brain activity via a generative model with brain-like auditory features&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jong-Yun Park, Mitsuaki Tsukamoto, Misato Tanaka, Yukiyasu Kamitani&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.11629&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(ETCAS) End-to-end translation of human neural activity to speech with a dual–dual generative adversarial network&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yina Guo, Ting Liu, Xiaofei Zhang, Anhong Wang, Wenwu Wang&lt;/em&gt;&lt;br&gt; Knowledge-Based Systems 2023 Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-soundspeech&#34;&gt;ETCAS&lt;/a&gt;] &lt;br&gt; [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0950705123005877&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qwe1218088/dual-dualgan&#34;&gt;Code&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Narratives) The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Samuel A. Nastase, Yun-Fei Liu, Hanna Hillman, Asieh Zadbood, Liat Hasenfratz, et al.&lt;/em&gt;&lt;br&gt; Scientific Data 2021 [&lt;a href=&#34;https://www.nature.com/articles/s41597-021-01033-3&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/snastase/narratives&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;fMRI-Music&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MusicGenre) Music genre neuroimaging dataset&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tomoya Nakai, Naoko Koide-Majima, Shinji Nishimoto&lt;/em&gt;&lt;br&gt; Data in Brief 2022 [&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2352340921009501&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://openneuro.org/datasets/ds003720&#34;&gt;Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;fMRI&amp;amp;EEG-Music&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;(MusicAffect) Neural and physiological data from participants listening to affective music&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ian Daly, Nicoletta Nicolaou, et al.&lt;/em&gt;&lt;br&gt; Scientific Data 2020 [&lt;a href=&#34;https://www.nature.com/articles/s41597-020-0507-6&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://openneuro.org/datasets/ds002725/versions/1.0.0&#34;&gt;Data-fMRI&amp;amp;EEG&lt;/a&gt;] [&lt;a href=&#34;https://openneuro.org/datasets/ds002722/versions/1.0.1&#34;&gt;Data-EEG&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Related Surveys &amp;amp; Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Subba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.10246&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Natural Image Reconstruction From fMRI Using Deep Learning: A Survey&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zarina Rakhimberdina, Quentin Jodelet, Xin Liu, Tsuyoshi Murata&lt;/em&gt;&lt;br&gt; Frontiers in Neuroscience 2021 [&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2021.795488/full&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Image Synthesis and Editing: A Survey and Taxonomy&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu§, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing&lt;/em&gt;&lt;br&gt; TPAMI 2023 [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10230895&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fnzhan/Generative-AI&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vision + Language Applications: A Survey&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yutong Zhou, Nobutaka Shimada&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Zhou_Vision__Language_Applications_A_Survey_CVPRW_2023_paper.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Our Work&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weijian Mai, Zhijun Zhang&lt;/em&gt;&lt;br&gt; arxiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.07428&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; Dataset [&lt;a href=&#34;https://raw.githubusercontent.com/MichaelMaiii/AIGC-Brain/main/#fmri-image&#34;&gt;NSD&lt;/a&gt;]&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this project for your research, please cite our papers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{mai2023brain,&#xA;  title={Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy},&#xA;  author={Mai, Weijian and Zhang, Jian and Fang, Pengfei and Zhang, Zhijun},&#xA;  journal={arXiv preprint arXiv:2401.00430},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>COD1995/A-Comprehensive-Note-on-Machine-Learning</title>
    <updated>2024-01-07T01:41:31Z</updated>
    <id>tag:github.com,2024-01-07:/COD1995/A-Comprehensive-Note-on-Machine-Learning</id>
    <link href="https://github.com/COD1995/A-Comprehensive-Note-on-Machine-Learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;These notes are for personal and educational use only. Any redistribution or sharing without my explicit consent is prohibited. They serve as a private learning and reference resource.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A Comprehensive Note on Machine Learning&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This document serves as an educational resource primarily designed for the courses I teach, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CSE 474/574 Introduction to Machine Learning&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CSE 455/555 Pattern Recognition&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CSE 676 Deep Learning&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, it functions as a personal reference in the field of machine learning.&lt;/p&gt; &#xA;&lt;h2&gt;Purpose and Use&lt;/h2&gt; &#xA;&lt;p&gt;This compilation aims to provide an extensive overview and guide for students and practitioners of machine learning, incorporating a blend of &lt;strong&gt;direct&lt;/strong&gt; references and &lt;strong&gt;adaptations&lt;/strong&gt; from established texts and sources. It is intended for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Educational Aid&lt;/strong&gt;: As a supplementary resource for teaching and learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reference Material&lt;/strong&gt;: For personal and academic use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This document is &lt;strong&gt;not authorized&lt;/strong&gt; for commercial use, redistribution, or sale without explicit consent. To read the pdf version of the document: &lt;a href=&#34;https://github.com/COD1995/A-Comprehensive-Note-on-Machine-Learning/raw/main/machine_learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;p&gt;From my side there will be constant updates but in order to motivate my students to read I have a &lt;strong&gt;3 bonus points&lt;/strong&gt; if they make significant and meaningful contribution to each chapters of the note. I welcome contributions and feedback from the community! If you have suggestions, corrections, or additional material you think would enhance this resource, please feel free to contribute. Here&#39;s how you can do that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fork the Repository&lt;/strong&gt;: Create your own fork of the project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Make Changes&lt;/strong&gt;: Add your contributions or modifications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Submit a Pull Request&lt;/strong&gt;: Open a pull request to the original repository with a clear list of what you&#39;ve done.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Review &amp;amp; Merge&lt;/strong&gt;: I will review your changes and merge them into the main document as appropriate.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Details on how to create a pull request: &lt;a href=&#34;https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request&#34;&gt;Creating a pull request&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;When contributing, please adhere to the following guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure that any added content is accurate and relevant to machine learning.&lt;/li&gt; &#xA; &lt;li&gt;Respect intellectual property and cite sources appropriately.&lt;/li&gt; &#xA; &lt;li&gt;Maintain a respectful and constructive tone in discussions and pull requests.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation of LaTeX&lt;/h2&gt; &#xA;&lt;p&gt;LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation. My preferred setup for writing LaTeX documents is Visual Studio Code (VSCode) with the LaTeX Workshop extension, offering a user-friendly and efficient LaTeX writing experience. Below is a guide to help you set up this environment.&lt;/p&gt; &#xA;&lt;h3&gt;1. Install LaTeX Distribution&lt;/h3&gt; &#xA;&lt;p&gt;To begin with, you need a LaTeX distribution installed on your computer.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Use MiKTeX or TeX Live. MiKTeX is more user-friendly for beginners, while TeX Live is more comprehensive. Download from &lt;a href=&#34;https://miktex.org/&#34;&gt;MiKTeX&lt;/a&gt; or &lt;a href=&#34;https://www.tug.org/texlive/&#34;&gt;TeX Live&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: Install MacTeX, which is a macOS version of TeX Live with additional tools. Download from &lt;a href=&#34;http://www.tug.org/mactex/&#34;&gt;MacTeX&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Most Linux distributions include TeX Live in their package repositories. Install it using your package manager (for example, &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; in Ubuntu).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. Install Visual Studio Code (VSCode)&lt;/h3&gt; &#xA;&lt;p&gt;VSCode is a free, open-source editor with a wide array of features.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and install VSCode from the &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;official website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Install LaTeX Workshop Extension in VSCode&lt;/h3&gt; &#xA;&lt;p&gt;LaTeX Workshop enhances VSCode with LaTeX typesetting capabilities.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open VSCode.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to Extensions (&lt;code&gt;Ctrl+Shift+X&lt;/code&gt; / &lt;code&gt;Cmd+Shift+X&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Search for &#34;LaTeX Workshop&#34; and install the extension.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4. Configure LaTeX Workshop&lt;/h3&gt; &#xA;&lt;p&gt;Customize LaTeX Workshop settings for your needs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File &amp;gt; Preferences &amp;gt; Settings&lt;/code&gt; (&lt;code&gt;Code &amp;gt; Preferences &amp;gt; Settings&lt;/code&gt; on macOS).&lt;/li&gt; &#xA; &lt;li&gt;Search for &#34;LaTeX Workshop&#34; settings.&lt;/li&gt; &#xA; &lt;li&gt;Configure according to your preferences, like setting up a default compiler or enabling auto-build on save.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;5. Start Writing LaTeX&lt;/h3&gt; &#xA;&lt;p&gt;Now, you are ready to write LaTeX documents.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new file with a &lt;code&gt;.tex&lt;/code&gt; extension in VSCode.&lt;/li&gt; &#xA; &lt;li&gt;Write your LaTeX content.&lt;/li&gt; &#xA; &lt;li&gt;Use the build feature in LaTeX Workshop to compile your document into a PDF.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;6. Additional Tools and Tips&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Git Integration&lt;/strong&gt;: VSCode&#39;s integrated support for Git is beneficial for version controlling your LaTeX documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Live Preview&lt;/strong&gt;: LaTeX Workshop supports live preview of your document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Snippets&lt;/strong&gt;: Create custom snippets for frequently used LaTeX commands to improve efficiency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This setup with VSCode and LaTeX Workshop provides a powerful, modern environment for writing and managing LaTeX documents, blending LaTeX&#39;s typesetting capabilities with the features of a contemporary code editor.&lt;/p&gt; &#xA;&lt;h2&gt;Primary Sources&lt;/h2&gt; &#xA;&lt;p&gt;The majority of the material referenced in this document comes from the following key sources:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Zhang, Aston, et al. &#34;Dive into Deep Learning.&#34; Cambridge University Press, 2023.&lt;/li&gt; &#xA; &lt;li&gt;Bishop, C. M., &amp;amp; Nasrabadi, N. M. &#34;Pattern Recognition and Machine Learning&#34; (Vol. 4, No. 4, p. 738). New York: Springer, 2006.&lt;/li&gt; &#xA; &lt;li&gt;Hart, P. E., Stork, D. G., &amp;amp; Duda, R. O. &#34;Pattern Classification.&#34; Hoboken: Wiley, 2000.&lt;/li&gt; &#xA; &lt;li&gt;Burkov, A. &#34;The Hundred-Page Machine Learning Book&#34; (Vol. 1, p. 32). Quebec City, QC, Canada: Andriy Burkov, 2019.&lt;/li&gt; &#xA; &lt;li&gt;Burkov, A. &#34;Machine Learning Engineering&#34; (Vol. 1). Montreal, QC, Canada: True Positive Incorporated, 2020.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Additional References&lt;/h2&gt; &#xA;&lt;p&gt;All other referenced materials and sources are cited in the bibliography section of this document.&lt;/p&gt; &#xA;&lt;h2&gt;Contact Information&lt;/h2&gt; &#xA;&lt;p&gt;For inquiries, permissions, or further information, please reach out to me at ( &lt;a href=&#34;mailto:jueguo@buffalo.edu&#34;&gt;jueguo@buffalo.edu&lt;/a&gt; ).&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This document is provided &#34;as is,&#34; and the author makes no representations or warranties, express or implied, regarding its completeness, accuracy, or reliability.&lt;/p&gt;</summary>
  </entry>
</feed>