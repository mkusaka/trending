<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-05T01:59:11Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>latex3/latex3</title>
    <updated>2022-06-05T01:59:11Z</updated>
    <id>tag:github.com,2022-06-05:/latex3/latex3</id>
    <link href="https://github.com/latex3/latex3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The LaTeX3 Development Repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The &lt;code&gt;expl3&lt;/code&gt; (LaTeX3) Development Repository&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The repository contains development material for &lt;code&gt;expl3&lt;/code&gt;. This includes not only code to be developed into the &lt;code&gt;expl3&lt;/code&gt; kernel, but also a variety of test, documentation and more experimental material. All of this code works on top of LaTeX2e.&lt;/p&gt; &#xA;&lt;p&gt;The following directories are present in the repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;l3kernel&lt;/code&gt;: code forms the &lt;code&gt;expl3&lt;/code&gt; kernel and with the exception of the contents of &lt;code&gt;l3candidates&lt;/code&gt;, all stable code. With a modern LaTeX2e kernel, this code is loaded during format creation; when using an older LaTeX2e kernel, this material is accessible using the &lt;code&gt;expl3&lt;/code&gt; package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3backend&lt;/code&gt;: code for backend (driver) level interfaces across the &lt;code&gt;expl3&lt;/code&gt; codebase; none of this code has public interfaces, and so no distinction is made between stable and experimental code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3packages&lt;/code&gt;: code which is written to be used on top of LaTeX2e to explore interfaces; these higher-level packages are &#39;stable&#39;. It is unlikely that any new packages will be added to this area.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3experimental&lt;/code&gt;: code which is written to be used on top of LaTeX2e to experiment with code and interface concepts. The interfaces for these packages are still under active discussion. Parts of this code may eventually be migrated to &lt;code&gt;l3kernel&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3trial&lt;/code&gt;: material which is under very active development, for potential addition to &lt;code&gt;l3kernel&lt;/code&gt; or &lt;code&gt;l3experimental&lt;/code&gt;. Material in this directory may include potential replacements for existing modules, where large-scale changes are under-way. This code is &lt;em&gt;not&lt;/em&gt; released to CTAN.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3leftovers&lt;/code&gt;: code which has been developed in the past by The LaTeX Project but is not suitable for use in its current form. Parts of this code may be used as the basis for new developments in &lt;code&gt;l3kernel&lt;/code&gt; or &lt;code&gt;l3experimental&lt;/code&gt; over time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Support material for development is found in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;support&lt;/code&gt;, which contains files for the automated test suite which are &#39;local&#39; to the repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Documentation is found in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;articles&lt;/code&gt;: discussion of concepts by team members for publication in &lt;a href=&#34;http://www.tug.org/tugboat&#34;&gt;&lt;em&gt;TUGBoat&lt;/em&gt;&lt;/a&gt; or elsewhere.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The repository also contains the directory &lt;code&gt;xpackages&lt;/code&gt;. This contain code which is being moved (broadly) &lt;code&gt;l3experimental&lt;/code&gt;. Over time, &lt;code&gt;xpackages&lt;/code&gt; is expected to be removed from the repository.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;p&gt;Discussion concerning the approach, suggestions for improvements, changes, additions, &lt;em&gt;etc.&lt;/em&gt; should be addressed to the list &lt;a href=&#34;https://listserv.uni-heidelberg.de/cgi-bin/wa?A0=LATEX-L&#34;&gt;LaTeX-L&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can subscribe to this list by sending mail to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;listserv@urz.uni-heidelberg.de&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;with the body containing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;subscribe LATEX-L  &amp;lt;Your-First-Name&amp;gt; &amp;lt;Your-Second-Name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;The issue tracker for &lt;code&gt;expl3&lt;/code&gt; is currently located &lt;a href=&#34;https://github.com/latex3/latex3/issues&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report specific issues with &lt;code&gt;expl3&lt;/code&gt; code there; more general discussion should be directed to the &lt;a href=&#34;https://raw.githubusercontent.com/latex3/latex3/main/#Discussion&#34;&gt;LaTeX-L list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build status&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Actions&lt;/a&gt; as a hosted continuous integration service. For each commit, the build status is tested using the current release of TeX Live.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Current build status:&lt;/em&gt; &lt;img src=&#34;https://github.com/latex3/latex3/actions/workflows/main.yaml/badge.svg?branch=main&#34; alt=&#34;build status&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development team&lt;/h2&gt; &#xA;&lt;p&gt;This code is developed by &lt;a href=&#34;https://latex-project.org&#34;&gt;The LaTeX Project&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;This README file is copyright 2021 The LaTeX Project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openmlsys/openmlsys-zh</title>
    <updated>2022-06-05T01:59:11Z</updated>
    <id>tag:github.com,2022-06-05:/openmlsys/openmlsys-zh</id>
    <link href="https://github.com/openmlsys/openmlsys-zh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《Machine Learning Systems: Design and Implementation》- Chinese Version&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;机器学习系统：设计和实现&lt;/h1&gt; &#xA;&lt;p&gt;本开源项目试图给读者讲解现代机器学习系统的设计原理和实现经验。&lt;/p&gt; &#xA;&lt;p&gt;🔥 &lt;strong&gt;书籍网页版：&lt;/strong&gt; &lt;a href=&#34;https://openmlsys.github.io/&#34;&gt;机器学习系统：设计和实现&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🔥 &lt;strong&gt;书籍PDF：&lt;/strong&gt; 将在勘误后，四月底发布&lt;/p&gt; &#xA;&lt;h2&gt;发布&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;17/03/2022: 本书处于勘误阶段。如发现文字和图片错误，可创建Issue并@&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/editors.md&#34;&gt;章节编辑&lt;/a&gt;。我们非常欢迎社区提交PR直接勘误。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;适用读者&lt;/h2&gt; &#xA;&lt;p&gt;本书的常见读者包括：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;学生：&lt;/strong&gt; 随着大量机器学习课程在大学中的普及，学生已经开始掌握大量机器学习的基础理论和神经网络的实现。然而，需要训练出可以实际应用的机器学习模型，需要对现代机器学习系统有充分的认识。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;科研人员：&lt;/strong&gt; 研发新型的机器学习模型不仅仅需要会使用基础的机器学习系统接口。同时，新型的模型需要给系统提供新的自定义算子（Custom Operators），又或者是会利用高级的分布式执行算子来实现大模型的开发。这一系列需求都需要对底层系统具有充分认识。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;开发人员：&lt;/strong&gt; 大量的数据和AI驱动的公司都部署了机器学习基础设施。这一设施的核心就是机器学习系统。因此了解机器学习系统有助于开发人员对于系统性能调优，以定位问题，并且根据业务需求对机器学习系统进行深度定制。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;内容介绍&lt;/h2&gt; &#xA;&lt;p&gt;现代机器学习框架具有复杂的内部架构和繁多的外部相关组件。在本书中，我们将对其细致拆分，深入解读：&lt;/p&gt; &#xA;&lt;p&gt;基础：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;编程接口：&lt;/strong&gt; 为了支持海量应用，机器学习框架的编程接口设计具有大量的设计哲学，在易用性和性能之间取得平衡。本书将讲述编程接口的演进，机器学习工作流，定义深度学习模型，以及用C/C++进行框架开发。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;计算图：&lt;/strong&gt; 机器学习框架需要支持自动微分，硬件加速器，多编程前端等。实现这些支持的核心技术是：计算图（Computational Graph）。本书将讲述计算图的基本构成，生成方法和调度策略。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;性能进阶：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;编译器前端：&lt;/strong&gt; 机器学习框架需要利用编译器前端技术对计算图进行功能拓展和性能优化。本书将讲述常见的前端技术，包括类型推导，中间表示（Intermediate Representation），自动微分等。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;编译器后端和运行时：&lt;/strong&gt; 机器学习框架的一个核心目标是：如何充分利用异构硬件。这其中会涉及编译器后端技术，以及将计算图算子（Operator）调度到硬件上的运行时（Runtime）。本书将讲述计算图优化，算子选择，内存分配和计算调度与执行。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;硬件加速器：&lt;/strong&gt; 机器学习框架的基本运行单元是算子，而算子的实现必须充分利用硬件加速器（GPU和Ascend）的特性。本书将会讲述硬件加速器的基本构成原理和常见的高性能编程接口。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;数据处理框架：&lt;/strong&gt; 机器学习框架会集成高性能框架来进行数据预处理。本书将会讲述这一类数据处理框架在设计中需要达到的多个目标：易用性，高效性，保序性，分布式等。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;模型部署：&lt;/strong&gt; 在模型完成训练后，用户需要将模型部署到终端设备（如云服务器，移动终端和无人车）。这其中涉及到的模型转换，模型压缩，模型推理和安全保护等知识也会在本书中讨论。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;分布式训练：&lt;/strong&gt; 机器学习模型的训练需要消耗大量资源。越来越多的机器学习框架因此原生支持分布式训练。在本书中我们将会讨论常见的分布式训练方法（包括数据并行，模型并行和流水线并行），以及实现这些方法的系统架构（包括集合通讯和参数服务器）。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;功能拓展：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;深度学习推荐系统：&lt;/strong&gt; 推荐系统是目前机器学习应用最成功的领域之一。本书将会概括推荐系统的运作原理，详细描述大规模工业场景下的推荐系统架构设计。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;联邦学习系统：&lt;/strong&gt; 随着数据保护法规和隐私保护的崛起，联邦学习正成为日益重要的研究领域。本书将会介绍联邦学习的常用方法以及相关系统实现。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;强化学习系统：&lt;/strong&gt; 强化学习是走向通用人工智能的关键技术。本书将会介绍目前常见的强化学习系统（包括单智能体和多智能体等）。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;可解释性AI系统：&lt;/strong&gt; 随着机器学习在安全攸关（Safety-critical）领域的应用，机器学习系统越来越需要对决策给出充分解释。本书将会讨论可解释AI系统的常用方法和落地实践经验。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;机器人系统：&lt;/strong&gt; 机器人（无人车，无人机，家用机器人等）作为机器学习技术重要的应用领域，在最近数年得到了广泛应用。在实践中，机器人系统在实时性，安全性，鲁棒性等方面都有极高要求，这要求开发者具有算法和系统的双重思维，从而解决实际问题。本书中我们将结合最新研究成果和机器人系统实践经验讲解该类系统的设计原则和实现细节。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我们在持续拓展拓展本书的内容，如元学习系统，自动并行，深度学习集群调度，绿色AI系统，图学习系统等。我们也非常欢迎社区对于新内容提出建议，贡献章节。&lt;/p&gt; &#xA;&lt;h2&gt;构建指南&lt;/h2&gt; &#xA;&lt;p&gt;请参考&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/info.md&#34;&gt;构建指南&lt;/a&gt;来了解如何构建本书的网页版本和PDF版本。&lt;/p&gt; &#xA;&lt;h2&gt;写作指南&lt;/h2&gt; &#xA;&lt;p&gt;我们欢迎大家来一起贡献和更新本书的内容。常见的贡献方式是提交PR来更新和添加Markdown文件。写作的风格和图片要求请参考&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/style.md&#34;&gt;风格指南&lt;/a&gt;。同时，机器学习领域涉及到大量的中英文翻译，相关的翻译要求请参考&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/terminology.md&#34;&gt;术语指南&lt;/a&gt;。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>weihaox/awesome-gan-inversion</title>
    <updated>2022-06-05T01:59:11Z</updated>
    <id>tag:github.com,2022-06-05:/weihaox/awesome-gan-inversion</id>
    <link href="https://github.com/weihaox/awesome-gan-inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of resources on GAN inversion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;p align=&#34;center&#34;&gt;&lt;code&gt;awesome gan-inversion&lt;/code&gt;&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo is a collection of resources on GAN inversion, as a supplement for our &lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;survey&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;survey&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Inversion: A Survey.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;citation&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;@article{xia2021survey,&#xA;    author  = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},&#xA;    title   = {GAN Inversion: A Survey},&#xA;    journal = {arXiv preprint arXiv: 2101.05278},&#xA;    year={2021}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;inverted pretrained model&lt;/h2&gt; &#xA;&lt;h3&gt;2D GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://axelsauer.com/&#34;&gt;Axel Sauer&lt;/a&gt;, &lt;a href=&#34;https://katjaschwarz.github.io/&#34;&gt;Katja Schwarz&lt;/a&gt;, &lt;a href=&#34;http://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.00273&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stylegan-xl/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/autonomousvision/stylegan_xl&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Distilled StyleGAN: Towards Generation from Internet Photos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rmokady.github.io/&#34;&gt;Ron Mokady&lt;/a&gt;, Michal Yarom, Omer Tov, Oran Lang, Daniel Cohen-Or, Tali Dekel, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.12211&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://self-distilled-stylegan.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/self-distilled-stylegan/self-distilled-internet-photos&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling Off-the-shelf Models for GAN Training.&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://nupurkmr9.github.io/&#34;&gt;Nupur Kumari&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2112.09130.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~vision-aided-gan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nupurkmr9/vision-aided-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN3: Alias-Free Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.12423&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/alias-free-gan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/alias-free-gan-pytorch&#34;&gt;Rosinality&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2-Ada: Training Generative Adversarial Networks with Limited Data.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.06676&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/woctezuma/steam-stylegan2-ada&#34;&gt;Steam StyleGAN2-ADA&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/samuli-laine&#34;&gt;Samuli Laine&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/miika-aittala&#34;&gt;Miika Aittala&lt;/a&gt;, Janne Hellsten, Jaakko Lehtinen, &lt;a href=&#34;https://research.nvidia.com/person/timo-aila&#34;&gt;Timo Aila&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.04958&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;PyTorch&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;Offical TF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/manicman1999/StyleGAN2-Tensorflow-2.0&#34;&gt;Unoffical Tensorflow 2.0&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Samuli Laine, Timo Aila.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1812.04948&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen.&lt;/em&gt;&lt;br&gt; ICLR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1710.10196&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tkarras/progressive_growing_of_gans&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D-aware GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;EG3D: Efficient Geometry-aware 3D Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, &lt;a href=&#34;https://matthew-a-chan.github.io/&#34;&gt;Matthew A. Chan&lt;/a&gt;, &lt;a href=&#34;https://luminohope.org/&#34;&gt;Koki Nagano&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~bxpan/&#34;&gt;Boxiao Pan&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/shalini-gupta&#34;&gt;Shalini De Mello&lt;/a&gt;, &lt;a href=&#34;https://oraziogallo.github.io/&#34;&gt;Orazio Gallo&lt;/a&gt;, &lt;a href=&#34;https://geometry.stanford.edu/member/guibas/&#34;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/jonathan-tremblay&#34;&gt;Jonathan Tremblay&lt;/a&gt;, &lt;a href=&#34;https://www.samehkhamis.com/&#34;&gt;Sameh Khamis&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.07945&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://matthew-a-chan.github.io/EG3D&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://jiataogu.me/&#34;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://totoro97.github.io/about.html&#34;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&#34;http://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.08985&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~royorel/&#34;&gt;Roy Or-El&lt;/a&gt;, &lt;a href=&#34;https://roxanneluo.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.11427&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylesdf.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/royorel/StyleSDF&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Marco Monteiro&lt;/a&gt;, &lt;a href=&#34;https://kellnhofer.xyz/&#34;&gt;Petr Kellnhofer&lt;/a&gt;, &lt;a href=&#34;https://jiajunwu.com/&#34;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00926&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/pi-GAN-pytorch&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;inversion method&lt;/h2&gt; &#xA;&lt;p&gt;This part contatins generatal inversion methods, while methods in the next &lt;em&gt;application&lt;/em&gt; part are mainly designed for specific tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Transformer for Image Inversion and Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xueqi Hu, Qiusheng Huang, Zhengyi Shi, Siyuan Li, Changxin Gao, Li Sun, Qingli Li.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.07932&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sapphire497/style-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity GAN Inversion for Image Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://tengfei-wang.github.io&#34;&gt;Tengfei Wang&lt;/a&gt;, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2109.06590&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://tengfei-wang.github.io/HFGI/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Tengfei-Wang/HFGI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperInverter: Improving StyleGAN Inversion via Hypernetwork.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://di-mi-ta.github.io/&#34;&gt;Tan M. Dinh&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/anhttranusc/&#34;&gt;Anh Tuan Tran&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/rangmanhonguyen/&#34;&gt;Rang Nguyen&lt;/a&gt;, &lt;a href=&#34;https://sonhua.github.io/&#34;&gt;Binh-Son Hua&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00719&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://di-mi-ta.github.io/HyperInverter/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.15666&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://yuval-alaluf.github.io/hyperstyle/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/hyperstyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Overparameterization Improves StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yohan Poirier-Ginter, Alexandre Lessard, Ryan Smith, Jean-François Lalonde.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation. [&lt;a href=&#34;https://arxiv.org/abs/2205.06304&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://lvsn.github.io/OverparamStyleGAN/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAlign: Analysis and Applications of Aligned StyleGAN Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.11323&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expanding the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yin Yu, Ghasedi Kamran, Wu HsiangTao, Yang Jiaolong, Tong Xi, Fu Yun.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.12530&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encode-in-Style: Latent-based Video Encoding using StyleGAN2.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://trevineoorloff.github.io/&#34;&gt;Trevine Oorloff&lt;/a&gt;, &lt;a href=&#34;https://www.umiacs.umd.edu/people/yaser&#34;&gt;Yaser Yacoob&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14512&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://trevineoorloff.github.io/Encode-in-Style.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/trevineoorloff/Encode-in-Style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-fidelity GAN Inversion with Padding Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ezioby.github.io/padinv/&#34;&gt;Qingyan Bai&lt;/a&gt;, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, Yujun Shen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11105&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with NerfGANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Wen-Sheng Chu, Abhishek Kumar, Dmitry Lagun, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.09061&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Third Time&#39;s the Charm? Image and Video Editing with StyleGAN3.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, Zongze Wu, Asif Zamir, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.13433&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/stylegan3-editing/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/stylegan3-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature-Style Encoder for Style-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.02183&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Min Jin Chong, Hsin-Ying Lee, David Forsyth.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.01619&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mchong6/SOAT&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real Image Inversion via Segments.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Futschik, Michal Lukáč, Eli Shechtman, Daniel Sýkora.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.06269&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic and Geometric Unfolding of StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mustafa Shukor, Xu Yao, Bharath Bhushan Damodaran, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.04481&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pivotal Tuning for Latent-based Editing of Real Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2106.05744.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Heyi Li, Jinlong Liu, Yunzhi Bai, Huayan Wang, Klaus Mueller.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.14230&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AnonSubm2021/TransStyleGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, Nenghai Yu.&lt;/em&gt;&lt;br&gt; TIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.07661&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wty-ustc.github.io/inversion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/StyleGAN-Inversion-Baseline&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oğuz Kaan Yüksel, &lt;a href=&#34;https://enis.dev&#34;&gt;Enis Simsar&lt;/a&gt;, Ezgi Gülperi Er, Pinar Yanardag.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.00820&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/latentclr&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-Control: Explicitly Controllable GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard Medioni.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.02477&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved StyleGAN Embedding: Where are the Good Latents?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/ZPdesu&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/RameenAbdal&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=ojgWPpgAAAAJ&amp;amp;hl=en&#34;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=rS1xJIIAAAAJ&amp;amp;hl=en&#34;&gt;John Femiani&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2012.09036&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZPdesu/II2S&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a Deep Reinforcement Learning Policy Over the Latent Space of a Pre-trained GAN for Semantic Age Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kumar Shubham, Gopalakrishnan Venkatesh, Reijul Sachdev, Akshi, Dinesh Babu Jayagopi, G. Srinivasaraghavan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.00954&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lifting 2D StyleGAN for 3D-Aware Face Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://seasonsh.github.io/&#34;&gt;Yichun Shi&lt;/a&gt;, Divyansh Aggarwal, &lt;a href=&#34;http://www.cse.msu.edu/~jain/&#34;&gt;Anil K. Jain&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13126&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Davis Wertheimer, Omid Poursaeed, Bharath Hariharan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13026&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Disentangled Manifolds in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. &lt;a href=&#34;https://arxiv.org/abs/2011.11842&#34;&gt;[PDF]&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Learning for Faster StyleGAN Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shanyan Guan, &lt;a href=&#34;https://tyshiwo.github.io/&#34;&gt;Ying Tai&lt;/a&gt;, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.01758&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Inversion and Generation Diversity in StyleGAN using a Gaussianized Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.06529&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explaining in Style: Training a GAN to explain a classifier in StyleSpace.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.13369&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://explaining-in-style.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BDInvert: GAN Inversion for Out-of-Range Images with Geometric Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kkang831.github.io/&#34;&gt;Kyoungkook Kang&lt;/a&gt;, Seongtae Kim, &lt;a href=&#34;https://www.scho.pe.kr/&#34;&gt;Sunghyun Cho&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.08998&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kkang831.github.io/publication/ICCV_2021_BDInvert/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Continuity to Editability: Inverting GANs with Consecutive Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://qingyang-xu.github.io/&#34;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&#34;https://www.csyongdu.com/&#34;&gt;Yong Du&lt;/a&gt;, Wenpeng Xiao, Xuemiao Xu and &lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/hengfenghe.com&#34;&gt;Shengfeng He&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.13812&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yuval-alaluf.github.io/&#34;&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02699&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/restyle-encoder/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/restyle-encoder&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disentangled Face Attribute Editing via Instance-Aware Latent Space Search.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxuan Han, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, Ying Fu.&lt;/em&gt;&lt;br&gt; IJCAI 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.12660&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yxuhan/IALS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prior Image-Constrained Reconstruction using Style-Based Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Varun A Kelkar, Mark Anastasio.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2102.12525.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intermediate Layer Optimization for Inverse Problems using Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Joseph Dean, Ajil Jalal, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.07364&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/giannisdaras/ilo&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Latent Space Regression to Analyze and Leverage Compositionality in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/lrchai/&#34;&gt;Lucy Chai&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, &lt;a href=&#34;http://web.mit.edu/phillipi/&#34;&gt;Phillip Isola&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=sjuuTm4vj0&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/latent-composition&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/latent-composition/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Peiye Zhuang, Oluwasanmi Koyejo, Alexander G. Schwing.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.01187&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Qianfen Jiao, Sheng Qian, Si Wu, Hau-San Wong.&lt;/em&gt;&lt;br&gt; AAAI 2021. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17017&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling with Deep Generative Views.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14551&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/gan-ensembling&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/gan-ensembling/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Navigating the GAN Parameter Space for Semantic Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anton Cherepkov, Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.13786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yandex-research/navigan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Dani Lischinski, Eli Shechtman.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). &lt;a href=&#34;https://arxiv.org/abs/2011.12799&#34;&gt;[PDF]&lt;/a&gt; [&lt;a href=&#34;https://github.com/betterze/StyleSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.00951&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/eladrich.github.io/pixel2style2pixel/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GHFeat: Generative Hierarchical Features from Synthesizing Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2007.10379.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/ghfeat&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/ghfeat/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hui-Po Wang, Ning Yu, Mario Fritz.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.14107&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;e4e: Designing an Encoder for StyleGAN Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Omer Tov&lt;/a&gt;, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02766&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.02401&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/StyleFlow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask-Guided Discovery of Semantic Manifolds in Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://mengyu.page/&#34;&gt;Mengyu Yang&lt;/a&gt;, &lt;a href=&#34;https://www.cdtps.utoronto.ca/people/directories/all-faculty/david-rokeby&#34;&gt;David Rokeby&lt;/a&gt;, &lt;a href=&#34;https://wxs.ca/&#34;&gt;Xavier Snelgrove&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop on Machine Learning for Creativity and Design. [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold/raw/main/masked-gan-manifold.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~amberman/&#34;&gt;Amit Bermano&lt;/a&gt;, &lt;a href=&#34;https://yangyan.li/&#34;&gt;Yangyan Li&lt;/a&gt;, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; SIGGRAPH ASIA 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/ID-disentanglement/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PIE: Portrait Image Embedding for Semantic Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.mpi-inf.mpg.de/~atewari/&#34;&gt;A. Tewari&lt;/a&gt;, M. Elgharib, M. BR, F. Bernard, H-P. Seidel, P. P‌érez, M. Zollhöfer, C.Theobalt.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/data/paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the Role of Individual Units in a Deep Neural Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; National Academy of Sciences 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.05041&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/dissect/&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://dissect.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming and Projecting Images into Class-conditional Generative Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://minyounghuh.com/&#34;&gt;Minyoung Huh&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/sparis/&#34;&gt;Sylvain Paris&lt;/a&gt;, &lt;a href=&#34;https://www.dgp.toronto.edu/~hertzman/&#34;&gt;Aaron Hertzmann&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;http://arxiv.org/abs/2005.01703&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/minyoungg/GAN-Transform-and-Project&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://minyoungg.github.io/GAN-Transform-and-Project/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&#34;https://jjthiagarajan.com/&#34;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/kailkhura1&#34;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/bremer5&#34;&gt;Timo Bremer&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; IJCV 2020. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-020-01310-5&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rewriting a Deep Generative Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.15646&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/rewriting&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2 Distillation for Feed-forward Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03581&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvgenyKashin/stylegan2-distillation&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In-Domain GAN Inversion for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.00049&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/idinvert/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/idinvert&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13659&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/deep-generative-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.12287&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Disentangling Invertible Interpretation Network for Explaining Latent Representations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Björn Ommer.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.13166&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/iin/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/iin&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing in Style: Uncovering the Local Semantics of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Edo Collins, Raja Bala, Bob Price, Sabine Süsstrunk.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.14367&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IVRL/GANLocalEditing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Processing Using Multi-Code GAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.07116&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/mganprior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/mganprior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN++: How to Edit the Embedded Images?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.11544&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Photo Manipulation with a Generative Image Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Hendrik Strobelt, William Peebles, Jonas, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; TOG 2019. [&lt;a href=&#34;https://arxiv.org/abs/2005.07727&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1904.03189&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/image2styleganv1-v2&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-based Projector for Faster Recovery with Convergence Guarantees in Linear Inverse Problems.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ankit Raj, Yuqi Li, Yoram Bresler.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1902.09698&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting Layers of a Large Generator.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_18.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detecting Overfitting in Deep Generators via Latent Recovery.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Webster_Detecting_Overfitting_of_Deep_Generative_Networks_via_Latent_Recovery_CVPR_2019_paper.pdf&#34;&gt;PDF&lt;/a&gt;][&lt;a href=&#34;https://colab.research.google.com/drive/1N6zP4xlPunWOkmakcl0mamfhq946nMLB?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network (II).&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil A Bharath.&lt;/em&gt;&lt;br&gt; TNNLS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1802.05701&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ToniCreswell/InvertingGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Invertibility of Convolutional Generative Networks from Partial Measurements.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fangchang Ma, Ulas Ayaz, Sertac Karaman.&lt;/em&gt;&lt;br&gt; NeurIPS 2018. [&lt;a href=&#34;https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fangchangma/invert-generative-networks&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Metrics for Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, Patrick van der Smagt.&lt;/em&gt;&lt;br&gt; AISTATS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1711.01204&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Understanding the Invertibility of Convolutional Neural Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee.&lt;/em&gt;&lt;br&gt; IJCAI 2017. [&lt;a href=&#34;https://arxiv.org/abs/1705.08664&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Network to Solve Them All - Solving Linear Inverse Problems using Deep Projection Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan.&lt;/em&gt;&lt;br&gt; ICCV 2017. [&lt;a href=&#34;https://arxiv.org/abs/1703.09912&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Precise Recovery of Latent Vectors from Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zachary C. Lipton, Subarna Tripathi.&lt;/em&gt;&lt;br&gt; ICLR 2017 workshop. [&lt;a href=&#34;https://arxiv.org/abs/1702.04782&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SubarnaTripathi/ReverseGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil Anthony Bharath.&lt;/em&gt;&lt;br&gt; NeurIPS 2016 Workshop. [&lt;a href=&#34;https://arxiv.org/abs/1611.05644&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Visual Manipulation on the Natural Image Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A. Efros.&lt;/em&gt;&lt;br&gt; ECCV 2016. [&lt;a href=&#34;https://arxiv.org/abs/1609.03552v2&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;3D GANs inverson&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D GAN Inversion for Controllable Portrait Image Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, David B. Lindell, Eric R. Chan, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13441&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.computationalimaging.org/publications/3dganinversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2NeRF: Unsupervised Conditional π-GAN for Single Image to Neural Radiance Fields Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.13162&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;latent space navigation&lt;/h2&gt; &#xA;&lt;p&gt;Inversion is not the ultimate goal. The reason that we invert a real image into the latent space of a trained GAN model is that we can manipulate the inverted image in the latent space by discovering the desired code with certain attributes. This technique is usually known as latent space navigation, GAN steerability, latent code manipulation, or other names in the literature. Although often regarded as an independent research field, it acts as an indispensable component of GAN inversion for manipulation. Many inversion methods also involve efficient discovery of a desired latent code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://enis.dev/&#34;&gt;Enis Simsar&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Umut Kocasari&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Ezgi Gülperi Er&lt;/a&gt;, &lt;a href=&#34;https://pinguar.org/&#34;&gt;Pinar Yanardag&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08516&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/fantasticstyles/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/styleatlas/classes/FFHQ/&#34;&gt;Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region-Based Semantic Factorization in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, Qifeng Chen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09649&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-level Latent Space Structuring for Generative Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://orenkatzir.github.io/&#34;&gt;Oren Katzir&lt;/a&gt;, Vicky Perepelook, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05910&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rayleigh EigenDirections (REDs): GAN Latent Space Traversals for Multidimensional Features.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guha Balakrishnan, Raghudeep Gadde, Aleix Martinez, Pietro Perona.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2201.10423.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Latent Space Directions For GAN-based Local Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ehsan Pajouheshgar, Tong Zhang, Sabine Süsstrunk.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.12583&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.10132&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor Component Analysis for Interpreting the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://james-oldfield.github.io/&#34;&gt;James Oldfield&lt;/a&gt;, Markos Georgopoulos, Yannis Panagakis, Mihalis A. Nicolaou, Ioannis Patras.&lt;/em&gt;&lt;br&gt; BMVC 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.11736&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://eecs.qmul.ac.uk/~jo001/TCA-latent-space/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/james-oldfield/TCA-latent-space&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Subspace Factorization for StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rene Haas, Stella Graßhof and Sami S. Brandt.&lt;/em&gt;&lt;br&gt; FG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.04554&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Emotion Editing in the StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;René Haas, Stella Graßhof, Sami S. Brandt.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation Workshop. [&lt;a href=&#34;https://arxiv.org/pdf/2205.06102.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LARGE: Latent-Based Regression through GAN Semantics.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=iLLlWr8AAAAJ&#34;&gt;Ofir Brenner&lt;/a&gt;, &lt;a href=&#34;https://danielcohenor.com/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.11186&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/LARGE&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/LARGE&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Image Animator: Learning to Animate Image via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yaohui Wang, Di Yang, Francois Bremond, Antitza Dantcheva.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=7r6kDq0mK_&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wyhsirius.github.io/LIA-project&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhsirius/LIA&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFusion: Disentangling Spatial Segments in StyleGAN-Generated Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.07437&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OmerKafri/StyleFusion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaewoong Choi, Changyeon Yoon, Junho Lee, Jung Ho Park, Geonho Hwang, Myungjoo Kang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.06959&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploratory Search of GANs with Contextual Bandits.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ivan Kropotov, Alan Medlar, Dorota Glowacka.&lt;/em&gt;&lt;br&gt; CIKM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3459637.3482103&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LowRankGAN: Low-Rank Subspaces in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, Qifeng Chen.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.04488&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhujiapeng/LowRankGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable and Compositional Generation with Latent-Space Energy-Based Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weili Nie, Arash Vahdat, Anima Anandkumar.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.10873&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Latent Transformer for Disentangled Face Editing in Images and Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Yao_A_Latent_Transformer_for_Disentangled_Face_Editing_in_Images_and_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2106.11895&#34;&gt;ArXiV&lt;/a&gt;] [&lt;a href=&#34;https://github.com/InterDigitalInc/latent-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Toward a Visual Concept Vocabulary for GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Schwettmann_Toward_a_Visual_Concept_Vocabulary_for_GAN_Latent_Space_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://visualvocab.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WarpedGANSpace: Finding Non-linear RBF Paths in GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Christos Tzelepis, Georgios Tzimiropoulos, Ioannis Patras.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.13357&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chi0tzp/WarpedGANSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Transformations via NeuralODEs for GAN-based Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Valentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KhrulkovV/nonlinear-image-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OroJaR: Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxiang Wei, Yupeng Shi, Xiao Liu, Zhilong Ji, Yuan Gao, Zhongqin Wu, Wangmeng Zuo.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.07668&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/csyxwei/OroJaR&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EigenGAN: Layer-Wise Eigen-Learning for GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhenliang He, Meina Kan, Shiguang Shan.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.12476&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LynnHo/EigenGAN-Tensorflow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SalS-GAN: Spatially-Adaptive Latent Space in StyleGAN for Real Image Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lingyun Zhang, Xiuxiu Bai, Yao Gao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475633&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Yi Liu, Xiwen Wei, Yang Zhang, Si Wu, Yong Xu, Hau San Wong.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475293&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Interpretable Latent Space Directions of GANs Beyond Binary Attributes.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiting Yang, Liangyu Chai, Qiang Wen, Shuang Zhao, Zixun Sun, Shengfeng He.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Discovering_Interpretable_Latent_Space_Directions_of_GANs_Beyond_Binary_Attributes_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Surrogate Gradient Field for Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Minjun Li, Yanghua Jin, Huachun Zhu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;http://arxiv.org/abs/2104.09065&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SeFa: Closed-Form Factorization of Latent Semantics in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2007.06600&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/sefa&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/sefa/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L2M-GAN: Learning To Manipulate Latent Space Semantics for Facial Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guoxing Yang, Nanyi Fei, Mingyu Ding, Guangzhen Liu, Zhiwu Lu, Tao Xiang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Yang_L2M-GAN_Learning_To_Manipulate_Latent_Space_Semantics_for_Facial_Attribute_CVPR_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songquanpeng/L2M-GAN&#34;&gt;Unofficial Pytorch&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoCoGAN-HD: A Good Image Generator Is What You Need for High-Resolution Video Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=6puCSjH3hwA&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/snap-research/MoCoGAN-HD&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Steerability without optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nurit Spingarn-Eliezer, Ron Banner, Tomer Michaeli.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/forum?id=zDy_nQCXiIj&#34;&gt;OpenReview&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2012.05328&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the &#34;steerability&#34; of generative adversarial networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Jahanian, Lucy Chai, Phillip Isola.&lt;/em&gt;&lt;br&gt; ICLR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.07171&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ali-design.github.io/gan_steerability/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GANSpace: Discovering Interpretable GAN Controls.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.02546&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/harskish/ganspace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs for Semantic Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://shenyujun.github.io/&#34;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, &lt;a href=&#34;http://www.ie.cuhk.edu.hk/people/xotang.shtml&#34;&gt;Xiaoou Tang&lt;/a&gt;, &lt;a href=&#34;http://bzhou.ie.cuhk.edu.hk/&#34;&gt;Bolei Zhou&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.10786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/interfacegan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/interfacegan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing What a GAN Cannot Generate.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.11626&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://ganseeing.csail.mit.edu/&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Interpretable Directions in the GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICML 2020. [&lt;a href=&#34;https://arxiv.org/abs/2002.03754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/anvoynov/GANLatentDiscovery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;application&lt;/h2&gt; &#xA;&lt;h3&gt;image and video generation and manipulation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Video Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Jihyun Bae, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.09273&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEAT: Face Editing with Attention.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xianxu Hou, Linlin Shen, Or Patashnik, Daniel Cohen-Or, Hui Huang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2202.02713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Bingchuan Li, Shaofei Cai, Wei Liu, Peng Zhang, Miao Hua, Qian He, Zili Yi.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.10737&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/phycvgan/DyStyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Semantic Image Synthesis Using StyleGAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuki Endo, Yoshihiro Kanamori.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14877&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/endo-yuki-t/Fewshot-SMIS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Heredity-aware Child Face Image Generation with Latent Space Disentanglement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiao Cui, Wengang Zhou, Yang Hu, Weilun Wang, Houqiang Li.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.11080&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image Transformation Learning via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kaiwen Zha, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.07751&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/trgan&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jialu Huang, Jing Liao, Sam Kwong.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2010.05713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity-Guided Face Generation with Multi-modal Contour Conditions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Qingyan Bai, Weihao Xia, Fei Yin, Yujiu Yang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.04854&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Wonseok Roh, Wonmin Byeon, Sang Ho Yoon, Chan Young Kim, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00007&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairMapper: Removing Hair from Portraits Using GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yong-Liang Yang&lt;/a&gt;, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Jin&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/HairMapper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/cvpr2022.htm&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ/raw/main&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ&#34;&gt;Non-hair-FFHQ Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attribute Group Editing for Reliable Few-shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanqi Ding, Xinzhe Han, Shuhui Wang, Shuzhe Wu, Xin Jin, Dandan Tu, Qingming Huang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08422&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UniBester/AGE&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InsetGAN for Full-Body Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://afruehstueck.github.io/&#34;&gt;Anna Frühstück&lt;/a&gt;, &lt;a href=&#34;http://krsingh.cs.ucdavis.edu/&#34;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/niloy-mitra/&#34;&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/jingwan-lu/&#34;&gt;Jingwan Lu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07200&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://afruehstueck.github.io/insetgan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/stylegan-human/StyleGAN-Human/raw/main/insetgan.py&#34;&gt;Unofficial&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaceEdit: Learning a Unified Editing Space for Open-Domain Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.cs.rochester.edu/u/jshi31/&#34;&gt;Jing Shi&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/ningxu/&#34;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/u/hzheng15/haitian_homepage/index.html&#34;&gt;Haitian Zheng&lt;/a&gt;, Alex Smith, &lt;a href=&#34;https://www.cs.rochester.edu/u/jluo/&#34;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/~cxu22/&#34;&gt;Chenliang Xu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00180&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In&amp;amp;Out: Diverse Image Outpainting via GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.00675&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/InOut/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InfinityGAN: Towards Infinite-Resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.03963&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://hubert0527.github.io/infinityGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Siavash Khodadadeh, Shabnam Ghadar, Saeid Motiian, Wei-An Lin, Ladislau Bölöni, Ratheesh Kalarot.&lt;/em&gt;&lt;br&gt; WACV 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2022/html/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.mpi-inf.mpg.de/~gfox/&#34;&gt;Gereon Fox&lt;/a&gt;, &lt;a href=&#34;https://www.mpi-inf.mpg.de/~atewari/&#34;&gt;Ayush Tewari&lt;/a&gt;, Mohamed Elgharib, &lt;a href=&#34;http://gvv.mpi-inf.mpg.de/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; BMVC 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2107.07224&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constrained Graphic Layout Generation via Latent Optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00871&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ktrk115/const_layout&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wongjong Jang, Gwangjin Ju, &lt;a href=&#34;https://ycjung.info/&#34;&gt;Yucheol Jung&lt;/a&gt;, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/xtong/&#34;&gt;Xin Tong&lt;/a&gt;, &lt;a href=&#34;http://phome.postech.ac.kr/~leesy/&#34;&gt;Seungyong Lee&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.04331.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PeterZhouSZ/StyleCariGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coarse-to-Fine: Facial Structure Editing of Portrait Images via Latent Space Classifications.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yongliang Yang&lt;/a&gt;, Qinjie Xiao, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Ji&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/paper46.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/sig2021.htm&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAM: Only a Matter of Style-Age Transformation Using a Style-Based Regression Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/SAM&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Adversarial Fake Images on Face Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dongze Li, Wei Wang, Hongxing Fan, Jing Dong.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.03272&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://sites.google.com/view/mafifi&#34;&gt;Mahmoud Afifi&lt;/a&gt;, Marcus A. Brubaker, Michael S. Brown.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.11731&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mahmoudnafifi/HistoGAN&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://ln2.sync.com/dl/1891becc0/uhsxtprq-33wfwmyq-dhhqeb3s-mtstuqw7/view/default/11118541390008&#34;&gt;4K Landscape&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Shot Face Swapping on Megapixels.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuhao Zhu, Qi Li, Jian Wang, Chengzhong Xu, Zhenan Sun.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2105.04932.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LOHO: Latent Optimization of Hairstyles via Orthogonalization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.03891&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dukebw/LOHO&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/blandocs&#34;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&#34;https://yunjey.github.io/&#34;&gt;Yunjey Choi&lt;/a&gt;, &lt;a href=&#34;https://github.com/taki0112&#34;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&#34;http://cmalab.snu.ac.kr/&#34;&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href=&#34;https://github.com/youngjung&#34;&gt;Youngjung Uh&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naver-ai/StyleMapGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;yaxing wang, Lu Yu, Joost van de Weijer.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.05867&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yaxingwang/DeepI2I&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;multimodal learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://haggaim.github.io/&#34;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/gal-chechik&#34;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00946&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylegan-nada.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/StyleGAN-nada&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2103.10951&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CI-GAN: Cycle-Consistent Inverse GAN for Text-to-Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.01361&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Image Generation and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.03308&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/Multi-Modal-CelebA-HQ&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/TediGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepLandscape: Adversarial Modeling of Landscape Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;E. Logacheva, R. Suvorov, O. Khomenko, A. Mashikhin, and V. Lempitsky.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680256.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/saic-mdal/deep-landscape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://saic-mdal.github.io/deep-landscape/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqun Mei, Pengfei Guo, Vishal M. Patel.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2203.16669&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jinsong Chen, Hu Han, Shiguang Shan.&lt;/em&gt;&lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-2208.ChenJ.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Time-Travel Rephotography.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~cecilia77/&#34;&gt;Xuaner Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/paul-yoo-768a3715b&#34;&gt;Paul Yoo&lt;/a&gt;, &lt;a href=&#34;http://www.ricardomartinbrualla.com/&#34;&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href=&#34;http://jasonlawrence.info/&#34;&gt;Jason Lawrence&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~seitz/&#34;&gt;Steven M. Seitz&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2021 (TOG). [&lt;a href=&#34;https://arxiv.org/abs/2012.12261&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Time-Travel-Rephotography/Time-Travel-Rephotography.github.io&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPEN: GAN Prior Embedded Network for Blind Face Restoration in the Wild.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_GAN_Prior_Embedded_Network_for_Blind_Face_Restoration_in_the_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ckkelvinchan.github.io/&#34;&gt;Kelvin C.K. Chan&lt;/a&gt;, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00739&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ckkelvinchan.github.io/projects/GLEAN&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ckkelvinchan/GLEAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=KjQLROoAAAAJ&#34;&gt;Honglun Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.04061&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03808&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adamian98/pulse&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTT-GAN: Looking Through Turbulence by Inverting GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kfmei.page/&#34;&gt;Kangfu Mei&lt;/a&gt;, &lt;a href=&#34;https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/&#34;&gt;Vishal M. Patel&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.02379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/LTT-GAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Generator Inversion for Image Enhancement and Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.cs.huji.ac.il/~avivga&#34;&gt;Aviv Gabbay&lt;/a&gt;, &lt;a href=&#34;http://www.cs.huji.ac.il/~ydidh&#34;&gt;Yedid Hoshen&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1906.11880&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.vision.huji.ac.il/style-image-prior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/avivga/style-image-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image understanding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir Navab.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.12518.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://segmentation-in-style.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/warmspringwinds/segmentation_in_style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finding an Unsupervised Image Segmenter in each of your Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=Ug-bgjgSlKV&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Labels4Free: Unsupervised Segmentation using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=kEQimk0AAAAJ&amp;amp;hl=en&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Gn8URq0AAAAJ&amp;amp;hl=en&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/n.mitra/&#34;&gt;Niloy Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14968&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/Labels4Free&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/Labels4Free&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.alexyuxuanzhang.com/&#34;&gt;Yuxuan Zhang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~linghuan/&#34;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&#34;https://kangxue.org/&#34;&gt;Kangxue Yin&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Jean-Francois Lafleche&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Adela Barriuso&lt;/a&gt;, &lt;a href=&#34;https://groups.csail.mit.edu/vision/torralbalab/&#34;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&#34;http://www.cs.utoronto.ca/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.06490&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nv-tlabs/datasetGAN_release&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/datasetGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repurposing GANs for One-shot Semantic Part Segmentation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nontawat Tritrong, Pitchaporn Rewatbowornwong, &lt;a href=&#34;https://www.supasorn.com/&#34;&gt;Supasorn Suwajanakorn&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2103.04379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://repurposegans.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bryandlee/repurpose-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StylePart: Image-based Shape Part Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;I-Chao Shen, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.10520&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://jitengmu.github.io/&#34;&gt;Jiteng Mu&lt;/a&gt;, Shalini De Mello, Zhiding Yu, Nuno Vasconcelos, Xiaolong Wang, Jan Kautz, Sifei Liu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.16521&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://jitengmu.github.io/CoordGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN2Shape: Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xingangpan.github.io/&#34;&gt;Xingang Pan&lt;/a&gt;, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ICLR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2011.00844&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/GAN2Shape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://xingangpan.github.io/projects/GAN2Shape.html&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, &lt;a href=&#34;https://qingguo-xu.com/&#34;&gt;Qingguo Xu&lt;/a&gt;, Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.11423&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OSTeC: One-Shot Texture Completion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Baris Gecer, Jiankang Deng, Stefanos Zafeiriou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.15370&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/barisgecer/OSTeC&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;compressed sensing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generator Surgery for Compressed Sensing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu, Jan-Willem van de Meent, Paul Hand.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop Deep Inverse. [&lt;a href=&#34;https://arxiv.org/abs/2102.11163&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nik-sm/generator-surgery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Task-Aware Compressed Sensing with Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maya Kabkab, Pouya Samangouei, Rama Chellappa.&lt;/em&gt;&lt;br&gt; AAAI 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1802.01284.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;medical imaging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Medical Image Generation via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhihang Ren, Stella X. Yu, David Whitney.&lt;/em&gt;&lt;br&gt; Human Vision and Electronic Imaging 2021. [&lt;a href=&#34;https://whitneylab.berkeley.edu/PDFs/Ren_MedImageGen.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-resolution Controllable Prostatic Histology Synthesis using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gagandeep B. Daroach, Josiah A. Yoder, Kenneth A. Iczkowski, Peter S. LaViolette.&lt;/em&gt;&lt;br&gt; BIOIMAGING 2021. [&lt;a href=&#34;https://www.scitepress.org/Papers/2021/103939/103939.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;security&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Imaging via Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Li, Chris Clifton.&lt;/em&gt;&lt;br&gt; IEEE Symposium on Security &amp;amp; Privacy (S&amp;amp;P) 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.05472&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for the constructive comments from anonymous reviewers and feedback from &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/anvoynov&#34;&gt;Andrey Voynov&lt;/a&gt;, and &lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>