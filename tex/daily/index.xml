<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-01T01:39:18Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ilaria-manco/multimodal-ml-music</title>
    <updated>2023-01-01T01:39:18Z</updated>
    <id>tag:github.com,2023-01-01:/ilaria-manco/multimodal-ml-music</id>
    <link href="https://github.com/ilaria-manco/multimodal-ml-music" rel="alternate"></link>
    <summary type="html">&lt;p&gt;List of academic resources on Multimodal ML for Music&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Multimodal Machine Learning for Music (MML4Music) &lt;a href=&#34;https://awesome.re&#34;&gt;&lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains a curated list of academic papers, datasets and other resources on multimodal machine learning (MML) research applied to music. By &lt;a href=&#34;http://ilariamanco.com/&#34;&gt;Ilaria Manco&lt;/a&gt; (&lt;a href=&#34;mailto:i.manco@qmul.ac.uk&#34;&gt;i.manco@qmul.ac.uk&lt;/a&gt;), &lt;a href=&#34;http://c4dm.eecs.qmul.ac.uk/&#34;&gt;Centre for Digital Music&lt;/a&gt;, &lt;a href=&#34;https://www.qmul.ac.uk/&#34;&gt;QMUL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is not meant to be an exhaustive list, as MML for music is a varied and growing field, tackling a wide variety of tasks, from music information retrieval to generation, through many different methods. Since this research area is also not yet well established, conventions and definitions aren&#39;t set in stone and this list aims to provide a point of reference for its ongoing development.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#papers&#34;&gt;Academic Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#survey-papers&#34;&gt;Survey Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#journal-and-conference-papers&#34;&gt;Journal and Conference Papers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#workshops-tutorials-&amp;amp;-talks&#34;&gt;Workshops, Tutorials &amp;amp; Talks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#other-projects&#34;&gt;Other Projects&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#statistics-&amp;amp;-visualisations&#34;&gt;Statistics &amp;amp; Visualisations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#how-to-contribute&#34;&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#other-resources&#34;&gt;Other Resources&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;Survey Papers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.05347.pdf&#34;&gt;Multimodal music information processing and retrieval: Survey and future challenges&lt;/a&gt; (F. Simonetta et al., 2019)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.04397.pdf&#34;&gt;Cross-Modal Music Retrieval and Applications: An Overview of Key Methodologies&lt;/a&gt; (M. Muller et al., 2019)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Journal and Conference Papers&lt;/h3&gt; &#xA;&lt;p&gt;Summary of papers on multimodal machine learning for music, including the review papers highlighted &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/#survey-papers&#34;&gt;above&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Audio-Text&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Paper Title&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.11671&#34;&gt;Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.google/pubs/pub51943/&#34;&gt;Conversational Music Retrieval with Synthetic Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.12208&#34;&gt;Contrastive audio-language learning for music&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ilaria-manco/muscall&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04214&#34;&gt;Learning music audio representations via weak language supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ilaria-manco/mulap&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.12415&#34;&gt;Mulan: A joint embedding of music audio and natural language&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.10901v1&#34;&gt;RECAP: Retrieval Augmented Music Captioner&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.04769&#34;&gt;Clap: Learning audio concepts from natural language supervision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/CLAP&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.14558&#34;&gt;Toward Universal Text-to-Music Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SeungHeonDoh/music-text-representation&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.11984&#34;&gt;MusCaps: Generating Captions for Music Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ilaria-manco/muscaps&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.nlp4musa-1.13&#34;&gt;MusicBERT - learning multi-modal representations for music and text&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.nlp4musa-1.14&#34;&gt;Music autotagging as captioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.08976.pdf&#34;&gt;Deep cross-modal correlation learning for audio and lyrics in music retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.07276.pdf&#34;&gt;Music mood detection based on audio and lyrics with deep neural net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://repositori.upf.edu/bitstream/handle/10230/33063/Oramas_ISMIR2016_expl.pdf?sequence=1&amp;amp;isAllowed=y&#34;&gt;Exploring customer reviews for music genre classification and evolutionary studies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2016&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1608.04868.pdf&#34;&gt;Towards Music Captioning: Generating Music Playlist Descriptions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2008&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.426&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Multimodal Music Mood Classification using Audio and Lyrics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Other&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Paper Title&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.16030.pdf&#34;&gt;Multimodal metric learning for tag-based music retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/minzwon/tag-based-music-retrieval&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2021&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.00437&#34;&gt;Enriched music representations with multiple cross-modal contrastive learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/andrebola/contrastive-mir-learning&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9053240&#34;&gt;Large-Scale Weakly-Supervised Content Embeddings for Music Recommendation and Tagging&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Gan_Music_Gesture_for_Visual_Sound_Separation_CVPR_2020_paper.pdf&#34;&gt;Music gesture for visual sound separation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560732.pdf&#34;&gt;Foley music: Learning to generate music from videos&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2020&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.14171.pdf&#34;&gt;Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio and Tags&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xavierfav/ae-w2v-attention&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1908.03744.pdf&#34;&gt;Audio-visual embedding for cross-modal music video retrieval through supervised deep CCA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://archives.ismir.net/ismir2019/paper/000015.pdf&#34;&gt;Query-by-Blending: a Music Exploration System Blending Latent Vector Representations of Lyric Word, Song Audio, and Artist&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1904.00150.pdf&#34;&gt;Learning Affective Correspondence between Music and Image&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.05347.pdf&#34;&gt;Multimodal music information processing and retrieval: Survey and future challenges&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.04397.pdf&#34;&gt;Cross-Modal Music Retrieval and Applications: An Overview of Key Methodologies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.08727.pdf&#34;&gt;Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2019&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/www.gracenote.com&#34;&gt;Query by Video: Cross-Modal Music Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.03160.pdf&#34;&gt;The Sound of Pixels&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hangzhaomit/Sound-of-Pixels&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w49/Qiu_Image_Generation_Associated_CVPR_2018_paper.pdf&#34;&gt;Image generation associated with music data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://transactions.ismir.net/articles/10.5334/tismir.10/&#34;&gt;Multimodal Deep Learning for Music Genre Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fvancesco/music_resnet_classification&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://arxiv.org/abs/1806.01483&#34;&gt;JTAV: Jointly Learning Social Media Content Representation by Fusing Textual, Acoustic, and Visual Features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mengshor/JTAV&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2018&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3206025.3206046&#34;&gt;Cbvmr: content-based video-music retrieval using soft intra-modal structure constraint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/csehong/VM-NET&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3125486.3125492&#34;&gt;A deep multimodal approach for cold-start music recommendation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sergiooramas/tartarus&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.jair.org/index.php/jair/article/view/11101/26292&#34;&gt;Learning neural audio embeddings for grounding semantics in auditory perception&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2017&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://ceur-ws.org/Vol-1905/recsys2017_poster18.pdf&#34;&gt;Music emotion recognition via end-To-end multimodal neural networks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ohadf.com/papers/FriedFiebrink_NIME2013.pdf&#34;&gt;Cross-modal Sound Mapping Using Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2013&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/31911/Fazekas%20Music%20Emotion%20Recognition%202012%20Accepted.pdf;jsessionid=76AE783B989ED4CDBFB8B9C5CE013CE4?sequence=1&#34;&gt;Music emotion recognition: From content- to context-based models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.449.4173&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Musiclef: A benchmark activity in multimodal music information retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2011&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/2072529.2072531&#34;&gt;The need for music information retrieval with user-centered and multimodal strategies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2009&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.swarthmore.edu/~turnbull/Papers/Turnbull_CombineMusicTags_SIGIR09.pdf&#34;&gt;Combining audio content and social context for semantic music discovery&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Modalities&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;&#34;&gt;MARD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal album reviews dataset&lt;/td&gt; &#xA;   &lt;td&gt;Text, Metadata, Audio descriptors&lt;/td&gt; &#xA;   &lt;td&gt;65,566 albums and 263,525 reviews&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www2.ece.rochester.edu/projects/air/projects/URMP.html&#34;&gt;URMP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi-instrument musical pieces of recorded performances&lt;/td&gt; &#xA;   &lt;td&gt;MIDI, Audio, Video&lt;/td&gt; &#xA;   &lt;td&gt;44 pieces (12.5GB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gaurav22verma.github.io/IMAC_Dataset.html&#34;&gt;IMAC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Affective correspondences between images and music&lt;/td&gt; &#xA;   &lt;td&gt;Images, Audio&lt;/td&gt; &#xA;   &lt;td&gt;85,000 images and 3,812 songs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ivyha010/EmoMV&#34;&gt;EmoMV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Affective Music-Video Correspondence&lt;/td&gt; &#xA;   &lt;td&gt;Audio, Video&lt;/td&gt; &#xA;   &lt;td&gt;5986 pairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Workshops, Tutorials &amp;amp; Talks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/nlp4musa&#34;&gt;First Workshop on NLP for Music and Audio&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Song Describer: a Platform for Collecting Textual Descriptions of Music Recordings - [&lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/song-describer.streamlit.app&#34;&gt;link&lt;/a&gt;] | [&lt;a href=&#34;https://ismir2022program.ismir.net/lbd_405.html&#34;&gt;paper&lt;/a&gt;] | [&lt;a href=&#34;https://github.com/ilaria-manco/song-describer&#34;&gt;code&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Statistics &amp;amp; Visualisations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;42 papers referenced. See the details in &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/multimodal_ml_music.bib&#34;&gt;multimodal_ml_music.bib&lt;/a&gt;. Number of articles per year: &lt;img src=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/fig/articles_per_year.png&#34; alt=&#34;Number of articles per year&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are applying multimodal ML to music, there are &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/authors.md&#34;&gt;139 other researchers&lt;/a&gt; in your field.&lt;/li&gt; &#xA; &lt;li&gt;10 tasks investigated. See the list of &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/tasks.md&#34;&gt;tasks&lt;/a&gt;. Tasks pie chart: &lt;img src=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/fig/pie_chart_task.png&#34; alt=&#34;Tasks pie chart&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;Only 13 articles (30%) provide their source code. by &lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Yann Bayle&lt;/a&gt; has a very useful list of &lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music/raw/master/reproducibility.md&#34;&gt;resources on reproducibility for MIR and ML&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How To Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/contributing.md&#34;&gt;contributing.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;You are free to copy, modify, and distribute &lt;em&gt;&lt;strong&gt;Multimodal Machine Learning for Music (MML4Music)&lt;/strong&gt;&lt;/em&gt; with attribution under the terms of the MIT license. See the &lt;a href=&#34;https://raw.githubusercontent.com/ilaria-manco/multimodal-ml-music/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details. This project is heavily based on &lt;a href=&#34;https://github.com/ybayle/awesome-deep-learning-music&#34;&gt;Deep Learning for Music&lt;/a&gt; by &lt;a href=&#34;http://yannbayle.fr/english/index.php&#34;&gt;Yann Bayle&lt;/a&gt; and uses other projects. You may refer to them for appropriate license information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ddbeck/readme-checklist&#34;&gt;Readme checklist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pylint.org/&#34;&gt;Pylint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.numpy.org/&#34;&gt;Numpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sciunto-org/python-bibtexparser&#34;&gt;Bibtexparser&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you use the information contained in this repository, please let us know!&lt;/p&gt;</summary>
  </entry>
</feed>