<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-20T01:36:24Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jamesfang8499/physics2</title>
    <updated>2024-03-20T01:36:24Z</updated>
    <id>tag:github.com,2024-03-20:/jamesfang8499/physics2</id>
    <link href="https://github.com/jamesfang8499/physics2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;高中物理甲种本（第二册）重排本&lt;/h1&gt; &#xA;&lt;p&gt;本项目是对高中物理甲种本（第二册）的致敬。虽然该书年代久远（1983—1985年出版，后在上世纪90年代以《高中物理读本》为名再版过），但是内容体系安排比如今的高中教材完整且合理。&lt;/p&gt; &#xA;&lt;p&gt;本重排本是根据网络上找到的此书扫描版电子文档，使用LaTeX制作而成的重排本电子文档（PDF格式）。书中的矢量图片采用电子版教材中的矢量图，或采用Tikz及Tkz-euclide制作而来。其余的点阵图则是来自于扫描版电子文档（限于作者的能力和精力，无法将所有内容均以矢量图全部重绘）。文档当中的电路图基于circuitikz绘制，请使用TeXLive2020以后的版本编译。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;注意：本项目的内容勿用于商业目的。电子版的原教材，可通过如下网址下载：&lt;a href=&#34;https://pan.baidu.com/s/1k2LGR&#34;&gt;https://pan.baidu.com/s/1k2LGR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;补充：高级中学物理（甲种本）第二册教学参考书，包含了各章的教学内容、教学建议、实验指导、习题解答、参考资料等。相关代码在2-ref文件夹中。&lt;/li&gt; &#xA; &lt;li&gt;PS: fig.rar当中存储的是本代码所调用的所有图片，请连同文件夹解压后放在工作目录中。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;目录&lt;/h1&gt; &#xA;&lt;h2&gt;说明&lt;/h2&gt; &#xA;&lt;h2&gt;第一章 分子运动论基础&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;分子运动论的建立&lt;/li&gt; &#xA; &lt;li&gt;物体是由分子组成的&lt;/li&gt; &#xA; &lt;li&gt;布朗运动&lt;/li&gt; &#xA; &lt;li&gt;分子间的相互作用力&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第二章 内能，能的转化和守恒定律&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;物体的内能&lt;/li&gt; &#xA; &lt;li&gt;改变内能的两种方式&lt;/li&gt; &#xA; &lt;li&gt;热功当量&lt;/li&gt; &#xA; &lt;li&gt;能的转化和守恒定律&lt;/li&gt; &#xA; &lt;li&gt;能的转化和守恒定律的建立及其意义&lt;/li&gt; &#xA; &lt;li&gt;能源的利用和开发&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第三章 气体的性质&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;气体的状态和状态参量&lt;/li&gt; &#xA; &lt;li&gt;气体的等温变化玻意耳-马略特定律&lt;/li&gt; &#xA; &lt;li&gt;气体的等容变化查理定律&lt;/li&gt; &#xA; &lt;li&gt;热力学温标&lt;/li&gt; &#xA; &lt;li&gt;理想气体的状态方程&lt;/li&gt; &#xA; &lt;li&gt;克拉珀龙方程&lt;/li&gt; &#xA; &lt;li&gt;气体分子运动的特点&lt;/li&gt; &#xA; &lt;li&gt;气体实验定律的微观解释&lt;/li&gt; &#xA; &lt;li&gt;理想气体的内能&lt;/li&gt; &#xA; &lt;li&gt;理想气体的内能变化&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第四章 固体和液体的性质&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;晶体和非晶体&lt;/li&gt; &#xA; &lt;li&gt;空间点阵&lt;/li&gt; &#xA; &lt;li&gt;液体的微观结构&lt;/li&gt; &#xA; &lt;li&gt;液体的表面现象&lt;/li&gt; &#xA; &lt;li&gt;浸润和不浸润&lt;/li&gt; &#xA; &lt;li&gt;毛细现象&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第五章 物态变化&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;熔解和凝固&lt;/li&gt; &#xA; &lt;li&gt;熔解热&lt;/li&gt; &#xA; &lt;li&gt;蒸发&lt;/li&gt; &#xA; &lt;li&gt;饱和汽与饱和汽压&lt;/li&gt; &#xA; &lt;li&gt;沸腾&lt;/li&gt; &#xA; &lt;li&gt;汽化热&lt;/li&gt; &#xA; &lt;li&gt;气体的液化&lt;/li&gt; &#xA; &lt;li&gt;空气的湿度&lt;/li&gt; &#xA; &lt;li&gt;露点湿度计&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第六章 电场&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;两种电荷电荷守恒定律&lt;/li&gt; &#xA; &lt;li&gt;库仑定律&lt;/li&gt; &#xA; &lt;li&gt;电场电场强度&lt;/li&gt; &#xA; &lt;li&gt;电力线&lt;/li&gt; &#xA; &lt;li&gt;电场中的导体&lt;/li&gt; &#xA; &lt;li&gt;电势能&lt;/li&gt; &#xA; &lt;li&gt;电势&lt;/li&gt; &#xA; &lt;li&gt;等势面&lt;/li&gt; &#xA; &lt;li&gt;电势差&lt;/li&gt; &#xA; &lt;li&gt;电势差跟电场强度的关系&lt;/li&gt; &#xA; &lt;li&gt;带电粒子在电场中的运动&lt;/li&gt; &#xA; &lt;li&gt;基本电荷的测定：密立根实验&lt;/li&gt; &#xA; &lt;li&gt;电容器电容&lt;/li&gt; &#xA; &lt;li&gt;电容器的连接&lt;/li&gt; &#xA; &lt;li&gt;静电的防止和应用&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第七章 稳恒电流&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;电流&lt;/li&gt; &#xA; &lt;li&gt;欧姆定律&lt;/li&gt; &#xA; &lt;li&gt;电阻定律电阻率&lt;/li&gt; &#xA; &lt;li&gt;电功和电功率&lt;/li&gt; &#xA; &lt;li&gt;焦耳定律&lt;/li&gt; &#xA; &lt;li&gt;串联电路&lt;/li&gt; &#xA; &lt;li&gt;并联电路&lt;/li&gt; &#xA; &lt;li&gt;分压和分流在伏特表和安培表中的应用&lt;/li&gt; &#xA; &lt;li&gt;电路的分析和计算&lt;/li&gt; &#xA; &lt;li&gt;电动势闭合电路的欧姆定律&lt;/li&gt; &#xA; &lt;li&gt;路端电压&lt;/li&gt; &#xA; &lt;li&gt;电池组&lt;/li&gt; &#xA; &lt;li&gt;电阻的测量&lt;/li&gt; &#xA; &lt;li&gt;惠斯通电桥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第八章 物质的导电性&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;金属的导电性&lt;/li&gt; &#xA; &lt;li&gt;液体的导电性&lt;/li&gt; &#xA; &lt;li&gt;法拉第电解定律&lt;/li&gt; &#xA; &lt;li&gt;电子电量的确定&lt;/li&gt; &#xA; &lt;li&gt;气体的导电性&lt;/li&gt; &#xA; &lt;li&gt;几种自激放电现象&lt;/li&gt; &#xA; &lt;li&gt;气体电光源&lt;/li&gt; &#xA; &lt;li&gt;真空中的电流&lt;/li&gt; &#xA; &lt;li&gt;示波管&lt;/li&gt; &#xA; &lt;li&gt;半导体的导电性&lt;/li&gt; &#xA; &lt;li&gt;N 型半导体和P 型半导体&lt;/li&gt; &#xA; &lt;li&gt;PN 结晶体二极管&lt;/li&gt; &#xA; &lt;li&gt;晶体三极管&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;学生实验&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;验证玻意耳-马略特定律&lt;/li&gt; &#xA; &lt;li&gt;验证气体状态方程&lt;/li&gt; &#xA; &lt;li&gt;测定冰的熔解热&lt;/li&gt; &#xA; &lt;li&gt;测定空气的相对湿度&lt;/li&gt; &#xA; &lt;li&gt;电场中等势线的描绘&lt;/li&gt; &#xA; &lt;li&gt;利用电容器放电测电容&lt;/li&gt; &#xA; &lt;li&gt;测定金属的电阻率&lt;/li&gt; &#xA; &lt;li&gt;把电流表改装为伏特表&lt;/li&gt; &#xA; &lt;li&gt;用安培表和伏特表测定电池的电动势和内电阻&lt;/li&gt; &#xA; &lt;li&gt;练习使用万用电表&lt;/li&gt; &#xA; &lt;li&gt;用惠斯通电桥测电阻&lt;/li&gt; &#xA; &lt;li&gt;测定铜的电化当量&lt;/li&gt; &#xA; &lt;li&gt;练习使用示波器&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;课外实验活动&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;观察扩散现象&lt;/li&gt; &#xA; &lt;li&gt;自制冰淇淋&lt;/li&gt; &#xA; &lt;li&gt;人造云雾&lt;/li&gt; &#xA; &lt;li&gt;测定水的汽化热&lt;/li&gt; &#xA; &lt;li&gt;估计水升高的温度&lt;/li&gt; &#xA; &lt;li&gt;用自制的验电器做静电实验&lt;/li&gt; &#xA; &lt;li&gt;自制电池&lt;/li&gt; &#xA; &lt;li&gt;研究电灯泡的电阻&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;常用的热学量和电学量的国际单位制单位&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>AlonzoLeeeooo/awesome-video-generation</title>
    <updated>2024-03-20T01:36:24Z</updated>
    <id>tag:github.com,2024-03-20:/AlonzoLeeeooo/awesome-video-generation</id>
    <link href="https://github.com/AlonzoLeeeooo/awesome-video-generation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of awesome video generation studies.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;A Collection of Video Generation Studies&lt;/h1&gt; &#xA;&lt;p&gt;This GitHub repository summarizes papers and resources related to the video generation task.&lt;/p&gt; &#xA;&lt;p&gt;If you have any suggestions about this repository, please feel free to &lt;a href=&#34;https://github.com/AlonzoLeeeooo/awesome-video-generation/issues/new&#34;&gt;start a new issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/AlonzoLeeeooo/awesome-video-generation/pulls&#34;&gt;pull requests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;&lt;span id=&#34;contents&#34;&gt;Contents&lt;/span&gt;&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#to-do-lists&#34;&gt;To-Do Lists&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#products&#34;&gt;Products&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-to-video-generation&#34;&gt;Text-to-Video Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#survey-papers&#34;&gt;Survey Papers&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#text-year-2021&#34;&gt;Year 2021&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-to-video-generation&#34;&gt;Image-to-Video Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#image-year-2022&#34;&gt;Year 2022&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#video-editing&#34;&gt;Video Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#editing-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#audio-to-video-generation&#34;&gt;Audio-to-Video Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#audio-year-2024&#34;&gt;Year 2024&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#audio-year-2023&#34;&gt;Year 2023&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#qa&#34;&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;To-Do Lists&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Latest Papers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update CVPR 2024 Papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update AAAI 2024 Papers &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update PDFs and References of ⚠️ Papers&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Published Versions of References&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update ICLR 2024 Papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update NeurIPS 2024 Papers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Previously Published Papers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Update Previous CVPR papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ICCV papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ECCV papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous NeurIPS papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ICLR papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous AAAI papers&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Update Previous ACM MM papers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Regular Maintenance of Preprint arXiv Papers and Missed Papers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Products&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Organization&lt;/th&gt; &#xA;   &lt;th&gt;Year&lt;/th&gt; &#xA;   &lt;th&gt;Research Paper&lt;/th&gt; &#xA;   &lt;th&gt;Website&lt;/th&gt; &#xA;   &lt;th&gt;Specialties&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sora&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.midjourney.com/home&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openai.com/sora&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lumiere&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.12945&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lumiere-video.github.io/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VideoPoet&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://sites.research.google/videopoet/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;W.A.I.T&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.06662.pdf&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://walt-video-diffusion.github.io/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gen-2&lt;/td&gt; &#xA;   &lt;td&gt;Runaway&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.runwayml.com/gen2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gen-1&lt;/td&gt; &#xA;   &lt;td&gt;Runaway&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://research.runwayml.com/gen1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Animate Anyone&lt;/td&gt; &#xA;   &lt;td&gt;Alibaba&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.17117.pdf&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://humanaigc.github.io/animate-anyone/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Outfit Anyone&lt;/td&gt; &#xA;   &lt;td&gt;Alibaba&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://outfitanyone.app/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Video&lt;/td&gt; &#xA;   &lt;td&gt;StabilityAI&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.15127.pdf&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.stablevideo.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pixeling&lt;/td&gt; &#xA;   &lt;td&gt;HiDream.ai&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hidreamai.com/#/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DomoAI&lt;/td&gt; &#xA;   &lt;td&gt;DomoAI&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://domoai.app/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Emu&lt;/td&gt; &#xA;   &lt;td&gt;Meta&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10709&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://emu-video.metademolab.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Genmo&lt;/td&gt; &#xA;   &lt;td&gt;Genmo&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.genmo.ai/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NeverEnds&lt;/td&gt; &#xA;   &lt;td&gt;NeverEnds&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://neverends.life/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Moonvalley&lt;/td&gt; &#xA;   &lt;td&gt;Moonvalley&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://moonvalley.ai/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Morph Studio&lt;/td&gt; &#xA;   &lt;td&gt;Morph&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.morphstudio.com/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pika&lt;/td&gt; &#xA;   &lt;td&gt;Pika&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pika.art/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixelDance&lt;/td&gt; &#xA;   &lt;td&gt;ByteDance&lt;/td&gt; &#xA;   &lt;td&gt;2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.10982&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://makepixelsdance.github.io/&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Papers&lt;/h1&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Survey Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;survey-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A Survey on Video Diffusion Models &lt;a href=&#34;&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Text-to-Video Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Vlogger:&lt;/strong&gt;&lt;/em&gt; Make Your Dream A Vlog &lt;a href=&#34;https://arxiv.org/pdf/2401.09414.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/Vlogger&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make Pixels Dance:&lt;/strong&gt;&lt;/em&gt; High-Dynamic Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.10982.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://makepixelsdance.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://makepixelsdance.github.io/demo.html&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VGen:&lt;/strong&gt;&lt;/em&gt; Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.04483&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/VGen&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://higen-t2v.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;GenTron:&lt;/strong&gt;&lt;/em&gt; Delving Deep into Diffusion Transformers for Image and Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2312.04557&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://www.shoufachen.com/gentron_website/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SimDA:&lt;/strong&gt;&lt;/em&gt; Simple Diffusion Adapter for Efficient Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2308.09710.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenHsing/SimDA&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://chenhsing.github.io/SimDA/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MicroCinema:&lt;/strong&gt;&lt;/em&gt; A Divide-and-Conquer Approach for Text-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.18829&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://wangyanhui666.github.io/MicroCinema.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://youtube.com/shorts/H7O-Ku_lqPA&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Generative Rendering:&lt;/strong&gt;&lt;/em&gt; Controllable 4D-Guided Video Generation with 2D Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2312.01409&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://primecai.github.io/generative_rendering/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;PEEKABOO:&lt;/strong&gt;&lt;/em&gt; Interactive Video Generation via Masked-Diffusion &lt;a href=&#34;https://arxiv.org/pdf/2312.07509&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/Peekaboo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://jinga-lala.github.io/projects/Peekaboo/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/anshuln/peekaboo-demo&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;EvalCrafter:&lt;/strong&gt;&lt;/em&gt; Benchmarking and Evaluating Large Video Generation Models &lt;a href=&#34;https://arxiv.org/pdf/2310.11440&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/EvalCrafter/EvalCrafter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://evalcrafter.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;A Recipe for Scaling up Text-to-Video Generation with Text-free Videos &lt;a href=&#34;https://arxiv.org/pdf/2312.15770&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/damo-vilab/i2vgen-xl&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://tf-t2v.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;BIVDiff:&lt;/strong&gt;&lt;/em&gt; A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2312.02813&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://bivdiff.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Mind the Time:&lt;/strong&gt;&lt;/em&gt; Scaled Spatiotemporal Transformers for Text-to-Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2402.14797&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://snap-research.github.io/snapvideo/video_ldm.html&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Animate Anyone:&lt;/strong&gt;&lt;/em&gt; Consistent and Controllable Image-to-video Synthesis for Character Animation &lt;a href=&#34;https://arxiv.org/pdf/2311.17117.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://humanaigc.github.io/animate-anyone/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;⚠️ Simple but Effective Text-to-Video Generation with Grid Diffusion Models &lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;⚠️ Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation &lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;⚠️ DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation &lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VDT:&lt;/strong&gt;&lt;/em&gt; General-purpose Video Diffusion Transformers via Mask Modeling &lt;a href=&#34;https://arxiv.org/pdf/2305.13311.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/RERV/VDT&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vdt-2023.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VersVideo:&lt;/strong&gt;&lt;/em&gt; Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation &lt;a href=&#34;https://openreview.net/pdf?id=K9sVJ17zvB&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Follow Your Pose:&lt;/strong&gt;&lt;/em&gt; Pose-Guided Text-to-Video Generation using Pose-Free Videos &lt;a href=&#34;https://arxiv.org/pdf/2304.01186&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/mayuelala/FollowYourPose&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://follow-your-pose.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;E2HQV:&lt;/strong&gt;&lt;/em&gt; High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning &lt;a href=&#34;https://arxiv.org/pdf/2401.08117&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2310.07697&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/pengbo807/ConditionVideo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://pengbo807.github.io/conditionvideo-website/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;F3-Pruning:&lt;/strong&gt;&lt;/em&gt; A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text to-Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2312.03459&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Lumiere:&lt;/strong&gt;&lt;/em&gt; A Space-Time Diffusion Model for Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2401.12945.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://lumiere-video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Boximator:&lt;/strong&gt;&lt;/em&gt; Generating Rich and Controllable Motions for Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2402.01566.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://boximator.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=reto_TYsYyQ&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;World Model on Million-Length Video And Language With RingAttention &lt;a href=&#34;https://arxiv.org/abs/2402.08268&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/LargeWorldModel/LWM&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://largeworldmodel.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Direct-a-Video:&lt;/strong&gt;&lt;/em&gt; Customized Video Generation with User-Directed Camera Movement and Object Motion &lt;a href=&#34;https://arxiv.org/pdf/2402.03162.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://direct-a-video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;WorldDreamer:&lt;/strong&gt;&lt;/em&gt; Towards General World Models for Video Generation via Predicting Masked Tokens &lt;a href=&#34;https://arxiv.org/pdf/2401.09985&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/JeffWang987/WorldDreamer&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://world-dreamer.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MagicVideo-V2:&lt;/strong&gt;&lt;/em&gt; Multi-Stage High-Aesthetic Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2401.04468.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://magicvideov2.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Latte:&lt;/strong&gt;&lt;/em&gt; Latent Diffusion Transformer for Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2401.03048&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://maxin-cn.github.io/latte_project&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Others&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Sora:&lt;/strong&gt;&lt;/em&gt; Video Generation Models as World Simulators &lt;a href=&#34;https://openai.com/research/video-generation-models-as-world-simulators&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Align your Latents:&lt;/strong&gt;&lt;/em&gt; High-resolution Video Synthesis with Latent Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2304.08818.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://github.com/srpkdyy/VideoLDM&#34;&gt;[Reproduced code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Text2Video-Zero:&lt;/strong&gt;&lt;/em&gt; Text-to-image Diffusion Models are Zero-shot Video Generators &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PAIR/Text2Video-Zero&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://text2video-zero.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Video Probabilistic Diffusion Models in Projected Latent Space &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/sihyun-yu/PVDM&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICCV&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://research.nvidia.com/labs/dir/pyoco/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Gen-1:&lt;/strong&gt;&lt;/em&gt; Structure and Content-guided Video Synthesis with Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://research.runwayml.com/gen1&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NeurIPS&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Video Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2204.03458.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://video-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CogVideo:&lt;/strong&gt;&lt;/em&gt; Large-scale Pretraining for Text-to-video Generation via Transformers &lt;a href=&#34;https://openreview.net/pdf?id=rB6TpjAuSRy&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://models.aminer.cn/cogvideo/&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make-A-Video:&lt;/strong&gt;&lt;/em&gt; Text-to-video Generation without Text-video Data &lt;a href=&#34;https://arxiv.org/pdf/2209.14792.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://makeavideo.studio/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/make-a-video-pytorch&#34;&gt;[Reproduced code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Phenaki:&lt;/strong&gt;&lt;/em&gt; Variable Length Video Generation From Open Domain Textual Description &lt;a href=&#34;https://openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lucidrains/phenaki-pytorch&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AnimateDiff:&lt;/strong&gt;&lt;/em&gt; Animate Your Personalized Text-to-image Diffusion Models without Specific Tuning &lt;a href=&#34;https://openreview.net/pdf?id=Fx2SbBgcte&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://animatediff.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Control-A-Video:&lt;/strong&gt;&lt;/em&gt; Controllable Text-to-video Generation with Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2305.13840.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Weifeng-Chen/control-a-video&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/wf-genius/Control-A-Video&#34;&gt;[Demo]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.13840.pdf&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;ControlVideo:&lt;/strong&gt;&lt;/em&gt; Training-free Controllable Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2305.13077.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/YBYBZhang/ControlVideo&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Imagen Video:&lt;/strong&gt;&lt;/em&gt; High Definition Video Generation with Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2210.02303.pdf&#34;&gt;[paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Latent-Shift:&lt;/strong&gt;&lt;/em&gt; Latent Diffusion with Temporal Shift for Efficient Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2304.08477.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://latent-shift.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;LAVIE:&lt;/strong&gt;&lt;/em&gt; High-quality Video Generation with Cascaded Latent Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2309.15103.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/LaVie&#34;&gt;[code]&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/LaVie-project/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Show-1:&lt;/strong&gt;&lt;/em&gt; Marrying Pixel and Latent Diffusion Models for Text-to-video Generation &lt;a href=&#34;https://showlab.github.io/Show-1/assets/Show-1.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/showlab/Show-1&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://showlab.github.io/Show-1/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Stable Video Diffusion:&lt;/strong&gt;&lt;/em&gt; Scaling Latent Video Diffusion Models to Large Datasets &lt;a href=&#34;https://arxiv.org/pdf/2311.15127.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://stability.ai/news/stable-video-diffusion-open-ai-video-model&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoComposer:&lt;/strong&gt;&lt;/em&gt; Compositional Video Synthesis with Motion Controllability &lt;a href=&#34;https://arxiv.org/pdf/2306.02018.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/videocomposer&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://videocomposer.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoFactory:&lt;/strong&gt;&lt;/em&gt; Swap Attention in Spatiotemporal Diffusions for Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2305.10874.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoGen:&lt;/strong&gt;&lt;/em&gt; A Reference-guided Latent Diffusion Approach for High Definition Text-to-video Generation &lt;a href=&#34;https://arxiv.org/pdf/2309.00398.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://videogen.github.io/VideoGen/&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;InstructVideo:&lt;/strong&gt;&lt;/em&gt; Instructing Video Diffusion Models with Human Feedback &lt;a href=&#34;https://arxiv.org/pdf/2312.12490.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/i2vgen-xl/raw/main/doc/InstructVideo.md&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2312.12490.pdf&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Emu Video:&lt;/strong&gt;&lt;/em&gt; Factorizing Text-to-Video Generation by Explicit Image Conditioning &lt;a href=&#34;https://arxiv.org/pdf/2311.10709.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;SEINE:&lt;/strong&gt;&lt;/em&gt; Short-to-Long Video Diffusion Model for Generative Transition and Prediction &lt;a href=&#34;https://arxiv.org/pdf/2310.20700.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/SEINE&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/SEINE-project/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoLCM:&lt;/strong&gt;&lt;/em&gt; Video Latent Consistency Model &lt;a href=&#34;https://arxiv.org/pdf/2312.09109.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2022&#34;&gt;&lt;strong&gt;Year 2022&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Show_Me_What_and_Tell_Me_How_Video_Synthesis_via_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/MMVID&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/MMVID/raw/main/mm_vox_celeb/README.md&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;text-year-2021&#34;&gt;&lt;strong&gt;Year 2021&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoGPT:&lt;/strong&gt;&lt;/em&gt; Video Generation using VQ-VAE and Transformers &lt;a href=&#34;https://arxiv.org/pdf/2104.10157.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://wilson1yan.github.io/videogpt/index.html&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MagicVideo:&lt;/strong&gt;&lt;/em&gt; Efficient Video Generation With Latent Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2211.11018&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Image-to-Video Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;image-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VideoBooth:&lt;/strong&gt;&lt;/em&gt; Diffusion-based Video Generation with Image Prompts &lt;a href=&#34;https://arxiv.org/pdf/2312.00777&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/VideoBooth&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/VideoBooth-project/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=10DxH1JETzI&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Decouple Content and Motion for Conditional Image-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2311.14294&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;I2V-Adapter:&lt;/strong&gt;&lt;/em&gt; A General Image-to-Video Adapter for Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2312.16693.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/I2V-Adapter/I2V-Adapter-repo&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Follow-Your-Click:&lt;/strong&gt;&lt;/em&gt; Open-domain Regional Image Animation via Short Prompts &lt;a href=&#34;https://arxiv.org/pdf/2403.08268&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/mayuelala/FollowYourClick&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://follow-your-click.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;AtomoVideo:&lt;/strong&gt;&lt;/em&gt; High Fidelity Image-to-Video Generation &lt;a href=&#34;https://arxiv.org/pdf/2403.01800.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://atomo-video.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/embed/36JIlk-U-vQ&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;image-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Conditional Image-to-Video Generation with Latent Flow Diffusion Models &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/nihaomiao/CVPR23_LFDM&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;I2VGen-XL:&lt;/strong&gt;&lt;/em&gt; High-quality Image-to-video Synthesis via Cascaded Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2311.04145.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/ali-vilab/i2vgen-xl&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DreamVideo:&lt;/strong&gt;&lt;/em&gt; High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance &lt;a href=&#34;https://arxiv.org/pdf/2312.03018&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/anonymous0769/DreamVideo&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://anonymous0769.github.io/DreamVideo/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DynamiCrafter:&lt;/strong&gt;&lt;/em&gt; Animating Open-domain Images with Video Diffusion Priors &lt;a href=&#34;https://arxiv.org/pdf/2310.12190&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://doubiiu.github.io/projects/DynamiCrafter/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://github.com/Doubiiu/DynamiCrafter&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=0NfmIsNAg-g&#34;&gt;[Video]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Doubiiu/DynamiCrafter&#34;&gt;[Demo]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;image-year-2022&#34;&gt;&lt;strong&gt;Year 2022&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Make It Move:&lt;/strong&gt;&lt;/em&gt; Controllable Image-to-Video Generation with Text Descriptions &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Make_It_Move_Controllable_Image-to-Video_Generation_With_Text_Descriptions_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Youncy-Hu/MAGE&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Audio-to-Video Generation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;audio-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;AAAI&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation &lt;a href=&#34;https://arxiv.org/pdf/2309.16429&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/guyyariv/TempoTokens&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;audio-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MM-Diffusion:&lt;/strong&gt;&lt;/em&gt; Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/researchmm/MM-Diffusion&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h2&gt;Video Editing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2024&#34;&gt;&lt;strong&gt;Year 2024&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CVPR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Fairy:&lt;/strong&gt;&lt;/em&gt; Fast Parallellized Instruction-Guided Video-to-Video Synthesis &lt;a href=&#34;https://arxiv.org/pdf/2312.13834&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://fairy-video2video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;CCEdit:&lt;/strong&gt;&lt;/em&gt; Creative and Controllable Video Editing via Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2309.16496&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/RuoyuFeng/CCEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ruoyufeng.github.io/CCEdit.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=UQw4jq-igN4&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DynVideo-E:&lt;/strong&gt;&lt;/em&gt; Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2310.10624&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://showlab.github.io/DynVideo-E/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=xiRH4Q6B3Yk&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Video-P2P:&lt;/strong&gt;&lt;/em&gt; Video Editing with Cross-attention Control &lt;a href=&#34;https://arxiv.org/pdf/2303.04761&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/Video-P2P&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://video-p2p.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.05856&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/STEM-Inv/stem-inv&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://stem-inv.github.io/page/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;MaskINT:&lt;/strong&gt;&lt;/em&gt; Video Editing via Interpolative Non-autoregressive Masked Transformers &lt;a href=&#34;https://arxiv.org/pdf/2312.12468&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://maskint.github.io/&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://maskint.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;VidToMe:&lt;/strong&gt;&lt;/em&gt; Video Token Merging for Zero-Shot Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2312.10656&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/lixirui142/VidToMe&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://vidtome-diffusion.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://youtu.be/cZPtwcRepNY&#34;&gt;[Video]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Towards Language-Driven Video Inpainting via Multimodal Large Language Models &lt;a href=&#34;https://arxiv.org/pdf/2401.10226.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/jianzongwu/Language-Driven-Video-Inpainting&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://jianzongwu.github.io/projects/rovi/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/jianzongwu/rovi&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;⚠️ &lt;em&gt;&lt;strong&gt;CAMEL:&lt;/strong&gt;&lt;/em&gt; CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing [Paper]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ICLR&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Ground-A-Video:&lt;/strong&gt;&lt;/em&gt; Zero-shot Grounded Video Editing using Text-to-image Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2310.01107&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/Ground-A-Video/Ground-A-Video&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://ground-a-video.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;TokenFlow:&lt;/strong&gt;&lt;/em&gt; Consistent Diffusion Features for Consistent Video Editing &lt;a href=&#34;https://arxiv.org/pdf/2307.10373&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/omerbt/TokenFlow&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://diffusion-tokenflow.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;UniEdit:&lt;/strong&gt;&lt;/em&gt; A Unified Tuning-Free Framework for Video Motion and Appearance Editing &lt;a href=&#34;https://arxiv.org/pdf/2402.13185&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/JianhongBai/UniEdit&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://jianhongbai.github.io/UniEdit/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;DragAnything:&lt;/strong&gt;&lt;/em&gt; Motion Control for Anything using Entity Representation &lt;a href=&#34;https://arxiv.org/pdf/2403.07420.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/showlab/DragAnything&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://weijiawu.github.io/draganything_page/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span id=&#34;editing-year-2023&#34;&gt;&lt;strong&gt;Year 2023&lt;/strong&gt;&lt;/span&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;arXiv&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;&lt;strong&gt;Style-A-Video:&lt;/strong&gt;&lt;/em&gt; Agile Diffusion for Arbitrary Text-based Video Style Transfer &lt;a href=&#34;https://arxiv.org/pdf/2305.05464.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[ICCV 2021] &lt;em&gt;&lt;strong&gt;WebVid-10M:&lt;/strong&gt;&lt;/em&gt; Frozen in Time: ️A Joint Video and Image Encoder for End to End Retrieval &lt;a href=&#34;https://arxiv.org/pdf/2104.00650.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://maxbain.com/webvid-dataset/&#34;&gt;[Dataset]&lt;/a&gt; &lt;a href=&#34;https://github.com/m-bain/webvid&#34;&gt;[GitHub]&lt;/a&gt; &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[ECCV 2022] &lt;strong&gt;ROS:&lt;/strong&gt; Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining &lt;a href=&#34;https://arxiv.org/pdf/2204.02393.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/metadriverse/ACO&#34;&gt;[Code]&lt;/a&gt; &lt;a href=&#34;https://mycuhk-my.sharepoint.com/personal/1155165194_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155165194%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2Fytb%5Fdriving%5Fvideos&amp;amp;ga=1&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[ICLR 2024] &lt;em&gt;&lt;strong&gt;InternVid:&lt;/strong&gt;&lt;/em&gt; A Large-scale Video-Text Dataset for Multimodal Understanding and Generation &lt;a href=&#34;https://arxiv.org/pdf/2307.06942&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[CVPR 2024] &lt;em&gt;&lt;strong&gt;Panda-70M:&lt;/strong&gt;&lt;/em&gt; Captioning 70M Videos with Multiple Cross-Modality Teachers &lt;a href=&#34;https://arxiv.org/pdf/2402.19479.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/Panda-70M&#34;&gt;[Dataset]&lt;/a&gt; &lt;a href=&#34;https://snap-research.github.io/Panda-70M&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[arXiv 2024] &lt;em&gt;&lt;strong&gt;VidProM:&lt;/strong&gt;&lt;/em&gt; A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models &lt;a href=&#34;https://arxiv.org/pdf/2403.06098.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://github.com/WangWenhao0716/VidProM&#34;&gt;[Dataset]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Q&amp;amp;A&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q: The conference sequence of this paper list?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This paper list is organized according to the following sequence: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CVPR&lt;/li&gt; &#xA;     &lt;li&gt;ICCV&lt;/li&gt; &#xA;     &lt;li&gt;ECCV&lt;/li&gt; &#xA;     &lt;li&gt;NeurIPS&lt;/li&gt; &#xA;     &lt;li&gt;ICLR&lt;/li&gt; &#xA;     &lt;li&gt;AAAI&lt;/li&gt; &#xA;     &lt;li&gt;ACM MM&lt;/li&gt; &#xA;     &lt;li&gt;SIGGRAPH&lt;/li&gt; &#xA;     &lt;li&gt;arXiv&lt;/li&gt; &#xA;     &lt;li&gt;Others&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q: What does &lt;code&gt;Others&lt;/code&gt; refers to?&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Some of the following studies (e.g., &lt;code&gt;Sora&lt;/code&gt;) does not publish their technical report on arXiv. Instead, they tend to write a blog in their official websites. The &lt;code&gt;Others&lt;/code&gt; category refers to such kind of studies.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;reference.bib&lt;/code&gt; file summarizes bibtex references of up-to-date image inpainting papers, widely used datasets, and toolkits. Based on the original references, I have made the following modifications to make their results look nice in the &lt;code&gt;LaTeX&lt;/code&gt; manuscripts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refereces are normally constructed in the form of &lt;code&gt;author-etal-year-nickname&lt;/code&gt;. Particularly, references of datasets and toolkits are directly constructed as &lt;code&gt;nickname&lt;/code&gt;, e.g., &lt;code&gt;imagenet&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In each reference, all names of conferences/journals are converted into abbreviations, e.g., &lt;code&gt;Computer Vision and Pattern Recognition -&amp;gt; CVPR&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;doi&lt;/code&gt;, &lt;code&gt;publisher&lt;/code&gt;, &lt;code&gt;organization&lt;/code&gt;, &lt;code&gt;editor&lt;/code&gt;, &lt;code&gt;series&lt;/code&gt; in all references are removed.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;pages&lt;/code&gt; of all references are added if they are missing.&lt;/li&gt; &#xA; &lt;li&gt;All paper names are in title case. Besides, I have added an additional &lt;code&gt;{}&lt;/code&gt; to make sure that the title case would also work well in some particular templates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have other demands of reference formats, you may refer to the original references of papers by searching their names in &lt;a href=&#34;https://dblp.org/&#34;&gt;DBLP&lt;/a&gt; or &lt;a href=&#34;https://scholar.google.com/&#34;&gt;Google Scholar&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- omit in toc --&gt; &#xA;&lt;h1&gt;Star History&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://api.star-history.com/svg?repos=AlonzoLeeeooo/awesome-video-generation&amp;amp;type=Date&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://api.star-history.com/svg?repos=AlonzoLeeeooo/awesome-video-generation&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlonzoLeeeooo/awesome-video-generation/main/#contents&#34;&gt;&lt;u&gt;&lt;small&gt;&amp;lt;🎯Back to Top&amp;gt;&lt;/small&gt;&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>calculation-methods/M8O-303B-21</title>
    <updated>2024-03-20T01:36:24Z</updated>
    <id>tag:github.com,2024-03-20:/calculation-methods/M8O-303B-21</id>
    <link href="https://github.com/calculation-methods/M8O-303B-21" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Лабораторные работы по ЧМ, весна 2024 г., 3 курс, гр. M8О-303Б-21&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Лабораторные работы по численным методам&lt;/h2&gt; &#xA;&lt;h3&gt;Задания&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-1.zip&#34;&gt;Лабораторная работа №1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-2.zip&#34;&gt;Лабораторная работа №2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-3.zip&#34;&gt;Лабораторная работа №3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/tasks/numeric-methods-lab-4.zip&#34;&gt;Лабораторная работа №4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Распределение вариантов&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ФИО&lt;/th&gt; &#xA;   &lt;th&gt;Вариант&lt;/th&gt; &#xA;   &lt;th&gt;Никнейм&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Батулин Евгений Андреевич&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;uggin-mai&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Белов Илья Александрович&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;xtempleZ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Будайчиев Гаджирасул&amp;nbsp;Сиражутдинович&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;nI1974In&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Голошумов Михаил Сергеевич&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;Goloshumovs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Гребнева Ольга Владимировна&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;thirteenames&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ерофеева Екатерина Сергеевна&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;kmakovtsvet&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ершов Станислав Григорьевич&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;stasOrel&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Жалялетдинов Мурат Ринатович&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;pokanepridymal&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Жилин Михаил Денисович&lt;/td&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;Krukrukruzhka&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Зверева Елизавета Леонидовна&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;zxBan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ковалев Андрей Олегович&lt;/td&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;Andrew32516&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Кон Юлия Вячеславовна&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;liadiann&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Кудрявцев Андрей Георгиевич&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;averylongandhardtoreadnickname&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Кучмистов Дмитрий Романович&lt;/td&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;Ketchounez&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Мохнач Тимур Вадимович&lt;/td&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;TypeOfFreak&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Наумов Герман Константинович&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;NaumovGerman&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Первухин Алексей Сергеевич&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;aaaalioxa&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Сабурова Серафима Павловна&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td&gt;SerafimaLil&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Сайфуллин Ильдар Камилович&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;IldarkoS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Своеволин Иван Сергеевич&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;Svoevolin&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Селезнев Илья Сергеевич&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td&gt;selfimprovementslander&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Субботина Мария Алексеевна&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td&gt;MarieSu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Сулейманян Денис Вадимович&lt;/td&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td&gt;DenisSuleymanyan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Трофимов Владислав Олегович&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;vampsh00ta&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Тысячный Владислав Валерьевич&lt;/td&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td&gt;Bradvurt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Хайруллина Ясмин Алмазовна&lt;/td&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td&gt;ysmn-al&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Хрушкова Валерия Витальевна&lt;/td&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td&gt;lera1kh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Шерматов Егор Дмитриеви&lt;/td&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td&gt;HaPPyDutCHoGGG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Оформление&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Задания выполняются написанием программного кода на языке Fortran, C, C++.&lt;/li&gt; &#xA; &lt;li&gt;Допускается применение любой парадигмы программирования, подходящей для реализации численных методов.&lt;/li&gt; &#xA; &lt;li&gt;По каждой лабораторной работе оформляется отдельный отчет в формате latex (образец оформления можно найти в папке &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/report&#34;&gt;report&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Исходный код и отчет (в формате PDF) по каждой лабораторной работе сдаются через Pull request (PR) на платформе github.com (&lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/#%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D0%BA-%D1%81%D0%B4%D0%B0%D1%87%D0%B8&#34;&gt;порядок сдачи&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Подготовка к работе студента&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Зарегистрировать аккаунт на платформе &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/github.com&#34;&gt;github.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Войти под своей учетной записью на сайте &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/github.com&#34;&gt;github.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Студенту сообщить старосте имя своего аккаунта. Старосте создать группу из аккаунтов средствами github.&lt;/li&gt; &#xA; &lt;li&gt;Сделать ответвление от настоящего репозитория (нажать кнопку &lt;code&gt;Fork&lt;/code&gt; на странице репозитория).&lt;/li&gt; &#xA; &lt;li&gt;Создать свою рабочую директорию в папке &lt;code&gt;stud&lt;/code&gt; собственного склонированного репозитория, назвав ее транслитерацией фамилии студента в нижнем регистре.&lt;/li&gt; &#xA; &lt;li&gt;Выполнить пример сдачи лабораторной работы: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Произвести редактирование файла README.md, добавив строку таблицы с ФИО студента и номером варианта, равному порядковому номеру студента в учебной группе. Фамилия должна быть добавлена в алфавитном порядке имеющегося списка.&lt;/li&gt; &#xA;   &lt;li&gt;Выполнить пп. 3-7 &lt;a href=&#34;https://raw.githubusercontent.com/calculation-methods/M8O-303B-21/master/#%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BE%D0%BA-%D1%81%D0%B4%D0%B0%D1%87%D0%B8&#34;&gt;порядок сдачи&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Порядок сдачи&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Написать программный код, решающий поставленную задачу.&lt;/li&gt; &#xA; &lt;li&gt;Поместить все необходимые для решения задачи файлы в подпапку рабочей директории.&lt;/li&gt; &#xA; &lt;li&gt;Создать запрос (Pull Request - PR) с ветки, имеющей имя, отличное от имени &lt;strong&gt;master&lt;/strong&gt;, на ветку &lt;strong&gt;master&lt;/strong&gt; клонированного репозитория для проверки работы: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;название запроса сделать латиницей по следующему шаблону &#34;Surname Name, lab 1, #1,...&#34;, где вместо многоточия пишутся номера задач, которые выполнены&lt;/li&gt; &#xA;   &lt;li&gt;указать в качестве Reviewer &lt;em&gt;pivovarov-mai&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;охарактеризовать степень готовности лабораторной работы (написать комментарий к запросу о том, что выполнено в работе)&lt;/li&gt; &#xA;   &lt;li&gt;снабдить двумя метками из списка (степень готовности и номер ЛР)&lt;/li&gt; &#xA;   &lt;li&gt;снабдить соответствующим &lt;em&gt;milestone&lt;/em&gt; из списка&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Ожидать обратной связи от преподавателя (комментарии, замечания, вопросы, одобрения).&lt;/li&gt; &#xA; &lt;li&gt;Учесть замечания, ответить на вопросы.&lt;/li&gt; &#xA; &lt;li&gt;После одобрения работы преподавателем добавить ее в репозиторий.&lt;/li&gt; &#xA; &lt;li&gt;Работа считается сданной в срок, если она добавлена в репозиторий до указанного в &lt;em&gt;milestone&lt;/em&gt; срока.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Критерии оценки&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Срок сдачи&lt;/li&gt; &#xA; &lt;li&gt;Порядок сдачи&lt;/li&gt; &#xA; &lt;li&gt;Полнота выполнения&lt;/li&gt; &#xA; &lt;li&gt;Самостоятельность выполнения&lt;/li&gt; &#xA; &lt;li&gt;Качество программного кода&lt;/li&gt; &#xA; &lt;li&gt;Ответы на вопросы по коду&lt;/li&gt; &#xA; &lt;li&gt;Ответы на вопросы по методу решения&lt;/li&gt; &#xA; &lt;li&gt;Содержательное использование git&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Обратная связь&lt;/h3&gt; &#xA;&lt;p&gt;Помимо обсуждения самих PR какие-либо вопросы и обсуждения можно выносить в &lt;em&gt;Discussions&lt;/em&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>