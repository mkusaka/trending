<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-19T01:38:01Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Heegu-sama/Homm3BG</title>
    <updated>2024-04-19T01:38:01Z</updated>
    <id>tag:github.com,2024-04-19:/Heegu-sama/Homm3BG</id>
    <link href="https://github.com/Heegu-sama/Homm3BG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository for rewriting the rule book.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Heroes of Might &amp;amp; Magic III: The Board Game üê¥ üõ°Ô∏è ‚öîÔ∏èÔ∏è&lt;br&gt;Rule Book Rewrite Project üìúü™∂&lt;/h1&gt; &#xA;&lt;p&gt;Please see the original thread on &lt;a href=&#34;https://boardgamegeek.com/thread/3235221/rule-book-rewrite-project/page/1&#34;&gt;BoardGameGeek&lt;/a&gt; ü§ì&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts &lt;strong&gt;three&lt;/strong&gt; documents. Efforts are ongoing to translate them to languages other than English. Please reach out if you&#39;d like to help with translating. Click in the table to download the most recent builds in the chosen language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;üìú &lt;strong&gt;Rewritten Rule Book&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;üñ®Ô∏è &lt;strong&gt;Rule Book - printable version&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;üìã &lt;strong&gt;Comprehensive Components List&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;üá¨üáß English&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/en/main_en.pdf&#34;&gt;üá¨üáßüìú&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/en/printable_en.pdf&#34;&gt;üá¨üáßüñ®Ô∏è&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/components_list_en/components_list_en.pdf&#34;&gt;üá¨üáßüìã&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;üáµüá± Poski&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/pl/main_pl.pdf&#34;&gt;üáµüá±üìú&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/pl/printable_pl.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/components_list_pl/components_list_pl.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;üá™üá∏ Espa√±ol&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/es/main_es.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/es/printable_es.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/components_list_es/components_list_es.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;üá´üá∑ Fran√ßais&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/fr/main_fr.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/fr/printable_fr.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qwrtln/Homm3BG-build-artifacts/components_list_fr/components_list_fr.pdf&#34;&gt;n/a&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;‚ö†Ô∏è The printable build appends page numbers to select clickable hyperlinks, and includes an index page at the end ü§û&lt;/p&gt; &#xA;&lt;p&gt;Componets List lists all the cards üÉè, minis üöπ, tokens, etc. for every box. See another &lt;a href=&#34;https://boardgamegeek.com/thread/3265461/article/43995671#43995671&#34;&gt;BoardGameGeek thread&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;üí° What Is This?&lt;/h3&gt; &#xA;&lt;p&gt;This project aims to rewrite the original rule book, in which the amount of vague language was just too vast to ignore.&lt;/p&gt; &#xA;&lt;p&gt;This repository hosts a document that aims to explain the rules clearly and concisely, and should eventually have an answer for any basic rules query you might have.&lt;/p&gt; &#xA;&lt;h3&gt;ü§î Why?&lt;/h3&gt; &#xA;&lt;p&gt;The content in the official English rule book is, simply put, insufficient as a teaching tool for the game or as a general rules reference. If you read the thread linked above you should understand how frustrating this has been for me.&lt;/p&gt; &#xA;&lt;h3&gt;üõ†Ô∏è How?&lt;/h3&gt; &#xA;&lt;p&gt;I am completely rewriting the rule book in LaTeX. It&#39;s possible that a finalized version will be later put together using other tools such as Adobe Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;This is a communal effort. This repository serves both as a means for me to preserve my work, but also for others to contribute to it as writers, proofreaders, or layout designers. If you wish to contribute directly, please contact me on BoardGameGeek or discord, my username is Heegu on both platforms.&lt;/p&gt; &#xA;&lt;h3&gt;üîÆ The Future&lt;/h3&gt; &#xA;&lt;p&gt;All new version of the rule book and their change logs will be published here and in the BGG thread. I will probably submit an indefinite number of changes before changing the version number again. The aim is to have a vastly superior &#34;1.0&#34; version ready before most people receive their pledges.&lt;/p&gt; &#xA;&lt;p&gt;The current aim is to produce a document that&#39;s meant more for digital reading, as most references to other rules and sections within the document are accomplished by using hyperlinks in the text. I know most people would also love a version that&#39;s designed more for printing, I&#39;ll see if I later have the energy to create that as well. A printable document would probably be more of a shorter reference, this document will always have 30+ pages.&lt;/p&gt; &#xA;&lt;p&gt;Please discuss any and all factual errors, bad language or other errors you&#39;ve found by either contacting me directly or in the thread. You can do this by reaching out to me directly or by opening pull requests with suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Local Development&lt;/h2&gt; &#xA;&lt;p&gt;To work on the document on your machine, you need the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://miktex.org/&#34;&gt;&lt;strong&gt;MiKTeX&lt;/strong&gt;&lt;/a&gt; (required) to build the PDF file from LaTeX files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://inkscape.org/&#34;&gt;&lt;strong&gt;Inkscape&lt;/strong&gt;&lt;/a&gt; (required) to render glyphs in the document (while installing on Windows, make sure to tick &lt;code&gt;Add Inkscape to the System Path&lt;/code&gt; option)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.texstudio.org/&#34;&gt;&lt;strong&gt;TeXstudio&lt;/strong&gt;&lt;/a&gt; (optional) to edit LaTeX files and rebuild the PDF file quickly&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://po4a.org/index.php.en&#34;&gt;&lt;strong&gt;po4a&lt;/strong&gt;&lt;/a&gt; (optional) to work on translating the document to other languages&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gimp.org/&#34;&gt;&lt;strong&gt;GIMP&lt;/strong&gt;&lt;/a&gt; (optional) to edit some images in &lt;code&gt;assets&lt;/code&gt; directory - see below for details&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://aspell.net/&#34;&gt;&lt;strong&gt;aspell&lt;/strong&gt;&lt;/a&gt; (optional) for spellchecking - see below for details&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To build the document in English, either run this in the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;latexmk -pdf -silent -shell-escape &#34;main_en&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or press the &lt;code&gt;Build &amp;amp; View&lt;/code&gt; ‚ñ∂Ô∏è (F5) button in TeXstudio. To build components list instead of the rule book, just replace &lt;code&gt;&#34;main_en&#34;&lt;/code&gt; with &lt;code&gt;&#34;components_list&#34;&lt;/code&gt;, or press &lt;code&gt;Build &amp;amp; View&lt;/code&gt; with that file open in TeXstudio.&lt;/p&gt; &#xA;&lt;p&gt;To build the printable version, make sure you&#39;ve built a regular one first at least once. Then, use the script:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è Be careful, as it edits all the files! Also, you&#39;ll need &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; for this üêç&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./make_printable.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üåç Translations&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have &lt;a href=&#34;https://po4a.org/index.php.en&#34;&gt;&lt;code&gt;po4a&lt;/code&gt;&lt;/a&gt; installed (&lt;a href=&#34;https://formulae.brew.sh/formula/po4a&#34;&gt;MacOS instructions&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;To translate a particular section:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;translations/&amp;lt;section_name&amp;gt;&lt;/code&gt; and open &lt;code&gt;&amp;lt;lang&amp;gt;.po&lt;/code&gt; file, e.g., &lt;code&gt;translations/introduction.tex/pl.po&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Choose a fragment to translate. Those start with &lt;code&gt;msgid&lt;/code&gt;. Write your new text in the line below starting with &lt;code&gt;msgstr&lt;/code&gt;. Example: &lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;msgid &#34;\\addsection{Introduction}{\\spells/magic_arrow.png}&#34;&#xA;msgstr &#34;\\addsection{Wprowadzenie}{\\spells/magic_arrow.png}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; This text (&lt;code&gt;msgstr&lt;/code&gt;) will replace the original (&lt;code&gt;msgid&lt;/code&gt;) in your translation.&lt;/li&gt; &#xA; &lt;li&gt;Regenerate your localized section: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;po4a --no-update po4a.cfg&#xA;&lt;/code&gt;&lt;/pre&gt; Disregard the errors about mismatched &lt;code&gt;multicols&lt;/code&gt;, as this is an upstream parser issue.&lt;/li&gt; &#xA; &lt;li&gt;Rebuild your PDF file (or press Build ‚ñ∂Ô∏è in TeXStudio). &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;latexmk -pdf -silent -shell-escape &#34;main_&amp;lt;lang&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Commit and repeat!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üîé Spellchecking&lt;/h3&gt; &#xA;&lt;p&gt;TeXstudio has built-in spellchecking, but the first steps have been made towards automated spellchecking with aspell. For local development, after installing the tool, you can run it from the command line for example with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;aspell -d en_US -p=./.aspell.homm3.pws --mode=tex --dont-backup check main.tex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or when wanting to check all &lt;code&gt;.tex&lt;/code&gt; files then with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find . -type f -name &#34;*.tex&#34; -exec aspell -d en_US -p=./.aspell.homm3.pws --mode=tex --dont-backup check {} \;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that currently the tool will flag many parameters in LaTeX commands. We are currently looking into it, how best to remediate this.&lt;/p&gt; &#xA;&lt;p&gt;The personal dictionary &lt;code&gt;.aspell.homm3.pwd&lt;/code&gt; currently contains only game-related words. It does not contain names (e.g., &#34;BoardGameGeek&#34;) or parameter values (e.g., &#34;px&#34;, &#34;svg&#34;) in order to minimize the chances of false-negatives in the main body of text.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Assets&lt;/h2&gt; &#xA;&lt;p&gt;All assets come from publicly available sources. Some of the images in the rule book (all in the &lt;a href=&#34;https://github.com/Heegu-sama/Homm3BG/tree/main/assets/examples&#34;&gt;&lt;code&gt;assets/examples&lt;/code&gt;&lt;/a&gt; directory as of writing) were generated by &lt;a href=&#34;https://www.gimp.org/&#34;&gt;GIMP&lt;/a&gt;. Their respective XCF files reside in &lt;a href=&#34;https://github.com/Heegu-sama/Homm3BG/tree/main/assets/gimp-files&#34;&gt;&lt;code&gt;assets/gimp-files&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fabiotosi92/Awesome-Deep-Stereo-Matching</title>
    <updated>2024-04-19T01:38:01Z</updated>
    <id>tag:github.com,2024-04-19:/fabiotosi92/Awesome-Deep-Stereo-Matching</id>
    <link href="https://github.com/fabiotosi92/Awesome-Deep-Stereo-Matching" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A curated list of awesome Deep Stereo Matching resources&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome-Deep-Stereo-Matching&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the &#34;Awesome-Deep-Stereo-Matching&#34; repository, a curated list of state-of-the-art deep stereo matching resources maintained by &lt;a href=&#34;https://fabiotosi92.github.io/&#34;&gt;Fabio Tosi&lt;/a&gt; and &lt;a href=&#34;https://mattpoggi.github.io/&#34;&gt;Matteo Poggi&lt;/a&gt;, both from the University of Bologna. This repository, inspired by &lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision&#34;&gt;awesome-computer-vision&lt;/a&gt;, aims to provide a comprehensive collection of the latest and most influential papers on deep stereo matching published in top-tier computer vision conferences and prestigious journals.&lt;/p&gt; &#xA;&lt;p&gt;The methods included in this repository are appropriately categorized to facilitate navigation and understanding of the diverse approaches and techniques employed in deep stereo matching research. Additionally, for anyone in need, we also release the &lt;a href=&#34;https://github.com/fabiotosi92/Awesome-Deep-Stereo-Matching/raw/main/references.bib&#34;&gt;reference bib&lt;/a&gt; which contains the bib entries for all the works included in this page.&lt;/p&gt; &#xA;&lt;p&gt;We use the &lt;span&gt;üö©&lt;/span&gt; symbol to highlight the absolute most groundbreaking works.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#fundamentals&#34;&gt;Survey &amp;amp; Fundamentals&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#codebase&#34;&gt;CodeBase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#datasets&#34;&gt;Datasets&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#real-world&#34;&gt;Real-World&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#synthetic&#34;&gt;Synthetic&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#frameworks&#34;&gt;Frameworks&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#learning-for-stereo-pipeline&#34;&gt;Learning for Stereo Pipeline&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#matching-cost&#34;&gt;Matching Cost&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#optimization&#34;&gt;Optimization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#refinement&#34;&gt;Refinement&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#end-to-end-architectures&#34;&gt;End-to-End Architectures&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#foundational&#34;&gt;Foundational Deep Stereo Architectures&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#efficient-oriented&#34;&gt;Efficient-Oriented Deep Stereo Architectures&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#multi-task&#34;&gt;Multi-Task Deep Stereo Architectures&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#multi-modal&#34;&gt;Beyond Visual Spectrum Deep Stereo Architectures&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#challenges-and-solutions&#34;&gt;Challenges &amp;amp; Solutions&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#over-smoothing&#34;&gt;Addressing the Over-Smoothing Issue&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#missing-gt&#34;&gt;Missing Ground Truth Depth&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#domain-shift&#34;&gt;Domain Shift&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#tom&#34;&gt;Transparent and Reflective (ToM) Surfaces &lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#asymmetric&#34;&gt;Asymmetric Stereo &lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#confidence-estimation&#34;&gt;Confidence Estimation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#workshops&#34;&gt;Workshops&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#tutorials-talks&#34;&gt;Tutorials &amp;amp; Talks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2 id=&#34;fundamentals&#34;&gt; Survey &amp;amp; Fundamentals &lt;/h2&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; Basics&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;A taxonomy and evaluation of dense two-frame stereo correspondence algorithms&#34;&lt;/em&gt;, &lt;em&gt;International Journal of Computer Vision (TPAMI), 2002&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233988&amp;amp;casa_token=thq8xzMfDVQAAAAA:LqT40M8CQY9Xt8j8pKTUJr2E89KAB9c1DGG1Pw9q1YMG__o5htMzH1Xx3_wlPwLcesYHgvc&amp;amp;tag=1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/taxonomy.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+taxonomy+and+evaluation+of+dense+two-frame+stereo+correspondence+algorithms&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;Evaluation of cost functions for stereo matching&#34;&lt;/em&gt;, CVPR, 2007. [&lt;a href=&#34;https://elib.dlr.de/53001/1/Hirschm%C3%BCllerStereoMatchingCvpr07.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/cost-functions.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Evaluation+of+cost+functions+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;Stereo Vision: Algorithms and Applications&#34;&lt;/em&gt; [&lt;a href=&#34;http://vision.deis.unibo.it/~smatt/Seminars/StereoVision.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/stereo-vision.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo+Vision:+Algorithms+and+Applications&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/ul&gt;&#xA;&lt;/details&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; Deep Stereo Matching&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;A survey on deep learning techniques for stereo-based depth estimation&#34;&lt;/em&gt;, &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233988&amp;amp;casa_token=thq8xzMfDVQAAAAA:LqT40M8CQY9Xt8j8pKTUJr2E89KAB9c1DGG1Pw9q1YMG__o5htMzH1Xx3_wlPwLcesYHgvc&amp;amp;tag=1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/survey-stereo-2.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+survey+on+deep+learning+techniques+for+stereo-based+depth+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;On the synergies between machine learning and binocular stereo for depth estimation from images: a survey&#34;&lt;/em&gt;, &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2004.08566.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/OnTheSynergies.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=On+the+synergies+between+machine+learning+and+binocular+stereo+for+depth+estimation+from+images:+a+survey&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; Learned Confidence Estimation &lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;Quantitative evaluation of confidence measures in a machine learning world&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2017&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2101.00431&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/QuantitativeConf.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Quantitative+evaluation+of+confidence+measures+in+a+machine+learning+world&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;&#34;On the Confidence of Stereo Matching in a Deep-Learning Era: A Quantitative Evaluation&#34;&lt;/em&gt;, &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2101.00431&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/OnTheConfidence.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=On+the+Confidence+of+Stereo+Matching+in+a+Deep-Learning+Era:+A+Quantitative+Evaluation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;CodeBase&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenStereo&lt;/strong&gt;: &lt;em&gt;&#34;OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong Baseline&#34;&lt;/em&gt;, &lt;em&gt;arXiv, 2023&lt;/em&gt; [&lt;a href=&#34;https://github.com/XiandaGuo/OpenStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/pdf/2312.00343.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/OpenStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=OpenStereo:+A+Comprehensive+Benchmark+for+Stereo+Matching+and+Strong+Baseline&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;details open id=&#34;real-world&#34;&gt;&#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Real-World&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open id=&#34;real-world RGB&#34;&gt;&#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; RGB &lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Middlebury v3&lt;/strong&gt;: &lt;em&gt;&#34;High-resolution stereo datasets with subpixel-accurate ground truth&#34;&lt;/em&gt;, &lt;em&gt;GCPR 2014&lt;/em&gt;. [&lt;a href=&#34;https://elib.dlr.de/90624/1/ScharsteinEtal2014.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vision.middlebury.edu/stereo/eval3/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Middlebury_v3.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=High-resolution+stereo+datasets+with+subpixel-accurate+ground+truth&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;KITTI 2012&lt;/strong&gt;: &lt;em&gt;&#34;Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2012&lt;/em&gt;. [&lt;a href=&#34;https://projet.liris.cnrs.fr/imagine/pub/proceedings/CVPR2012/data/papers/424_O3C-04.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cvlibs.net/datasets/kitti/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/KITTI_2012.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Are+we+ready+for+Autonomous+Driving?+The+KITTI+Vision+Benchmark+Suite&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;KITTI 2015&lt;/strong&gt;: &lt;em&gt;&#34;Object Scene Flow for Autonomous Vehicles&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2015&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Menze_Object_Scene_Flow_2015_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cvlibs.net/datasets/kitti/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/KITTI_2015.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Object+Scene+Flow+for+Autonomous+Vehicles&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ETH3D&lt;/strong&gt;: &lt;em&gt;&#34;A multi-view stereo benchmark with high-resolution images and multi-camera videos&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Schops_A_Multi-View_Stereo_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.eth3d.net/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ETH3D.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+multi-view+stereo+benchmark+with+high-resolution+images+and+multi-camera+videos&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DrivingStereo&lt;/strong&gt;: &lt;em&gt;&#34;DrivingStereo: A Large-Scale Dataset for Stereo Matching in Autonomous Driving Scenarios&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_DrivingStereo_A_Large-Scale_Dataset_for_Stereo_Matching_in_Autonomous_Driving_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://drivingstereo-dataset.github.io/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DrivingStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=DrivingStereo:+A+Large-Scale+Dataset+for+Stereo+Matching+in+Autonomous+Driving+Scenarios&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;WSVD&lt;/strong&gt;: &lt;em&gt;&#34;Web stereo video supervision for depth prediction from dynamic scenes&#34;&lt;/em&gt;, &lt;em&gt;3DV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8885937&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/wsvd/home&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/WSVD.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Web+stereo+video+supervision+for+depth+prediction+from+dynamic+scenes&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flickr1024&lt;/strong&gt;: &lt;em&gt;&#34;Flickr1024: A large-scale dataset for stereo image super-resolution&#34;&lt;/em&gt;, &lt;em&gt;ICCVW, 2019&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yingqianwang.github.io/Flickr1024/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Flickr1024.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Flickr1024:+A+large-scale+dataset+for+stereo+image+super-resolution&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Middlebury 2021 Mobile Dataset&lt;/strong&gt;: [&lt;a href=&#34;https://vision.middlebury.edu/stereo/data/scenes2021/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Middlebury_v3.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;The Booster Dataset&lt;/strong&gt;: &lt;em&gt;&#34;Open Challenges in Deep Stereo: The Booster Dataset&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Ramirez_Open_Challenges_in_Deep_Stereo_The_Booster_Dataset_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/booster-web/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Booster.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Open+Challenges+in+Deep+Stereo:+The+Booster+Dataset&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Holopix50k&lt;/strong&gt;: &lt;em&gt;&#34;Holopix50k: A Large-Scale In-the-Wild Stereo Image Dataset&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2003.11172&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://leiainc.github.io/holopix50k/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Holopix50k.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Holopix50k:+A+Large-Scale+In-the-Wild+Stereo+Image+Dataset&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;InStereo2K&lt;/strong&gt;: &lt;em&gt;&#34;InStereo2K: A Large Real Dataset for Stereo Matching in Indoor Scenes&#34;&lt;/em&gt;, &lt;em&gt;Science China Information Sciences, 2020&lt;/em&gt;. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11432-019-2803-x&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YuhuaXu/StereoDataset&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/InStereo2K.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=InStereo2K%3A+A+Large+Real+Dataset+for+Stereo+Matching+in+Indoor+Scenes&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt;&#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; Multimodal &lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CATS&lt;/strong&gt;: &lt;em&gt;&#34;CATS: A Color and Thermal Stereo Benchmark&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Treible_CATS_A_Color_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bigdatavision.org/CAT/download.html&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CATS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=InStereo2K:+A+Large+Real+Dataset+for+Stereo+Matching+in+Indoor+Scenes&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RGB-NIR-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Deep material-aware cross-spectral stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhi_Deep_Material-Aware_Cross-Spectral_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tiancheng-zhi/cs-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CS-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+material-aware+cross-spectral+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MVSEC&lt;/strong&gt;: &lt;em&gt;&#34;The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception&#34;&lt;/em&gt;, &lt;em&gt;RAL 2018&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/1801.10202&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://daniilidis-group.github.io/mvsec/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MVSEC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=The+Multivehicle+Stereo+Event+Camera+Dataset:+An+Event+Camera+Dataset+for+3D+Perception&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DSEC&lt;/strong&gt;: &lt;em&gt;&#34;DSEC: A Stereo Event Camera Dataset for Driving Scenarios&#34;&lt;/em&gt;, &lt;em&gt;RAL, 2021&lt;/em&gt;. [&lt;a href=&#34;https://rpg.ifi.uzh.ch/docs/RAL21_DSEC.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/uzh-rpg/DSEC&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://dsec.ifi.uzh.ch/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DSEC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=DSEC:+A+Stereo+Event+Camera+Dataset+for+Driving+Scenarios&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RGB-MS&lt;/strong&gt;: &lt;em&gt;&#34;RGB-Multispectral Matching: Dataset, Learning Methodology, Evaluation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Tosi_RGB-Multispectral_Matching_Dataset_Learning_Methodology_Evaluation_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/rgb-ms-web/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RGB-MS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=RGB-Multispectral+Matching:+Dataset,+Learning+Methodology,+Evaluation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;M3ED&lt;/strong&gt;: &lt;em&gt;&#34;M3ed: Multi-robot, multi-sensor, multi-environment event dataset&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chaney_M3ED_Multi-Robot_Multi-Sensor_Multi-Environment_Event_Dataset_CVPRW_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://m3ed.io/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/M3ED.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=M3ed:+Multi-robot,+multi-sensor,+multi-environment+event+dataset&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gated Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Gated Stereo: Joint Depth Estimation from Gated and Wide-Baseline Active Stereo Cues&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Walz_Gated_Stereo_Joint_Depth_Estimation_From_Gated_and_Wide-Baseline_Active_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://light.princeton.edu/gatedstereo/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Gated.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Gated+Stereo:+Joint+Depth+Estimation+from+Gated+and+Wide-Baseline+Active+Stereo+Cues&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MS^2&lt;/strong&gt;: &lt;em&gt;&#34;Deep Depth Estimation From Thermal Image&#34;&lt;/em&gt;, &lt;em&gt;CVPR 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/multi-spectral-stereo-dataset&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MS2.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+Depth+Estimation+From+Thermal+Image&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt;&#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; Rendered &lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;The NeRF-Stereo Dataset&lt;/strong&gt;: &lt;em&gt;&#34;NeRF-Supervised Deep Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://amsacta.unibo.it/id/eprint/7218/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NS-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=NeRF-Supervised+Deep+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;synthetic&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Synthetic&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;MPI Sintel&lt;/strong&gt;: &lt;em&gt;&#34;A naturalistic open source movie for optical flow evaluation&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2012&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://sintel.is.tue.mpg.de/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MPISintel.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+naturalistic+open+source+movie+for+optical+flow+evaluation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Freiburg SceneFlow&lt;/strong&gt;: &lt;em&gt;&#34;A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2016&lt;/em&gt;. [&lt;a href=&#34;https://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16/paper-MIFDB16.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SceneFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+Large+Dataset+to+Train+Convolutional+Networks+for+Disparity,+Optical+Flow,+and+Scene+Flow+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;HS-VS&lt;/strong&gt;: &lt;em&gt;&#34;Hierarchical deep stereo matching on high-resolution image&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1912.06704.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gengshan-y/high-res-stereo?tab=readme-ov-file&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/HSMNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Hierarchical+deep+stereo+matching+on+high-resolution+image&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Virtual KITTI&lt;/strong&gt;: &lt;em&gt;&#34;Virtual kitti 2&#34;&lt;/em&gt;, &lt;em&gt;arXiv, 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2001.10773.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/VirtualKITTI.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Virtual+kitti+2&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;TartanAir&lt;/strong&gt;: &lt;em&gt;&#34;TartanAir: A dataset to push the limits of visual slam&#34;&lt;/em&gt;, &lt;em&gt;IROS, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/9341801?denied=&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://theairlab.org/tartanair-dataset&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/TartanAir.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=TartanAir:+A+dataset+to+push+the+limits+of+visual+slam&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Semi-synthesis&lt;/strong&gt;: &lt;em&gt;&#34;Semi-synthesis: A fast way to produce effective datasets for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ICCVW, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/He_Semi-Synthesis_A_Fast_Way_To_Produce_Effective_Datasets_for_Stereo_CVPRW_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Semi-synthesis.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Semi-synthesis:+A+fast+way+to+produce+effective+datasets+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;UnrealStereo4K&lt;/strong&gt;: &lt;em&gt;&#34;SMD-Nets: Stereo Mixture Density Networks&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;http://www.cvlibs.net/publications/Tosi2021CVPR.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/SMD-Nets&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SMD-Nets.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=SMD-Nets:+Stereo+Mixture+Density+Networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;IRS&lt;/strong&gt;: &lt;em&gt;&#34;IRS: A large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation&#34;&lt;/em&gt;, &lt;em&gt;ICME, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428423&amp;amp;casa_token=wdItdcnJ3QkAAAAA:OZDqHpMb0LlKp_t_unpp8aZWh0dvRai0KkZAtdI_jvioFMqjv3goGrR3AIjmObzMjRmE_7k&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HKBU-HPML/IRS&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/IRS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=IRS:+A+large+naturalistic+indoor+robotics+stereo+dataset+to+train+deep+models+for+disparity+and+surface+normal+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;CREStereo&lt;/strong&gt;: &lt;em&gt;&#34;Practical stereo matching via cascaded recurrent network with adaptive correlation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/megvii-research/CREStereo&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CREStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Practical+stereo+matching+via+cascaded+recurrent+network+with+adaptive+correlation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SimStereo&lt;/strong&gt;: &lt;em&gt;&#34;Active-Passive SimStereo ‚Äì Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods&#34;&lt;/em&gt;, &lt;em&gt;NeurIPS, 2022&lt;/em&gt;. [&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/bc3a68a20e5c8ba5cbefc1ecf74bfaaa-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ieee-dataport.org/open-access/active-passive-simstereo&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SimStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Active-Passive+SimStereo+%E2%80%93+Benchmarking+the+Cross-Generalization+Capabilities+of+Deep+Learning-based+Stereo+Methods&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Spring&lt;/strong&gt;: &lt;em&gt;&#34;Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/html/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://spring-benchmark.org&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Spring.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Spring:+A+High-Resolution+High-Detail+Dataset+and+Benchmark+for+Scene+Flow,+Optical+Flow+and+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic Replica&lt;/strong&gt;: &lt;em&gt;&#34;DynamicStereo: Consistent Dynamic Depth From Stereo Videos&#34;&lt;/em&gt;, &lt;em&gt;CVPR 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Karaev_DynamicStereo_Consistent_Dynamic_Depth_From_Stereo_Videos_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dynamic-stereo.github.io/&#34;&gt;Dataset&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Dynamic_Replica.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=DynamicStereo:+Consistent+Dynamic+Depth+From+Stereo+Videos&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Frameworks&lt;/h2&gt; &#xA;&lt;h3&gt;Learning for Stereo Pipeline&lt;/h3&gt; &#xA;&lt;details open id=&#34;matching-cost&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Matching Cost&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Embed&lt;/strong&gt;: &lt;em&gt;&#34;A deep visual correspondence embedding model for stereo matching costs&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2015&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_iccv_2015/papers/Chen_A_Deep_Visual_ICCV_2015_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Deep_Embed.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+deep+visual+correspondence+embedding+model+for+stereo+matching+costs&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;MC-CNN&lt;/strong&gt;: &lt;em&gt;&#34;Stereo matching by training a convolutional neural network to compare image patches&#34;&lt;/em&gt;, &lt;em&gt;JMLR, 2016&lt;/em&gt;. [&lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jzbontar/mc-cnn&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MC-CNN1.txt&#34;&gt;Bibtex1&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MC-CNN.txt&#34;&gt;Bibtex2&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Computing+the+Stereo+Matching+Cost+with+a+Convolutional+Neural+Network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Content CNN&lt;/strong&gt;: &lt;em&gt;&#34;Efficient deep learning for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2016&lt;/em&gt;. [&lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Luo_Efficient_Deep_Learning_CVPR_2016_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/datvuthanh/Stereo-Matching&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Content_CNN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Efficient+deep+learning+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Per-pixel pyramid-pooling&lt;/strong&gt;: &lt;em&gt;&#34;Look wider to match image patches with convolutional neural networks&#34;&lt;/em&gt;, &lt;em&gt;SPR, 2016&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778222&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/per-pixel_pyramid-pooling.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Look+wider+to+match+image+patches+with+convolutional+neural+networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistency and Distinctiveness&lt;/strong&gt;: &lt;em&gt;&#34;Fundamental principles on learning new features for effective dense matching&#34;&lt;/em&gt;, &lt;em&gt;TIP, 2017&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038003&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Consistency_and_Distinctiveness.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Fundamental+principles+on+learning+new+features+for+effective+dense+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;MC-CNN-WS&lt;/strong&gt;: &lt;em&gt;&#34;Weakly supervised learning of deep metrics for stereo reconstruction&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2017/papers/Tulyakov_Weakly_Supervised_Learning_ICCV_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tlkvstepan/mc-cnn-ws&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MC-CNN-WS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Weakly+supervised+learning+of+deep+metrics+for+stereo+reconstruction&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;CBMV&lt;/strong&gt;: &lt;em&gt;&#34;CBMV: A coalesced bidirectional matching volume for disparity estimation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CBMV.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=CBMV:+A+coalesced+bidirectional+matching+volume+for+disparity+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SDC&lt;/strong&gt;: &lt;em&gt;&#34;SDC - stacked dilated convolution: A unified descriptor network for dense matching tasks&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Schuster_SDC_-_Stacked_Dilated_Convolution_A_Unified_Descriptor_Network_for_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SDC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=SDC+-+stacked+dilated+convolution:+A+unified+descriptor+network+for+dense+matching+tasks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Semi-dense Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Semi-dense Stereo Matching using Dual CNNs&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8659297&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Semi-dense_Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Semi-dense+Stereo+Matching+using+Dual+CNNs&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;optimization&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Optimization&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;GCP&lt;/strong&gt;: &lt;em&gt;&#34;Learning to detect ground control points for improving the accuracy of stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2014&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2014/papers/Spyropoulos_Learning_to_Detect_2014_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GCP.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+detect+ground+control+points+for+improving+the+accuracy+of+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;LevStereo&lt;/strong&gt;: &lt;em&gt;&#34;Leveraging stereo matching with learning-based confidence measures&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2015&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Park_Leveraging_Stereo_Matching_2015_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LevStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Leveraging+stereo+matching+with+learning-based+confidence+measures&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;O1&lt;/strong&gt;: &lt;em&gt;&#34;Learning a general-purpose confidence measure based on o (1) features and a smarter aggregation strategy for semi global matching&#34;&lt;/em&gt;, &lt;em&gt;3DV, 2016&lt;/em&gt;. [&lt;a href=&#34;http://vision.disi.unibo.it/~mpoggi/papers/3dv2016_o1.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/O1.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+a+general-purpose+confidence+measure+based+on+o+(1)+features+and+a+smarter+aggregation+strategy+for+semi+global+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PBCP&lt;/strong&gt;: &lt;em&gt;&#34;Patch Based Confidence Prediction for Dense Disparity Map&#34;&lt;/em&gt;, &lt;em&gt;BMVC, 2016&lt;/em&gt;. [&lt;a href=&#34;https://www.cvlibs.net/projects/autonomous_vision_survey/literature/Seki2016BMVC.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PBCP.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Patch+Based+Confidence+Prediction+for+Dense+Disparity+Map&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sgm-Nets&lt;/strong&gt;: &lt;em&gt;&#34;Sgm-Nets: Semi-global matching with neural networks&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Seki_SGM-Nets_Semi-Global_Matching_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Sgm-Nets.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Sgm-Nets:+Semi-global+matching+with+neural+networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SGM-Forest&lt;/strong&gt;: &lt;em&gt;&#34;Learning to fuse proposals from multiple scanline optimizations in semi-global matching&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Johannes_Schoenberger_Learning_to_Fuse_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SGM-Forest.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+fuse+proposals+from+multiple+scanline+optimizations+in+semi-global+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;refinement&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Refinement&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;GDN&lt;/strong&gt;: &lt;em&gt;&#34;Improved stereo matching with constant highway networks and reflective confidence learning&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Shaked_Improved_Stereo_Matching_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/amitshaked/resmatch&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GDN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Improved+stereo+matching+with+constant+highway+networks+and+reflective+confidence+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DRR&lt;/strong&gt;: &lt;em&gt;&#34;Detect, replace, refine: Deep structured prediction for pixel wise labeling&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Gidaris_Detect_Replace_Refine_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gidariss/DRR_struct_pred/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DRR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Detect,+replace,+refine:+Deep+structured+prediction+for+pixel+wise+labeling&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;OSD&lt;/strong&gt;: &lt;em&gt;&#34;Efficient stereo matching leveraging deep local and context information&#34;&lt;/em&gt;, &lt;em&gt;IEEE Access, 2017&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/OSD.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Efficient+stereo+matching+leveraging+deep+local+and+context+information&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Recresnet&lt;/strong&gt;: &lt;em&gt;&#34;Recresnet: A recurrent residual cnn architecture for disparity map enhancement&#34;&lt;/em&gt;, &lt;em&gt;3DV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8490974&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kbatsos/RecResNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Recresnet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Recresnet:+A+recurrent+residual+cnn+architecture+for+disparity+map+enhancement&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;LRCR&lt;/strong&gt;: &lt;em&gt;&#34;Left-right comparative recurrent model for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LRCR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Left-right+comparative+recurrent+model+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;VRN&lt;/strong&gt;: &lt;em&gt;&#34;Learned collaborative stereo refinement&#34;&lt;/em&gt;, &lt;em&gt;IJCV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-021-01485-5&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/VRN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learned+collaborative+stereo+refinement&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;FD-Fusion&lt;/strong&gt;: &lt;em&gt;&#34;Fast stereo disparity maps refinement by fusion of data-based and model-based estimations&#34;&lt;/em&gt;, &lt;em&gt;3DV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886031&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ferreram/FD-Fusion&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FD-Fusion.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Fast+stereo+disparity+maps+refinement+by+fusion+of+data-based+and+model-based+estimations&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;NDR&lt;/strong&gt;: &#34;&lt;em&gt;Neural disparity refinement for arbitrary resolution stereo&lt;/em&gt;&#34;, &lt;em&gt;3DV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9665913?casa_token=3rm4WpqLb_QAAAAA:5Sa0RO547j8LsaEYUeppzB33gZJg5Y3tfiPVwM9rzs9MEAuoHSta0Kdw3Cm9NrtfOOdFkIwp&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/neural-disparity-refinement-web/&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NDR.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;End-to-End Architectures&lt;/h3&gt; &#xA;&lt;details open id=&#34;foundational&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Foundational Deep Stereo Architectures&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;CNN-based Cost Volume Aggregation&lt;/summary&gt;&#xA;   &lt;ul&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;2D Architectures&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;DispNet-C&lt;/strong&gt;: &lt;em&gt;&#34;A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2016&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SceneFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+large+dataset+to+train+convolutional+networks+for+disparity,+optical+flow,+and+scene+flow+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;CNN+CRF&lt;/strong&gt;: &lt;em&gt;&#34;End-to-end training of hybrid CNN-CRF models for stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Knobelreiter_End-To-End_Training_of_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tuananh1007/End-to-End-Training-of-Hybrid-CNN-CRF-Models-for-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SceneFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=End-to-end+training+of+hybrid+CNN-CRF+models+for+stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;CRL&lt;/strong&gt;: &lt;em&gt;&#34;Cascade residual learning: A two-stage convolutional neural network for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPRW, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w17/Pang_Cascade_Residual_Learning_ICCV_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jiahaopang/crl&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CRL.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Cascade+residual+learning:+A+two-stage+convolutional+neural+network+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;iResNet&lt;/strong&gt;: &lt;em&gt;&#34;Learning for disparity estimation through feature constancy&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Learning_for_Disparity_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/leonzfa/iResNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/iResNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+for+disparity+estimation+through+feature+constancy&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;DispNet-CSS&lt;/strong&gt;: &lt;em&gt;&#34;Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Eddy_Ilg_Occlusions_Motion_and_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lmb-freiburg/netdef_models&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DispNet-CSS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Occlusions,+motion+and+depth+boundaries+with+a+generic+network+for+disparity,+optical+flow+or+scene+flow+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;AutoDispNet-CSS&lt;/strong&gt;: &lt;em&gt;&#34;Autodispnet: Improving disparity estimation with automl&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Saikia_AutoDispNet_Improving_Disparity_Estimation_With_AutoML_ICCV_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lmb-freiburg/autodispnet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/AutoDispNet-CSS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Autodispnet:+Improving+disparity+estimation+with+automl&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;HD&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;: &lt;em&gt;&#34;Hierarchical discrete distribution decomposition for match density estimation&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Hierarchical_Discrete_Distribution_Decomposition_for_Match_Density_Estimation_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ucbdrive/hd3&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/HD3.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Hierarchical+discrete+distribution+decomposition+for+match+density+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;EdgeStereo&lt;/strong&gt;: &lt;em&gt;&#34;Edgestereo: A context integrated residual pyramid network for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1803.05196.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Edgestereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Edgestereo:+A+context+integrated+residual+pyramid+network+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;AANet&lt;/strong&gt;: &lt;em&gt;&#34;AANet: Adaptive Aggregation Network for Efficient Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_AANet_Adaptive_Aggregation_Network_for_Efficient_Stereo_Matching_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haofeixu/aanet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/AANet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=AANet:+Adaptive+Aggregation+Network+for+Efficient+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bi3D&lt;/strong&gt;: &lt;em&gt;&#34;Bi3D: Stereo Depth Estimation via Binary Classifications&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Badki_Bi3D_Stereo_Depth_Estimation_via_Binary_Classifications_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/Bi3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Bi3D.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Bi3D:+Stereo+Depth+Estimation+via+Binary+Classifications&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open class=&#34;nested-details&#34;&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;3D Architectures&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;GC-Net&lt;/strong&gt;: &lt;em&gt;&#34;End-to-end learning of geometry and context for deep stereo regression&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2017/papers/Kendall_End-To-End_Learning_of_ICCV_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GC-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=End-to-end+learning+of+geometry+and+context+for+deep+stereo+regression&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;ECA&lt;/strong&gt;: &lt;em&gt;&#34;Deep stereo matching with explicit cost aggregation sub-architecture&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2018&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/download/12267/12126&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ECA.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+stereo+matching+with+explicit+cost+aggregation+sub-architecture&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;PSMNet&lt;/strong&gt;: &lt;em&gt;&#34;Pyramid Stereo Matching Network&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JiaRenChang/PSMNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PSMNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Pyramid+Stereo+Matching+Network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;HSMNet&lt;/strong&gt;: &lt;em&gt;&#34;Hierarchical deep stereo matching on high-resolution images&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Hierarchical_Deep_Stereo_Matching_on_High-Resolution_Images_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gengshan-y/high-res-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/HSMNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Hierarchical+deep+stereo+matching+on+high-resolution+images&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;GWCNet&lt;/strong&gt;: &lt;em&gt;&#34;Group-wise correlation stereo network&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_Group-Wise_Correlation_Stereo_Network_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xy-guo/GwcNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GWCNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Group-wise+correlation+stereo+network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;EMCUA&lt;/strong&gt;: &lt;em&gt;&#34;Multi-Level Context Ultra-Aggregation for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Nie_Multi-Level_Context_Ultra-Aggregation_for_Stereo_Matching_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EMCUA.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Multi-Level+Context+Ultra-Aggregation+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;CSPN&lt;/strong&gt;: &lt;em&gt;&#34;Learning depth with convolutional spatial propagation network&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2019&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1810.02695.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XinJCheng/CSPN&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CSPN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+depth+with+convolutional+spatial+propagation+network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;GA-Net&lt;/strong&gt;: &lt;em&gt;&#34;Ga-net: Guided aggregation net for end-to-end stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_GA-Net_Guided_Aggregation_Net_for_End-To-End_Stereo_Matching_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/feihuzhang/GANet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GANet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Ga-net:+Guided+aggregation+net+for+end-to-end+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stereodrnet&lt;/strong&gt;: &lt;em&gt;&#34;Stereodrnet: Dilated residual stereonet&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Chabra_StereoDRNet_Dilated_Residual_StereoNet_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Stereodrnet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereodrnet:+Dilated+residual+stereonet&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;PDSNet&lt;/strong&gt;: &lt;em&gt;&#34;Practical deep stereo (pds): Toward applications-friendly deep stereo matching&#34;&lt;/em&gt;, &lt;em&gt;NeurIPS, 2018&lt;/em&gt;. [&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2018/file/ade55409d1224074754035a5a937d2e0-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tlkvstepan/PracticalDeepStereo_NIPS2018&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PDSNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Practical+deep+stereo+(pds):+Toward+applications-friendly+deep+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;CasStereo&lt;/strong&gt;: &lt;em&gt;&#34;Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alibaba/cascade-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CasStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Cascade+Cost+Volume+for+High-Resolution+Multi-View+Stereo+and+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;WaveletStereo&lt;/strong&gt;: &lt;em&gt;&#34;WaveletStereo: Learning Wavelet Coefficients of Disparity Map in Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_WaveletStereo_Learning_Wavelet_Coefficients_of_Disparity_Map_in_Stereo_Matching_CVPR_2020&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/WaveletStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=WaveletStereo:+Learning+Wavelet+Coefficients+of+Disparity+Map+in+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;CFNet&lt;/strong&gt;: &lt;em&gt;&#34;CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_CFNet_Cascade_and_Fused_Cost_Volume_for_Robust_Stereo_Matching_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gallenszl/CFNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CFNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=CFNet:+Cascade+and+Fused+Cost+Volume+for+Robust+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;UASNet&lt;/strong&gt;: &lt;em&gt;&#34;UASNet: Uncertainty Adaptive Sampling Network for Deep Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2021&lt;/em&gt; [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Mao_UASNet_Uncertainty_Adaptive_Sampling_Network_for_Deep_Stereo_Matching_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UASNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=UASNet:+Uncertainty+Adaptive+Sampling+Network+for+Deep+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;PCR&lt;/strong&gt;: &lt;em&gt;&#34;Parallax contextual representations for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ICIP, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506747&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PCR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=PARALLAX+CONTEXTUAL+REPRESENTATIONS+FOR+STEREO+MATCHING&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;PCWNet&lt;/strong&gt;: &lt;em&gt;&#34;PCW-Net: Pyramid Combination and Warping Cost Volume for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920280.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gallenszl/PCWNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PCWNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=PCW-Net:+Pyramid+Combination+and+Warping+Cost+Volume+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;ICVP&lt;/strong&gt;: &lt;em&gt;&#34;Image-Coupled Volume Propagation for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ICIP, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/10222247&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ohkwon718/icvp&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ICVP.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Image-Coupled+Volume+Propagation+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;SEDNet&lt;/strong&gt;: &lt;em&gt;&#34;Learning the distribution of errors in stereo matching for joint disparity and uncertainty estimation&#34;&lt;/em&gt;, CVPR, 2023. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_the_Distribution_of_Errors_in_Stereo_Matching_for_Joint_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lly00412/SEDNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SEDNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+the+distribution+of+errors+in+stereo+matching+for+joint+disparity+and+uncertainty+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;   &lt;/ul&gt;&#xA;  &lt;/details&gt; &#xA;  &lt;details open class=&#34;nested-details&#34;&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Neural Architecture Search (NAS)&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LEAStereo&lt;/strong&gt;: &lt;em&gt;&#34;Hierarchical Neural Architecture Search for Deep Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;NeurIPS, 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2010.13501.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XuelianCheng/LEAStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LEAStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Hierarchical+Neural+Architecture+Search+for+Deep+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;EASNet&lt;/strong&gt;: &lt;em&gt;&#34;EASNet: searching elastic and accurate network architecture for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920434.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HKBU-HPML/EASNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EASNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=EASNet:+searching+elastic+and+accurate+network+architecture+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open class=&#34;nested-details&#34;&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Iterative Optimized-based Architectures&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;RAFT-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;3DV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2109.07547&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/princeton-vl/RAFT-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RAFT-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=RAFT-Stereo:+Multilevel+Recurrent+Field+Transforms+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SCV-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;SCV-Stereo: Learning Stereo Matching from a Sparse Cost Volume&#34;&lt;/em&gt;, &lt;em&gt;ICIP, 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2107.08187&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/scv-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SCV-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=SCV-Stereo:+Learning+Stereo+Matching+from+a+Sparse+Cost+Volume&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CREStereo&lt;/strong&gt;: &lt;em&gt;&#34;Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/megvii-research/CREStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CREStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Practical+Stereo+Matching+via+Cascaded+Recurrent+Network+with+Adaptive+Correlation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;EAI-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;EAI-Stereo: Error Aware Iterative Network for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ACCV2022/html/Zhao_EAI-Stereo_Error_Aware_Iterative_Network_for_Stereo_Matching_ACCV_2022_paper.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/smartadpole/EAI-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EAI-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=EAI-Stereo:+Error+Aware+Iterative+Network+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;IGEV-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Iterative Geometry Encoding Volume for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Iterative_Geometry_Encoding_Volume_for_Stereo_Matching_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gangweiX/IGEV&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/IGEV-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Iterative+Geometry+Encoding+Volume+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DLNR&lt;/strong&gt;: &lt;em&gt;&#34;High-Frequency Stereo Matching Network&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/David-Zhao-1997/High-frequency-Stereo-Matching-Network&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DLNR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=High-Frequency+Stereo+Matching+Network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic Stereo&lt;/strong&gt;: &lt;em&gt;&#34;DynamicStereo: Consistent Dynamic Depth From Stereo Videos&#34;&lt;/em&gt;, &lt;em&gt;CVPR 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Karaev_DynamicStereo_Consistent_Dynamic_Depth_From_Stereo_Videos_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dynamic-stereo.github.io/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Dynamic_Replica.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=DynamicStereo:+Consistent+Dynamic+Depth+From+Stereo+Videos&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CREStereo++&lt;/strong&gt;: &lt;em&gt;&#34;Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Jing_Uncertainty_Guided_Adaptive_Warping_for_Robust_and_Efficient_Stereo_Matching_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CREStereo++.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Uncertainty+Guided+Adaptive+Warping+for+Robust+and+Efficient+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Selective-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2403.00486.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Windsrain/Selective-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Selective-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Selective-Stereo:+Adaptive+Frequency+Information+Selection+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Any-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Any-Stereo: Arbitrary Scale Disparity Estimation for Iterative Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2024&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/28119/28242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Zhaohuai-L/Any-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Any-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Any-Stereo:+Arbitrary+Scale+Disparity+Estimation+for+Iterative+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MC-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;3DV, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2311.02340.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MiaoJieF/MC-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MC-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=MC-Stereo:+Multi-peak+Lookup+and+Cascade+Search+Range+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ICGNet&lt;/strong&gt;: &lt;em&gt;&#34;Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2402.19270.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DFSDDDDD1199/ICGNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ICGNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+Intra-view+and+Cross-view+Geometric+Knowledge+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MoCha-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;MoCha-Stereo: Motif Channel Attention Network for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2404.06842.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZYangChen/MoCha-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MoCha-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.de/scholar?q=MoCha-Stereo:%20Motif%20Channel%20Attention%20Network%20for%20Stereo%20Matching&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;XR-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended Reality&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2024&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2024/papers/Cheng_Stereo_Matching_in_Time_100_FPS_Video_Stereo_Matching_for_WACV_2024_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/za-cheng/XR-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/XR-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo+Matching+in+Time:+100++FPS+Video+Stereo+Matching+for+Extended+Reality&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open class=&#34;nested-details&#34;&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Transformer-based Architectures&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;STTR&lt;/strong&gt;: &lt;em&gt;&#34;Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2021&lt;/em&gt; [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mli0603/stereo-transformer&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/STTR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Revisiting+Stereo+Depth+Estimation+From+a+Sequence-to-Sequence+Perspective+With+Transformers&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CEST&lt;/strong&gt;: &lt;em&gt;&#34;Context-enhanced stereo transformer&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920263.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CEST.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Context-enhanced+stereo+transformer&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chitransformer&lt;/strong&gt;: &lt;em&gt;&#34;Chitransformer: Towards Reliable Stereo From Cues&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Su_Chitransformer_Towards_Reliable_Stereo_From_Cues_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ISL-CV/ChiTransformer&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Chitransformer.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Chitransformer:+Towards+Reliable+Stereo+From+Cues&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;GMStereo&lt;/strong&gt;: &lt;em&gt;&#34;Unifying Flow, Stereo and Depth Estimation&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2211.05783.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://haofeixu.github.io/unimatch/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GMStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unifying+Flow,+Stereo+and+Depth+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CroCo v2&lt;/strong&gt;: &lt;em&gt;&#34;CroCo v2: Improved Cross-View Completion Pre-training for Stereo Matching and Optical Flow&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Weinzaepfel_CroCo_v2_Improved_Cross-view_Completion_Pre-training_for_Stereo_Matching_and_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naver/croco&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CroCo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=CroCo+v2:+Improved+Cross-View+Completion+Pre-training+for+Stereo+Matching+and+Optical+Flow&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ELFNet&lt;/strong&gt;: &lt;em&gt;&#34;Elfnet: Evidential local-global fusion for stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Lou_ELFNet_Evidential_Local-global_Fusion_for_Stereo_Matching_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Lou_ELFNet_Evidential_Local-global_Fusion_for_Stereo_Matching_ICCV_2023_paper.pdf&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ELFNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Elfnet:+Evidential+local-global+fusion+for+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;GOAT&lt;/strong&gt;: &lt;em&gt;&#34;Global Occlusion-Aware Transformer for Robust Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2024&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Global_Occlusion-Aware_Transformer_for_Robust_Stereo_Matching_WACV_2024_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Magicboomliu/GOAT&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GOAT.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Global+Occlusion-Aware+Transformer+for+Robust+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open class=&#34;nested-details&#34;&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Markov Random Field-based Architectures&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;NMRF&lt;/strong&gt;: &lt;em&gt;&#34;Neural Markov Random Field for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2403.11193.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aeolusguan/NMRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NMRF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Neural+Markov+Random+Field+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;efficient-oriented&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Efficient-Oriented Deep Stereo Architectures&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Compact Cost Volume Representation&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stereonet&lt;/strong&gt;: &lt;em&gt;&#34;Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Sameh_Khamis_StereoNet_Guided_Hierarchical_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/neka-nat/StereoNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Stereonet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereonet:+Guided+hierarchical+refinement+for+real-time+edge-aware+depth+prediction&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fast DS-CS&lt;/strong&gt;: &lt;em&gt;&#34;Fast Deep Stereo with 2D Convolutional Processing of Cost Signatures&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2020&lt;/em&gt; [&lt;a href=&#34;https://openaccess.thecvf.com/content_WACV_2020/papers/Yee_Fast_Deep_Stereo_with_2D_Convolutional_Processing_of_Cost_Signatures_WACV_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ayanc/fdscs&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FDCSC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Fast+Deep+Stereo+with+2D+Convolutional+Processing+of+Cost+Signatures&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DecNet&lt;/strong&gt;: &lt;em&gt;&#34;A Decomposition Model for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yao_A_Decomposition_Model_for_Stereo_Matching_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YaoChengTang/DecNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DecNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+Decomposition+Model+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ACVNet&lt;/strong&gt;: &lt;em&gt;&#34;Attention Concatenation Volume for Accurate and Efficient Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Attention_Concatenation_Volume_for_Accurate_and_Efficient_Stereo_Matching_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gangweiX/ACVNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ACVNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Attention+Concatenation+Volume+for+Accurate+and+Efficient+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;PCVNet&lt;/strong&gt;: &lt;em&gt;&#34;Parameterized Cost Volume for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Zeng_Parameterized_Cost_Volume_for_Stereo_Matching_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jiaxiZeng/Parameterized-Cost-Volume-for-Stereo-Matching&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PCVNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Parameterized+Cost+Volume+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Efficient Cost Volume Processing&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deeppruner&lt;/strong&gt;: &lt;em&gt;&#34;Deeppruner: Learning efficient stereo matching via differentiable patchmatch&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Duggal_DeepPruner_Learning_Efficient_Stereo_Matching_via_Differentiable_PatchMatch_ICCV_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/uber-research/DeepPruner&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Deeppruner.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deeppruner:+Learning+efficient+stereo+matching+via+differentiable+patchmatch&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CasStereo&lt;/strong&gt;: &lt;em&gt;&#34;Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alibaba/cascade-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CasStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Cascade+Cost+Volume+for+High-Resolution+Multi-View+Stereo+and+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;BGNet&lt;/strong&gt;: &lt;em&gt;&#34;Bilateral Grid Learning for Stereo Matching Networks&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Bilateral_Grid_Learning_for_Stereo_Matching_Networks_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/3DCVdeveloper/BGNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/BGNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Bilateral+Grid+Learning+for+Stereo+Matching+Networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MABNet&lt;/strong&gt;: &lt;em&gt;&#34;MABNet: a lightweight stereo network based on multibranch adjustable bottleneck module&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730341.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JumpXing/MABNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MABNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=MABNet:+a+lightweight+stereo+network+based+on+multibranch+adjustable+bottleneck+module&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Separable-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Separable Convolutions for Optimizing 3D Stereo Networks&#34;&lt;/em&gt;, &lt;em&gt;ICIP, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cogsys-tuebingen/separable-3D-convs-for-stereo-matching&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Separable-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Separable+Convolutions+for+Optimizing+3D+Stereo+Networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;TemporalStereo&lt;/strong&gt;: &lt;em&gt;&#34;TemporalStereo: Efficient Spatial-Temporal Stereo Matching Network&#34;&lt;/em&gt;, &lt;em&gt;IROS, 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2211.13755.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/youmi-zym/TemporalStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/TemporalStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=TemporalStereo:+Efficient+Spatial-Temporal+Stereo+Matching+Network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;IINet&lt;/strong&gt;: &lt;em&gt;&#34;IINet: Implicit Intra-inter Information Fusion for Real-Time Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2024&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/28107/28218&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/IINet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=IINet:+Implicit+Intra-inter+Information+Fusion+for+Real-Time+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Efficient Inference Schemes&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;StereoVAE&lt;/strong&gt;: &lt;em&gt;&#34;StereoVAE: A lightweight stereo-matching system using embedded GPUs&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/10160441&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/StereoVAE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=StereoVAE:+A+lightweight+stereo-matching+system+using+embedded+GPUs&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Anytime&lt;/strong&gt;: &lt;em&gt;&#34;Anytime stereo image depth estimation on mobile devices&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8794003&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mileyan/AnyNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Anytime.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Anytime+stereo+image+depth+estimation+on+mobile+devices&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Lightweight Network Architecture Design&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;NVStereoNet&lt;/strong&gt;: &lt;em&gt;&#34;On the importance of stereo for accurate depth estimation: An efficient semi-supervised deep neural network approach&#34;&lt;/em&gt;, &lt;em&gt;CVPRW, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w14/Smolyanskiy_On_the_Importance_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVIDIA-AI-IOT/redtail&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NVStereoNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=On+the+Importance+of+Stereo+for+Accurate+Depth+Estimation%3A+An+Efficient+Semi-Supervised+Deep+Neural+Network+Approach&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MadNet&lt;/strong&gt;: &lt;em&gt;&#34;Real-Time Self-Adaptive Deep Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo?tab=readme-ov-file&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MadNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Real-Time+Self-Adaptive+Deep+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MobileStereoNet&lt;/strong&gt;: &lt;em&gt;&#34;MobileStereoNet: Towards Lightweight Deep Networks for Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2022/papers/Shamsafar_MobileStereoNet_Towards_Lightweight_Deep_Networks_for_Stereo_Matching_WACV_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cogsys-tuebingen/mobilestereonet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MobileStereoNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=MobileStereoNet:+Towards+Lightweight+Deep+Networks+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;PBCStereo&lt;/strong&gt;: &lt;em&gt;&#34;PBCStereo: A Compressed Stereo Network with Pure Binary Convolutional Operations&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ACCV2022/papers/Cai_PBCStereo_A_Compressed_Stereo_Network_with_Pure_Binary_Convolutional_Operations_ACCV_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PBCStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=PBCStereo:+A+Compressed+Stereo+Network+with+Pure+Binary+Convolutional+Operations&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fadnet&lt;/strong&gt;: &lt;em&gt;&#34;Fadnet: A Fast and Accurate Network for Disparity Estimation&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9197031&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HKBU-HPML/FADNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Fadnet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Fadnet:+A+Fast+and+Accurate+Network+for+Disparity+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;AAFS&lt;/strong&gt;: &lt;em&gt;&#34;Attention-Aware Feature Aggregation for Real-time Stereo Matching on Edge Devices&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2020&lt;/em&gt; [&lt;a href=&#34;https://github.com/JiaRenChang/RealtimeStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://openaccess.thecvf.com/content/ACCV2020/papers/Chang_Attention-Aware_Feature_Aggregation_for_Real-time_Stereo_Matching_on_Edge_Devices_ACCV_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/AAFS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Attention-Aware+Feature+Aggregation+for+Real-time+Stereo+Matching+on+Edge+Devices&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;HITNet&lt;/strong&gt;: &lt;em&gt;&#34;HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2007.12140&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/hitnet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/HITNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=HITNet:+Hierarchical+Iterative+Tile+Refinement+Network+for+Real-time+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoEX&lt;/strong&gt;: &lt;em&gt;&#34;Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation&#34;&lt;/em&gt;, &lt;em&gt;IROS, 2021&lt;/em&gt;. [&lt;a href=&#34;https://antabangun.github.io/projects/CoEx/&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/antabangun/coex&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CoEX.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Correlate-and-Excite:+Real-Time+Stereo+Matching+via+Guided+Cost+Volume+Excitation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RLStereo&lt;/strong&gt;: &lt;em&gt;&#34;RLStereo: Real-time stereo matching based on reinforcement learning&#34;&lt;/em&gt;, &lt;em&gt;TIP, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9614986&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RLStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=RLStereo:+Real-time+stereo+matching+based+on+reinforcement+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MadNet2&lt;/strong&gt;: &lt;em&gt;&#34;Federated Online Adaptation for Deep Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FedStereo.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;multi-task&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Multi-Task Deep Stereo Architectures&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Normal-Assisted Stereo Matching&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;NA-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Normal Assisted Stereo Depth Estimation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Kusupati_Normal_Assisted_Stereo_Depth_Estimation_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/udaykusupati/Normal-Assisted-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NA-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Normal+Assisted+Stereo+Depth+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;HITNet&lt;/strong&gt;: &lt;em&gt;&#34;HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/abs/2007.12140&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/hitnet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/HITNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=HITNet:+Hierarchical+Iterative+Tile+Refinement+Network+for+Real-time+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Joint Stereo Matching and Optical Flow&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;BridgeDepthFlow&lt;/strong&gt;: &lt;em&gt;&#34;Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Lai_Bridging_Stereo_Matching_and_Optical_Flow_via_Spatiotemporal_Correspondence_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lelimite4444/BridgeDepthFlow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/BridgeDepthFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Bridging+Stereo+Matching+and+Optical+Flow+via+Spatiotemporal+Correspondence&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UnOS&lt;/strong&gt;: &lt;em&gt;&#34;UnOS: Unified Unsupervised Optical-Flow and Stereo-Depth Estimation by Watching Videos&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/34/9185119/08769907.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/baidu-research/UnDepthflow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UnOS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=UnOS:+Unified+Unsupervised+Optical-Flow+and+Stereo-Depth+Estimation+by+Watching+Videos&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Feature-Level Collaboration&lt;/strong&gt;: &lt;em&gt;&#34;Feature-Level Collaboration: Joint Unsupervised Learning of Optical Flow, Stereo Depth and Camera Motion&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Chi_Feature-Level_Collaboration_Joint_Unsupervised_Learning_of_Optical_Flow_Stereo_Depth_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Feature-Level_Collaboration.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;StereoFlowGAN&lt;/strong&gt;: &lt;em&gt;&#34;StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation&#34;&lt;/em&gt;, &lt;em&gt;BMVC, 2023&lt;/em&gt;. [&lt;a href=&#34;https://papers.bmvc2023.org/0240.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/StereoFlowGAN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=StereoFlowGAN:+Co-training+for+Stereo+and+Flow+with+Unsupervised+Domain+Adaptation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Joint Stereo Matching and Semantic Segmentation&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Segstereo&lt;/strong&gt;: &lt;em&gt;&#34;Segstereo: Exploiting semantic information for disparity estimation&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Guorun_Yang_SegStereo_Exploiting_Semantic_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yangguorun/SegStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Segstereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Segstereo:+Exploiting+semantic+information+for+disparity+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DSNet&lt;/strong&gt;: &lt;em&gt;&#34;DSNet: Joint learning for scene segmentation and disparity estimation&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8793573&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DSNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=DSNet:+Joint+learning+for+scene+segmentation+and+disparity+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dispsegnet&lt;/strong&gt;: &lt;em&gt;&#34;Dispsegnet: Leveraging semantics for end-to-end learning of disparity estimation from stereo imagery&#34;&lt;/em&gt;, &lt;em&gt;RAL, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/7083369/7339444/08624344.pdf?casa_token=6L9mfcue0jkAAAAA:iMjaxtOTrLvn-9kP5g-NbVDoSAKS5M9LNoIuT3wHecgLmtjrvyhAJ0IXNU4JZjg-4XaOiAsa&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Dispsegnet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Dispsegnet:+Leveraging+semantics+for+end-to-end+learning+of+disparity+estimation+from+stereo+imagery&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SSPCV-Net&lt;/strong&gt;: &lt;em&gt;&#34;Semantic stereo matching with pyramid cost volumes&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Semantic_Stereo_Matching_With_Pyramid_Cost_Volumes_ICCV_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SSPCV-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Semantic+stereo+matching+with+pyramid+cost+volumes&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RSS-Net&lt;/strong&gt;: &lt;em&gt;&#34;Real-time semantic stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9196784/&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RSS-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Real-time+semantic+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SGNet&lt;/strong&gt;: &lt;em&gt;&#34;SGNet: Semantics Guided Deep Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ACCV2020/papers/Chen_SGNet_Semantics_Guided_Deep_Stereo_Matching_ACCV_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SGNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=SGNet:+Semantics+Guided+Deep+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Joint Stereo Matching and Uncertainty&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RCN&lt;/strong&gt;: &lt;em&gt;&#34;Improved stereo matching with constant highway networks and reflective confidence learning&#34;&lt;/em&gt;, CVPR, 2017. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Shaked_Improved_Stereo_Matching_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/amitshaked/resmatch&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RCN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Improved+stereo+matching+with+constant+highway+networks+and+reflective+confidence+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UCN&lt;/strong&gt;: &lt;em&gt;&#34;Unified confidence estimation networks for robust stereo matching&#34;&lt;/em&gt;, TIP, 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/83/4358840/08510870.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UCN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unified+confidence+estimation+networks+for+robust+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ACN&lt;/strong&gt;: &lt;em&gt;&#34;Adversarial confidence estimation networks for robust stereo matching&#34;&lt;/em&gt;, T-ITS, 2020. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_LAF-Net_Locally_Adaptive_Fusion_Networks_for_Stereo_Confidence_Estimation_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ACN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Adversarial+confidence+estimation+networks+for+robust+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;AcfNet&lt;/strong&gt;: &lt;em&gt;&#34;Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6991/6845&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/youmi-zym/AcfNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/WaveletStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Adaptive+Unimodal+Cost+Volume+Filtering+for+Deep+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Weak Adversarial Learning&lt;/strong&gt;: &lt;em&gt;&#34;Leveraging a weakly adversarial paradigm for joint learning of disparity and confidence estimation&#34;&lt;/em&gt;, &lt;em&gt;ICPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/9411940/9411911/09412594.pdf?casa_token=aC1mxhZB1hYAAAAA:ynHT4tbmo7UZSf0kCNTUsbDTsB5BhI-bvKrfWu6PuhkFY27FTPVYeHS7y6qJkeJ9H6AgaatE&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/WAL.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Leveraging+a+weakly+adversarial+paradigm+for+joint+learning+of+disparity+and+confidence+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bayesian&lt;/strong&gt;: &lt;em&gt;&#34;Joint estimation of depth and its uncertainty from stereo images using bayesian deep learning&#34;&lt;/em&gt;, &lt;em&gt;ISPRS, 2022&lt;/em&gt;. [&lt;a href=&#34;https://isprs-annals.copernicus.org/articles/V-2-2022/69/2022/isprs-annals-V-2-2022-69-2022.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Bayesian.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Joint+estimation+of+depth+and+its+uncertainty+from+stereo+images+using+bayesian+deep+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SEDNet&lt;/strong&gt;: &lt;em&gt;&#34;Learning the distribution of errors in stereo matching for joint disparity and uncertainty estimation&#34;&lt;/em&gt;, CVPR, 2023. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_the_Distribution_of_Errors_in_Stereo_Matching_for_Joint_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lly00412/SEDNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SEDNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+the+distribution+of+errors+in+stereo+matching+for+joint+disparity+and+uncertainty+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt; Scene Flow &lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;FlowNet3.0&lt;/strong&gt;: &lt;em&gt;&#34;Occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Eddy_Ilg_Occlusions_Motion_and_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lmb-freiburg/netdef_models&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FlowNet3.0.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Occlusions,+motion+and+depth+boundaries+with+a+generic+network+for+disparity,+optical+flow+or+scene+flow+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DRISF&lt;/strong&gt;: &lt;em&gt;&#34;Deep Rigid Instance Scene Flow&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Ma_Deep_Rigid_Instance_Scene_Flow_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DRISF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+Rigid+Instance+Scene+Flow&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeblurringSF&lt;/strong&gt;: &lt;em&gt;&#34;Joint stereo video deblurring, scene flow estimation and moving object segmentation&#34;&lt;/em&gt;, &lt;em&gt;TIP, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/83/4358840/08866754.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DeblurringSF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Joint+stereo+video+deblurring,+scene+flow+estimation+and+moving+object+segmentation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;IOSF&lt;/strong&gt;: &lt;em&gt;&#34;Learning Independent Object Motion From Unlabelled Stereoscopic Videos&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2019&lt;/em&gt;. [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/IOSF&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/IOSF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+Independent+Object+Motion+From+Unlabelled+Stereoscopic+Videos&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;EPC++&lt;/strong&gt;: &lt;em&gt;&#34;Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/34/9185119/08769907.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chenxuluo/EPC&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EPC++.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Every+pixel+counts++:+Joint+learning+of+geometry+and+motion+with+3d+holistic+understanding&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SENSE&lt;/strong&gt;: &lt;em&gt;&#34;Sense: A shared encoder network for scene-flow estimation&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_SENSE_A_Shared_Encoder_Network_for_Scene-Flow_Estimation_ICCV_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/SENSE&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SENSE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Sense:+A+shared+encoder+network+for+scene-flow+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;StereoExpansion&lt;/strong&gt;: &lt;em&gt;&#34;Upgrading Optical Flow to 3D Scene Flow through Optical Expansion&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Upgrading_Optical_Flow_to_3D_Scene_Flow_Through_Optical_Expansion_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gengshan-y/expansion&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/StereoExpansion.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Upgrading+Optical+Flow+to+3D+Scene+Flow+through+Optical+Expansion&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DWARF&lt;/strong&gt;: &lt;em&gt;&#34;Learning end-to-end scene flow by distilling single tasks knowledge&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6613/6467&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FilippoAleotti/DWARF-Tensorflow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DWARF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+end-to-end+scene+flow+by+distilling+single+tasks+knowledge&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SceneFlowFields++&lt;/strong&gt;: &lt;em&gt;&#34;SceneFlowFields++: Multi-frame matching, visibility prediction, and robust interpolation for scene flow estimation&#34;&lt;/em&gt;, &lt;em&gt;IJCV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1902.10099.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SceneFlowFields++.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=SceneFlowFields++:+Multi-frame+matching,+visibility+prediction,+and+robust+interpolation+for+scene+flow+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Effiscene&lt;/strong&gt;: &lt;em&gt;&#34;Effiscene: Efficient per-pixel rigidity inference for unsupervised joint learning of optical flow, depth, camera pose and motion segmentation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Jiao_EffiScene_Efficient_Per-Pixel_Rigidity_Inference_for_Unsupervised_Joint_Learning_of_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Effiscene.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Effiscene:+Efficient+per-pixel+rigidity+inference+for+unsupervised+joint+learning+of+optical+flow,+depth,+camera+pose+and+motion+segmentation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RigidMask&lt;/strong&gt;: &lt;em&gt;&#34;Learning to Segment Rigid Motions from Two Frames&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Learning_To_Segment_Rigid_Motions_From_Two_Frames_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gengshan-y/rigidmask&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RigidMask.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+Segment+Rigid+Motions+from+Two+Frames&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self-superflow&lt;/strong&gt;: &lt;em&gt;&#34;Self-superflow: self-supervised scene flow prediction in stereo sequences&#34;&lt;/em&gt;, &lt;em&gt;ICIP, 2022&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/9897158/9897159/09897832.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Self-superflow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=+Self-SuperFlow%3A+Self-supervised+Scene+Flow+Prediction+in+Stereo+Sequences&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CamLiFlow&lt;/strong&gt;: &lt;em&gt;&#34;Learning optical flow and scene flow with bidirectional camera-lidar fusion&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10310261&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MCG-NJU/CamLiFlow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CamLiFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+optical+flow+and+scene+flow+with+bidirectional+camera-lidar+fusion&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;M-FUSE&lt;/strong&gt;: &lt;em&gt;&#34;M-fuse: Multi-frame fusion for scene flow estimation&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2023/papers/Mehl_M-FUSE_Multi-Frame_Fusion_for_Scene_Flow_Estimation_WACV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cv-stuttgart/M-FUSE&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/M-FUSE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=M-fuse:+Multi-frame+fusion+for+scene+flow+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpticalExpansion&lt;/strong&gt;: &lt;em&gt;&#34;Learning Optical Expansion from Scale Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content/CVPR2023/papers/Ling_Learning_Optical_Expansion_From_Scale_Matching_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/OpticalExpansion.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+Optical+Expansion+from+Scale+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt;&#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;multi-modal&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Beyond Visual Spectrum Deep Stereo Architectures&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Depth-Guided Sensor Stereo Networks&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LidarStereoFusion&lt;/strong&gt;: &lt;em&gt;&#34;High-precision depth estimation with the 3d lidar and stereo fusion&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2018&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8461048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LidarStereoFusion.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=High-precision+depth+estimation+with+the+3d+lidar+and+stereo+fusion&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;GSD&lt;/strong&gt;: &lt;em&gt;&#34;Guided stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Poggi_Guided_Stereo_Matching_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mattpoggi/guided-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GSD.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Guided+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LidarStereoNet&lt;/strong&gt;: &lt;em&gt;&#34;Noise-Aware Unsupervised Deep Lidar-Stereo Fusion&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Cheng_Noise-Aware_Unsupervised_Deep_Lidar-Stereo_Fusion_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XuelianCheng/LidarStereoNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LidarStereoNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Noise-Aware+Unsupervised+Deep+Lidar-Stereo+Fusion&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stereo-LiDAR-CCVNorm&lt;/strong&gt;: &lt;em&gt;&#34;3d lidar and stereo fusion using stereo matching network with conditional cost volume normalization&#34;&lt;/em&gt;, &lt;em&gt;IROS, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8968170?casa_token=ejW3T3WNT8IAAAAA:LN2rFtefaSnrUWMzZFIE8LJESFM6WbY3xQwjM-jzSo0fGHTBTqVjPa1f2pYrqbz1Oe173gUZ&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zswang666/Stereo-LiDAR-CCVNorm&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LidarStereoNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=3D+LiDAR+and+Stereo+Fusion+using+Stereo+Matching+Network+with+Conditional+Cost+Volume+Normalization&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pseudo-LiDAR++&lt;/strong&gt;: &lt;em&gt;&#34;Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving&#34;&lt;/em&gt;, &lt;em&gt;ICLR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1906.06310.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mileyan/Pseudo_Lidar_V2&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Pseudo-LiDAR++.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Pseudo-LiDAR++:+Accurate+Depth+for+3D+Object+Detection+in+Autonomous+Driving&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Listereo&lt;/strong&gt;: &lt;em&gt;&#34;Listereo: Generate dense depth maps from lidar and stereo imagery&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196628&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Listereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Listereo%3A+Generate+dense+depth+maps+from+lidar+and+stereo+imagery&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;S&lt;sup&gt;3&lt;/sup&gt;&lt;/strong&gt;: &lt;em&gt;&#34;S&lt;sup&gt;3&lt;/sup&gt;: Learnable sparse signal superdensity for guided depth estimation&#34;&lt;/em&gt;, CVPR, 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_S3_Learnable_Sparse_Signal_Superdensity_for_Guided_Depth_Estimation_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/S3.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=+Learnable+sparse+signal+superdensity+for+guided+depth+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LSMD-Net&lt;/strong&gt;: &lt;em&gt;&#34;LSMD-Net: LiDAR-Stereo Fusion with Mixture Density Network for Depth Sensing&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ACCV2022/papers/Yin_LSMD-Net_LiDAR-Stereo_Fusion_with_Mixture_Density_Network_for_Depth_Sensing_ACCV_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LSMD-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=LSMD-Net:+LiDAR-Stereo+Fusion+with+Mixture+Density+Network+for+Depth+Sensing&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CamLiFlow&lt;/strong&gt;: &lt;em&gt;&#34;Learning optical flow and scene flow with bidirectional camera-lidar fusion&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10310261&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MCG-NJU/CamLiFlow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CamLiFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+optical+flow+and+scene+flow+with+bidirectional+camera-lidar+fusion&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Active Disparity Sampling&lt;/strong&gt;: &lt;em&gt;&#34;Active Disparity Sampling for Stereo Matching With Adjoint Network&#34;&lt;/em&gt;, &lt;em&gt;TIP, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/83/4358840/10367813.pdf?casa_token=LTr0bj333I8AAAAA:VbxzhxvyA--lpHpIUfFhMkHADyz8v4kIq6jxdQFYgi_ruTy2ahOIZk13YjuE0ynwkHfUnxOV&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ActiveDisparitySampling.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Active+Disparity+Sampling+for+Stereo+Matching+With+Adjoint+Network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;VPP&lt;/strong&gt;: &lt;em&gt;&#34;Active Stereo Without Pattern Projector&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Bartolomei_Active_Stereo_Without_Pattern_Projector_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vppstereo.github.io/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/VPP.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Active+Stereo+Without+Pattern+Projector&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SDG-Depth&lt;/strong&gt;: &lt;em&gt;&#34;Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2404.07545.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SJTU-ViSYS/SDG-Depth&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SDG-Depth.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo-LiDAR+Depth+Estimation+with+Deformable+Propagation+and+Learned+Disparity-Depth+Conversion&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Pattern Projection-Based Stereo Networks&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ActiveStereoNet&lt;/strong&gt;: &lt;em&gt;&#34;ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo Systems&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Yinda_Zhang_Active_Stereo_Net_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ActiveStereoNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=ActiveStereoNet:+End-to-End+Self-Supervised+Learning+for+Active+Stereo+Systems&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Polka Lines&lt;/strong&gt;: &lt;em&gt;&#34;Polka Lines: Learning Structured Illumination and Reconstruction for Active Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Baek_Polka_Lines_Learning_Structured_Illumination_and_Reconstruction_for_Active_Stereo_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Polka.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Polka+Lines:+Learning+Structured+Illumination+and+Reconstruction+for+Active+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Activezero&lt;/strong&gt;: &lt;em&gt;&#34;Activezero: Mixed domain learning for active stereovision with zero annotation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_ActiveZero_Mixed_Domain_Learning_for_Active_Stereovision_With_Zero_Annotation_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haosulab/ActiveZero/tree/master&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Activezero.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Activezero:+Mixed+domain+learning+for+active+stereovision+with+zero+annotation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MonoStereoFusion&lt;/strong&gt;: &lt;em&gt;&#34;Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Depth_Estimation_by_Combining_Binocular_Stereo_and_Monocular_Structured-Light_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YuhuaXu/MonoStereoFusion&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MonoStereoFusion.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Depth+Estimation+by+Combining+Binocular+Stereo+and+Monocular+Structured-Light&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Activezero++&lt;/strong&gt;: &lt;em&gt;&#34;Activezero++: Mixed domain learning stereo and confidence-based depth completion with zero annotation&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219021&amp;amp;casa_token=tEPEB1O5d74AAAAA:YLArSfK76hUPakJOvHaWTylzix6QgWx6MgSprE9AhUtIpp79yXFYFOXe0H9ahHEByzJsDpY&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Activezero++.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Activezero++:+Mixed+domain+learning+stereo+and+confidence-based+depth+completion+with+zero+annotation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Cross-Spectral Stereo Networks&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CS-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Deep material-aware cross-spectral stereo matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhi_Deep_Material-Aware_Cross-Spectral_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tiancheng-zhi/cs-stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CS-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+material-aware+cross-spectral+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UCSS&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised cross-spectral stereo matching by learning to synthesize&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/download/4894/4767&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rish-av/cross_spectral_stereo&#34;&gt;Code - Unofficial&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UCSS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+cross-spectral+stereo+matching+by+learning+to+synthesize&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SS-MCE&lt;/strong&gt;: &lt;em&gt;&#34;There and back again: Self-supervised multispectral correspondence estimation&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561621&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SS-MCE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=There+and+back+again:+Self-supervised+multispectral+correspondence+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RGB-MS&lt;/strong&gt;: &lt;em&gt;&#34;RGB-Multispectral matching: Dataset, learning methodology, evaluation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Tosi_RGB-Multispectral_Matching_Dataset_Learning_Methodology_Evaluation_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/rgb-ms-web/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RGB-MS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=RGB-Multispectral+matching:+Dataset,+learning+methodology,+evaluation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DPS-Net&lt;/strong&gt;: &lt;em&gt;&#34;DPS-Net: Deep Polarimetric Stereo Depth Estimation&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_DPS-Net_Deep_Polarimetric_Stereo_Depth_Estimation_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Ethereal-Tian/DPS-Net&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DPS-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=DPS-Net:+Deep+Polarimetric+Stereo+Depth+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CrossSP&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised Cross-Spectrum Depth Estimation by Visible-Light and Thermal Cameras&#34;&lt;/em&gt;, &lt;em&gt;T-ITS, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10146199&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/whitecrow1027/CrossSP_Depth&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CrossSP.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+Cross-Spectrum+Depth+Estimation+by+Visible-Light+and+Thermal+Cameras&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Event Stereo Networks&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Event-IntensityStereo&lt;/strong&gt;: &lt;em&gt;&#34;Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Mostafavi_Event-Intensity_Stereo_Estimating_Depth_by_the_Best_of_Both_Worlds_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yonseivnl/se-cff&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Event-IntensityStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Event-Intensity+Stereo:+Estimating+Depth+by+the+Best+of+Both+Worlds&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SE-CFF&lt;/strong&gt;: &lt;em&gt;&#34;Stereo Depth From Events Cameras: Concentrate and Focus on the Future&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Nam_Stereo_Depth_From_Events_Cameras_Concentrate_and_Focus_on_the_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yonseivnl/se-cff&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SE-CFF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo+Depth+From+Events+Cameras:+Concentrate+and+Focus+on+the+Future&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SCSNet&lt;/strong&gt;: &lt;em&gt;&#34;Selection and Cross Similarity for Event-Image Deep Stereo&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920467.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Chohoonhee/SCSNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SCSNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Selection+and+Cross+Similarity+for+Event-Image+Deep+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;DTC-SPADE&lt;/strong&gt;: &lt;em&gt;&#34;Discrete Time Convolution for Fast Event-Based Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Discrete_Time_Convolution_for_Fast_Event-Based_Stereo_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DTC-SPADE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Discrete+Time+Convolution+for+Fast+Event-Based+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;EFS&lt;/strong&gt;: &lt;em&gt;&#34;Event-image fusion stereo using cross-modality feature propagation&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2022&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/download/19923/19682&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EFS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Event-Image+Fusion+Stereo+Using+Cross-Modality+Feature+Propagation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ADES&lt;/strong&gt;: &lt;em&gt;&#34;Learning Adaptive Dense Event Stereo From the Image Domain&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Learning_Adaptive_Dense_Event_Stereo_From_the_Image_Domain_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ADES.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+Adaptive+Dense+Event+Stereo+From+the+Image+Domain&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SAFE&lt;/strong&gt;: &lt;em&gt;&#34;Depth From Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach&#34;&lt;/em&gt;, &lt;em&gt;WACV, 2024&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SAFE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Depth+From+Asymmetric+Frame-Event+Stereo:+A+Divide-and-Conquer+Approach&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Gated Stereo Networks&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;GatedStereo&lt;/strong&gt;: &lt;em&gt;&#34;Gated Stereo: Joint Depth Estimation from Gated and Wide-Baseline Active Stereo Cues&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content/CVPR2023/papers/Walz_Gated_Stereo_Joint_Depth_Estimation_From_Gated_and_Wide-Baseline_Active_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://light.princeton.edu/publication/gatedstereo/&#34;&gt;WebPage&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Gated.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Gated+Stereo:+Joint+Depth+Estimation+from+Gated+and+Wide-Baseline+Active+Stereo+Cues&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Stereo Networks with Echoes &lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;StereoEchoes&lt;/strong&gt;: &lt;em&gt;&#34;Stereo Depth Estimation with Echoes&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870489.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/StereoEchoes.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo+Depth+Estimation+with+Echoes&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Challenges &amp;amp; Solutions&lt;/h3&gt; &#xA;&lt;details open id=&#34;over-smoothing&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Addressing the Over-Smoothing Issue&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SM-CDE&lt;/strong&gt;: &lt;em&gt;&#34;On the over-smoothing problem of cnn based disparity estimation&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_On_the_Over-Smoothing_Problem_of_CNN_Based_Disparity_Estimation_ICCV_2019_paper.html&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SM-CDE.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=On+the+over-smoothing+problem+of+cnn+based+disparity+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;AcfNet&lt;/strong&gt;: &lt;em&gt;&#34;Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;AAAI, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/6991/6845&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/youmi-zym/AcfNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/WaveletStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Adaptive+Unimodal+Cost+Volume+Filtering+for+Deep+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;CDN&lt;/strong&gt;: &lt;em&gt;&#34;Wasserstein Distances for Stereo Disparity Estimation&#34;&lt;/em&gt;, &lt;em&gt;NeurIPS, 2020&lt;/em&gt;. [&lt;a href=&#34;https://papers.nips.cc/paper/2020/file/fe7ecc4de28b2c83c016b5c6c2acd826-Paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Div99/W-Stereo-Disp&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CDN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Wasserstein+Distances+for+Stereo+Disparity+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SMD-Nets&lt;/strong&gt;: &lt;em&gt;&#34;SMD-Nets: Stereo Mixture Density Networks&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;http://www.cvlibs.net/publications/Tosi2021CVPR.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/SMD-Nets&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SMD-Nets.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=SMD-Nets:+Stereo+Mixture+Density+Networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;NDR&lt;/strong&gt;: &#34;&lt;em&gt;Neural disparity refinement for arbitrary resolution stereo&lt;/em&gt;&#34;, &lt;em&gt;3DV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9665913?casa_token=3rm4WpqLb_QAAAAA:5Sa0RO547j8LsaEYUeppzB33gZJg5Y3tfiPVwM9rzs9MEAuoHSta0Kdw3Cm9NrtfOOdFkIwp&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/neural-disparity-refinement-web/&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NDR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Neural+disparity+refinement+for+arbitrary+resolution+stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;LaC&lt;/strong&gt;: &#34;&lt;em&gt;Local similarity pattern and cost self-reassembling for deep stereo matching networks&lt;/em&gt;&#34;, &lt;em&gt;AAAI, 2022&lt;/em&gt;. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/20056/19815&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SpadeLiu/Lac-GwcNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LaC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Local+similarity+pattern+and+cost+self-reassembling+for+deep+stereo+matching+networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ADL&lt;/strong&gt;: &#34;&lt;em&gt;Adaptive Multi-Modal Cross-Entropy Loss for Stereo Matching&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2306.15612.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xxxupeng/ADL&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ADL.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Adaptive+Multi-Modal+Cross-Entropy+Loss+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;missing-gt&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Missing Ground Truth Depth&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Self-Supervised&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;MonoDepth/StereoDepth&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised monocular depth estimation with left-right consistency&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mrharicot/monodepth&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MonoDepth.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+monocular+depth+estimation+with+left-right+consistency&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;USM&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised learning of stereo matching&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Unsupervised_Learning_of_ICCV_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Flow2Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+learning+of+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;OASM-Net&lt;/strong&gt;: &lt;em&gt;&#34;Occlusion aware stereo matching via cooperative unsupervised learning&#34;&lt;/em&gt;, &lt;em&gt;ACCV, 2018&lt;/em&gt;. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-20876-9_13&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/OASM-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Occlusion+aware+stereo+matching+via+cooperative+unsupervised+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UnOS&lt;/strong&gt;: &lt;em&gt;&#34;UnOS: Unified Unsupervised Optical-Flow and Stereo-Depth Estimation by Watching Videos&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/34/9185119/08769907.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/baidu-research/UnDepthflow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UnOS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=UnOS:+Unified+Unsupervised+Optical-Flow+and+Stereo-Depth+Estimation+by+Watching+Videos&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;BridgeDepthFlow&lt;/strong&gt;: &lt;em&gt;&#34;Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Lai_Bridging_Stereo_Matching_and_Optical_Flow_via_Spatiotemporal_Correspondence_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lelimite4444/BridgeDepthFlow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/BridgeDepthFlow.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Bridging+Stereo+Matching+and+Optical+Flow+via+Spatiotemporal+Correspondence&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Correspondence Consistency&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised stereo matching using confidential correspondence consistency&#34;&lt;/em&gt;, &lt;em&gt;T-ITS, 2019&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/6979/4358928/08721668.pdf?casa_token=YfFwIZKeGJ4AAAAA:LCkTQefrcYIOljN6Yyc55dxXCUthgkrnmJLwu7gSjD_Cd65HZ_EOpGDJq49V5GylbgxtGKG4&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Correspondence_Consistency.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+stereo+matching+using+confidential+correspondence+consistency&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flow2Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Flow2Stereo_Effective_Self-Supervised_Learning_of_Optical_Flow_and_Stereo_Matching_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ppliuboy/Flow2Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Flow2Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Flow2Stereo:+Effective+Self-Supervised+Learning+of+Optical+Flow+and+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;PASMNet&lt;/strong&gt;: &lt;em&gt;&#34;Parallax attention for unsupervised stereo correspondence learning&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9206116&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/The-Learning-And-Vision-Atelier-LAVA/PAM&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PASMNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Parallax+attention+for+unsupervised+stereo+correspondence+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MultiscopicVision&lt;/strong&gt;: &lt;em&gt;&#34;Stereo matching by self-supervision of multiscopic vision&#34;&lt;/em&gt;, &lt;em&gt;IROS, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9206116&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/multiscopic&#34;&gt;WebPage&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MultiscopicVision.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo+matching+by+self-supervision+of+multiscopic+vision&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Feature-Level Collaboration&lt;/strong&gt;: &lt;em&gt;&#34;Feature-Level Collaboration: Joint Unsupervised Learning of Optical Flow, Stereo Depth and Camera Motion&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Chi_Feature-Level_Collaboration_Joint_Unsupervised_Learning_of_Optical_Flow_Stereo_Depth_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Feature-Level_Collaboration.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Occlusion-Aware Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised Occlusion-Aware Stereo Matching With Directed Disparity Smoothing&#34;&lt;/em&gt;, &lt;em&gt;T-ITS, 2022&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9404889&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Occlusion-Aware_Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+Occlusion-Aware+Stereo+Matching+With+Directed+Disparity+Smoothing&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Cross-Framework/Proxy Supervision&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reversing-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Reversing the cycle: self-supervised deep stereo through enhanced monocular distillation&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2008.07130.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FilippoAleotti/Reversing&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Reversing-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Reversing+the+cycle:+self-supervised+deep+stereo+through+enhanced+monocular+distillation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Revealing-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Revealing the Reciprocal Relations between Self-Supervised Stereo and Monocular Depth Estimation&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Revealing_the_Reciprocal_Relations_Between_Self-Supervised_Stereo_and_Monocular_Depth_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Revealing-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Revealing+the+Reciprocal+Relations+between+Self-Supervised+Stereo+and+Monocular+Depth+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Two-in-One&lt;/strong&gt;: &lt;em&gt;&#34;Two-in-one depth: Bridging the gap between monocular and binocular self-supervised depth estimation&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Two-in-One_Depth_Bridging_the_Gap_Between_Monocular_and_Binocular_Self-Supervised_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZM-Zhou/TiO-Depth_pytorch&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Two-in-One.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Two-in-one+depth:+Bridging+the+gap+between+monocular+and+binocular+self-supervised+depth+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;NeRF-Supervised Stereo&lt;/strong&gt;: &#34;&lt;em&gt;NeRF-Supervised Deep Stereo&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nerfstereo.github.io/&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NS-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;domain-shift&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Domain Shift&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Zero-shot Generalization&lt;/summary&gt;&#xA;   &lt;ul&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Domain-Agnostic Feature Modeling&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;DSM-Net&lt;/strong&gt;: &#34;&lt;em&gt;Domain-invariant Stereo Matching Networks&lt;/em&gt;&#34;, &lt;em&gt;ECCV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470409.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/feihuzhang/DSMNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DSM-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Domain-invariant+Stereo+Matching+Networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;FCStereo&lt;/strong&gt;: &#34;&lt;em&gt;Revisiting Domain Generalized Stereo Matching Networks From a Feature Consistency Perspective&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Revisiting_Domain_Generalized_Stereo_Matching_Networks_From_a_Feature_Consistency_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jiaw-z/FCStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FCStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DSM-Net.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Revisiting+Domain+Generalized+Stereo+Matching+Networks+From+a+Feature+Consistency+Perspective&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;GraftNet&lt;/strong&gt;: &#34;&lt;em&gt;GraftNet: Towards Domain Generalized Stereo Matching With a Broad-Spectrum and Task-Oriented Feature&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_GraftNet_Towards_Domain_Generalized_Stereo_Matching_With_a_Broad-Spectrum_and_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SpadeLiu/Graft-PSMNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GraftNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=GraftNet%3A+Towards+Domain+Generalized+Stereo+Matching+With+a+Broad-Spectrum+and+Task-Oriented+Feature&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;ITSA&lt;/strong&gt;: &#34;&lt;em&gt;ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Chuah_ITSA_An_Information-Theoretic_Approach_to_Automatic_Shortcut_Avoidance_and_Domain_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/waychin-weiqin/ITSA&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ITSA.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=ITSA%3A+An+Information-Theoretic+Approach+to+Automatic+Shortcut+Avoidance+and+Domain+Generalization+in+Stereo+Matching+Network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;HVT&lt;/strong&gt;: &#34;&lt;em&gt;Domain Generalized Stereo Matching via Hierarchical Visual Transformation&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cty8998/HVT-PSMNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/HVT.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Domain+Generalized+Stereo+Matching+via+Hierarchical+Visual+Transformation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;MRL-Stereo&lt;/strong&gt;: &#34;&lt;em&gt;Masked representation learning for domain generalized stereo matching&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Rao_Masked_Representation_Learning_for_Domain_Generalized_Stereo_Matching_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MRL-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Masked+representation+learning+for+domain+generalized+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Non-parametric Cost Volumes&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;MS-Nets&lt;/strong&gt;: &#34;&lt;em&gt;Matching-space Stereo Networks for Cross-domain Generalization&lt;/em&gt;&#34;, &lt;em&gt;3DV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://mordohai.github.io/public/Cai_MatchingSpaceStereo20.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ccj5351/MS-Nets&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MS-Nets.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Matching-space+Stereo+Networks+for+Cross-domain+Generalization&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;ARStereo&lt;/strong&gt;: &#34;&lt;em&gt;Revisiting Non-Parametric Matching Cost Volumes for Robust and Generalizable Stereo Matching&lt;/em&gt;&#34;, &lt;em&gt;NeurIPS, 2022&lt;/em&gt;. [&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/6794f555524c9069e26970a408d353cc-Paper-Conference.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kelkelcheng/AdversariallyRobustStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ARStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Revisiting+Non-Parametric+Matching+Cost+Volumes+for+Robust+and+Generalizable+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Integration of Additional Geometric Cues&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;NDR&lt;/strong&gt;: &#34;&lt;em&gt;Neural disparity refinement for arbitrary resolution stereo&lt;/em&gt;&#34;, &lt;em&gt;3DV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9665913?casa_token=3rm4WpqLb_QAAAAA:5Sa0RO547j8LsaEYUeppzB33gZJg5Y3tfiPVwM9rzs9MEAuoHSta0Kdw3Cm9NrtfOOdFkIwp&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/neural-disparity-refinement-web/&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NDR.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Neural+disparity+refinement+for+arbitrary+resolution+stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;EVHS&lt;/strong&gt;: &#34;&lt;em&gt;Expansion of Visual Hints for Improved Generalization in Stereo Matching&lt;/em&gt;&#34;, &lt;em&gt;WACV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2023/papers/Pilzer_Expansion_of_Visual_Hints_for_Improved_Generalization_in_Stereo_Matching_WACV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EVHS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Expansion+of+Visual+Hints+for+Improved+Generalization+in+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Synthetic Data Generation and Domain Translation&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;StereoGAN&lt;/strong&gt;: &#34;&lt;em&gt;StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_StereoGAN_Bridging_Synthetic-to-Real_Domain_Gap_by_Joint_Optimization_of_Domain_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ruiliu-ai/StereoGAN&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/StereoGAN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=StereoGAN%3A+Bridging+Synthetic-to-Real+Domain+Gap+by+Joint+Optimization+of+Domain+Translation+and+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;LSSI&lt;/strong&gt;: &#34;&lt;em&gt;Learning Stereo from Single Images&lt;/em&gt;&#34;, &lt;em&gt;ECCV, 2020&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460698.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nianticlabs/stereo-from-mono/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LSSI.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+Stereo+from+Single+Images&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;FoggyStereo&lt;/strong&gt;: &#34;&lt;em&gt;FoggyStereo: Stereo Matching with Fog Volume Representation&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_FoggyStereo_Stereo_Matching_With_Fog_Volume_Representation_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nianticlabs/stereo-from-mono/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FoggyStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=FoggyStereo%3A+Stereo+Matching+with+Fog+Volume+Representation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;      &lt;li&gt; &lt;p&gt;&lt;strong&gt;NeRF-Supervised Stereo&lt;/strong&gt;: &#34;&lt;em&gt;NeRF-Supervised Deep Stereo&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nerfstereo.github.io/&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NS-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=NeRF-Supervised+Deep+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Knowledge Transfer&lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;DKT-Stereo&lt;/strong&gt;: &#34;&lt;em&gt;Robust Synthetic-to-Real Transfer for Stereo Matching&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2403.07705&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jiaw-z/DKT-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DKT-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Robust+Synthetic-to-Real+Transfer+for+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;    &lt;details open&gt; &#xA;     &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Data Augmentation Analysis &lt;/summary&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;NLCA-Net_v2&lt;/strong&gt;: &#34;&lt;em&gt;Rethinking training strategy in stereo matching&lt;/em&gt;&#34;, &lt;em&gt;TNNLS, 2022&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/5962385/10273172/09709604.pdf?casa_token=Bs47IDDe0r8AAAAA:J0ru7iqCpc1X1HGE3xsJ6u5I1gTAU8Z_wFAE_4QYg0-50X0dCUHvqrp7l8FZVsROvqZYwayc&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Archaic-Atom/NLCA-Net_v2&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NLCA-Net_v2.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Rethinking+Training+Strategy+in+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;     &lt;/ul&gt; &#xA;    &lt;/details&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Offline Adaptation&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open-World Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Open-world stereo video matching with deep rnn&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2018&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Yiran_Zhong_Open-World_Stereo_Video_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Open-World.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Open-world+stereo+video+matching+with+deep+rnn&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Confidence-guided Adaptation&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised adaptation for deep stereo&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Confidence_guided_Adaptation_0.txt&#34;&gt;Bibtex1&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Confidence_guided_Adaptation_1.txt&#34;&gt;Bibtex2&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ZOLE&lt;/strong&gt;: &lt;em&gt;&#34;Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domain&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2018&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Pang_Zoom_and_Learn_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jiahaopang/zole&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/v.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+adaptation+for+deep+stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;AdaStereo&lt;/strong&gt;: &lt;em&gt;&#34;AdaStereo: A Simple and Efficient Approach for Adaptive Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Song_AdaStereo_A_Simple_and_Efficient_Approach_for_Adaptive_Stereo_Matching_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/AdaStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=AdaStereo:+A+Simple+and+Efficient+Approach+for+Adaptive+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UnDAF&lt;/strong&gt;: &lt;em&gt;&#34;UnDAF: A General Unsupervised Domain Adaptation Framework for Disparity or Optical Flow Estimation&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9811811&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/undaf&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UnDAF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=UnDAF:+A+General+Unsupervised+Domain+Adaptation+Framework+for+Disparity+or+Optical+Flow+Estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UCFNet&lt;/strong&gt;: &lt;em&gt;&#34;Digging Into Uncertainty-Based Pseudo-Label for Robust Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2023&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/2307.16509.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gallenszl/UCFNet?tab=readme-ov-file&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UCFNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Digging+Into+Uncertainty-Based+Pseudo-Label+for+Robust+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;StereoFlowGAN&lt;/strong&gt;: &lt;em&gt;&#34;StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation&#34;&lt;/em&gt;, &lt;em&gt;BMVC, 2023&lt;/em&gt;. [&lt;a href=&#34;https://papers.bmvc2023.org/0240.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/StereoFlowGAN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=StereoFlowGAN:+Co-training+for+Stereo+and+Flow+with+Unsupervised+Domain+Adaptation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Online Continual Adaptation&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;span&gt;üö©&lt;/span&gt; &lt;strong&gt;MadNet&lt;/strong&gt;: &lt;em&gt;&#34;Real-Time Self-Adaptive Deep Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo?tab=readme-ov-file&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MadNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Real-Time+Self-Adaptive+Deep+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning2Adapt&lt;/strong&gt;: &lt;em&gt;&#34;Learning to adapt for stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2019&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Tonioni_Learning_to_Adapt_for_Stereo_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CVLAB-Unibo/Learning2AdaptForStereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Learning2Adapt.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+adapt+for+stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Continual Adaptation for Deep Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Continual adaptation for deep stereo&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/document/9418523?denied=&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo?tab=readme-ov-file&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ContinualAdaptation.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Continual+adaptation+for+deep+stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RAG&lt;/strong&gt;: &lt;em&gt;&#34;Continual Stereo Matching of Continuous Driving Scenes With Growing Architecture&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Continual_Stereo_Matching_of_Continuous_Driving_Scenes_With_Growing_Architecture_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RAG.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Continual+Stereo+Matching+of+Continuous+Driving+Scenes+With+Growing+Architecture&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;PointFix&lt;/strong&gt;: &lt;em&gt;&#34;PointFix: Learning to Fix Domain Bias for Robust Online Stereo Adaptation&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2022&lt;/em&gt;. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980557.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PointFix.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=PointFix:+Learning+to+Fix+Domain+Bias+for+Robust+Online+Stereo+Adaptation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;FedStereo&lt;/strong&gt;: &lt;em&gt;&#34;Federated Online Adaptation for Deep Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2024&lt;/em&gt;. [&lt;a href=&#34;https://fedstereo.github.io/&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FedStereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Federated+Online+Adaptation+for+Deep+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RAG-Continual&lt;/strong&gt;: &lt;em&gt;&#34;Reusable Architecture Growth for Continual Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;TPAMI, 2024&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10475553&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chzhang18/RAG&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RAG-Continual.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Reusable+Architecture+Growth+for+Continual+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt;&#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;tom&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Transparent and Reflective (ToM) Surfaces &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DDF&lt;/strong&gt;: &lt;em&gt;&#34;Deep Depth Fusion for Black, Transparent, Reflective and Texture-Less Objects&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2020&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9196894&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chzhang18/RAG&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DDF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+Depth+Fusion+for+Black,+Transparent,+Reflective+and+Texture-Less+Objects&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;TA-Stereo&lt;/strong&gt;: &lt;em&gt;&#34;Transparent Objects: A Corner Case in Stereo Matching&#34;&lt;/em&gt;, &lt;em&gt;ICRA, 2023&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=10161385&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/mias.group/TA-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/TA-Stereo.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Transparent+Objects:+A+Corner+Case+in+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Depth4ToM&lt;/strong&gt;: &lt;em&gt;&#34;Learning Depth Estimation for Transparent and Mirror Surfaces&#34;&lt;/em&gt;, &lt;em&gt;ICCV, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Costanzino_Learning_Depth_Estimation_for_Transparent_and_Mirror_Surfaces_ICCV_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/Depth4ToM/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Depth4ToM.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+Depth+Estimation+for+Transparent+and+Mirror+Surfaces&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open id=&#34;asymmetric&#34;&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Asymmetric Stereo &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visually-Imbalanced Stereo&lt;/strong&gt;: &#34;&lt;em&gt;Visually Imbalanced Stereo Matching&lt;/em&gt;&#34;, &lt;em&gt;CVPR, 2020&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Visually_Imbalanced_Stereo_Matching_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DandilionLau/Visually-Imbalanced-Stereo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/VI-SM.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;NDR&lt;/strong&gt;: &#34;&lt;em&gt;Neural disparity refinement for arbitrary resolution stereo&lt;/em&gt;&#34;, &lt;em&gt;3DV, 2021&lt;/em&gt;. [&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9665913?casa_token=3rm4WpqLb_QAAAAA:5Sa0RO547j8LsaEYUeppzB33gZJg5Y3tfiPVwM9rzs9MEAuoHSta0Kdw3Cm9NrtfOOdFkIwp&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cvlab-unibo.github.io/neural-disparity-refinement-web/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/NDR.txt&#34;&gt;Bibtex&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;DA-AS&lt;/strong&gt;: &lt;em&gt;&#34;Degradation-agnostic Correspondence from Resolution-asymmetric Stereo&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2022&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Degradation-Agnostic_Correspondence_From_Resolution-Asymmetric_Stereo_CVPR_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/DA-AS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Degradation-agnostic+Correspondence+from+Resolution-asymmetric+Stereo&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;SASS&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised Deep Asymmetric Stereo Matching with Spatially-Adaptive Self-Similarity&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2023&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Unsupervised_Deep_Asymmetric_Stereo_Matching_With_Spatially-Adaptive_Self-Similarity_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SASS.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+Deep+Asymmetric+Stereo+Matching+with+Spatially-Adaptive+Self-Similarity&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;   &#xA;&lt;h3&gt;Confidence Estimation&lt;/h3&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Machine Learning Approaches&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Disparity-based&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ENS7&lt;/strong&gt;: &lt;em&gt;&#34;Ensemble learning for confidence measures in stereo vision&#34;&lt;/em&gt;, CVPR, 2013. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2013/papers/Haeusler_Ensemble_Learning_for_2013_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ENS23.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Ensemble+learning+for+confidence+measures+in+stereo+vision&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;O1&lt;/strong&gt;: &lt;em&gt;&#34;Learning a general-purpose confidence measure based on o (1) features and a smarter aggregation strategy for semi global matching&#34;&lt;/em&gt;, 3DV, 2016. [&lt;a href=&#34;http://vision.disi.unibo.it/~mpoggi/papers/3dv2016_o1.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/O1.txt&#34;&gt;Bibtex1&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/O1_1.txt&#34;&gt;Bibtex2&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+a+general-purpose+confidence+measure+based+on+o+%281%29+features+and+a+smarter+aggregation+strategy+for+semi+global+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Cost Volume-based&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ENS23&lt;/strong&gt;: &lt;em&gt;&#34;Ensemble learning for confidence measures in stereo vision&#34;&lt;/em&gt;, CVPR, 2013. [&lt;a href=&#34;http://openaccess.thecvf.com/content_cvpr_2013/papers/Haeusler_Ensemble_Learning_for_2013_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ENS23.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+a+general-purpose+confidence+measure+based+on+o+(1)+features+and+a+smarter+aggregation+strategy+for+semi+global+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;GCP&lt;/strong&gt;: &lt;em&gt;&#34;Learning to detect ground control points for improving the accuracy of stereo matching&#34;&lt;/em&gt;, CVPR, 2014. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2014/papers/Spyropoulos_Learning_to_Detect_2014_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GCP.txt&#34;&gt;Bibtex1&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/GCP1.txt&#34;&gt;Bibtex2&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+detect+ground+control+points+for+improving+the+accuracy+of+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LEV&lt;/strong&gt;: &lt;em&gt;&#34;Leveraging stereo matching with learning-based confidence measures&#34;&lt;/em&gt;, CVPR, 2015. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Park_Leveraging_Stereo_Matching_2015_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LEV.txt&#34;&gt;Bibtex1&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LEV1.txt&#34;&gt;Bibtex2&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Leveraging+stereo+matching+with+learning-based+confidence+measures&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;FA&lt;/strong&gt;: &lt;em&gt;&#34;Feature augmentation for learning confidence measure in stereo matching&#34;&lt;/em&gt;, TIP, 2017. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Park_Leveraging_Stereo_Matching_2015_CVPR_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/FA.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+detect+ground+control+points+for+improving+the+accuracy+of+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;SGM-specific&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;SGMForest&lt;/strong&gt;: &lt;em&gt;&#34;Learning to fuse proposals from multiple scanline optimizations in semi-global matching&#34;&lt;/em&gt;, ECCV, 2018. [&lt;a href=&#34;http://vision.disi.unibo.it/~mpoggi/papers/3dv2016_o1.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SGMForest.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+fuse+proposals+from+multiple+scanline+optimizations+in+semi-global+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Deep Learning Approaches&lt;/summary&gt;&#xA; &lt;ul&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Disparity-based&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CCNN&lt;/strong&gt;: &lt;em&gt;&#34;Learning from scratch a confidence measure&#34;&lt;/em&gt;, BMVC, 2016. [&lt;a href=&#34;https://www.researchgate.net/profile/Stefano-Mattoccia-2/publication/317191595_Learning_from_scratch_a_confidence_measure/links/593f9db3a6fdcc1b10aac9ec/Learning-from-scratch-a-confidence-measure.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/CCNN-Tensorflow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CCNN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+from+scratch+a+confidence+measure&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;PBCP&lt;/strong&gt;: &lt;em&gt;&#34;Patch Based Confidence Prediction for Dense Disparity Map&#34;&lt;/em&gt;, BMVC, 2016. [&lt;a href=&#34;https://www.cvlibs.net/projects/autonomous_vision_survey/literature/Seki2016BMVC.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/PBCP.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Patch+Based+Confidence+Prediction+for+Dense+Disparity+Map&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;EFN/LFN&lt;/strong&gt;: &lt;em&gt;&#34;Stereo matching confidence learning based on multi-modal convolution neural networks&#34;&lt;/em&gt;, RFMI, 2017. [&lt;a href=&#34;http://www.arts-pi.org.tn/rfmi2017/papers/10_CameraReadySubmission_llncs2e%20(3).pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EFN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo+matching+confidence+learning+based+on+multi-modal+convolution+neural+networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MMC&lt;/strong&gt;: &lt;em&gt;&#34;Learning confidence measures by multi-modal convolutional neural networks&#34;&lt;/em&gt;, WACV, 2018. [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/WACV&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MMC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+confidence+measures+by+multi-modal+convolutional+neural+networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LGC/ConfNet&lt;/strong&gt;: &lt;em&gt;&#34;Beyond local reasoning for stereo confidence estimation with deep learning&#34;&lt;/em&gt;, ECCV, 2018. [&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Fabio_Tosi_Beyond_local_reasoning_ECCV_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/LGC-Tensorflow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LGC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Beyond+local+reasoning+for+stereo+confidence+estimation+with+deep+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;SEDNet&lt;/strong&gt;: &lt;em&gt;&#34;Learning the distribution of errors in stereo matching for joint disparity and uncertainty estimation&#34;&lt;/em&gt;, CVPR, 2023. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_the_Distribution_of_Errors_in_Stereo_Matching_for_Joint_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lly00412/SEDNet&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SEDNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+the+distribution+of+errors+in+stereo+matching+for+joint+disparity+and+uncertainty+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Cost Volume-based&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;RCN&lt;/strong&gt;: &lt;em&gt;&#34;Improved stereo matching with constant highway networks and reflective confidence learning&#34;&lt;/em&gt;, CVPR, 2017. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Shaked_Improved_Stereo_Matching_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/amitshaked/resmatch&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/RCN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Improved+stereo+matching+with+constant+highway+networks+and+reflective+confidence+learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;MPN&lt;/strong&gt;: &lt;em&gt;&#34;Deep stereo confidence prediction for depth estimation&#34;&lt;/em&gt;, ICIP, 2017. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/8267582/8296222/08296430.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MPN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Deep+stereo+confidence+prediction+for+depth+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;UCN&lt;/strong&gt;: &lt;em&gt;&#34;Unified confidence estimation networks for robust stereo matching&#34;&lt;/em&gt;, TIP, 2018. [&lt;a href=&#34;https://ieeexplore.ieee.org/iel7/83/4358840/08510870.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/UCN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unified+confidence+estimation+networks+for+robust+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;LAF&lt;/strong&gt;: &lt;em&gt;&#34;Laf-net: Locally adaptive fusion networks for stereo confidence estimation&#34;&lt;/em&gt;, CVPR, 2019. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_LAF-Net_Locally_Adaptive_Fusion_Networks_for_Stereo_Confidence_Estimation_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/LAF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Laf-net:+Locally+adaptive+fusion+networks+for+stereo+confidence+estimation&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CRNN&lt;/strong&gt;: &lt;em&gt;&#34;Pixel-Wise Confidences for Stereo Disparities Using Recurrent Neural Networks&#34;&lt;/em&gt;, BMVC, 2019. [&lt;a href=&#34;https://publica.fraunhofer.de/bitstreams/c1f200e0-49e6-488c-84de-d217a550bdf6/download&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CRNN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Pixel-Wise+Confidences+for+Stereo+Disparities+Using+Recurrent+Neural+Networks&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ACN&lt;/strong&gt;: &lt;em&gt;&#34;Adversarial confidence estimation networks for robust stereo matching&#34;&lt;/em&gt;, T-ITS, 2020. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_LAF-Net_Locally_Adaptive_Fusion_Networks_for_Stereo_Confidence_Estimation_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/ACN.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Adversarial+confidence+estimation+networks+for+robust+stereo+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;CVA&lt;/strong&gt;: &lt;em&gt;&#34;Cnn-based cost volume analysis as confidence measure for dense matching&#34;&lt;/em&gt;, ICCVW, 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCVW_2019/papers/3DRW/Mehltretter_CNN-Based_Cost_Volume_Analysis_as_Confidence_Measure_for_Dense_Matching_ICCVW_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CVA.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Cnn-based+cost+volume+analysis+as+confidence+measure+for+dense+matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Disparity Plane Sweep&lt;/strong&gt;: &lt;em&gt;&#34;Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep&#34;&lt;/em&gt;, AAAI, 2024. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/28071/28148&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Disparity_Plane_Sweep.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Modeling+Stereo-Confidence+Out+of+the+End-to-End+Stereo-Matching+Network+via+Disparity+Plane+Sweep&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Multiple Confidence Fusion&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning Local Consistency&lt;/strong&gt;: &lt;em&gt;&#34;Learning to predict stereo reliability enforcing local consistency of confidence maps&#34;&lt;/em&gt;, CVPR, 2017. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Poggi_Learning_to_Predict_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/++.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+to+predict+stereo+reliability+enforcing+local+consistency+of+confidence+maps&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;&lt;strong&gt;EMC&lt;/strong&gt;: &lt;em&gt;&#34;Even More Confident Predictions With Deep Machine-Learning&#34;&lt;/em&gt;, CVPRW, 2017. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Poggi_Learning_to_Predict_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/EMC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Even+More+Confident+Predictions+With+Deep+Machine-Learning&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary style=&#34;font-size: larger; font-weight: bold;&#34;&gt;Sensor-based&lt;/summary&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Lidar-Confidence&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised confidence for lidar depth maps and applications&#34;&lt;/em&gt;, IROS, 2022. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9981654&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Lidar-Confidence.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://github.com/andreaconti/lidar-confidence&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+confidence+for+LiDAR+depth+maps+and+applications&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;p&gt;(Not an exhaustive list)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep3d&lt;/strong&gt;: &lt;em&gt;&#34;Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2016&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1604.03650.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/piiswrong/deep3d&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Deep3d.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=eep3d%3A+Fully+automatic+2d-to-3d+video+conversion+with+deep+convolutional+neural+network&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geometry to the Rescue&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised cnn for single view depth estimation: Geometry to the rescue&#34;&lt;/em&gt;, &lt;em&gt;ECCV, 2016&lt;/em&gt;. [&lt;a href=&#34;https://arxiv.org/pdf/1603.04992&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Geometry_to_the_Rescue.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+CNN+for+single+view+depth+estimation%3A+Geometry+to+the+rescu&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MonoDepth/StereoDepth&lt;/strong&gt;: &lt;em&gt;&#34;Unsupervised monocular depth estimation with left-right consistency&#34;&lt;/em&gt;, &lt;em&gt;CVPR, 2017&lt;/em&gt;. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mrharicot/monodepth&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MonoDepth.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Unsupervised+monocular+depth+estimation+with+left-right+consistency&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SVSM&lt;/strong&gt;: &#34;Single View Stereo Matching&#34;, CVPR, 2028. [&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Single_View_Stereo_CVPR_2018_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yanqi1811/Single-View-Stereo-Matching&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SVSM.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Single+View+Stereo+Matching&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MonoResMatch&lt;/strong&gt;: &#34;Learning monocular depth estimation infusing traditional stereo knowledge&#34;, CVPR, 2019. [&lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Tosi_Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge_CVPR_2019_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fabiotosi92/monoResMatch-Tensorflow&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/MonoResMatch.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+monocular+depth+estimation+infusing+traditional+stereo+knowledge&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ida-3d&lt;/strong&gt;: &#34;Ida-3d: Instance-depth-aware 3d object detection from stereo vision for autonomous driving&#34;, CVPR, 2020. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_IDA-3D_Instance-Depth-Aware_3D_Object_Detection_From_Stereo_Vision_for_Autonomous_CVPR_2020_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/swords123/IDA-3D&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Ida-3d.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=IDA-3D%3A+Instance-Depth-Aware+3D+Object+Detection+From+Stereo+Vision+for+Autonomous+Driving&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stereopifu&lt;/strong&gt;: &#34;Stereopifu: Depth aware clothed human digitization via stereo vision&#34;, CVPR, 2021. [&lt;a href=&#34;http://openaccess.thecvf.com/content/CVPR2021/papers/Hong_StereoPIFu_Depth_Aware_Clothed_Human_Digitization_via_Stereo_Vision_CVPR_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CrisHY1995/StereoPIFu_Code&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Stereopifu.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=StereoPIFu%3A+Depth+Aware+Clothed+Human+Digitization+via+Stereo+Vision&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Smart Glasses&lt;/strong&gt;: &#34;A Practical Stereo Depth System for Smart Glasses&#34;, CVPR, 2023. [&lt;a href=&#34;http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_A_Practical_Stereo_Depth_System_for_Smart_Glasses_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Smart_Glasses.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=A+Practical+Stereo+Depth+System+for+Smart+Glasses&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cross Attention Renderer&lt;/strong&gt;: &#34;Learning to render novel views from wide-baseline stereo pairs&#34;, CVPR, 2023. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Learning_To_Render_Novel_Views_From_Wide-Baseline_Stereo_Pairs_CVPR_2023_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yilundu/cross_attention_renderer&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/Cross_Attention_Renderer.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Learning+To+Render+Novel+Views+From+Wide-Baseline+Stereo+Pairs&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SDCNet&lt;/strong&gt;: &#34;Stereo-augmented depth completion from a single rgb-lidar image&#34;, ICRA, 2021. [&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9561557&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/SDCNet.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Stereo-augmented+depth+completion+from+a+single+rgb-lidar+image&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VPPDC&lt;/strong&gt;: &#34;Revisiting Depth Completion from a Stereo Matching Perspective for Cross-domain Generalization&#34;, 3DV, 2024. [&lt;a href=&#34;https://vppdc.github.io/assets/paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vppdc.github.io/&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/VPPDC.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Revisiting+Depth+Completion+from+a+Stereo+Matching+Perspective+for+Cross-domain+Generalization&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoPoNeRF&lt;/strong&gt;: &#34;Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs&#34;, CVPR, 2024. [&lt;a href=&#34;https://arxiv.org/pdf/2312.07246v1&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KU-CVLAB/CoPoNeRF&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/fabiotosi92/Awesome-Deep-Stereo-Matching/main/bibliography/CoPoNeRF.txt&#34;&gt;Bibtex&lt;/a&gt;] [&lt;a href=&#34;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=Unifying+Correspondence%2C+Pose+and+NeRF+for+Pose-Free+Novel+View+Synthesis+from+Stereo+Pairs&amp;amp;btnG=&#34;&gt;Google Scholar&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Workshops&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NTIRE 2024: HR Depth from Images of Specular and Transparent Surfaces&lt;/strong&gt;. P. Z. Ramirez, F. Tosi, L. Di Stefano, R. Timofte A. Costanzino, M. Poggi, S. Salti, S. Mattoccia; CVPRW 2024, Seattle, US [&lt;a href=&#34;https://cvlab-unibo.github.io/booster-web/ntire.html&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NTIRE 2023: HR Depth from Images of Specular and Transparent Surfaces&lt;/strong&gt;. P. Z. Ramirez, F. Tosi, L. Di Stefano, R. Timofte A. Costanzino, M. Poggi, S. Salti, S. Mattoccia; CVPRW 2023, Vancouver, Canada [&lt;a href=&#34;https://cvlab-unibo.github.io/booster-web/ntire.html&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Robust Vision Challenge (ROB)&lt;/strong&gt;, Zendel et al., ECCV 2022 [&lt;a href=&#34;http://www.robustvision.net/&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 id=&#34;tutorials-talks&#34;&gt; Tutorials &amp;amp; Talks &lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Facing depth estimation in-the-wild with deep networks&lt;/strong&gt;. M. Poggi, F. Tosi, F. Aleotti, K. Batsos, P. Mordohai, S. Mattoccia; ECCV 2020, SEC, Glasgow [&lt;a href=&#34;https://sites.google.com/view/eccv-2020-robust-depth/home&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning and understanding single image depth estimation in the wild&lt;/strong&gt;. M. Poggi, F. Tosi, F. Aleotti, S. Mattoccia, C. Godard, J. Watson, M. Firman, G.J. Brostow; CVPR 2020, Seattle, Washington, US [&lt;a href=&#34;https://sites.google.com/view/cvpr-2020-depth-from-mono/home&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning-based depth estimation from stereo and monocular images: successes, limitations and future challenges&lt;/strong&gt;. M. Poggi, F. Tosi, K. Batsos, P. Mordohai, S. Mattoccia, CVPR 2019, Long Beach, California, US [&lt;a href=&#34;https://sites.google.com/view/cvpr-2019-depth-from-image/home&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning-based depth estimation from stereo and monocular images: successes, limitations and future challenges&lt;/strong&gt;. M. Poggi, F. Tosi, K. Batsos, P. Mordohai, S. Mattoccia; 3DV 2018, Verona, Italy [&lt;a href=&#34;https://sites.google.com/view/3dv-2018-depth-from-image/home&#34;&gt;Website&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lecture: Computer Vision (Prof. Andreas Geiger, University of T√ºbingen)&lt;/strong&gt;. [&lt;a href=&#34;https://www.youtube.com/watch?v=6hr6xpOU-uw&amp;amp;list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&amp;amp;index=12&amp;amp;pp=iAQB&#34;&gt;Preliminaries&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=EVzEJQl8WFk&amp;amp;list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&amp;amp;index=13&amp;amp;t=830s&amp;amp;pp=iAQB&#34;&gt;Block Matching&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=vLgsiIXNf0I&amp;amp;list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&amp;amp;index=14&amp;amp;pp=iAQB&#34;&gt;Siamese Networks&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=gqz6R1qChVQ&amp;amp;list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&amp;amp;index=15&amp;amp;t=359s&amp;amp;pp=iAQB&#34;&gt;Spatial Regularization&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=9vrmwZ9Pl4o&amp;amp;list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&amp;amp;index=16&amp;amp;t=639s&amp;amp;pp=iAQB&#34;&gt;End-to-End Learning&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please consider citing this list if you find this repository useful:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{poggi2021synergies,&#xA;  title={On the synergies between machine learning and binocular stereo for depth estimation from images: a survey},&#xA;  author={Poggi, Matteo and Tosi, Fabio and Batsos, Konstantinos and Mordohai, Philippos and Mattoccia, Stefano},&#xA;  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},&#xA;  volume={44},&#xA;  number={9},&#xA;  pages={5314--5334},&#xA;  year={2021},&#xA;  publisher={IEEE}&#xA;}&#xA;&#xA;@article{poggi2021confidence,&#xA;  title={On the confidence of stereo matching in a deep-learning era: a quantitative evaluation},&#xA;  author={Poggi, Matteo and Kim, Seungryong and Tosi, Fabio and Kim, Sunok and Aleotti, Filippo and Min, Dongbo and Sohn, Kwanghoon and Mattoccia, Stefano},&#xA;  journal={IEEE transactions on pattern analysis and machine intelligence},&#xA;  volume={44},&#xA;  number={9},&#xA;  pages={5293--5313},&#xA;  year={2021},&#xA;  publisher={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>prg-titech/pro-paper</title>
    <updated>2024-04-19T01:38:01Z</updated>
    <id>tag:github.com,2024-04-19:/prg-titech/pro-paper</id>
    <link href="https://github.com/prg-titech/pro-paper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>