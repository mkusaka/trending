<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-27T01:46:53Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chainsawriot/grafzahl</title>
    <updated>2023-01-27T01:46:53Z</updated>
    <id>tag:github.com,2023-01-27:/chainsawriot/grafzahl</id>
    <link href="https://github.com/chainsawriot/grafzahl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üßõ fine-tuning Transformers for text data from within R&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;grafzahl &lt;img src=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/man/figures/grafzahl_logo.svg?sanitize=true&#34; align=&#34;right&#34; height=&#34;200&#34;&gt;&lt;/h1&gt; &#xA;&lt;!-- badges: start --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://CRAN.R-project.org/package=grafzahl&#34;&gt;&lt;img src=&#34;https://www.r-pkg.org/badges/version/grafzahl&#34; alt=&#34;CRAN status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- badges: end --&gt; &#xA;&lt;p&gt;The goal of grafzahl (&lt;strong&gt;G&lt;/strong&gt;racious &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;A&lt;/strong&gt;nalytical &lt;strong&gt;F&lt;/strong&gt;ramework for &lt;strong&gt;Z&lt;/strong&gt;appy &lt;strong&gt;A&lt;/strong&gt;nalysis of &lt;strong&gt;H&lt;/strong&gt;uman &lt;strong&gt;L&lt;/strong&gt;anguages [1]) is to duct tape the &lt;a href=&#34;https://github.com/quanteda/quanteda&#34;&gt;quanteda&lt;/a&gt; ecosystem to modern &lt;a href=&#34;https://simpletransformers.ai/&#34;&gt;Transformer-based text classification models&lt;/a&gt;, e.g.&amp;nbsp;BERT, RoBERTa, etc. The model object looks and feels like the textmodel S3 object from the package &lt;a href=&#34;https://github.com/quanteda/quanteda.textmodels&#34;&gt;quanteda.textmodels&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you don‚Äôt know what I am talking about, don‚Äôt worry, this package is gracious. You don‚Äôt need to know a lot about Transformers to use this package. See the examples below.&lt;/p&gt; &#xA;&lt;p&gt;Please cite this software as:&lt;/p&gt; &#xA;&lt;p&gt;Chan, C., (2023). &lt;a href=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/paper/grafzahl_sp.pdf&#34;&gt;grafzahl: fine-tuning Transformers for text data from within R&lt;/a&gt;. &lt;em&gt;Computational Communication Research&lt;/em&gt; (Accepted)&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install the development version of grafzahl like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;remotes::install_github(&#34;chainsawriot/grafzahl&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, you can install the CRAN version&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(&#34;grafzahl&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, you need to setup your conda environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;require(grafzahl)&#xA;setup_grafzahl(cuda = TRUE) ## if you have GPU(s)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Suppose you have a bunch of tweets in quanteda corpus format. And the corpus has exactly one docvar that denotes the labels you want to predict. The data is from &lt;a href=&#34;https://github.com/pablobarbera/incivility-sage-open&#34;&gt;this repository&lt;/a&gt; (Theocharis et al., 2020).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unciviltweets&#xA;#&amp;gt; Corpus consisting of 19,982 documents and 1 docvar.&#xA;#&amp;gt; text1 :&#xA;#&amp;gt; &#34;@ @ Karma gave you a second chance yesterday.  Start doing m...&#34;&#xA;#&amp;gt; &#xA;#&amp;gt; text2 :&#xA;#&amp;gt; &#34;@ With people like you, Steve King there&#39;s still hope for we...&#34;&#xA;#&amp;gt; &#xA;#&amp;gt; text3 :&#xA;#&amp;gt; &#34;@ @ You bill is a joke and will sink the GOP. #WEDESERVEBETT...&#34;&#xA;#&amp;gt; &#xA;#&amp;gt; text4 :&#xA;#&amp;gt; &#34;@ Dream on. The only thing trump understands is how to enric...&#34;&#xA;#&amp;gt; &#xA;#&amp;gt; text5 :&#xA;#&amp;gt; &#34;@ @ Just like the Democrat taliban party was up front with t...&#34;&#xA;#&amp;gt; &#xA;#&amp;gt; text6 :&#xA;#&amp;gt; &#34;@ you are going to have more of the same with HRC, and you a...&#34;&#xA;#&amp;gt; &#xA;#&amp;gt; [ reached max_ndoc ... 19,976 more documents ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to train a Transfomer model, please select the &lt;code&gt;model_name&lt;/code&gt; from &lt;a href=&#34;https://huggingface.co/models&#34;&gt;Hugging Face‚Äôs list&lt;/a&gt;. The table below lists some common choices. In most of the time, providing &lt;code&gt;model_name&lt;/code&gt; is sufficient, there is no need to provide &lt;code&gt;model_type&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Suppose you want to train a Transformer model using ‚Äúbertweet‚Äù (Nguyen et al., 2020) because it matches your domain of usage. By default, it will save the model in the &lt;code&gt;output&lt;/code&gt; directory of the current directory. You can change it to elsewhere using the &lt;code&gt;output_dir&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;model &amp;lt;- grafzahl(unciviltweets, model_type = &#34;bertweet&#34;, model_name = &#34;vinai/bertweet-base&#34;)&#xA;### If you are hardcore quanteda user:&#xA;## model &amp;lt;- textmodel_transformer(unciviltweets,&#xA;##                                model_type = &#34;bertweet&#34;, model_name = &#34;vinai/bertweet-base&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make prediction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predict(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That is it.&lt;/p&gt; &#xA;&lt;h2&gt;Extended examples&lt;/h2&gt; &#xA;&lt;p&gt;Several extended examples are also available.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Examples&lt;/th&gt; &#xA;   &lt;th&gt;file&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;van Atteveldt et al.&amp;nbsp;(2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/paper/vanatteveldt.md&#34;&gt;paper/vanatteveldt.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dobbrick et al.&amp;nbsp;(2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/paper/dobbrick.md&#34;&gt;paper/dobbrick.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Theocharis et al.&amp;nbsp;(2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/paper/theocharis.md&#34;&gt;paper/theocharis.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OffensEval-TR (2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/paper/coltekin.md&#34;&gt;paper/coltekin.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Amharic News Text classification Dataset (2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chainsawriot/grafzahl/v0.1/paper/azime.md&#34;&gt;paper/azime.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Some common choices of &lt;code&gt;model_name&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Your data&lt;/th&gt; &#xA;   &lt;th&gt;model_type&lt;/th&gt; &#xA;   &lt;th&gt;model_name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English tweets&lt;/td&gt; &#xA;   &lt;td&gt;bertweet&lt;/td&gt; &#xA;   &lt;td&gt;vinai/bertweet-base&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lightweight&lt;/td&gt; &#xA;   &lt;td&gt;mobilebert&lt;/td&gt; &#xA;   &lt;td&gt;google/mobilebert-uncased&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;distilbert&lt;/td&gt; &#xA;   &lt;td&gt;distilbert-base-uncased&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Long Text&lt;/td&gt; &#xA;   &lt;td&gt;longformer&lt;/td&gt; &#xA;   &lt;td&gt;allenai/longformer-base-4096&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bigbird&lt;/td&gt; &#xA;   &lt;td&gt;google/bigbird-roberta-base&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English (General)&lt;/td&gt; &#xA;   &lt;td&gt;bert&lt;/td&gt; &#xA;   &lt;td&gt;bert-base-uncased&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bert&lt;/td&gt; &#xA;   &lt;td&gt;bert-base-cased&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;electra&lt;/td&gt; &#xA;   &lt;td&gt;google/electra-small-discriminator&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;roberta&lt;/td&gt; &#xA;   &lt;td&gt;roberta-base&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multilingual&lt;/td&gt; &#xA;   &lt;td&gt;xlm&lt;/td&gt; &#xA;   &lt;td&gt;xlm-mlm-17-1280&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;xml&lt;/td&gt; &#xA;   &lt;td&gt;xlm-mlm-100-1280&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bert&lt;/td&gt; &#xA;   &lt;td&gt;bert-base-multilingual-cased&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;xlmroberta&lt;/td&gt; &#xA;   &lt;td&gt;xlm-roberta-base&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;xlmroberta&lt;/td&gt; &#xA;   &lt;td&gt;xlm-roberta-large&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Theocharis, Y., Barber√°, P., Fazekas, Z., &amp;amp; Popa, S. A. (2020). The dynamics of political incivility on Twitter. Sage Open, 10(2), 2158244020919447.&lt;/li&gt; &#xA; &lt;li&gt;Nguyen, D. Q., Vu, T., &amp;amp; Nguyen, A. T. (2020). BERTweet: A pre-trained language model for English Tweets. arXiv preprint arXiv:2005.10200.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Yes, I totally made up the meaningless long name. Actually, it is the German name of the &lt;em&gt;Sesame Street&lt;/em&gt; character &lt;a href=&#34;https://de.wikipedia.org/wiki/Sesamstra%C3%9Fe#Graf_Zahl&#34;&gt;Count von Count&lt;/a&gt;, meaning ‚ÄúCount (the noble title) Number‚Äù. And it seems to be so that it is compulsory to name absolutely everything related to Transformers after Seasame Street characters.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>jonsterling/math</title>
    <updated>2023-01-27T01:46:53Z</updated>
    <id>tag:github.com,2023-01-27:/jonsterling/math</id>
    <link href="https://github.com/jonsterling/math" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A mini-book on category theory. Superseded by https://github.com/jonsterling/forest&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notice:&lt;/strong&gt;&lt;/em&gt; These lecture notes have now been incorporated into my &lt;a href=&#34;https://github.com/jonsterling/forest&#34;&gt;Forest&lt;/a&gt;. This repository will not be updated except in order to fix broken redirects.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repository is based on &lt;a href=&#34;https://github.com/paolobrasolin/krater&#34;&gt;Krater&lt;/a&gt;, a tool for building math-rich websites; the purpose of the present code is to support &#34;Stacks Project&#34;-style websites, where you create a bunch of files like &lt;code&gt;_nodes/00A4.md&lt;/code&gt; that may import each other, and then specify a root like &lt;code&gt;_lectures/lecture.md&lt;/code&gt; that imports some node.&lt;/p&gt; &#xA;&lt;h3&gt;Running on your machine&lt;/h3&gt; &#xA;&lt;p&gt;You will need a running LaTeX installation, including &lt;code&gt;latexmk&lt;/code&gt;, &lt;code&gt;dvisvgm&lt;/code&gt; and whatever engine and packages you plan on using. Chances are that if you&#39;re reading this you already have everything you need.&lt;/p&gt; &#xA;&lt;p&gt;You will need to &lt;a href=&#34;https://www.ruby-lang.org/it/documentation/installation/&#34;&gt;install Ruby 2.7.5&lt;/a&gt;; you will need to ensure that you have ghostscript installed, with the &lt;code&gt;LIBGS&lt;/code&gt; environment variable correctly set. On macOS with Homebrew, after installing ghostscript you must add the following line to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.zshrc&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LIBGS=/opt/homebrew/lib/libgs.dylib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, you can simply&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install dependencies&#xA;bundle&#xA;# run Jekyll&#xA;bundle exec jekyll serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and your website will be available at &lt;code&gt;http://127.0.0.1:4000/&amp;lt;REPO_NAME&amp;gt;/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Node UID policy and drafts&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Node UIDs should be added in order; so if the greatest node UID is &lt;code&gt;00X2&lt;/code&gt;, then the next node that is committed should be &lt;code&gt;00X3&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When a new node &lt;code&gt;00X3&lt;/code&gt; is committed, it is permanent: it is not permitted to rename it, however it is permitted to &lt;em&gt;delete&lt;/em&gt; it.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If two branches contain drafted nodes, when one is merged, the node UIDs chosen on the other branch have to be renamed so that when &lt;em&gt;they&lt;/em&gt; are committed to &lt;code&gt;main&lt;/code&gt;, there are no conflicts and node UIDs are consecutive. This causes considerable friction, so we will use the following method for drafts.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In a draft, name any &lt;em&gt;new&lt;/em&gt; node with an underscore &lt;code&gt;_00X3&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prior to merging, you may acquire a temporary lock on the entire repository and rename every node draft node to the next consecutive &#34;permanent&#34; UID.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull requests that involve draft nodes should be squashed prior to being merged; this is because we want to avoid the auto-generated changelog from referring to draft nodes that do not exist at merge-time.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The script &lt;code&gt;mint-node.rkt&lt;/code&gt; will assist you in determining what the next permanent UID is. For now you must handle the renaming manually, but in the future we can include scripts to automate this process.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pantelis/data-mining</title>
    <updated>2023-01-27T01:46:53Z</updated>
    <id>tag:github.com,2023-01-27:/pantelis/data-mining</id>
    <link href="https://github.com/pantelis/data-mining" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The contents of my data-mining course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Course Content&lt;/h1&gt; &#xA;&lt;h2&gt;Building the book&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to develop and/or build the book, you should:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Launch the devcontainer via vscode dev container extension and run &lt;code&gt;poetry shell&lt;/code&gt; to lauch the virtual environment &lt;code&gt;data-mining-book-py3.9&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Edit the books source files located in the &lt;code&gt;data_mining/&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Run &lt;code&gt;jupyter-book clean data_mining/&lt;/code&gt; to remove any existing builds&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;sphinx-autobuild --host 0.0.0.0 data_mining _build/html&lt;/code&gt; for interactive editing and liveview.&lt;/li&gt; &#xA; &lt;li&gt;(Optiona) Run &lt;code&gt;jupyter-book build data_mining/&lt;/code&gt; for an offline build&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A fully-rendered HTML version of the book will be built in &lt;code&gt;data_mining/_build/html/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Hosting the book&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://jupyterbook.org/publish/web.html&#34;&gt;Jupyter Book documentation&lt;/a&gt; to discover options for deploying a book online using services such as GitHub, GitLab, or Netlify.&lt;/p&gt; &#xA;&lt;p&gt;For GitHub and GitLab deployment specifically, the &lt;a href=&#34;https://github.com/executablebooks/cookiecutter-jupyter-book&#34;&gt;cookiecutter-jupyter-book&lt;/a&gt; includes templates for, and information about, optional continuous integration (CI) workflow files to help easily and automatically deploy books online with GitHub or GitLab. For example, if you chose &lt;code&gt;github&lt;/code&gt; for the &lt;code&gt;include_ci&lt;/code&gt; cookiecutter option, your book template was created with a GitHub actions workflow file that, once pushed to GitHub, automatically renders and pushes your book to the &lt;code&gt;gh-pages&lt;/code&gt; branch of your repo and hosts it on GitHub Pages when a push or pull request is made to the main branch.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;We welcome and recognize all contributions. You can see a list of current contributors in the &lt;a href=&#34;https://github.com/pantelis/data_mining/graphs/contributors&#34;&gt;contributors tab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This project is created using the excellent open source &lt;a href=&#34;https://jupyterbook.org/&#34;&gt;Jupyter Book project&lt;/a&gt; and the &lt;a href=&#34;https://github.com/executablebooks/cookiecutter-jupyter-book&#34;&gt;executablebooks/cookiecutter-jupyter-book template&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>