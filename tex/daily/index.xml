<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-09T01:41:07Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mwxely/AIGS</title>
    <updated>2023-10-09T01:41:07Z</updated>
    <id>tag:github.com,2023-10-09:/mwxely/AIGS</id>
    <link href="https://github.com/mwxely/AIGS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI-Generated Images as Data Source: The Dawn of Synthetic Era&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/title.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.01830&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2310.01830-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Survey&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Naereen/StrapDown.js/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/Naereen/Strapdown.js&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;AI-Generated Images as Data Source: The Dawn of Synthetic Era [&lt;a href=&#34;https://arxiv.org/abs/2310.01830&#34;&gt;Paper&lt;/a&gt;]&lt;/h1&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://mwxely.github.io/&#34; target=&#34;_blank&#34;&gt;Zuhao Yang&lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://fnzhan.com/&#34; target=&#34;_blank&#34;&gt;Fangneng Zhan&lt;/a&gt;â€ƒ &#xA;  &lt;a href=&#34;https://kunhao-liu.github.io/&#34; target=&#34;_blank&#34;&gt;Kunhao Liu&lt;/a&gt;â€ƒ Muyu Xuâ€ƒ &#xA;  &lt;a href=&#34;https://personal.ntu.edu.sg/shijian.lu/&#34; target=&#34;_blank&#34;&gt;Shijian Lu&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   Nanyang Technological University, Max Planck Institute for Informaticsâ€ƒ &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/teaser.png&#34; align=&#34;center&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is associated with our survey paper which comprehensively contextualizes the advance of the recent &lt;strong&gt;AI&lt;/strong&gt;-&lt;strong&gt;G&lt;/strong&gt;enerated Images as Data &lt;strong&gt;S&lt;/strong&gt;ource (&lt;strong&gt;AIGS&lt;/strong&gt;) and visual AIGC by formulating taxonomies according to methodologies and applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt; You are welcome to promote papers via pull request. &lt;br&gt; The process to submit a pull request:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a. Fork the project into your own repository.&lt;/li&gt; &#xA; &lt;li&gt;b. Add the Title, Author, Conference, Paper link, Project link, and Code link in &lt;code&gt;README.md&lt;/code&gt; with below format:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;**Title**&amp;lt;br&amp;gt;&#xA;*Author*&amp;lt;br&amp;gt;&#xA;Conference&#xA;[[Paper](Paper link)]&#xA;[[Project](Project link)]&#xA;[[Code](Code link)]&#xA;[[Video](Video link)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c. Submit the pull request to this branch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Related Surveys &amp;amp; Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Machine Learning for Synthetic Data Generation: A Review&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yingzhou Lu, Minjie Shen, Huazheng Wang, Wenqi Wei&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.04062&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Data in Human Analysis: A Survey&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Indu Joshi, Marcel Grimmer, Christian Rathgeb, Christoph Busch, Francois Bremond, Antitza Dantcheva&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2208.09191&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Review of Synthetic Image Data and Its Use in Computer Vision&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Keith Man, Javaan Chahl&lt;/em&gt;&lt;br&gt; J. Imaging 2022 [&lt;a href=&#34;https://www.mdpi.com/2313-433X/8/11/310&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Survey on Synthetic Data Generation, Evaluation Methods and GANs&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alvaro Figueira, Bruno Vaz&lt;/em&gt;&lt;br&gt; Mathematics 2022 [&lt;a href=&#34;https://www.mdpi.com/2227-7390/10/15/2733&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents (Work in Progress)&lt;/h2&gt; &#xA;&lt;p&gt;Methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#GenerativeModels-link&#34;&gt;Generative Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#GenLabelAcquisition-link&#34;&gt;Label Acquisition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#GenDataAugmentation-link&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#NeuralRendering-link&#34;&gt;Neural Rendering&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#NeuLabelAcquisition-link&#34;&gt;Label Acquisition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#NeuDataAugmentation-link&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#2DVisualPerception-link&#34;&gt;2D Visual Perception&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Classification-link&#34;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Segmentation-link&#34;&gt;Image Segmentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Detection-link&#34;&gt;Object Detection&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#VisualGeneration-link&#34;&gt;Visual Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#SelfsupervisedLearning-link&#34;&gt;Self-supervised Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#3DVisualPerception-link&#34;&gt;3D Visual Perception&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Robotics-link&#34;&gt;Robotics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#AutonomousDriving-link&#34;&gt;Autonomous Driving&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#OtherApplications-link&#34;&gt;Other Applications&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Medical-link&#34;&gt;Medical&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#Test-link&#34;&gt;Testing Data&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#TextimageAligned-link&#34;&gt;Text-image Aligned&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#HumanPreference-link&#34;&gt;Human Preference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mwxely/AIGS/main/#DeepfakeDetection-link&#34;&gt;Deepfake Detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Methods&lt;/h1&gt; &#xA;&lt;h2&gt;Generative Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;GenerativeModels-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Label Acquisition&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;GenLabelAcquisition-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[BigGAN] Large Scale GAN Training for High Fidelity Natural Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andrew Brock, Jeff Donahue, Karen Simonyan&lt;/em&gt;&lt;br&gt; ICLR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1809.11096&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[VQ-Diffusion] Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/cientgu/VQ-Diffusion#vector-quantized-diffusion-model-for-text-to-image-synthesis-cvpr2022-oral&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[LDM] High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Imagen] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi&lt;/em&gt;&lt;br&gt; NeurIPS 2022 [&lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://imagen-ai.com/?v=0j&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DALL-E 2] Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt;&lt;br&gt; ICML 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.06490&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/datasetGAN/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/datasetGAN_release/tree/master&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen&lt;/em&gt;&lt;br&gt; NeurIPS 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.06160&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://weijiawu.github.io/DatasetDM_page/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/showlab/DatasetDM&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2206.09592&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/gyhandy/Text2Image-for-Detection&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;Data Augmentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;GenDataAugmentation-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[StyleGAN 2]Analyzing and Improving the Image Quality of StyleGAN&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila&lt;/em&gt;&lt;br&gt; CVPR 2020 [&lt;a href=&#34;https://arxiv.org/abs/1912.04958&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=c-NJtV9Jvp0&amp;amp;feature=youtu.be&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn, Alexander Hammers, David Alexander Dickie, Maria ValdÃ©s HernÃ¡ndez, Joanna Wardlaw, Daniel Rueckert&lt;/em&gt;&lt;br&gt; arXiv 2018 [&lt;a href=&#34;https://arxiv.org/abs/1810.10863&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancement of Image Classification Using Transfer Learning and GAN-Based Synthetic Data Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Subhajit Chatterjee, Debapriya Hazra, Yung-Cheol Byun, Yong-Woon Kim&lt;/em&gt;&lt;br&gt; Mathmatics 2022 [&lt;a href=&#34;https://www.mdpi.com/2227-7390/10/9/1541&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A data augmentation perspective on diffusion models and retrieval&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Max F. Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, Chris Russell&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.10253&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Effective Data Augmentation With Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.07944&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;http://btrabuc.co/da-fusion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Neural Rendering&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;NeuralRendering-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Label Acquisition&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;NeuLabelAcquisition-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VMRF: View Matching Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu, Wenqing Zhang, Bai Song, Xiaoqin Zhang, Shijian Lu&lt;/em&gt;&lt;br&gt; ACM MM 2022 [&lt;a href=&#34;https://arxiv.org/abs/2207.02621&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Thomas Lips, Victor-Louis De Gusseme, Francis wyffels&lt;/em&gt;&lt;br&gt; ICRA 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.01913&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yenchenlin.me/nerf-supervision/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yenchenlin/nerf-supervision-public&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=_zN-wVwPH1s&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;Data Augmentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;NeuDataAugmentation-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural-Sim: Learning to Generate Training Data with NeRF&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar, Neel Joshi, Yale Song, Xin Wang, Laurent Itti, Vibhav Vineet&lt;/em&gt;&lt;br&gt; ECCV 2022 [&lt;a href=&#34;https://arxiv.org/abs/2207.11368&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/gyhandy/Neural-Sim-NeRF&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Data Augmentation for Driving Scenes on Camera&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, Hongyang Li&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.10340&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h1&gt;Applications&lt;/h1&gt; &#xA;&lt;h2&gt;2D Visual Perception&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;2DVisualPerception-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Classification&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Classification-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Is synthetic data from generative models ready for image recognition?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, Xiaojuan Qi&lt;/em&gt;&lt;br&gt; ICLR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2206.09592&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/CVMI-Lab/SyntheticData&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Data from Diffusion Models Improves ImageNet Classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, David J. Fleet&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.08466&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Pierre Chambon, Christian Bluethgen, Curtis P. Langlotz, Akshay Chaudhari&lt;/em&gt;&lt;br&gt; NeurIPS 2022 [&lt;a href=&#34;https://arxiv.org/abs/2210.04133&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenGAN: Open-Set Recognition via Open Data Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shu Kong, Deva Ramanan&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.02939&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.cs.cmu.edu/~shuk/OpenGAN.html&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/aimerykong/OpenGAN&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=CNYqYXyUHn0&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Captions are Natural Prompts for Text-to-Image Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Segmentation-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Learning Semantic Segmentation from Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuhua Chen, Wen Li, Xiaoran Chen, Luc Van Gool&lt;/em&gt;&lt;br&gt; CVPR 2019 [&lt;a href=&#34;https://arxiv.org/abs/1812.05040&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.05833&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/semanticGAN/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/semanticGAN_code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repurposing GANs for One-shot Semantic Part Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nontawat Tritrong, Pitchaporn Rewatbowornwong, Supasorn Suwajanakorn&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2103.04379&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://repurposegans.github.io/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/bryandlee/repurpose-gan/&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.09316&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DifFSS: Diffusion Model for Few-Shot Semantic Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weimin Tan, Siyuan Chen, Bo Yan&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.00773&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot 3D Multi-modal Medical Image Segmentation using Generative Adversarial Learning&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Adela Barriuso, Sanja Fidler, Antonio Torralba&lt;/em&gt;&lt;br&gt; CVPR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2201.04684&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://nv-tlabs.github.io/big-datasetgan/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/nv-tlabs/bigdatasetgan_code&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HandsOff: Labeled Dataset Generation With No Additional Human Annotations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Austin Xu, Mariya I. Vasileva, Achal Dave, Arjun Seshadri&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2212.12645&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://austinxu87.github.io/handsoff/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/austinxu87/handsoff/&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Quang Nguyen, Truong Vu, Anh Tran, Khoi Nguyen&lt;/em&gt;&lt;br&gt; NeurIPS 2023 [&lt;a href=&#34;https://arxiv.org/abs/2309.14303&#34;&gt;Paper&lt;/a&gt;[&lt;a href=&#34;https://github.com/VinAIResearch/Dataset-Diffusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, Chen Change Loy&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2309.13042&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/Jiahao000/MosaicFusion&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Label-Efficient Semantic Segmentation with Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, Artem Babenko&lt;/em&gt;&lt;br&gt; ICLR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2112.03126&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yandex-research.github.io/ddpm-segmentation/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yandex-research/ddpm-segmentation&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[ODISE] Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, Shalini De Mello&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.04803&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://jerryxu.net/ODISE/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/NVlabs/ODISE&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Detection-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[GeoDiffusion] Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.04607&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://kaichen1998.github.io/projects/geodiffusion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explore the Power of Synthetic Data on Few-shot Object Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shaobo Lin, Kun Wang, Xingyu Zeng, Rui Zhao&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13221&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[SODGAN] Synthetic Data Supervised Salient Object Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhenyu Wu, Lin Wang, Wei Wang, Tengfei Shi, Chenglizhao Chen, Aimin Hao, Shuo Li&lt;/em&gt;&lt;br&gt; ACM MM 2022 [&lt;a href=&#34;https://arxiv.org/abs/2210.13835&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImaginaryNet: Learning Object Detectors without Real Images and Annotations&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Minheng Ni, Zitong Huang, Kailai Feng, Wangmeng Zuo&lt;/em&gt;&lt;br&gt; ICLR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2210.06886&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/kodenii/ImaginaryNet&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Roy Voetman, Maya Aghaei, Klaas Dijkstra&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.09762&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Visual Generation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;VisualGeneration-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Re-Aging GAN: Toward Personalized Face Age Transformation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Farkhod Makhmudkhujaev, Sungeun Hong, and In Kyu Park&lt;/em&gt;&lt;br&gt; ICCV 2021 [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Makhmudkhujaev_Re-Aging_GAN_Toward_Personalized_Face_Age_Transformation_ICCV_2021_paper.pdf&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=NRl0GPgtcBY&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Only a Matter of Style: Age Transformation Using a Style-Based Regression Model&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, Daniel Cohen-Or&lt;/em&gt;&lt;br&gt; SIGGRAPH 2021 [&lt;a href=&#34;https://arxiv.org/abs/2102.02754&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yuval-alaluf.github.io/SAM/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yuval-alaluf/SAM&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=X_pYC_LtBFw&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Production-Ready Face Re-Aging for Visual Effects&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gaspard Zoss, Prashanth Chandran, Eftychios Sifakis, Markus Gross, Paulo Gotardo, Derek Bradley&lt;/em&gt;&lt;br&gt; TOG 2021 [&lt;a href=&#34;https://studios.disneyresearch.com/app/uploads/2022/10/Production-Ready-Face-Re-Aging-for-Visual-Effects.pdf&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=ZP1ApcdyAjk&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick&lt;/em&gt;&lt;br&gt; ICCV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.11328&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamBooth3D: Subject-Driven Text-to-3D Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2303.13508&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://dreambooth3d.github.io/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://youtu.be/kKVDrbfvOoA&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Self-supervised Learning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;SelfsupervisedLearning-link&#34;&gt;&lt;/a&gt; &lt;strong&gt;Generative Models as a Data Source for Multiview Representation Learning&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Jahanian, Xavier Puig, Yonglong Tian, Phillip Isola&lt;/em&gt;&lt;br&gt; ICLR 2022 [&lt;a href=&#34;https://arxiv.org/abs/2106.05258&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://ali-design.github.io/GenRep/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/ali-design/GenRep&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=qYmGvVrGZno&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2306.00984&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling with Deep Generative Views&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang&lt;/em&gt;&lt;br&gt; CVPR 2021 [&lt;a href=&#34;https://arxiv.org/abs/2104.14551&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://chail.github.io/gan-ensembling/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/chail/gan-ensembling&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTeacher: Pretraining Image Backbones with Deep Generative Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt;&lt;br&gt; ICCV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.07487&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/DreamTeacher/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;3D Visual Perception&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;3DVisualPerception-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Robotics&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Robotics-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;INeRF: Inverting Neural Radiance Fields for Pose Estimation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin&lt;/em&gt;&lt;br&gt; IROS 2021 [&lt;a href=&#34;https://arxiv.org/abs/2012.05877&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://yenchenlin.me/inerf/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/yenchenlin/iNeRF-public&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=eQuCZaQN0tI&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LENS: Localization enhanced by NeRF synthesis&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle&lt;/em&gt;&lt;br&gt; CoRL 2021 [&lt;a href=&#34;https://arxiv.org/abs/2110.06558&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=DgIpVoS6ejY&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fu Li, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, Slobodan Ilic&lt;/em&gt;&lt;br&gt; arXiv 2022 [&lt;a href=&#34;https://arxiv.org/abs/2203.04802&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vision-only robot navigation in a neural radiance world&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager&lt;/em&gt;&lt;br&gt; RAL 2022 [&lt;a href=&#34;https://arxiv.org/abs/2110.00168&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://mikh3x4.github.io/nerf-navigation/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/mikh3x4/nerf-navigation&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=5JjWpv9BaaE&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Event-based Camera Tracker by âˆ‡t NeRF&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mana Masuda, Yusuke Sekikawa, Hideo Saito&lt;/em&gt;&lt;br&gt; WACV 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.04559&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;Autonomous Driving&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;AutonomousDriving-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, Ying-Cong Chen&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.03526&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://len-li.github.io/lift3d-web/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/EnVision-Research/Lift3D&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniSim: A Neural Closed-Loop Sensor Simulator&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, Raquel Urtasun&lt;/em&gt;&lt;br&gt; CVPR 2023 [&lt;a href=&#34;https://arxiv.org/abs/2308.01898&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://waabi.ai/unisim/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://research-assets.waabi.ai/wp-content/uploads/2023/05/UniSim-video_compressed.mp4&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous Driving&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, Hao Zhao&lt;/em&gt;&lt;br&gt; CICAI 2023 [&lt;a href=&#34;https://arxiv.org/abs/2307.15058&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://open-air-sun.github.io/mars/&#34;&gt;Project&lt;/a&gt;][&lt;a href=&#34;https://github.com/OPEN-AIR-SUN/mars&#34;&gt;Code&lt;/a&gt;][&lt;a href=&#34;https://www.youtube.com/watch?v=AdC-jglWvfU&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;Other Applications&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;OtherApplications-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Medical&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Medical-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[MedGAN] Generating Multi-label Discrete Patient Records using Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, Jimeng Sun&lt;/em&gt;&lt;br&gt; MLHC 2017 [&lt;a href=&#34;https://arxiv.org/abs/1703.06490&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CorGAN: Correlation-Capturing Convolutional Generative Adversarial Networks for Generating Synthetic Healthcare Records&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Amirsina Torfi, Edward A. Fox&lt;/em&gt;&lt;br&gt; FLAIRS 2020 [&lt;a href=&#34;https://arxiv.org/abs/2001.09346&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/astorfi/cor-gan&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Skin Lesion Classification Using GAN based Data Augmentation&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rashid Haroon, Tanveer M. Asjid, Aqeel Khan Hassan&lt;/em&gt;&lt;br&gt; EMBC 2019 [&lt;a href=&#34;https://ieeexplore.ieee.org/document/8857905&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan&lt;/em&gt;&lt;br&gt; Neurocomputing 2018 [&lt;a href=&#34;https://arxiv.org/abs/1803.01229&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic data augmentation using GAN for improved liver lesion classification&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan&lt;/em&gt;&lt;br&gt; ISBI 2018 [&lt;a href=&#34;https://arxiv.org/abs/1801.02385&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leveraging GANs for data scarcity of COVID-19: Beyond the hype&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hazrat Ali, Christer Gronlund, Zubair Shah&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.03536&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;Testing Data&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a id=&#34;Test-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benchmarking Deepart Detection&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yabin Wang, Zhiwu Huang, Xiaopeng Hong&lt;/em&gt;&lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2302.14475&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benchmarking Robustness to Text-Guided Corruptions&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mohammadreza Mofayezi, Yasamin Medghalchi&lt;/em&gt;&lt;br&gt; CVPRW 2023 [&lt;a href=&#34;https://arxiv.org/abs/2304.02963&#34;&gt;Paper&lt;/a&gt;][&lt;a href=&#34;https://github.com/ckoorosh/RobuText&#34;&gt;Code&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;h2&gt;Text-image Aligned&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;TextimageAligned-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DiffusionDB (&lt;a href=&#34;https://github.com/poloclub/diffusiondb&#34;&gt;https://github.com/poloclub/diffusiondb&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;JourneyDB (&lt;a href=&#34;https://github.com/JourneyDB/JourneyDB&#34;&gt;https://github.com/JourneyDB/JourneyDB&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Human Preference&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;HumanPreference-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pick-a-Pic v2 (&lt;a href=&#34;https://huggingface.co/datasets/yuvalkirstain/pickapic_v2&#34;&gt;https://huggingface.co/datasets/yuvalkirstain/pickapic_v2&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;ImageReward (&lt;a href=&#34;https://huggingface.co/datasets/THUDM/ImageRewardDB&#34;&gt;https://huggingface.co/datasets/THUDM/ImageRewardDB&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;HPD v2 (&lt;a href=&#34;https://huggingface.co/datasets/xswu/human_preference_dataset&#34;&gt;https://huggingface.co/datasets/xswu/human_preference_dataset&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Deepfake Detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;DeepfakeDetection-link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DFFD (&lt;a href=&#34;http://cvlab.cse.msu.edu/dffd-dataset.html&#34;&gt;http://cvlab.cse.msu.edu/dffd-dataset.html&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;ForgeryNet (&lt;a href=&#34;https://yinanhe.github.io/projects/forgerynet.html&#34;&gt;https://yinanhe.github.io/projects/forgerynet.html&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;CNNSpot (&lt;a href=&#34;https://github.com/peterwang512/CNNDetection&#34;&gt;https://github.com/peterwang512/CNNDetection&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;CIFAKE (&lt;a href=&#34;https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images&#34;&gt;https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;GenImage (&lt;a href=&#34;https://github.com/GenImage-Dataset/GenImage&#34;&gt;https://github.com/GenImage-Dataset/GenImage&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ¤Ÿ&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yang2023aigs,&#xA;  title={AI-Generated Images as Data Source: The Dawn of Synthetic Era},&#xA;  author={Zuhao Yang and Fangneng Zhan and Kunhao Liu and Muyu Xu and Shijian Lu},&#xA;  journal={arXiv preprint arXiv:2310.01830},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>