<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-05T01:35:02Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ConsequentAI/fneval</title>
    <updated>2024-03-05T01:35:02Z</updated>
    <id>tag:github.com,2024-03-05:/ConsequentAI/fneval</id>
    <link href="https://github.com/ConsequentAI/fneval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Functional Benchmarks and the Reasoning Gap&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Functional Benchmarks and Reasoning Gap&lt;/h1&gt; &#xA;&lt;p&gt;This repository accompanies the paper &lt;a href=&#34;https://arxiv.org/abs/2402.19450&#34;&gt;Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note (Feb&#39;24): This repo and associated paper will not be finalized until the Q2&#39;24 release. We are working to get 100% coverage over MATH and GSM8K. We are releasing the first version (Q1&#39;24) for early access to the community.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;We propose a way to evaluate reasoning capabilities of models that could have been trained on the entire (compressed on uncompressed) text of the internet, and possibly retrieval augmented.&lt;/p&gt; &#xA;&lt;p&gt;We rewrite each benchmark question, in parameterized functional form, such that the reasoning involved in solving each instance is identical, but the question and answer are textually distinct each time. We snapshot a new instance of the entire benchmark each month. A model has to solve the last K instances correctly to count as correctly reasoning through that question. We notate these functionalized versions of the benchmarks with &#34;()&#34;, e.g., &#34;MATH()&#34; for the &#34;MATH&#34; benchmark.&lt;/p&gt; &#xA;&lt;p&gt;For each model the evaluation script tabulates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Static accuracy: accuracy on the static benchmark (e.g., MATH).&lt;/li&gt; &#xA; &lt;li&gt;Functional accuracy: accuracy over the last K instances of the functional benchmark (e.g., MATH()) and the static benchmark.&lt;/li&gt; &#xA; &lt;li&gt;Reasoning Gap: Percent drop from static to functional.&lt;/li&gt; &#xA; &lt;li&gt;Hallucination [0,100]: Percent times incorrect solution output, instead of stating no solution.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Run&lt;/h1&gt; &#xA;&lt;p&gt;Make sure you have your API keys exported, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m evaluate&#xA;&#xA;# check list of available stat FNs using the cmd below,&#xA;# i.e., no stat_fn # provided, run by providing names&#xA;python3 -m summarize_evals --stat_fn&#xA;&#xA;# dump out accuracies for static/functional&#xA;python3 -m summarize_evals --stat_fn stat_accuracy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;API Keys needed&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_ORG=&#34;org-...&#34;&#xA;export OPENAI_API_KEY=&#34;sk-...&#34;&#xA;export ANTHROPIC_API_KEY=&#34;sk-...&#34;&#xA;export TOGETHER_API_KEY=&#34;...&#34;&#xA;export MISTRAL_API_KEY=&#34;...&#34;&#xA;export HUGGING_FACE_HUB_TOKEN=&#34;hf_...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Known Issues&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More permissive output equivalence needed. Examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;a) Output in expression form: &lt;code&gt;C(21,7)&lt;/code&gt; instead of &lt;code&gt;116280&lt;/code&gt; (Claude); &lt;code&gt;9 choose 2 = 36&lt;/code&gt; instead of &lt;code&gt;36&lt;/code&gt; (Mixtral)&lt;/li&gt; &#xA;   &lt;li&gt;b) Output not simplified: &lt;code&gt;5! = 5 × 4 × 3 × 2 × 1 = 1&lt;/code&gt; (likely max_tokens truncated) when the ground truth is indeed &lt;code&gt;120&lt;/code&gt; (Mixtral); &lt;code&gt;33(22-12+1)=33*11=363&lt;/code&gt; when the ground truth is indeed &lt;code&gt;363&lt;/code&gt; (Mixtral).&lt;/li&gt; &#xA;   &lt;li&gt;c) Non-semantic characters, which are not known: &lt;code&gt;$\frac{1}{16}$&lt;/code&gt; for the ground truth &lt;code&gt;\frac{1}{16}&lt;/code&gt; in &lt;code&gt;counting_and_probability/289.json&lt;/code&gt; (Mixtral)&lt;/li&gt; &#xA;   &lt;li&gt;d) Additional verbiage: &lt;code&gt;$g(x) = 3 - 2f(x)$&lt;/code&gt; for ground truth &lt;code&gt;3 - 2f(x)&lt;/code&gt; (Mistral medium); &lt;code&gt;The remainder when $2^8$ is divided by 5 is 1.&lt;/code&gt; when the ground truth is &lt;code&gt;1&lt;/code&gt; (Mistral Medium)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Functional instantiations can sometimes result in problems or answers that are too long. E.g., in one of the instantiations in the &lt;code&gt;Dec-2023&lt;/code&gt; snapshot a benchmark&#39;s output number has more than 40 digits.&lt;/li&gt; &#xA; &lt;li&gt;Anthropic&#39;s Claude 2 is verbose and its output appears to be CoT-like by default, and the answer is not up-front in its output. More dedicated post-processing might help. Similar, is Mixtral-8x7B-Instruct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;FAQs&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Why do the accuracy numbers differ from the best reported for the models? We have not optimized for getting the best reported accuracy for each model. E.g., &lt;a href=&#34;https://arxiv.org/pdf/2401.04088.pdf&#34;&gt;Mixtral 8x7B reports&lt;/a&gt; 28.4% on MATH (4-shot with maj@4) and 74.4% on GSM8K (8-shot with maj@8). Known reasons: The few shot examples used may not be optimal, and we do not do maj@k or pass@k. Future iterations of this repo will fix that as long as across models the same few-shot examples are used, and same maj@k is used. The specific &#34;role&#34; and &#34;instruction prompt&#34; can differ, based on what works best for each model. We conjecture that the overall conclusions about reasoning gap presence will not change, but it is an open question about how it affects the gap across models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open question: CoT increases accuracy, but how does it change the reasoning gap? We’re curious too and are actively working on it. We currently use the original MATH benchmark&#39;s default answer equivalence procedure. CoT requires custom answer extractors, which we are currently implementing. The next release will include evaluation with and without CoT.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open question: pass@k and maj@k increase topline performance, but do they increase or decrease the reasoning gap? Work-in-progress. TBD.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Why is the GPT4 number not 78.2% as reported in the &lt;a href=&#34;https://arxiv.org/abs/2305.20050&#34;&gt;Let&#39;s why step by step&lt;/a&gt; paper? As mentioned in their repository section &lt;a href=&#34;https://github.com/openai/prm800k#math-splits&#34;&gt;MATH Splits&lt;/a&gt; and in the paper, their evaluation is non-standard: &#34;In order to avoid the risk of over-fitting on the 7,500 MATH training problems, we expanded the training set to include 4,500 MATH test split problems. We therefore evaluate our models only on the remaining 500 held-out problems. We selected these 500 test problems uniformly at random, and we believe they are representative of the test set as a whole.&#34; For the 500 representative problems they pick PRM, ORM, and Majority Voting get 78.2%, 72.4% and 69.6% when using best-of-1860 (Figure 3).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Need a static benchmark functionalized? Better yet, have suggestions for what to add to this eval? We would love that. Please email &lt;a href=&#34;mailto:info@consequent.ai&#34;&gt;info@consequent.ai&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{srivastava2024functional,&#xA;      title={Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap},&#xA;      author={Saurabh Srivastava and Annarose M B and Anto P V au2 and Shashank Menon and Ajay Sukumar and Adwaith Samod T and Alan Philipose and Stevin Prince and Sooraj Thomas},&#xA;      year={2024},&#xA;      eprint={2402.19450},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>zeramorphic/cambridge-maths-notes</title>
    <updated>2024-03-05T01:35:02Z</updated>
    <id>tag:github.com,2024-03-05:/zeramorphic/cambridge-maths-notes</id>
    <link href="https://github.com/zeramorphic/cambridge-maths-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX sources for notes for the maths courses at Cambridge.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Maths Notes&lt;/h1&gt; &#xA;&lt;p&gt;LaTeX sources for my notes on the maths courses at Cambridge. To view the compiled PDFs, click &lt;a href=&#34;https://thirdsgames.co.uk/maths.html&#34;&gt;here&lt;/a&gt;. Please note that these are &lt;em&gt;not&lt;/em&gt; exhaustive descriptions of the courses themselves - please watch all of the lectures! This is simply a way of collating much of the courses&#39; information in one place.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Install a TeX distribution, such as MiKTeX or TeX Live.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Go 1.16.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;go get ./...&lt;/code&gt; to fetch dependencies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;go run . build&lt;/code&gt; to build the project. You can execute &lt;code&gt;go run . help&lt;/code&gt; for more information about various other commands.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For the &lt;code&gt;fmt&lt;/code&gt; command, you must install &lt;code&gt;latexindent&lt;/code&gt; and add it to &lt;code&gt;PATH&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For the &lt;code&gt;optimise&lt;/code&gt; command, you must install &lt;code&gt;ghostscript&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To contribute to the project (such as making corrections or adding explanations), fork the repository and then submit a pull request. Any contributions must be made under the same license as the project itself.&lt;/p&gt; &#xA;&lt;p&gt;It is recommended to add the line &lt;code&gt;go run . fmt&lt;/code&gt; to your &lt;code&gt;.git/hooks/pre-commit&lt;/code&gt; so that your source code is formatted nicely before committing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/cheriot-sail</title>
    <updated>2024-03-05T01:35:02Z</updated>
    <id>tag:github.com,2024-03-05:/microsoft/cheriot-sail</id>
    <link href="https://github.com/microsoft/cheriot-sail" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Sail code model of the CHERIoT ISA&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CHERIoT Sail model&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains an implementation of the CHERIoT ISA in &lt;a href=&#34;http://github.com/rems-project/sail&#34;&gt;Sail&lt;/a&gt;. It contains an executable description of the CHERIoT instruction set that can be used to build an instruction set emulator and also prove &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/cheriot-sail/main/properties&#34;&gt;properties&lt;/a&gt; of the ISA using Sail&#39;s SMT support. A full description of the ISA, including extracts from this repository, can be found in the &lt;a href=&#34;https://aka.ms/cheriot-tech-report&#34;&gt;CHERIoT technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The code is dervied from &lt;a href=&#34;http://github.com/CTSRD-CHERI/sail-cheri-riscv&#34;&gt;sail-cheri-riscv&lt;/a&gt; and uses the &lt;a href=&#34;http://github.com/rems-project/sail-riscv&#34;&gt;sail-riscv&lt;/a&gt; model as a submodule.&lt;/p&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;The easiest way to use this emulator is to use the dev container for the &lt;a href=&#34;http://github.com/microsoft/cheriot-rtos&#34;&gt;CHERIoT RTOS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, if you wish to build from source you must first install dependencies, including Sail. For example, on Ubuntu 20.04 we have tested:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt update&#xA;sudo apt install opam z3 libgmp-dev&#xA;opam init&#xA;opam install sail&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/microsoft/cheriot-sail&#xA;cd cheriot-sail&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally build the C emulator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make csim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will produce an executable in &lt;code&gt;c_emulator/cheriot_sim&lt;/code&gt; that can be used to run ELF files produced by the CHERIoT compiler.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>