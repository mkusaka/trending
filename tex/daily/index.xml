<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-05T01:59:11Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>latex3/latex3</title>
    <updated>2022-06-05T01:59:11Z</updated>
    <id>tag:github.com,2022-06-05:/latex3/latex3</id>
    <link href="https://github.com/latex3/latex3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The LaTeX3 Development Repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The &lt;code&gt;expl3&lt;/code&gt; (LaTeX3) Development Repository&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The repository contains development material for &lt;code&gt;expl3&lt;/code&gt;. This includes not only code to be developed into the &lt;code&gt;expl3&lt;/code&gt; kernel, but also a variety of test, documentation and more experimental material. All of this code works on top of LaTeX2e.&lt;/p&gt; &#xA;&lt;p&gt;The following directories are present in the repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;l3kernel&lt;/code&gt;: code forms the &lt;code&gt;expl3&lt;/code&gt; kernel and with the exception of the contents of &lt;code&gt;l3candidates&lt;/code&gt;, all stable code. With a modern LaTeX2e kernel, this code is loaded during format creation; when using an older LaTeX2e kernel, this material is accessible using the &lt;code&gt;expl3&lt;/code&gt; package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3backend&lt;/code&gt;: code for backend (driver) level interfaces across the &lt;code&gt;expl3&lt;/code&gt; codebase; none of this code has public interfaces, and so no distinction is made between stable and experimental code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3packages&lt;/code&gt;: code which is written to be used on top of LaTeX2e to explore interfaces; these higher-level packages are &#39;stable&#39;. It is unlikely that any new packages will be added to this area.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3experimental&lt;/code&gt;: code which is written to be used on top of LaTeX2e to experiment with code and interface concepts. The interfaces for these packages are still under active discussion. Parts of this code may eventually be migrated to &lt;code&gt;l3kernel&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3trial&lt;/code&gt;: material which is under very active development, for potential addition to &lt;code&gt;l3kernel&lt;/code&gt; or &lt;code&gt;l3experimental&lt;/code&gt;. Material in this directory may include potential replacements for existing modules, where large-scale changes are under-way. This code is &lt;em&gt;not&lt;/em&gt; released to CTAN.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;l3leftovers&lt;/code&gt;: code which has been developed in the past by The LaTeX Project but is not suitable for use in its current form. Parts of this code may be used as the basis for new developments in &lt;code&gt;l3kernel&lt;/code&gt; or &lt;code&gt;l3experimental&lt;/code&gt; over time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Support material for development is found in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;support&lt;/code&gt;, which contains files for the automated test suite which are &#39;local&#39; to the repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Documentation is found in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;articles&lt;/code&gt;: discussion of concepts by team members for publication in &lt;a href=&#34;http://www.tug.org/tugboat&#34;&gt;&lt;em&gt;TUGBoat&lt;/em&gt;&lt;/a&gt; or elsewhere.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The repository also contains the directory &lt;code&gt;xpackages&lt;/code&gt;. This contain code which is being moved (broadly) &lt;code&gt;l3experimental&lt;/code&gt;. Over time, &lt;code&gt;xpackages&lt;/code&gt; is expected to be removed from the repository.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;p&gt;Discussion concerning the approach, suggestions for improvements, changes, additions, &lt;em&gt;etc.&lt;/em&gt; should be addressed to the list &lt;a href=&#34;https://listserv.uni-heidelberg.de/cgi-bin/wa?A0=LATEX-L&#34;&gt;LaTeX-L&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can subscribe to this list by sending mail to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;listserv@urz.uni-heidelberg.de&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;with the body containing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;subscribe LATEX-L  &amp;lt;Your-First-Name&amp;gt; &amp;lt;Your-Second-Name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;The issue tracker for &lt;code&gt;expl3&lt;/code&gt; is currently located &lt;a href=&#34;https://github.com/latex3/latex3/issues&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please report specific issues with &lt;code&gt;expl3&lt;/code&gt; code there; more general discussion should be directed to the &lt;a href=&#34;https://raw.githubusercontent.com/latex3/latex3/main/#Discussion&#34;&gt;LaTeX-L list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build status&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Actions&lt;/a&gt; as a hosted continuous integration service. For each commit, the build status is tested using the current release of TeX Live.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Current build status:&lt;/em&gt; &lt;img src=&#34;https://github.com/latex3/latex3/actions/workflows/main.yaml/badge.svg?branch=main&#34; alt=&#34;build status&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development team&lt;/h2&gt; &#xA;&lt;p&gt;This code is developed by &lt;a href=&#34;https://latex-project.org&#34;&gt;The LaTeX Project&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;This README file is copyright 2021 The LaTeX Project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openmlsys/openmlsys-zh</title>
    <updated>2022-06-05T01:59:11Z</updated>
    <id>tag:github.com,2022-06-05:/openmlsys/openmlsys-zh</id>
    <link href="https://github.com/openmlsys/openmlsys-zh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ã€ŠMachine Learning Systems: Design and Implementationã€‹- Chinese Version&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼šè®¾è®¡å’Œå®ç°&lt;/h1&gt; &#xA;&lt;p&gt;æœ¬å¼€æºé¡¹ç›®è¯•å›¾ç»™è¯»è€…è®²è§£ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡åŸç†å’Œå®ç°ç»éªŒã€‚&lt;/p&gt; &#xA;&lt;p&gt;ğŸ”¥ &lt;strong&gt;ä¹¦ç±ç½‘é¡µç‰ˆï¼š&lt;/strong&gt; &lt;a href=&#34;https://openmlsys.github.io/&#34;&gt;æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼šè®¾è®¡å’Œå®ç°&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ğŸ”¥ &lt;strong&gt;ä¹¦ç±PDFï¼š&lt;/strong&gt; å°†åœ¨å‹˜è¯¯åï¼Œå››æœˆåº•å‘å¸ƒ&lt;/p&gt; &#xA;&lt;h2&gt;å‘å¸ƒ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;17/03/2022: æœ¬ä¹¦å¤„äºå‹˜è¯¯é˜¶æ®µã€‚å¦‚å‘ç°æ–‡å­—å’Œå›¾ç‰‡é”™è¯¯ï¼Œå¯åˆ›å»ºIssueå¹¶@&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/editors.md&#34;&gt;ç« èŠ‚ç¼–è¾‘&lt;/a&gt;ã€‚æˆ‘ä»¬éå¸¸æ¬¢è¿ç¤¾åŒºæäº¤PRç›´æ¥å‹˜è¯¯ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;é€‚ç”¨è¯»è€…&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬ä¹¦çš„å¸¸è§è¯»è€…åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å­¦ç”Ÿï¼š&lt;/strong&gt; éšç€å¤§é‡æœºå™¨å­¦ä¹ è¯¾ç¨‹åœ¨å¤§å­¦ä¸­çš„æ™®åŠï¼Œå­¦ç”Ÿå·²ç»å¼€å§‹æŒæ¡å¤§é‡æœºå™¨å­¦ä¹ çš„åŸºç¡€ç†è®ºå’Œç¥ç»ç½‘ç»œçš„å®ç°ã€‚ç„¶è€Œï¼Œéœ€è¦è®­ç»ƒå‡ºå¯ä»¥å®é™…åº”ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œéœ€è¦å¯¹ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿæœ‰å……åˆ†çš„è®¤è¯†ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç§‘ç ”äººå‘˜ï¼š&lt;/strong&gt; ç ”å‘æ–°å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ä»…ä»…éœ€è¦ä¼šä½¿ç”¨åŸºç¡€çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿæ¥å£ã€‚åŒæ—¶ï¼Œæ–°å‹çš„æ¨¡å‹éœ€è¦ç»™ç³»ç»Ÿæä¾›æ–°çš„è‡ªå®šä¹‰ç®—å­ï¼ˆCustom Operatorsï¼‰ï¼Œåˆæˆ–è€…æ˜¯ä¼šåˆ©ç”¨é«˜çº§çš„åˆ†å¸ƒå¼æ‰§è¡Œç®—å­æ¥å®ç°å¤§æ¨¡å‹çš„å¼€å‘ã€‚è¿™ä¸€ç³»åˆ—éœ€æ±‚éƒ½éœ€è¦å¯¹åº•å±‚ç³»ç»Ÿå…·æœ‰å……åˆ†è®¤è¯†ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¼€å‘äººå‘˜ï¼š&lt;/strong&gt; å¤§é‡çš„æ•°æ®å’ŒAIé©±åŠ¨çš„å…¬å¸éƒ½éƒ¨ç½²äº†æœºå™¨å­¦ä¹ åŸºç¡€è®¾æ–½ã€‚è¿™ä¸€è®¾æ–½çš„æ ¸å¿ƒå°±æ˜¯æœºå™¨å­¦ä¹ ç³»ç»Ÿã€‚å› æ­¤äº†è§£æœºå™¨å­¦ä¹ ç³»ç»Ÿæœ‰åŠ©äºå¼€å‘äººå‘˜å¯¹äºç³»ç»Ÿæ€§èƒ½è°ƒä¼˜ï¼Œä»¥å®šä½é—®é¢˜ï¼Œå¹¶ä¸”æ ¹æ®ä¸šåŠ¡éœ€æ±‚å¯¹æœºå™¨å­¦ä¹ ç³»ç»Ÿè¿›è¡Œæ·±åº¦å®šåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å†…å®¹ä»‹ç»&lt;/h2&gt; &#xA;&lt;p&gt;ç°ä»£æœºå™¨å­¦ä¹ æ¡†æ¶å…·æœ‰å¤æ‚çš„å†…éƒ¨æ¶æ„å’Œç¹å¤šçš„å¤–éƒ¨ç›¸å…³ç»„ä»¶ã€‚åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹å…¶ç»†è‡´æ‹†åˆ†ï¼Œæ·±å…¥è§£è¯»ï¼š&lt;/p&gt; &#xA;&lt;p&gt;åŸºç¡€ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¼–ç¨‹æ¥å£ï¼š&lt;/strong&gt; ä¸ºäº†æ”¯æŒæµ·é‡åº”ç”¨ï¼Œæœºå™¨å­¦ä¹ æ¡†æ¶çš„ç¼–ç¨‹æ¥å£è®¾è®¡å…·æœ‰å¤§é‡çš„è®¾è®¡å“²å­¦ï¼Œåœ¨æ˜“ç”¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ¬ä¹¦å°†è®²è¿°ç¼–ç¨‹æ¥å£çš„æ¼”è¿›ï¼Œæœºå™¨å­¦ä¹ å·¥ä½œæµï¼Œå®šä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥åŠç”¨C/C++è¿›è¡Œæ¡†æ¶å¼€å‘ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;è®¡ç®—å›¾ï¼š&lt;/strong&gt; æœºå™¨å­¦ä¹ æ¡†æ¶éœ€è¦æ”¯æŒè‡ªåŠ¨å¾®åˆ†ï¼Œç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œå¤šç¼–ç¨‹å‰ç«¯ç­‰ã€‚å®ç°è¿™äº›æ”¯æŒçš„æ ¸å¿ƒæŠ€æœ¯æ˜¯ï¼šè®¡ç®—å›¾ï¼ˆComputational Graphï¼‰ã€‚æœ¬ä¹¦å°†è®²è¿°è®¡ç®—å›¾çš„åŸºæœ¬æ„æˆï¼Œç”Ÿæˆæ–¹æ³•å’Œè°ƒåº¦ç­–ç•¥ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ€§èƒ½è¿›é˜¶ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¼–è¯‘å™¨å‰ç«¯ï¼š&lt;/strong&gt; æœºå™¨å­¦ä¹ æ¡†æ¶éœ€è¦åˆ©ç”¨ç¼–è¯‘å™¨å‰ç«¯æŠ€æœ¯å¯¹è®¡ç®—å›¾è¿›è¡ŒåŠŸèƒ½æ‹“å±•å’Œæ€§èƒ½ä¼˜åŒ–ã€‚æœ¬ä¹¦å°†è®²è¿°å¸¸è§çš„å‰ç«¯æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç±»å‹æ¨å¯¼ï¼Œä¸­é—´è¡¨ç¤ºï¼ˆIntermediate Representationï¼‰ï¼Œè‡ªåŠ¨å¾®åˆ†ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¼–è¯‘å™¨åç«¯å’Œè¿è¡Œæ—¶ï¼š&lt;/strong&gt; æœºå™¨å­¦ä¹ æ¡†æ¶çš„ä¸€ä¸ªæ ¸å¿ƒç›®æ ‡æ˜¯ï¼šå¦‚ä½•å……åˆ†åˆ©ç”¨å¼‚æ„ç¡¬ä»¶ã€‚è¿™å…¶ä¸­ä¼šæ¶‰åŠç¼–è¯‘å™¨åç«¯æŠ€æœ¯ï¼Œä»¥åŠå°†è®¡ç®—å›¾ç®—å­ï¼ˆOperatorï¼‰è°ƒåº¦åˆ°ç¡¬ä»¶ä¸Šçš„è¿è¡Œæ—¶ï¼ˆRuntimeï¼‰ã€‚æœ¬ä¹¦å°†è®²è¿°è®¡ç®—å›¾ä¼˜åŒ–ï¼Œç®—å­é€‰æ‹©ï¼Œå†…å­˜åˆ†é…å’Œè®¡ç®—è°ƒåº¦ä¸æ‰§è¡Œã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¡¬ä»¶åŠ é€Ÿå™¨ï¼š&lt;/strong&gt; æœºå™¨å­¦ä¹ æ¡†æ¶çš„åŸºæœ¬è¿è¡Œå•å…ƒæ˜¯ç®—å­ï¼Œè€Œç®—å­çš„å®ç°å¿…é¡»å……åˆ†åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆGPUå’ŒAscendï¼‰çš„ç‰¹æ€§ã€‚æœ¬ä¹¦å°†ä¼šè®²è¿°ç¡¬ä»¶åŠ é€Ÿå™¨çš„åŸºæœ¬æ„æˆåŸç†å’Œå¸¸è§çš„é«˜æ€§èƒ½ç¼–ç¨‹æ¥å£ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®å¤„ç†æ¡†æ¶ï¼š&lt;/strong&gt; æœºå™¨å­¦ä¹ æ¡†æ¶ä¼šé›†æˆé«˜æ€§èƒ½æ¡†æ¶æ¥è¿›è¡Œæ•°æ®é¢„å¤„ç†ã€‚æœ¬ä¹¦å°†ä¼šè®²è¿°è¿™ä¸€ç±»æ•°æ®å¤„ç†æ¡†æ¶åœ¨è®¾è®¡ä¸­éœ€è¦è¾¾åˆ°çš„å¤šä¸ªç›®æ ‡ï¼šæ˜“ç”¨æ€§ï¼Œé«˜æ•ˆæ€§ï¼Œä¿åºæ€§ï¼Œåˆ†å¸ƒå¼ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ¨¡å‹éƒ¨ç½²ï¼š&lt;/strong&gt; åœ¨æ¨¡å‹å®Œæˆè®­ç»ƒåï¼Œç”¨æˆ·éœ€è¦å°†æ¨¡å‹éƒ¨ç½²åˆ°ç»ˆç«¯è®¾å¤‡ï¼ˆå¦‚äº‘æœåŠ¡å™¨ï¼Œç§»åŠ¨ç»ˆç«¯å’Œæ— äººè½¦ï¼‰ã€‚è¿™å…¶ä¸­æ¶‰åŠåˆ°çš„æ¨¡å‹è½¬æ¢ï¼Œæ¨¡å‹å‹ç¼©ï¼Œæ¨¡å‹æ¨ç†å’Œå®‰å…¨ä¿æŠ¤ç­‰çŸ¥è¯†ä¹Ÿä¼šåœ¨æœ¬ä¹¦ä¸­è®¨è®ºã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆ†å¸ƒå¼è®­ç»ƒï¼š&lt;/strong&gt; æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒéœ€è¦æ¶ˆè€—å¤§é‡èµ„æºã€‚è¶Šæ¥è¶Šå¤šçš„æœºå™¨å­¦ä¹ æ¡†æ¶å› æ­¤åŸç”Ÿæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€‚åœ¨æœ¬ä¹¦ä¸­æˆ‘ä»¬å°†ä¼šè®¨è®ºå¸¸è§çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•ï¼ˆåŒ…æ‹¬æ•°æ®å¹¶è¡Œï¼Œæ¨¡å‹å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œï¼‰ï¼Œä»¥åŠå®ç°è¿™äº›æ–¹æ³•çš„ç³»ç»Ÿæ¶æ„ï¼ˆåŒ…æ‹¬é›†åˆé€šè®¯å’Œå‚æ•°æœåŠ¡å™¨ï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;åŠŸèƒ½æ‹“å±•ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿï¼š&lt;/strong&gt; æ¨èç³»ç»Ÿæ˜¯ç›®å‰æœºå™¨å­¦ä¹ åº”ç”¨æœ€æˆåŠŸçš„é¢†åŸŸä¹‹ä¸€ã€‚æœ¬ä¹¦å°†ä¼šæ¦‚æ‹¬æ¨èç³»ç»Ÿçš„è¿ä½œåŸç†ï¼Œè¯¦ç»†æè¿°å¤§è§„æ¨¡å·¥ä¸šåœºæ™¯ä¸‹çš„æ¨èç³»ç»Ÿæ¶æ„è®¾è®¡ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;è”é‚¦å­¦ä¹ ç³»ç»Ÿï¼š&lt;/strong&gt; éšç€æ•°æ®ä¿æŠ¤æ³•è§„å’Œéšç§ä¿æŠ¤çš„å´›èµ·ï¼Œè”é‚¦å­¦ä¹ æ­£æˆä¸ºæ—¥ç›Šé‡è¦çš„ç ”ç©¶é¢†åŸŸã€‚æœ¬ä¹¦å°†ä¼šä»‹ç»è”é‚¦å­¦ä¹ çš„å¸¸ç”¨æ–¹æ³•ä»¥åŠç›¸å…³ç³»ç»Ÿå®ç°ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼š&lt;/strong&gt; å¼ºåŒ–å­¦ä¹ æ˜¯èµ°å‘é€šç”¨äººå·¥æ™ºèƒ½çš„å…³é”®æŠ€æœ¯ã€‚æœ¬ä¹¦å°†ä¼šä»‹ç»ç›®å‰å¸¸è§çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼ˆåŒ…æ‹¬å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“ç­‰ï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¯è§£é‡Šæ€§AIç³»ç»Ÿï¼š&lt;/strong&gt; éšç€æœºå™¨å­¦ä¹ åœ¨å®‰å…¨æ”¸å…³ï¼ˆSafety-criticalï¼‰é¢†åŸŸçš„åº”ç”¨ï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿè¶Šæ¥è¶Šéœ€è¦å¯¹å†³ç­–ç»™å‡ºå……åˆ†è§£é‡Šã€‚æœ¬ä¹¦å°†ä¼šè®¨è®ºå¯è§£é‡ŠAIç³»ç»Ÿçš„å¸¸ç”¨æ–¹æ³•å’Œè½åœ°å®è·µç»éªŒã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æœºå™¨äººç³»ç»Ÿï¼š&lt;/strong&gt; æœºå™¨äººï¼ˆæ— äººè½¦ï¼Œæ— äººæœºï¼Œå®¶ç”¨æœºå™¨äººç­‰ï¼‰ä½œä¸ºæœºå™¨å­¦ä¹ æŠ€æœ¯é‡è¦çš„åº”ç”¨é¢†åŸŸï¼Œåœ¨æœ€è¿‘æ•°å¹´å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚åœ¨å®è·µä¸­ï¼Œæœºå™¨äººç³»ç»Ÿåœ¨å®æ—¶æ€§ï¼Œå®‰å…¨æ€§ï¼Œé²æ£’æ€§ç­‰æ–¹é¢éƒ½æœ‰æé«˜è¦æ±‚ï¼Œè¿™è¦æ±‚å¼€å‘è€…å…·æœ‰ç®—æ³•å’Œç³»ç»Ÿçš„åŒé‡æ€ç»´ï¼Œä»è€Œè§£å†³å®é™…é—®é¢˜ã€‚æœ¬ä¹¦ä¸­æˆ‘ä»¬å°†ç»“åˆæœ€æ–°ç ”ç©¶æˆæœå’Œæœºå™¨äººç³»ç»Ÿå®è·µç»éªŒè®²è§£è¯¥ç±»ç³»ç»Ÿçš„è®¾è®¡åŸåˆ™å’Œå®ç°ç»†èŠ‚ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æˆ‘ä»¬åœ¨æŒç»­æ‹“å±•æ‹“å±•æœ¬ä¹¦çš„å†…å®¹ï¼Œå¦‚å…ƒå­¦ä¹ ç³»ç»Ÿï¼Œè‡ªåŠ¨å¹¶è¡Œï¼Œæ·±åº¦å­¦ä¹ é›†ç¾¤è°ƒåº¦ï¼Œç»¿è‰²AIç³»ç»Ÿï¼Œå›¾å­¦ä¹ ç³»ç»Ÿç­‰ã€‚æˆ‘ä»¬ä¹Ÿéå¸¸æ¬¢è¿ç¤¾åŒºå¯¹äºæ–°å†…å®¹æå‡ºå»ºè®®ï¼Œè´¡çŒ®ç« èŠ‚ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æ„å»ºæŒ‡å—&lt;/h2&gt; &#xA;&lt;p&gt;è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/info.md&#34;&gt;æ„å»ºæŒ‡å—&lt;/a&gt;æ¥äº†è§£å¦‚ä½•æ„å»ºæœ¬ä¹¦çš„ç½‘é¡µç‰ˆæœ¬å’ŒPDFç‰ˆæœ¬ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å†™ä½œæŒ‡å—&lt;/h2&gt; &#xA;&lt;p&gt;æˆ‘ä»¬æ¬¢è¿å¤§å®¶æ¥ä¸€èµ·è´¡çŒ®å’Œæ›´æ–°æœ¬ä¹¦çš„å†…å®¹ã€‚å¸¸è§çš„è´¡çŒ®æ–¹å¼æ˜¯æäº¤PRæ¥æ›´æ–°å’Œæ·»åŠ Markdownæ–‡ä»¶ã€‚å†™ä½œçš„é£æ ¼å’Œå›¾ç‰‡è¦æ±‚è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/style.md&#34;&gt;é£æ ¼æŒ‡å—&lt;/a&gt;ã€‚åŒæ—¶ï¼Œæœºå™¨å­¦ä¹ é¢†åŸŸæ¶‰åŠåˆ°å¤§é‡çš„ä¸­è‹±æ–‡ç¿»è¯‘ï¼Œç›¸å…³çš„ç¿»è¯‘è¦æ±‚è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/openmlsys/openmlsys-zh/main/info/terminology.md&#34;&gt;æœ¯è¯­æŒ‡å—&lt;/a&gt;ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>weihaox/awesome-gan-inversion</title>
    <updated>2022-06-05T01:59:11Z</updated>
    <id>tag:github.com,2022-06-05:/weihaox/awesome-gan-inversion</id>
    <link href="https://github.com/weihaox/awesome-gan-inversion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of resources on GAN inversion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;p align=&#34;center&#34;&gt;&lt;code&gt;awesome gan-inversion&lt;/code&gt;&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat&#34; alt=&#34;PR&#39;s Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo is a collection of resources on GAN inversion, as a supplement for our &lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;survey&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;survey&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Inversion: A Survey.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.05278&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;citation&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;@article{xia2021survey,&#xA;    author  = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},&#xA;    title   = {GAN Inversion: A Survey},&#xA;    journal = {arXiv preprint arXiv: 2101.05278},&#xA;    year={2021}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;inverted pretrained model&lt;/h2&gt; &#xA;&lt;h3&gt;2D GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://axelsauer.com/&#34;&gt;Axel Sauer&lt;/a&gt;, &lt;a href=&#34;https://katjaschwarz.github.io/&#34;&gt;Katja Schwarz&lt;/a&gt;, &lt;a href=&#34;http://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.00273&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/stylegan-xl/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/autonomousvision/stylegan_xl&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Distilled StyleGAN: Towards Generation from Internet Photos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rmokady.github.io/&#34;&gt;Ron Mokady&lt;/a&gt;, Michal Yarom, Omer Tov, Oran Lang, Daniel Cohen-Or, Tali Dekel, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.12211&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://self-distilled-stylegan.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/self-distilled-stylegan/self-distilled-internet-photos&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling Off-the-shelf Models for GAN Training.&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://nupurkmr9.github.io/&#34;&gt;Nupur Kumari&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2112.09130.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~vision-aided-gan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nupurkmr9/vision-aided-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN3: Alias-Free Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Samuli Laine, Erik HÃ¤rkÃ¶nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.12423&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/alias-free-gan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan3&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/alias-free-gan-pytorch&#34;&gt;Rosinality&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2-Ada: Training Generative Adversarial Networks with Limited Data.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.06676&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/woctezuma/steam-stylegan2-ada&#34;&gt;Steam StyleGAN2-ADA&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/samuli-laine&#34;&gt;Samuli Laine&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/miika-aittala&#34;&gt;Miika Aittala&lt;/a&gt;, Janne Hellsten, Jaakko Lehtinen, &lt;a href=&#34;https://research.nvidia.com/person/timo-aila&#34;&gt;Timo Aila&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.04958&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;PyTorch&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;Offical TF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/manicman1999/StyleGAN2-Tensorflow-2.0&#34;&gt;Unoffical Tensorflow 2.0&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Samuli Laine, Timo Aila.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://arxiv.org/abs/1812.04948&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/stylegan&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen.&lt;/em&gt;&lt;br&gt; ICLR 2018. [&lt;a href=&#34;https://arxiv.org/abs/1710.10196&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tkarras/progressive_growing_of_gans&#34;&gt;Offical TF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D-aware GANs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;EG3D: Efficient Geometry-aware 3D Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, &lt;a href=&#34;https://matthew-a-chan.github.io/&#34;&gt;Matthew A. Chan&lt;/a&gt;, &lt;a href=&#34;https://luminohope.org/&#34;&gt;Koki Nagano&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~bxpan/&#34;&gt;Boxiao Pan&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/shalini-gupta&#34;&gt;Shalini De Mello&lt;/a&gt;, &lt;a href=&#34;https://oraziogallo.github.io/&#34;&gt;Orazio Gallo&lt;/a&gt;, &lt;a href=&#34;https://geometry.stanford.edu/member/guibas/&#34;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/jonathan-tremblay&#34;&gt;Jonathan Tremblay&lt;/a&gt;, &lt;a href=&#34;https://www.samehkhamis.com/&#34;&gt;Sameh Khamis&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/tero-karras&#34;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.07945&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://matthew-a-chan.github.io/EG3D&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://jiataogu.me/&#34;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&#34;https://lingjie0206.github.io/&#34;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&#34;https://totoro97.github.io/about.html&#34;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&#34;http://people.mpi-inf.mpg.de/~theobalt/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.08985&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://jiataogu.me/style_nerf/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://homes.cs.washington.edu/~royorel/&#34;&gt;Roy Or-El&lt;/a&gt;, &lt;a href=&#34;https://roxanneluo.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.11427&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylesdf.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/royorel/StyleSDF&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ericryanchan.github.io/&#34;&gt;Eric R. Chan&lt;/a&gt;, &lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Marco Monteiro&lt;/a&gt;, &lt;a href=&#34;https://kellnhofer.xyz/&#34;&gt;Petr Kellnhofer&lt;/a&gt;, &lt;a href=&#34;https://jiajunwu.com/&#34;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00926&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://marcoamonteiro.github.io/pi-GAN-website/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/pi-GAN-pytorch&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;inversion method&lt;/h2&gt; &#xA;&lt;p&gt;This part contatins generatal inversion methods, while methods in the next &lt;em&gt;application&lt;/em&gt; part are mainly designed for specific tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Transformer for Image Inversion and Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xueqi Hu, Qiusheng Huang, Zhengyi Shi, Siyuan Li, Changxin Gao, Li Sun, Qingli Li.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.07932&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sapphire497/style-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity GAN Inversion for Image Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://tengfei-wang.github.io&#34;&gt;Tengfei Wang&lt;/a&gt;, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2109.06590&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://tengfei-wang.github.io/HFGI/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Tengfei-Wang/HFGI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperInverter: Improving StyleGAN Inversion via Hypernetwork.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://di-mi-ta.github.io/&#34;&gt;Tan M. Dinh&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/anhttranusc/&#34;&gt;Anh Tuan Tran&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/site/rangmanhonguyen/&#34;&gt;Rang Nguyen&lt;/a&gt;, &lt;a href=&#34;https://sonhua.github.io/&#34;&gt;Binh-Son Hua&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00719&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://di-mi-ta.github.io/HyperInverter/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.15666&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://yuval-alaluf.github.io/hyperstyle/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/hyperstyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Overparameterization Improves StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yohan Poirier-Ginter, Alexandre Lessard, Ryan Smith, Jean-FranÃ§ois Lalonde.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation. [&lt;a href=&#34;https://arxiv.org/abs/2205.06304&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://lvsn.github.io/OverparamStyleGAN/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAlign: Analysis and Applications of Aligned StyleGAN Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.11323&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expanding the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yin Yu, Ghasedi Kamran, Wu HsiangTao, Yang Jiaolong, Tong Xi, Fu Yun.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.12530&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encode-in-Style: Latent-based Video Encoding using StyleGAN2.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://trevineoorloff.github.io/&#34;&gt;Trevine Oorloff&lt;/a&gt;, &lt;a href=&#34;https://www.umiacs.umd.edu/people/yaser&#34;&gt;Yaser Yacoob&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14512&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://trevineoorloff.github.io/Encode-in-Style.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/trevineoorloff/Encode-in-Style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-fidelity GAN Inversion with Padding Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ezioby.github.io/padinv/&#34;&gt;Qingyan Bai&lt;/a&gt;, Yinghao Xu, Jiapeng Zhu, Weihao Xia, Yujiu Yang, Yujun Shen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.11105&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EzioBy/padinv&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with NerfGANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Wen-Sheng Chu, Abhishek Kumar, Dmitry Lagun, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.09061&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Third Time&#39;s the Charm? Image and Video Editing with StyleGAN3.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, Zongze Wu, Asif Zamir, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.13433&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/stylegan3-editing/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/stylegan3-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature-Style Encoder for Style-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.02183&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Min Jin Chong, Hsin-Ying Lee, David Forsyth.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.01619&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mchong6/SOAT&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real Image Inversion via Segments.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Futschik, Michal LukÃ¡Ä, Eli Shechtman, Daniel SÃ½kora.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.06269&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic and Geometric Unfolding of StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Mustafa Shukor, Xu Yao, Bharath Bhushan Damodaran, Pierre Hellier.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.04481&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pivotal Tuning for Latent-based Editing of Real Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniel Roich, Ron Mokady, Amit H. Bermano, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2106.05744.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/danielroich/PTI&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming the Latent Space of StyleGAN for Real Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Heyi Li, Jinlong Liu, Yunzhi Bai, Huayan Wang, Klaus Mueller.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.14230&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AnonSubm2021/TransStyleGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, Nenghai Yu.&lt;/em&gt;&lt;br&gt; TIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.07661&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wty-ustc.github.io/inversion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/StyleGAN-Inversion-Baseline&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;OÄŸuz Kaan YÃ¼ksel, &lt;a href=&#34;https://enis.dev&#34;&gt;Enis Simsar&lt;/a&gt;, Ezgi GÃ¼lperi Er, Pinar Yanardag.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.00820&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/catlab-team/latentclr&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-Control: Explicitly Controllable GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard Medioni.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.02477&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved StyleGAN Embedding: Where are the Good Latents?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/ZPdesu&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/RameenAbdal&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=ojgWPpgAAAAJ&amp;amp;hl=en&#34;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=rS1xJIIAAAAJ&amp;amp;hl=en&#34;&gt;John Femiani&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2012.09036&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZPdesu/II2S&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a Deep Reinforcement Learning Policy Over the Latent Space of a Pre-trained GAN for Semantic Age Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kumar Shubham, Gopalakrishnan Venkatesh, Reijul Sachdev, Akshi, Dinesh Babu Jayagopi, G. Srinivasaraghavan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.00954&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lifting 2D StyleGAN for 3D-Aware Face Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://seasonsh.github.io/&#34;&gt;Yichun Shi&lt;/a&gt;, Divyansh Aggarwal, &lt;a href=&#34;http://www.cse.msu.edu/~jain/&#34;&gt;Anil K. Jain&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13126&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Davis Wertheimer, Omid Poursaeed, Bharath Hariharan.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.13026&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Disentangled Manifolds in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. &lt;a href=&#34;https://arxiv.org/abs/2011.11842&#34;&gt;[PDF]&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Learning for Faster StyleGAN Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shanyan Guan, &lt;a href=&#34;https://tyshiwo.github.io/&#34;&gt;Ying Tai&lt;/a&gt;, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang Yang.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.01758&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Inversion and Generation Diversity in StyleGAN using a Gaussianized Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.06529&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explaining in Style: Training a GAN to explain a classifier in StyleSpace.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal Irani, Inbar Mosseri.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.13369&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://explaining-in-style.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BDInvert: GAN Inversion for Out-of-Range Images with Geometric Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kkang831.github.io/&#34;&gt;Kyoungkook Kang&lt;/a&gt;, Seongtae Kim, &lt;a href=&#34;https://www.scho.pe.kr/&#34;&gt;Sunghyun Cho&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.08998&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kkang831.github.io/publication/ICCV_2021_BDInvert/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Continuity to Editability: Inverting GANs with Consecutive Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://qingyang-xu.github.io/&#34;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&#34;https://www.csyongdu.com/&#34;&gt;Yong Du&lt;/a&gt;, Wenpeng Xiao, Xuemiao Xu and &lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/hengfenghe.com&#34;&gt;Shengfeng He&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.13812&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yuval-alaluf.github.io/&#34;&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02699&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yuval-alaluf.github.io/restyle-encoder/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/restyle-encoder&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disentangled Face Attribute Editing via Instance-Aware Latent Space Search.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxuan Han, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, Ying Fu.&lt;/em&gt;&lt;br&gt; IJCAI 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.12660&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yxuhan/IALS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prior Image-Constrained Reconstruction using Style-Based Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Varun A Kelkar, Mark Anastasio.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2102.12525.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intermediate Layer Optimization for Inverse Problems using Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Joseph Dean, Ajil Jalal, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.07364&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/giannisdaras/ilo&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Latent Space Regression to Analyze and Leverage Compositionality in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.csail.mit.edu/lrchai/&#34;&gt;Lucy Chai&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/jwulff/&#34;&gt;Jonas Wulff&lt;/a&gt;, &lt;a href=&#34;http://web.mit.edu/phillipi/&#34;&gt;Phillip Isola&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=sjuuTm4vj0&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/latent-composition&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/latent-composition/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Peiye Zhuang, Oluwasanmi Koyejo, Alexander G. Schwing.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.01187&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Qianfen Jiao, Sheng Qian, Si Wu, Hau-San Wong.&lt;/em&gt;&lt;br&gt; AAAI 2021. [&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17017&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ensembling with Deep Generative Views.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14551&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chail/gan-ensembling&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://chail.github.io/gan-ensembling/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Navigating the GAN Parameter Space for Semantic Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anton Cherepkov, Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.13786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yandex-research/navigan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zongze Wu, Dani Lischinski, Eli Shechtman.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). &lt;a href=&#34;https://arxiv.org/abs/2011.12799&#34;&gt;[PDF]&lt;/a&gt; [&lt;a href=&#34;https://github.com/betterze/StyleSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.00951&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/weihaox/awesome-gan-inversion/main/eladrich.github.io/pixel2style2pixel/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GHFeat: Generative Hierarchical Features from Synthesizing Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2007.10379.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/ghfeat&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/ghfeat/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hui-Po Wang, Ning Yu, Mario Fritz.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.14107&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;e4e: Designing an Encoder for StyleGAN Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Omer Tov&lt;/a&gt;, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02766&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2008.02401&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/StyleFlow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask-Guided Discovery of Semantic Manifolds in Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://mengyu.page/&#34;&gt;Mengyu Yang&lt;/a&gt;, &lt;a href=&#34;https://www.cdtps.utoronto.ca/people/directories/all-faculty/david-rokeby&#34;&gt;David Rokeby&lt;/a&gt;, &lt;a href=&#34;https://wxs.ca/&#34;&gt;Xavier Snelgrove&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop on Machine Learning for Creativity and Design. [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold/raw/main/masked-gan-manifold.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bmolab/masked-gan-manifold&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io/&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~amberman/&#34;&gt;Amit Bermano&lt;/a&gt;, &lt;a href=&#34;https://yangyan.li/&#34;&gt;Yangyan Li&lt;/a&gt;, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; SIGGRAPH ASIA 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/ID-disentanglement/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PIE: Portrait Image Embedding for Semantic Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://people.mpi-inf.mpg.de/~atewari/&#34;&gt;A. Tewari&lt;/a&gt;, M. Elgharib, M. BR, F. Bernard, H-P. Seidel, P. Pâ€ŒÃ©rez, M. ZollhÃ¶fer, C.Theobalt.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/data/paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://gvv.mpi-inf.mpg.de/projects/PIE/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the Role of Individual Units in a Deep Neural Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; National Academy of Sciences 2020. [&lt;a href=&#34;https://arxiv.org/abs/2009.05041&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/dissect/&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://dissect.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Identity Disentanglement via Latent Space Mapping.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2020. [&lt;a href=&#34;https://arxiv.org/abs/2005.07728&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/ID-disentanglement&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transforming and Projecting Images into Class-conditional Generative Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://minyounghuh.com/&#34;&gt;Minyoung Huh&lt;/a&gt;, &lt;a href=&#34;https://richzhang.github.io/&#34;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;http://people.csail.mit.edu/sparis/&#34;&gt;Sylvain Paris&lt;/a&gt;, &lt;a href=&#34;https://www.dgp.toronto.edu/~hertzman/&#34;&gt;Aaron Hertzmann&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;http://arxiv.org/abs/2005.01703&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/minyoungg/GAN-Transform-and-Project&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://minyoungg.github.io/GAN-Transform-and-Project/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&#34;https://jjthiagarajan.com/&#34;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/kailkhura1&#34;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&#34;https://people.llnl.gov/bremer5&#34;&gt;Timo Bremer&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; IJCV 2020. [&lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-020-01310-5&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rewriting a Deep Generative Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2007.15646&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/davidbau/rewriting&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN2 Distillation for Feed-forward Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03581&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvgenyKashin/stylegan2-distillation&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In-Domain GAN Inversion for Real Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.00049&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/idinvert/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/idinvert&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.13659&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/deep-generative-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.12287&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Disentangling Invertible Interpretation Network for Explaining Latent Representations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, BjÃ¶rn Ommer.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.13166&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/iin/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/iin&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing in Style: Uncovering the Local Semantics of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Edo Collins, Raja Bala, Bob Price, Sabine SÃ¼sstrunk.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.14367&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IVRL/GANLocalEditing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Processing Using Multi-Code GAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1912.07116&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/mganprior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/mganprior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN++: How to Edit the Embedded Images?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1911.11544&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Photo Manipulation with a Generative Image Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Hendrik Strobelt, William Peebles, Jonas, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba.&lt;/em&gt;&lt;br&gt; TOG 2019. [&lt;a href=&#34;https://arxiv.org/abs/2005.07727&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rameen Abdal, &lt;a href=&#34;http://yipengqin.github.io/&#34;&gt;Yipeng Qin&lt;/a&gt;, Peter Wonka.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1904.03189&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/image2styleganv1-v2&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN-based Projector for Faster Recovery with Convergence Guarantees in Linear Inverse Problems.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ankit Raj, Yuqi Li, Yoram Bresler.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1902.09698&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting Layers of a Large Generator.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_18.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detecting Overfitting in Deep Generators via Latent Recovery.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie.&lt;/em&gt;&lt;br&gt; CVPR 2019. [&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Webster_Detecting_Overfitting_of_Deep_Generative_Networks_via_Latent_Recovery_CVPR_2019_paper.pdf&#34;&gt;PDF&lt;/a&gt;][&lt;a href=&#34;https://colab.research.google.com/drive/1N6zP4xlPunWOkmakcl0mamfhq946nMLB?usp=sharing&#34;&gt;Colab&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network (II).&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil A Bharath.&lt;/em&gt;&lt;br&gt; TNNLS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1802.05701&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ToniCreswell/InvertingGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Invertibility of Convolutional Generative Networks from Partial Measurements.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Fangchang Ma, Ulas Ayaz, Sertac Karaman.&lt;/em&gt;&lt;br&gt; NeurIPS 2018. [&lt;a href=&#34;https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fangchangma/invert-generative-networks&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Metrics for Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, Patrick van der Smagt.&lt;/em&gt;&lt;br&gt; AISTATS 2018. [&lt;a href=&#34;https://arxiv.org/abs/1711.01204&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Understanding the Invertibility of Convolutional Neural Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee.&lt;/em&gt;&lt;br&gt; IJCAI 2017. [&lt;a href=&#34;https://arxiv.org/abs/1705.08664&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Network to Solve Them All - Solving Linear Inverse Problems using Deep Projection Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;J. H. Rick Chang, Chun-Liang Li, Barnabas Poczos, B. V. K. Vijaya Kumar, Aswin C. Sankaranarayanan.&lt;/em&gt;&lt;br&gt; ICCV 2017. [&lt;a href=&#34;https://arxiv.org/abs/1703.09912&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Precise Recovery of Latent Vectors from Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zachary C. Lipton, Subarna Tripathi.&lt;/em&gt;&lt;br&gt; ICLR 2017 workshop. [&lt;a href=&#34;https://arxiv.org/abs/1702.04782&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SubarnaTripathi/ReverseGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverting The Generator Of A Generative Adversarial Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Antonia Creswell, Anil Anthony Bharath.&lt;/em&gt;&lt;br&gt; NeurIPS 2016 Workshop. [&lt;a href=&#34;https://arxiv.org/abs/1611.05644&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Visual Manipulation on the Natural Image Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jun-Yan Zhu, Philipp KrÃ¤henbÃ¼hl, Eli Shechtman, Alexei A. Efros.&lt;/em&gt;&lt;br&gt; ECCV 2016. [&lt;a href=&#34;https://arxiv.org/abs/1609.03552v2&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;3D GANs inverson&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D GAN Inversion for Controllable Portrait Image Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://connorzlin.com/&#34;&gt;Connor Z. Lin&lt;/a&gt;, David B. Lindell, Eric R. Chan, &lt;a href=&#34;https://stanford.edu/~gordonwz/&#34;&gt;Gordon Wetzstein&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13441&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://www.computationalimaging.org/publications/3dganinversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2NeRF: Unsupervised Conditional Ï€-GAN for Single Image to Neural Radiance Fields Translation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.13162&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;latent space navigation&lt;/h2&gt; &#xA;&lt;p&gt;Inversion is not the ultimate goal. The reason that we invert a real image into the latent space of a trained GAN model is that we can manipulate the inverted image in the latent space by discovering the desired code with certain attributes. This technique is usually known as latent space navigation, GAN steerability, latent code manipulation, or other names in the literature. Although often regarded as an independent research field, it acts as an indispensable component of GAN inversion for manipulation. Many inversion methods also involve efficient discovery of a desired latent code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://enis.dev/&#34;&gt;Enis Simsar&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Umut Kocasari&lt;/a&gt;, &lt;a href=&#34;https://catlab-team.github.io/&#34;&gt;Ezgi GÃ¼lperi Er&lt;/a&gt;, &lt;a href=&#34;https://pinguar.org/&#34;&gt;Pinar Yanardag&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08516&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/fantasticstyles/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://catlab-team.github.io/styleatlas/classes/FFHQ/&#34;&gt;Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region-Based Semantic Factorization in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, Qifeng Chen.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09649&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-level Latent Space Structuring for Generative Control.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://orenkatzir.github.io/&#34;&gt;Oren Katzir&lt;/a&gt;, Vicky Perepelook, Dani Lischinski, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05910&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rayleigh EigenDirections (REDs): GAN Latent Space Traversals for Multidimensional Features.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guha Balakrishnan, Raghudeep Gadde, Aleix Martinez, Pietro Perona.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2201.10423.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Latent Space Directions For GAN-based Local Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ehsan Pajouheshgar, Tong Zhang, Sabine SÃ¼sstrunk.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.12583&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.10132&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor Component Analysis for Interpreting the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://james-oldfield.github.io/&#34;&gt;James Oldfield&lt;/a&gt;, Markos Georgopoulos, Yannis Panagakis, Mihalis A. Nicolaou, Ioannis Patras.&lt;/em&gt;&lt;br&gt; BMVC 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.11736&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://eecs.qmul.ac.uk/~jo001/TCA-latent-space/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/james-oldfield/TCA-latent-space&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Subspace Factorization for StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rene Haas, Stella GraÃŸhof and Sami S. Brandt.&lt;/em&gt;&lt;br&gt; FG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.04554&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensor-based Emotion Editing in the StyleGAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;RenÃ© Haas, Stella GraÃŸhof, Sami S. Brandt.&lt;/em&gt;&lt;br&gt; CVPR 2022 Workshop on AI for Content Creation Workshop. [&lt;a href=&#34;https://arxiv.org/pdf/2205.06102.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LARGE: Latent-Based Regression through GAN Semantics.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://yotamnitzan.github.io&#34;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=iLLlWr8AAAAJ&#34;&gt;Ofir Brenner&lt;/a&gt;, &lt;a href=&#34;https://danielcohenor.com/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.11186&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YotamNitzan/LARGE&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://yotamnitzan.github.io/LARGE&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Image Animator: Learning to Animate Image via Latent Space Navigation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yaohui Wang, Di Yang, Francois Bremond, Antitza Dantcheva.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=7r6kDq0mK_&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://wyhsirius.github.io/LIA-project&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhsirius/LIA&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleFusion: Disentangling Spatial Segments in StyleGAN-Generated Images.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or.&lt;/em&gt;&lt;br&gt; TOG 2022. [&lt;a href=&#34;https://arxiv.org/abs/2107.07437&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OmerKafri/StyleFusion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jaewoong Choi, Changyeon Yoon, Junho Lee, Jung Ho Park, Geonho Hwang, Myungjoo Kang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.06959&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploratory Search of GANs with Contextual Bandits.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ivan Kropotov, Alan Medlar, Dorota Glowacka.&lt;/em&gt;&lt;br&gt; CIKM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3459637.3482103&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LowRankGAN: Low-Rank Subspaces in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, Qifeng Chen.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.04488&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhujiapeng/LowRankGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable and Compositional Generation with Latent-Space Energy-Based Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weili Nie, Arash Vahdat, Anima Anandkumar.&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.10873&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Latent Transformer for Disentangled Face Editing in Images and Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Yao_A_Latent_Transformer_for_Disentangled_Face_Editing_in_Images_and_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2106.11895&#34;&gt;ArXiV&lt;/a&gt;] [&lt;a href=&#34;https://github.com/InterDigitalInc/latent-transformer&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Toward a Visual Concept Vocabulary for GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/html/Schwettmann_Toward_a_Visual_Concept_Vocabulary_for_GAN_Latent_Space_ICCV_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://visualvocab.csail.mit.edu/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WarpedGANSpace: Finding Non-linear RBF Paths in GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Christos Tzelepis, Georgios Tzimiropoulos, Ioannis Patras.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.13357&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chi0tzp/WarpedGANSpace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Transformations via NeuralODEs for GAN-based Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Valentin Khrulkov, Leyla Mirvakhabova, Ivan Oseledets, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Khrulkov_Latent_Transformations_via_NeuralODEs_for_GAN-Based_Image_Editing_ICCV_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KhrulkovV/nonlinear-image-editing&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OroJaR: Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuxiang Wei, Yupeng Shi, Xiao Liu, Zhilong Ji, Yuan Gao, Zhongqin Wu, Wangmeng Zuo.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.07668&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/csyxwei/OroJaR&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EigenGAN: Layer-Wise Eigen-Learning for GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhenliang He, Meina Kan, Shiguang Shan.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.12476&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LynnHo/EigenGAN-Tensorflow&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SalS-GAN: Spatially-Adaptive Latent Space in StyleGAN for Real Image Embedding.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Lingyun Zhang, Xiuxiu Bai, Yao Gao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475633&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanyue Li, Yi Liu, Xiwen Wei, Yang Zhang, Si Wu, Yong Xu, Hau San Wong.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3474085.3475293&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Interpretable Latent Space Directions of GANs Beyond Binary Attributes.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiting Yang, Liangyu Chai, Qiang Wen, Shuang Zhao, Zixun Sun, Shengfeng He.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Discovering_Interpretable_Latent_Space_Directions_of_GANs_Beyond_Binary_Attributes_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Surrogate Gradient Field for Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Minjun Li, Yanghua Jin, Huachun Zhu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;http://arxiv.org/abs/2104.09065&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SeFa: Closed-Form Factorization of Latent Semantics in GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2007.06600&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/sefa&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/sefa/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;L2M-GAN: Learning To Manipulate Latent Space Semantics for Facial Attribute Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guoxing Yang, Nanyi Fei, Mingyu Ding, Guangzhen Liu, Zhiwu Lu, Tao Xiang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/html/Yang_L2M-GAN_Learning_To_Manipulate_Latent_Space_Semantics_for_Facial_Attribute_CVPR_2021_paper.html&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songquanpeng/L2M-GAN&#34;&gt;Unofficial Pytorch&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoCoGAN-HD: A Good Image Generator Is What You Need for High-Resolution Video Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/pdf?id=6puCSjH3hwA&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/snap-research/MoCoGAN-HD&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN Steerability without optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nurit Spingarn-Eliezer, Ron Banner, Tomer Michaeli.&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://openreview.net/forum?id=zDy_nQCXiIj&#34;&gt;OpenReview&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2012.05328&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the &#34;steerability&#34; of generative adversarial networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ali Jahanian, Lucy Chai, Phillip Isola.&lt;/em&gt;&lt;br&gt; ICLR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.07171&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ali-design.github.io/gan_steerability/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GANSpace: Discovering Interpretable GAN Controls.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Erik HÃ¤rkÃ¶nen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2004.02546&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/harskish/ganspace&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting the Latent Space of GANs for Semantic Face Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://shenyujun.github.io/&#34;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&#34;http://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, &lt;a href=&#34;http://www.ie.cuhk.edu.hk/people/xotang.shtml&#34;&gt;Xiaoou Tang&lt;/a&gt;, &lt;a href=&#34;http://bzhou.ie.cuhk.edu.hk/&#34;&gt;Bolei Zhou&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/1907.10786&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/interfacegan/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/genforce/interfacegan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing What a GAN Cannot Generate.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba.&lt;/em&gt;&lt;br&gt; ICCV 2019. [&lt;a href=&#34;https://arxiv.org/abs/1910.11626&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://ganseeing.csail.mit.edu/&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Interpretable Directions in the GAN Latent Space.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Andrey Voynov, Artem Babenko.&lt;/em&gt;&lt;br&gt; ICML 2020. [&lt;a href=&#34;https://arxiv.org/abs/2002.03754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/anvoynov/GANLatentDiscovery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;application&lt;/h2&gt; &#xA;&lt;h3&gt;image and video generation and manipulation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Video Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Jihyun Bae, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.09273&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEAT: Face Editing with Attention.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xianxu Hou, Linlin Shen, Or Patashnik, Daniel Cohen-Or, Hui Huang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2202.02713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Bingchuan Li, Shaofei Cai, Wei Liu, Peng Zhang, Miao Hua, Qian He, Zili Yi.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2109.10737&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/phycvgan/DyStyle&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Semantic Image Synthesis Using StyleGAN Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuki Endo, Yoshihiro Kanamori.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14877&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/endo-yuki-t/Fewshot-SMIS&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Heredity-aware Child Face Image Generation with Latent Space Disentanglement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Xiao Cui, Wengang Zhou, Yang Hu, Weilun Wang, Houqiang Li.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.11080&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image Transformation Learning via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kaiwen Zha, Yujun Shen, Bolei Zhou.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.07751&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://genforce.github.io/trgan&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jialu Huang, Jing Liao, Sam Kwong.&lt;/em&gt;&lt;br&gt; arxiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2010.05713&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity-Guided Face Generation with Multi-modal Contour Conditions.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Qingyan Bai, Weihao Xia, Fei Yin, Yujiu Yang.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.04854&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sound-Guided Semantic Image Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Seung Hyun Lee, Wonseok Roh, Wonmin Byeon, Sang Ho Yoon, Chan Young Kim, Jinkyu Kim, Sangpil Kim.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00007&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairCLIP: Design Your Hair by Text and Reference Image.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05142&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wty-ustc/HairCLIP&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HairMapper: Removing Hair from Portraits Using GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yong-Liang Yang&lt;/a&gt;, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Jin&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/HairMapper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/cvpr2022/cvpr2022.htm&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ/raw/main&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/oneThousand1000/non-hair-FFHQ&#34;&gt;Non-hair-FFHQ Data&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attribute Group Editing for Reliable Few-shot Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Guanqi Ding, Xinzhe Han, Shuhui Wang, Shuzhe Wu, Xin Jin, Dandan Tu, Qingming Huang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08422&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UniBester/AGE&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InsetGAN for Full-Body Image Generation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://afruehstueck.github.io/&#34;&gt;Anna FrÃ¼hstÃ¼ck&lt;/a&gt;, &lt;a href=&#34;http://krsingh.cs.ucdavis.edu/&#34;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/eli-shechtman/&#34;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/niloy-mitra/&#34;&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;, &lt;a href=&#34;https://research.adobe.com/person/jingwan-lu/&#34;&gt;Jingwan Lu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07200&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://afruehstueck.github.io/insetgan&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/stylegan-human/StyleGAN-Human/raw/main/insetgan.py&#34;&gt;Unofficial&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaceEdit: Learning a Unified Editing Space for Open-Domain Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.cs.rochester.edu/u/jshi31/&#34;&gt;Jing Shi&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/ningxu/&#34;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/u/hzheng15/haitian_homepage/index.html&#34;&gt;Haitian Zheng&lt;/a&gt;, Alex Smith, &lt;a href=&#34;https://www.cs.rochester.edu/u/jluo/&#34;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&#34;https://www.cs.rochester.edu/~cxu22/&#34;&gt;Chenliang Xu&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.00180&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In&amp;amp;Out: Diverse Image Outpainting via GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.00675&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/InOut/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InfinityGAN: Towards Infinite-Resolution Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, Ming-Hsuan Yang.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.03963&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://hubert0527.github.io/infinityGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Siavash Khodadadeh, Shabnam Ghadar, Saeid Motiian, Wei-An Lin, Ladislau BÃ¶lÃ¶ni, Ratheesh Kalarot.&lt;/em&gt;&lt;br&gt; WACV 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2022/html/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.html&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.mpi-inf.mpg.de/~gfox/&#34;&gt;Gereon Fox&lt;/a&gt;, &lt;a href=&#34;https://www.mpi-inf.mpg.de/~atewari/&#34;&gt;Ayush Tewari&lt;/a&gt;, Mohamed Elgharib, &lt;a href=&#34;http://gvv.mpi-inf.mpg.de/&#34;&gt;Christian Theobalt&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; BMVC 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2107.07224&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constrained Graphic Layout Generation via Latent Optimization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00871&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ktrk115/const_layout&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Wongjong Jang, Gwangjin Ju, &lt;a href=&#34;https://ycjung.info/&#34;&gt;Yucheol Jung&lt;/a&gt;, &lt;a href=&#34;http://jlyang.org/&#34;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/xtong/&#34;&gt;Xin Tong&lt;/a&gt;, &lt;a href=&#34;http://phome.postech.ac.kr/~leesy/&#34;&gt;Seungyong Lee&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.04331.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PeterZhouSZ/StyleCariGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coarse-to-Fine: Facial Structure Editing of Portrait Images via Latent Space Classifications.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://onethousandwu.com/&#34;&gt;Yiqian Wu&lt;/a&gt;, &lt;a href=&#34;http://www.yongliangyang.net/&#34;&gt;Yongliang Yang&lt;/a&gt;, Qinjie Xiao, &lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin&#34;&gt;Xiaogang Ji&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/paper46.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.cad.zju.edu.cn/home/jin/sig2021/sig2021.htm&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAM: Only a Matter of Style-Age Transformation Using a Style-Based Regression Model.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuval Alaluf, Or Patashnik, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; TOG 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.02754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuval-alaluf/SAM&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Adversarial Fake Images on Face Manifold.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Dongze Li, Wei Wang, Hongxing Fan, Jing Dong.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.03272&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://sites.google.com/view/mafifi&#34;&gt;Mahmoud Afifi&lt;/a&gt;, Marcus A. Brubaker, Michael S. Brown.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2011.11731&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mahmoudnafifi/HistoGAN&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://ln2.sync.com/dl/1891becc0/uhsxtprq-33wfwmyq-dhhqeb3s-mtstuqw7/view/default/11118541390008&#34;&gt;4K Landscape&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Shot Face Swapping on Megapixels.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yuhao Zhu, Qi Li, Jian Wang, Chengzhong Xu, Zhenan Sun.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2105.04932.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LOHO: Latent Optimization of Hairstyles via Orthogonalization.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Rohit Saha, Brendan Duke, Florian Shkurti, Graham W. Taylor, Parham Aarabi.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.03891&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dukebw/LOHO&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://github.com/blandocs&#34;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&#34;https://yunjey.github.io/&#34;&gt;Yunjey Choi&lt;/a&gt;, &lt;a href=&#34;https://github.com/taki0112&#34;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&#34;http://cmalab.snu.ac.kr/&#34;&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href=&#34;https://github.com/youngjung&#34;&gt;Youngjung Uh&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.14754&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naver-ai/StyleMapGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;yaxing wang, Lu Yu, Joost van de Weijer.&lt;/em&gt;&lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.05867&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yaxingwang/DeepI2I&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;multimodal learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://rinongal.github.io/&#34;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&#34;https://orpatashnik.github.io/&#34;&gt;Or Patashnik&lt;/a&gt;, &lt;a href=&#34;https://haggaim.github.io/&#34;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/gal-chechik&#34;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&#34;https://www.cs.tau.ac.il/~dcor/&#34;&gt;Daniel Cohen-Or&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.00946&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://stylegan-nada.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rinongal/StyleGAN-nada&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Word.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, Antonio Torralba.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2103.10951&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CI-GAN: Cycle-Consistent Inverse GAN for Text-to-Image Synthesis.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao.&lt;/em&gt;&lt;br&gt; ACM MM 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.01361&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TediGAN: Text-Guided Diverse Image Generation and Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.03308&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/Multi-Modal-CelebA-HQ&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weihaox/TediGAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepLandscape: Adversarial Modeling of Landscape Videos.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;E. Logacheva, R. Suvorov, O. Khomenko, A. Mashikhin, and V. Lempitsky.&lt;/em&gt;&lt;br&gt; ECCV 2020. [&lt;a href=&#34;https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680256.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/saic-mdal/deep-landscape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://saic-mdal.github.io/deep-landscape/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image restoration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Yiqun Mei, Pengfei Guo, Vishal M. Patel.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2203.16669&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards High-Fidelity Face Self-Occlusion Recovery via Multi-View Residual-Based GAN Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Jinsong Chen, Hu Han, Shiguang Shan.&lt;/em&gt;&lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://www.aaai.org/AAAI22Papers/AAAI-2208.ChenJ.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Time-Travel Rephotography.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Xuan Luo&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~cecilia77/&#34;&gt;Xuaner Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/paul-yoo-768a3715b&#34;&gt;Paul Yoo&lt;/a&gt;, &lt;a href=&#34;http://www.ricardomartinbrualla.com/&#34;&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href=&#34;http://jasonlawrence.info/&#34;&gt;Jason Lawrence&lt;/a&gt;, &lt;a href=&#34;https://homes.cs.washington.edu/~seitz/&#34;&gt;Steven M. Seitz&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; SIGGRAPH Asia 2021 (TOG). [&lt;a href=&#34;https://arxiv.org/abs/2012.12261&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://time-travel-rephotography.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Time-Travel-Rephotography/Time-Travel-Rephotography.github.io&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPEN: GAN Prior Embedded Network for Blind Face Restoration in the Wild.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_GAN_Prior_Embedded_Network_for_Blind_Face_Restoration_in_the_CVPR_2021_paper.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://ckkelvinchan.github.io/&#34;&gt;Kelvin C.K. Chan&lt;/a&gt;, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.00739&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://ckkelvinchan.github.io/projects/GLEAN&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ckkelvinchan/GLEAN&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=KjQLROoAAAAJ&#34;&gt;Honglun Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.04061&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin.&lt;/em&gt;&lt;br&gt; CVPR 2020. [&lt;a href=&#34;https://arxiv.org/abs/2003.03808&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adamian98/pulse&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTT-GAN: Looking Through Turbulence by Inverting GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://kfmei.page/&#34;&gt;Kangfu Mei&lt;/a&gt;, &lt;a href=&#34;https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/&#34;&gt;Vishal M. Patel&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.02379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/LTT-GAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style Generator Inversion for Image Enhancement and Animation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;http://www.cs.huji.ac.il/~avivga&#34;&gt;Aviv Gabbay&lt;/a&gt;, &lt;a href=&#34;http://www.cs.huji.ac.il/~ydidh&#34;&gt;Yedid Hoshen&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; arxiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1906.11880&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;http://www.vision.huji.ac.il/style-image-prior/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/avivga/style-image-prior&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;image understanding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir Navab.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2107.12518.pdf&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://segmentation-in-style.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/warmspringwinds/segmentation_in_style&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finding an Unsupervised Image Segmenter in each of your Deep Generative Models.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi.&lt;/em&gt;&lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=Ug-bgjgSlKV&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Labels4Free: Unsupervised Segmentation using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=kEQimk0AAAAJ&amp;amp;hl=en&#34;&gt;Rameen Abdal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Gn8URq0AAAAJ&amp;amp;hl=en&#34;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/n.mitra/&#34;&gt;Niloy Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.14968&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://rameenabdal.github.io/Labels4Free&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RameenAbdal/Labels4Free&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://www.alexyuxuanzhang.com/&#34;&gt;Yuxuan Zhang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~linghuan/&#34;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&#34;https://kangxue.org/&#34;&gt;Kangxue Yin&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Jean-Francois Lafleche&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Adela Barriuso&lt;/a&gt;, &lt;a href=&#34;https://groups.csail.mit.edu/vision/torralbalab/&#34;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&#34;http://www.cs.utoronto.ca/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.06490&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nv-tlabs/datasetGAN_release&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/datasetGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repurposing GANs for One-shot Semantic Part Segmentation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Nontawat Tritrong, Pitchaporn Rewatbowornwong, &lt;a href=&#34;https://www.supasorn.com/&#34;&gt;Supasorn Suwajanakorn&lt;/a&gt;.&lt;/em&gt;&lt;br&gt; CVPR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2103.04379&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://repurposegans.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bryandlee/repurpose-gan&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;3D&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;StylePart: Image-based Shape Part Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;I-Chao Shen, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen.&lt;/em&gt;&lt;br&gt; arxiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.10520&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://jitengmu.github.io/&#34;&gt;Jiteng Mu&lt;/a&gt;, Shalini De Mello, Zhiding Yu, Nuno Vasconcelos, Xiaolong Wang, Jan Kautz, Sifei Liu.&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.16521&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://jitengmu.github.io/CoordGAN/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GAN2Shape: Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://xingangpan.github.io/&#34;&gt;Xingang Pan&lt;/a&gt;, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo.&lt;/em&gt;&lt;br&gt; ICLR 2021 (oral). [&lt;a href=&#34;https://arxiv.org/abs/2011.00844&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XingangPan/GAN2Shape&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://xingangpan.github.io/projects/GAN2Shape.html&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, &lt;a href=&#34;https://qingguo-xu.com/&#34;&gt;Qingguo Xu&lt;/a&gt;, Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.11423&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Shape Completion through GAN-Inversion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://junzhezhang.github.io/&#34;&gt;Junzhe Zhang&lt;/a&gt;, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/pdf/2104.13366&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://junzhezhang.github.io/projects/ShapeInversion/&#34;&gt;Project&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OSTeC: One-Shot Texture Completion.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Baris Gecer, Jiankang Deng, Stefanos Zafeiriou.&lt;/em&gt;&lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.15370&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/barisgecer/OSTeC&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;compressed sensing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generator Surgery for Compressed Sensing.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu, Jan-Willem van de Meent, Paul Hand.&lt;/em&gt;&lt;br&gt; NeurIPS 2020 Workshop Deep Inverse. [&lt;a href=&#34;https://arxiv.org/abs/2102.11163&#34;&gt;PDF&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nik-sm/generator-surgery&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Task-Aware Compressed Sensing with Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Maya Kabkab, Pouya Samangouei, Rama Chellappa.&lt;/em&gt;&lt;br&gt; AAAI 2018. [&lt;a href=&#34;https://arxiv.org/pdf/1802.01284.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;medical imaging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Medical Image Generation via Generative Adversarial Networks.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Zhihang Ren, Stella X. Yu, David Whitney.&lt;/em&gt;&lt;br&gt; Human Vision and Electronic Imaging 2021. [&lt;a href=&#34;https://whitneylab.berkeley.edu/PDFs/Ren_MedImageGen.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-resolution Controllable Prostatic Histology Synthesis using StyleGAN.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Gagandeep B. Daroach, Josiah A. Yoder, Kenneth A. Iczkowski, Peter S. LaViolette.&lt;/em&gt;&lt;br&gt; BIOIMAGING 2021. [&lt;a href=&#34;https://www.scitepress.org/Papers/2021/103939/103939.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;security&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Imaging via Latent Space Manipulation.&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Tao Li, Chris Clifton.&lt;/em&gt;&lt;br&gt; IEEE Symposium on Security &amp;amp; Privacy (S&amp;amp;P) 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.05472&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h2&gt;acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for the constructive comments from anonymous reviewers and feedback from &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;https://github.com/anvoynov&#34;&gt;Andrey Voynov&lt;/a&gt;, and &lt;a href=&#34;https://rushila.com/&#34;&gt;Rushil Anirudh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>