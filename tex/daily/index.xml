<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-09T01:54:04Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TomEversdijk/Bomen_en_grafen_oefeningen</title>
    <updated>2022-06-09T01:54:04Z</updated>
    <id>tag:github.com,2022-06-09:/TomEversdijk/Bomen_en_grafen_oefeningen</id>
    <link href="https://github.com/TomEversdijk/Bomen_en_grafen_oefeningen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Oefeningen Bomen en grafen&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>mohuangrui/ucasthesis</title>
    <updated>2022-06-09T01:54:04Z</updated>
    <id>tag:github.com,2022-06-09:/mohuangrui/ucasthesis</id>
    <link href="https://github.com/mohuangrui/ucasthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LaTeX Thesis Template for the University of Chinese Academy of Sciences&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;code&gt;ucasthesis&lt;/code&gt; 国科大学位论文 LaTeX 模板 [最新样式]&lt;/h1&gt; &#xA;&lt;h2&gt;模板下载&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;页面右边点击：&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE#linuxoverleaf-%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;Overleaf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;重要建议&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;关于 ucasthesis 编译和设计的问题，请先读 &lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;，如发问需遵从&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;提问流程&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;开题报告见：&lt;a href=&#34;https://github.com/mohuangrui/ucasproposal&#34;&gt;ucasproposal: 中国科学院大学开题报告 LaTeX 模板&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;书脊制作见：&lt;a href=&#34;https://github.com/mohuangrui/latexspine&#34;&gt;latexspine: LaTeX 书脊模板&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img width=&#34;50%&#34; src=&#34;https://github.com/mohuangrui/mohuangrui/raw/main/gallery/ucasthesis.gif&#34; alt=&#34;ucasthesis&#34;&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;模板简介&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ucasthesis 为撰写中国科学院大学&lt;strong&gt;本&lt;/strong&gt;、&lt;strong&gt;硕&lt;/strong&gt;、&lt;strong&gt;博&lt;/strong&gt;学位论文和&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-&#34;&gt;&lt;strong&gt;任意高校博后&lt;/strong&gt;&lt;/a&gt;报告的 LaTeX 模版。ucasthesis 提供了简单明了的&lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;。无论你是否具有 LaTeX 使用经验，都可较为轻松地使用以完成学位论文的撰写和排版。谢谢大家的测试、反馈和支持，我们一起的努力让 ucasthesis 非常荣幸地得到了国科大本科部陆晴老师、本科部学位办丁云云老师和中科院数学与系统科学研究院吴凌云研究员的支持，并得到吴凌云学长在 &lt;a href=&#34;http://www.ctex.org/HomePage&#34;&gt;CTEX&lt;/a&gt; 的发布。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;考虑到许多同学可能缺乏 LaTeX 使用经验，ucasthesis 将 LaTeX 的复杂性高度封装，开放出简单的接口，以便轻易使用。同时，对用 LaTeX 撰写论文的一些主要难题，如制图、制表、文献索引等，进行了详细说明，并提供了相应的代码样本，理解了上述问题后，对于初学者而言，使用此模板撰写学位论文将不存在实质性的困难。所以，如果你是初学者，请不要直接放弃，因为同样为初学者的我，十分明白让 LaTeX 简单易用的重要性，而这正是 ucasthesis 所追求和体现的。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;此中国科学院大学学位论文模板 ucasthesis 基于中科院数学与系统科学研究院吴凌云研究员的 CASthesis 模板发展而来。当前 ucasthesis 模板满足最新的中国科学院大学学位论文撰写要求和封面设定。兼顾操作系统：Windows、Linux、MacOS；LaTeX 编译引擎：pdflatex、xelatex、lualatex；文献编译引擎：bibtex、biber (biblatex)；文献样式：著者-出版年制（authoryear）、顺序编码制（numbers）、上标顺序编码制（super）、字符编码制（alpha）。支持中文书签、中文渲染、中文粗体显示、拷贝 PDF 中的文本到其他文本编辑器等特性（&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;Windows 系统 PDF 拷贝乱码的解决方案需见：字体配置&lt;/a&gt;）。此外，对模板的文档结构进行了精心设计，撰写了编译脚本提高模板的易用性和使用效率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ucasthesis 的目标在于简化学位论文的撰写，利用 LaTeX 格式与内容分离的特征，模板将格式设计好后，作者可只需关注论文内容。 同时，ucasthesis 有着整洁一致的代码结构和扼要的注解，对文档的仔细阅读可为初学者提供一个学习 LaTeX 的窗口。此外，模板的架构十分注重通用性，事实上，ucasthesis 不仅是国科大学位论文模板，同时，通过少量修改即可成为使用 LaTeX 撰写中英文文章或书籍的通用模板，并为使用者的个性化设定提供了接口。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;重要通知&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2021-09-27&lt;/code&gt; 模板样式进行了修改，请查看下面的修改描述，以决定是否需要更新。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新记录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-09-27&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/359&#34;&gt;benkwoook, issue #359&lt;/a&gt;，增强 artratex.sty，提供去掉“引言”类章节的章节编号的功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2021-03-30&lt;/code&gt; 更新原创性声明和使用声明页。移除英文封面声明中的 &#34;the&#34;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-07-28&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/299&#34;&gt;Tony, issue #299&lt;/a&gt;，更新 bibtex 样式。文献样式更多讨论可见：&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F&#34;&gt;文献样式&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-07-22&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/296&#34;&gt;hushidong, zepinglee, issue #296&lt;/a&gt;，完善 biblatex 和 bibtex 样式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-07-17&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/296&#34;&gt;GitatHub, hushidong, issue #296&lt;/a&gt;，更新 bibtex 国标样式 &lt;a href=&#34;https://github.com/CTeX-org/gbt7714-bibtex-style&#34;&gt;gbt7714-bibtex-style&lt;/a&gt; ，增加 biblatex 国标样式 &lt;a href=&#34;https://github.com/hushidong/biblatex-gb7714-2015&#34;&gt;biblatex-gb7714-2015&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-05-22&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/281&#34;&gt;lipcaty, issue #281&lt;/a&gt; 修复 ctex 移除 xeCJKfntef 后对 ulem 的加载。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-03-20&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/250&#34;&gt;zepinglee, issue #250&lt;/a&gt; 增加 LaTeX 和依赖宏包版本检测功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-02-11&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/182&#34;&gt;ck2019ML, issue #182&lt;/a&gt;、&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/229&#34;&gt;univeryinli, issue #229&lt;/a&gt; 将 ucasthesis 在 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE#linuxoverleaf-%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;Overleaf&lt;/a&gt; 发布并支持调用外部字体，详见&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE&#34;&gt;字体配置&lt;/a&gt;。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/231&#34;&gt;xiaokongkong, issue #231&lt;/a&gt;修正几个书写。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2020-01-09&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/223&#34;&gt;NineSH, issue #223&lt;/a&gt; 修复&lt;code&gt;bicaption&lt;/code&gt;错误。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-12-06&lt;/code&gt; 移除 commit 中的二进制文件，以极大减少 Fork 后的文件大小。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/198&#34;&gt;huiwenzhang, issue #198&lt;/a&gt; 修复&lt;code&gt;mainmatter&lt;/code&gt;下&lt;code&gt;\chapter*&lt;/code&gt;的页眉错误。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/195&#34;&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; 调整由&lt;code&gt;AutoFakeBold&lt;/code&gt;控制的伪粗体加粗程度。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/190&#34;&gt;Pantrick, issue #190&lt;/a&gt; 采用 &lt;a href=&#34;https://github.com/muzimuzhi&#34;&gt;muzimuzhi&lt;/a&gt; 提供的方法实现&lt;code&gt;\advisor{}&lt;/code&gt;和&lt;code&gt;\institute{}&lt;/code&gt;的自动换行功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/183&#34;&gt;vectorliu, issue #183&lt;/a&gt; 修改英文模式下的&lt;code&gt;plain&lt;/code&gt;选项为&lt;code&gt;scheme=plain&lt;/code&gt;以消除对&lt;code&gt;Algorithm&lt;/code&gt;样式的修改。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/177&#34;&gt;HaorenWang, issue #177&lt;/a&gt; 调整矢量、矩阵、张量字体样式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/170&#34;&gt;DRjy, issue #170&lt;/a&gt; 轻微缩减目录中编号与标题的间距；&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/174&#34;&gt;e71828, issue #174&lt;/a&gt; 轻微增加页眉中编号与标题的间距。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/169&#34;&gt;CDMA2019, issue #169&lt;/a&gt; 提供横排图表环境下页眉页脚的横排，具体使用见 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8&#34;&gt;横排图表&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; 拓展模版兼容 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-&#34;&gt;博后报告&lt;/a&gt;。修复 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/156&#34;&gt;gsp2014, issue #156&lt;/a&gt; 文献引用中的连字符的间断显示和上标引用中逗号下沉。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; 修复 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/117&#34;&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;失效问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; 修复国际生需要的&lt;code&gt;plain&lt;/code&gt;模式下无法改变英文章标题字体大小的问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; 对部分宏命令进行调整，无功能及样式上的修改。若需更新，建议参考 &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97&#34;&gt;更新指南&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/134&#34;&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ，调整行距使&lt;code&gt;LaTeX&lt;/code&gt;版与&lt;code&gt;Word&lt;/code&gt;版的行数和每行字数相一致。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/49&#34;&gt;zssasa, allenwoods, issue #49&lt;/a&gt; ，修复&lt;code&gt;bicaption&lt;/code&gt;对&lt;code&gt;longtable&lt;/code&gt;的兼容性。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/133&#34;&gt;BowenHou, issue #133&lt;/a&gt; ，使下划线能对长标题自动换行。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/127&#34;&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; ，为&lt;code&gt;摘要&lt;/code&gt;等无需在目录中显示的结构元素建立书签。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/130&#34;&gt;muzimuzhi, issue #130&lt;/a&gt; ，修正对&lt;code&gt;\voffset&lt;/code&gt;的使用。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/121&#34;&gt;opt-gaobin, issue #121&lt;/a&gt; ，修正中文标点使下划线断掉的问题。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/120&#34;&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; ，修复&lt;code&gt;\proofname&lt;/code&gt;命令对2015年及更早&lt;code&gt;LaTeX&lt;/code&gt;编译器的兼容性问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/100&#34;&gt;opt-gaobin, issue #100&lt;/a&gt; ，增加定理、定义、证明等数学环境。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/102&#34;&gt;DRjy, issue #102&lt;/a&gt; ，调整&lt;code&gt;\mathcal&lt;/code&gt;字体样式。[zike Liu, email] ，适当缩减目录列表的缩进。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/105&#34;&gt;xiaoyaoE, issue #105&lt;/a&gt; ，使数字字体和英文字体一致。完善中文版和国际版之间的中英格式切换。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/57&#34;&gt;mnpengjk, issue #57&lt;/a&gt; ，将公式编号前加点纳入模版默认，更多讨论可见：&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82&#34;&gt;琐屑细节&lt;/a&gt; 。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/95&#34;&gt;yunyun2019, issue #95&lt;/a&gt;，更新文献样式。[邵岳林, email] ，将附录复原为常规的排版设置，若需将附录置于参考文献后，请见：&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82&#34;&gt;琐屑细节&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; 根据国科大本科部陆晴老师和本科部学位办丁云云老师的复审审核建议再次修复一些样式细节问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; 模板进行了重大更新，修复了样式、字体、格式等许多问题。&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;根据国科大本科部陆晴老师的建议对模版样式进行了诸多拓展和修正，并完善对本科生论文元素的兼容性。&lt;/li&gt; &#xA;   &lt;li&gt;在 &lt;a href=&#34;https://github.com/CTeX-org/ctex-kit&#34;&gt;ctex&lt;/a&gt; 开发者的帮助下解决了如何多次调用&lt;code&gt;Times New Roman&lt;/code&gt;而不导致黑体调用错误的问题。[twn1993, email]，修复默认黑体为微软雅黑而不是&lt;code&gt;SimHei&lt;/code&gt;的问题。&lt;/li&gt; &#xA;   &lt;li&gt;繁复折腾测试后终于找出一个在&lt;code&gt;ctex&lt;/code&gt;默认黑体替换粗宋体设定环境内全局&lt;code&gt;AutoFakeBold&lt;/code&gt;失效状态下折衷特定字体库不全条件下生僻字显示和系统默认字重不全条件下粗宋体显示以及不同操作系统下如何平衡上述字库自重矛盾还有根据操作系统自动调用所带有的&lt;code&gt;Times&lt;/code&gt;字体的方案。&lt;/li&gt; &#xA;   &lt;li&gt;设定论文封面据英文学位名如自动切换。密级据是否填写自动显示。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; 演示表标题居表上，加粗图表标注，设置长图表标题悬挂缩进（由于&lt;code&gt;bicaption&lt;/code&gt;宏包无法正确接受&lt;code&gt;caption&lt;/code&gt;宏包的&lt;code&gt;margin&lt;/code&gt;选项，图表中英标题第一行无法正确同步缩进，从而放弃第一行的缩进），强调多图中子图标题的规范使用，通过摘要和符号列表演示标题不在目录中显示却仍在页眉中显示。[赵永明, email]，设置双语图表标题和&lt;code&gt;bicaption&lt;/code&gt;不在图形列表和表格列表中显示英文标题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; &lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/42&#34;&gt;zhanglinbo, issue #42&lt;/a&gt; ，使用 &lt;a href=&#34;https://github.com/xiaoyao9933/UCASthesis&#34;&gt;xiaoyao9933&lt;/a&gt; 制作的&lt;code&gt;ucas_logo.pdf&lt;/code&gt;使学校&lt;code&gt;logo&lt;/code&gt;放大不失真。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/41&#34;&gt;Starsky Wong, issue #41&lt;/a&gt; ，设置标题英文设为&lt;code&gt;Times New Roman&lt;/code&gt;。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/29&#34;&gt;will0n, issue #29&lt;/a&gt; ，&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/26&#34;&gt;Man-Ting-Fang, issue #26&lt;/a&gt; ，&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/12&#34;&gt;diyiliaoya, issue #12&lt;/a&gt; ，和 [赵永明, email] ，矫正一些格式细节问题。&lt;a href=&#34;https://github.com/mohuangrui/ucasthesis/issues/30&#34;&gt;tangjie1992, issue #30&lt;/a&gt; ，配置算法环境。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; 在 &lt;a href=&#34;https://github.com/CTeX-org/ctex-kit&#34;&gt;ctex&lt;/a&gt; 开发者的帮助下修复误用字体命令导致的粗宋体异常。然后，将模板兼容性进一步扩展为兼容操作系统&lt;code&gt;Windows&lt;/code&gt;，&lt;code&gt;Linux&lt;/code&gt;，&lt;code&gt;MacOS&lt;/code&gt;和&lt;code&gt;LaTeX &lt;/code&gt;编译引擎&lt;code&gt;pdflatex&lt;/code&gt;，&lt;code&gt;xelatex&lt;/code&gt;，&lt;code&gt;lualatex&lt;/code&gt;。移除&lt;code&gt;microtype&lt;/code&gt;宏包以提高编译效率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; 基于国科大&lt;code&gt;2018&lt;/code&gt;新版论文规范进行了重大修改，采用新的封面、声明、页眉页脚样式。展示标题中使用数学公式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; [赵永明, email] ，增加&lt;code&gt;\citepns{}&lt;/code&gt;和&lt;code&gt;\citetns{}&lt;/code&gt;命令提供上标引用下混合非上标引用的需求。[臧光明, email] ，添加设定论文为&lt;code&gt;thesis&lt;/code&gt;或&lt;code&gt;dissertation&lt;/code&gt;的命令。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>terryum/awesome-deep-learning-papers</title>
    <updated>2022-06-09T01:54:04Z</updated>
    <id>tag:github.com,2022-06-09:/terryum/awesome-deep-learning-papers</id>
    <link href="https://github.com/terryum/awesome-deep-learning-papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The most cited deep learning papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt; &#xA;&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt; &#xA;&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href=&#34;https://github.com/kjw0612/awesome-deep-vision&#34;&gt;Deep Vision&lt;/a&gt; and &lt;a href=&#34;https://github.com/kjw0612/awesome-rnn&#34;&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href=&#34;https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap&#34;&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt; &#xA;&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt; &#xA;&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href=&#34;https://twitter.com/TerryUm_ML&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;https://www.facebook.com/terryum.io/&#34;&gt;facebook page&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Awesome list criteria&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt; &#xA; &lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016&#34; section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt; &#xA; &lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt; &#xA; &lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; : +60 citations or &#34;More Papers from 2016&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; : +200 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; : +400 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; : +600 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; : +800 citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request. (Please read the &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/Contributing.md&#34;&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt; &#xA;&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/fetch_papers.py&#34;&gt;this&lt;/a&gt; and collect all authors&#39; names with &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/get_authors.py&#34;&gt;this&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/top100papers.bib&#34;&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href=&#34;https://github.com/sunshinemyson&#34;&gt;Sven&lt;/a&gt; and &lt;a href=&#34;https://github.com/grepinsight&#34;&gt;grepinsight&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#understanding--generalization--transfer&#34;&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#optimization--training-techniques&#34;&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#unsupervised--generative-models&#34;&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#convolutional-neural-network-models&#34;&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#image-segmentation--object-detection&#34;&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#image--video--etc&#34;&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#natural-language-processing--rnns&#34;&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#speech--other-domain&#34;&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#reinforcement-learning--robotics&#34;&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#more-papers-from-2016&#34;&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#new-papers&#34;&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#old-papers&#34;&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#hw--sw--dataset&#34;&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#book--survey--review&#34;&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#video-lectures--tutorials--blogs&#34;&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#appendix-more-than-top-100&#34;&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Understanding / Generalization / Transfer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href=&#34;http://arxiv.org/pdf/1503.02531&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href=&#34;http://arxiv.org/pdf/1412.1897&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href=&#34;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href=&#34;http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href=&#34;http://arxiv.org/pdf/1311.2901&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href=&#34;http://arxiv.org/pdf/1310.1531&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Jason Yosinski](https://scholar.google.ca/citations?hl=en&amp;user=gxL1qj8AAAAJ) --&gt; &#xA;&lt;h3&gt;Optimization / Training Techniques&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href=&#34;http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href=&#34;http://arxiv.org/pdf/1502.03167&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href=&#34;http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href=&#34;http://arxiv.org/pdf/1412.6980&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href=&#34;http://arxiv.org/pdf/1207.0580.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href=&#34;http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers] [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Christian Szegedy](https://scholar.google.ca/citations?hl=en&amp;user=3QeF7mAAAAAJ), [Sergey Ioffe](https://scholar.google.ca/citations?user=S5zOyIkAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&amp;user=DhtAFkwAAAAJ), [Diederik P. Kingma](https://scholar.google.ca/citations?hl=en&amp;user=yyIoQu4AAAAJ)--&gt; &#xA;&lt;h3&gt;Unsupervised / Generative Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href=&#34;http://arxiv.org/pdf/1601.06759v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href=&#34;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href=&#34;https://arxiv.org/pdf/1511.06434v2&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href=&#34;http://arxiv.org/pdf/1502.04623&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href=&#34;http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href=&#34;http://arxiv.org/pdf/1312.6114&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href=&#34;http://arxiv.org/pdf/1112.6209&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers] [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Ian Goodfellow](https://scholar.google.ca/citations?user=iYN86KEAAAAJ), [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)--&gt; &#xA;&lt;h3&gt;Convolutional Neural Network Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.07261&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.05027v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href=&#34;http://arxiv.org/pdf/1512.03385&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href=&#34;http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href=&#34;http://arxiv.org/pdf/1409.1556&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href=&#34;http://arxiv.org/pdf/1405.3531&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href=&#34;http://arxiv.org/pdf/1312.6229&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href=&#34;http://arxiv.org/pdf/1302.4389v4&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href=&#34;http://arxiv.org/pdf/1312.4400&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href=&#34;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Christian Szegedy](https://scholar.google.ca/citations?hl=en&amp;user=3QeF7mAAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&amp;user=DhtAFkwAAAAJ), [Shaoqing Ren](https://scholar.google.ca/citations?hl=en&amp;user=AUhj438AAAAJ), [Jian Sun](https://scholar.google.ca/citations?hl=en&amp;user=ALVSZAYAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Yann LeCun](https://scholar.google.ca/citations?hl=en&amp;user=WLN3QrAAAAAJ)--&gt; &#xA;&lt;h3&gt;Image: Segmentation / Object Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href=&#34;http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href=&#34;http://arxiv.org/pdf/1406.4729&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href=&#34;https://arxiv.org/pdf/1412.7062&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href=&#34;https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Ross Girshick](https://scholar.google.ca/citations?hl=en&amp;user=W8VIEZgAAAAJ), [Jeff Donahue](https://scholar.google.ca/citations?hl=en&amp;user=UfbuDH8AAAAJ), [Trevor Darrell](https://scholar.google.ca/citations?hl=en&amp;user=bh-uRFMAAAAJ)--&gt; &#xA;&lt;h3&gt;Image / Video / Etc&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href=&#34;https://arxiv.org/pdf/1501.00092v3.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.06576&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href=&#34;http://arxiv.org/pdf/1502.03044&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;:&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href=&#34;http://vision.stanford.edu/pdf/karpathy14.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href=&#34;http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Andrej Karpathy](https://scholar.google.ca/citations?user=l8WuQJgAAAAJ)--&gt; &#xA;&lt;!--[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)--&gt; &#xA;&lt;h3&gt;Natural Language Processing / RNNs&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href=&#34;http://aclweb.org/anthology/N/N16/N16-1030.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.02410&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href=&#34;http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.04025&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href=&#34;https://arxiv.org/pdf/1410.3916&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href=&#34;https://arxiv.org/pdf/1410.5401&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href=&#34;http://arxiv.org/pdf/1409.0473&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href=&#34;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href=&#34;http://arxiv.org/pdf/1406.1078&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href=&#34;http://arxiv.org/pdf/1404.2188v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href=&#34;http://arxiv.org/pdf/1408.5882&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href=&#34;http://anthology.aclweb.org/D/D14/D14-1162.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href=&#34;http://arxiv.org/pdf/1405.4053&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href=&#34;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href=&#34;http://arxiv.org/pdf/1301.3781&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href=&#34;https://arxiv.org/pdf/1308.0850&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Kyunghyun Cho](https://scholar.google.ca/citations?user=0RAmmIAAAAAJ), [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Richard Socher](https://scholar.google.ca/citations?hl=en&amp;user=FaOcyfMAAAAJ), [Tomas Mikolov](https://scholar.google.ca/citations?user=oBu8kMMAAAAJ), [Christopher D. Manning](https://scholar.google.ca/citations?user=1zmDOdwAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ)--&gt; &#xA;&lt;h3&gt;Speech / Other Domain&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.04395&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href=&#34;https://arxiv.org/pdf/1512.02595&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href=&#34;http://arxiv.org/pdf/1303.5778.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href=&#34;http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href=&#34;http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Dong Yu](https://scholar.google.ca/citations?hl=en&amp;user=tMY31_gAAAAJ)--&gt; &#xA;&lt;h3&gt;Reinforcement Learning / Robotics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.02199&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href=&#34;https://arxiv.org/pdf/1509.06461.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href=&#34;http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href=&#34;https://arxiv.org/pdf/1509.02971&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href=&#34;http://www.davidqiu.com:8888/research/nature14236.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href=&#34;http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href=&#34;http://arxiv.org/pdf/1312.5602.pdf&#34;&gt;[pdf]&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!--[Key researchers]  [Sergey Levine](https://scholar.google.ca/citations?user=8R35rCwAAAAJ), [Volodymyr Mnih](https://scholar.google.ca/citations?hl=en&amp;user=rLdfJ1gAAAAJ), [David Silver](https://scholar.google.ca/citations?user=-8DNE4UAAAAJ)--&gt; &#xA;&lt;h3&gt;More Papers from 2016&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href=&#34;https://arxiv.org/pdf/1607.06450v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href=&#34;http://arxiv.org/pdf/1606.04474v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href=&#34;https://arxiv.org/pdf/1609.03499v2&#34;&gt;[pdf]&lt;/a&gt; &lt;a href=&#34;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.08511&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href=&#34;https://arxiv.org/pdf/1609.03552&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href=&#34;https://arxiv.org/pdf/1512.02325&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.07360&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href=&#34;http://arxiv.org/pdf/1602.01528&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href=&#34;https://arxiv.org/pdf/1602.02830&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href=&#34;https://www.gwern.net/docs/2016-graves.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Google&#39;s neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href=&#34;https://arxiv.org/pdf/1609.08144&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;New papers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href=&#34;https://arxiv.org/pdf/1704.04861.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href=&#34;https://arxiv.org/pdf/1705.03122&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href=&#34;https://arxiv.org/pdf/1702.01932&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href=&#34;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.07511v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.03864v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.06211v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href=&#34;https://128.84.21.199/pdf/1703.06870&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href=&#34;http://arxiv.org/pdf/1703.05192v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href=&#34;http://arxiv.org/pdf/1702.07825v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href=&#34;http://arxiv.org/pdf/1702.06506v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href=&#34;https://arxiv.org/abs/1702.03275&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href=&#34;https://arxiv.org/pdf/1701.07875v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href=&#34;https://arxiv.org/pdf/1611.03530&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href=&#34;https://arxiv.org/abs/1611.04076v2&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Old Papers&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href=&#34;http://arxiv.org/pdf/1103.0398&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href=&#34;http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href=&#34;http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href=&#34;http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href=&#34;http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href=&#34;http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href=&#34;http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href=&#34;http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;HW / SW / Dataset&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.05250.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.01540&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href=&#34;http://arxiv.org/pdf/1603.04467&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt; &#xA; &lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href=&#34;https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href=&#34;http://arxiv.org/pdf/1412.4564&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href=&#34;http://arxiv.org/pdf/1409.0575&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href=&#34;http://arxiv.org/pdf/1408.5093&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Book / Survey / Review&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href=&#34;https://arxiv.org/pdf/1702.07800&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href=&#34;http://arxiv.org/pdf/1701.07274v2.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href=&#34;http://arxiv.org/pdf/1703.01619v1.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/index.html&#34;&gt;[html]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href=&#34;http://www.deeplearningbook.org/&#34;&gt;[html]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href=&#34;https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href=&#34;https://arxiv.org/pdf/1606.05908&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href=&#34;http://arxiv.org/pdf/1404.7828&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href=&#34;http://arxiv.org/pdf/1206.5538&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href=&#34;http://cs231n.stanford.edu/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href=&#34;http://cs224d.stanford.edu/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href=&#34;https://github.com/oxford-cs-deepnlp-2017/lectures&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href=&#34;https://nips.cc/Conferences/2016/Schedule?type=Tutorial&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href=&#34;http://techtalks.tv/icml/2016/tutorials/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href=&#34;http://videolectures.net/iclr2016_san_juan/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href=&#34;http://videolectures.net/deeplearning2016_montreal/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href=&#34;https://www.bayareadlschool.org/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI &lt;a href=&#34;https://www.openai.com/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Distill &lt;a href=&#34;http://distill.pub/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Andrej Karpathy Blog &lt;a href=&#34;http://karpathy.github.io/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Colah&#39;s Blog &lt;a href=&#34;http://colah.github.io/&#34;&gt;[Web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WildML &lt;a href=&#34;http://www.wildml.com/&#34;&gt;[Web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastML &lt;a href=&#34;http://www.fastml.com/&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TheMorningPaper &lt;a href=&#34;https://blog.acolyer.org&#34;&gt;[web]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Appendix: More than Top 100&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.06147&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href=&#34;http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html&#34;&gt;[html]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href=&#34;https://arxiv.org/pdf/1503.00949&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href=&#34;https://arxiv.org/pdf/1505.03540&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href=&#34;https://arxiv.org/pdf/1610.09038&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href=&#34;https://ishmaelbelghazi.github.io/ALI/&#34;&gt;[web]&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/pdf/1606.00704v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href=&#34;https://arxiv.org/pdf/1605.09081v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href=&#34;https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href=&#34;http://arxiv.org/pdf/1603.08983&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href=&#34;https://arxiv.org/pdf/1608.06993v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt; &#xA; &lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v48/gu16.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.02858&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href=&#34;https://arxiv.org/pdf/1604.00788&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href=&#34;https://arxiv.org/pdf/1606.01781&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href=&#34;https://arxiv.org/pdf/1607.01759&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href=&#34;https://arxiv.org/pdf/1601.01705&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href=&#34;https://arxiv.org/pdf/1603.08155&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href=&#34;http://arxiv.org/pdf/1412.1842&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href=&#34;https://arxiv.org/pdf/1502.05082&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href=&#34;http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href=&#34;https://arxiv.org/pdf/1603.09382&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href=&#34;http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href=&#34;http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href=&#34;http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mind&#39;s eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href=&#34;http://arxiv.org/pdf/1502.05698&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href=&#34;http://arxiv.org/pdf/1506.07285&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href=&#34;https://arxiv.org/pdf/1510.00149&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href=&#34;https://arxiv.org/pdf/1503.00075&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href=&#34;https://arxiv.org/pdf/1508.06615&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href=&#34;http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href=&#34;https://arxiv.org/pdf/1505.04366v1&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href=&#34;https://arxiv.org/pdf/1506.06579&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep generative image models using a￼ laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href=&#34;http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v37/chung15.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href=&#34;https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href=&#34;http://papers.nips.cc/paper/5866-pointer-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href=&#34;https://arxiv.org/pdf/1506.02078&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href=&#34;http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href=&#34;http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href=&#34;https://arxiv.org/pdf/1506.05869.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href=&#34;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&#34;&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href=&#34;http://aclweb.org/anthology/P/P15/P15-1033.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href=&#34;http://aclweb.org/anthology/D/D15/D15-1041.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href=&#34;http://aclweb.org/anthology/D/D15/D15-1176.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href=&#34;https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href=&#34;http://arxiv.org/pdf/1406.6247.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href=&#34;https://arxiv.org/pdf/1412.3555&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href=&#34;https://arxiv.org/pdf/1410.8206&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt; &#xA; &lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href=&#34;http://arxiv.org/pdf/1409.2329&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href=&#34;https://arxiv.org/pdf/1312.6199.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href=&#34;http://www.jmlr.org/proceedings/papers/v32/graves14.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href=&#34;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href=&#34;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href=&#34;https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href=&#34;http://www.aclweb.org/anthology/N13-1#page=784&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href=&#34;http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href=&#34;http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf&#34;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href=&#34;https://github.com/terryum/awesome-deep-learning-papers/raw/master/Contributing.md&#34;&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;&lt;img src=&#34;http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg?sanitize=true&#34; alt=&#34;CC0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To the extent possible under law, &lt;a href=&#34;https://www.facebook.com/terryum.io/&#34;&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</summary>
  </entry>
</feed>