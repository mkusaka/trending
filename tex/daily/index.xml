<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub TeX Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-02T01:41:47Z</updated>
  <subtitle>Daily Trending of TeX in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>eellak/glossAPI</title>
    <updated>2023-12-02T01:41:47Z</updated>
    <id>tag:github.com,2023-12-02:/eellak/glossAPI</id>
    <link href="https://github.com/eellak/glossAPI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;Στόχος αυτού του έργου είναι η ανάπτυξη ενός Ελληνικού γλωσσικού μοντέλου ανοιχτού λογισμικού &#34;Greek OSS LLM&#34;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🚀&lt;/span&gt; &lt;strong&gt;Τρέχουσα δράση: Καταγραφή - αποτίμηση ανοιχτών πηγών κειμένου στα Ελληνικά&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;➡&lt;/span&gt; Συμβουλευτείτε το CONTRIBUTING.md για να συνεισφέρετε στην &lt;span&gt;🎯&lt;/span&gt; συγκέντρωση και προτεραιοποίηση συνόλων κειμενικών δεδομένων στα Ελληνικά.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;➡&lt;/span&gt; Επισκεφτείτε το &lt;a href=&#34;https://github.com/eellak/glossAPI/wiki/%CE%9A%CE%B1%CF%84%CE%B1%CE%B3%CF%81%CE%B1%CF%86%CE%AE-%CE%A0%CE%B7%CE%B3%CF%8E%CE%BD&#34;&gt;wiki&lt;/a&gt; για να δείτε ή να τροποποιήσετε τις καταγεγραμμένες πηγές.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;➡&lt;/span&gt; Έχετε μια ιδέα που δεν βλέπετε στο αποθετήριο; Θέλετε να προτείνετε μια διόρθωση; 🚩 Ανοίξτε ένα &lt;a href=&#34;https://github.com/eellak/glossAPI/issues&#34;&gt;Issue&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;📰&lt;/span&gt; &lt;strong&gt;Διαβάστε την αρθρογραφία μας για τα γλωσσικά μοντέλα και τις διεθνείς εξελίξεις&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://edu.ellak.gr/2023/04/11/nevronika-diktia-ke-michaniki-mathisi/&#34;&gt;Νευρωνικά Δίκτυα και Μηχανική Μάθηση&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openstandards.ellak.gr/2023/10/26/anichtos-kodikas-ke-proigmena-glossika-nevronika-diktia/&#34;&gt;Ανοιχτός Κώδικας και Προηγμένα Γλωσσικά Νευρωνικά Δίκτυα&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.ellak.gr/2023/11/21/ti-xeroume-gia-tis-ragdees-exelixis-stin-openai-pou-elavan-chora-tis-teleftees-72-ores/&#34;&gt;Τί ξέρουμε για τις ραγδαίες εξελίξεις στην OpenAI...&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Εισαγωγικό σημείωμα στα γλωσσικά μοντέλα&lt;/h3&gt; &#xA;&lt;p&gt;Τα γλωσσικά μοντέλα είναι στην ουσία νευρωνικά δίκτυα εκπαιδευμένα σε μεγάλα σύνολα δεδομένων. Η εντυπωσιακή τους επίδοση οφείλεται στην διαμόρφωση αφηρημένων εσωτερικών αναπαραστάσεων της γλώσσας, που τους επιτρέπει να επιδόνται με επιτυχία σε σειρά έργων.&lt;/p&gt; &#xA;&lt;p&gt;Επομένως η κρίσιμη συνθήκη για την εκπαίδευση ενός τέτοιου μοντέλου στα Ελληνικά είναι η έκθεσή του σε κατάλληλο αριθμό κειμενικών παραδειγμάτων στην ελληνική γλώσσα. Σκοπός αυτής της εκπαίδευσης είναι να αποκτήσει το μοντέλο ανάλογη ευελιξία στην ελληνική γλώσσα με αυτήν που επιδεικνύει στα αγγλικά. Σε αυτό πρέπει να ληφθεί υπόψη η ιστορικότητα της ελληνικής γλώσσας και οι διάφορες ποικιλίες που εμφανίζονται συγχρονικά ή διαχρονικά.&lt;/p&gt; &#xA;&lt;h4&gt;Επισκόπηση των διαθέσιμων μοντέλων και δεδομένων στα Ελληνικά&lt;/h4&gt; &#xA;&lt;p&gt;Προέχει με συστηματικό τρόπο να αξιολογηθούν τα διαθέσιμα μοντέλα &lt;a href=&#34;https://huggingface.co/search/full-text?q=greek&amp;amp;type=model&#34;&gt;huggingface&lt;/a&gt; και τα δεδομένα &lt;a href=&#34;https://huggingface.co/search/full-text?q=greek&amp;amp;type=dataset&#34;&gt;huggingface-dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Η προεκπαίδευση στα ελληνικά είναι σημαντική αλλά δεν πρέπει να είναι από μόνη της κριτήριο. &lt;strong&gt;Το είδος της προεκπαίδευσης που μας ωφελεί είναι εκείνο που μας ανακουφίζει από το υπολογιστικό και χρονικό κόστος&lt;/strong&gt;. Προτιμάμε ένα μοντέλο που ήδη καταλαβαίνει κάποια ελληνικά, αλλά για να δούμε αξιόλογες επιδόσεις σε έργα με μικρά περιθώρια λάθους, θα πρέπει το υπό εξέταση μοντέλο να έχει επιπλέον, τόσο ευρεία σημασιολογική γνώση, όσο και απαρτιωμένη αντίληψη διαγλωσσικών παραμέτρων.&lt;/p&gt; &#xA;&lt;p&gt;Επιπλέον, αυτός είναι ο λόγος που προτιμούμε να βαθμονομήσουμε ένα μοντέλο από το να αναπτύξουμε ένα από το μηδέν.&lt;/p&gt; &#xA;&lt;p&gt;Πρέπει να ληφθεί υπόψη ότι και για τις δοκιμές θα πρέπει να συστηθεί μια αποτελεσματική ομάδα έργου με ικανούς πόρους στη διάθεσή της, όπως εκείνους που παρέχονται από το &lt;a href=&#34;https://hpc.grnet.gr/&#34;&gt;HPC&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Συγκέντρωση, διαλογή, καθαρισμός και επισημείωση διαθέσιμων σωμάτων δεδομένων&lt;/h4&gt; &#xA;&lt;p&gt;Όπως αναφέρθηκε οι γλωσσικές ποικιλίες της ελληνικής γλώσσας, καθώς και τα εξειδικευμένα λεξιλόγιά της, οι επιστημονικές ορολογίες, οι διαφορετικοί τόνοι (επίσημος, ανεπίσημος, γραπτός, προφορικός, κλπ), θα πρέπει να εκπροσωπούνται με συστηματικό τρόπο στα δεδομένα εκπαίδευσης.&lt;/p&gt; &#xA;&lt;p&gt;Επιπλέον τα δεδομένα αυτά θα πρέπει να παρουσιαστούν με δομημένο τρόπο στα προγράμματα εκπαίδευσης των νευρωνικών δικτύων, και μάλιστα με τέτοια αναπαράσταση που να μην επιβαρύνει άσκοπα τους υπολογιστικούς πόρους.&lt;/p&gt; &#xA;&lt;p&gt;Πρόκειται επομένως για ένα αρκετά απαιτητικό έργο διαχείρισης δεδομένων, το οποίο όσο καλύτερα γίνει τόσο πιο εντυπωσιακά αναμένεται να είναι τα αποτελέσματα της εκπαίδευσης.&lt;/p&gt; &#xA;&lt;p&gt;Πρέπει ακόμα να ληφθεί υπόψη ότι αντιμετωπίζουμε μία πληθώρα επιλογών σε μία συγκυρία ραγδαίων εξελίξεων. Προκύπτει επομένως ότι θα πρέπει να δοκιμαστούν και να αξιολογηθούν διαφορετικά μοντέλα. Εξ ού προκύπτει με τη σειρά της η ανάγκη ανάπτυξης πρότυπων συνόλων δεδομένων προς διασταύρωση της επίδοσης των εναλλακτικών μοντέλων σε δοσμένα έργα-ορόσημα όπως η ταξινόμηση κείμένου, η συμπλήρωση κενών, κλπ&lt;/p&gt; &#xA;&lt;p&gt;Αυτό συνεπάγεται αρκετή ανθρωποπροσπάθεια σε συνεργατική ανάπτυξη μοναδιαίων τεστ κώδικα, αλλά και επισημείωση και επαλήθευση των δεδομένων.&lt;/p&gt; &#xA;&lt;h4&gt;Βαθμονόμηση των μοντέλων στην Ελληνική γλώσσα&lt;/h4&gt; &#xA;&lt;p&gt;Απαραίτητο βήμα για να πάμε σε εξειδικευμένα έργα είναι να διδάξουμε στο μοντέλο καλύτερα ελληνικά.&lt;/p&gt; &#xA;&lt;p&gt;Τα μοντέλα φέρουν ήδη σημασιολογική και πραγματολογική γνώση, καθώς και αφαιρέσεις στατιστικών κανονικοτήτων που διατρέχουν πολλές γλώσσες. Αυτό δίνει ένα προβάδιμα, για τη σκοποθεσία μας, στα προεκπαιδευμένα μοντέλα, αφού εκπαιδεύοντας ένα νευρωνικό δίκτυο από το μηδέν στις γλωσσικές ποικιλίες της ελληνικής, χάνουμε όλη αυτήν την γνώση υποβάθρου αλλά και τις διαγλωσσικές συστηματικότητες που τα προεκπαιδευμένα μοντέλα ήδη κατέχουν.&lt;/p&gt; &#xA;&lt;p&gt;Με αυτόν το τρόπο, αναμένουμε, η έκθεση σε ικανό αριθμό καλά δομημένων παραδειγμάτων στην ελληνική γλώσσα, θα επιτρέψει στο μοντέλο να αποκτήσει ευελιξία με αυτήν, πράγμα που είμαστε σε προνομιακή θέση να επιτύχουμε, αλλά η επιτυχία αυτή θα εξαρτηθεί από τον όγκο, την ποιότητα, και την αντιπροσωπευτικότητα των δεδομένων εκπαίδευσης.&lt;/p&gt; &#xA;&lt;p&gt;Απαραίτητη σε αυτό το στάδιο είναι η εξασφάλιση υπολογιστικών πόρων, από το HPC, τα πανεπιστήμια, τα ερευνητικά κέντρα, αλλά και η εξέταση των κατανεμημένων πρωτοκόλλων εκπαίδευσης γλωσσικών μοντέλων. Η κατανεμημένη εκπαίδευση θα επέτρεπε λχ να ασχολούνται διαφορετικές ερευνητικές ομάδες με διαφορετικά υποσύνολα των δεδομένων εκπαίδευσης, και να ενημερώνουν τις παραμέτρους ενός κεντρικού, απομακρυσμένου μοντέλου.&lt;/p&gt; &#xA;&lt;p&gt;Φυσικά αυτή η λύση απαιτεί αποτελεσματικότατη διαχείριση έργου, και ισχυρές σχέσεις συνεργασίας ανάμεσα σε διαφορετικούς οργανισμούς και ομάδες, που ίσως πρέπει να οικοδομηθεί με τις κατάλληλες ενέργειες, όπως εργαστήρια, συναντήσεις εργασίας, και άλλα δρώμενα.&lt;/p&gt; &#xA;&lt;h4&gt;Βαθμονόμηση των ελληνοποιημένων μοντέλων σε ειδικά έργα και ανταπόκριση σε οδηγίες&lt;/h4&gt; &#xA;&lt;p&gt;Σε αυτό το στάδιο εκπαιδεύουμε τα μοντέλα, τα οποία ήδη θα έχουμε φέρει σε ένα επίπεδο κατανόησης της ελληνικής γλώσσας, έτσι ώστε να ακολουθούν οδηγίες και να επιτελούν συγκεκριμένα έργα, όπως&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;μετάφραση&lt;/li&gt; &#xA; &lt;li&gt;περίληψη&lt;/li&gt; &#xA; &lt;li&gt;παραγωγή κειμένου&lt;/li&gt; &#xA; &lt;li&gt;απάντηση σε ερωτήσεις&lt;/li&gt; &#xA; &lt;li&gt;ανάκτηση πληροφοριών&lt;/li&gt; &#xA; &lt;li&gt;ταξιμόνηση και επισημείωση κειμένου&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Για τα έργα αυτά θα πρέπει να καταρτιστούν ειδικά σύνολα δεδομένων εκπαίδευσης που να αντανακλούν τη δομή του ζητούμενου έργου, και να επιτρέπουν την ποσοτική και ποιοτική αξιολόγηση της επίδοσης. Και εδώ ισχύουν όσα αναφέρθηκαν για τη συγκέντρωση και διαλογή δεδομένων.&lt;/p&gt; &#xA;&lt;p&gt;Η ποιότητα και αυτών των δεδομένων εκπαίδευσης, η οποία μπορεί επιπλέον να είναι η ίδια μια ανατροφοδοτούμενη διαδικασία με κύκλους εκπαίδευσης, αξιολόγησης, αναθεώρησης του συνόλου δεδομένων εκπαίδευσης, των παραμέτρων του μοντέλου, και επανεκπαίδευσής του.&lt;/p&gt; &#xA;&lt;p&gt;Αυτό προϋποθέτει την συνέχεια της απασχόλησης των εμπλεκομένων ατόμων στο έργο, και καλές πρακτικές διακυβέρνησης των γνωσιακών βάσεων, του πηγαίου κώδικα, και των βάσεων δεδομένων του έργου.&lt;/p&gt; &#xA;&lt;h4&gt;Αξιολόγηση της επίδοσης και βαθμονόμηση παραμέτρων&lt;/h4&gt; &#xA;&lt;p&gt;Εκτός από την αναθεώρηση και την διακυβέρνηση των δεδομένων εκπαίδευσης, θα πρέπει να επιτηρούμε στενά και τις παραμέτρους εκπαίδευσης του μοντέλου, όπως το ρυθμό μάθησης, τα μεγέθη κατατεμαχισμού των δεδομένων, τους αριθμούς επαναλήψεων, και άλλων τεχνικών όψεων, για τις οποίες καθ᾽ ύλην αρμόδιοι είναι οι ειδικοί μηχανικής μάθησης, και ιδίως των νευρωνικών δικτύων.&lt;/p&gt; &#xA;&lt;p&gt;Και εδώ είναι σημαντική η κατάρτιση πρότυπων έλεγχων επίδοσης για τα επιμέρους έργα που να παρουσιάζονται με ευσύνοπτο συστηματικό τρόπο.&lt;/p&gt; &#xA;&lt;p&gt;Τα αποτελέσματα θα πρέπει να παρουσιάζονται στη διοίκηση του έργου με τους καθιερωμένους τρόπους και τις μετρικές με τις οποίες αξιολογείται η ποιότητα προσαρμογής και η επίδοση του μοντέλου σε διαφορετικές όψεις του έργου, έτσι ώστε το κρίσιμο αυτό στάδιο της εκπαίδευσης να οδηγήσει σε ένα αξιόπιστης επίδοσης, γενικεύσιμο μοντέλο, που να μπορεί να έχει πραγματικές χρήσεις σε διαφορετικούς τομείς.&lt;/p&gt; &#xA;&lt;h4&gt;Αυτοτροφοδοτούμενη μάθηση με ανθρώπινη επίβλεψη&lt;/h4&gt; &#xA;&lt;p&gt;Με τις τεχνικές του reinforcement learning μπορεί να τελειοποιηθεί η επίδοση του μοντέλου, έτσι ώστε να προσαρμοστεί σε μια συνεχιζόμενη ανατροφοδότηση που θα λαμβάνει από ανθρώπινους δοκιμαστές, όπως πχ μέσα από την αξιολόγηση των απαντήσεών του ως ικανοποιητικών ή μη.&lt;/p&gt; &#xA;&lt;p&gt;Αυτό προυποθέτει την ανάπτυξη ενός συστήματος ανατροφοδότησης της επίδοσης και την διεπαφή αξιολόγησης από τους δοκιμαστικούς χρήστες.&lt;/p&gt; &#xA;&lt;p&gt;Καθ᾽υλην αρμόδιοι για αυτό είναι οι ειδικοί μηχανικής μάθησης, κατά προτίμηση με εμπειρία στο reinforcement learning, και ειδικοί στην ανάπτυξη διεπαφών χρήστη για γλωσσικά μοντέλα που ακολουθούν οδηγίες.&lt;/p&gt; &#xA;&lt;h4&gt;Χρήση του μοντέλου σε διαφορετικούς τομείς και επαναξιολόγηση&lt;/h4&gt; &#xA;&lt;p&gt;Εφόσον το μοντέλο αποκτήσει ικανοποιητική, γενική και ειδική, επίδοση μπορεί να χρησιμοποιηθεί σε διαφορετικά έργα, ή να ανοιχτεί ως API, και να συγκεντρωθούν επιπλέον στοιχεία για την συνεχιζόμενη βελτίωση της επίδοσής του.&lt;/p&gt; &#xA;&lt;p&gt;Αυτό προϋποθέτει τις απαραίτητες υποδομές για να φιλοξενηθεί και να μπορεί να εξυπηρετεί πολλαπλά αιτήματα χεηστών. Πρέπει να ληφθεί υπόψη ότι η επιτυχής εκπαίδευση ενός τέτοιου έργου στα ελληνικά θα αξιοποιηθεί ευρέως από κάθε είδους εφαρμογές, και μπορεί να προεξοφληθεί ότι οι απαιτήσεις φιλοξενίας επίσης μπορεί να μην είναι αμελητέες.&lt;/p&gt; &#xA;&lt;h4&gt;Ενδεικτικά, μια τέτοια ομάδα θα μπορούσε να αποτελείται από:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Μηχανικούς τεχνητής νοημοσύνης και γνωσιακής επιστήμης&lt;/li&gt; &#xA; &lt;li&gt;Μηχανικούς NLP και υπολογιστικής γλωσσολογίας&lt;/li&gt; &#xA; &lt;li&gt;Αναλυτές δεδομένων&lt;/li&gt; &#xA; &lt;li&gt;Ειδικούς πληροφοριακών συστημάτων&lt;/li&gt; &#xA; &lt;li&gt;Ειδικούς σε μεταδεδομένα, οντολογίες, ελεγχόμενα λεξιλόγια&lt;/li&gt; &#xA; &lt;li&gt;Ειδικούς κοινωνικής γλωσσολογίας&lt;/li&gt; &#xA; &lt;li&gt;Ειδικούς ποιοτικής ανάλυσης περιεχομένου&lt;/li&gt; &#xA; &lt;li&gt;Κατά τομέα ειδικούς πχ νομικά&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>poligenius/LaTeX_resume</title>
    <updated>2023-12-02T01:41:47Z</updated>
    <id>tag:github.com,2023-12-02:/poligenius/LaTeX_resume</id>
    <link href="https://github.com/poligenius/LaTeX_resume" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Here the LaTeX code of my personal resume, you can use it to build your own cv.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AltaCV, LaTeX CV/Résumé class&lt;/h1&gt; &#xA;&lt;p&gt;v1.1 (25 July 2023), by Marco Marini (&lt;a href=&#34;mailto:poligeniushelp@gmail.com&#34;&gt;poligeniushelp@gmail.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;This repo is to give you, LaTeX lover, a cool idea for a resume. You can use this template to create in few minutes your own personal cv.&lt;/p&gt; &#xA;&lt;p&gt;Below you can fine a selection of point to follow to personalize your cv, have fun. For any question you can write me at &lt;a href=&#34;mailto:poligeniushelp@gmail.com&#34;&gt;poligeniushelp@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resume Look&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/poligenius/LaTeX_resume/assets/48245313/8e26f6ce-0f2d-4018-98ce-39d80d6e98db&#34; alt=&#34;Screenshot 2023-10-07 alle 15 01 09&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Editor&lt;/h2&gt; &#xA;&lt;p&gt;In case you don&#39;t know LaTeX, don&#39;t worry, you can use &lt;a href=&#34;https://overleaf.com&#34;&gt;Overleaf&lt;/a&gt;, it is a cool online free editor, just create an account, create a new project and load the files inside this repo.&lt;/p&gt; &#xA;&lt;p&gt;To have the files on your pc just clone this repo:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;select the location where you want to store the file in your terminal&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Projects/resume&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;clone the repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/poligenius/LaTeX_resume.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;After you have loaded the files you will need only to change the content of the files in order to write whatever you want in your resume. It is really intuitive, in case you need more info on what to modify you can check the following sections.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Compilation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;At line 76 of main you can insert your personal info&lt;/li&gt; &#xA; &lt;li&gt;page1sidebar.tex contains the code for the right part of the cv&lt;/li&gt; &#xA; &lt;li&gt;remember to substitute the image with your photo and use a png format&lt;/li&gt; &#xA; &lt;li&gt;If you would like to change the colors go at line 48 of main.tex&lt;/li&gt; &#xA; &lt;li&gt;AltaCV uses &lt;a href=&#34;http://www.ctan.org/pkg/fontawesome&#34;&gt;&lt;code&gt;fontawesome&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;http://www.ctan.org/pkg/academicons&#34;&gt;&lt;code&gt;academicons&lt;/code&gt;&lt;/a&gt;; they&#39;re included in both TeX Live 2016 and MikTeX 2.9.&lt;/li&gt; &#xA; &lt;li&gt;Loading &lt;code&gt;academicons&lt;/code&gt; is optional: enable it by adding the &lt;code&gt;academicons&lt;/code&gt; option to &lt;code&gt;\documentclass&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Can now be compiled with pdflatex, XeLaTeX and LuaLaTeX!&lt;/li&gt; &#xA; &lt;li&gt;However if you&#39;re using &lt;code&gt;academicons&lt;/code&gt;, you &lt;em&gt;must&lt;/em&gt; use either XeLaTeX or LuaLaTeX. If the doc then compiles but the icons don&#39;t show up in the output PDF, try compiling with LuaLaTeX instead.&lt;/li&gt; &#xA; &lt;li&gt;The samples here use the &lt;a href=&#34;http://www.latofonts.com/lato-free-fonts/&#34;&gt;Lato&lt;/a&gt; font.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;The code is here for you to be read, edited, modified, reused, for free, do whatever you want! However, if you&#39;d like to &lt;a href=&#34;https://paypal.me/MarcoMariniING?country.x=IT&amp;amp;locale.x=it_IT&#34;&gt;offer me&lt;/a&gt; a coffee or a beer, then cheers! xD&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>synlp/RRG-Review</title>
    <updated>2023-12-02T01:41:47Z</updated>
    <id>tag:github.com,2023-12-02:/synlp/RRG-Review</id>
    <link href="https://github.com/synlp/RRG-Review" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official github repository of the survey paper &#34;A Systematic Review of Deep Learning-based Research on Radiology Report Generation&#34;.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;A Systematic Review of Deep Learning-based Research on Radiology Report Generation&lt;/h1&gt; &#xA;&lt;p&gt;The official GitHub repository of the survey paper titled &#34;&lt;a href=&#34;https://arxiv.org/abs/2311.14199&#34;&gt;A Systematic Review of Deep Learning-based Research on Radiology Report Generation&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;We maintain this repository to summarize papers and resources related to the radiology report generation (RRG) task.&lt;/p&gt; &#xA;&lt;p&gt;In &lt;code&gt;reference.bib&lt;/code&gt;, we summarize the &lt;code&gt;bibtex&lt;/code&gt; references of existing RRG papers, widely used datasets, and related toolkits.&lt;/p&gt; &#xA;&lt;p&gt;If you have any suggestions about papers, code implementations, and other resources, please feel free to &lt;a href=&#34;https://github.com/synlp/RRG-Review/issues&#34;&gt;start a new issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/synlp/RRG-Review/pulls&#34;&gt;pull requests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If your research is related to our work, please cite the following paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;@misc{liu2023systematic,&#xA;      title={A Systematic Review of Deep Learning-based Research on Radiology Report Generation}, &#xA;      author={Chang Liu and Yuanhe Tian and Yan Song},&#xA;      year={2023},&#xA;      eprint={2311.14199},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#2023&#34;&gt;2023&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#2022&#34;&gt;2022&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#2021&#34;&gt;2021&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#2020&#34;&gt;2020&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#2019&#34;&gt;2019&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#2018&#34;&gt;2018&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/synlp/RRG-Review/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;2023&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ACL 2023: &lt;a href=&#34;https://aclanthology.org/2023.acl-long.451/&#34;&gt;ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Wenjun Hou, Kaishuai Xu, Yi Cheng, Wenjie Li, Jiang Liu&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ACL 2023: &lt;a href=&#34;https://aclanthology.org/2023.findings-acl.683.pdf&#34;&gt;Replace and Report: NLP Assisted Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Kaveri Kale, Pushpak Bhattacharyya, Kshitij Jadhav&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;CVPR 2023: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2023_paper.pdf&#34;&gt;Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Mingjie Li, Bingqian Li, Zicong Chen, Haokun Lin, Xiaodan Liang, Xiaojun Chang&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;CVPR 2023: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Tanida_Interactive_and_Explainable_Region-Guided_Radiology_Report_Generation_CVPR_2023_paper.pdf&#34;&gt;Interactive and Explainable Region-guided Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Tim Tanida, Philip Muller, Georgios Kaissis, Daniel Rueckert&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;CVPR 2023: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf&#34;&gt;KiUT: Knowledge-injected U-Transformer for Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Zhongzhen Huang, Xiaofan Zhang, Shaoting Zhang&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;CVPR 2023: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_METransformer_Radiology_Report_Generation_by_Transformer_With_Multiple_Learnable_Expert_CVPR_2023_paper.pdf&#34;&gt;METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;IEEE Transactions on Multimedia, 2023: &lt;a href=&#34;https://ieeexplore.ieee.org/document/9606584&#34;&gt;Joint Embedding of Deep Visual and Semantic Features for Medical Image Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Yan Yang, Jun Yu, Jian Zhang, Weidong Han, Hanliang Jiang, Qingming Huang&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;IEEE Transactions on Multimedia 2023: &lt;a href=&#34;https://ieeexplore.ieee.org/document/10119200&#34;&gt;Semi-supervised Medical Report Generation via Graph-guided Hybrid Feature Consistency&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Ke Zhang, Hanliang Jiang, Jian Zhang, Qingming Huang, Jianping Fan, Jun Yu, Weidong Han&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;WWW 2023: &lt;a href=&#34;https://arxiv.org/abs/2006.03744&#34;&gt;Auxiliary Signal‑guided Knowledge Encoder‑decoder for&amp;nbsp;Medical Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Mingjie&amp;nbsp;Li, Rui&amp;nbsp;Liu, Fuyu&amp;nbsp;Wang, Xiaojun&amp;nbsp;Chang, Xiaodan&amp;nbsp;Liang&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EACL 2023: &lt;a href=&#34;https://aclanthology.org/2023.eacl-main.246/&#34;&gt;KGVL-BART: Knowledge Graph Augmented Visual Language BART for Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Kaveri Kale, Pushpak Bhattacharyya, Milind Gune, Aditya Shetty, Rustom Lawyer&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2022&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ACL 2022: &lt;a href=&#34;https://aclanthology.org/2022.findings-acl.38/&#34;&gt;Reinforced Cross-modal Alignment for Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Han Qin, Yan Song&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;IEEE Transactions on Medical Imaging 2022: &lt;a href=&#34;https://ieeexplore.ieee.org/document/9768661&#34;&gt;Automated Radiographic Report Generation Purely on Transformer: A Multicriteria Supervised Approach&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Zhanyu Wang, Hongwei Han, Lei Wang&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EMNLP, 2022: &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.319/&#34;&gt;Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, Curtis P. Langlotz&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EMNLP 2022: &lt;a href=&#34;https://aclanthology.org/2022.emnlp-main.480/&#34;&gt;Factual Accuracy is not Enough: Planning Consistent Description Order for Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Toru Nishino, Yasuhide Miura, Tomoki Taniguchi, Tomoko Ohkuma, Yuki Suzuki, Shoji Kido, Noriyuki Tomiyama&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Datasets: JLiverCT, MIMIC-CXR&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;AACL 2022: &lt;a href=&#34;https://aclanthology.org/2022.aacl-main.47/&#34;&gt;Multimodal Generation of Radiology Reports using Knowledge-Grounded Extraction of Entities and Relations&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Francesco Dalla Serra, William Clackett, Chaoyang Wang, Hamish MacKinnon, Fani Deligianni, Jeffrey Dalton, Alison Q O’Neil&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2021&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ACL 2021: &lt;a href=&#34;https://aclanthology.org/2021.acl-long.459.pdf&#34;&gt;Cross-modal Memory Networks for Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;COLING 2021: &lt;a href=&#34;https://aclanthology.org/2022.coling-1.523/&#34;&gt;JPG - Jointly Learn to Align: Automated Disease Prediction and Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Jingyi You, Dongyuan Li, Manabu Okumura, Kenji Suzuki&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EMNLP 2021: &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.241/&#34;&gt;Progressive Transformer-Based Generation of Radiology Reports&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Farhad Nooralahzadeh, Nicolas Perez Gonzales, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EMNLP 2021: &lt;a href=&#34;https://aclanthology.org/2021.findings-emnlp.336.pdf&#34;&gt;Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;An Yan, Zexue He, Xing Lu, Jiang Du, Eric Chang, Amilcare Gentili, Julian McAuley, Chun-Nan Hsu&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;CVPR 2021: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_Prior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf&#34;&gt;Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, Yuexian zou&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;CVPR 2021: &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_A_Self-Boosting_Framework_for_Automated_Radiographic_Report_Generation_CVPR_2021_paper.pdf&#34;&gt;A Self-boosting Framework for Automated Radiographic Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Zhanyu Wang, Luping Zhou, Lei Wang&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ICCV 2021: &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Visual-Textual_Attentive_Semantic_Consistency_for_Medical_Report_Generation_ICCV_2021_paper.pdf&#34;&gt;Visual-Textual Attentive Semantic Consistency for Medical Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Yi Zhou, Lei Huang, Tao Zhou, Huazhu Fu, Ling Shao&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2020&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EMNLP 2020: &lt;a href=&#34;https://aclanthology.org/2020.emnlp-main.112/&#34;&gt;Generating Radiology Reports via Memory-driven Transformer&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Zhihong Chen, Yan Song, Tsung-Hui Chang, Xiang Wan&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;EMNLP 2020: &lt;a href=&#34;https://aclanthology.org/2020.findings-emnlp.202/&#34;&gt;Reinforcement Learning with Imbalanced Dataset for Data-to-Text Medical Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Toru Nishino, Ryuji Kano, Ryota Ozaki, Norihisa Nakano, Tomoko Ohkuma, Fuji Xerox&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;AAAI 2020: &lt;a href=&#34;https://arxiv.org/abs/2002.08277&#34;&gt;When Radiology Report Generation Meets Knowledge Graph&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Yuille, Daguang Xu&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2019&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ACL 2019: &lt;a href=&#34;https://aclanthology.org/P19-1657/&#34;&gt;Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-Ray Reports&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Baoyu Jing, Zeya Wang, Eric Xing&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;AAAI 2019: &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/4637/4515&#34;&gt;Knowledge-Driven Encode, Retrieve, Paraphrase for Medical Image Report Generation&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2018&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ACL 2018: &lt;a href=&#34;https://aclanthology.org/P18-1240/&#34;&gt;On the Automatic Generation of Medical Imaging Reports&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Baoyu Jing, Pengtao Xie, Eric P. Xing&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;J. Am. Medical Informatics Assoc.: &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/26133894/&#34;&gt;Preparing A Collection of Radiology Examinations for Distribution and Retrieval&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Dina Demner-Fushman, Marc D. Kohli, Marc B. Rosenman, Sonya E. Shooshan, Laritza Rodriguez, Sameer Antani, George R. Thoma, Clement J. McDonald&lt;/p&gt; &#xA;  &lt;p&gt;Official website: &lt;a href=&#34;https://openi.nlm.nih.gov/faq&#34;&gt;https://openi.nlm.nih.gov/faq&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Scientific Data: &lt;a href=&#34;https://www.nature.com/articles/s41597-019-0322-0&#34;&gt;IMIC-CXR, A De-identified Publicly Available Database of Chest Radiographs with Free-text Reports&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Roger G. Mark &amp;amp; Steven Horng&lt;/p&gt; &#xA;  &lt;p&gt;Official website: &lt;a href=&#34;https://physionet.org/content/mimic-cxr/1.0.0/&#34;&gt;https://physionet.org/content/mimic-cxr/1.0.0/&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;ArXiv: &lt;a href=&#34;https://arxiv.org/abs/1901.07042&#34;&gt;MIMIC-CXR-JPG, A Large Publicly Available Database of Labeled Chest Radiographs&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Roger G. Mark &amp;amp; Steven Horng&lt;/p&gt; &#xA;  &lt;p&gt;Official website: &lt;a href=&#34;https://physionet.org/content/mimic-cxr/2.0.0/&#34;&gt;https://physionet.org/content/mimic-cxr/2.0.0/&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>