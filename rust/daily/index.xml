<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-08T01:35:02Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tensorchord/VectorChord</title>
    <updated>2024-12-08T01:35:02Z</updated>
    <id>tag:github.com,2024-12-08:/tensorchord/VectorChord</id>
    <link href="https://github.com/tensorchord/VectorChord" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scalable, Fast, and Disk-friendly Vector search in Postgres, the Successor of pgvecto.rs.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;VectorChord&lt;/h1&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt;Effortlessly host 100 million 768-dimensional vectors (250GB+) on an AWS i4i.xlarge instance ($250/month), featuring 4 vCPUs and 32GB of RAM with VectorChord.&lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/KqswhpVgdU&#34;&gt;&lt;img alt=&#34;discord invitation link&#34; src=&#34;https://dcbadge.vercel.app/api/server/KqswhpVgdU?style=flat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/TensorChord&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/tensorchord?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/tensorchord/vchord-postgres&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/tensorchord/vchord-postgres&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt;Prior release: &lt;a href=&#34;https://hub.docker.com/r/tensorchord/pgvecto-rs&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/tensorchord/pgvecto-rs&#34; alt=&#34;Previous Docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;VectorChord (vchord) is a PostgreSQL extension designed for scalable, high-performance, and disk-efficient vector similarity search, and serves as the successor to &lt;a href=&#34;https://github.com/tensorchord/pgvecto.rs&#34;&gt;pgvecto.rs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With VectorChord, you can store 400,000 vectors for just $1, enabling significant savings: 6x more vectors compared to Pinecone&#39;s optimized storage and 26x more than pgvector/pgvecto.rs for the same price[^1]. For further insights, check out our &lt;a href=&#34;https://blog.pgvecto.rs/vectorchord-store-400k-vectors-for-1-in-postgresql&#34;&gt;launch blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[^1]: Based on &lt;a href=&#34;https://myscale.github.io/benchmark/#/&#34;&gt;MyScale Benchmark&lt;/a&gt; with 768-dimensional vectors and 95% recall.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;VectorChord introduces remarkable enhancements over pgvecto.rs and pgvector:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;⚡ Enhanced Performance&lt;/strong&gt;: Delivering optimized operations with up to 5x faster queries, 16x higher insert throughput, and 16x quicker[^3] index building compared to pgvector&#39;s HNSW implementation.&lt;/p&gt; &#xA;&lt;p&gt;[^3]: Based on &lt;a href=&#34;https://myscale.github.io/benchmark/#/&#34;&gt;MyScale Benchmark&lt;/a&gt; with 768-dimensional vectors. Please checkout our &lt;a href=&#34;https://blog.pgvecto.rs/vectorchord-store-400k-vectors-for-1-in-postgresql&#34;&gt;blog post&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;💰 Affordable Vector Search&lt;/strong&gt;: Query 100M 768-dimensional vectors using just 32GB of memory, achieving 35ms P50 latency with top10 recall@95%, helping you keep infrastructure costs down while maintaining high search quality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🔌 Seamless Integration&lt;/strong&gt;: Fully compatible with pgvector data types and syntax while providing optimal defaults out of the box - no manual parameter tuning needed. Just drop in VectorChord for enhanced performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🔧 External Index Build&lt;/strong&gt;: Leverage IVF to build indexes externally (e.g., on GPU) for faster KMeans clustering, combined with RaBitQ[^2] compression to efficiently store vectors while maintaining search quality through autonomous reranking.&lt;/p&gt; &#xA;&lt;p&gt;[^2]: Gao, Jianyang, and Cheng Long. &#34;RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search.&#34; Proceedings of the ACM on Management of Data 2.3 (2024): 1-27.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;For new users, we recommend using the Docker image to get started quickly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run \&#xA;  --name vectorchord-demo \&#xA;  -e POSTGRES_PASSWORD=mysecretpassword \&#xA;  -p 5432:5432 \&#xA;  -d tensorchord/vchord-postgres:pg17-v0.1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can connect to the database using the &lt;code&gt;psql&lt;/code&gt; command line tool. The default username is &lt;code&gt;postgres&lt;/code&gt;, and the default password is &lt;code&gt;mysecretpassword&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;psql -h localhost -p 5432 -U postgres&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the following SQL to ensure the extension is enabled.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;CREATE EXTENSION IF NOT EXISTS vchord CASCADE;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And make sure to add &lt;code&gt;vchord.so&lt;/code&gt; to the &lt;code&gt;shared_preload_libraries&lt;/code&gt; in &lt;code&gt;postgresql.conf&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;-- Add vchord and pgvector to shared_preload_libraries --&#xA;ALTER SYSTEM SET shared_preload_libraries = &#39;vchord.so&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create the VectorChord RaBitQ(vchordrq) index, you can use the following SQL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;CREATE INDEX ON gist_train USING vchordrq (embedding vector_l2_ops) WITH (options = $$&#xA;residual_quantization = true&#xA;[build.internal]&#xA;lists = [4096]&#xA;spherical_centroids = false&#xA;$$);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Query&lt;/h3&gt; &#xA;&lt;p&gt;The query statement is exactly the same as pgvector. VectorChord supports any filter operation and WHERE/JOIN clauses like pgvecto.rs with VBASE.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;SELECT * FROM items ORDER BY embedding &amp;lt;-&amp;gt; &#39;[3,1,2]&#39; LIMIT 5;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Supported distance functions are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&amp;lt;-&amp;gt; - L2 distance&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;#&amp;gt; - (negative) inner product&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;=&amp;gt; - cosine distance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ### Range Query&#xA;&#xA;&gt; [!NOTE]  &#xA;&gt; Due to the limitation of postgresql query planner, we cannot support the range query like `SELECT embedding &lt;-&gt; &#39;[3,1,2]&#39; as distance WHERE distance &lt; 0.1 ORDER BY distance` directly.&#xA;&#xA;To query vectors within a certain distance range, you can use the following syntax.&#xA;```SQL&#xA;-- Query vectors within a certain distance range&#xA;-- sphere(center, radius) means the vectors within the sphere with the center and radius, aka range query&#xA;-- &lt;&lt;-&gt;&gt; is L2 distance, &lt;&lt;#&gt;&gt; is inner product, &lt;&lt;=&gt;&gt; is cosine distance&#xA;SELECT vec FROM t WHERE vec &lt;&lt;-&gt;&gt; sphere(&#39;[0.24, 0.24, 0.24]&#39;::vector, 0.012) &#xA;``` --&gt; &#xA;&lt;h3&gt;Query Performance Tuning&lt;/h3&gt; &#xA;&lt;p&gt;You can fine-tune the search performance by adjusting the &lt;code&gt;probes&lt;/code&gt; and &lt;code&gt;epsilon&lt;/code&gt; parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- Set probes to control the number of lists scanned. &#xA;-- Recommended range: 3%–10% of the total `lists` value.&#xA;SET vchordrq.probes = 100;&#xA;&#xA;-- Set epsilon to control the reranking precision.&#xA;-- Larger value means more rerank for higher recall rate.&#xA;-- Don&#39;t change it unless you only have limited memory.&#xA;-- Recommended range: 1.0–1.9. Default value is 1.9.&#xA;SET vchordrq.epsilon = 1.9;&#xA;&#xA;-- vchordrq relies on a projection matrix to optimize performance.&#xA;-- Add your vector dimensions to the `prewarm_dim` list to reduce latency.&#xA;-- If this is not configured, the first query will have higher latency as the matrix is generated on demand.&#xA;-- Default value: &#39;64,128,256,384,512,768,1024,1536&#39;&#xA;-- Note: This setting requires a database restart to take effect.&#xA;ALTER SYSTEM SET vchordrq.prewarm_dim = &#39;64,128,256,384,512,768,1024,1536&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And for postgres&#39;s setting&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;-- If using SSDs, set `effective_io_concurrency` to 200 for faster disk I/O.&#xA;SET effective_io_concurrency = 200;&#xA;&#xA;-- Disable JIT (Just-In-Time Compilation) as it offers minimal benefit (1–2%) &#xA;-- and adds overhead for single-query workloads.&#xA;SET jit = off;&#xA;&#xA;-- Allocate at least 25% of total memory to `shared_buffers`. &#xA;-- For disk-heavy workloads, you can increase this to up to 90% of total memory. You may also want to disable swap with network storage to avoid io hang.&#xA;-- Note: A restart is required for this setting to take effect.&#xA;ALTER SYSTEM SET shared_buffers = &#39;8GB&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Indexing prewarm&lt;/h3&gt; &#xA;&lt;p&gt;To prewarm the index, you can use the following SQL. It will significantly improve performance when using limited memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;-- vchordrq_prewarm(index_name::regclass) to prewarm the index into the shared buffer&#xA;SELECT vchordrq_prewarm(&#39;gist_train_embedding_idx&#39;::regclass)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Index Build Time&lt;/h3&gt; &#xA;&lt;p&gt;Index building can parallelized, and with external centroid precomputation, the total time is primarily limited by disk speed. Optimize parallelism using the following settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;-- Set this to the number of CPU cores available for parallel operations.&#xA;SET max_parallel_maintenance_workers = 8;&#xA;SET max_parallel_workers = 8;&#xA;&#xA;-- Adjust the total number of worker processes. &#xA;-- Note: A restart is required for this setting to take effect.&#xA;ALTER SYSTEM SET max_worker_processes = 8;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Indexing Progress&lt;/h3&gt; &#xA;&lt;p&gt;You can check the indexing progress by querying the &lt;code&gt;pg_stat_progress_create_index&lt;/code&gt; view.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;SELECT phase, round(100.0 * blocks_done / nullif(blocks_total, 0), 1) AS &#34;%&#34; FROM pg_stat_progress_create_index;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;External Index Precomputation&lt;/h3&gt; &#xA;&lt;p&gt;Unlike pure SQL, an external index precomputation will first do clustering outside and insert centroids to a PostgreSQL table. Although it might be more complicated, external build is definitely much faster on larger dataset (&amp;gt;5M).&lt;/p&gt; &#xA;&lt;p&gt;To get started, you need to do a clustering of vectors using &lt;code&gt;faiss&lt;/code&gt;, &lt;code&gt;scikit-learn&lt;/code&gt; or any other clustering library.&lt;/p&gt; &#xA;&lt;p&gt;The centroids should be preset in a table of any name with 3 columns:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;id(integer): id of each centroid, should be unique&lt;/li&gt; &#xA; &lt;li&gt;parent(integer, nullable): parent id of each centroid, should be NULL for normal clustering&lt;/li&gt; &#xA; &lt;li&gt;vector(vector): representation of each centroid, &lt;code&gt;pgvector&lt;/code&gt; vector type&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And example could be like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- Create table of centroids&#xA;CREATE TABLE public.centroids (id integer NOT NULL UNIQUE, parent integer, vector vector(768));&#xA;-- Insert centroids into it&#xA;INSERT INTO public.centroids (id, parent, vector) VALUES (1, NULL, &#39;{0.1, 0.2, 0.3, ..., 0.768}&#39;);&#xA;INSERT INTO public.centroids (id, parent, vector) VALUES (2, NULL, &#39;{0.4, 0.5, 0.6, ..., 0.768}&#39;);&#xA;INSERT INTO public.centroids (id, parent, vector) VALUES (3, NULL, &#39;{0.7, 0.8, 0.9, ..., 0.768}&#39;);&#xA;-- ...&#xA;&#xA;-- Create index using the centroid table&#xA;CREATE INDEX ON gist_train USING vchordrq (embedding vector_l2_ops) WITH (options = $$&#xA;[build.external]&#xA;table = &#39;public.centroids&#39;&#xA;$$);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To simplify the workflow, we provide end-to-end scripts for external index pre-computation, see &lt;a href=&#34;https://raw.githubusercontent.com/tensorchord/VectorChord/main/scripts/README.md#run-external-index-precomputation-toolkit&#34;&gt;scripts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installing From Source&lt;/h3&gt; &#xA;&lt;p&gt;Install pgrx according to &lt;a href=&#34;https://github.com/pgcentralfoundation/pgrx?tab=readme-ov-file#getting-started&#34;&gt;pgrx&#39;s instruction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo install --locked cargo-pgrx&#xA;cargo pgrx init --pg17 $(which pg_config) # To init with system postgres, with pg_config in PATH&#xA;cargo pgrx install --release --sudo # To install the extension into the system postgres with sudo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Type Support: Currently, only the &lt;code&gt;f32&lt;/code&gt; data type is supported for vectors.&lt;/li&gt; &#xA; &lt;li&gt;Architecture Compatibility: The fast-scan kernel is optimized for x86_64 architectures. While it runs on aarch64, performance may be lower.&lt;/li&gt; &#xA; &lt;li&gt;KMeans Clustering: The built-in KMeans clustering is not yet fully optimized and may require substantial memory. We strongly recommend using external centroid precomputation for efficient index construction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/tensorchord/VectorChord/main/LICENSE&#34;&gt;GNU Affero General Public License v3.0&lt;/a&gt; and as commercial software. For commercial licensing, please contact us at &lt;a href=&#34;mailto:support@tensorchord.ai&#34;&gt;support@tensorchord.ai&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vlcn-io/cr-sqlite</title>
    <updated>2024-12-08T01:35:02Z</updated>
    <id>tag:github.com,2024-12-08:/vlcn-io/cr-sqlite</id>
    <link href="https://github.com/vlcn-io/cr-sqlite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convergent, Replicated SQLite. Multi-writer and CRDT support for SQLite&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cr-sqlite - Convergent, Replicated, SQLite&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/c-tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/c-tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;c-tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/c-valgrind.yaml&#34;&gt;&lt;img src=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/c-valgrind.yaml/badge.svg?sanitize=true&#34; alt=&#34;c-valgrind&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/py-tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/py-tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;py-tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/rs-tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/vlcn-io/cr-sqlite/actions/workflows/rs-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;rs-tests&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A component of the &lt;a href=&#34;https://vlcn.io&#34;&gt;vulcan&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/AtdVY6zDW3&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/AtdVY6zDW3&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Examples&lt;/h1&gt; &#xA;&lt;p&gt;Example applications using cr-sqlite to sync state.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vite starter - &lt;a href=&#34;https://vite-starter2.fly.dev/&#34;&gt;Example&lt;/a&gt; | &lt;a href=&#34;https://github.com/vlcn-io/vite-starter&#34;&gt;Repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TodoMVC - &lt;a href=&#34;https://vlcn-live-examples.fly.dev/&#34;&gt;Example&lt;/a&gt; | &lt;a href=&#34;https://github.com/vlcn-io/live-examples&#34;&gt;Repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Azarattum/CRStore&#34;&gt;Svelte Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vlcn.io/docs/cr-sqlite/networking/whole-crr-sync&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tantaman/strut&#34;&gt;WIP Local-First Presentation Editor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Basic setup &amp;amp; sync via an &lt;a href=&#34;https://observablehq.com/@tantaman/cr-sqlite-basic-setup&#34;&gt;Observable Notebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&#34;It&#39;s like Git, for your data.&#34;&lt;/h1&gt; &#xA;&lt;p&gt;CR-SQLite is a &lt;a href=&#34;https://www.sqlite.org/loadext.html&#34;&gt;run-time loadable extension&lt;/a&gt; for &lt;a href=&#34;https://www.sqlite.org/index.html&#34;&gt;SQLite&lt;/a&gt; and &lt;a href=&#34;https://github.com/libsql/libsql&#34;&gt;libSQL&lt;/a&gt;. It allows merging different SQLite databases together that have taken independent writes.&lt;/p&gt; &#xA;&lt;p&gt;In other words, you can write to your SQLite database while offline. I can write to mine while offline. We can then both come online and merge our databases together, without conflict.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In technical terms:&lt;/strong&gt; cr-sqlite adds multi-master replication and partition tolerance to SQLite via conflict free replicated data types (&lt;a href=&#34;https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type&#34;&gt;CRDTs&lt;/a&gt;) and/or causally ordered event logs.&lt;/p&gt; &#xA;&lt;h1&gt;When is this useful?&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Syncing data between devices&lt;/li&gt; &#xA; &lt;li&gt;Implementing realtime collaboration&lt;/li&gt; &#xA; &lt;li&gt;Offline editing&lt;/li&gt; &#xA; &lt;li&gt;Being resilient to network conditions&lt;/li&gt; &#xA; &lt;li&gt;Enabling instantaneous interactions&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All of the above involve a merging of independent edits problem. If your database can handle this for you, you don&#39;t need custom code in your application to handle those 5 cases.&lt;/p&gt; &#xA;&lt;p&gt;Discussions of these problems in the application space:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://museapp.com/podcast/56-sync/&#34;&gt;Meta Muse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwareengineeringdaily.com/2020/03/31/facebook-messenger-engineering-with-mohsen-agsen/&#34;&gt;FB Messenger re-write&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;Companies: &lt;a href=&#34;https://turso.tech&#34;&gt;&lt;img src=&#34;https://images.ctfassets.net/8fv5t5my8687/01j7yaLj77zqmYK62Y49g7/aee841e7bd176864aa5388448db0f8ef/iku-turquoise.svg?sanitize=true&#34; width=&#34;64&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fly.io&#34;&gt;&lt;img src=&#34;https://fly.io/static/images/brand/brandmark.svg?sanitize=true&#34; height=&#34;64&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://reflect.app/&#34;&gt;&lt;img src=&#34;https://reflect.app/_next/image?url=%2Fsite%2Ficons%2F1024x1024.png&amp;amp;w=64&amp;amp;q=100&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://expo.dev&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/12504344?s=200&amp;amp;v=4&#34; width=&#34;64&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://electric-sql.com&#34;&gt;&lt;img width=&#34;108&#34; alt=&#34;Screenshot 2023-11-16 at 8 29 27 AM&#34; src=&#34;https://github.com/vlcn-io/cr-sqlite/assets/1009003/5c0c8ab3-005a-4b03-ba0a-de7ed213e26d&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Individuals: &lt;a href=&#34;https://github.com/robinvasan&#34;&gt;robinvasan&lt;/a&gt; | &lt;a href=&#34;https://github.com/iansinnott&#34;&gt;iansinnott&lt;/a&gt; | &lt;a href=&#34;https://github.com/davefowler&#34;&gt;davefowler&lt;/a&gt; | &lt;a href=&#34;https://github.com/barbalex&#34;&gt;barbalex&lt;/a&gt; | &lt;a href=&#34;https://github.com/MohannadNaj&#34;&gt;MohannadNaj&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Perf&lt;/h1&gt; &#xA;&lt;p&gt;Perf data: &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/raw/main/py/perf/perf.ipynb&#34;&gt;https://github.com/vlcn-io/cr-sqlite/blob/main/py/perf/perf.ipynb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Currently inserts into CRRs are 2.5x slower than inserts into regular SQLite tables.&lt;/li&gt; &#xA; &lt;li&gt;Reads are the same speed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;The full documentation site is available &lt;a href=&#34;https://vlcn.io/docs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;crsqlite&lt;/code&gt; exposes three main APIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A function extension (&lt;code&gt;crsql_as_crr&lt;/code&gt;) to upgrade existing tables to &#34;crrs&#34; or &#34;conflict free replicated relations&#34; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SELECT crsql_as_crr(&#39;table_name&#39;)&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;A virtual table (&lt;code&gt;crsql_changes&lt;/code&gt;) to ask the database for changesets or to apply changesets from another database &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SELECT &#34;table&#34;, &#34;pk&#34;, &#34;cid&#34;, &#34;val&#34;, &#34;col_version&#34;, &#34;db_version&#34;, &#34;site_id&#34;, cl, seq FROM crsql_changes WHERE db_version &amp;gt; x AND site_id = crsql_site_id()&lt;/code&gt; -- to get local changes&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SELECT &#34;table&#34;, &#34;pk&#34;, &#34;cid&#34;, &#34;val&#34;, &#34;col_version&#34;, &#34;db_version&#34;, &#34;site_id&#34;, cl, seq FROM crsql_changes WHERE db_version &amp;gt; x AND site_id != some_site_id&lt;/code&gt; -- to get all changes excluding those synced from some actor&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;INSERT INTO crsql_changes VALUES ([patches received from select on another peer])&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;And &lt;code&gt;crsql_begin_alter(&#39;table_name&#39;)&lt;/code&gt; &amp;amp; &lt;code&gt;crsql_alter_commit(&#39;table_name&#39;)&lt;/code&gt; primitives to allow altering table definitions that have been upgraded to &lt;code&gt;crr&lt;/code&gt;s. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Until we move forward with extending the syntax of SQLite to be CRR aware, altering CRRs looks like: &lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT crsql_begin_alter(&#39;table_name&#39;);&#xA;-- 1 or more alterations to `table_name`&#xA;ALTER TABLE table_name ...;&#xA;SELECT crsql_commit_alter(&#39;table_name&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; A future version of cr-sqlite may extend the SQL syntax to make this more natural.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Application code uses the function extension to enable crr support on tables.&lt;/p&gt; &#xA;&lt;p&gt;Networking code uses the &lt;code&gt;crsql_changes&lt;/code&gt; virtual table to fetch and apply changes.&lt;/p&gt; &#xA;&lt;p&gt;Usage looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- load the extension if it is not statically linked&#xA;.load crsqlite&#xA;.mode qbox&#xA;-- create tables as normal&#xA;create table foo (a primary key not null, b);&#xA;create table baz (a primary key not null, b, c, d);&#xA;&#xA;-- update those tables to be crrs / crdts&#xA;select crsql_as_crr(&#39;foo&#39;);&#xA;select crsql_as_crr(&#39;baz&#39;);&#xA;&#xA;-- insert some data / interact with tables as normal&#xA;insert into foo (a,b) values (1,2);&#xA;insert into baz (a,b,c,d) values (&#39;a&#39;, &#39;woo&#39;, &#39;doo&#39;, &#39;daa&#39;);&#xA;&#xA;-- ask for a record of what has changed&#xA;select &#34;table&#34;, &#34;pk&#34;, &#34;cid&#34;, &#34;val&#34;, &#34;col_version&#34;, &#34;db_version&#34;, &#34;site_id&#34;, &#34;cl&#34;, &#34;seq&#34; from crsql_changes;&#xA;&#xA;┌───────┬─────────────┬─────┬───────┬─────────────┬────────────┬──────────────────────────────────────┬────┬─────┐&#xA;│ table │     pk      │ cid │  val  │ col_version │ db_version │ &#34;site_id&#34; │ cl │ seq │&#xA;├───────┼─────────────┼─────┼───────┼─────────────┼────────────┼──────────────────────────────────────┼────┼─────┤&#xA;│ &#39;foo&#39; │ x&#39;010901&#39;   │ &#39;b&#39; │ 2     │ 1           │ 1          │ x&#39;049c48eadf4440d7944ed9ec88b13ea5&#39;  │ 1  │ 0   │&#xA;│ &#39;baz&#39; │ x&#39;010b0161&#39; │ &#39;b&#39; │ &#39;woo&#39; │ 1           │ 2          │ x&#39;049c48eadf4440d7944ed9ec88b13ea5&#39;  │ 1  │ 0   │&#xA;│ &#39;baz&#39; │ x&#39;010b0161&#39; │ &#39;c&#39; │ &#39;doo&#39; │ 1           │ 2          │ x&#39;049c48eadf4440d7944ed9ec88b13ea5&#39;  │ 1  │ 1   │&#xA;│ &#39;baz&#39; │ x&#39;010b0161&#39; │ &#39;d&#39; │ &#39;daa&#39; │ 1           │ 2          │ x&#39;049c48eadf4440d7944ed9ec88b13ea5&#39;  │ 1  │ 2   │&#xA;└───────┴─────────────┴─────┴───────┴─────────────┴────────────┴──────────────────────────────────────┴────┴─────┘&#xA;&#xA;-- merge changes from a peer&#xA;insert into crsql_changes&#xA;  (&#34;table&#34;, &#34;pk&#34;, &#34;cid&#34;, &#34;val&#34;, &#34;col_version&#34;, &#34;db_version&#34;, &#34;site_id&#34;, &#34;cl&#34;, &#34;seq&#34;)&#xA;  values&#xA;  (&#39;foo&#39;, x&#39;010905&#39;, &#39;b&#39;, &#39;thing&#39;, 5, 5, X&#39;7096E2D505314699A59C95FABA14ABB5&#39;, 1, 0);&#xA;insert into crsql_changes (&#34;table&#34;, &#34;pk&#34;, &#34;cid&#34;, &#34;val&#34;, &#34;col_version&#34;, &#34;db_version&#34;, &#34;site_id&#34;, &#34;cl&#34;, &#34;seq&#34;)&#xA;  values&#xA;  (&#39;baz&#39;, x&#39;010b0161&#39;, &#39;b&#39;, 123, 101, 233, X&#39;7096E2D505314699A59C95FABA14ABB5&#39;, 1, 0);&#xA;&#xA;-- check that peer&#39;s changes were applied&#xA;sqlite&amp;gt; select * from foo;&#xA;┌───┬─────────┐&#xA;│ a │    b    │&#xA;├───┼─────────┤&#xA;│ 1 │ 2       │&#xA;│ 5 │ &#39;thing&#39; │&#xA;└───┴─────────┘&#xA;&#xA;select * from baz;&#xA;┌─────┬─────┬───────┬───────┐&#xA;│  a  │  b  │   c   │   d   │&#xA;├─────┼─────┼───────┼───────┤&#xA;│ &#39;a&#39; │ 123 │ &#39;doo&#39; │ &#39;daa&#39; │&#xA;└─────┴─────┴───────┴───────┘&#xA;&#xA;-- tear down the extension before closing the connection&#xA;-- https://sqlite.org/forum/forumpost/c94f943821&#xA;select crsql_finalize();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Packages&lt;/h1&gt; &#xA;&lt;p&gt;Pre-built binaries of the extension are available in the &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/releases&#34;&gt;releases section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These can be loaded into &lt;code&gt;sqlite&lt;/code&gt; via the &lt;a href=&#34;https://www.sqlite.org/loadext.html#loading_an_extension&#34;&gt;&lt;code&gt;load_extension&lt;/code&gt; command&lt;/a&gt; from any language (Python, NodeJS, C++, Rust, etc.) that has SQLite bindings.&lt;/p&gt; &#xA;&lt;p&gt;The entrypoint to the loadable extension is &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/raw/92df9b4f3a6bdf2bd7c5d9a76949496fa5dc88cf/core/src/crsqlite.c#L536&#34;&gt;&lt;code&gt;sqlite3_crsqlite_init&lt;/code&gt; &lt;/a&gt; so you&#39;ll either need to provide that to &lt;code&gt;load_extension&lt;/code&gt; or rename your binary to &lt;code&gt;crsqlite.[dylib/dll/so]&lt;/code&gt;. See the linked sqlite &lt;a href=&#34;https://www.sqlite.org/loadext.html#loading_an_extension&#34;&gt;&lt;code&gt;load_extension&lt;/code&gt; docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;load_extension(extension_path, &#39;sqlite3_crsqlite_init&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: if you&#39;re using &lt;code&gt;cr-sqlite&lt;/code&gt; as a run time loadable extension, loading the extension should be the &lt;em&gt;first&lt;/em&gt; operation you do after opening a connection to the database. The extension needs to be loaded on every connection you create.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For a WASM build that works in the browser, see the &lt;a href=&#34;https://github.com/vlcn-io/js&#34;&gt;js&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;For UI integrations (e.g., React) see the &lt;a href=&#34;https://github.com/vlcn-io/js&#34;&gt;js&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h1&gt;How does it work?&lt;/h1&gt; &#xA;&lt;p&gt;There are two approaches with very different tradeoffs. Both will eventually be supported by &lt;code&gt;cr-sqlite&lt;/code&gt;. &lt;code&gt;v1&lt;/code&gt; (and current releases) support the first approach. &lt;code&gt;v2&lt;/code&gt; will support both approaches.&lt;/p&gt; &#xA;&lt;h2&gt;Approach 1: History-free CRDTs&lt;/h2&gt; &#xA;&lt;p&gt;Approach 1 is characterized by the following properties:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Keeps no history / only keeps the current state&lt;/li&gt; &#xA; &lt;li&gt;Automatically handles merge conflicts. No options for manual merging.&lt;/li&gt; &#xA; &lt;li&gt;Tables are Grow Only Sets or variants of Observe-Remove Sets&lt;/li&gt; &#xA; &lt;li&gt;Rows are maps of CRDTs. The column names being the keys, column values being a specific CRDT type&lt;/li&gt; &#xA; &lt;li&gt;Columns can be counter, fractional index or last write wins CRDTs. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;multi-value registers, RGA and others to come in future iterations&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Tables which should be synced are defined as a composition of other types of CRDTs.&lt;/p&gt; &#xA;&lt;p&gt;Example table definition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE CLSet post (&#xA; id INTEGER PRIMARY KEY NOT NULL,&#xA; views COUNTER,&#xA; content PERITEXT,&#xA; owner_id LWW INTEGER&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;note: given that extensions can&#39;t extend the SQLite syntax this is notional. We are, however, extending the libSQL syntax so this will be available in that fork. In base SQLite you&#39;d run the &lt;code&gt;select crsql_as_crr&lt;/code&gt; function as seen earlier.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CLSet - &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3380787.3393678&#34;&gt;causal length set&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;COUNTER - &lt;a href=&#34;https://www.cs.utexas.edu/~rossbach/cs380p/papers/Counters.html&#34;&gt;distributed counter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PERITEXT - &lt;a href=&#34;https://www.inkandswitch.com/peritext/&#34;&gt;collaborative text&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Under approach 1, merging two tables works roughly like so:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Rows are identified by primary key&lt;/li&gt; &#xA; &lt;li&gt;Tables are unioned (and a delete log is consulted) such that both tables will have the same rows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If a row was modified in multiple places, then we merge the row. Merging a row involves merging each column of that row according to the semantics of the CRDT for the column.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Last-write wins just picks the lastest write&lt;/li&gt; &#xA; &lt;li&gt;Counter CRDT sums the values&lt;/li&gt; &#xA; &lt;li&gt;Multi-value registers keep all conflicting values&lt;/li&gt; &#xA; &lt;li&gt;Fractional indices are taken as last write&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more background see &lt;a href=&#34;https://vlcn.io/blog/gentle-intro-to-crdts.html&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LWW, Fractional Index, Observe-Remove sets are available now.&lt;/li&gt; &#xA; &lt;li&gt;Counter and rich-text CRDTs are still &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/issues/65&#34;&gt;being implemented&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Custom SQL syntax will be available in our libSQL integration. The SQLite extension requires a slightly different syntax than what is depicted above.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Approach 2: Causal Event Log&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To be implemented in v2 of cr-sqlite&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Approach 2 has the following properties:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A history of every modification that happens to the database is kept &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;This history can be garbage collected in certain network topologies&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Merge conflicts can be automatically handled (via CRDT style rules) or the developer can define their own conflict resolution plan.&lt;/li&gt; &#xA; &lt;li&gt;The developer can choose to fork the data on merge conflict rather than merging&lt;/li&gt; &#xA; &lt;li&gt;Forks can live indefinitely or a specific fork can be chosen and other forks dropped&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This is much more akin to git and event sourcing but with the drawback being that it is much more write heavy and much more space intensive.&lt;/p&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;For a stable version, build against a &lt;a href=&#34;https://github.com/vlcn-io/cr-sqlite/releases&#34;&gt;release tag&lt;/a&gt; as main may not be 100% stable.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll need to install Rust.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installing Rust: &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;https://www.rust-lang.org/tools/install&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://www.sqlite.org/loadext.html&#34;&gt;Run Time Loadable Extension&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Instructions on building a native library that can be loaded into SQLite in non-wasm environments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rustup toolchain install nightly # make sure you have the rust nightly toolchain&#xA;git clone --recurse-submodules git@github.com:vlcn-io/cr-sqlite.git&#xA;cd cr-sqlite/core&#xA;make loadable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a shared library at &lt;code&gt;dist/crsqlite.[lib extension]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;[lib extension]:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux: &lt;code&gt;.so&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Darwin / OS X: &lt;code&gt;.dylib&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows: &lt;code&gt;.dll&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WASM&lt;/h2&gt; &#xA;&lt;p&gt;For a WASM build that works in the browser, see the &lt;a href=&#34;https://github.com/vlcn-io/js&#34;&gt;js&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;CLI&lt;/h2&gt; &#xA;&lt;p&gt;Instructions on building a &lt;code&gt;sqlite3&lt;/code&gt; CLI that has &lt;code&gt;cr-sqlite&lt;/code&gt; statically linked and pre-loaded.&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;core&lt;/code&gt; directory of the project, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make sqlite3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a &lt;code&gt;sqlite3&lt;/code&gt; binary at &lt;code&gt;dist/sqlite3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;core:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd core&#xA;make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;py integration tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd core&#xA;make loadable&#xA;cd ../py/correctness&#xA;./install-and-test.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;JS APIs&lt;/h1&gt; &#xA;&lt;p&gt;JS APIs for using &lt;code&gt;cr-sqlite&lt;/code&gt; in the browser are not yet documented but exist in the &lt;a href=&#34;https://github.com/vlcn-io/js&#34;&gt;js repo&lt;/a&gt;. You can also see examples of them in use here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://observablehq.com/@tantaman/cr-sqlite-basic-setup&#34;&gt;Observable Notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vlcn-io/live-examples&#34;&gt;https://github.com/vlcn-io/live-examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Research &amp;amp; Prior Art&lt;/h1&gt; &#xA;&lt;p&gt;cr-sqlite was inspired by and built on ideas from these papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://munin.uit.no/bitstream/handle/10037/22344/thesis.pdf?sequence=2&#34;&gt;Towards a General Database Management System of Conflict-Free Replicated Relations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hal.inria.fr/hal-02983557/document&#34;&gt;Conflict-Free Replicated Relations for Multi-Synchronous Database Management at Edge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.00107.pdf&#34;&gt;Merkle-CRDTs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lamport.azurewebsites.net/pubs/time-clocks.pdf&#34;&gt;Time, Clocks, and the Ordering of Events in a Distributed System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://csl.skku.edu/papers/jpdc11.pdf&#34;&gt;Replicated abstract data types: Building blocks for collaborative applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://josephg.com/blog/crdts-go-brrr/&#34;&gt;CRDTs for Brrr&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>