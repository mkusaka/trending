<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-17T01:34:49Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gosub-io/gosub-engine</title>
    <updated>2024-10-17T01:34:49Z</updated>
    <id>tag:github.com,2024-10-17:/gosub-io/gosub-engine</id>
    <link href="https://github.com/gosub-io/gosub-engine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Our main browser engine repository.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gosub: Gateway to Optimized Searching and Unlimited Browsing&lt;/h1&gt; &#xA;&lt;p&gt;This repository holds the Gosub browser engine. It will become a standalone library that can be used by other projects but will ultimately be used by the Gosub browser user-agent. See the &lt;a href=&#34;https://raw.githubusercontent.com/gosub-io/gosub-engine/main/#about&#34;&gt;About&lt;/a&gt; section for more information.&lt;/p&gt; &#xA;&lt;p&gt;Join us at our development &lt;a href=&#34;https://chat.developer.gosub.io&#34;&gt;Zulip chat&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;For more general information you can also join our &lt;a href=&#34;https://chat.gosub.io&#34;&gt;Discord server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in contributing to Gosub, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/gosub-io/gosub-engine/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This repository is part of the Gosub browser engine project. This is the main engine that holds the following components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HTML5 tokenizer / parser&lt;/li&gt; &#xA; &lt;li&gt;CSS3 tokenizer / parser&lt;/li&gt; &#xA; &lt;li&gt;Document tree&lt;/li&gt; &#xA; &lt;li&gt;Several APIs for connecting to javascript&lt;/li&gt; &#xA; &lt;li&gt;Configuration store&lt;/li&gt; &#xA; &lt;li&gt;Networking stack&lt;/li&gt; &#xA; &lt;li&gt;Rendering engine&lt;/li&gt; &#xA; &lt;li&gt;JS bridge&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More will follow as the engine grows. The idea is that this engine will receive some kind of stream of bytes (most likely from a socket or file) and parse this into a valid HTML5 document tree and CSS stylesheets. From that point, it can be fed to a renderer engine that will render the document tree into a window, or it can be fed to a more simplistic engine that will render it in a terminal. JS can be executed on the document tree and the document tree can be modified by JS.&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This project is in its infancy. There is no usable browser yet. However, you can look at simple html pages and parse them into a document tree and do some initial rendering.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We can parse HTML5 and CSS3 files into a document tree or the respective css tree. This tree can be shown in the terminal or be rendered in a very unfinished renderer. Our renderer cannot render everything yet, but it can render simple html pages, sort of.&lt;/p&gt; &#xA;&lt;p&gt;We already implemented other parts of the engine, for a JS engine, networking stack, a configuration store and other things however these aren&#39;t integrated yet. You can try these out by running the respective binary.&lt;/p&gt; &#xA;&lt;p&gt;We can render a part for our own &lt;a href=&#34;https://gosub.io&#34;&gt;site&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gosub-io/gosub-engine/main/resources/images/current_progress.png&#34; alt=&#34;Gosub.io&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to run&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Installing dependencies &lt;/summary&gt; &#xA; &lt;p&gt;This project uses &lt;a href=&#34;https://doc.rust-lang.org/cargo/&#34;&gt;cargo&lt;/a&gt; and &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;rustup&lt;/a&gt;. First you must install &lt;code&gt;rustup&lt;/code&gt; at the link provided. After installing &lt;code&gt;rustup&lt;/code&gt;, run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rustup toolchain install 1.73&#xA;$ rustc --version&#xA;rustc 1.73.0 (cc66ad468 2023-10-03)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Once Rust is installed, run this command to pre-build the dependencies:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You can run the following binaries:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin config-store&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bin&lt;/td&gt; &#xA;   &lt;td&gt;A simple test application of the config store for testing purposes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin css3-parser&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bin&lt;/td&gt; &#xA;   &lt;td&gt;Show the parsed css tree&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin display-text-tree&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bin&lt;/td&gt; &#xA;   &lt;td&gt;A simple parser that will try and return a textual presentation of the website&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin gosub-parser&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bin&lt;/td&gt; &#xA;   &lt;td&gt;The actual html5 parser/tokenizer that allows you to convert html5 into a document tree.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin html5-parser-test&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;test&lt;/td&gt; &#xA;   &lt;td&gt;A test suite that tests all html5lib tests for the treebuilding&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin parser-test&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;test&lt;/td&gt; &#xA;   &lt;td&gt;A test suite for the parser that tests specific tests. This will be removed as soon as the parser is completely finished as this tool is for developement only.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin renderer&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bin&lt;/td&gt; &#xA;   &lt;td&gt;Render a html page (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cargo run -r --bin run-js&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bin&lt;/td&gt; &#xA;   &lt;td&gt;Run a JS file (Note: console and event loop are not yet implemented)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For running the binaries, take a look at a quick introduction at &lt;a href=&#34;https://raw.githubusercontent.com/gosub-io/gosub-engine/main/docs/binaries.md&#34;&gt;/docs/binaries.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark and test suites&lt;/h2&gt; &#xA;&lt;p&gt;To run the tests and benchmark suite, do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make test&#xA;cargo bench&#xA;ls target/criterion/report&#xA;index.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Wasm&lt;/h2&gt; &#xA;&lt;p&gt;Our engine can also be compiled to WebAssembly. You need to use WasmPack for this. To build the Wasm version, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wasm-pack build --target web&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards you need to serve the small useragent around the wasm version in the &lt;code&gt;wasm/&lt;/code&gt; directory. You can do this by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd wasm&#xA;npm run dev  # you can also use `bun run dev`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use this demo, you need to enable webgpu in chromium and disable the same origin policy.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chromium --disable-web-security --enable-features=Vulkan --enable-unsafe-webgpu --user-data-dir=/tmp/chromium-temp-profile &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command works on Linux only, if someone uses Windows or macOS, please open an PR!&lt;/p&gt; &#xA;&lt;p&gt;And then you have it! A browser in a browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gosub-io/gosub-engine/main/resources/images/browser-wasm-hackernews.png&#34; alt=&#34;Browser in browser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to the project&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to this project but the current status makes that we are spending a lot of time researching, building small proof-of-concepts and figuring out what needs to be done next. Much time of a contributor at this stage of the project will be non-coding.&lt;/p&gt; &#xA;&lt;p&gt;We do like to hear from you if you are interested in contributing to the project and you can join us currently at our &lt;a href=&#34;https://chat.developer.gosub.io&#34;&gt;Zulip chat&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spiraldb/vortex</title>
    <updated>2024-10-17T01:34:49Z</updated>
    <id>tag:github.com,2024-10-17:/spiraldb/vortex</id>
    <link href="https://github.com/spiraldb/vortex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An extensible, state-of-the-art columnar file format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vortex&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/spiraldb/vortex/actions&#34;&gt;&lt;img src=&#34;https://github.com/fulcrum-so/vortex/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/vortex-array&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/vortex-array.svg?sanitize=true&#34; alt=&#34;Crates.io&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/vortex-array&#34;&gt;&lt;img src=&#34;https://docs.rs/vortex-array/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/vortex-array/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/vortex-array&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Vortex is an extensible, state-of-the-art columnar file format, with associated tools for working with compressed Apache Arrow arrays in-memory, on-disk, and over-the-wire.&lt;/p&gt; &#xA;&lt;p&gt;Vortex is an aspiring successor to Apache Parquet, with dramatically faster random access reads (100-200x faster) and scans (2-10x faster), while preserving approximately the same compression ratio and write throughput as Parquet with zstd. It will also support very wide tables (at least 10s of thousands of columns) and (eventually) on-device decompression on GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Vortex is designed to be to columnar file formats what Apache DataFusion is to query engines: highly extensible, extremely fast, batteries-included.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] This library is still under rapid development and is a work in progress!&lt;/p&gt; &#xA; &lt;p&gt;Some key features are not yet implemented, both the API and the serialized format are likely to change in breaking ways, and we cannot yet guarantee correctness in all cases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The major features of Vortex are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logical Types&lt;/strong&gt; - a schema definition that makes no assertions about physical layout.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-Copy to Arrow&lt;/strong&gt; - &#34;canonicalized&#34; (i.e., fully decompressed) Vortex arrays can be zero-copy converted to/from Apache Arrow arrays.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensible Encodings&lt;/strong&gt; - a pluggable set of physical layouts. In addition to the builtin set of Arrow-compatible encodings, the Vortex repository includes a number of state-of-the-art encodings (e.g., FastLanes, ALP, FSST, etc.) that are implemented as extensions. While arbitrary encodings can be implemented as extensions, we have intentionally chosen a small set of encodings that are highly data-parallel, which in turn allows for efficient vectorized decoding, random access reads, and (in the future) decompression on GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cascading Compression&lt;/strong&gt; - data can be recursively compressed with multiple nested encodings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pluggable Compression Strategies&lt;/strong&gt; - the built-in Compressor is based on BtrBlocks, but other strategies can trivially be used instead.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compute&lt;/strong&gt; - basic compute kernels that can operate over encoded data (e.g., for filter pushdown).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Statistics&lt;/strong&gt; - each array carries around lazily computed summary statistics, optionally populated at read-time. These are available to compute kernels as well as to the compressor.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Serialization&lt;/strong&gt; - Zero-copy serialization of arrays, both for IPC and for file formats.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Columnar File Format (in progress)&lt;/strong&gt; - A modern file format that uses the Vortex serde library to store compressed array data. Optimized for random access reads and extremely fast scans; an aspiring successor to Apache Parquet.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview: Logical vs Physical&lt;/h2&gt; &#xA;&lt;p&gt;One of the core design principles in Vortex is strict separation of logical and physical concerns.&lt;/p&gt; &#xA;&lt;p&gt;For example, a Vortex array is defined by a logical data type (i.e., the type of scalar elements) as well as a physical encoding (the type of the array itself). Vortex ships with several built-in encodings, as well as several extension encodings.&lt;/p&gt; &#xA;&lt;p&gt;The built-in encodings are primarily designed to model the Apache Arrow in-memory format, enabling us to construct Vortex arrays with zero-copy from Arrow arrays. There are also several built-in encodings (e.g., &lt;code&gt;sparse&lt;/code&gt; and &lt;code&gt;chunked&lt;/code&gt;) that are useful building blocks for other encodings. The included extension encodings are mostly designed to model compressed in-memory arrays, such as run-length or dictionary encoding.&lt;/p&gt; &#xA;&lt;p&gt;Analogously, &lt;code&gt;vortex-serde&lt;/code&gt; is designed to handle the low-level physical details of reading and writing Vortex arrays. Choices about which encodings to use or how to logically chunk data are left up to the &lt;code&gt;Compressor&lt;/code&gt; implementation.&lt;/p&gt; &#xA;&lt;p&gt;One of the unique attributes of the (in-progress) Vortex file format is that it encodes the physical layout of the data within the file&#39;s footer. This allows the file format to be effectively self-describing and to evolve without breaking changes to the file format specification.&lt;/p&gt; &#xA;&lt;p&gt;In fact, the format is designed to support forward compatibility by optionally embedding WASM decoders directly into the files themselves. This should help avoid the rapid calcification that has plagued other columnar file formats.&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;h3&gt;Logical Types&lt;/h3&gt; &#xA;&lt;p&gt;The Vortex type-system is still in flux. The current set of logical types is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Null&lt;/li&gt; &#xA; &lt;li&gt;Bool&lt;/li&gt; &#xA; &lt;li&gt;Integer(8, 16, 32, 64)&lt;/li&gt; &#xA; &lt;li&gt;Float(16, b16, 32, 64)&lt;/li&gt; &#xA; &lt;li&gt;Binary&lt;/li&gt; &#xA; &lt;li&gt;UTF8&lt;/li&gt; &#xA; &lt;li&gt;Struct&lt;/li&gt; &#xA; &lt;li&gt;List (partially implemented)&lt;/li&gt; &#xA; &lt;li&gt;Date/Time/DateTime/Duration (implemented as an extension type)&lt;/li&gt; &#xA; &lt;li&gt;Decimal: TODO&lt;/li&gt; &#xA; &lt;li&gt;FixedList: TODO&lt;/li&gt; &#xA; &lt;li&gt;Tensor: TODO&lt;/li&gt; &#xA; &lt;li&gt;Union: TODO&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Canonical/Flat Encodings&lt;/h3&gt; &#xA;&lt;p&gt;Vortex includes a base set of &#34;flat&#34; encodings that are designed to be zero-copy with Apache Arrow. These are the canonical representations of each of the logical data types. The canonical encodings currently supported are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Null&lt;/li&gt; &#xA; &lt;li&gt;Bool&lt;/li&gt; &#xA; &lt;li&gt;Primitive (Integer, Float)&lt;/li&gt; &#xA; &lt;li&gt;Struct&lt;/li&gt; &#xA; &lt;li&gt;VarBin (Binary, UTF8)&lt;/li&gt; &#xA; &lt;li&gt;VarBinView (Binary, UTF8)&lt;/li&gt; &#xA; &lt;li&gt;Extension&lt;/li&gt; &#xA; &lt;li&gt;...with more to come&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compressed Encodings&lt;/h3&gt; &#xA;&lt;p&gt;Vortex includes a set of highly data-parallel, vectorized encodings. These encodings each correspond to a compressed in-memory array implementation, allowing us to defer decompression. Currently, these are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adaptive Lossless Floating Point (ALP)&lt;/li&gt; &#xA; &lt;li&gt;BitPacked (FastLanes)&lt;/li&gt; &#xA; &lt;li&gt;Constant&lt;/li&gt; &#xA; &lt;li&gt;Chunked&lt;/li&gt; &#xA; &lt;li&gt;Delta (FastLanes)&lt;/li&gt; &#xA; &lt;li&gt;Dictionary&lt;/li&gt; &#xA; &lt;li&gt;Fast Static Symbol Table (FSST)&lt;/li&gt; &#xA; &lt;li&gt;Frame-of-Reference&lt;/li&gt; &#xA; &lt;li&gt;Run-end Encoding&lt;/li&gt; &#xA; &lt;li&gt;RoaringUInt&lt;/li&gt; &#xA; &lt;li&gt;RoaringBool&lt;/li&gt; &#xA; &lt;li&gt;Sparse&lt;/li&gt; &#xA; &lt;li&gt;ZigZag&lt;/li&gt; &#xA; &lt;li&gt;...with more to come&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compression&lt;/h3&gt; &#xA;&lt;p&gt;Vortex&#39;s default compression strategy is based on the &lt;a href=&#34;https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf&#34;&gt;BtrBlocks&lt;/a&gt; paper.&lt;/p&gt; &#xA;&lt;p&gt;Roughly, for each chunk of data, a sample of at least ~1% of the data is taken. Compression is then attempted ( recursively) with a set of lightweight encodings. The best-performing combination of encodings is then chosen to encode the entire chunk. This sounds like it would be very expensive, but given basic statistics about a chunk, it is possible to cheaply prune many encodings and ensure the search space does not explode in size.&lt;/p&gt; &#xA;&lt;h3&gt;Compute&lt;/h3&gt; &#xA;&lt;p&gt;Vortex provides the ability for each encoding to specialize the implementation of a compute function to avoid decompressing where possible. For example, filtering a dictionary-encoded UTF8 array can be more cheaply performed by filtering the dictionary first.&lt;/p&gt; &#xA;&lt;p&gt;Note--as mentioned above--that Vortex does not intend to become a full-fledged compute engine, but rather to implement basic compute operations as may be required for efficient scanning &amp;amp; pushdown.&lt;/p&gt; &#xA;&lt;h3&gt;Statistics&lt;/h3&gt; &#xA;&lt;p&gt;Vortex arrays carry lazily-computed summary statistics. Unlike other array libraries, these statistics can be populated from disk formats such as Parquet and preserved all the way into a compute engine. Statistics are available to compute kernels as well as to the compressor.&lt;/p&gt; &#xA;&lt;p&gt;The current statistics are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BitWidthFreq&lt;/li&gt; &#xA; &lt;li&gt;TrailingZeroFreq&lt;/li&gt; &#xA; &lt;li&gt;IsConstant&lt;/li&gt; &#xA; &lt;li&gt;IsSorted&lt;/li&gt; &#xA; &lt;li&gt;IsStrictSorted&lt;/li&gt; &#xA; &lt;li&gt;Max&lt;/li&gt; &#xA; &lt;li&gt;Min&lt;/li&gt; &#xA; &lt;li&gt;RunCount&lt;/li&gt; &#xA; &lt;li&gt;TrueCount&lt;/li&gt; &#xA; &lt;li&gt;NullCount&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Serialization / Deserialization (Serde)&lt;/h3&gt; &#xA;&lt;p&gt;The goals of the &lt;code&gt;vortex-serde&lt;/code&gt; implementation are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support scanning (column projection + row filter) with zero-copy and zero heap allocation.&lt;/li&gt; &#xA; &lt;li&gt;Support random access in constant or near-constant time.&lt;/li&gt; &#xA; &lt;li&gt;Forward statistical information (such as sortedness) to consumers.&lt;/li&gt; &#xA; &lt;li&gt;Provide IPC format for sending arrays between processes.&lt;/li&gt; &#xA; &lt;li&gt;Provide an extensible, best-in-class file format for storing columnar data on disk or in object storage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;TODO: insert diagram here&lt;/p&gt; &#xA;&lt;h2&gt;Integration with Apache Arrow&lt;/h2&gt; &#xA;&lt;p&gt;Apache Arrow is the de facto standard for interoperating on columnar array data. Naturally, Vortex is designed to be maximally compatible with Apache Arrow. All Arrow arrays can be converted into Vortex arrays with zero-copy, and a Vortex array constructed from an Arrow array can be converted back to Arrow, again with zero-copy.&lt;/p&gt; &#xA;&lt;p&gt;It is important to note that Vortex and Arrow have different--albeit complementary--goals.&lt;/p&gt; &#xA;&lt;p&gt;Vortex explicitly separates logical types from physical encodings, distinguishing it from Arrow. This allows Vortex to model more complex arrays while still exposing a logical interface. For example, Vortex can model a UTF8 &lt;code&gt;ChunkedArray&lt;/code&gt; where the first chunk is run-length encoded and the second chunk is dictionary encoded. In Arrow, &lt;code&gt;RunLengthArray&lt;/code&gt; and &lt;code&gt;DictionaryArray&lt;/code&gt; are separate incompatible types, and so cannot be combined in this way.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;For best performance we recommend using &lt;a href=&#34;https://github.com/microsoft/mimalloc&#34;&gt;MiMalloc&lt;/a&gt; as the application&#39;s allocator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;#[global_allocator]&#xA;static GLOBAL_ALLOC: MiMalloc = MiMalloc;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/spiraldb/vortex/develop/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In order to build vortex, you may also need to install the flatbuffer compiler (flatc):&lt;/p&gt; &#xA;&lt;h3&gt;Mac&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install flatbuffers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This repo uses rye to manage the combined Rust/Python monorepo build. First, make sure to run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install Rye from https://rye-up.com, and setup the virtualenv&#xA;rye sync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).&lt;/p&gt; &#xA;&lt;h2&gt;Governance&lt;/h2&gt; &#xA;&lt;p&gt;Vortex is and will remain an open-source project. Our intent is to model its governance structure after the &lt;a href=&#34;https://substrait.io/governance/&#34;&gt;Substrait project&lt;/a&gt;, which in turn is based on the model of the Apache Software Foundation. Expect more details on this in Q4 2024.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments 🏆&lt;/h2&gt; &#xA;&lt;p&gt;This project is inspired by and--in some cases--directly based upon the existing, excellent work of many researchers and OSS developers.&lt;/p&gt; &#xA;&lt;p&gt;In particular, the following academic papers greatly influenced the development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Maximilian Kuschewski, David Sauerwein, Adnan Alhomssi, and Viktor Leis. &lt;a href=&#34;https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf&#34;&gt;BtrBlocks: Efficient Columnar Compression for Data Lakes&lt;/a&gt;. Proc. ACM Manag. Data 1, 2, Article 118 (June 2023), 14 pages.&lt;/li&gt; &#xA; &lt;li&gt;Azim Afroozeh and Peter Boncz. &lt;a href=&#34;https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf&#34;&gt;The FastLanes Compression Layout: Decoding &amp;gt;100 Billion Integers per Second with Scalar Code&lt;/a&gt;. PVLDB, 16(9): 2132 - 2144, 2023.&lt;/li&gt; &#xA; &lt;li&gt;Peter Boncz, Thomas Neumann, and Viktor Leis. &lt;a href=&#34;https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf&#34;&gt;FSST: Fast Random Access String Compression&lt;/a&gt;. PVLDB, 13(11): 2649-2661, 2020.&lt;/li&gt; &#xA; &lt;li&gt;Azim Afroozeh, Leonardo X. Kuffo, and Peter Boncz. &lt;a href=&#34;https://ir.cwi.nl/pub/33334/33334.pdf&#34;&gt;ALP: Adaptive Lossless floating-Point Compression&lt;/a&gt;. Proc. ACM Manag. Data 1, 4 (SIGMOD), Article 230 (December 2023), 26 pages.&lt;/li&gt; &#xA; &lt;li&gt;Biswapesh Chattopadhyay, Priyam Dutta, Weiran Liu, Ott Tinn, Andrew Mccormick, Aniket Mokashi, Paul Harvey, Hector Gonzalez, David Lomax, Sagar Mittal, et al. &lt;a href=&#34;https://dl.acm.org/citation.cfm?id=3360438&#34;&gt;Procella: Unifying serving and analytical data at YouTube&lt;/a&gt;. PVLDB, 12(12): 2022-2034, 2019.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, we benefited greatly from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the existence, ideas, &amp;amp; implementation of &lt;a href=&#34;https://arrow.apache.org&#34;&gt;Apache Arrow&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;likewise for the excellent &lt;a href=&#34;https://github.com/apache/datafusion&#34;&gt;Apache DataFusion&lt;/a&gt; project.&lt;/li&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://github.com/jorgecarleitao/parquet2&#34;&gt;parquet2&lt;/a&gt; project by &lt;a href=&#34;https://github.com/jorgecarleitao&#34;&gt;Jorge Leitao&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;the public discussions around choices of compression codecs, as well as the C++ implementations thereof, from &lt;a href=&#34;https://github.com/duckdb/duckdb&#34;&gt;duckdb&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://github.com/facebookincubator/velox&#34;&gt;Velox&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookincubator/nimble&#34;&gt;Nimble&lt;/a&gt; projects, and discussions with their maintainers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to all of the aforementioned for sharing their work and knowledge with the world! 🚀&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ReFirmLabs/binwalk</title>
    <updated>2024-10-17T01:34:49Z</updated>
    <id>tag:github.com,2024-10-17:/ReFirmLabs/binwalk</id>
    <link href="https://github.com/ReFirmLabs/binwalk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Firmware Analysis Tool&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Binwalk v3&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ReFirmLabs/binwalk/master/images/binwalk_animated.svg?sanitize=true&#34; alt=&#34;binwalk v3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This is an updated version of the Binwalk firmware analysis tool. It has been re-written in Rust, and is currently in the beta testing phase.&lt;/p&gt; &#xA;&lt;p&gt;While the usage and output is similar to that of previous Binwalk releases, this version has several notable improvements:&lt;/p&gt; &#xA;&lt;h3&gt;Smart Signature Matching&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;While Binwalk still fundamentally relies on identifying files and data based on their &lt;a href=&#34;https://en.wikipedia.org/wiki/Magic_number_(programming)#In_files&#34;&gt;magic signatures&lt;/a&gt;, each signature has an associated file parser which is repsonsible for parsing the expected file format and validating the data for correctness.&lt;/p&gt; &#xA; &lt;p&gt;Signatures that are deemed to be valid have an associated &lt;a href=&#34;https://raw.githubusercontent.com/ReFirmLabs/binwalk/master/#command-line-output&#34;&gt;confidence level&lt;/a&gt;, indicating how confident the file parser is in the accuracy of the reported result.&lt;/p&gt; &#xA; &lt;p&gt;This results in fewer false positives, more reliable file extraction, and more detailed analysis results.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Faster Analysis and Extraction&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Rust is inherently faster than Python; this, combined with &lt;a href=&#34;https://cp-algorithms.com/string/aho_corasick.html&#34;&gt;efficient&lt;/a&gt; pattern matching and multi-threaded recursive extraction makes analysis and extraction much faster than previous Binwalk releases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;JSON Output&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Analysis and extraction results can be saved in &lt;a href=&#34;https://raw.githubusercontent.com/ReFirmLabs/binwalk/master/#json-logging&#34;&gt;JSON format&lt;/a&gt;, making Binwalk results easily ingestible by other tools.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;A fork of the old Binwalk repository is currently maintained by &lt;a href=&#34;https://github.com/OSPG/binwalk&#34;&gt;OSPG&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported Platforms&lt;/h2&gt; &#xA;&lt;p&gt;Binwalk is only supported on 64-bit Linux systems, and only tested on DUbuntu Linux. It is recommended that you run Binwalk on a Ubuntu-based system.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Dockerfile&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The easiest way to get up and running quickly is to use the included &lt;code&gt;Dockerfile&lt;/code&gt; to build a &lt;a href=&#34;https://www.docker.com&#34;&gt;docker&lt;/a&gt; image.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To build a Binwalk docker image with all dependencies included:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install git docker.io&#xA;git clone https://github.com/ReFirmLabs/binwalk.git&#xA;cd binwalk&#xA;sudo docker build -t binwalk . &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it!&lt;/p&gt; &#xA;&lt;p&gt;To analyze and extract a local file &lt;code&gt;/tmp/firmware.bin&lt;/code&gt; using the Binwalk docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo docker run -t -v /tmp:/home/appuser binwalk -Me firmware.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The extracted files will be located in your &lt;code&gt;/tmp/extractions&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Compiling From Source&lt;/h3&gt; &#xA;&lt;p&gt;To compile Binwalk, you must first have the &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;Rust compiler&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1&lt;/h4&gt; &#xA;&lt;p&gt;Download the Binwalk git repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install git&#xA;git clone https://github.com/ReFirmLabs/binwalk.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 2&lt;/h4&gt; &#xA;&lt;p&gt;Install build/runtime dependencies.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Binwalk relies on several external utilities to perform extraction at runtime. These are not required, but automated extraction of certain file types will fail if they are not installed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To install all build and runtime dependencies (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo ./binwalk/dependencies/ubuntu.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OR, to install only the required build dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install build-essential libfontconfig1-dev liblzma-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 3&lt;/h4&gt; &#xA;&lt;p&gt;Build Binwalk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd binwalk&#xA;cargo build --release&#xA;./target/release/binwalk --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Binwalk binary will be located at the &lt;code&gt;target/release/binwalk&lt;/code&gt; path, as shown above. You may copy it to, and run it from, any location on your system that you prefer.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;List all signatures and required extraction utilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk --list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Scan a file&#39;s contents:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Exclude specific signatures from a scan:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk --exclude=jpeg,png,pdf file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Only serch for specific signatures during a scan:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk --include=jpeg,png,pdf file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Scan a file and extract its contents (default output directory is &lt;code&gt;extractions&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk -e file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Recursively scan and extract a file&#39;s contents:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk -Me file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate an entropy graph of the specified file (a PNG image will be saved to the current working directory):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk -E file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Save signature or entropy analysis results to a JSON file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;binwalk --log=results.json file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Command Line Output&lt;/h2&gt; &#xA;&lt;p&gt;For each identified file type, Binwalk displays the file offset in both decimal and hexadecimal, along with a brief description.&lt;/p&gt; &#xA;&lt;p&gt;Output is color-coded to indicate the confidence of the reported results:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ReFirmLabs/binwalk/master/images/output.png&#34; alt=&#34;example output&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;There is no strict definition for the confidence level of each result, but they can generally be interpreted as:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] High confidence; both file metadata and at least some portions of the file data were checked for accuracy&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Medium confidence; a reasonable amount of validation/sanity-checking was performed on the file metadata&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] Low confidence; the &#34;magic bytes&#34; for the reported file type were identified, but little-to-no additional validation was performed&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] During recursive extraction only &#34;interesting&#34; results will be displayed; use the &lt;code&gt;--verbose&lt;/code&gt; command line option to display &lt;em&gt;all&lt;/em&gt; results.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported Signatures&lt;/h2&gt; &#xA;&lt;p&gt;All supported file signatures and their corresponding extraction utility (if any) can be displayed with the &lt;code&gt;--list&lt;/code&gt; command line option:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ReFirmLabs/binwalk/master/images/binwalk_list.png&#34; alt=&#34;signature list&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Each signature is color-coded to indicate:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Signature is fully supported&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Signature is prone to false positives and will only be matched at the beginning of a file&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] The values displayed in the &lt;code&gt;Signature Name&lt;/code&gt; column can be used with the &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; signature filter arguments.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Entropy Graphs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Entropy_(information_theory)&#34;&gt;Entropy&lt;/a&gt; graphs (&lt;code&gt;--entropy&lt;/code&gt;) display a plot of how random the contents of a file are, with the level of randomness displayed on the y axis and the file offset displayed on the x axis:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ReFirmLabs/binwalk/master/images/entropy.png&#34; alt=&#34;example entropy&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Randomness is calculated on a unit-less scale of &lt;code&gt;0&lt;/code&gt; (not random at all) to &lt;code&gt;8&lt;/code&gt; (very random). Since compressed and encrypted data is, by nature, very random, this is useful for identifying sections of a file that have been compressed or encrypted.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;JSON Logging&lt;/h2&gt; &#xA;&lt;p&gt;The JSON logs (&lt;code&gt;--log&lt;/code&gt;) include more detailed signature and extraction information than is reported on the command line.&lt;/p&gt; &#xA;&lt;p&gt;If an entropy scan was requested (&lt;code&gt;--entropy&lt;/code&gt;), the JSON data will contain the raw entropy data for the specified file.&lt;/p&gt; &#xA;&lt;h2&gt;Errors and Logging&lt;/h2&gt; &#xA;&lt;p&gt;Errors and debug logs are handled by the Rust &lt;a href=&#34;https://docs.rs/env_logger/latest/env_logger/&#34;&gt;env_logger&lt;/a&gt;, which allows users to control log levels via the &lt;code&gt;RUST_LOG&lt;/code&gt; environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUST_LOG=off binwalk -Me file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUST_LOG=info binwalk -Me file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUST_LOG=debug binwalk -Me file_name.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All errors and debug information are printed to stderr.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Currently binwalk is primarily a command line utility, but a Rust library is available.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Binwalk can be very resource intensive. By default it will use all available CPU cores, and reads files into memory in their entirety.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Use the &lt;code&gt;--threads&lt;/code&gt; argument to limit the number of concurrent threads.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>