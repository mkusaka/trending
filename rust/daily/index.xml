<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-03T01:42:23Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>twitter/rezolus</title>
    <updated>2023-04-03T01:42:23Z</updated>
    <id>tag:github.com,2023-04-03:/twitter/rezolus</id>
    <link href="https://github.com/twitter/rezolus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Systems performance telemetry&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rezolus&lt;/h1&gt; &#xA;&lt;p&gt;Rezolus is a tool for collecting detailed systems performance telemetry and exposing burst patterns through high-resolution telemetry. Rezolus provides instrumentation of basic systems metrics, performance counters, and support for eBPF (extended Berkeley Packet Filter) telemetry. Measurement is the first step toward improved performance.&lt;/p&gt; &#xA;&lt;p&gt;Per-metric documentation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/rezolus/master/docs/METRICS.md&#34;&gt;METRICS&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/rezolus/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License: Apache-2.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/twitter/rezolus/actions/workflows/cargo.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/twitter/rezolus/CI/master?label=CI&#34; alt=&#34;Build Status: CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Rezolus collects telemetry from several different sources. Currently, Rezolus collects telemetry from traditional sources (procfs, sysfs), the perf_events subsystem, and from BPF. Each sampler implements a consistent set of functions so that new ones can be easily added to further extend the capabilities of Rezolus.&lt;/p&gt; &#xA;&lt;p&gt;Each telemetry source is oversampled so that we can build a histogram across a time interval. This histogram allows us to capture variations which will appear in the far upper and lower percentiles. This oversampling approach is one of the key differentiators of Rezolus when compared to other telemetry agents.&lt;/p&gt; &#xA;&lt;p&gt;With its support for BPF as well as more common telemetry sources, Rezolus is a very sophisticated tool for capturing performance anomalies, profiling systems performance, and conducting performance diagnostics.&lt;/p&gt; &#xA;&lt;p&gt;More detailed information about the underlying metrics library and sampler design can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/twitter/rezolus/master/docs/DESIGN.md&#34;&gt;DESIGN&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;traditional telemetry sources (procfs, sysfs, ...)&lt;/li&gt; &#xA; &lt;li&gt;perf_events support for hardware performance counters&lt;/li&gt; &#xA; &lt;li&gt;BPF support to instrument kernel and user space activities&lt;/li&gt; &#xA; &lt;li&gt;oversampling and percentile metrics to capture bursts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Traditional Telemetry Sources&lt;/h3&gt; &#xA;&lt;p&gt;Rezolus collects metrics from traditional sources (procfs, sysfs) to provide basic telemetry for CPU, disk, and network. Rezolus exports CPU utilization, disk bandwidth, disk IOPs, network bandwidth, network packet rate, network errors, as well as TCP and UDP protocol counters.&lt;/p&gt; &#xA;&lt;p&gt;These basic telemetry sources, when coupled with the approach of oversampling to capture their bursts, often provide a high-level view of systems performance and may readily indicate areas where resources are saturated or errors are occurring.&lt;/p&gt; &#xA;&lt;h3&gt;Perf Events&lt;/h3&gt; &#xA;&lt;p&gt;Perf Events allow us to report on both hardware and software events. Typical software events are things like page faults, context switches, and CPU migrations. Typical hardware events are things like CPU cycles, instructions retired, cache hits, cache misses, and a variety of other detailed metrics about how a workload is running on the underlying hardware.&lt;/p&gt; &#xA;&lt;p&gt;These metrics are typically used for advanced performance debugging, as well as for tuning and optimization efforts.&lt;/p&gt; &#xA;&lt;h3&gt;BPF&lt;/h3&gt; &#xA;&lt;p&gt;There is an expansive amount of performance information that can be exposed through BPF, which allows us to have the Linux Kernel perform telemetry capture and aggregation at very fine-grained levels.&lt;/p&gt; &#xA;&lt;p&gt;Rezolus comes with samplers that capture block IO size distribution, EXT4 and XFS operation latency distribution, and scheduler run queue latency distribution. You&#39;ll see that here we are mainly exposing distributions of sizes and latencies The kernel is recording the appropriate value for each operation into a histogram. Rezolus then accesses this histogram from user-space and transfers the values over to its own internal storage where it is then exposed to external aggregators.&lt;/p&gt; &#xA;&lt;p&gt;By collecting telemetry in-kernel, we&#39;re able to gather data about events that happen at extremely high rates - e.g., task scheduling - with minimal performance overhead for collecting the telemetry. The BPF samplers can be used to both capture runtime performance anomalies as well as characterize workloads.&lt;/p&gt; &#xA;&lt;h3&gt;Sampling rate and resolution&lt;/h3&gt; &#xA;&lt;p&gt;In order to accurately reflect the intensity of a burst, the sampling rate must be at least twice the duration of the shortest burst to record accurately. This ensures that at least 1 sample completely overlaps the burst section of the event. With a traditional minutely time series, this means that a spike must least 120 seconds or more to be accurately recorded in terms of intensity. Rezolus allows for sampling rate to be configured, allowing us to make a trade-off between resolution and resource consumption. At 10Hz sampling, 200ms or more of consecutive burst is enough to be accurately reflected in the pMax. Contrast that with minutely metrics requiring 120_000ms, or secondly requiring 2000ms of consecutive burst to be accurately recorded.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;Rezolus is built with the standard Rust toolchain which can be installed and managed via &lt;a href=&#34;https://rustup.rs&#34;&gt;rustup&lt;/a&gt; or by following the directions on the Rust &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The rest of the guide assumes you&#39;ve chosen to install the toolchain via rustup.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Rezolus is intended to be built and deployed on Linux systems but has some very limited support for MacOS to test the overall framework. It is focused on providing systems telemetry for Linux systems. To get the best experience and develop new samplers, you should build and run on Linux.&lt;/p&gt; &#xA;&lt;h4&gt;Clone and build Rezolus from source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/twitter/rezolus&#xA;cd rezolus&#xA;&#xA;# create an unoptimized development build&#xA;cargo build&#xA;&#xA;# run the unoptimized binary and display help&#xA;cargo run -- --help&#xA;&#xA;# create an optimized release build&#xA;cargo build --release&#xA;&#xA;# run the optimized binary and display help&#xA;cargo run --release -- --help&#xA;&#xA;# run the optimized binary with the example config (needs sudo for perf_events)&#xA;cargo build --release &amp;amp;&amp;amp; \&#xA;sudo target/release/rezolus --config configs/example.toml&#xA;&#xA;# metrics can be viewed in human-readable form with curl&#xA;curl --silent http://localhost:4242/vars&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building with BPF Support&lt;/h3&gt; &#xA;&lt;p&gt;By default, BPF support is not compiled in. If you wish to produce a build with BPF support enabled, follow the steps below:&lt;/p&gt; &#xA;&lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA;&lt;p&gt;BPF support requires that we link against the &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;BPF Compiler Collection&lt;/a&gt;. You may either use the version provided by your distribution, or can build BCC and install from source. It is critical to know which version of BCC you have installed. Rezolus supports multiple versions by utilizing different feature flags at build time.&lt;/p&gt; &#xA;&lt;p&gt;Our current policy is to support BCC versions back to the version in the distribution repository for Debian Stable or CentOS 7 (whichever is older) to the most recent version of BCC.&lt;/p&gt; &#xA;&lt;p&gt;This policy provides coverage for current stable and testing versions of Debian, Ubuntu, CentOS, Fedora, Arch, and Gentoo. Users of other distributions may need to build a supported version of BCC from source. See &lt;a href=&#34;https://github.com/iovisor/bcc/raw/master/INSTALL.md&#34;&gt;BCC Installation Guide&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h4&gt;Building&lt;/h4&gt; &#xA;&lt;p&gt;As mentioned above, we provide different feature flags to map to various supported BCC versions. The &lt;code&gt;bpf&lt;/code&gt; feature will map to the newest version supported by the &lt;a href=&#34;https://github.com/rust-bpf/rust-bcc&#34;&gt;rust-bcc&lt;/a&gt; project. For most users, this will be the right flag to use. However, if you must link against an older BCC version, you will need to use a more specific form of the feature flag. These version-specific flags take the form of &lt;code&gt;bpf_v0_10_0&lt;/code&gt; with the BCC version being reflected in the name of the feature. You can find a complete list of the feature flags in the &lt;a href=&#34;https://github.com/twitter/rezolus/raw/master/Cargo.toml&#34;&gt;cargo manifest&lt;/a&gt; for this project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create an optimized release build with BPF support&#xA;cargo build --release --features bpf&#xA;sudo target/release/rezolus --config configs/example.toml&#xA;&#xA;# metrics can be viewed in human-readable form with curl&#xA;curl --silent http://localhost:4242/vars&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;HTTP Exposition&lt;/h3&gt; &#xA;&lt;p&gt;Rezolus exposes metrics over HTTP, with different paths corresponding to different exposition formats.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;human-readable: &lt;code&gt;/vars&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;JSON: &lt;code&gt;/vars.json&lt;/code&gt;, &lt;code&gt;/metrics.json&lt;/code&gt;, &lt;code&gt;/admin/metrics.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prometheus: &lt;code&gt;/metrics&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; currently, JSON exposition is provided by default for any other path. This behavior may change in the future and should not be relied on.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, you can get the running version on the root-level path &lt;code&gt;/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Create a &lt;a href=&#34;https://github.com/twitter/rezolus/issues/new&#34;&gt;new issue&lt;/a&gt; on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We feel that a welcoming community is important and we ask that you follow Twitter&#39;s &lt;a href=&#34;https://github.com/twitter/code-of-conduct/raw/master/code-of-conduct.md&#34;&gt;Open Source Code of Conduct&lt;/a&gt; in all interactions with the community.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Brian Martin &lt;a href=&#34;mailto:bmartin@twitter.com&#34;&gt;bmartin@twitter.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A full list of &lt;a href=&#34;https://github.com/twitter/rezolus/graphs/contributors?type=a&#34;&gt;contributors&lt;/a&gt; can be found on GitHub.&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/twitteross&#34;&gt;@TwitterOSS&lt;/a&gt; on Twitter for updates.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2019 Twitter, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Security Issues?&lt;/h2&gt; &#xA;&lt;p&gt;Please report sensitive security issues via Twitter&#39;s bug-bounty program (&lt;a href=&#34;https://hackerone.com/twitter&#34;&gt;https://hackerone.com/twitter&lt;/a&gt;) rather than GitHub.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Plonky3/Plonky3</title>
    <updated>2023-04-03T01:42:23Z</updated>
    <id>tag:github.com,2023-04-03:/Plonky3/Plonky3</id>
    <link href="https://github.com/Plonky3/Plonky3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A toolkit for implementing polynomial IOPs (PIOPs)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Plonky3&lt;/h1&gt; &#xA;&lt;p&gt;Plonky3 is a toolkit for implementing polynomial IOPs (PIOPs), such as PLONK and STARKs. It aims to support several polynomial commitment schemes, such as Brakedown.&lt;/p&gt; &#xA;&lt;p&gt;This is the &#34;core&#34; repo, but the plan is to move each crate into its own repo once APIs stabilize.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under either of&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apache License, Version 2.0, (&lt;a href=&#34;https://raw.githubusercontent.com/Plonky3/Plonky3/main/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;MIT license (&lt;a href=&#34;https://raw.githubusercontent.com/Plonky3/Plonky3/main/LICENSE-MIT&#34;&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href=&#34;http://opensource.org/licenses/MIT&#34;&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;at your option.&lt;/p&gt; &#xA;&lt;h3&gt;Contribution&lt;/h3&gt; &#xA;&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rustformers/llama-rs</title>
    <updated>2023-04-03T01:42:23Z</updated>
    <id>tag:github.com,2023-04-03:/rustformers/llama-rs</id>
    <link href="https://github.com/rustformers/llama-rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run LLaMA inference on CPU, with Rust 🦀🚀🦙&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA-rs&lt;/h1&gt; &#xA;&lt;!-- markdownlint-disable-file MD026 --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Do the LLaMA thing, but now in Rust 🦀🚀🦙&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/logo2.png&#34; alt=&#34;A llama riding a crab, AI-generated&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Image by &lt;a href=&#34;https://github.com/darthdeus/&#34;&gt;@darthdeus&lt;/a&gt;, using Stable Diffusion&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/F1F8DNO5D&#34;&gt;&lt;img src=&#34;https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true&#34; alt=&#34;ko-fi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/llama_rs&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/llama-rs.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT&#34;&gt; &lt;a href=&#34;https://discord.gg/YB9WaXYAWU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1085885067601137734&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/llama_gif.gif&#34; alt=&#34;Gif showcasing language generation using llama-rs&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLaMA-rs&lt;/strong&gt; is a Rust port of the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project. This allows running inference for Facebook&#39;s &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; model on a CPU with good performance using full precision, f16 or 4-bit quantized versions of the model.&lt;/p&gt; &#xA;&lt;p&gt;Just like its C++ counterpart, it is powered by the &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;&lt;code&gt;ggml&lt;/code&gt;&lt;/a&gt; tensor library, achieving the same performance as the original code.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have a Rust 1.65.0 or above and C toolchain[^1] set up, and get a copy of the model&#39;s weights[^2].&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llama-rs&lt;/code&gt; is a Rust library, while &lt;code&gt;llama-cli&lt;/code&gt; is a CLI application that wraps &lt;code&gt;llama-rs&lt;/code&gt; and offers basic inference capabilities.&lt;/p&gt; &#xA;&lt;p&gt;The following instructions explain how to build &lt;code&gt;llama-cli&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: For best results, make sure to build and run in release mode. Debug builds are going to be very slow.&lt;/p&gt; &#xA;&lt;h3&gt;Building using &lt;code&gt;cargo&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo install --git https://github.com/rustformers/llama-rs llama-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to install &lt;code&gt;llama-cli&lt;/code&gt; to your Cargo &lt;code&gt;bin&lt;/code&gt; directory, which &lt;code&gt;rustup&lt;/code&gt; is likely to have added to your &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It can then be run through &lt;code&gt;llama-cli&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Building from repository&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository, and then build it through&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting binary will be at &lt;code&gt;target/release/llama-cli[.exe]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It can also be run directly through Cargo, using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo run --release -- &amp;lt;ARGS&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is useful for development.&lt;/p&gt; &#xA;&lt;h3&gt;Running&lt;/h3&gt; &#xA;&lt;p&gt;For example, try the following prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;llama-cli -m &amp;lt;path&amp;gt;/ggml-model-q4_0.bin -p &#34;Tell me how cool the Rust programming language is:&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some additional things to try:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--help&lt;/code&gt; to see a list of available options.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have the &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;alpaca-lora&lt;/a&gt; weights, try &lt;code&gt;--repl&lt;/code&gt; mode!&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;llama-cli -m &amp;lt;path&amp;gt;/ggml-alpaca-7b-q4.bin -f examples/alpaca_prompt.txt --repl&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/alpaca_repl_screencap.gif&#34; alt=&#34;Gif showcasing alpaca repl mode&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Sessions can be loaded (&lt;code&gt;--load-session&lt;/code&gt;) or saved (&lt;code&gt;--save-session&lt;/code&gt;) to file. To automatically load and save the same session, use &lt;code&gt;--persist-session&lt;/code&gt;. This can be used to cache prompts to reduce load time, too:&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/prompt_caching_screencap.gif&#34; alt=&#34;Gif showcasing prompt caching&#34;&gt;&lt;/p&gt; &lt;p&gt;(This GIF shows an older version of the flags, but the mechanics are still the same.)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: A modern-ish C toolchain is required to compile &lt;code&gt;ggml&lt;/code&gt;. A C++ toolchain should not be necessary.&lt;/p&gt; &#xA;&lt;p&gt;[^2]: The only legal source to get the weights at the time of writing is &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/README.md#llama&#34;&gt;this repository&lt;/a&gt;. The choice of words also may or may not hint at the existence of other kinds of sources.&lt;/p&gt; &#xA;&lt;h2&gt;Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h3&gt;Why did you do this?&lt;/h3&gt; &#xA;&lt;p&gt;It was not my choice. Ferris appeared to me in my dreams and asked me to rewrite this in the name of the Holy crab.&lt;/p&gt; &#xA;&lt;h3&gt;Seriously now.&lt;/h3&gt; &#xA;&lt;p&gt;Come on! I don&#39;t want to get into a flame war. You know how it goes, &lt;em&gt;something something&lt;/em&gt; memory &lt;em&gt;something something&lt;/em&gt; cargo is nice, don&#39;t make me say it, everybody knows this already.&lt;/p&gt; &#xA;&lt;h3&gt;I insist.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Sheesh! Okaaay&lt;/em&gt;. After seeing the huge potential for &lt;strong&gt;llama.cpp&lt;/strong&gt;, the first thing I did was to see how hard would it be to turn it into a library to embed in my projects. I started digging into the code, and realized the heavy lifting is done by &lt;code&gt;ggml&lt;/code&gt; (a C library, easy to bind to Rust) and the whole project was just around ~2k lines of C++ code (not so easy to bind). After a couple of (failed) attempts to build an HTTP server into the tool, I realized I&#39;d be much more productive if I just ported the code to Rust, where I&#39;m more comfortable.&lt;/p&gt; &#xA;&lt;h3&gt;Is this the real reason?&lt;/h3&gt; &#xA;&lt;p&gt;Haha. Of course &lt;em&gt;not&lt;/em&gt;. I just like collecting imaginary internet points, in the form of little stars, that people seem to give to me whenever I embark on pointless quests for &lt;em&gt;rewriting X thing, but in Rust&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How is this different from &lt;code&gt;llama.cpp&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;This is a reimplementation of &lt;code&gt;llama.cpp&lt;/code&gt; that does not share any code with it outside of &lt;code&gt;ggml&lt;/code&gt;. This was done for a variety of reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires a C++ compiler, which can cause problems for cross-compilation to more esoteric platforms. An example of such a platform is WebAssembly, which can require a non-standard compiler SDK.&lt;/li&gt; &#xA; &lt;li&gt;Rust is easier to work with from a development and open-source perspective; it offers better tooling for writing &#34;code in the large&#34; with many other authors. Additionally, we can benefit from the larger Rust ecosystem with ease.&lt;/li&gt; &#xA; &lt;li&gt;We would like to make &lt;code&gt;ggml&lt;/code&gt; an optional backend (see &lt;a href=&#34;https://github.com/rustformers/llama-rs/issues/31&#34;&gt;this issue&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, we hope to build a solution for model inferencing that is as easy to use and deploy as any other Rust crate.&lt;/p&gt;</summary>
  </entry>
</feed>