<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-14T01:37:04Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>samuel-vitorino/lm.rs</title>
    <updated>2024-10-14T01:37:04Z</updated>
    <id>tag:github.com,2024-10-14:/samuel-vitorino/lm.rs</id>
    <link href="https://github.com/samuel-vitorino/lm.rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimal LLM inference in Rust&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;lmrs logo&#34; src=&#34;https://raw.githubusercontent.com/samuel-vitorino/lm.rs/main/repo_cover.svg?sanitize=true&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;p&gt;lm.rs: run inference on Language Models locally on the CPU with Rust&lt;/p&gt; &#xA; &lt;h3&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/samuel-vitorino/lm.rs-webui&#34;&gt;WebUI&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/collections/samuel-vitorino/lmrs-66c7da8a50ce52b61bee70b7&#34;&gt;Hugging Face&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=FAIN5Jxc0nE&#34;&gt;Video Demo&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;ðŸŒƒ Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inspired by Karpathy&#39;s &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; and &lt;a href=&#34;https://github.com/karpathy/llm.c&#34;&gt;llm.c&lt;/a&gt; I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google&#39;s Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now.&lt;/p&gt; &#xA;&lt;p&gt;Disclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn&#39;t it incredible that in a few years, we could have AGI running in a few lines of poorly written Rust code?&lt;/p&gt; &#xA;&lt;h2&gt;Prepared models&lt;/h2&gt; &#xA;&lt;p&gt;Some benchmarks and download links for the models and tokenizers. I recommend using Q8_0, Q4_0 quantization still being improved. Speed measured on a 16-core AMD Epyc.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/gemma2-2b-it-q4_0-LMRS&#34;&gt;Gemma 2 2B IT Q4_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.39G&lt;/td&gt; &#xA;   &lt;td&gt;20 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/gemma2-2b-it-q8_0-LMRS&#34;&gt;Gemma 2 2B IT Q8_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.66GB&lt;/td&gt; &#xA;   &lt;td&gt;18 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/gemma2-9b-it-q4_0-LMRS&#34;&gt;Gemma 2 9B IT Q4_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4.91GB&lt;/td&gt; &#xA;   &lt;td&gt;7 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/gemma2-9b-it-q8_0-LMRS&#34;&gt;Gemma 2 9B IT Q8_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;9.53GB&lt;/td&gt; &#xA;   &lt;td&gt;8 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-LMRS&#34;&gt;Llama 3.2 1B IT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4.94GB&lt;/td&gt; &#xA;   &lt;td&gt;20 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS&#34;&gt;Llama 3.2 1B IT Q8_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.27GB&lt;/td&gt; &#xA;   &lt;td&gt;35 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q4_0-LMRS&#34;&gt;Llama 3.2 3B IT Q4_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.71GB&lt;/td&gt; &#xA;   &lt;td&gt;17 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q8_0-LMRS&#34;&gt;Llama 3.2 3B IT Q8_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.31GB&lt;/td&gt; &#xA;   &lt;td&gt;16 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/Phi-3.5-vision-instruct-q8_0-LMRS&#34;&gt;PHI 3.5 IT Vision Q8_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4.28GB&lt;/td&gt; &#xA;   &lt;td&gt;15 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/samuel-vitorino/Phi-3.5-mini-instruct-q8_0-LMRS&#34;&gt;PHI 3.5 IT Mini Q8_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.94GB&lt;/td&gt; &#xA;   &lt;td&gt;16 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;You can download the prepared quantized model and tokenizer model files in the lmrs format from huggingface. If you&#39;d prefer to convert the models published by Google/Meta on huggingface yourself, please refer to the following section. Otherwise, you can skip ahead to the build section.&lt;/p&gt; &#xA;&lt;h3&gt;Model Conversion&lt;/h3&gt; &#xA;&lt;p&gt;Install additional python dependencies (assuming you already have pytorch installed) used in export.py and tokenizer.py:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the &lt;strong&gt;.safetensors&lt;/strong&gt; and &lt;strong&gt;config.json&lt;/strong&gt; files from the original model&#39;s page on huggingface (So we don&#39;t have to clone the pytorch repo). For multimodal models (PHI3.5 Vision), we also need the CLIP &lt;strong&gt;.config&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/config.json&#34;&gt;file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use the export.py script to convert the model bfloat16 weights into the LMRS format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To export the quantized version use the &lt;strong&gt;--quantize&lt;/strong&gt; and &lt;strong&gt;--quantize-type&lt;/strong&gt; flags. The int8 quantized model size should be 4X smaller (from ~9.8G to ~2.5G, depending on the group size). For multimodal models include the &lt;strong&gt;--vision-config&lt;/strong&gt; argument.&lt;/p&gt; &#xA;&lt;p&gt;Use the tokenizer.py script to convert the tokenizer model into the LMRS tokenizer format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;Compile the rust code with cargo (make sure to pass the target-cpu flag):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;RUSTFLAGS=&#34;-C target-cpu=native&#34; cargo build --release --bin chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable multimodality, include the multimodal feature by passing the &lt;strong&gt;--features multimodal&lt;/strong&gt; argument.&lt;/p&gt; &#xA;&lt;p&gt;And you are good to go:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;./target/release/chat --model [model weights file]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other arguments include tokenizer, temperature, top-p, show-metrics etc. To check available arguments run with --help. For multimodal models use the &lt;strong&gt;--image&lt;/strong&gt; argument with the image path.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To run the backend for the &lt;a href=&#34;https://github.com/samuel-vitorino/lm.rs-webui&#34;&gt;WebUI&lt;/a&gt;, first compile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;RUSTFLAGS=&#34;-C target-cpu=native&#34; cargo build --release --features backend --bin backend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For multimodality enable the &lt;strong&gt;backend-multimodal&lt;/strong&gt; feature.&lt;/p&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;./target/release/backend --model [model weights file]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can change the ip and port with --ip and --port. Other flags such as temperature, etc. are also available. For multimodal compatibility use the &lt;strong&gt;--multimodal&lt;/strong&gt; flag. You can now connect via the web interface.&lt;/p&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;p&gt;Some things to do in the future:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add other sampling methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Test the 9B and 27B models (tested the 9B, 27B would be too slow).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Parallelize the multi head attention loop.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add performance metrics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Ability to give a system prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Quantization support (int8, int4).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>