<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-15T02:58:03Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Atome-FE/llama-node</title>
    <updated>2023-04-15T02:58:03Z</updated>
    <id>tag:github.com,2023-04-15:/Atome-FE/llama-node</id>
    <link href="https://github.com/Atome-FE/llama-node" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Believe in AI democratization. llama for nodejs backed by llama-rs and llama.cpp, work locally on your laptop CPU. support llama/alpaca/gpt4all/vicuna model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama-node&lt;/h1&gt; &#xA;&lt;p&gt;Large Language Model LLaMA on node.js&lt;/p&gt; &#xA;&lt;p&gt;This project is in an early stage, the API for nodejs may change in the future, use it with caution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/README-zh-CN.md&#34;&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/doc/assets/llama.png&#34; width=&#34;300px&#34; height=&#34;300px&#34; alt=&#34;LLaMA generated by Stable diffusion&#34;&gt; &#xA;&lt;p&gt;&lt;sub&gt;Picture generated by stable diffusion.&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/hlhr202/llama-node/llama-build.yml&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/l/llama-node&#34; alt=&#34;NPM&#34;&gt; &lt;a href=&#34;https://www.npmjs.com/package/llama-node&#34;&gt;&lt;img alt=&#34;npm&#34; src=&#34;https://img.shields.io/npm/v/llama-node&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/npm/types/llama-node&#34; alt=&#34;npm type definitions&#34;&gt; &lt;a href=&#34;https://twitter.com/hlhr202&#34;&gt;&lt;img alt=&#34;twitter&#34; src=&#34;https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fhlhr202&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#llama-node&#34;&gt;llama-node&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#getting-the-weights&#34;&gt;Getting the weights&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#model-versioning&#34;&gt;Model versioning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#usage-llamacpp-backend&#34;&gt;Usage (llama.cpp backend)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#tokenize&#34;&gt;Tokenize&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#embedding&#34;&gt;Embedding&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#usage-llama-rs-backend&#34;&gt;Usage (llama-rs backend)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#inference-1&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#tokenize-1&#34;&gt;Tokenize&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#embedding-1&#34;&gt;Embedding&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#performance-related&#34;&gt;Performance related&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#manual-compilation-from-node_modules&#34;&gt;Manual compilation (from node_modules)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#manual-compilation-from-source&#34;&gt;Manual compilation (from source)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Atome-FE/llama-node/main/#future-plan&#34;&gt;Future plan&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This is a nodejs client library for llama (or llama based) LLM built on top of &lt;a href=&#34;https://github.com/rustformers/llama-rs&#34;&gt;llama-rs&lt;/a&gt; and &lt;a href=&#34;https://github.com/sobelio/llm-chain/tree/main/llm-chain-llama/sys&#34;&gt;llm-chain-llama-sys&lt;/a&gt; which generate bindings for &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;. It uses &lt;a href=&#34;https://github.com/napi-rs/napi-rs&#34;&gt;napi-rs&lt;/a&gt; for channel messages between node.js and llama thread.&lt;/p&gt; &#xA;&lt;p&gt;From v0.0.21, both llama-rs and llama.cpp backends are supported!&lt;/p&gt; &#xA;&lt;p&gt;Currently supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;darwin-x64&lt;/li&gt; &#xA; &lt;li&gt;darwin-arm64&lt;/li&gt; &#xA; &lt;li&gt;linux-x64-gnu&lt;/li&gt; &#xA; &lt;li&gt;win32-x64-msvc&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I do not have hardware for testing 13B or larger models, but I have tested it supported llama 7B model with both ggml llama and ggml alpaca.&lt;/p&gt; &#xA;&lt;!-- Download one of the llama ggml models from the following links:&#xA;- [llama 7B int4 (old model for llama.cpp)](https://huggingface.co/hlhr202/llama-7B-ggml-int4/blob/main/ggml-model-q4_0.bin)&#xA;- [alpaca 7B int4](https://huggingface.co/hlhr202/alpaca-7B-ggml-int4/blob/main/ggml-alpaca-7b-q4.bin) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install main package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install llama-node&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install llama-rs backend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @llama-node/core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install llama.cpp backend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @llama-node/llama-cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting the weights&lt;/h2&gt; &#xA;&lt;p&gt;The llama-node uses llama-rs under the hook and uses the model format derived from llama.cpp. Due to the fact that the meta-release model is only used for research purposes, this project does not provide model downloads. If you have obtained the original &lt;strong&gt;.pth&lt;/strong&gt; model, please read the document &lt;a href=&#34;https://github.com/rustformers/llama-rs#getting-the-weights&#34;&gt;Getting the weights&lt;/a&gt; and use the convert tool provided by llama-rs for conversion.&lt;/p&gt; &#xA;&lt;h3&gt;Model versioning&lt;/h3&gt; &#xA;&lt;p&gt;There are now 3 versions from llama.cpp community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GGML: legacy format, oldest ggml tensor file format&lt;/li&gt; &#xA; &lt;li&gt;GGMF: also legacy format, newer than GGML, older than GGJT&lt;/li&gt; &#xA; &lt;li&gt;GGJT: mmap-able format&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The llama-rs backend now only supports GGML/GGMF models, and llama.cpp backend only supports GGJT models.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage (llama.cpp backend)&lt;/h2&gt; &#xA;&lt;p&gt;The current version supports only one inference session on one LLama instance at the same time&lt;/p&gt; &#xA;&lt;p&gt;If you wish to have multiple inference sessions concurrently, you need to create multiple LLama instances&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaCpp, LoadConfig } from &#34;llama-node/dist/llm/llama-cpp.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-vicuna-7b-4bit-rev1.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaCpp);&#xA;&#xA;const config: LoadConfig = {&#xA;    path: model,&#xA;    enableLogging: true,&#xA;    nCtx: 1024,&#xA;    nParts: -1,&#xA;    seed: 0,&#xA;    f16Kv: false,&#xA;    logitsAll: false,&#xA;    vocabOnly: false,&#xA;    useMlock: false,&#xA;    embedding: false,&#xA;};&#xA;&#xA;llama.load(config);&#xA;&#xA;const template = `How are you`;&#xA;&#xA;const prompt = `### Human:&#xA;&#xA;${template}&#xA;&#xA;### Assistant:`;&#xA;&#xA;llama.createCompletion(&#xA;    {&#xA;        nThreads: 4,&#xA;        nTokPredict: 2048,&#xA;        topK: 40,&#xA;        topP: 0.1,&#xA;        temp: 0.2,&#xA;        repeatPenalty: 1,&#xA;        stopSequence: &#34;### Human&#34;,&#xA;        prompt,&#xA;    },&#xA;    (response) =&amp;gt; {&#xA;        process.stdout.write(response.token);&#xA;    }&#xA;);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tokenize&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaCpp, LoadConfig } from &#34;llama-node/dist/llm/llama-cpp.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-vicuna-7b-4bit-rev1.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaCpp);&#xA;&#xA;const config: LoadConfig = {&#xA;    path: model,&#xA;    enableLogging: true,&#xA;    nCtx: 1024,&#xA;    nParts: -1,&#xA;    seed: 0,&#xA;    f16Kv: false,&#xA;    logitsAll: false,&#xA;    vocabOnly: false,&#xA;    useMlock: false,&#xA;    embedding: false,&#xA;};&#xA;&#xA;llama.load(config);&#xA;&#xA;const content = &#34;how are you?&#34;;&#xA;&#xA;llama.tokenize({ content, nCtx: 2048 }).then(console.log);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embedding&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaCpp, LoadConfig } from &#34;llama-node/dist/llm/llama-cpp.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-vicuna-7b-4bit-rev1.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaCpp);&#xA;&#xA;const config: LoadConfig = {&#xA;    path: model,&#xA;    enableLogging: true,&#xA;    nCtx: 1024,&#xA;    nParts: -1,&#xA;    seed: 0,&#xA;    f16Kv: false,&#xA;    logitsAll: false,&#xA;    vocabOnly: false,&#xA;    useMlock: false,&#xA;    embedding: false,&#xA;};&#xA;&#xA;llama.load(config);&#xA;&#xA;const prompt = `Who is the president of the United States?`;&#xA;&#xA;const params = {&#xA;    nThreads: 4,&#xA;    nTokPredict: 2048,&#xA;    topK: 40,&#xA;    topP: 0.1,&#xA;    temp: 0.2,&#xA;    repeatPenalty: 1,&#xA;    prompt,&#xA;};&#xA;&#xA;llama.getEmbedding(params).then(console.log);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage (llama-rs backend)&lt;/h2&gt; &#xA;&lt;p&gt;The current version supports only one inference session on one LLama instance at the same time&lt;/p&gt; &#xA;&lt;p&gt;If you wish to have multiple inference sessions concurrently, you need to create multiple LLama instances&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaRS } from &#34;llama-node/dist/llm/llama-rs.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-alpaca-7b-q4.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaRS);&#xA;&#xA;llama.load({ path: model });&#xA;&#xA;const template = `how are you`;&#xA;&#xA;const prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;&#xA;${template}&#xA;&#xA;### Response:`;&#xA;&#xA;llama.createCompletion(&#xA;    {&#xA;        prompt,&#xA;        numPredict: 128,&#xA;        temp: 0.2,&#xA;        topP: 1,&#xA;        topK: 40,&#xA;        repeatPenalty: 1,&#xA;        repeatLastN: 64,&#xA;        seed: 0,&#xA;        feedPrompt: true,&#xA;    },&#xA;    (response) =&amp;gt; {&#xA;        process.stdout.write(response.token);&#xA;    }&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tokenize&lt;/h3&gt; &#xA;&lt;p&gt;Get tokenization result from LLaMA&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaRS } from &#34;llama-node/dist/llm/llama-rs.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-alpaca-7b-q4.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaRS);&#xA;&#xA;llama.load({ path: model });&#xA;&#xA;const content = &#34;how are you?&#34;;&#xA;&#xA;llama.tokenize(content).then(console.log);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embedding&lt;/h3&gt; &#xA;&lt;p&gt;Preview version, embedding end token may change in the future. Do not use it in production!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaRS } from &#34;llama-node/dist/llm/llama-rs.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;import fs from &#34;fs&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-alpaca-7b-q4.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaRS);&#xA;&#xA;llama.load({ path: model });&#xA;&#xA;const getWordEmbeddings = async (prompt: string, file: string) =&amp;gt; {&#xA;    const data = await llama.getEmbedding({&#xA;        prompt,&#xA;        numPredict: 128,&#xA;        temp: 0.2,&#xA;        topP: 1,&#xA;        topK: 40,&#xA;        repeatPenalty: 1,&#xA;        repeatLastN: 64,&#xA;        seed: 0,&#xA;    });&#xA;&#xA;    console.log(prompt, data);&#xA;&#xA;    await fs.promises.writeFile(&#xA;        path.resolve(process.cwd(), file),&#xA;        JSON.stringify(data)&#xA;    );&#xA;};&#xA;&#xA;const run = async () =&amp;gt; {&#xA;    const dog1 = `My favourite animal is the dog`;&#xA;    await getWordEmbeddings(dog1, &#34;./example/semantic-compare/dog1.json&#34;);&#xA;&#xA;    const dog2 = `I have just adopted a cute dog`;&#xA;    await getWordEmbeddings(dog2, &#34;./example/semantic-compare/dog2.json&#34;);&#xA;&#xA;    const cat1 = `My favourite animal is the cat`;&#xA;    await getWordEmbeddings(cat1, &#34;./example/semantic-compare/cat1.json&#34;);&#xA;};&#xA;&#xA;run();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Performance related&lt;/h2&gt; &#xA;&lt;p&gt;We provide prebuild binaries for linux-x64, win32-x64, apple-x64, apple-silicon. For other platforms, before you install the npm package, please install rust environment for self built.&lt;/p&gt; &#xA;&lt;p&gt;Due to complexity of cross compilation, it is hard for pre-building a binary that fits all platform needs with best performance.&lt;/p&gt; &#xA;&lt;p&gt;If you face low performance issue, I would strongly suggest you do a manual compilation. Otherwise you have to wait for a better pre-compiled native binding. I am trying to investigate the way to produce a matrix of multi-platform supports.&lt;/p&gt; &#xA;&lt;h3&gt;Manual compilation (from node_modules)&lt;/h3&gt; &#xA;&lt;p&gt;The following steps will allow you to compile the binary with best quality on your platform&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pre-request: install rust&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under node_modules/@llama-node/core folder&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Manual compilation (from source)&lt;/h3&gt; &#xA;&lt;p&gt;The following steps will allow you to compile the binary with best quality on your platform&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pre-request: install rust&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under root folder, run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install &amp;amp;&amp;amp; npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under packages/core folder, run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use the dist under root folder&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Future plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; prompt extensions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; more platforms and cross compile (performance related)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; tweak embedding API, make end token configurable&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; cli and interactive&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; support more open source models as llama-rs planned &lt;a href=&#34;https://github.com/rustformers/llama-rs/pull/85&#34;&gt;https://github.com/rustformers/llama-rs/pull/85&lt;/a&gt; &lt;a href=&#34;https://github.com/rustformers/llama-rs/issues/75&#34;&gt;https://github.com/rustformers/llama-rs/issues/75&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; more backends (eg. rwkv) supports!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>hiro-codes/bolt</title>
    <updated>2023-04-15T02:58:03Z</updated>
    <id>tag:github.com,2023-04-15:/hiro-codes/bolt</id>
    <link href="https://github.com/hiro-codes/bolt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bolt is a desktop application that is designed to make the process of developing and testing APIs easier and more efficient.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/yMEKS2hk&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1018936651612967043&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hiro-codes/bolt/search?l=rust&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/languages/top/hiro-codes/bolt&#34; alt=&#34;GitHub top language&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Bolt ‚ö°&lt;/h1&gt; &#xA;&lt;p&gt;Bolt is a desktop application that is designed to make the process of developing and testing APIs easier and more efficient. Like Postman, but open source and written in Rust ü¶Ä&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hiro-codes/bolt/raw/master/screenshot.png?raw=true&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Bolt is experimental software. Expect:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Bugs&lt;/li&gt; &#xA;  &lt;li&gt;Missing features&lt;/li&gt; &#xA;  &lt;li&gt;Breaking changes&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Download pre-built binaries&lt;/h3&gt; &#xA;&lt;p&gt;Pre-built binaries for Windows, macOS and Linux can be found in the &lt;a href=&#34;https://github.com/hiro-codes/bolt/releases/latest&#34;&gt;latest release&lt;/a&gt; downloads section.&lt;/p&gt; &#xA;&lt;h3&gt;Build from source üë©‚Äçüíª&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è Prerequisites&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;Rust&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.gnu.org/software/make/#download&#34;&gt;Make&lt;/a&gt; or &lt;a href=&#34;https://github.com/casey/just&#34;&gt;just&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hiro-codes/bolt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd bolt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just setup # or make setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just build # or make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features üöß&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Http Requests&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Collections&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Testing and benchmarking&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Websockets&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Logging&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TCP/UDP&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors ‚ú®&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/hiro-codes/bolt/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=hiro-codes/bolt&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Donate/Sponsor ‚≠ê&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/0xhiro&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://cdn.buymeacoffee.com/buttons/v2/default-white.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: 60px !important;width: 217px !important;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Made with ‚ù§Ô∏è by &lt;a href=&#34;https://twitter.com/hiro_codes&#34;&gt;0xHiro&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>