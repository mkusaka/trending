<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-26T01:30:39Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lbcb-sci/herro</title>
    <updated>2024-04-26T01:30:39Z</updated>
    <id>tag:github.com,2024-04-26:/lbcb-sci/herro</id>
    <link href="https://github.com/lbcb-sci/herro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;HERRO is a highly-accurate, haplotype-aware, deep-learning tool for error correction of Nanopore R10.4.1 or R9.4.1 reads (read length of &gt;= 10 kbps is recommended).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HERRO&lt;/h1&gt; &#xA;&lt;p&gt;HERRO (Haplotype-aware ERRor cOrrection) is a highly accurate, haplotype-aware, deep-learning tool for error correction of Nanopore R10.4.1, Kit 14 reads (length of â‰¥ 10000bp is recommended). An experimental model for R9.4.1 data is also provided for download.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux OS (tested on RHEL 8.6 and Ubuntu 22.04)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://facebook.github.io/zstd/&#34;&gt;Zstandard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python (and conda) for data preprocessing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compile from source&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://download.pytorch.org/libtorch/cu117/libtorch-shared-with-deps-2.0.1%2Bcu117.zip&#34;&gt;libtorch 2.0.*&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rustup.rs/&#34;&gt;rustup&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/dominikstanojevic/herro.git&#xA;cd herro&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create conda environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create --file scripts/herro-env.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Build &lt;code&gt;herro&lt;/code&gt; binary (singularity or compile from source)&lt;/p&gt; &lt;p&gt;a. Download singularity image:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download the image&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget http://complex.zesoi.fer.hr/data/downloads/herro.sif&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;b. Build singularity image (requires sudo)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo singularity build herro.sif herro-singularity.def&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the tool (see &lt;a href=&#34;https://raw.githubusercontent.com/lbcb-sci/herro/main/#usage&#34;&gt;Usage&lt;/a&gt;) with: &lt;code&gt;singularity run --nv --bind &amp;lt;host_path&amp;gt;:&amp;lt;dest_path&amp;gt; herro.sif inference &amp;lt;args&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;c. Compile&lt;/p&gt; &lt;p&gt;When compiling from source, ensure that libtorch and rustup are downloaded and installed.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export LIBTORCH=&amp;lt;libtorch_path&amp;gt;&#xA;export LD_LIBRARY_PATH=$LIBTORCH/lib:$LD_LIBRARY_PATH&#xA;RUSTFLAGS=&#34;-Ctarget-cpu=native&#34; cargo build -q --release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Path to the resulting binary: &lt;code&gt;target/release/herro&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Model Download&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For R10.4.1 data,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget http://complex.zesoi.fer.hr/data/downloads/model_v0.1.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For R9.4.1 data (experimental),&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget http://complex.zesoi.fer.hr/data/downloads/model_R9_v0.1.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Preprocess reads&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;scripts/preprocess.sh &amp;lt;input_fastq&amp;gt; &amp;lt;output_prefix&amp;gt; &amp;lt;number_of_threads&amp;gt; &amp;lt;parts_to_split_job_into&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Porechop loads all reads into memory, so the input may need to be split into multiple parts. Set &amp;lt;parts_to_split_job_into&amp;gt; to 1 if splitting is not needed. In Dorado v0.5, adapter trimming was added, so adapter trimming and splitting using Porechop and duplex tools will probably be removed in the future.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;minimap2 alignment and batching&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Although minimap2 can be run from the &lt;code&gt;herro&lt;/code&gt; binary (omit --read-alns or use --write-alns to store batched alignments for future use), the preferred method is to initially run minimap2 and then utilize it to generate alignment batches. These batches will be used as input for the &lt;code&gt;herro&lt;/code&gt; binary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;scripts/create_batched_alignments.sh &amp;lt;output_from_reads_preprocessing&amp;gt; &amp;lt;read_ids&amp;gt; &amp;lt;num_of_threads&amp;gt; &amp;lt;directory_for_batches_of_alignments&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Read ids can be obtained with seqkit: &lt;code&gt;seqkit seq -ni &amp;lt;reads&amp;gt; &amp;gt; &amp;lt;read_ids&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Error-correction&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;herro inference --read-alns &amp;lt;directory_alignment_batches&amp;gt; -t &amp;lt;feat_gen_threads_per_device&amp;gt; -d &amp;lt;gpus&amp;gt; -m &amp;lt;model_path&amp;gt; -b &amp;lt;batch_size&amp;gt; &amp;lt;preprocessed_reads&amp;gt; &amp;lt;fasta_output&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: GPUs are specified using their IDs. For example, if the value of the parameter -d is set to 0,1,3, herro will use the first, second, and fourth GPU cards. Parameter &lt;code&gt;-t&lt;/code&gt; is given &lt;strong&gt;per device&lt;/strong&gt; - e.g., if &lt;code&gt;-t&lt;/code&gt; is set to &lt;code&gt;8&lt;/code&gt; and 3 GPUs are used, herro will create 24 feature generation theads in total. Recommended batch size is 64 for GPUs with 40 GB (possibly also for 32 GB) of VRAM and 128 for GPUs with 80 GB of VRAM.&lt;/p&gt; &#xA;&lt;h2&gt;Results on HG002 data&lt;/h2&gt; &#xA;&lt;p&gt;HG002 data was assembled using hifiasm and compared to HiFi reads. Results for uncorrected reads are not given since they produce poor assembly. Currently, data is not publicly available.&lt;/p&gt; &#xA;&lt;p&gt;Assembly results and comparison with Hifi reads and uncorrected UL are given in the table below. Assemblies were perform using:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Hifi reads/Duplex ONT reads/Corrected UL reads&lt;/li&gt; &#xA; &lt;li&gt;Uncorrected Ultra-long ONT reads as UL reads&lt;/li&gt; &#xA; &lt;li&gt;Parental Illumina data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Hifiasm command used for all experiments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;hifiasm -o &amp;lt;output_prefix&amp;gt; -t &amp;lt;num_threads&amp;gt; --ul &amp;lt;UL_reads&amp;gt; --ul-cut 10000 -1 &amp;lt;parent1_yak&amp;gt; -2 &amp;lt;parent2_yak&amp;gt; &amp;lt;HiFi/Duplex/Corrected UL reads&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lbcb-sci/herro/main/hg002-assm-results.png&#34; alt=&#34;HG002 Assembly Results&#34; width=&#34;600&#34;&gt; &#xA;&lt;h2&gt;Results on Error-corrected HG002 experimental, high-accuracy, UL data&lt;/h2&gt; &#xA;&lt;p&gt;Experimental high-accuracy, UL HG002 error-corrected reads can be found in the s3 bucket. Raw data used for the error-correction can be found &lt;a href=&#34;https://labs.epi2me.io/gm24385_ncm23_preview/&#34;&gt;here&lt;/a&gt;. Assemblies were done in the same way as in the previous section.&lt;/p&gt; &#xA;&lt;h3&gt;Download&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download error-corrected reads:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget http://complex.zesoi.fer.hr/data/downloads/HG002.experimentalUL.corrected.fasta.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;Assembly results and comparison with Hifi reads and uncorrected UL are given in the table below. Assemblies were perform using:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Hifi/HQ Uncorrected UL/Corrected UL reads&lt;/li&gt; &#xA; &lt;li&gt;HQ Uncorrected Ultra-long ONT reads as UL reads&lt;/li&gt; &#xA; &lt;li&gt;Parental Illumina data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lbcb-sci/herro/main/hg002-hq-assm-results.png&#34; alt=&#34;HG002 HQ Assembly Results&#34; width=&#34;600&#34;&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This work has been supported by AI Singapore 100 Experiments (100E) Programme under the project AI-driven De Novo Diploid Assembler (AISG2-100E-2021-076) in collaboration with Agency for Science, Technology and Research (A*STAR), and Oxford Nanopore Technologies plc. (ONT).&lt;/p&gt;</summary>
  </entry>
</feed>