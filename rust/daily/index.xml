<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-03T01:43:47Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>srush/llama2.rs</title>
    <updated>2023-08-03T01:43:47Z</updated>
    <id>tag:github.com,2023-08-03:/srush/llama2.rs</id>
    <link href="https://github.com/srush/llama2.rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama2.rs&lt;/h1&gt; &#xA;&lt;p&gt;This is a one-file Rust implementation of Llama2 that works pretty well. It&#39;s Rust port of Karpathy&#39;s &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/srush/llama2.rs/assets/35882/dac9a285-b141-409f-bb46-c81a28516cd1&#34; width=&#34;300px&#34;&gt; &#xA;&lt;p&gt;To build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run (follow instructions to get &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2_7b.bin&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; target/release/llama2_rs llama2_7b.bin 0.0 11 &#34;The only thing&#34;&#xA;The only thing that is certain in life is change.&#xA;achieved tok/s: 1.0298662&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It actually seems like it is pretty fast! On my computer this is the speed and output of running the original llama2.c&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ./run llama2_7b.bin 0.0 11 &#34;The only thing&#34;&#xA;The only thing that is certain in life is change.&#xA;achieved tok/s: 0.139889&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;See Also&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gaxler/llama2.rs&#34;&gt;llama2.rs&lt;/a&gt; from @gaxler&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leo-du/llama2.rs&#34;&gt;llama2.rs&lt;/a&gt; from @leo-du&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LaurentMazare/candle&#34;&gt;candle&lt;/a&gt; and candle llama from @LaurentMazare&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How does it work?&lt;/h3&gt; &#xA;&lt;p&gt;This is basically a port of the original code, with extra type information to make it easier to extend.&lt;/p&gt; &#xA;&lt;p&gt;There are two dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;memmap2&lt;/code&gt;for memory mapping&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rayon&lt;/code&gt; for parallel computation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Todo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; - Generic over floating point size&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; - Faster matrix multiplications&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; - More safety, remove some of the C hacks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why?&lt;/h3&gt; &#xA;&lt;p&gt;Mostly this was an exercise in learning some Rust. Was curious how you port over things like memory mapping, parallel processing, and some of the mathematical tricks.&lt;/p&gt; &#xA;&lt;p&gt;This is my first Rust project, so if you are an expert I would love a code review!&lt;/p&gt;</summary>
  </entry>
</feed>