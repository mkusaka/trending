<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-31T01:43:58Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cube-js/cube</title>
    <updated>2023-03-31T01:43:58Z</updated>
    <id>tag:github.com,2023-03-31:/cube-js/cube</id>
    <link href="https://github.com/cube-js/cube" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üìä Cube ‚Äî The Semantic Layer for Building Data Applications&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://cube.dev?ref=github-readme&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cube-js/cube.js/master/docs/content/cube-logo-with-bg.png&#34; alt=&#34;Cube ‚Äî Semantic Layer for Data Applications&#34; width=&#34;300px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cube.dev?ref=github-readme&#34;&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://cube.dev/docs/getting-started?ref=github-readme&#34;&gt;Getting Started&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://cube.dev/docs?ref=github-readme&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://cube.dev/docs/examples?ref=github-readme&#34;&gt;Examples&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://cube.dev/blog?ref=github-readme&#34;&gt;Blog&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://slack.cube.dev?ref=github-readme&#34;&gt;Slack&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://twitter.com/the_cube_dev&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/js/%40cubejs-backend%2Fserver&#34;&gt;&lt;img src=&#34;https://badge.fury.io/js/%40cubejs-backend%2Fserver.svg?sanitize=true&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cube-js/cube/actions?query=workflow%3ABuild+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/cube-js/cube/workflows/Build/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.fossa.io/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cube is the semantic layer for building data applications.&lt;/strong&gt; It helps data engineers and application developers access data from modern data stores, organize it into consistent definitions, and deliver it to every application.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cube-js/cube.js/master/docs/content/cube-scheme-dark.png&#34; style=&#34;border: none&#34; width=&#34;100%&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;Learn more about connecting Cube to &lt;a href=&#34;https://cube.dev/docs/config/databases?ref=github-readme&#34; target=&#34;_blank&#34;&gt;data sources&lt;/a&gt; and &lt;a href=&#34;https://cube.dev/docs/config/downstream?ref=github-readme&#34; target=&#34;_blank&#34;&gt;analytics &amp;amp; visualization tools&lt;/a&gt;.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;p&gt;Cube was designed to work with all SQL-enabled data sources, including cloud data warehouses like Snowflake or Google BigQuery, query engines like Presto or Amazon Athena, and application databases like Postgres. Cube has a built-in relational caching engine to provide sub-second latency and high concurrency for API requests.&lt;/p&gt; &#xA;&lt;p&gt;For more details, see the &lt;a href=&#34;https://cube.dev/docs/cubejs-introduction?ref=github-readme&#34;&gt;introduction&lt;/a&gt; page in our documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Why Cube?&lt;/h2&gt; &#xA;&lt;p&gt;If you are building a data application‚Äîsuch as a business intelligence tool or a customer-facing analytics feature‚Äîyou‚Äôll probably face the following problems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;SQL code organization.&lt;/strong&gt; Sooner or later, modeling even a dozen metrics with a dozen dimensions using pure SQL queries becomes a maintenance nightmare, which leads to building a modeling framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Performance.&lt;/strong&gt; Most of the time and effort in modern analytics software development is spent providing adequate time to insight. In a world where every company‚Äôs data is big data, writing just SQL queries to get insight isn‚Äôt enough anymore.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Access Control.&lt;/strong&gt; It is important to secure and govern access to data for all downstream data consuming applications.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Cube has the necessary infrastructure and features to implement efficient data modeling, access control, and performance optimizations so that every application‚Äîlike embedded analytics, dashboarding and reporting tools, data notebooks, and other tools‚Äîcan access consistent data via REST, SQL, and GraphQL APIs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cube-js/cube.js/master/docs/content/old-was-vs-cubejs-way.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started üöÄ&lt;/h2&gt; &#xA;&lt;h3&gt;Cube Cloud&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cube.dev/cloud?ref=github-readme&#34;&gt;Cube Cloud&lt;/a&gt; is the fastest way to get started with Cube. It provides managed infrastructure as well as an instant and free access for development projects and proofs of concept.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cubecloud.dev/auth/signup?ref=github-readme&#34;&gt;&lt;img src=&#34;https://cubedev-blog-images.s3.us-east-2.amazonaws.com/f1f1eac0-0b44-4c47-936e-33b5c06eedf0.png&#34; alt=&#34;Get started now&#34; width=&#34;200px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For a step-by-step guide on Cube Cloud, &lt;a href=&#34;https://cube.dev/docs/cloud/getting-started/create?ref=github-readme&#34;&gt;see the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you can get started with Cube locally or self-host it with &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once Docker is installed, in a new folder for your project, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 4000:4000 \&#xA;  -v ${PWD}:/cube/conf \&#xA;  -e CUBEJS_DEV_MODE=true \&#xA;  cubejs/cube&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, open &lt;a href=&#34;http://localhost:4000&#34;&gt;http://localhost:4000&lt;/a&gt; in your browser to continue setup.&lt;/p&gt; &#xA;&lt;p&gt;For a step-by-step guide on Docker, &lt;a href=&#34;https://cube.dev/docs/getting-started-docker?ref=github-readme&#34;&gt;see the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cube.dev/docs?ref=github-readme&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cube.dev/docs/getting-started?ref=github-readme&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cube.dev/docs/examples?ref=github-readme&#34;&gt;Examples &amp;amp; Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cube.dev/docs/cubejs-introduction?ref=github-readme#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or need help - &lt;a href=&#34;https://slack.cube.dev?ref=github-readme&#34;&gt;please join our Slack community&lt;/a&gt; of amazing developers and data engineers.&lt;/p&gt; &#xA;&lt;p&gt;You are also welcome to join our &lt;strong&gt;monthly community calls&lt;/strong&gt; where we discuss community news, Cube Dev team&#39;s plans, backlogs, use cases, etc. If you miss the call, the recordings will also be available after the meeting.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When: Second Wednesday of each month at &lt;a href=&#34;https://www.thetimezoneconverter.com/?t=09:00&amp;amp;tz=PT%20%28Pacific%20Time%29&#34;&gt;9am Pacific Time&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Meeting link: &lt;a href=&#34;https://us02web.zoom.us/j/86717042169?pwd=VlBEd2VVK01DNDVVbU1EUXd5ajhsdz09&#34;&gt;https://us02web.zoom.us/j/86717042169?pwd=VlBEd2VVK01DNDVVbU1EUXd5ajhsdz09&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cube.dev/community-call/&#34;&gt;Meeting page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Recordings will be posted on the &lt;a href=&#34;https://www.youtube.com/playlist?list=PLtdXl_QTQjpb1dHZCM09qKTsgvgqjSvc9&#34;&gt;Community Call Playlist&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Our quarterly roadmap&lt;/h3&gt; &#xA;&lt;p&gt;We publish our open source roadmap every quarter and discuss them during our &lt;a href=&#34;https://cube.dev/community-call/&#34;&gt;monthly community calls&lt;/a&gt;. You can find our roadmap under &lt;a href=&#34;https://github.com/cube-js/cube/projects?query=is%3Aopen+sort%3Aupdated-desc&#34;&gt;projects in our Cube.js repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;There are many ways you can contribute to Cube! Here are a few possibilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Star this repo and follow us on &lt;a href=&#34;https://twitter.com/the_cube_dev&#34;&gt;Twitter&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add Cube to your stack on &lt;a href=&#34;https://stackshare.io/cube-js&#34;&gt;Stackshare&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Upvote issues with üëç reaction so we know what&#39;s the demand for particular issue to prioritize it within road map.&lt;/li&gt; &#xA; &lt;li&gt;Create issues every time you feel something is missing or goes wrong.&lt;/li&gt; &#xA; &lt;li&gt;Ask questions on &lt;a href=&#34;https://stackoverflow.com/questions/tagged/cube.js&#34;&gt;Stack Overflow with cube.js tag&lt;/a&gt; if others can have these questions as well.&lt;/li&gt; &#xA; &lt;li&gt;Provide pull requests for all open issues and especially for those with &lt;a href=&#34;https://github.com/cube-js/cube/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;help wanted&lt;/a&gt; and &lt;a href=&#34;https://github.com/cube-js/cube/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&#34;&gt;good first issue&lt;/a&gt; labels.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All sort of contributions are &lt;strong&gt;welcome and extremely helpful&lt;/strong&gt; üôå Please refer to &lt;a href=&#34;https://github.com/cube-js/cube/raw/master/CONTRIBUTING.md&#34;&gt;the contribution guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Cube Client is &lt;a href=&#34;https://raw.githubusercontent.com/cube-js/cube/master/packages/cubejs-client-core/LICENSE&#34;&gt;MIT licensed&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Cube Backend is &lt;a href=&#34;https://raw.githubusercontent.com/cube-js/cube/master/packages/cubejs-server/LICENSE&#34;&gt;Apache 2.0 licensed&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.io/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HigherOrderCO/HVM</title>
    <updated>2023-03-31T01:43:58Z</updated>
    <id>tag:github.com,2023-03-31:/HigherOrderCO/HVM</id>
    <link href="https://github.com/HigherOrderCO/HVM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A massively parallel, optimal functional runtime in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Higher-order Virtual Machine (HVM)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Higher-order Virtual Machine (HVM)&lt;/strong&gt; is a pure functional runtime that is &lt;strong&gt;lazy&lt;/strong&gt;, &lt;strong&gt;non-garbage-collected&lt;/strong&gt; and &lt;strong&gt;massively parallel&lt;/strong&gt;. It is also &lt;strong&gt;beta-optimal&lt;/strong&gt;, meaning that, for higher-order computations, it can, in some cases, be exponentially (in the asymptotical sense) faster than alternatives, including Haskell&#39;s GHC.&lt;/p&gt; &#xA;&lt;p&gt;That is possible due to a new model of computation, the &lt;strong&gt;Interaction Net&lt;/strong&gt;, which supersedes the &lt;strong&gt;Turing Machine&lt;/strong&gt; and the &lt;strong&gt;Lambda Calculus&lt;/strong&gt;. Previous implementations of this model have been inefficient in practice, however, a recent breakthrough has drastically improved its efficiency, resulting in the HVM. Despite being relatively new, it already beats mature compilers in some cases, and is being continuously improved.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Welcome to the massively parallel future of computers!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Examples&lt;/h1&gt; &#xA;&lt;p&gt;Essentially, HVM is a minimalist functional language that is compiled to a novel runtime based on &lt;a href=&#34;https://pdf.sciencedirectassets.com/272575/1-s2.0-S0890540100X00600/1-s2.0-S0890540197926432/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEkaCXVzLWVhc3QtMSJGMEQCIDQQ1rA8jV9UQEKlaFzLfqINP%2B%2FrcTj4NCbI40n%2FrGbyAiBLG%2FpnaXmp6BGaG1Yplr3YYsHzxet6aQXc1qnkbb3W0irMBAhSEAUaDDA1OTAwMzU0Njg2NSIM1FOMCDHcoyvFFAU6KqkEgJPcH6B8%2BRYsdLtUERtVlwtcXZBW38xnvb%2FPRSkmxcNaK%2BmQTa7L3ZFuZt9rpNjrB3sJHg%2Bxc%2FqdAF%2FsthEb1NreHNze7LmbStuRufZCGEcxax%2FsyjnSb9bnrHuDEpnck1Dhk2YPqR8%2Bim%2BdQisDUp%2F4torZsCK1%2BPAEQkQAmGqinioexAr8dEE0BOlHgxBz5YRIkV9pjLoq%2FjWFqiUSO2bPdVi2AfpDbXI48ek6gQs%2F6VTIFRShfezfAr1HoDlQEoyyVYnVy6wI%2Fu1WVB%2FA0JJHK1B7rZFEYilPSAdUpVSOvjhNHN9elxIxlFX6hOZz3YJ4QDeLCPztfMClYYxAex6hoBBVzTkRzszs18hK1K%2FMUMwF4o%2FDy1i3WLeUmC36CL7WXDik%2BTZ7WjJNYGVRILH6cDsHrg17A0MVI5njvw7iM%2FrYKoOgBD2ESct4nO3mpRkKVq%2F9UyKScwVT5VrNpuLWLnrg29BDvE%2BDoFI6c71cisENjhIhGPNrBCQvZLNe1k%2BD54NyfqOe4a1DguuzxBnsNj6BBD2lM6TyDvCz9w36u194aN8oks9hLuTuKp7Rk05dTt6rj4pThkHA%2FQQymmx74MlQtTXTnD5v%2F%2BmGSUz6vHzqaV2Ft5xjWf9w9NJHfTkFkpxNEv8fTUUSMBEhL4nF8wj0wiNbSwp9NvPOj3YMIG2icNxdAZyNsJYJUowOCXi4JTwCkqb2WdNOi88pOSaAautZrBg7nzCKyuCbBjqqATOzXItndBn%2Be6oyH2l8sD%2B5v%2FjIqCz8%2Bx%2Bz%2FZA3dntddFac64iWFGPbJeRGw05BiPX5TKBnrR%2BmaqfO%2F7SxoYfTV4hl5Z2lmJcoiEd%2BWUmNK2wntMlGtFn%2FmFeeljKBeMxnfh8DN0qRz10NZAfxhvqxAEBu67G0ZXpECGxr8fAiBrdvnEac6rWfv8%2FT0VA%2Fu6xjIMIrrwU65xAuVuIG%2BXpsdC073VLm1%2BEW&amp;amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Date=20221119T011901Z&amp;amp;X-Amz-SignedHeaders=host&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Credential=ASIAQ3PHCVTYYRK5XVMW%2F20221119%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Signature=74892553e56ba432350974a6f4dbebfd97418e2187a5c4e183da61dd0e951609&amp;amp;hash=bc1de316d0b6ee58191106c1cdbc34d1eaeab536a9bbc02dfae09818a8cc2510&amp;amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;amp;pii=S0890540197926432&amp;amp;tid=spdf-00500b38-a41c-4d5b-98bb-4a2754da3953&amp;amp;sid=17532fa99b4522476f2b00d636dc838e7e36gxrqa&amp;amp;type=client&amp;amp;ua=515904515402570a0401&amp;amp;rr=76c51d7eea7b4d36&#34;&gt;Interaction Nets&lt;/a&gt;. This approach is not only memory-efficient (no GC needed), but also has two significant advantages: &lt;strong&gt;automatic parallelism&lt;/strong&gt; and &lt;strong&gt;beta-optimality&lt;/strong&gt;. The idea is that you write a simple functional program, and HVM will turn it into a massively parallel, beta-optimal executable. The examples below highlight these advantages in action.&lt;/p&gt; &#xA;&lt;h2&gt;Bubble Sort&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;From: &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/examples/sort/bubble/main.hvm&#34;&gt;HVM/examples/sort/bubble/main.hvm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;From: &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/examples/sort/bubble/main.hs&#34;&gt;HVM/examples/sort/bubble/main.hs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// sort : List -&amp;gt; List&#xA;(Sort Nil)         = Nil&#xA;(Sort (Cons x xs)) = (Insert x (Sort xs))&#xA;&#xA;// Insert : U60 -&amp;gt; List -&amp;gt; List&#xA;(Insert v Nil)         = (Cons v Nil)&#xA;(Insert v (Cons x xs)) = (SwapGT (&amp;gt; v x) v x xs)&#xA;&#xA;// SwapGT : U60 -&amp;gt; U60 -&amp;gt; U60 -&amp;gt; List -&amp;gt; List&#xA;(SwapGT 0 v x xs) = (Cons v (Cons x xs))&#xA;(SwapGT 1 v x xs) = (Cons x (Insert v xs))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;sort&#39; :: List -&amp;gt; List&#xA;sort&#39; Nil         = Nil&#xA;sort&#39; (Cons x xs) = insert x (sort&#39; xs)&#xA;&#xA;insert :: Word64 -&amp;gt; List -&amp;gt; List&#xA;insert v Nil         = Cons v Nil&#xA;insert v (Cons x xs) = swapGT (if v &amp;gt; x then 1 else 0) v x xs&#xA;&#xA;swapGT :: Word64 -&amp;gt; Word64 -&amp;gt; Word64 -&amp;gt; List -&amp;gt; List&#xA;swapGT 0 v x xs = Cons v (Cons x xs)&#xA;swapGT 1 v x xs = Cons x (insert v xs)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/bench/_results_/sort-bubble.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;On this example, we run a simple, recursive &lt;a href=&#34;https://en.wikipedia.org/wiki/Bubble_sort&#34;&gt;Bubble Sort&lt;/a&gt; on both HVM and GHC (Haskell&#39;s compiler). Notice the algorithms are identical. The chart shows how much time each runtime took to sort a list of given size (the lower, the better). The purple line shows GHC (single-thread), the green lines show HVM (1, 2, 4 and 8 threads). As you can see, both perform similarly, with HVM having a small edge. Sadly, here, its performance doesn&#39;t improve with added cores. That&#39;s because Bubble Sort is an &lt;em&gt;inherently sequential&lt;/em&gt; algorithm, so HVM can&#39;t improve it.&lt;/p&gt; &#xA;&lt;h2&gt;Radix Sort&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;From: &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/examples/sort/radix/main.hvm&#34;&gt;HVM/examples/sort/radix/main.hvm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;From: &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/examples/sort/radix/main.hs&#34;&gt;HVM/examples/sort/radix/main.hs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// Sort : Arr -&amp;gt; Arr&#xA;(Sort t) = (ToArr 0 (ToMap t))&#xA;&#xA;// ToMap : Arr -&amp;gt; Map&#xA;(ToMap Null)       = Free&#xA;(ToMap (Leaf a))   = (Radix a)&#xA;(ToMap (Node a b)) =&#xA;  (Merge (ToMap a) (ToMap b))&#xA;&#xA;// ToArr : Map -&amp;gt; Arr&#xA;(ToArr x Free) = Null&#xA;(ToArr x Used) = (Leaf x)&#xA;(ToArr x (Both a b)) =&#xA;  let a = (ToArr (+ (* x 2) 0) a)&#xA;  let b = (ToArr (+ (* x 2) 1) b)&#xA;  (Node a b)&#xA;&#xA;// Merge : Map -&amp;gt; Map -&amp;gt; Map&#xA;(Merge Free       Free)       = Free&#xA;(Merge Free       Used)       = Used&#xA;(Merge Used       Free)       = Used&#xA;(Merge Used       Used)       = Used&#xA;(Merge Free       (Both c d)) = (Both c d)&#xA;(Merge (Both a b) Free)       = (Both a b)&#xA;(Merge (Both a b) (Both c d)) =&#xA;  (Both (Merge a c) (Merge b d))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;sort :: Arr -&amp;gt; Arr&#xA;sort t = toArr 0 (toMap t)&#xA;&#xA;toMap :: Arr -&amp;gt; Map&#xA;toMap Null       = Free&#xA;toMap (Leaf a)   = radix a&#xA;toMap (Node a b) =&#xA;  merge (toMap a) (toMap b)&#xA;&#xA;toArr :: Word64 -&amp;gt; Map -&amp;gt; Arr&#xA;toArr x Free       = Null&#xA;toArr x Used       = Leaf x&#xA;toArr x (Both a b) =&#xA;  let a&#39; = toArr (x * 2 + 0) a&#xA;      b&#39; = toArr (x * 2 + 1) b&#xA;  in Node a&#39; b&#39;&#xA;&#xA;merge :: Map -&amp;gt; Map -&amp;gt; Map&#xA;merge Free       Free       = Free&#xA;merge Free       Used       = Used&#xA;merge Used       Free       = Used&#xA;merge Used       Used       = Used&#xA;merge Free       (Both c d) = (Both c d)&#xA;merge (Both a b) Free       = (Both a b)&#xA;merge (Both a b) (Both c d) =&#xA;  (Both (merge a c) (merge b d))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/bench/_results_/sort-radix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;On this example, we try a &lt;a href=&#34;https://en.wikipedia.org/wiki/Radix_sort&#34;&gt;Radix Sort&lt;/a&gt;, based on merging immutable trees. In this test, for now, single-thread performance was superior on GHC - and this is often the case, since GHC is much older and has astronomically more micro-optimizations - yet, since this algorithm is &lt;em&gt;inherently parallel&lt;/em&gt;, HVM was able to outperform GHC given enough cores. With &lt;strong&gt;8 threads&lt;/strong&gt;, HVM sorted a large list &lt;strong&gt;2.5x faster&lt;/strong&gt; than GHC.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind one could parallelize the Haskell version with &lt;code&gt;par&lt;/code&gt; annotations, but that would demand time-consuming, expensive refactoring - and, in some cases, it isn&#39;t even &lt;em&gt;possible&lt;/em&gt; to use all the available parallelism with &lt;code&gt;par&lt;/code&gt; alone. HVM, on the other hands, will automatically distribute parallel workloads through all available cores, achieving horizontal scalability. As HVM matures, the single-thread gap will decrease significantly.&lt;/p&gt; &#xA;&lt;h2&gt;Lambda Multiplication&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;From: &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/examples/lambda/multiplication/main.hvm&#34;&gt;HVM/examples/lambda/multiplication/main.hvm &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;From: &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/examples/lambda/multiplication/main.hs&#34;&gt;HVM/examples/lambda/multiplication/main.hs &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// Increments a Bits by 1&#xA;// Inc : Bits -&amp;gt; Bits&#xA;(Inc xs) = Œªex Œªox Œªix&#xA;  let e = ex&#xA;  let o = ix&#xA;  let i = Œªp (ox (Inc p))&#xA;  (xs e o i)&#xA;&#xA;// Adds two Bits&#xA;// Add : Bits -&amp;gt; Bits -&amp;gt; Bits&#xA;(Add xs ys) = (App xs Œªx(Inc x) ys)&#xA;&#xA;// Multiplies two Bits&#xA;// Mul : Bits -&amp;gt; Bits -&amp;gt; Bits&#xA;(Mul xs ys) =&#xA;  let e = End&#xA;  let o = Œªp (B0 (Mul p ys))&#xA;  let i = Œªp (Add ys (B0 (Mul p ys)))&#xA;  (xs e o i)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-haskell&#34;&gt;-- Increments a Bits by 1&#xA;inc :: Bits -&amp;gt; Bits&#xA;inc xs = Bits $ \ex -&amp;gt; \ox -&amp;gt; \ix -&amp;gt;&#xA;  let e = ex&#xA;      o = ix&#xA;      i = \p -&amp;gt; ox (inc p)&#xA;  in get xs e o i&#xA;&#xA;-- Adds two Bits&#xA;add :: Bits -&amp;gt; Bits -&amp;gt; Bits&#xA;add xs ys = app xs (\x -&amp;gt; inc x) ys&#xA;&#xA;-- Muls two Bits&#xA;mul :: Bits -&amp;gt; Bits -&amp;gt; Bits&#xA;mul xs ys =&#xA;  let e = end&#xA;      o = \p -&amp;gt; b0 (mul p ys)&#xA;      i = \p -&amp;gt; add ys (b0 (mul p ys))&#xA;  in get xs e o i&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/bench/_results_/lambda-multiplication.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This example implements bitwise multiplication using &lt;a href=&#34;https://en.wikipedia.org/wiki/Church_encoding&#34;&gt;Œª-encodings&lt;/a&gt;. Its purpose is to show yet another important advantage of HVM: beta-optimality. This chart isn&#39;t wrong: HVM multiplies Œª-encoded numbers &lt;strong&gt;exponentially faster&lt;/strong&gt; than GHC, since it can deal with very higher-order programs with optimal asymptotics, while GHC can not. As esoteric as this technique may look, it can actually be very useful to design efficient functional algorithms. One application, for example, is to implement &lt;a href=&#34;https://github.com/Kindelia/HVM/issues/167#issuecomment-1314665474&#34;&gt;runtime deforestation&lt;/a&gt; for immutable datatypes. In general, HVM is capable of applying any fusible function &lt;code&gt;2^n&lt;/code&gt; times in linear time, which sounds impossible, but is indeed true.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Charts made on &lt;a href=&#34;https://chart-studio.plotly.com/&#34;&gt;plotly.com&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Rust nightly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;rustup toolchain install nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install HVM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cargo +nightly install hvm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run an HVM expression:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;hvm run &#34;(@x(+ x 1) 41)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;That&#39;s it! For more advanced usage, check the &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/guide/README.md&#34;&gt;complete guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To learn more about the &lt;strong&gt;underlying tech&lt;/strong&gt;, check &lt;a href=&#34;https://raw.githubusercontent.com/HigherOrderCO/HVM/master/guide/HOW.md&#34;&gt;guide/HOW.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To ask questions and &lt;strong&gt;join our community&lt;/strong&gt;, check our &lt;a href=&#34;https://discord.gg/kindelia&#34;&gt;Discord Server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To &lt;strong&gt;contact the author&lt;/strong&gt; directly, send an email to &lt;a href=&#34;mailto:taelin@higherorderco.org&#34;&gt;taelin@higherorderco.org&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h3&gt;Is HVM faster than GHC in a single core today?&lt;/h3&gt; &#xA;&lt;p&gt;No. For now, HVM seems to be from 50% faster to 3x slower in single thread performance, to even worse if the Haskell code exploits optimizations that HVM doesn&#39;t have yet (ST Monad, mutable arrays, inlining, loops).&lt;/p&gt; &#xA;&lt;h3&gt;Is HVM faster than Rust today?&lt;/h3&gt; &#xA;&lt;p&gt;No.&lt;/p&gt; &#xA;&lt;h3&gt;Is HVM faster than C today?&lt;/h3&gt; &#xA;&lt;p&gt;No!&lt;/p&gt; &#xA;&lt;h3&gt;Can HVM be faster than these one day?&lt;/h3&gt; &#xA;&lt;p&gt;Hard question. Perhaps! The underlying model is very efficient. HVM shares the same initial core as Rust (an affine Œª-calculus), has great memory management (no thunks, no garbage-collection). Some people think interaction nets are an overhead, but that&#39;s not the case - they&#39;re the &lt;em&gt;lack&lt;/em&gt; of overhead. For example, a lambda on HVM uses only 2 64-bit pointers, which is about as lightweight as it gets. Furthermore, every reduction rule of HVM is a lightweight, constant-time operation that can be compiled to very fast machine code. As such, given enough optimizations, from proper inlining, to real loops, to inner mutability (FBIP-like?), I believe HVM could one day compare to GHC and even Rust or C. But we&#39;re still far from that.&lt;/p&gt; &#xA;&lt;h3&gt;Why do the benchmarks compare single-thread vs multi-core?&lt;/h3&gt; &#xA;&lt;p&gt;They do not! Notice all benchmarks include a line for single-threaded HVM execution, which is usually 3x slower than GHC. We do include multi-core HVM execution to let us visualize how its performance scales with added cores, without any change of the code. We do not include multi-core GHC execution because GHC doesn&#39;t support automatic parallelism, so it is not possible to make use of threads without changing the code. Keep in mind, once again, the benchmarks are NOT claiming that HVM is faster than GHC today.&lt;/p&gt; &#xA;&lt;h3&gt;Does HVM support the full Œª-Calculus, or System-F?&lt;/h3&gt; &#xA;&lt;p&gt;Not yet! HVM is an impementation of the bookkeeping-free version of the reduction algorithm proposed on &lt;a href=&#34;https://www.researchgate.net/publication/235778993_The_optimal_implementation_of_functional_programming_languages&#34;&gt;TOIOFPL&lt;/a&gt; book, up to page 40. As such, it doesn&#39;t support some Œª-terms, such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(Œªx.(x x) Œªf.Œªx.(f (f x)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;HVM is, though, Turing complete, so you could implement a full Œª-calculus interpreter on it - that limitation only addresses built-in closures. Keep in mind many popular languages don&#39;t include the full Œª-calculus closures either; Rust, for example, covers a very restricted subset, due to the borrow system. That said, HVM covers a wide class of Œª-terms, including the the Y-combinator, church encodings (even algorithms like addition, multiplication and exponentiation), as well as arbitrary datatypes (both native and scott encoded) and recursion.&lt;/p&gt; &#xA;&lt;h3&gt;Will HVM support the full Œª-Calculus, or System-F?&lt;/h3&gt; &#xA;&lt;p&gt;Yes! We plan to, by implementing the full algorithm described on the &lt;a href=&#34;https://www.researchgate.net/publication/235778993_The_optimal_implementation_of_functional_programming_languages&#34;&gt;TOIOFPL&lt;/a&gt;, i.e., after page 40. Sadly, this results in an overhead that affects the performance of beta-reduction by about 10x. As such, we want to do so with caution to keep HVM efficient. Currently, the plan is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Split lambdas into full-lambdas and light-lambdas&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Light lambdas are what HVM has today. They&#39;re fast, but don&#39;t support the full Œª-Calculus.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Full lambdas will be slower, but support the full Œª-Calculus, via &#34;internal brackets/croissants&#34;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To decrease the overhead, convert full-lambdas to light-lambdas using EAL inference&lt;/p&gt; &lt;p&gt;Elementary Affine Logic is a substructural logic that rejects the structural rule of contraction, replacing it by a controlled form of duplication. By extending HVM with EAL inference, we&#39;ll be able to convert most full-lambdas into lightweight lambdas, greatly reducing the associated slowdown.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, keep in mind this only concerns lambdas. Low-order terms (constructors, trees, recursion) aren&#39;t affected.&lt;/p&gt; &#xA;&lt;h3&gt;Are unsupported terms &#34;Undefined Behavior&#34;?&lt;/h3&gt; &#xA;&lt;p&gt;No! Unsupported Œª-terms like &lt;code&gt;Œªx.(x x) Œªf.Œªx.(f (f x))&lt;/code&gt; don&#39;t cause HVM to display undefined behavior. HVM will always behave deterministically, and give you a correct result to any input, except it will be in terms of &lt;a href=&#34;https://github.com/Kindelia/Wikind/raw/master/IC/_.kind2&#34;&gt;Interaction Calculus&lt;/a&gt; (IC) semantics. The IC is an alternative to the Lambda Calculus (LC) which differs slightly in how non-linear variables are treated. As such, these &#34;unsupported&#34; terms are just cases where the LC and the IC evaluation disagree. In theory, you could use the HVM as a Interaction Net runtime, and it would always give you perfectly correct answers under these semantics - but that&#39;s not usual, so we don&#39;t talk about it often.&lt;/p&gt; &#xA;&lt;h3&gt;What is HVM&#39;s main innovation, in simple terms?&lt;/h3&gt; &#xA;&lt;p&gt;In complex terms, HVM&#39;s main innovation is that it is an efficient implementation of the Interaction Net, which is a concurrent model of computation. But there is a way to translate it to more familiar terms. HVM&#39;s performance, parallelism and GC-freedom all come from the fact it is based on a linear core - just like Rust! But, on top of it, instead of adding loops and references (plus a &#34;borrow checker&#34;), HVM adds recursion and a &lt;em&gt;lazy, incremental cloning primitive&lt;/em&gt;. For example, the expression below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let xs = (Cons 1 (Cons 2 (Cons 3 Nil))) in [xs, xs]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Computes to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let xs = (Cons 2 (Cons 3 Nil)) in [(Cons 1 xs), (Cons 1 xs)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice the first &lt;code&gt;Cons 1&lt;/code&gt; layer was cloned incrementally. This makes cloning essentially free, for the same reason Haskell&#39;s lazy evaluator allows you to make infinite lists: there is no cost until you actually read the copy! That lazy-cloning primitive is pervasive, and covers all primitives of HVM&#39;s runtime: constructors, numbers and lambdas. This idea, though, breaks down for lambdas: how do you incrementally copy a lambda?&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let f = Œªx. (2 + x) in [f, f]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you try it, you&#39;ll realize why that&#39;s not possible:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let f = (2 + x) in [Œªx. f, Œªx. f]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The solution to that question is the main insight that the Interaction Net model brought to the table, and it is described in more details on the &lt;a href=&#34;https://github.com/Kindelia/HVM/raw/master/guide/HOW.md&#34;&gt;HOW.md&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;h3&gt;Is HVM always &lt;em&gt;asymptotically&lt;/em&gt; faster than GHC?&lt;/h3&gt; &#xA;&lt;p&gt;No. In most common cases, it will have the same asymptotics. In some cases, it is exponentially faster. In &lt;a href=&#34;https://github.com/Kindelia/HVM/issues/60&#34;&gt;this issue&lt;/a&gt;, a user noticed that HVM displays quadratic asymptotics for certain functions that GHC computes in linear time. That was a surprise to me, and, as far as I can tell, despite the &#34;optimal&#34; brand, seems to be a limitation of the underlying theory. That said, there are multiple ways to alleviate, or solve, this problem. One approach would be to implement &#34;safe pointers&#34;, also described on the book, which would reduce the cloning overhead and make some quadratic cases linear. But that wouldn&#39;t work for all cases. A complimentary approach would be to do linearity analysis, converting problematic quadratic programs in faster, linear versions. Finally, in the worst case, we could add references just like Haskell, but that should be made with a lot of caution, in order not to break the assumptions made by the parallel execution engine. For a more in depth explanation, check &lt;a href=&#34;https://news.ycombinator.com/edit?id=35342297&#34;&gt;read comment on Hacker News&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Is HVM&#39;s optimality only relevant for weird, academic Œª-encoded terms?&lt;/h3&gt; &#xA;&lt;p&gt;No. HVM&#39;s optimality has some very practical benefits. For example, all the &#34;deforesting&#34; techniques that Haskell employs as compile-time rewrite rules, happen naturally, at runtime, on the HVM. For example, Haskell optimizes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;map f . map g&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Into:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;map (f . g)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a hardcoded optimization. On HVM, that occurs naturally, at runtime, in a very general and pervasive way. So, for example, if you have something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;foldr (.) id funcs :: [Int -&amp;gt; Int]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;GHC won&#39;t be able to &#34;fuse&#34; the functions on the &lt;code&gt;funcs&lt;/code&gt; list, since they&#39;re not known at compile time. HVM will do that just fine. See &lt;a href=&#34;https://github.com/Kindelia/HVM/issues/167&#34;&gt;this issue&lt;/a&gt; for a practical example.&lt;/p&gt; &#xA;&lt;p&gt;Another practical application for Œª-encodings is for monads. On Haskell, the Free Monad library uses Church encodings as an important optimization. Without it, the asymptotics of binding make free monads much less practical. HVM has optimal asymptotics for Church encoded data, making it great for these problems.&lt;/p&gt; &#xA;&lt;h3&gt;Why is HVM so parallelizable?&lt;/h3&gt; &#xA;&lt;p&gt;Because it is fully linear: every piece of data only occurs in one place at the same time, which reduces need for synchronization. Furthermore, it is pure, so there are no global side effects that demand communication. Because of that, reducing HVM expressions in parallel is actually quite simple: we just keep a work strealing queue of redexes, and let a pool of threads computing them. That said, there are two places where HVM needs synchronization:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;On dup nodes, used by lazy cloning: a lock is needed to prevent threads from passing through, and, thus, accessing the same data&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On the substitution operation: that&#39;s because substitution could send data from one thread to another, so it must be done atomically&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In theory, Haskell could be parallelized too, and GHC devs tried it at a point, but I believe the non-linearity of the STG model would make the problem much more complex than it is for the HVM, making it hard to not lose too much performance due to synchronization overhead.&lt;/p&gt; &#xA;&lt;h3&gt;How is the memory footprint of HVM, compared to other runtimes?&lt;/h3&gt; &#xA;&lt;p&gt;It is a common misconception that an &#34;interactional&#34; runtime would somehow consume more memory than a &#34;procedural&#34; runtime like Rust&#39;s. That&#39;s not the case. Interaction nets, as implemented on HVM, add no overhead, and HVM instantly collects any piece of data that becomes unreachable, just like Rust, so there are no accumulating thunks that result in world-stopping garbage collection, as happens in Haskell currently.&lt;/p&gt; &#xA;&lt;p&gt;That said, currently, HVM doesn&#39;t implement memory-efficient features like references, loops and local mutability. As such, to do anything on HVM today, you need to use immutable datatypes and recursion, which are naturally memory-hungry. Thus, HVM programs today will have increased memory footprint, in relation to C and Rust programs. Thankfully, there is no theoretical limitation preventing us from adding loops and local mutability, and, once/if we do, one can expect the same memory footprint as Rust. The only caveat, though, is shared references: we&#39;re not sure if we want to add these, as they might impact parallelism. As such, it is possible that we choose to let lazy clones to be the only form of non-linearity, which would preserve parallelism, at the cost of making some algorithms more memory-hungry.&lt;/p&gt; &#xA;&lt;h3&gt;Is HVM meant to replace GHC?&lt;/h3&gt; &#xA;&lt;p&gt;No! GHC is actually a superb, glorious runtime that is very hard to match. HVM is meant to be a lightweight, massively parallel runtime for functional, and even imperative, languages, from Elm to JavaScript. That said, we do want to support Haskell, but that will require HVM being in a much later stage of maturity, as well as provide support for full lambdas, which it doesn&#39;t do yet. Once we do, HVM could be a great alternative for GHC, giving the Haskell community an option to run it in a runtime with automatic parallelism, no slow garbage-collector and beta-optimality. Which will be the best option will likely depend on the type of application you&#39;re compiling, but having more choices is generally good and, as such, HVM can be a great tool for the Haskell community.&lt;/p&gt; &#xA;&lt;h3&gt;Is HVM production-ready?&lt;/h3&gt; &#xA;&lt;p&gt;No. HVM is still to be considered a prototype. Right now, I had less than 3 months to work on it directly. It is considerably less mature than other compilers and runtimes like GHC and V8. That said, we&#39;re raising funds to have a proper team of engineers working on the HVM. If all goes well, we can expect a production-ready release by Q1 2024.&lt;/p&gt; &#xA;&lt;h3&gt;I&#39;ve ran an HVM program and it consumed 1950 GB and my computer exploded.&lt;/h3&gt; &#xA;&lt;p&gt;HVM is a prototype. Bugs are expected. Please, open an issue!&lt;/p&gt; &#xA;&lt;h3&gt;I&#39;ve used HVM in production and now my company is bankrupt.&lt;/h3&gt; &#xA;&lt;p&gt;I quit.&lt;/p&gt; &#xA;&lt;h1&gt;Related Work&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/inpla/inpla&#34;&gt;Inpla&lt;/a&gt; - a pure interaction net framework, without the &#34;functional/calculus&#34; style of HVM&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>cloudquery/pg_gpt</title>
    <updated>2023-03-31T01:43:58Z</updated>
    <id>tag:github.com,2023-03-31:/cloudquery/pg_gpt</id>
    <link href="https://github.com/cloudquery/pg_gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experimental extension that brings OpenAI API to your PostgreSQL to run queries in human language.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Postgres &amp;lt;&amp;gt; ChatGPT&lt;/h1&gt; &#xA;&lt;p&gt;Experimental PostgreSQL extension that enables the use of OpenAI GPT API inside PostgreSQL, allowing for queries to be written using natural language.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1121616/228234378-40c796d3-0a38-465a-92da-9370fb21b93b.mp4&#34;&gt;https://user-images.githubusercontent.com/1121616/228234378-40c796d3-0a38-465a-92da-9370fb21b93b.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(This demo uses data from the &lt;a href=&#34;https://www.cloudquery.io/integrations/hackernews/postgresql&#34;&gt;Hacker News&lt;/a&gt; and &lt;a href=&#34;https://www.cloudquery.io/integrations/azure/postgresql&#34;&gt;Azure&lt;/a&gt; CloudQuery plugins)&lt;/p&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;p&gt;The extension sends a subset of the database schema to ChatGPT and asks it to generate a query based on this and the user input.&lt;/p&gt; &#xA;&lt;h2&gt;Before you start&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: This plugins sends schema (without the data) to OpenAI GPT API, so it is not recommended to use it on production databases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: This is an experimental plugin and not officially supported by CloudQuery.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Requires &lt;a href=&#34;https://github.com/tcdi/pgx&#34;&gt;pgx&lt;/a&gt;. Install this first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo install --locked cargo-pgx&#xA;cargo pgx init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can install the extension:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cloudquery/pg_gpt&#xA;cd pg_gpt&#xA;export OPENAI_KEY=&amp;lt;YOUR_KEY&amp;gt;&#xA;cargo pgx run&#xA;# will drop into psql shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create extension pg_gpt;&#xA;set openai.key = &#39;&amp;lt;YOUR OPENAPI API KEY HERE&amp;gt;&#39;; -- set your key&#xA;select gpt(&#39;show me all open aws s3 buckets&#39;);&#xA;-- will output the following query, so you can execute it&#xA;-- select * from aws_s3_bucket;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt(text)&lt;/code&gt; - Generates a query based on the user input and the full database schema. This works fine for databases with small schemas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt_tables(table_pattern, text)&lt;/code&gt; - Similar to gpt, but only uses the tables that match the pattern. The pattern is passed to a &lt;code&gt;table_name LIKE&lt;/code&gt; query, so &lt;code&gt;%&lt;/code&gt; can be used as wildcard.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installing the extension on an existing Postgres instance&lt;/h2&gt; &#xA;&lt;p&gt;First run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo pgx install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This places the extension in the postgres extensions directory. Then, in your postgres instance, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create extension pg_gpt;&#xA;set openai.key = &#39;&amp;lt;YOUR OPENAPI API KEY HERE&amp;gt;&#39;;&#xA;-- proceed to use the extension&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Schema Size - Currently we use gpt-3.5-turbo, which is limited to 4096 tokens. Use &lt;code&gt;gpt_tables&lt;/code&gt; to narrow down the set of tables.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>