<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-22T01:33:42Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>firstbatchxyz/dkn-compute-node</title>
    <updated>2024-08-22T01:33:42Z</updated>
    <id>tag:github.com,2024-08-22:/firstbatchxyz/dkn-compute-node</id>
    <link href="https://github.com/firstbatchxyz/dkn-compute-node" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Compute Node of Dria Knowledge Network.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/firstbatchxyz/dria-js-client/master/logo.svg?sanitize=true&#34; alt=&#34;logo&#34; width=&#34;142&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt; Dria Compute Node &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;Dria Compute Node serves the computation results within Dria Knowledge Network.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://opensource.org/license/apache-2-0&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;License: Apache-2.0&#34; src=&#34;https://img.shields.io/badge/license-Apache%202.0-7CB9E8.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/firstbatchxyz/dkn-compute-node/master/.github/workflows/test.yml&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Workflow: Tests&#34; src=&#34;https://github.com/firstbatchxyz/dkn-compute-node/actions/workflows/tests.yml/badge.svg?branch=master&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/repository/docker/firstbatch/dkn-compute-node/general&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Docker Version&#34; src=&#34;https://img.shields.io/docker/v/firstbatch/dkn-compute-node?logo=Docker&amp;amp;label=image&amp;amp;color=2496ED&amp;amp;sort=semver&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/dria&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://dcbadge.vercel.app/api/server/dria?style=flat&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;strong&gt;Dria Compute Node&lt;/strong&gt; is a unit of computation within the Dria Knowledge Network. It&#39;s purpose is to process tasks given by the &lt;strong&gt;Dria Admin Node&lt;/strong&gt;, and receive rewards for providing correct results.&lt;/p&gt; &#xA;&lt;p&gt;To get started, &lt;a href=&#34;https://raw.githubusercontent.com/firstbatchxyz/dkn-compute-node/master/#setup&#34;&gt;setup&lt;/a&gt; your envrionment and then see &lt;a href=&#34;https://raw.githubusercontent.com/firstbatchxyz/dkn-compute-node/master/#usage&#34;&gt;usage&lt;/a&gt; to run the node.&lt;/p&gt; &#xA;&lt;h3&gt;Tasks&lt;/h3&gt; &#xA;&lt;p&gt;Compute nodes can technically do any arbitrary task, from computing the square root of a given number to finding LLM outputs from a given prompt, or validating an LLM&#39;s output with respect to knowledge available on the web accessed via tools.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ping/Pong&lt;/strong&gt;: Dria Admin Node broadcasts &lt;strong&gt;ping&lt;/strong&gt; messages at a set interval, it is a required duty of the compute node to respond with a &lt;strong&gt;pong&lt;/strong&gt; to these so that they can be included in the list of available nodes for task assignment. These tasks will respect the type of model provided within the pong message, e.g. if a task requires &lt;code&gt;gpt-4o&lt;/code&gt; and you are running &lt;code&gt;phi3&lt;/code&gt;, you won&#39;t be selected for that task.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Workflows&lt;/strong&gt;: Each task is given in the form of a workflow, based on &lt;a href=&#34;https://github.com/andthattoo/ollama-workflows&#34;&gt;Ollama Workflows&lt;/a&gt; (see repository for more information). In simple terms, each workflow defines the agentic behavior of an LLM, all captured in a single JSON file, and can represent things ranging from simple LLM generations to iterative web searching.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The compute node is a very lightweight process, with few MBs of memory usage along with an image size of less than ~65MBs. If you are using Ollama, you will need the memory to run large models locally, which depend on the model&#39;s size that you are willing to.&lt;/p&gt; &#xA;&lt;p&gt;You need the following applications to run compute node:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Git&lt;/strong&gt;: We will use &lt;code&gt;git&lt;/code&gt; to clone the repository from GitHub, and pull latest changes for updates later.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Our services will make use of Docker so that the node can run on any machine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;You can check if you have these via:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;which git&#xA;which docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;To be able to run a node, we need to make a few simple preparations. Follow the steps below one by one.&lt;/p&gt; &#xA;&lt;h3&gt;1. Clone the repository&lt;/h3&gt; &#xA;&lt;p&gt;This repository has the necessary setup to run the node, so start by cloning it using the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/firstbatchxyz/dkn-compute-node&#xA;cd dkn-compute-node&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Prepare Environment Variables&lt;/h3&gt; &#xA;&lt;p&gt;Dria Compute Node makes use of several environment variables. Create a &lt;code&gt;.env&lt;/code&gt; file, and copy the environment variables as given in &lt;a href=&#34;https://raw.githubusercontent.com/firstbatchxyz/dkn-compute-node/master/.env.example&#34;&gt;.env.example&lt;/a&gt;. We will fill out the missing parts in a moment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;DKN_ADMIN_PUBLIC_KEY&lt;/code&gt; is used to verify that the tasks are given by certain nodes, so that your node does not work for tasks given to the network by untrusted people. You don&#39;t need to change this, simply copy and paste it to your &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;While adding anything to your &lt;code&gt;.env&lt;/code&gt;, you can do it without leaving the terminal. For example, suppose you want to set &lt;code&gt;VALUE&lt;/code&gt; to some &lt;code&gt;KEY&lt;/code&gt;, you can do it as:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#34;KEY=VALUE&#34; &amp;gt;&amp;gt; .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you would like to view the &lt;code&gt;.env&lt;/code&gt; without leaving the terminal, you can do:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cat .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3. Prepare Ethereum Wallet&lt;/h3&gt; &#xA;&lt;p&gt;Dria makes use of the same Ethereum wallet, that is the recipient of your hard-earned rewards! Place your private key at &lt;code&gt;DKN_WALLET_SECRET_KEY&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; without the 0x prefix. It should look something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;DKN_WALLET_SECRET_KEY=ac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION]&lt;/p&gt; &#xA; &lt;p&gt;Always make sure your private key is within the .gitignore&#39;d &lt;code&gt;.env&lt;/code&gt; file, nowhere else! To be even safer, you can use a throwaway wallet, you can always transfer your rewards to a main wallet afterwards.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;4. Setup LLM Provider&lt;/h3&gt; &#xA;&lt;p&gt;For the final step, we need to make sure we can serve LLM requests.&lt;/p&gt; &#xA;&lt;h4&gt;For OpenAI&lt;/h4&gt; &#xA;&lt;p&gt;If you will be using OpenAI to serve its models, you need to have an API key in the environment. Simply set the key within your &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;OPENAI_API_KEY=&amp;lt;YOUR_KEY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For Ollama&lt;/h4&gt; &#xA;&lt;p&gt;Of course, first you have to install Ollama; see their &lt;a href=&#34;https://ollama.com/download&#34;&gt;download page&lt;/a&gt;. Then, you must &lt;strong&gt;first pull a small embedding model that is used internally&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ollama pull hellord/mxbai-embed-large-v1:f16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the models that you choose (see list of models just below &lt;a href=&#34;https://raw.githubusercontent.com/firstbatchxyz/dkn-compute-node/master/#1-choose-models&#34;&gt;here&lt;/a&gt;) you can download them with same command. Note that if your model size is large, pulling them may take a while.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# example for phi3:3.8b&#xA;ollama pull phi3:3.8b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;Alternatively, you can set &lt;code&gt;OLLAMA_AUTO_PULL=true&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; so that the compute node will always download the missing models for you.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Optional Services&lt;/h4&gt; &#xA;&lt;p&gt;Based on presence of API keys, &lt;a href=&#34;https://github.com/andthattoo/ollama-workflows/&#34;&gt;Ollama Workflows&lt;/a&gt; may use more superior services instead of free alternatives, e.g. &lt;a href=&#34;https://serper.dev/&#34;&gt;Serper&lt;/a&gt; instead of &lt;a href=&#34;https://duckduckgo.com/&#34;&gt;DuckDuckGo&lt;/a&gt; or &lt;a href=&#34;https://jina.ai/&#34;&gt;Jina&lt;/a&gt; without rate-limit instead of with rate-limit. Add these within your &lt;code&gt;.env&lt;/code&gt; as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;SERPER_API_KEY=&amp;lt;key-here&amp;gt;&#xA;JINA_API_KEY=&amp;lt;key-here&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;With all setup steps above completed, we are ready to start a node!&lt;/p&gt; &#xA;&lt;h3&gt;1. Choose Model(s)&lt;/h3&gt; &#xA;&lt;p&gt;Based on the resources of your machine, you must decide which models that you will be running locally. For example, you can use OpenAI with their models, not running anything locally at all; or you can use Ollama with several models loaded to disk, and only one loaded to memory during its respective task. Available models (see &lt;a href=&#34;https://github.com/andthattoo/ollama-workflows/raw/main/src/program/atomics.rs#L269&#34;&gt;here&lt;/a&gt; for latest) are:&lt;/p&gt; &#xA;&lt;h4&gt;Ollama Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;adrienbrault/nous-hermes2theta-llama3-8b:q8_0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi3:14b-medium-4k-instruct-q4_1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi3:14b-medium-128k-instruct-q4_1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi3:3.8b&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llama3.1:latest&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;OpenAI Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt-3.5-turbo&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt-4-turbo&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt-4o-mini&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;If you are using Ollama, make sure you have pulled the required models, as specified in the &lt;a href=&#34;https://raw.githubusercontent.com/firstbatchxyz/dkn-compute-node/master/#4-setup-ollama-for-ollama-users&#34;&gt;section above&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. Start Docker&lt;/h3&gt; &#xA;&lt;p&gt;Our node will be running within a Docker container, so we should make sure that Docker is running before the next step. You can launch Docker via its &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;desktop application&lt;/a&gt;, or a command such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo systemctl start docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;You don&#39;t need to do this step if Docker is already running in the background.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;3. Run Node&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s time to run our compute node. We have a starter script that makes this much easier, you can see available commadns with:&lt;/p&gt; &#xA;&lt;p&gt;See the available commands with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;chmod +x start.sh&#xA;./start.sh --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Simply run the script with the model names provided, such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./start.sh -m=llama3.1:latest -m=gpt-3.5-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start script will run the containers in the background. You can check their logs either via the terminal or from &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker Desktop&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Running in Debug Mode&lt;/h4&gt; &#xA;&lt;p&gt;To print DEBUG-level logs for the compute node, you can add &lt;code&gt;--dev&lt;/code&gt; argument to the start script. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./start.sh -m=gpt-4o-mini --dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Running in debug mode will also allow you to see behind the scenes of Ollama Workflows, i.e. you can see the reasoning of the LLM as it executes the task.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Similarly, you can run in trace mode with &lt;code&gt;--trace&lt;/code&gt; to see trace logs, which cover low-level logs from the p2p client.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;4. Looking at Logs&lt;/h3&gt; &#xA;&lt;p&gt;To see your logs, you can go to &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker Desktop&lt;/a&gt; and see the running containers and find &lt;code&gt;dkn-compute-node&lt;/code&gt;. There, open the containers within the compose (click on &lt;code&gt;&amp;gt;&lt;/code&gt; to the left) and click on any of the container to see its logs.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can use &lt;code&gt;docker compose logs&lt;/code&gt; such as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker compose logs -f compute  # compute node logs&#xA;docker compose logs -f ollama   # ollama logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; option is so that you can track the logs from terminal. If you prefer to simply check the latest logs, you can use a command such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# logs from last 1 hour&#xA;docker compose logs --since=1h compute&#xA;&#xA;# logs from last 30 minutes&#xA;docker compose logs --since=30m compute&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Stopping the Node&lt;/h3&gt; &#xA;&lt;p&gt;When you start your node with &lt;code&gt;./start.sh&lt;/code&gt;, it will wait for you in the same terminal to do CTRL+C before stopping. Once you do that, the containers will be stopped and removed. You can also kill the containers manually, doing CTRL+C afterwards will do nothing in such a case.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;Sometimes it may not immediately exit whilst executing a task, if you REALLY need to quite the process you can kill it manually.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Using Ollama&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you don&#39;t have Ollama installed, you can ignore this section.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you have Ollama installed already (e.g. via &lt;code&gt;brew install ollama&lt;/code&gt;) then you must indicate that you will be using that Ollama, instead of a Docker container. To do this, we set the provide the argument &lt;code&gt;--local-ollama=true&lt;/code&gt; which is &lt;code&gt;true&lt;/code&gt; by default. With this, the compute node will use the Ollama server on your machine, instead of a Docker container.&lt;/p&gt; &#xA;&lt;p&gt;If the Ollama server is not running, the start script will initiate it with &lt;code&gt;ollama serve&lt;/code&gt; and terminate it when the node is being stopped.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If &lt;code&gt;--local-ollama=false&lt;/code&gt; or the local Ollama server is reachable, the compute node will use a Docker Compose service for it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;There are three Docker Compose Ollama options: &lt;code&gt;ollama-cpu&lt;/code&gt;, &lt;code&gt;ollama-cuda&lt;/code&gt;, and &lt;code&gt;ollama-rocm&lt;/code&gt;. The start script will decide which option to use based on the host machine&#39;s GPU specifications.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run with local ollama&#xA;./start.sh -m=phi3 --local-ollama=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional Static Nodes&lt;/h3&gt; &#xA;&lt;p&gt;You can add additional relay nodes &amp;amp; bootstrap nodes from environment, using the &lt;code&gt;DKN_RELAY_NODES&lt;/code&gt; and &lt;code&gt;DKN_BOOTSTRAP_NODES&lt;/code&gt; variables respectively. Simply write the &lt;code&gt;Multiaddr&lt;/code&gt; string of the static nodes as comma-separated values, and the compute node will pick them up at the start.&lt;/p&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;We have 3 types of releases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versioned&lt;/strong&gt;: With each release, a versioned image is deployed on Docker hub with the version tag &lt;code&gt;:vX.X.X&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Latest&lt;/strong&gt;: As usual, the latest version is kept under &lt;code&gt;:latest&lt;/code&gt; tag.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: On each push to &lt;code&gt;master&lt;/code&gt;, a new image is created with &lt;code&gt;:unstable&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See deployed images on &lt;a href=&#34;https://hub.docker.com/orgs/firstbatch/members&#34;&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you have a feature that you would like to add with respect to its respective issue, or a bug fix, feel free to fork &amp;amp; create a PR!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you would like to run the node from source (which is really handy during development), you can use our shorthand scripts within the Makefile. You can see the available commands with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will need OpenSSL installed as well, see shorthand commands &lt;a href=&#34;https://github.com/sfackler/rust-openssl/issues/855#issuecomment-450057552&#34;&gt;here&lt;/a&gt;. While running Ollama elsewhere (if you are using it) or with an OpenAI API key provided, you can run the compute node with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make run      # info-level logs&#xA;make debug    # debug-level logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing &amp;amp; Benchmarking&lt;/h3&gt; &#xA;&lt;p&gt;You can the tests as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make test         # unit tests&#xA;make test-ollama  # Ollama tests (requires a running Ollama client)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To measure the speed of some Ollama models we have a benchmark that uses some models for a few prompts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cargo run --release --example ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also benchmark these models using a larger task list at a given path, with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;JSON_PATH=&#34;./path/to/your.json&#34; cargo run --release --example ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;Open crate docs using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Styling&lt;/h3&gt; &#xA;&lt;p&gt;Lint and format with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make lint   # clippy&#xA;make format # rustfmt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Profiling&lt;/h3&gt; &#xA;&lt;p&gt;To create a flamegraph of the application, do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make profile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a profiling build that inherits &lt;code&gt;release&lt;/code&gt; mode, except with debug information.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;Profiling requires superuser access.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://opensource.org/license/Apache-2.0&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>