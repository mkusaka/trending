<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-15T01:34:35Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ArthurBrussee/brush</title>
    <updated>2024-11-15T01:34:35Z</updated>
    <id>tag:github.com,2024-11-15:/ArthurBrussee/brush</id>
    <link href="https://github.com/ArthurBrussee/brush" rel="alternate"></link>
    <summary type="html">&lt;p&gt;3D Reconstruction for all&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Brush - universal splats&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/b7f55b9c-8632-49f9-b34b-d5de52a7a8b0&#34;&gt;https://github.com/user-attachments/assets/b7f55b9c-8632-49f9-b34b-d5de52a7a8b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Brush is a 3D reconstruction engine, using &lt;a href=&#34;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/&#34;&gt;Gaussian splatting&lt;/a&gt;. It aims to be highly portable, flexible and fast. 3D reconstruction should be accessible to everyone!&lt;/p&gt; &#xA;&lt;p&gt;Brush works on a wide range of systems: &lt;strong&gt;macOS/windows/linux&lt;/strong&gt;, &lt;strong&gt;AMD/Nvidia&lt;/strong&gt; cards, &lt;strong&gt;Android&lt;/strong&gt;, and in a &lt;strong&gt;browser&lt;/strong&gt;. To achieve this, it uses WebGPU compatible tech, like the &lt;a href=&#34;https://github.com/tracel-ai/burn&#34;&gt;Burn&lt;/a&gt; machine learning framework, which has a portable &lt;a href=&#34;https://github.com/gfx-rs/wgpu&#34;&gt;&lt;code&gt;wgpu&lt;/code&gt;&lt;/a&gt; backend. This project is currently still a proof of concept, and doesn&#39;t yet implement the many extensions to gaussian splatting that have been developed, nor is performance optimal yet.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arthurbrussee.github.io/brush-demo&#34;&gt;&lt;strong&gt;Try the (experimental) web demo&lt;/strong&gt; &lt;img src=&#34;https://cdn-icons-png.flaticon.com/256/888/888846.png&#34; alt=&#34;chrome logo&#34; width=&#34;24&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;NOTE: This only works on desktop Chrome 129+ currently (Oct 2024). Firefox and Safari are hopefully supported &lt;a href=&#34;https://caniuse.com/webgpu&#34;&gt;soon&lt;/a&gt;, but currently even firefox nightly and safari technical preview do not work&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/r6jukQBK96&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/r6jukQBK96&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The demo can load pretrained ply splats, and can load datasets to train on. Currently only two formats are supported. A .zip file containing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A transform_train.json and images, like the synthetic nerf scene dataset.&lt;/li&gt; &#xA; &lt;li&gt;An &lt;code&gt;images&lt;/code&gt; &amp;amp; &lt;code&gt;sparse&lt;/code&gt; folder with &lt;a href=&#34;https://github.com/colmap/colmap&#34;&gt;&lt;code&gt;COLMAP&lt;/code&gt;&lt;/a&gt; data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;While training you can interact with the scene and see the training dynamics live, and compare the current rendering to training / eval views as the training progresses.&lt;/p&gt; &#xA;&lt;h2&gt;Web&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/4c70f892-cfd2-419f-8098-b0e20dba23c7&#34;&gt;https://github.com/user-attachments/assets/4c70f892-cfd2-419f-8098-b0e20dba23c7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Rerun&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c&#34;&gt;https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;While training, additional data can be visualized with the excellent &lt;a href=&#34;https://rerun.io/&#34;&gt;rerun&lt;/a&gt;. To install rerun on your machine, please follow their &lt;a href=&#34;https://rerun.io/docs/getting-started/installing-viewer&#34;&gt;instructions&lt;/a&gt;. Open the ./brush_blueprint.rbl in the viewer for best results.&lt;/p&gt; &#xA;&lt;h2&gt;Mobile&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/d6751cb3-ff58-45a4-8321-77d3b0a7b051&#34;&gt;https://github.com/user-attachments/assets/d6751cb3-ff58-45a4-8321-77d3b0a7b051&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Training on a pixel 7&lt;/p&gt; &#xA;&lt;h1&gt;Why&lt;/h1&gt; &#xA;&lt;p&gt;Machine learning for real time rendering has a lot of potential, but most popular ML tools don&#39;t align well with it. Rendering requires low latency, usually involve dynamic shapes, and it&#39;s not pleasant to attempt to ship apps with large PyTorch/Jax/CUDA deps calling out to python in a rendering loop. The usual fix is to write a seperate training and inference application. Brush on the other hand, written in rust using &lt;code&gt;wgpu&lt;/code&gt; and &lt;code&gt;burn&lt;/code&gt;, can produce simple dependency free binaries, and can run on nearly all devices.&lt;/p&gt; &#xA;&lt;h1&gt;Getting started&lt;/h1&gt; &#xA;&lt;p&gt;Install rust 1.81+ and run &lt;code&gt;cargo run&lt;/code&gt; or &lt;code&gt;cargo run --release&lt;/code&gt;. You can run tests with &lt;code&gt;cargo test --all&lt;/code&gt;. Brush uses the wonderful &lt;a href=&#34;https://raw.githubusercontent.com/ArthurBrussee/brush/main/rerun.io&#34;&gt;rerun&lt;/a&gt; for additional visualizations while training. It currently requires rerun 0.19 however, which isn&#39;t released yet.&lt;/p&gt; &#xA;&lt;h3&gt;Windows/macOS/Linux&lt;/h3&gt; &#xA;&lt;p&gt;Simply &lt;code&gt;cargo run&lt;/code&gt; or &lt;code&gt;cargo run --release&lt;/code&gt; from the workspace root.&lt;/p&gt; &#xA;&lt;p&gt;Note: Linux has not yet been tested but &lt;em&gt;should&lt;/em&gt; work. Windows works well, but does currently only works on Vulkan.&lt;/p&gt; &#xA;&lt;h3&gt;Web&lt;/h3&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://github.com/trunk-rs/trunk&#34;&gt;&lt;code&gt;trunk&lt;/code&gt;&lt;/a&gt; to build for the web. Install trunk, and then run &lt;code&gt;trunk serve&lt;/code&gt; or &lt;code&gt;trunk serve --release&lt;/code&gt; to run a development server.&lt;/p&gt; &#xA;&lt;p&gt;WebGPU is still a new standard, and as such, only the latest versions of Chrome work currently. Firefox nightly should work but unfortunately crashes currently.&lt;/p&gt; &#xA;&lt;p&gt;The public web demo is registered for the &lt;a href=&#34;https://chromestatus.com/feature/5126409856221184&#34;&gt;subgroups origin trial&lt;/a&gt;. To run the web demo for yourself, please enable the &#34;Unsafe WebGPU support&#34; flag in Chrome.&lt;/p&gt; &#xA;&lt;h3&gt;Android&lt;/h3&gt; &#xA;&lt;p&gt;To build on Android, see the more detailed README instructions at crates/brush-android.&lt;/p&gt; &#xA;&lt;h3&gt;iOS&lt;/h3&gt; &#xA;&lt;p&gt;Brush &lt;em&gt;should&lt;/em&gt; work on iOs but there is currently no project setup to do so.&lt;/p&gt; &#xA;&lt;h2&gt;Technical details&lt;/h2&gt; &#xA;&lt;p&gt;Brush is split into various crates. A quick overview of the different responsibilities are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-render&lt;/code&gt; is the main crate that pulls together the kernels into rendering functions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-train&lt;/code&gt; has code to actually train Gaussians, and handle larger scale optimizations like splitting/cloning gaussians etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-viewer&lt;/code&gt; handles the UI and integrating the training loop.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-android&lt;/code&gt; is the binary target for running on android, while &lt;code&gt;brush-desktop&lt;/code&gt; is for running both on web, and mac/Windows/Linux.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-wgsl&lt;/code&gt; handles some kernel inspection for generating CPU-side structs and interacing with &lt;a href=&#34;https://github.com/bevyengine/naga_oil&#34;&gt;naga-oil&lt;/a&gt; to handle shader imports.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-dataset&lt;/code&gt; handles importing different datasets like COLMAP or synthetic nerf data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;brush-prefix-sum&lt;/code&gt; and &lt;code&gt;brush-sort&lt;/code&gt; are only compute kernels and should be largely independent of Brush (other than &lt;code&gt;brush-wgsl&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rrfd&lt;/code&gt; is a small extension of &lt;a href=&#34;https://github.com/PolyMeilex/rfd&#34;&gt;&lt;code&gt;rfd&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Kernels&lt;/h3&gt; &#xA;&lt;p&gt;The kernels are written in a &#34;sparse&#34; style, that is, only work for visible gaussians is done, though the final calculated gradients are dense. Brush uses a GPU radix sort based on &lt;a href=&#34;https://www.amd.com/en/products/graphics/technologies/fidelityfx.html&#34;&gt;FidelityFX&lt;/a&gt; (see &lt;code&gt;crates/brush-sort&lt;/code&gt;). The sorting is done in two parts - first splats are sorted only by depth, then sorted by their tile ID, which saves some sorting time compared to sorting both depth and tile ids at the same time.&lt;/p&gt; &#xA;&lt;p&gt;Compatibility with WebGPU does bring some challenges, even with (the excellent) &lt;a href=&#34;https://github.com/gfx-rs/wgpu&#34;&gt;wgpu&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;WebGPU lacks native atomic floating point additions, and a software CAS loop has to be used.&lt;/li&gt; &#xA; &lt;li&gt;GPU readbacks have to be async on WebGPU. A rendering pass can&#39;t do this unless the whole rendering becomes async, which has its own perils, and isn&#39;t great for an UI. The reference tile renderer requires reading back the number of &#34;intersections&#34; (each visible tile of a gaussian is one intersection), but this is not feasible. This is worked around by assuming a worst case. To reduce the number of tiles the rasterizer culls away unused tiles by intersecting the gaussian ellipses with the screenspace tiles.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The WGSL kernels use &lt;a href=&#34;https://github.com/bevyengine/naga_oil&#34;&gt;naga_oil&lt;/a&gt; to manage imports. brush-wgsl additionally does some reflection to generate rust code to send uniform data to a kernel. In the future, it might be possible to port the kernels to Burns new &lt;a href=&#34;https://github.com/tracel-ai/cubecl&#34;&gt;&lt;code&gt;CubeCL&lt;/code&gt;&lt;/a&gt; language, which is much more ergonomic and would allow generating CUDA / rocM kernels. It might also be possible to integrate with George Kopanos&#39; &lt;a href=&#34;https://github.com/google/slang-gaussian-rasterization&#34;&gt;Slang kernels&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Rendering performance is expected to be very competitive with gSplat, while training performance is still a bit slower. You can run some benchmarks using &lt;code&gt;cargo bench&lt;/code&gt;. The performance of the splatting forward and backwards kernel are faster than the &lt;em&gt;legacy&lt;/em&gt; gSplat kernels as they use some new techniques for better performance, but they haven&#39;t been compared yet to the more recent gSplat kernels. End-to-end training performance is also still slower, due to other overheads.&lt;/p&gt; &#xA;&lt;p&gt;For additional profiling, you can use &lt;a href=&#34;https://github.com/wolfpld/tracy&#34;&gt;tracy&lt;/a&gt; and run with &lt;code&gt;cargo run --release --feature=tracy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quality&lt;/h3&gt; &#xA;&lt;p&gt;Quality is similair, but for now still somewhat lagging behind the original GS implementation. This is likely due to some suboptimal splitting/cloning heuristics.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scene&lt;/th&gt; &#xA;   &lt;th&gt;Brush&lt;/th&gt; &#xA;   &lt;th&gt;GS paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bicycle@7K&lt;/td&gt; &#xA;   &lt;td&gt;23.4&lt;/td&gt; &#xA;   &lt;td&gt;23.604&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Garden@7k&lt;/td&gt; &#xA;   &lt;td&gt;26.1&lt;/td&gt; &#xA;   &lt;td&gt;26.245&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stump@7k&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;   &lt;td&gt;25.709&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nerfstudio-project/gsplat&#34;&gt;&lt;strong&gt;gSplat&lt;/strong&gt;&lt;/a&gt;, for their reference version of the kernels&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Peter Hedman, George Kopanas &amp;amp; Bernhard Kerbl&lt;/strong&gt;, for the many discussions &amp;amp; pointers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Burn team&lt;/strong&gt;, for help &amp;amp; improvements to Burn along the way&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raph Levien&lt;/strong&gt;, for the &lt;a href=&#34;https://github.com/googlefonts/compute-shader-101/pull/31&#34;&gt;original version&lt;/a&gt; of the GPU radix sort.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; an official Google product. This repository is a forked public version of &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/brush_splat&#34;&gt;the google-research repository&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorzero/tensorzero</title>
    <updated>2024-11-15T01:34:35Z</updated>
    <id>tag:github.com,2024-11-15:/tensorzero/tensorzero</id>
    <link href="https://github.com/tensorzero/tensorzero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorZero creates a feedback loop for optimizing LLM applications — turning production data into smarter, faster, and cheaper models.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9&#34; width=&#34;128&#34; height=&#34;128&#34;&gt; &#xA;&lt;h1&gt;TensorZero&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;TensorZero creates a feedback loop for optimizing LLM applications — turning production data into smarter, faster, and cheaper models.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Integrate our model gateway&lt;/li&gt; &#xA; &lt;li&gt;Send metrics or feedback&lt;/li&gt; &#xA; &lt;li&gt;Optimize prompts, models, and inference strategies&lt;/li&gt; &#xA; &lt;li&gt;Watch your LLMs improve over time&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;It provides a &lt;strong&gt;data &amp;amp; learning flywheel for LLMs&lt;/strong&gt; by unifying:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Inference:&lt;/strong&gt; one API for all LLMs, with &amp;lt;1ms P99 overhead&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Observability:&lt;/strong&gt; inference &amp;amp; feedback → your database&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Optimization:&lt;/strong&gt; from prompts to fine-tuning and RL (&amp;amp; even 🍓? &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations&#34;&gt;→&lt;/a&gt;&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Experimentation:&lt;/strong&gt; built-in A/B testing, routing, fallbacks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/docs&#34; target=&#34;_blank&#34;&gt;Docs&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.x.com/tensorzero&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/slack&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/discord&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt;&lt;/b&gt; &lt;br&gt; &lt;br&gt; &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/quickstart&#34; target=&#34;_blank&#34;&gt;Quick Start (5min)&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/tutorial&#34; target=&#34;_blank&#34;&gt;Comprehensive Tutorial&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/deployment&#34; target=&#34;_blank&#34;&gt;Deployment Guide&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/api-reference&#34; target=&#34;_blank&#34;&gt;API Reference&lt;/a&gt;&lt;/b&gt; · &lt;b&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/deployment&#34; target=&#34;_blank&#34;&gt;Configuration Reference&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.tensorzero.com/docs&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270&#34;&gt; &#xA;   &lt;img alt=&#34;TensorZero Flywheel&#34; src=&#34;https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6&#34; width=&#34;720&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/&#34;&gt;TensorZero Gateway&lt;/a&gt;&lt;/strong&gt; is a high-performance model gateway written in Rust 🦀 that provides a unified API interface for all major LLM providers, allowing for seamless cross-platform integration and fallbacks.&lt;/li&gt; &#xA; &lt;li&gt;It handles structured schema-based inference with &amp;lt;1ms P99 latency overhead (see &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/benchmarks&#34;&gt;Benchmarks&lt;/a&gt;&lt;/strong&gt;) and built-in observability, experimentation, and &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations&#34;&gt;inference-time optimizations&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It also collects downstream metrics and feedback associated with these inferences, with first-class support for multi-step LLM systems.&lt;/li&gt; &#xA; &lt;li&gt;Everything is stored in a ClickHouse data warehouse that you control for real-time, scalable, and developer-friendly analytics.&lt;/li&gt; &#xA; &lt;li&gt;Over time, &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/recipes&#34;&gt;TensorZero Recipes&lt;/a&gt;&lt;/strong&gt; leverage this structured dataset to optimize your prompts and models: run pre-built recipes for common workflows like fine-tuning, or create your own with complete flexibility using any language and platform.&lt;/li&gt; &#xA; &lt;li&gt;Finally, the gateway&#39;s experimentation features and GitOps orchestration enable you to iterate and deploy with confidence, be it a single LLM or thousands of LLMs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Our goal is to help engineers build, manage, and optimize the next generation of LLM applications: systems that learn from real-world experience. Read more about our &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/vision-roadmap/&#34;&gt;Vision &amp;amp; Roadmap&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Next steps?&lt;/strong&gt; The &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/strong&gt; shows it&#39;s easy to set up an LLM application with TensorZero. If you want to dive deeper, the &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/docs/gateway/tutorial&#34;&gt;Tutorial&lt;/a&gt;&lt;/strong&gt; teaches how to build a simple chatbot, an email copilot, a weather RAG system, and a structured data extraction pipeline.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Questions?&lt;/strong&gt; Ask us on &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/slack&#34;&gt;Slack&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href=&#34;https://www.tensorzero.com/discord&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using TensorZero at work?&lt;/strong&gt; Email us at &lt;strong&gt;&lt;a href=&#34;mailto:hello@tensorzero.com&#34;&gt;hello@tensorzero.com&lt;/a&gt;&lt;/strong&gt; to set up a Slack or Teams channel with your team (free).&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;We are working on a series of &lt;strong&gt;complete runnable examples&lt;/strong&gt; illustrating TensorZero&#39;s data &amp;amp; learning flywheel.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences&#34;&gt;Writing Haikus to Satisfy a Judge with Hidden Preferences&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;This example fine-tunes GPT-4o Mini to generate haikus tailored to a specific taste. You&#39;ll see TensorZero&#39;s &#34;data flywheel in a box&#34; in action: better variants leads to better data, and better data leads to better variants. You&#39;ll see progress by fine-tuning the LLM multiple times.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/tensorzero/tensorzero/tree/main/examples/ner-fine-tuning&#34;&gt;Improving Data Extraction (NER) by Fine-Tuning a Llama 3 Model&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;This example shows that an optimized Llama 3.1 8B model can be trained to outperform GPT-4o on a Named Entity Recognition (NER) task using a small amount of training data, and served by Fireworks at a fraction of the cost and latency.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles-best-of-n-sampling/&#34;&gt;Improving LLM Chess Ability with Best-of-N Sampling&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;This example showcases how best-of-N sampling can significantly enhance an LLM&#39;s chess-playing abilities by selecting the most promising moves from multiple generated options.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/tensorzero/tensorzero/tree/main/examples/ner-dicl&#34;&gt;Improving Data Extraction (NER) with Dynamic In-Context Learning&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;This example demonstrates how Dynamic In-Context Learning (DICL) can enhance Named Entity Recognition (NER) performance by leveraging relevant historical examples to improve data extraction accuracy and consistency without having to fine-tune a model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy&#34;&gt;Improving Math Reasoning with a Custom Recipe for Automated Prompt Engineering (DSPy)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows. But you can also easily create your own recipes and workflows! This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;em&gt;&amp;amp; many more on the way!&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>