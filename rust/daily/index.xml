<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-10T01:44:35Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/candle</title>
    <updated>2023-08-10T01:44:35Z</updated>
    <id>tag:github.com,2023-08-10:/huggingface/candle</id>
    <link href="https://github.com/huggingface/candle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimalist ML framework for Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;candle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/candle-core&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/candle-core.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/candle-core&#34;&gt;&lt;img src=&#34;https://docs.rs/candle-core/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/crates/l/candle-core.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Candle is a minimalist ML framework for Rust with a focus on easiness of use and on performance (including GPU support). Try our online demos: &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;llama2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;let a = Tensor::randn(0f32, 1., (2, 3), &amp;amp;Device::Cpu)?;&#xA;let b = Tensor::randn(0f32, 1., (3, 4), &amp;amp;Device::Cpu)?;&#xA;&#xA;let c = a.matmul(&amp;amp;b)?;&#xA;println!(&#34;{c}&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Check out our examples&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/&#34;&gt;examples&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/whisper/&#34;&gt;Whisper&lt;/a&gt;: speech recognition model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/llama/&#34;&gt;Llama and Llama-v2&lt;/a&gt;: general LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/falcon/&#34;&gt;Falcon&lt;/a&gt;: general LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bert/&#34;&gt;Bert&lt;/a&gt;: useful for sentence embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bigcode/&#34;&gt;StarCoder&lt;/a&gt;: LLM specialized to code generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run them using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --example whisper --release&#xA;cargo run --example llama --release&#xA;cargo run --example falcon --release&#xA;cargo run --example bert --release&#xA;cargo run --example bigcode --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to use &lt;strong&gt;CUDA&lt;/strong&gt; add &lt;code&gt;--features cuda&lt;/code&gt; to the example command line.&lt;/p&gt; &#xA;&lt;p&gt;There are also some wasm examples for whisper and &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;. You can either build them with &lt;code&gt;trunk&lt;/code&gt; or try them online: &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;llama2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For llama2, run the following command to retrieve the weight files and start a test server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd candle-wasm-examples/llama2-c&#xA;wget https://karpathy.ai/llama2c/model.bin&#xA;wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin&#xA;trunk serve --release --public-url /candle-llama2/ --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then browse to &lt;a href=&#34;http://localhost:8081/candle-llama2&#34;&gt;http://localhost:8081/candle-llama2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ANCHOR: features ---&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple syntax, looks and feels like PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;CPU and Cuda backends, m1, f16, bf16.&lt;/li&gt; &#xA; &lt;li&gt;Enable serverless (CPU), small and fast deployments&lt;/li&gt; &#xA; &lt;li&gt;WASM support, run your models in a browser.&lt;/li&gt; &#xA; &lt;li&gt;Model training.&lt;/li&gt; &#xA; &lt;li&gt;Distributed computing using NCCL.&lt;/li&gt; &#xA; &lt;li&gt;Models out of the box: Llama, Whisper, Falcon, StarCoder...&lt;/li&gt; &#xA; &lt;li&gt;Embed user-defined ops/kernels, such as &lt;a href=&#34;https://github.com/huggingface/candle/raw/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152&#34;&gt;flash-attention v2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ANCHOR_END: features ---&gt; &#xA;&lt;h2&gt;How to use ?&lt;/h2&gt; &#xA;&lt;!-- ANCHOR: cheatsheet ---&gt; &#xA;&lt;p&gt;Cheatsheet:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Using PyTorch&lt;/th&gt; &#xA;   &lt;th&gt;Using Candle&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Creation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.Tensor([[1, 2], [3, 4]])&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Tensor::new(&amp;amp;[[1f32, 2.], [3., 4.]], &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Creation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.zeros((2, 2))&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Tensor::zeros((2, 2), DType::F32, &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Indexing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor[:, :4]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.i((.., ..4))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.view((2, 2))&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.reshape((2, 2))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a.matmul(b)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a.matmul(&amp;amp;b)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arithmetic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a + b&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;amp;a + &amp;amp;b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Device&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to(device=&#34;cuda&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to_device(&amp;amp;Device::Cuda(0))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dtype&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to(dtype=torch.float16)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to_dtype(&amp;amp;DType::F16)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Saving&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.save({&#34;A&#34;: A}, &#34;model.bin&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;candle::safetensors::save(&amp;amp;HashMap::from([(&#34;A&#34;, A)]), &#34;model.safetensors&#34;)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Loading&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;weights = torch.load(&#34;model.bin&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;candle::safetensors::load(&#34;model.safetensors&#34;, &amp;amp;device)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- ANCHOR_END: cheatsheet ---&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-core&#34;&gt;candle-core&lt;/a&gt;: Core ops, devices, and &lt;code&gt;Tensor&lt;/code&gt; struct definition&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-nn/&#34;&gt;candle-nn&lt;/a&gt;: Facilities to build real models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/&#34;&gt;candle-examples&lt;/a&gt;: Real-world like examples on how to use the library in real settings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-kernels/&#34;&gt;candle-kernels&lt;/a&gt;: CUDA custom kernels&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-datasets/&#34;&gt;candle-datasets&lt;/a&gt;: Datasets and data loaders.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-transformers&#34;&gt;candle-transformers&lt;/a&gt;: Transformer related utilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-flash-attn&#34;&gt;candle-flash-attn&lt;/a&gt;: Flash attention v2 layer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Why Candle?&lt;/h3&gt; &#xA;&lt;p&gt;Candle stems from the need to reduce binary size in order to &lt;em&gt;enable serverless&lt;/em&gt; possible by making the whole engine smaller than PyTorch very large library volume. This enables creating runtimes on a cluster much faster.&lt;/p&gt; &#xA;&lt;p&gt;And simply &lt;em&gt;removing Python&lt;/em&gt; from production workloads. Python can really add overhead in more complex workflows and the &lt;a href=&#34;https://www.backblaze.com/blog/the-python-gil-past-present-and-future/&#34;&gt;GIL&lt;/a&gt; is a notorious source of headaches.&lt;/p&gt; &#xA;&lt;p&gt;Rust is cool, and a lot of the HF ecosystem already has Rust crates &lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;safetensors&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;tokenizers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other ML frameworks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/coreylowman/dfdx&#34;&gt;dfdx&lt;/a&gt; is a formidable crate, with shapes being included in types preventing a lot of headaches by getting compiler to complain about shape mismatch right off the bat However we found that some features still require nightly and writing code can be a bit daunting for non rust experts.&lt;/p&gt; &lt;p&gt;We&#39;re leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each other&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/burn-rs/burn&#34;&gt;burn&lt;/a&gt; is a general crate that can leverage multiple backends so you can choose the best engine for your workload&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/LaurentMazare/tch-rs.git&#34;&gt;tch-rs&lt;/a&gt; Bindings to the torch library in Rust. Extremely versatile, but they do bring in the entire torch library into the runtime. The main contributor of &lt;code&gt;tch-rs&lt;/code&gt; is also involved in the development of &lt;code&gt;candle&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Missing symbols when compiling with the mkl feature.&lt;/h3&gt; &#xA;&lt;p&gt;If you get some missing symbols when compiling binaries/tests using the mkl features, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#39;:&#xA;          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#39; collect2: error: ld returned 1 exit status&#xA;&#xA;  = note: some `extern` functions couldn&#39;t be found; some native libraries may need to be installed or have their path specified&#xA;  = note: use the `-l` flag to specify native libraries to link&#xA;  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo (see https://doc.rust-lang.org/cargo/reference/build-scripts.html#cargorustc-link-libkindname)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is likely due to some missing linker flag that enable the mkl library. You can try adding the following at the top of your binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;extern crate intel_mkl_src;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to know where an error comes from.&lt;/h3&gt; &#xA;&lt;p&gt;You can set &lt;code&gt;RUST_BACKTRACE=1&lt;/code&gt; to be provided with backtraces when a candle error is generated.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pnpm/pacquet</title>
    <updated>2023-08-10T01:44:35Z</updated>
    <id>tag:github.com,2023-08-10:/pnpm/pacquet</id>
    <link href="https://github.com/pnpm/pacquet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;experimental package manager for node.js&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pacquet&lt;/h1&gt; &#xA;&lt;p&gt;Experimental package manager for node.js written in rust.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: This is mostly a playground for me to learn Rust and understand how package managers work.&lt;/p&gt; &#xA;&lt;h3&gt;TODO&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;.npmrc&lt;/code&gt; support (for supported features &lt;a href=&#34;https://raw.githubusercontent.com/pnpm/pacquet/main/crates/npmrc/README.md&#34;&gt;readme.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CLI commands (for supported features &lt;a href=&#34;https://raw.githubusercontent.com/pnpm/pacquet/main/crates/cli/README.md&#34;&gt;readme.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Content addressable file store support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Shrink-file support in sync with &lt;code&gt;pnpm-lock.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Workspace support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Full sync with &lt;a href=&#34;https://pnpm.io/errors&#34;&gt;pnpm error codes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate a &lt;code&gt;node_modules/.bin&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add CLI report&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Debugging&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;TRACE=pacquet_tarball just cli add fastify&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>