<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-14T01:44:53Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hlhr202/llama-node</title>
    <updated>2023-04-14T01:44:53Z</updated>
    <id>tag:github.com,2023-04-14:/hlhr202/llama-node</id>
    <link href="https://github.com/hlhr202/llama-node" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Believe in AI democratization. llama for nodejs backed by llama-rs and llama.cpp, work locally on your laptop CPU. support llama/alpaca/gpt4all/vicuna model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama-node&lt;/h1&gt; &#xA;&lt;p&gt;Large Language Model LLaMA on node.js&lt;/p&gt; &#xA;&lt;p&gt;This project is in an early stage, the API for nodejs may change in the future, use it with caution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/README-zh-CN.md&#34;&gt;中文文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/doc/assets/llama.png&#34; width=&#34;300px&#34; height=&#34;300px&#34; alt=&#34;LLaMA generated by Stable diffusion&#34;&gt; &#xA;&lt;p&gt;&lt;sub&gt;Picture generated by stable diffusion.&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/hlhr202/llama-node/llama-build.yml&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/l/llama-node&#34; alt=&#34;NPM&#34;&gt; &lt;a href=&#34;https://www.npmjs.com/package/llama-node&#34;&gt;&lt;img alt=&#34;npm&#34; src=&#34;https://img.shields.io/npm/v/llama-node&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/npm/types/llama-node&#34; alt=&#34;npm type definitions&#34;&gt; &lt;a href=&#34;https://twitter.com/hlhr202&#34;&gt;&lt;img alt=&#34;twitter&#34; src=&#34;https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fhlhr202&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#llama-node&#34;&gt;llama-node&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#getting-the-weights&#34;&gt;Getting the weights&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#model-versioning&#34;&gt;Model versioning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#usage-llamacpp-backend&#34;&gt;Usage (llama.cpp backend)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#tokenize&#34;&gt;Tokenize&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#embedding&#34;&gt;Embedding&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#usage-llama-rs-backend&#34;&gt;Usage (llama-rs backend)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#inference-1&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#tokenize-1&#34;&gt;Tokenize&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#embedding-1&#34;&gt;Embedding&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#performance-related&#34;&gt;Performance related&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#manual-compilation-from-node_modules&#34;&gt;Manual compilation (from node_modules)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#manual-compilation-from-source&#34;&gt;Manual compilation (from source)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hlhr202/llama-node/main/#future-plan&#34;&gt;Future plan&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This is a nodejs client library for llama (or llama based) LLM built on top of &lt;a href=&#34;https://github.com/rustformers/llama-rs&#34;&gt;llama-rs&lt;/a&gt; and &lt;a href=&#34;https://github.com/sobelio/llm-chain/tree/main/llm-chain-llama/sys&#34;&gt;llm-chain-llama-sys&lt;/a&gt; which generate bindings for &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;. It uses &lt;a href=&#34;https://github.com/napi-rs/napi-rs&#34;&gt;napi-rs&lt;/a&gt; for channel messages between node.js and llama thread.&lt;/p&gt; &#xA;&lt;p&gt;From v0.0.21, both llama-rs and llama.cpp backends are supported!&lt;/p&gt; &#xA;&lt;p&gt;Currently supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;darwin-x64&lt;/li&gt; &#xA; &lt;li&gt;darwin-arm64&lt;/li&gt; &#xA; &lt;li&gt;linux-x64-gnu&lt;/li&gt; &#xA; &lt;li&gt;win32-x64-msvc&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I do not have hardware for testing 13B or larger models, but I have tested it supported llama 7B model with both ggml llama and ggml alpaca.&lt;/p&gt; &#xA;&lt;!-- Download one of the llama ggml models from the following links:&#xA;- [llama 7B int4 (old model for llama.cpp)](https://huggingface.co/hlhr202/llama-7B-ggml-int4/blob/main/ggml-model-q4_0.bin)&#xA;- [alpaca 7B int4](https://huggingface.co/hlhr202/alpaca-7B-ggml-int4/blob/main/ggml-alpaca-7b-q4.bin) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install main package&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install llama-node&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install llama-rs backend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @llama-node/core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install llama.cpp backend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @llama-node/llama-cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Getting the weights&lt;/h2&gt; &#xA;&lt;p&gt;The llama-node uses llama-rs under the hook and uses the model format derived from llama.cpp. Due to the fact that the meta-release model is only used for research purposes, this project does not provide model downloads. If you have obtained the original &lt;strong&gt;.pth&lt;/strong&gt; model, please read the document &lt;a href=&#34;https://github.com/rustformers/llama-rs#getting-the-weights&#34;&gt;Getting the weights&lt;/a&gt; and use the convert tool provided by llama-rs for conversion.&lt;/p&gt; &#xA;&lt;h3&gt;Model versioning&lt;/h3&gt; &#xA;&lt;p&gt;There are now 3 versions from llama.cpp community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GGML: legacy format, oldest ggml tensor file format&lt;/li&gt; &#xA; &lt;li&gt;GGMF: also legacy format, newer than GGML, older than GGJT&lt;/li&gt; &#xA; &lt;li&gt;GGJT: mmap-able format&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The llama-rs backend now only supports GGML/GGMF models, and llama.cpp backend only supports GGJT models.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage (llama.cpp backend)&lt;/h2&gt; &#xA;&lt;p&gt;The current version supports only one inference session on one LLama instance at the same time&lt;/p&gt; &#xA;&lt;p&gt;If you wish to have multiple inference sessions concurrently, you need to create multiple LLama instances&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaCpp, LoadConfig } from &#34;llama-node/dist/llm/llama-cpp.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-vicuna-7b-4bit-rev1.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaCpp);&#xA;&#xA;const config: LoadConfig = {&#xA;    path: model,&#xA;    enableLogging: true,&#xA;    nCtx: 1024,&#xA;    nParts: -1,&#xA;    seed: 0,&#xA;    f16Kv: false,&#xA;    logitsAll: false,&#xA;    vocabOnly: false,&#xA;    useMlock: false,&#xA;    embedding: false,&#xA;};&#xA;&#xA;llama.load(config);&#xA;&#xA;const template = `How are you`;&#xA;&#xA;const prompt = `### Human:&#xA;&#xA;${template}&#xA;&#xA;### Assistant:`;&#xA;&#xA;llama.createCompletion(&#xA;    {&#xA;        nThreads: 4,&#xA;        nTokPredict: 2048,&#xA;        topK: 40,&#xA;        topP: 0.1,&#xA;        temp: 0.2,&#xA;        repeatPenalty: 1,&#xA;        stopSequence: &#34;### Human&#34;,&#xA;        prompt,&#xA;    },&#xA;    (response) =&amp;gt; {&#xA;        process.stdout.write(response.token);&#xA;    }&#xA;);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tokenize&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaCpp, LoadConfig } from &#34;llama-node/dist/llm/llama-cpp.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-vicuna-7b-4bit-rev1.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaCpp);&#xA;&#xA;const config: LoadConfig = {&#xA;    path: model,&#xA;    enableLogging: true,&#xA;    nCtx: 1024,&#xA;    nParts: -1,&#xA;    seed: 0,&#xA;    f16Kv: false,&#xA;    logitsAll: false,&#xA;    vocabOnly: false,&#xA;    useMlock: false,&#xA;    embedding: false,&#xA;};&#xA;&#xA;llama.load(config);&#xA;&#xA;const content = &#34;how are you?&#34;;&#xA;&#xA;llama.tokenize({ content, nCtx: 2048 }).then(console.log);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embedding&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaCpp, LoadConfig } from &#34;llama-node/dist/llm/llama-cpp.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-vicuna-7b-4bit-rev1.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaCpp);&#xA;&#xA;const config: LoadConfig = {&#xA;    path: model,&#xA;    enableLogging: true,&#xA;    nCtx: 1024,&#xA;    nParts: -1,&#xA;    seed: 0,&#xA;    f16Kv: false,&#xA;    logitsAll: false,&#xA;    vocabOnly: false,&#xA;    useMlock: false,&#xA;    embedding: false,&#xA;};&#xA;&#xA;llama.load(config);&#xA;&#xA;const prompt = `Who is the president of the United States?`;&#xA;&#xA;const params = {&#xA;    nThreads: 4,&#xA;    nTokPredict: 2048,&#xA;    topK: 40,&#xA;    topP: 0.1,&#xA;    temp: 0.2,&#xA;    repeatPenalty: 1,&#xA;    prompt,&#xA;};&#xA;&#xA;llama.getEmbedding(params).then(console.log);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage (llama-rs backend)&lt;/h2&gt; &#xA;&lt;p&gt;The current version supports only one inference session on one LLama instance at the same time&lt;/p&gt; &#xA;&lt;p&gt;If you wish to have multiple inference sessions concurrently, you need to create multiple LLama instances&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaRS } from &#34;llama-node/dist/llm/llama-rs.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-alpaca-7b-q4.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaRS);&#xA;&#xA;llama.load({ path: model });&#xA;&#xA;const template = `how are you`;&#xA;&#xA;const prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;&#xA;${template}&#xA;&#xA;### Response:`;&#xA;&#xA;llama.createCompletion(&#xA;    {&#xA;        prompt,&#xA;        numPredict: 128,&#xA;        temp: 0.2,&#xA;        topP: 1,&#xA;        topK: 40,&#xA;        repeatPenalty: 1,&#xA;        repeatLastN: 64,&#xA;        seed: 0,&#xA;        feedPrompt: true,&#xA;    },&#xA;    (response) =&amp;gt; {&#xA;        process.stdout.write(response.token);&#xA;    }&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tokenize&lt;/h3&gt; &#xA;&lt;p&gt;Get tokenization result from LLaMA&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaRS } from &#34;llama-node/dist/llm/llama-rs.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-alpaca-7b-q4.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaRS);&#xA;&#xA;llama.load({ path: model });&#xA;&#xA;const content = &#34;how are you?&#34;;&#xA;&#xA;llama.tokenize(content).then(console.log);&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embedding&lt;/h3&gt; &#xA;&lt;p&gt;Preview version, embedding end token may change in the future. Do not use it in production!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { LLama } from &#34;llama-node&#34;;&#xA;import { LLamaRS } from &#34;llama-node/dist/llm/llama-rs.js&#34;;&#xA;import path from &#34;path&#34;;&#xA;import fs from &#34;fs&#34;;&#xA;&#xA;const model = path.resolve(process.cwd(), &#34;./ggml-alpaca-7b-q4.bin&#34;);&#xA;&#xA;const llama = new LLama(LLamaRS);&#xA;&#xA;llama.load({ path: model });&#xA;&#xA;const getWordEmbeddings = async (prompt: string, file: string) =&amp;gt; {&#xA;    const data = await llama.getEmbedding({&#xA;        prompt,&#xA;        numPredict: 128,&#xA;        temp: 0.2,&#xA;        topP: 1,&#xA;        topK: 40,&#xA;        repeatPenalty: 1,&#xA;        repeatLastN: 64,&#xA;        seed: 0,&#xA;    });&#xA;&#xA;    console.log(prompt, data);&#xA;&#xA;    await fs.promises.writeFile(&#xA;        path.resolve(process.cwd(), file),&#xA;        JSON.stringify(data)&#xA;    );&#xA;};&#xA;&#xA;const run = async () =&amp;gt; {&#xA;    const dog1 = `My favourite animal is the dog`;&#xA;    await getWordEmbeddings(dog1, &#34;./example/semantic-compare/dog1.json&#34;);&#xA;&#xA;    const dog2 = `I have just adopted a cute dog`;&#xA;    await getWordEmbeddings(dog2, &#34;./example/semantic-compare/dog2.json&#34;);&#xA;&#xA;    const cat1 = `My favourite animal is the cat`;&#xA;    await getWordEmbeddings(cat1, &#34;./example/semantic-compare/cat1.json&#34;);&#xA;};&#xA;&#xA;run();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Performance related&lt;/h2&gt; &#xA;&lt;p&gt;We provide prebuild binaries for linux-x64, win32-x64, apple-x64, apple-silicon. For other platforms, before you install the npm package, please install rust environment for self built.&lt;/p&gt; &#xA;&lt;p&gt;Due to complexity of cross compilation, it is hard for pre-building a binary that fits all platform needs with best performance.&lt;/p&gt; &#xA;&lt;p&gt;If you face low performance issue, I would strongly suggest you do a manual compilation. Otherwise you have to wait for a better pre-compiled native binding. I am trying to investigate the way to produce a matrix of multi-platform supports.&lt;/p&gt; &#xA;&lt;h3&gt;Manual compilation (from node_modules)&lt;/h3&gt; &#xA;&lt;p&gt;The following steps will allow you to compile the binary with best quality on your platform&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pre-request: install rust&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under node_modules/@llama-node/core folder&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Manual compilation (from source)&lt;/h3&gt; &#xA;&lt;p&gt;The following steps will allow you to compile the binary with best quality on your platform&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pre-request: install rust&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under root folder, run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install &amp;amp;&amp;amp; npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Under packages/core folder, run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use the dist under root folder&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Future plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; prompt extensions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; more platforms and cross compile (performance related)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; tweak embedding API, make end token configurable&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; cli and interactive&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; support more open source models as llama-rs planned &lt;a href=&#34;https://github.com/rustformers/llama-rs/pull/85&#34;&gt;https://github.com/rustformers/llama-rs/pull/85&lt;/a&gt; &lt;a href=&#34;https://github.com/rustformers/llama-rs/issues/75&#34;&gt;https://github.com/rustformers/llama-rs/issues/75&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; more backends (eg. rwkv) supports!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lsk569937453/silverwind</title>
    <updated>2023-04-14T01:44:53Z</updated>
    <id>tag:github.com,2023-04-14:/lsk569937453/silverwind</id>
    <link href="https://github.com/lsk569937453/silverwind" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The High Performance Proxy/Load Balancer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Silverwind-The Next Generation High-Performance Proxy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lsk569937453/silverwind/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/printfn/fend/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;English &lt;a href=&#34;https://raw.githubusercontent.com/lsk569937453/silverwind/main/README-zh_CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Silverwind is a high-performance reverse proxy/load balancer. And it could be also used as the ingress controller in the k8s.&lt;/p&gt; &#xA;&lt;h2&gt;Silverwind-Dashboard&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Start the Silverwind-Dashboard over docker-compose.&lt;br&gt; The docker-compose.yaml is like following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;version: &#34;3.9&#34;&#xA;services:&#xA;  silverwind-dashboard:&#xA;    image: lsk569937453/silverwind-dashboard:0.0.6&#xA;    container_name: silverwind-dashboard&#xA;    ports:&#xA;      - &#34;4486:4486&#34;&#xA;&#xA;  silverwind:&#xA;      image: lsk569937453/silverwind:0.0.6&#xA;      container_name: silverwind&#xA;      ports:&#xA;        - &#34;6980:6980&#34;&#xA;      environment:&#xA;        ADMIN_PORT: 6980&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You could check the &lt;a href=&#34;http://localhost:4486/index.html&#34;&gt;main page&lt;/a&gt; for Silverwind -Dashboard after you execute the &lt;strong&gt;docker-compose up&lt;/strong&gt; command.&lt;/p&gt; &#xA;&lt;h2&gt;Why we chose Silverwind&lt;/h2&gt; &#xA;&lt;h3&gt;Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;We do the performance testing between several popular proxies including NGINX, Envoy, and Caddy. The benchmarks show &lt;a href=&#34;https://github.com/lsk569937453/silverwind/raw/main/benchmarks.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The test results show that under the same machine configuration (4 cores 8G), in some indicators (requests per second, average response time), the data of Silverwind is almost the same as the NGINX and Envoy. In terms of request latency, Silverwind is better than NGINX and Envoy.&lt;/p&gt; &#xA;&lt;h3&gt;All basic functions are developed in native language - fast&lt;/h3&gt; &#xA;&lt;p&gt;Silverwind is not only a reverse proxy/load balancer, but also an API gateway. As an API gateway, Silverwind will cover all basic functions (black and white list/authorization/fuse limit/gray release , blue-green publishing/monitoring/caching/protocol conversion).&lt;/p&gt; &#xA;&lt;p&gt;Compared with other gateways, Silverwind has the advantage of covering all the basic services of the API gateway, and has high performance. Second, Silverwind&#39;s dynamic configuration is close to real-time. Every time the configuration is modified, it will take effect within 5 seconds (close to real-time).&lt;/p&gt; &#xA;&lt;h3&gt;Kong&lt;/h3&gt; &#xA;&lt;p&gt;The free Ratelimiting plugin for Kong is &lt;a href=&#34;https://github.com/Kong/kong/issues/5311&#34;&gt;inaccurate&lt;/a&gt;. If we want to achieve more accurate Ratelimiting, we have to buy the enterprise version of Kong.&lt;/p&gt; &#xA;&lt;h3&gt;Envoy&lt;/h3&gt; &#xA;&lt;p&gt;Envoy does not have built-in ratelimiting. Envoy provides a ratelimiting interface for users to implement by themselves. Currently the most used is this &lt;a href=&#34;https://github.com/envoyproxy/ratelimit&#34;&gt;project&lt;/a&gt;. The first disadvantage is that the project only supports fixed-window ratelimiting. The disadvantage of the fixed window ratelimiting is that it does not support burst traffic. The second disadvantage is that every time Envoy is requested, it will use grpc to request the ratelimiting cluster. Compared with the built-in current limiting algorithm, this actually adds an additional network hop.&lt;/p&gt; &#xA;&lt;h2&gt;Dynamic Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You could change the configuration over the rest API. And the new configuration will have an effect &lt;strong&gt;in 5 seconds&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Compile or Download the release&lt;/h2&gt; &#xA;&lt;h3&gt;Compile&lt;/h3&gt; &#xA;&lt;p&gt;You have to install the rust first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd rust-proxy&#xA;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You could get the release in the target/release.&lt;/p&gt; &#xA;&lt;h3&gt;Download the release&lt;/h3&gt; &#xA;&lt;p&gt;Download the release from the &lt;a href=&#34;https://github.com/lsk569937453/silverwind/releases&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Config Introduction&lt;/h2&gt; &#xA;&lt;h3&gt;Silverwind as the http proxy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;- listen_port: 9969&#xA;  service_config:&#xA;    server_type: HTTP&#xA;    routes:&#xA;    - matcher:&#xA;        prefix: /&#xA;        prefix_rewrite: ssss&#xA;      route_cluster:&#xA;        type: RandomRoute&#xA;        routes:&#xA;        - base_route:&#xA;            endpoint: http://localhost:8888/&#xA;            try_file: null&#xA;        - base_route:&#xA;            endpoint: http://localhost:9999/&#xA;            try_file: null&#xA;        - base_route:&#xA;            endpoint: http://localhost:7777/&#xA;            try_file: null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The proxy will listen the 9969 port and forward the traffic to the &lt;a href=&#34;http://localhost:8888/,http://localhost:9999/.http://localhost:7777/&#34;&gt;http://localhost:8888/,http://localhost:9999/.http://localhost:7777/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Silverwind as the tcp proxy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;- listen_port: 4486&#xA;  service_config:&#xA;    server_type: TCP&#xA;    routes:&#xA;    - matcher:&#xA;        prefix: &#34;/&#34;&#xA;        prefix_rewrite: ssss&#xA;      route_cluster:&#xA;        type: RandomRoute&#xA;        routes:&#xA;        - base_route:&#xA;            endpoint: httpbin.org:443&#xA;            try_file: null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup:&lt;/h3&gt; &#xA;&lt;h4&gt;Windows Startup&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$env:CONFIG_FILE_PATH=&#39;D:\code\app_config.yaml&#39;; .\rust-proxy.exe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you could start without the config file like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.\rust-proxy.exe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Rest Api&lt;/h2&gt; &#xA;&lt;h3&gt;Change the routes&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;POST /appConfig HTTP/1.1&#xA;Host: 127.0.0.1:8870&#xA;Content-Type: application/json&#xA;Content-Length: 1752&#xA;&#xA;[&#xA;    {&#xA;        &#34;listen_port&#34;: 4486,&#xA;        &#34;service_config&#34;: {&#xA;            &#34;server_type&#34;: &#34;HTTP&#34;,&#xA;            &#34;cert_str&#34;: null,&#xA;            &#34;key_str&#34;: null,&#xA;            &#34;routes&#34;: [&#xA;                {&#xA;                    &#34;matcher&#34;: {&#xA;                        &#34;prefix&#34;: &#34;ss&#34;,&#xA;                        &#34;prefix_rewrite&#34;: &#34;ssss&#34;&#xA;                    },&#xA;                    &#34;allow_deny_list&#34;: null,&#xA;                    &#34;route_cluster&#34;: {&#xA;                        &#34;type&#34;: &#34;WeightBasedRoute&#34;,&#xA;                        &#34;routes&#34;: [&#xA;                            {&#xA;                                &#34;base_route&#34;: {&#xA;                                    &#34;endpoint&#34;: &#34;http://localhost:10000&#34;,&#xA;                                    &#34;try_file&#34;: null&#xA;                                }&#xA;                            }&#xA;                        ]&#xA;                    }&#xA;                },&#xA;                {&#xA;                    &#34;matcher&#34;: {&#xA;                        &#34;prefix&#34;: &#34;sst&#34;,&#xA;                        &#34;prefix_rewrite&#34;: &#34;ssss&#34;&#xA;                    },&#xA;                    &#34;allow_deny_list&#34;: null,&#xA;                    &#34;route_cluster&#34;: {&#xA;                        &#34;type&#34;: &#34;WeightBasedRoute&#34;,&#xA;                        &#34;routes&#34;: [&#xA;                            {&#xA;                                &#34;base_route&#34;: {&#xA;                                    &#34;endpoint&#34;: &#34;http://localhost:9898&#34;,&#xA;                                    &#34;try_file&#34;: null&#xA;                                }&#xA;                            }&#xA;                        ]&#xA;                    }&#xA;                }&#xA;            ]&#xA;        }&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get the appConfig&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET /appConfig HTTP/1.1&#xA;Host: 127.0.0.1:8870&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Update the routes&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;PUT /route HTTP/1.1&#xA;Host: 127.0.0.1:8870&#xA;Content-Type: application/json&#xA;Content-Length: 629&#xA;&#xA;{&#xA;    &#34;route_id&#34;: &#34;90c66439-5c87-4902-aebb-1c2c9443c154&#34;,&#xA;    &#34;host_name&#34;: null,&#xA;    &#34;matcher&#34;: {&#xA;        &#34;prefix&#34;: &#34;/&#34;,&#xA;        &#34;prefix_rewrite&#34;: &#34;ssss&#34;&#xA;    },&#xA;    &#34;allow_deny_list&#34;: null,&#xA;    &#34;authentication&#34;: null,&#xA;    &#34;anomaly_detection&#34;: null,&#xA;    &#34;liveness_config&#34;: null,&#xA;    &#34;health_check&#34;: null,&#xA;    &#34;ratelimit&#34;: null,&#xA;    &#34;route_cluster&#34;: {&#xA;        &#34;type&#34;: &#34;RandomRoute&#34;,&#xA;        &#34;routes&#34;: [&#xA;            {&#xA;                &#34;base_route&#34;: {&#xA;                    &#34;endpoint&#34;: &#34;http://127.0.0.1:10000&#34;,&#xA;                    &#34;try_file&#34;: null,&#xA;                    &#34;is_alive&#34;: null&#xA;                }&#xA;            }&#xA;        ]&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete the route&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;DELETE /route/90c66439-5c87-4902-aebb-1c2c9443c154 HTTP/1.1&#xA;Host: 127.0.0.1:8870&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span id=&#34;api-gateway&#34;&gt;The Base Function in Api Gateway&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lsk569937453/image_repo/main/api-gateway.png&#34; alt=&#34;alt tag&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Silverwind has implemented the following functions:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IP Allow-and-Deny list&lt;/li&gt; &#xA; &lt;li&gt;Authentication(Basic Auth,ApiKey Auth)&lt;/li&gt; &#xA; &lt;li&gt;Rate limiting(Token Bucket,Fixed Window)&lt;/li&gt; &#xA; &lt;li&gt;Routing&lt;/li&gt; &#xA; &lt;li&gt;Load Balancing(Poll,Random,Weight,Header Based)&lt;/li&gt; &#xA; &lt;li&gt;HealthCheck&amp;amp;AnomalyDetection&lt;/li&gt; &#xA; &lt;li&gt;Free Https Certificate&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Configuration(Rest Api)&lt;/li&gt; &#xA; &lt;li&gt;Dashboard For Silverwind&lt;/li&gt; &#xA; &lt;li&gt;Monitoring(Prometheus)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Future&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Protocol Translation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Caching&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>supabase/edge-runtime</title>
    <updated>2023-04-14T01:44:53Z</updated>
    <id>tag:github.com,2023-04-14:/supabase/edge-runtime</id>
    <link href="https://github.com/supabase/edge-runtime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A server based on Deno runtime, capable of running JavaScript, TypeScript, and WASM services.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Supabase Edge Runtime&lt;/h1&gt; &#xA;&lt;p&gt;A web server based on &lt;a href=&#34;https://deno.land&#34;&gt;Deno&lt;/a&gt; runtime, capable of running JavaScript, TypeScript, and WASM services. Edge Runtime is built and maintained by the &lt;a href=&#34;https://supabase.io&#34;&gt;Supabase team&lt;/a&gt;. For more details, read the &lt;a href=&#34;https://supabase.com/blog/edge-runtime-self-hosted-deno-functions&#34;&gt;intro blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use it to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Locally test and self-host Supabase&#39;s Edge Functions (or any Deno Function)&lt;/li&gt; &#xA; &lt;li&gt;As a programmable HTTP Proxy: You can intercept / route HTTP requests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING: Beta Software. There will be breaking changes to APIs / Configuration Options&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/supabase/edge-runtime/raw/main/edge-runtime-diagram.svg?raw=true&#34; alt=&#34;Sequence diagram of Edge Runtime request flow&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The edge runtime can be divided into two runtimes with different purposes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Main runtime: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An instance for the &lt;em&gt;main runtime&lt;/em&gt; is responsible for proxying the transactions to the &lt;em&gt;user runtime&lt;/em&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The main runtime is meant to be an entry point before running user functions, where you can authentication, etc. before calling the user function.&lt;/li&gt; &#xA;   &lt;li&gt;Has no user-facing limits&lt;/li&gt; &#xA;   &lt;li&gt;Has access to all environment variables.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;User runtime: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;An instance for the &lt;em&gt;user runtime&lt;/em&gt; is responsible for executing users&#39; code.&lt;/li&gt; &#xA;   &lt;li&gt;Limits are required to be set such as: Memory and Timeouts.&lt;/li&gt; &#xA;   &lt;li&gt;Has access to environment variables explictly allowed by the main runtime.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to run locally&lt;/h2&gt; &#xA;&lt;p&gt;To serve all functions in the examples folder on port 9000, you can do this with the &lt;a href=&#34;https://raw.githubusercontent.com/supabase/edge-runtime/main/examples/main/index.ts&#34;&gt;example main service&lt;/a&gt; provided with this repo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./run.sh start --main-service ./examples/main -p 9000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Test by calling the &lt;a href=&#34;https://raw.githubusercontent.com/supabase/edge-runtime/main/examples/hello-world/index.ts&#34;&gt;hello world function&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl --request POST &#39;http://localhost:9000/hello-world&#39; \&#xA;--header &#39;Content-Type: application/json&#39; \&#xA;--data-raw &#39;{&#xA;    &#34;name&#34;: &#34;John Doe&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run with a different entry point, you can pass a different main service like below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./run.sh start --main-service /path/to/main-service-directory -p 9000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;using Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t edge-runtime .&#xA;docker run -it --rm -p 9000:9000 -v /path/to/supabase/functions:/usr/services supabase/edge-runtime start --main-service /usr/services&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to run tests&lt;/h2&gt; &#xA;&lt;p&gt;Read about running tests &lt;a href=&#34;https://github.com/supabase/edge-runtime/raw/main/testing.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to update to a newer Deno version&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Select the Deno version to upgrade and visit its tag on GitHub (eg: &lt;a href=&#34;https://github.com/denoland/deno/raw/v1.30.3/Cargo.toml&#34;&gt;https://github.com/denoland/deno/blob/v1.30.3/Cargo.toml&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;Cargo.toml&lt;/code&gt; at the root of of this repo and modify all &lt;code&gt;deno_*&lt;/code&gt; modules to match to the selected tag of Deno.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Supabase Edge Runtime!&lt;/p&gt; &#xA;&lt;p&gt;To get started either open an issue on &lt;a href=&#34;https://github.com/supabase/edge-runtime/issues&#34;&gt;GitHub&lt;/a&gt; or drop us a message on &lt;a href=&#34;https://discord.com/invite/R7bSpeBSJE&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Edge Runtime follows Supabase&#39;s &lt;a href=&#34;https://github.com/supabase/.github/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>