<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-02T01:44:44Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZhangHanDong/prompt-description-language</title>
    <updated>2023-06-02T01:44:44Z</updated>
    <id>tag:github.com,2023-06-02:/ZhangHanDong/prompt-description-language</id>
    <link href="https://github.com/ZhangHanDong/prompt-description-language" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Prompt Description Language [POC]&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Prompt Description Language (V0.1.1 POC)&lt;/h1&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;PDL (Prompt Description Language) format provides an extensible way to describe the behavior and characteristics of prompts. Inspired by &lt;code&gt;json/yaml/toml/markdown&lt;/code&gt; designs, PDL aims to minimize the token count compared to &lt;code&gt;json/yaml/toml/markdown&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Basic Syntax Description&lt;/h2&gt; &#xA;&lt;p&gt;The basic rules of the PDL structure are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;{}&lt;/code&gt; represents a structure, and &lt;code&gt;.&lt;/code&gt; is also used to express hierarchical structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@&lt;/code&gt; represents a reference to a specified structure field.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;import&lt;/code&gt; signifies the importation of the structure referenced by &lt;code&gt;@&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;Key: Value&lt;/code&gt; key-value pairs, the Key generally does not need to be enclosed in quotes (unless it contains other special characters, in which case single or double quotes can be used). Value can take the following forms: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Key: &#34;value&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Key: [&#34;v1&#34;, &#34;v2&#34;]&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Key: {k1: &#39;v1&#39;, k2: &#39;v2&#39;, k3: &#39;v3&#39;, ...}&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Key: {&#39;1/3&#39;: &#39;v1&#39;, &#39;2/3&#39;: &#39;v2&#39;, &#39;3/3&#39;: &#39;v3&#39;}&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Key: &#34;&amp;lt;Value&amp;gt;&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Key: &#34;&amp;lt;Value&amp;gt;&#34; / None&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Key [ v1, v2, ...]&lt;/code&gt;, used to define a sequence.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage Case`&lt;/h2&gt; &#xA;&lt;p&gt;Example：&lt;a href=&#34;https://github.com/Illumine-Labs/Mr.trans&#34;&gt;Mr.Trans&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;Despite its advantages in design, PDL still has some potential drawbacks or limitations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Complexity: The syntax of PDL can be relatively complex, especially for those unfamiliar with programming or scripting languages. This may make it difficult to create and modify PDL files, especially for complex model behaviors.&lt;/li&gt; &#xA; &lt;li&gt;Lack of documentation and tutorials: Since PDL is a specialized language that is not widely used, there may be a lack of sufficient tutorials and documentation, which can increase the difficulty for new users to learn and use it.&lt;/li&gt; &#xA; &lt;li&gt;Scalability and flexibility: Although PDL is designed to be an extensible language, it may still encounter situations where it cannot accommodate certain specific needs, especially when dealing with uncommon or unusual AI model behaviors.&lt;/li&gt; &#xA; &lt;li&gt;Tool support: Due to PDL not being a widely adopted language, there may be a lack of corresponding development and debugging tools, which can impact development efficiency and quality.&lt;/li&gt; &#xA; &lt;li&gt;Readability and maintainability: If a PDL file becomes too large or its structure becomes overly complex, it can affect its readability and maintainability. This is particularly true when lacking proper documentation and comments, which can make subsequent maintenance work difficult.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Based on actual use cases and requirements, there may be additional specific challenges and issues that need to be addressed.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Kudaes/EPI</title>
    <updated>2023-06-02T01:44:44Z</updated>
    <id>tag:github.com,2023-06-02:/Kudaes/EPI</id>
    <link href="https://github.com/Kudaes/EPI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Process injection through entry points hijacking.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EPI&lt;/h1&gt; &#xA;&lt;p&gt;EPI (Entry Point Injection) is a tool that leverages a new threadless process injection technique that relies on hijacking loaded dll&#39;s &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/win32/dlls/dllmain&#34;&gt;entry points&lt;/a&gt;. To achieve this goal, EPI patches the target process&#39; PEB such that one of the already loaded dll&#39;s entry point is redirected to a injected shellcode (which by default is the Loader previously converted to sRDI). Once a new thread is naturally spawned by the process or whenever a running thread exits, all loaded modules&#39; entry points will be called which includes our injected shellcode.&lt;/p&gt; &#xA;&lt;p&gt;Since we want the target process to continue its execution smoothly, generally speaking it is a bad idea to run our payload direcly on the thread that is calling the hijacked entry point. For example, the direct execution of a C2 becon would completely hijack the thread which would surely lead to the program crash in case that the application is expecting the thread to perform a certain task. To deal with this situation, EPI by default does not directly inject the desired payload but a custom Loader, which is a regular dll converted to &lt;a href=&#34;https://github.com/monoxgas/sRDI&#34;&gt;sRDI&lt;/a&gt;. The Loader has embedded the encrypted final payload (for example, the previously commented C2 beacon), and its main task is to decrypt, allocate and run this payload in a stable way. To achieve the execution keeping the &#34;threadless&#34; nature of the technique, the Loader will use the process&#39; thread pool to run the payload by calling &lt;a href=&#34;https://learn.microsoft.com/es-es/windows/win32/api/threadpoollegacyapiset/nf-threadpoollegacyapiset-queueuserworkitem&#34;&gt;QueueUserWorkItem&lt;/a&gt;. The use of QueueUserWorkItem ensures that, even in the case that a new thread is spawned (it depends on the thread pool availability), the start routine&#39;s address will never point to our payload avoiding that particular IOC.&lt;/p&gt; &#xA;&lt;p&gt;Before exiting, the Loader restores the PEB and other modified structures to their previous state, preventing the multiple execution of our payload and allowing the process to continue its normal execution.&lt;/p&gt; &#xA;&lt;p&gt;By default, this tool hijacks &lt;code&gt;kernelbase.dll&lt;/code&gt; entry point. Feel free to target a different dll, but make sure that the dll is loaded in both processes involved in this activity.&lt;/p&gt; &#xA;&lt;p&gt;The provided shellcode embbeded in the Loader spawns a new &lt;code&gt;cmd.exe /k msg &#34;hello from kudaes&#34;&lt;/code&gt; process.&lt;/p&gt; &#xA;&lt;p&gt;The advantages of this technique are the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Both threadless or threaded execution, at will.&lt;/li&gt; &#xA; &lt;li&gt;No hooking.&lt;/li&gt; &#xA; &lt;li&gt;No generation of private memory regions on well known dll&#39;s RX memory pages.&lt;/li&gt; &#xA; &lt;li&gt;No RWX memory permissions required.&lt;/li&gt; &#xA; &lt;li&gt;The targeted process can continue its regular execution.&lt;/li&gt; &#xA; &lt;li&gt;No new threads with a start address pointing to our shellcode.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Compilation&lt;/h1&gt; &#xA;&lt;p&gt;Since we are using &lt;a href=&#34;https://github.com/anvie/litcrypt.rs&#34;&gt;LITCRYPT&lt;/a&gt; plugin to obfuscate string literals, it is required to set up the environment variable LITCRYPT_ENCRYPT_KEY before compiling the code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI&amp;gt; set LITCRYPT_ENCRYPT_KEY=&#34;setarandomkeyeachtime&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, depending on how you want to use the tool we have three diferent compilation processes.&lt;/p&gt; &#xA;&lt;h2&gt;Use the tool as it is provided&lt;/h2&gt; &#xA;&lt;p&gt;In this case, you just need to compile the EPI project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI&amp;gt; cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, run the tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -h &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;No Loader - Custom payload&lt;/h2&gt; &#xA;&lt;p&gt;If you just want to directly execute your custom shellcode without using the Loader, you have to replace the value of the &lt;code&gt;bytes&lt;/code&gt; variable (&lt;code&gt;EPI::src::main.rs:13&lt;/code&gt;) with the hexadecimal content of your payload. Then, compile the project and run the tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI&amp;gt; cargo build --release&#xA;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -h &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be aware that, depending on the behaviour of your shellcode, you might end up hijacking the thread and potentially causing a process crash.&lt;/p&gt; &#xA;&lt;h2&gt;Loader &amp;amp; Custom payload&lt;/h2&gt; &#xA;&lt;p&gt;This is my recommended choice, since it allows you to fully customize the execution in the most reliable way. This is the right option if you want to run a different payload than the one provided and use the functionality of the Loader to avoid the crash of the target process.&lt;/p&gt; &#xA;&lt;p&gt;First, you have to replace the value of the &lt;code&gt;bytes&lt;/code&gt; variable in the Loader (&lt;code&gt;Loader::src::lib.rs:17&lt;/code&gt;) with the hexadecimal content of your payload. Then, compile the project as usual:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\Loader&amp;gt; cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, use the provided Python script &lt;code&gt;ConvertToShellcode.py&lt;/code&gt; to convert the generated &lt;code&gt;loader.dll&lt;/code&gt; into sRDI. I&#39;ve obtained this script from the fantastic &lt;a href=&#34;https://github.com/monoxgas/sRDI/tree/master&#34;&gt;sRDI&lt;/a&gt; project after fixing some &lt;a href=&#34;https://github.com/monoxgas/sRDI/pull/32&#34;&gt;issues&lt;/a&gt; that were generating multi-hour long delays.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\sRDI&amp;gt; python3 ConvertToShellcode.py -f run loader.dll&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This execution should generate a &lt;code&gt;loader.bin&lt;/code&gt; file. Again, get its hex content and use it to replace the value of the &lt;code&gt;bytes&lt;/code&gt; variable in the EPI project (&lt;code&gt;EPI::src::main.rs:13&lt;/code&gt;). Finally, compile EPI and run the tool:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI&amp;gt; cargo build --release&#xA;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -h &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;The basic usage is by passing to the tool the PID of the target process and waiting for a thread to spawn/exit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -p 1337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In case that you need to enable the &lt;code&gt;DEBUG&lt;/code&gt; privilege to perform the injection, you can use the flag &lt;code&gt;-d&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -p 1337 -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not want to wait until a new thread is naturally spawned, you can use the flag &lt;code&gt;-f&lt;/code&gt; to spawn a new dummy thread. This dummy thread will run &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-exitthread&#34;&gt;ExitThread&lt;/a&gt; (i.e. it&#39;s a self destructing thread), but before that will call every single loaded module&#39;s entry point, including our shellcode. The good part of this is that despite making the technique threaded, the new spawned thread&#39;s initial routine will point to ExitThread and not to our injected shellcode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -p 1337 -f&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can also force the execution of the injected shellcode by sending a &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/win32/winmsg/wm-quit&#34;&gt;WM_QUIT&lt;/a&gt; message to ALL threads of the target process. If there is any thread listening for this kind of messages it will exit itself by calling ExitThread, which internally calls every loaded module&#39;s entry point to allow them to uninitialize and free resources. In this scenario, our shellcode will be executed as well. &lt;strong&gt;BE AWARE&lt;/strong&gt; that this most likely will &#34;terminate&#34; the process, meaning that the user won&#39;t be able to interact with it anymore although the shellcode execution will continue in the background. This method is not recommended to run any long-term payload.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\Users\User\Desktop\EPI\EPI\target\release&amp;gt; epi.exe -p 1337 -s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Tips&lt;/h1&gt; &#xA;&lt;p&gt;If you want to exploit the threadless nature of this technique, you need to chose wisely the target process. The best processes are those with user interaction, since they are constantly creating and destroying threads.&lt;/p&gt; &#xA;&lt;p&gt;To test EPI, I like to target my favourite text editor: Sublime Text. Besides the fact that I love it, it&#39;s also very simple to force it to spawn a new thread, allowing me to easily test EPI. If you want to do it as well, just follow these simple steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run Sublime Text.&lt;/li&gt; &#xA; &lt;li&gt;Inject on it using EPI&#39;s basic usage.&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;File&#34; -&amp;gt; &#34;Open File&#34;. This will create a new thread and your shellcode will be executed.&lt;/li&gt; &#xA; &lt;li&gt;Keep using Sublime to verify that the process continues to run normally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kudaes/EPI/main/images/sublime1.png&#34; alt=&#34;Sublime Text injection.&#34; title=&#34;Sublime Text injection.&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case that you want to test the execution of the shellcode when a thread exits, you can do so as well with Sublime Text this way:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run Sublime Text.&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;File&#34; -&amp;gt; &#34;Open File&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Inject your shellcode using EPI&#39;s basic usage.&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;Cancel&#34; to exit the previously generated thread. Your shellcode will be executed by the terminating thread.&lt;/li&gt; &#xA; &lt;li&gt;Keep using Sublime to verify that the process continues to run normally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Actually, you could also just wait for a minute or less since most of this kind of apps are constantly creating new threads in the background even without any user interaction.&lt;/p&gt; &#xA;&lt;h1&gt;TODO&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clean memory artifacts.&lt;/li&gt; &#xA; &lt;li&gt;Test other sRDI generators.&lt;/li&gt; &#xA; &lt;li&gt;Allow to target other dll than kernelbase.dll.&lt;/li&gt; &#xA; &lt;li&gt;Indirect syscalls and other maldev stuff.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/monoxgas&#34;&gt;monoxgas&lt;/a&gt; for the astonishing &lt;a href=&#34;https://github.com/monoxgas/sRDI&#34;&gt;sRDI&lt;/a&gt; project that I have leveraged to convert the Loader dll into PIC.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/memN0ps&#34;&gt;memN0ps&lt;/a&gt; for the hard work and all the effort shown in improving &lt;a href=&#34;https://github.com/memN0ps/srdi-rs&#34;&gt;srdi-rs&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lancedb/lance</title>
    <updated>2023-06-02T01:44:44Z</updated>
    <id>tag:github.com,2023-06-02:/lancedb/lance</id>
    <link href="https://github.com/lancedb/lance" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modern columnar data format for ML and LLMs implemented in Rust. Convert from parquet in 2 lines of code for 100x faster random access, vector index, and data versioning. Compatible with Pandas, DuckDB, Polars, Pyarrow, with more integrations coming..&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;257&#34; alt=&#34;Lance Logo&#34; src=&#34;https://user-images.githubusercontent.com/917119/199353423-d3e202f7-0269-411d-8ff2-e747e419e492.png&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;&lt;strong&gt;Modern columnar data format for ML. Convert from parquet in 2-lines of code for 100x faster random access, a vector index, data versioning, and more.&lt;br&gt;&lt;/strong&gt; &lt;strong&gt;Compatible with pandas, duckdb, polars, pyarrow, with more integrations on the way.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://lancedb.github.io/lance/&#34;&gt;Documentation&lt;/a&gt; • &lt;a href=&#34;https://blog.lancedb.com/&#34;&gt;Blog&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/zMM32dvNtd&#34;&gt;Discord&lt;/a&gt; • &lt;a href=&#34;https://twitter.com/lancedb&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/lancedb/lance/actions/workflows/rust.yml&#34;&gt;&lt;img src=&#34;https://github.com/lancedb/lance/actions/workflows/rust.yml/badge.svg?sanitize=true&#34; alt=&#34;CI Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lancedb.github.io/lance/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-passing-brightgreen&#34; alt=&#34;Docs Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/lance&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/lance.svg?sanitize=true&#34; alt=&#34;crates.io badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pylance/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pylance&#34; alt=&#34;Python versions badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Lance is a modern columnar data format that is optimized for ML workflows and datasets. Lance is perfect for:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Building search engines and features stores.&lt;/li&gt; &#xA; &lt;li&gt;Large-scale ML training requiring high performance IO and shuffles.&lt;/li&gt; &#xA; &lt;li&gt;Storing, querying, and inspecting deeply nested data for robotics or large blobs like images, point-clouds, and more.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The key features of Lance include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;High-performance random access:&lt;/strong&gt; 100x faster than Parquet without sacrificing scan performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector search:&lt;/strong&gt; find nearest neighbors in milliseconds and combine OLAP-queries with vector search.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-copy, automatic versioning:&lt;/strong&gt; manage versions of your data without needing extra infrastructure.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ecosystem integrations:&lt;/strong&gt; Apache-Arrow, Pandas, Polars, DuckDB and more on the way.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install pylance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Converting to Lance&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import lance&#xA;&#xA;import pandas as pd&#xA;import pyarrow as pa&#xA;import pyarrow.dataset&#xA;&#xA;df = pd.DataFrame({&#34;a&#34;: [5], &#34;b&#34;: [10]})&#xA;uri = &#34;/tmp/test.parquet&#34;&#xA;tbl = pa.Table.from_pandas(df)&#xA;pa.dataset.write_dataset(tbl, uri, format=&#39;parquet&#39;)&#xA;&#xA;parquet = pa.dataset.dataset(uri, format=&#39;parquet&#39;)&#xA;lance.write_dataset(parquet, &#34;/tmp/test.lance&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reading Lance data&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = lance.dataset(&#34;/tmp/test.lance&#34;)&#xA;assert isinstance(dataset, pa.dataset.Dataset)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = dataset.to_table().to_pandas()&#xA;df&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;DuckDB&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import duckdb&#xA;&#xA;# If this segfaults, make sure you have duckdb v0.7+ installed&#xA;duckdb.query(&#34;SELECT * FROM dataset LIMIT 10&#34;).to_df()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector search&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the sift1m subset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz&#xA;tar -xzf sift.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Convert it to Lance&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import lance&#xA;from lance.vector import vec_to_table&#xA;import numpy as np&#xA;import struct&#xA;&#xA;nvecs = 1000000&#xA;ndims = 128&#xA;with open(&#34;sift/sift_base.fvecs&#34;, mode=&#34;rb&#34;) as fobj:&#xA;    buf = fobj.read()&#xA;    data = np.array(struct.unpack(&#34;&amp;lt;128000000f&#34;, buf[4 : 4 + 4 * nvecs * ndims])).reshape((nvecs, ndims))&#xA;    dd = dict(zip(range(nvecs), data))&#xA;&#xA;table = vec_to_table(dd)&#xA;uri = &#34;vec_data.lance&#34;&#xA;sift1m = lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the index&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sift1m.create_index(&#34;vector&#34;,&#xA;                    index_type=&#34;IVF_PQ&#34;, &#xA;                    num_partitions=256,  # IVF&#xA;                    num_sub_vectors=16)  # PQ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Search the dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get top 10 similar vectors&#xA;import duckdb&#xA;&#xA;dataset = lance.dataset(uri)&#xA;&#xA;# Sample 100 query vectors. If this segfaults, make sure you have duckdb v0.7+ installed&#xA;sample = duckdb.query(&#34;SELECT vector FROM dataset USING SAMPLE 100&#34;).to_df()&#xA;query_vectors = np.array([np.array(x) for x in sample.vector])&#xA;&#xA;# Get nearest neighbors for all of them&#xA;rs = [dataset.to_table(nearest={&#34;column&#34;: &#34;vector&#34;, &#34;k&#34;: 10, &#34;q&#34;: q})      &#xA;      for q in query_vectors]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Directory structure&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Directory&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lancedb/lance/main/rust&#34;&gt;rust&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Core Rust implementation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lancedb/lance/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python bindings (pyo3)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lancedb/lance/main/docs&#34;&gt;docs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Documentation source&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;What makes Lance different&lt;/h2&gt; &#xA;&lt;p&gt;Here we will highlight a few aspects of Lance’s design. For more details, see the full &lt;a href=&#34;https://lancedb.github.io/lance/format.html&#34;&gt;Lance design document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector index&lt;/strong&gt;: Vector index for similarity search over embedding space&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Encodings&lt;/strong&gt;: to achieve both fast columnar scan and sub-linear point queries, Lance uses custom encodings and layouts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nested fields&lt;/strong&gt;: Lance stores each subfield as a separate column to support efficient filters like “find images where detected objects include cats”.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Versioning&lt;/strong&gt;: a Manifest can be used to record snapshots. Currently we support creating new versions automatically via appends, overwrites, and index creation&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast updates&lt;/strong&gt; (ROADMAP): Updates will be supported via write-ahead logs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rich secondary indices&lt;/strong&gt; (ROADMAP):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inverted index for fuzzy search over many label / annotation fields&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;h3&gt;Vector search&lt;/h3&gt; &#xA;&lt;p&gt;We used the sift dataset to benchmark our results with 1M vectors of 128D&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For 100 randomly sampled query vectors, we get &amp;lt;1ms average response time (on a 2023 m2 macbook air)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lancedb/lance/main/docs/avg_latency.png&#34; alt=&#34;avg_latency.png&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;ANN is always a trade-off between recall and performance&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lancedb/lance/main/docs/recall_vs_latency.png&#34; alt=&#34;avg_latency.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Vs parquet&lt;/h3&gt; &#xA;&lt;p&gt;We create a Lance dataset using the Oxford Pet dataset to do some preliminary performance testing of Lance as compared to Parquet and raw image/xmls. For analytics queries, Lance is 50-100x better than reading the raw metadata. For batched random access, Lance is 100x better than both parquet and raw files.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lancedb/lance/main/docs/lance_perf.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why are you building yet another data format?!&lt;/h2&gt; &#xA;&lt;p&gt;Machine Learning development cycle involves the steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR&#xA;    A[Collection] --&amp;gt; B[Exploration];&#xA;    B --&amp;gt; C[Analytics];&#xA;    C --&amp;gt; D[Feature Engineer];&#xA;    D --&amp;gt; E[Training];&#xA;    E --&amp;gt; F[Evaluation];&#xA;    F --&amp;gt; C;&#xA;    E --&amp;gt; G[Deployment];&#xA;    G --&amp;gt; H[Monitoring];&#xA;    H --&amp;gt; A;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;People use different data representations to varying stages for the performance or limited by the tooling available. The academia mainly uses XML / JSON for annotations and zipped images/sensors data for deep learning, which is difficult to integrated into data infrastructure and slow to train over cloud storage. While the industry uses data lake (Parquet-based techniques, i.e., Delta Lake, Iceberg) or data warehouse (AWS Redshift or Google BigQuery) to collect and analyze data, they have to convert the data into training-friendly formats, such as &lt;a href=&#34;https://github.com/eto-ai/rikai&#34;&gt;Rikai&lt;/a&gt;/&lt;a href=&#34;https://github.com/uber/petastorm&#34;&gt;Petastorm&lt;/a&gt; or &lt;a href=&#34;https://www.tensorflow.org/tutorials/load_data/tfrecord&#34;&gt;Tfrecord&lt;/a&gt;. Multiple single-purpose data transforms, as well as syncing copies between cloud storage to local training instances have become a common practice among ML practices.&lt;/p&gt; &#xA;&lt;p&gt;While each of the existing data formats excel at its original designed workload, we need a new data format to tailored for multistage ML development cycle to reduce the fraction in tools and data silos.&lt;/p&gt; &#xA;&lt;p&gt;A comparison of different data formats in each stage of ML development cycle.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Lance&lt;/th&gt; &#xA;   &lt;th&gt;Parquet &amp;amp; ORC&lt;/th&gt; &#xA;   &lt;th&gt;JSON &amp;amp; XML&lt;/th&gt; &#xA;   &lt;th&gt;Tfrecord&lt;/th&gt; &#xA;   &lt;th&gt;Database&lt;/th&gt; &#xA;   &lt;th&gt;Warehouse&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Analytics&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Slow&lt;/td&gt; &#xA;   &lt;td&gt;Slow&lt;/td&gt; &#xA;   &lt;td&gt;Decent&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Feature Engineering&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Decent&lt;/td&gt; &#xA;   &lt;td&gt;Slow&lt;/td&gt; &#xA;   &lt;td&gt;Decent&lt;/td&gt; &#xA;   &lt;td&gt;Good&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Decent&lt;/td&gt; &#xA;   &lt;td&gt;Slow&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Exploration&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Slow&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Slow&lt;/td&gt; &#xA;   &lt;td&gt;Fast&lt;/td&gt; &#xA;   &lt;td&gt;Decent&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Infra Support&lt;/td&gt; &#xA;   &lt;td&gt;Rich&lt;/td&gt; &#xA;   &lt;td&gt;Rich&lt;/td&gt; &#xA;   &lt;td&gt;Decent&lt;/td&gt; &#xA;   &lt;td&gt;Limited&lt;/td&gt; &#xA;   &lt;td&gt;Rich&lt;/td&gt; &#xA;   &lt;td&gt;Rich&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Community Highlights&lt;/h2&gt; &#xA;&lt;p&gt;Lance is currently used in production by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lancedb/lancedb&#34;&gt;LanceDB&lt;/a&gt;, a serverless, low-latency vector database for ML applications&lt;/li&gt; &#xA; &lt;li&gt;Self-driving car company for large-scale storage, retrieval and processing of multi-modal data.&lt;/li&gt; &#xA; &lt;li&gt;E-commerce company for billion-scale+ vector personalized search.&lt;/li&gt; &#xA; &lt;li&gt;and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Presentations and Talks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1a4nAiQAkPDBtOfXFpPg7lbeDAxcNDVKgoUkw3cUs2rE/edit#slide=id.p&#34;&gt;Lance: A New Columnar Data Format&lt;/a&gt;, &lt;a href=&#34;https://www.scipy2022.scipy.org/posters&#34;&gt;Scipy 2022, Austin, TX&lt;/a&gt;. July, 2022.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>