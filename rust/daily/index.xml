<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-06T01:43:50Z</updated>
  <subtitle>Daily Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JasonWei512/code-radio-cli</title>
    <updated>2023-05-06T01:43:50Z</updated>
    <id>tag:github.com,2023-05-06:/JasonWei512/code-radio-cli</id>
    <link href="https://github.com/JasonWei512/code-radio-cli" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üéµ A command line music radio client for https://coderadio.freecodecamp.org, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Code Radio CLI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/code-radio-cli&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/code-radio-cli.svg?color=orange&#34; alt=&#34;Crate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JasonWei512/code-radio-cli/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/JasonWei512/code-radio-cli&#34; alt=&#34;GitHub release (latest by date)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üéµ 24/7 music designed for coding, now in your terminal!&lt;/p&gt; &#xA;&lt;p&gt;A command line music radio client for &lt;a href=&#34;https://coderadio.freecodecamp.org&#34;&gt;https://coderadio.freecodecamp.org&lt;/a&gt; (&lt;a href=&#34;https://www.freecodecamp.org/news/code-radio-24-7/&#34;&gt;about&lt;/a&gt;), written in Rust.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JasonWei512/code-radio-cli/develop/.github/images/screenshot.jpg&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download prebuilt binary from &lt;a href=&#34;https://github.com/JasonWei512/code-radio-cli/releases&#34;&gt;GitHub release page&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ü™ü On Windows, install with &lt;a href=&#34;https://github.com/microsoft/winget-cli&#34;&gt;WinGet&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;winget install code-radio-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ü¶Ä Install with &lt;a href=&#34;https://rustup.rs/&#34;&gt;Cargo&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cargo install code-radio-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After installation, run &lt;code&gt;code-radio&lt;/code&gt; in command line to start. You may need to restart your shell first.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚ö† About building and running on Linux üêß&lt;/h3&gt; &#xA;&lt;p&gt;This program uses &lt;a href=&#34;https://github.com/rustaudio/cpal&#34;&gt;rustaudio/cpal&lt;/a&gt; lib to play audio, which requires ALSA development files on Linux.&lt;/p&gt; &#xA;&lt;p&gt;In order to build and run this program on Linux, you need to installÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;libasound2-dev&lt;/code&gt; on Debian / Ubuntu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;alsa-lib-devel&lt;/code&gt; on Fedora&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;code-radio [OPTIONS]&#xA;&#xA;OPTIONS:&#xA;    -h, --help                 Print help information&#xA;    -n, --no-logo              Do not display logo&#xA;    -s, --select-station       Manually select a station&#xA;    -v, --volume &amp;lt;VOLUME&amp;gt;      Volume, between 0 and 9 [default: 9]&#xA;    -V, --version              Print version information&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>rustformers/llm</title>
    <updated>2023-05-06T01:43:50Z</updated>
    <id>tag:github.com,2023-05-06:/rustformers/llm</id>
    <link href="https://github.com/rustformers/llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run inference for Large Language Models on CPU, with Rust ü¶ÄüöÄü¶ô&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llm&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llm/main/doc/resources/logo2.png&#34; alt=&#34;A llama riding a crab, AI-generated&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Image by &lt;a href=&#34;https://github.com/darthdeus/&#34;&gt;@darthdeus&lt;/a&gt;, using Stable Diffusion&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/llm.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://shields.io/badge/license-MIT%2FApache--2.0-blue&#34; alt=&#34;MIT/Apache2&#34;&gt; &lt;a href=&#34;https://discord.gg/YB9WaXYAWU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1085885067601137734&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llm&lt;/code&gt; is a Rust ecosystem of libraries for running inference on large language models, inspired by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary crate is the &lt;code&gt;llm&lt;/code&gt; crate, which wraps &lt;code&gt;llm-base&lt;/code&gt; and supported model crates. This is used by &lt;code&gt;llm-cli&lt;/code&gt; to provide inference for all supported models.&lt;/p&gt; &#xA;&lt;p&gt;It is powered by the &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;&lt;code&gt;ggml&lt;/code&gt;&lt;/a&gt; tensor library, and aims to bring the robustness and ease of use of Rust to the world of large language models.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have a Rust 1.65.0 or above and C toolchain[^1] set up.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llm&lt;/code&gt; is a Rust library that re-exports &lt;code&gt;llm-base&lt;/code&gt; and the model crates (e.g. &lt;code&gt;bloom&lt;/code&gt;, &lt;code&gt;gpt2&lt;/code&gt; &lt;code&gt;llama&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llm-cli&lt;/code&gt; (binary name &lt;code&gt;llm&lt;/code&gt;) is a basic application that provides a CLI interface to the library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: For best results, make sure to build and run in release mode. Debug builds are going to be very slow.&lt;/p&gt; &#xA;&lt;h3&gt;Building using &lt;code&gt;cargo&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo install --git https://github.com/rustformers/llm llm-cli&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to install &lt;code&gt;llm&lt;/code&gt; to your Cargo &lt;code&gt;bin&lt;/code&gt; directory, which &lt;code&gt;rustup&lt;/code&gt; is likely to have added to your &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The CLI application can then be run through &lt;code&gt;llm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llm/main/doc/resources/llama_gif.gif&#34; alt=&#34;Gif showcasing language generation using llm&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Building from repository&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository and then build it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone --recurse-submodules git@github.com:rustformers/llm.git&#xA;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting binary will be at &lt;code&gt;target/release/llm[.exe]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It can also be run directly through Cargo, using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo run --release --bin llm -- &amp;lt;ARGS&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is useful for development.&lt;/p&gt; &#xA;&lt;h3&gt;Getting models&lt;/h3&gt; &#xA;&lt;p&gt;GGML files are easy to acquire. Currently, the following models are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt2&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gptj&#34;&gt;GPT-J&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt_neox&#34;&gt;GPT-NeoX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/bloom&#34;&gt;BLOOM&lt;/a&gt; (partial support, results inconsistent)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Certain older GGML formats are not supported by this project, but the goal is to maintain feature parity with the upstream GGML project. For problems relating to loading models, or requesting support for &lt;a href=&#34;https://github.com/ggerganov/ggml#roadmap&#34;&gt;supported GGML model types&lt;/a&gt;, please &lt;a href=&#34;https://github.com/rustformers/llm/issues/new&#34;&gt;open an Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;From Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;Hugging Face ü§ó is a leader in open-source machine learning and hosts hundreds of GGML models. &lt;a href=&#34;https://huggingface.co/models?search=ggml&#34;&gt;Search for GGML models on Hugging Face ü§ó&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;r/LocalLLaMA&lt;/h4&gt; &#xA;&lt;p&gt;This Reddit community maintains &lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/wiki/index/&#34;&gt;a wiki&lt;/a&gt; related to GGML models, including well organized lists of links for acquiring &lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/wiki/models/&#34;&gt;GGML models&lt;/a&gt; (mostly from Hugging Face ü§ó).&lt;/p&gt; &#xA;&lt;h4&gt;LLaMA original weights&lt;/h4&gt; &#xA;&lt;p&gt;Currently, the only legal source to get the original weights is &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/README.md#llama&#34;&gt;this repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After acquiring the weights, it is necessary to convert them into a format that is compatible with ggml. To achieve this, follow the steps outlined below:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;To run the Python scripts, a Python version of 3.9 or 3.10 is required. 3.11 is unsupported at the time of writing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Convert the model to f16 ggml format&#xA;python3 scripts/convert-pth-to-ggml.py /path/to/your/models/7B/ 1&#xA;&#xA;# Quantize the model to 4-bit ggml format&#xA;cargo run --bin llm llama quantize /path/to/your/models/7B/ggml-model-f16.bin /path/to/your/models/7B/ggml-model-q4_0.bin q4_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp repository&lt;/a&gt; has additional information on how to obtain and run specific models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Running&lt;/h3&gt; &#xA;&lt;p&gt;For example, try the following prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;llm llama infer -m &amp;lt;path&amp;gt;/ggml-model-q4_0.bin -p &#34;Tell me how cool the Rust programming language is:&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some additional things to try:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--help&lt;/code&gt; to see a list of available options.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have the &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;alpaca-lora&lt;/a&gt; weights, try &lt;code&gt;repl&lt;/code&gt; mode!&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;llm llama repl -m &amp;lt;path&amp;gt;/ggml-alpaca-7b-q4.bin -f examples/alpaca_prompt.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llm/main/doc/resources/alpaca_repl_screencap.gif&#34; alt=&#34;Gif showcasing alpaca repl mode&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Sessions can be loaded (&lt;code&gt;--load-session&lt;/code&gt;) or saved (&lt;code&gt;--save-session&lt;/code&gt;) to file. To automatically load and save the same session, use &lt;code&gt;--persist-session&lt;/code&gt;. This can be used to cache prompts to reduce load time, too:&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llm/main/doc/resources/prompt_caching_screencap.gif&#34; alt=&#34;Gif showcasing prompt caching&#34;&gt;&lt;/p&gt; &lt;p&gt;(This GIF shows an older version of the flags, but the mechanics are still the same.)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: A modern-ish C toolchain is required to compile &lt;code&gt;ggml&lt;/code&gt;. A C++ toolchain should not be necessary.&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# To build (This will take some time, go grab some coffee):&#xA;docker build -t llm .&#xA;&#xA;# To run with prompt:&#xA;docker run --rm --name llm -it -v ${PWD}/data:/data -v ${PWD}/examples:/examples llm llama infer -m data/gpt4all-lora-quantized-ggml.bin -p &#34;Tell me how cool the Rust programming language is:&#34;&#xA;&#xA;# To run with prompt file and repl (will wait for user input):&#xA;docker run --rm --name llm -it -v ${PWD}/data:/data -v ${PWD}/examples:/examples llm llama repl -m data/gpt4all-lora-quantized-ggml.bin -f examples/alpaca_prompt.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h3&gt;Why did you do this?&lt;/h3&gt; &#xA;&lt;p&gt;It was not my choice. Ferris appeared to me in my dreams and asked me to rewrite this in the name of the Holy crab.&lt;/p&gt; &#xA;&lt;h3&gt;Seriously now.&lt;/h3&gt; &#xA;&lt;p&gt;Come on! I don&#39;t want to get into a flame war. You know how it goes, &lt;em&gt;something something&lt;/em&gt; memory &lt;em&gt;something something&lt;/em&gt; cargo is nice, don&#39;t make me say it, everybody knows this already.&lt;/p&gt; &#xA;&lt;h3&gt;I insist.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Sheesh! Okaaay&lt;/em&gt;. After seeing the huge potential for &lt;strong&gt;llama.cpp&lt;/strong&gt;, the first thing I did was to see how hard would it be to turn it into a library to embed in my projects. I started digging into the code, and realized the heavy lifting is done by &lt;code&gt;ggml&lt;/code&gt; (a C library, easy to bind to Rust) and the whole project was just around ~2k lines of C++ code (not so easy to bind). After a couple of (failed) attempts to build an HTTP server into the tool, I realized I&#39;d be much more productive if I just ported the code to Rust, where I&#39;m more comfortable.&lt;/p&gt; &#xA;&lt;h3&gt;Is this the real reason?&lt;/h3&gt; &#xA;&lt;p&gt;Haha. Of course &lt;em&gt;not&lt;/em&gt;. I just like collecting imaginary internet points, in the form of little stars, that people seem to give to me whenever I embark on pointless quests for &lt;em&gt;rewriting X thing, but in Rust&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How is this different from &lt;code&gt;llama.cpp&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;p&gt;This is a reimplementation of &lt;code&gt;llama.cpp&lt;/code&gt; that does not share any code with it outside of &lt;code&gt;ggml&lt;/code&gt;. This was done for a variety of reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires a C++ compiler, which can cause problems for cross-compilation to more esoteric platforms. An example of such a platform is WebAssembly, which can require a non-standard compiler SDK.&lt;/li&gt; &#xA; &lt;li&gt;Rust is easier to work with from a development and open-source perspective; it offers better tooling for writing &#34;code in the large&#34; with many other authors. Additionally, we can benefit from the larger Rust ecosystem with ease.&lt;/li&gt; &#xA; &lt;li&gt;We would like to make &lt;code&gt;ggml&lt;/code&gt; an optional backend (see &lt;a href=&#34;https://github.com/rustformers/llm/issues/31&#34;&gt;this issue&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, we hope to build a solution for model inferencing that is as easy to use and deploy as any other Rust crate.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>optiv/Freeze.rs</title>
    <updated>2023-05-06T01:43:50Z</updated>
    <id>tag:github.com,2023-05-06:/optiv/Freeze.rs</id>
    <link href="https://github.com/optiv/Freeze.rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Freeze.rs is a payload toolkit for bypassing EDRs using suspended processes, direct syscalls written in RUST&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/Freeze.jpg&#34; height=&#34;310&#34; border=&#34;2px solid #555&#34;&gt; &lt;br&gt; Freeze.rs &lt;/h1&gt; &#xA;&lt;h3&gt;More Information&lt;/h3&gt; &#xA;&lt;p&gt;If you want to learn more about the techniques utilized in this framework, please take a look at &lt;a href=&#34;https://www.optiv.com/insights/source-zero/blog/sacrificing-suspended-processes&#34;&gt;SourceZero Blog&lt;/a&gt; and the &lt;a href=&#34;https://github.com/optiv/Freeze&#34;&gt;original tool&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Freeze.rs is a payload creation tool used for circumventing EDR security controls to execute shellcode in a stealthy manner. Freeze.rs utilizes multiple techniques to not only remove Userland EDR hooks, but to also execute shellcode in such a way that it circumvents other endpoint monitoring controls.&lt;/p&gt; &#xA;&lt;h3&gt;Creating A Suspended Process&lt;/h3&gt; &#xA;&lt;p&gt;When a process is created, Ntdll.dll is the first DLL that is loaded; this happens before any EDR DLLs are loaded. This means that there is a bit of a delay before an EDR can be loaded and start hooking and modifying the assembly of system DLLs. In looking at Windows syscalls in Ntdll.dll, we can see that nothing is hooked yet. If we create a process in a suspend state (one that is frozen in time), we can see that no other DLLs are loaded, except for Ntdll.dll. You can also see that no EDR DLLs are loaded, meaning that the syscalls located in Ntdll.dll are unmodified.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/Suspended_Process.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Address Space Layout Randomization&lt;/h3&gt; &#xA;&lt;p&gt;To use this clean suspended process to remove hooks from Freeze.rs loader, we need a way to programmatically find and read the clean suspended process&#39; memory. This is where address space layout randomization (ASLR) comes into play. ASLR is a security mechanism to prevent stack memory corruption-based vulnerabilities. ASLR randomizes the address space inside of a process, to ensure that all memory-mapped objects, the stack, the heap, and the executable program itself, are unique. Now, this is where it gets interesting because while ASLR works, it does not work for position-independent code such as DLLs. What happens with DLLs, (specifically known system DLLs) is that the address space is randomized once at boot time. This means that we don&#39;t need to enumerate a remote process information to find the base address of its ntdll.dll because it is the same in all processes, including the one that we control. Since the address of every DLL is the same place per boot, we can pull this information from our own process and never have to enumerate the suspended process to find the address.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/Base_Address.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;With this information, we can use the API ReadProcessMemory to read a process&#39; memory. This API call is commonly associated with the reading of LSASS as part of any credential-based attack; however, on its own it is inherently not malicious, especially if we are just reading an arbitrary section of memory. The only time ReadProcessMemory will be flagged as part of something suspicious is if you are reading something you shouldn&#39;t (like the contents of LSASS). EDR products should never flag the fact that ReadProcessMemory was called, as there are legitimate operational uses for this function and would result in many false positives.&lt;/p&gt; &#xA;&lt;p&gt;We can take this a step further by only reading a section of Ntdll.dll where all syscalls are stored - its .text section, rather than reading the entire DLL.&lt;/p&gt; &#xA;&lt;p&gt;Combining these elements, we can programmatically get a copy of the .text section of Ntdll.dll to overwrite our existing hooked .text section prior to executing shellcode.&lt;/p&gt; &#xA;&lt;h3&gt;ETW Patching&lt;/h3&gt; &#xA;&lt;p&gt;ETW utilizes built-in syscalls to generate this telemetry. Since ETW is also a native feature built into Windows, security products do not need to &#34;hook&#34; the ETW syscalls to access the information. As a result, to prevent ETW, Freeze.rs patches numerous ETW syscalls, flushing out the registers and returning the execution flow to the next instruction. Patching ETW is now default in all loaders.&lt;/p&gt; &#xA;&lt;h3&gt;Shellcode&lt;/h3&gt; &#xA;&lt;p&gt;Since only Ntdll.dll is restored, all subsequent calls to execute shellcode need to reside in Ntdll.dll. Using Rust&#39;s NTAPI Crate (note you can do this in other languages but in Rust, its quite easy to implement) we can define and call the NT syscalls needed to allocate, write, and protect the shellcode, effectively skipping the standard calls that are located in Kernel32.dll, and Kernelbase.dll, as these may still be hooked.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/Syscalls.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;With Rust&#39;s NTAPI crate, you can see that all these calls do not show up under ntdll.dll, however they do still exist with in the process.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/APIMonitor.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;As a result:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/Userland_EDR.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/optiv/Freeze.rs/main/Screenshots/Kernel_EDR.png&#34; border=&#34;2px solid #555&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Why Rust?&lt;/h2&gt; &#xA;&lt;p&gt;This started out a fun project to learn Rust and has grown into its own framework.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Freeze.rs was developed in Rust.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;If &lt;code&gt;Rust&lt;/code&gt; and &lt;code&gt;Rustup&lt;/code&gt; is not installed please install them. If you are compiling it from OSX or Linux sure you have the target &#34;x86_64-pc-windows-gnu&#34; added. To so run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rustup target add x86_64-pc-windows-gnu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once done you can compile Freeze.rs, run the following commands, or use the compiled binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From there the compiled version will be found in in target/release (note if you don&#39;t put &lt;code&gt;--release&lt;/code&gt; the file will be in target/debug/ )&lt;/p&gt; &#xA;&lt;h2&gt;Help&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;    ___________                                                      &#xA;    \_   _____/______   ____   ____ ________ ____     _______  ______&#xA;     |    __) \_  __ \_/ __ \_/ __ \\___   // __ \    \_  __ \/  ___/&#xA;     |     \   |  | \/\  ___/\  ___/ /    /\  ___/     |  | \/\___ \ &#xA;     \___  /   |__|    \___  &amp;gt;\___  &amp;gt;_____ \\___  &amp;gt; /\ |__|  /____  &amp;gt;&#xA;         \/                \/     \/      \/    \/  \/            \/    &#xA;                                        (@Tyl0us)&#xA;    Soon they will learn that revenge is a dish... best served COLD &amp;amp; Rusty...&#xA;    &#xA;     &#xA;&#xA;USAGE:&#xA;    Freeze-rs [FLAGS] [OPTIONS]&#xA;&#xA;FLAGS:&#xA;    -c, --console    Only for Binary Payloads - Generates verbose console information when the payload is executed. This&#xA;                     will disable the hidden window feature&#xA;    -h, --help       Prints help information&#xA;    -n, --noetw      Disables the ETW patching that prevents ETW events from being generated.&#xA;    -s, --sandbox    Enables sandbox evasion by checking:&#xA;                                 Is Endpoint joined to a domain?&#xA;                                 Does the Endpoint have more than 2 CPUs?&#xA;                                 Does the Endpoint have more than 4 gigs of RAM?&#xA;    -V, --version    Prints version information&#xA;&#xA;OPTIONS:&#xA;    -E, --Encrypt &amp;lt;ENCRYPT&amp;gt;    Encrypts the shellcode using either AES 256, ELZMA or RC4 encryption&#xA;    -I, --Input &amp;lt;INPUT&amp;gt;        Path to the raw 64-bit shellcode.&#xA;    -O, --Output &amp;lt;OUTPUT&amp;gt;      Name of output file (e.g. loader.exe or loader.dll). Depending on what file extension&#xA;                               defined will determine if Freeze makes a dll or exe.&#xA;    -p, --process &amp;lt;PROCESS&amp;gt;    The name of process to spawn. This process has to exist in C:\Windows\System32\. Example&#xA;                               &#39;notepad.exe&#39;  &#xA;    -e, --export &amp;lt;export&amp;gt;      Defines a custom export function name for any DLL.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Binary vs DLL&lt;/h2&gt; &#xA;&lt;p&gt;Freeze.rs can generate either a &lt;code&gt;.exe&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; file. To specify this, ensure that the &lt;code&gt;-O&lt;/code&gt; command line option ends with either a &lt;code&gt;.exe&lt;/code&gt; for binaries or &lt;code&gt;.dll&lt;/code&gt; for dlls. No other file types are currently supported. In the case of DLL files, Freeze.rs can also add additional export functionality. To do this use the &lt;code&gt;-export&lt;/code&gt; with specific export function name.&lt;/p&gt; &#xA;&lt;h2&gt;Encryption&lt;/h2&gt; &#xA;&lt;p&gt;Encrypting shellcode is an important technique used to protect it from being detected and analyzed by EDRs and other security products. Freeze.rs comes with multiple methods to encrypt shellcode, these include AES, ELZMA, and RC4.&lt;/p&gt; &#xA;&lt;h3&gt;AES&lt;/h3&gt; &#xA;&lt;p&gt;AES (Advanced Encryption Standard) is a symmetric encryption algorithm that is widely used to encrypt data. Freeze.rs uses AES-256 bit size to encrypt the shellcode. The advantage of using AES to encrypt shellcode is that it provides strong encryption and is widely supported by cryptographic libraries. However, the use of a fixed block size can make it vulnerable to certain attacks, such as the padding oracle attack.&lt;/p&gt; &#xA;&lt;h3&gt;ELZMA&lt;/h3&gt; &#xA;&lt;p&gt;ELZMA is a compression and encryption algorithm that is often used in malware to obfuscate the code. To encrypt shellcode using ELZMA, the shellcode is first compressed using the ELZMA algorithm. The compressed data is then encrypted using a random key. The encrypted data and the key are then embedded in the exploit code. The advantage of using ELZMA to encrypt shellcode is that it provides both compression and encryption in a single algorithm. This can help to reduce the size of the exploit code and make it more difficult to detect.&lt;/p&gt; &#xA;&lt;h3&gt;RC4&lt;/h3&gt; &#xA;&lt;p&gt;RC4 is a symmetric encryption algorithm that is often used in malware to encrypt shellcode. It is a stream cipher that can use variable-length keys and is known for its simplicity and speed.&lt;/p&gt; &#xA;&lt;h2&gt;Console&lt;/h2&gt; &#xA;&lt;p&gt;Freeze.rs utilizes a technique to first create the process and then move it into the background. This does two things - first it helps keep the process hidden, and second, avoids being detected by any EDR product. Spawning a process right away in the background can be very suspicious and an indicator of maliciousness. Freeze.rs does this by calling the ‚ÄòGetConsoleWindow‚Äô and ‚ÄòShowWindow‚Äô Windows function after the process is created and the EDR‚Äôs hooks are loaded, and then changes the windows attributes to hidden.&lt;/p&gt; &#xA;&lt;p&gt;If the &lt;code&gt;-console&lt;/code&gt; command-line option is selected, Freeze.rs will not hide the process in the background. Instead, Freeze.rs will add several debug messages displaying what the loader is doing.&lt;/p&gt;</summary>
  </entry>
</feed>