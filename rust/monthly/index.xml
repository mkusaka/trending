<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-01T01:55:35Z</updated>
  <subtitle>Monthly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>prefix-dev/pixi</title>
    <updated>2024-09-01T01:55:35Z</updated>
    <id>tag:github.com,2024-09-01:/prefix-dev/pixi</id>
    <link href="https://github.com/prefix-dev/pixi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Package management made easy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt; &lt;a href=&#34;https://github.com/prefix-dev/pixi/&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source srcset=&#34;https://github.com/prefix-dev/pixi/assets/4995967/a3f9ff01-c9fb-4893-83c0-2a3f924df63e&#34; type=&#34;image/webp&#34;&gt; &#xA;   &lt;source srcset=&#34;https://github.com/prefix-dev/pixi/assets/4995967/e42739c4-4cd9-49bb-9d0a-45f8088494b5&#34; type=&#34;image/png&#34;&gt; &#xA;   &lt;img src=&#34;https://github.com/prefix-dev/pixi/assets/4995967/e42739c4-4cd9-49bb-9d0a-45f8088494b5&#34; alt=&#34;banner&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/h1&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD--3--Clause-blue?style=flat-square&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://github.com/prefix-dev/pixi/actions/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/prefix-dev/pixi/rust.yml?style=flat-square&amp;amp;branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/kKV8ZxyzY4&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1082332781146800168.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2&amp;amp;style=flat-square&#34; alt=&#34;Project Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pixi.sh&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/prefix-dev/pixi/main/assets/badge/v0.json&amp;amp;style=flat-square&#34; alt=&#34;Pixi Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/h1&gt; &#xA;&lt;h1&gt;pixi: Package Management Made Easy&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pixi&lt;/code&gt; is a cross-platform, multi-language package manager and workflow tool built on the foundation of the conda ecosystem. It provides developers with an exceptional experience similar to popular package managers like &lt;code&gt;cargo&lt;/code&gt; or &lt;code&gt;yarn&lt;/code&gt;, but for any language.&lt;/p&gt; &#xA;&lt;p&gt;Developed with ‚ù§Ô∏è at &lt;a href=&#34;https://prefix.dev&#34;&gt;prefix.dev&lt;/a&gt;. &lt;a href=&#34;https://asciinema.org/a/636482&#34;&gt;&lt;img src=&#34;https://github.com/prefix-dev/pixi/assets/12893423/0fc8f8c8-ac13-4c14-891b-dc613f25475b&#34; alt=&#34;Real-time pixi_demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports &lt;strong&gt;multiple languages&lt;/strong&gt; including Python, C++, and R using Conda packages. You can find available packages on &lt;a href=&#34;https://prefix.dev&#34;&gt;prefix.dev&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Compatible with all major operating systems: Linux, Windows, macOS (including Apple Silicon).&lt;/li&gt; &#xA; &lt;li&gt;Always includes an up-to-date &lt;strong&gt;lock file&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Provides a clean and simple Cargo-like &lt;strong&gt;command-line interface&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Allows you to install tools &lt;strong&gt;per-project&lt;/strong&gt; or &lt;strong&gt;system-wide&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Entirely written in &lt;strong&gt;Rust&lt;/strong&gt; and built on top of the &lt;strong&gt;&lt;a href=&#34;https://github.com/mamba-org/rattler&#34;&gt;rattler&lt;/a&gt;&lt;/strong&gt; library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://raw.githubusercontent.com/prefix-dev/pixi/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è &lt;a href=&#34;https://raw.githubusercontent.com/prefix-dev/pixi/main/examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://pixi.sh/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üòç &lt;a href=&#34;https://raw.githubusercontent.com/prefix-dev/pixi/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî® &lt;a href=&#34;https://raw.githubusercontent.com/prefix-dev/pixi/main/#built-using-pixi&#34;&gt;Built using Pixi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://github.com/prefix-dev/setup-pixi&#34;&gt;GitHub Action&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;Pixi is ready for production! We are working hard to keep file-format changes compatible with the previous versions so that you can rely on pixi with peace of mind.&lt;/p&gt; &#xA;&lt;p&gt;Some notable features we envision for upcoming releases are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build and publish&lt;/strong&gt; your project as a Conda package.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;strong&gt;dependencies from source&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;More powerful &#34;global installation&#34; of packages towards a deterministic setup of global packages on multiple machines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pixi&lt;/code&gt; can be installed on macOS, Linux, and Windows. The provided scripts will automatically download the latest version of &lt;code&gt;pixi&lt;/code&gt;, extract it, and move the &lt;code&gt;pixi&lt;/code&gt; binary to &lt;code&gt;~/.pixi/bin&lt;/code&gt;. If this directory does not exist, the script will create it.&lt;/p&gt; &#xA;&lt;h3&gt;macOS and Linux&lt;/h3&gt; &#xA;&lt;p&gt;To install Pixi on macOS and Linux, open a terminal and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -fsSL https://pixi.sh/install.sh | bash&#xA;# or with brew&#xA;brew install pixi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will also update your ~/.bash_profile to include ~/.pixi/bin in your PATH, allowing you to invoke the pixi command from anywhere. You might need to restart your terminal or source your shell for the changes to take effect.&lt;/p&gt; &#xA;&lt;p&gt;Starting with macOS Catalina &lt;a href=&#34;https://support.apple.com/en-us/102360&#34;&gt;zsh is the default login shell and interactive shell&lt;/a&gt;. Therefore, you might want to use &lt;code&gt;zsh&lt;/code&gt; instead of &lt;code&gt;bash&lt;/code&gt; in the install command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;curl -fsSL https://pixi.sh/install.sh | zsh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will also update your ~/.zshrc to include ~/.pixi/bin in your PATH, allowing you to invoke the pixi command from anywhere.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;To install Pixi on Windows, open a PowerShell terminal (you may need to run it as an administrator) and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;iwr -useb https://pixi.sh/install.ps1 | iex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will inform you once the installation is successful and add the ~/.pixi/bin directory to your PATH, which will allow you to run the pixi command from any location. Or with &lt;code&gt;winget&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;winget install prefix-dev.pixi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Autocompletion&lt;/h3&gt; &#xA;&lt;p&gt;To get autocompletion follow the instructions for your shell. Afterwards, restart the shell or source the shell config file.&lt;/p&gt; &#xA;&lt;h4&gt;Bash (default on most Linux systems)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;eval &#34;$(pixi completion --shell bash)&#34;&#39; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Zsh (default on macOS)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;echo &#39;eval &#34;$(pixi completion --shell zsh)&#34;&#39; &amp;gt;&amp;gt; ~/.zshrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;PowerShell (pre-installed on all Windows systems)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-pwsh&#34;&gt;Add-Content -Path $PROFILE -Value &#39;(&amp;amp; pixi completion --shell powershell) | Out-String | Invoke-Expression&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If this fails with &#34;Failure because no profile file exists&#34;, make sure your profile file exists. If not, create it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-PowerShell&#34;&gt;New-Item -Path $PROFILE -ItemType File -Force&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Fish&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fish&#34;&gt;echo &#39;pixi completion --shell fish | source&#39; &amp;gt;&amp;gt; ~/.config/fish/config.fish&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Nushell&lt;/h4&gt; &#xA;&lt;p&gt;Add the following to the end of your Nushell env file (find it by running &lt;code&gt;$nu.env-path&lt;/code&gt; in Nushell):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-nushell&#34;&gt;mkdir ~/.cache/pixi&#xA;pixi completion --shell nushell | save -f ~/.cache/pixi/completions.nu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And add the following to the end of your Nushell configuration (find it by running &lt;code&gt;$nu.config-path&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-nushell&#34;&gt;use ~/.cache/pixi/completions.nu *&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Elvish&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elv&#34;&gt;echo &#39;eval (pixi completion --shell elvish | slurp)&#39; &amp;gt;&amp;gt; ~/.elvish/rc.elv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Distro Packages&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repology.org/project/pixi/versions&#34;&gt;&lt;img src=&#34;https://repology.org/badge/vertical-allrepos/pixi.svg?sanitize=true&#34; alt=&#34;Packaging status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Arch Linux&lt;/h4&gt; &#xA;&lt;p&gt;You can install &lt;code&gt;pixi&lt;/code&gt; from the &lt;a href=&#34;https://archlinux.org/packages/extra/x86_64/pixi/&#34;&gt;extra repository&lt;/a&gt; using &lt;a href=&#34;https://wiki.archlinux.org/title/Pacman&#34;&gt;pacman&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pacman -S pixi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Alpine Linux&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;pixi&lt;/code&gt; is available for &lt;a href=&#34;https://pkgs.alpinelinux.org/packages?name=pixi&amp;amp;branch=edge&#34;&gt;Alpine Edge&lt;/a&gt;. It can be installed via &lt;a href=&#34;https://wiki.alpinelinux.org/wiki/Alpine_Package_Keeper&#34;&gt;apk&lt;/a&gt; after enabling the &lt;a href=&#34;https://wiki.alpinelinux.org/wiki/Repositories&#34;&gt;testing repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;apk add pixi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build/install from source&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pixi&lt;/code&gt; is 100% written in Rust and therefore it can be installed, built and tested with cargo. To start using &lt;code&gt;pixi&lt;/code&gt; from a source build run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo install --locked --git https://github.com/prefix-dev/pixi.git pixi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We don&#39;t publish to &lt;code&gt;crates.io&lt;/code&gt; anymore, so you need to install it from the repository. The reason for this is that we depend on some unpublished crates which disallows us to publish to &lt;code&gt;crates.io&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;or when you want to make changes use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo build&#xA;cargo test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have any issues building because of the dependency on &lt;code&gt;rattler&lt;/code&gt; checkout it&#39;s &lt;a href=&#34;https://github.com/mamba-org/rattler/tree/main#give-it-a-try&#34;&gt;compile steps&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Uninstall&lt;/h2&gt; &#xA;&lt;p&gt;To uninstall the pixi binary should be removed. Delete &lt;code&gt;pixi&lt;/code&gt; from the &lt;code&gt;$PIXI_DIR&lt;/code&gt; which is default to &lt;code&gt;~/.pixi/bin/pixi&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;So on Linux its:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;rm ~/.pixi/bin/pixi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and on Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$PIXI_BIN = &#34;$Env:LocalAppData\pixi\bin\pixi&#34;; Remove-Item -Path $PIXI_BIN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this command you can still use the tools you installed with &lt;code&gt;pixi&lt;/code&gt;. To remove these as well just remove the whole &lt;code&gt;~/.pixi&lt;/code&gt; directory and remove the directory from your path.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;The cli looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;‚ûú pixi&#xA;A package management and workflow tool&#xA;&#xA;Usage: pixi [OPTIONS] &amp;lt;COMMAND&amp;gt;&#xA;&#xA;Commands:&#xA;  completion  Generates a completion script for a shell&#xA;  init        Creates a new project&#xA;  add         Adds a dependency to the project&#xA;  run         Runs task in project&#xA;  shell       Start a shell in the pixi environment of the project&#xA;  global      Global is the main entry point for the part of pixi that executes on the global(system) level&#xA;  auth        Login to prefix.dev or anaconda.org servers to access private channels&#xA;  install     Install all dependencies&#xA;  task        Command management in project&#xA;  info        Information about the system and project&#xA;  upload      Upload a package to a prefix.dev channel&#xA;  search      Search a package, output will list the latest version of package&#xA;  project&#xA;  help        Print this message or the help of the given subcommand(s)&#xA;&#xA;Options:&#xA;  -v, --verbose...     More output per occurrence&#xA;  -q, --quiet...       Less output per occurrence&#xA;      --color &amp;lt;COLOR&amp;gt;  Whether the log needs to be colored [default: auto] [possible values: always, never, auto]&#xA;  -h, --help           Print help&#xA;  -V, --version        Print version&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Creating a pixi project&lt;/h2&gt; &#xA;&lt;p&gt;Initialize a new project and navigate to the project directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pixi init myproject&#xA;cd myproject&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add the dependencies you want to use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pixi add cowpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the installed package in its environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pixi run cowpy &#34;Thanks for using pixi&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Activate a shell in the environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pixi shell&#xA;cowpy &#34;Thanks for using pixi&#34;&#xA;exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installing a conda package globally&lt;/h2&gt; &#xA;&lt;p&gt;You can also globally install conda packages into their own environment. This behavior is similar to &lt;a href=&#34;https://github.com/pypa/pipx&#34;&gt;&lt;code&gt;pipx&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://github.com/mariusvniekerk/condax&#34;&gt;&lt;code&gt;condax&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pixi global install cowpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Use in GitHub Actions&lt;/h2&gt; &#xA;&lt;p&gt;You can use pixi in GitHub Actions to install dependencies and run commands. It supports automatic caching of your environments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;- uses: prefix-dev/setup-pixi@v0.6.0&#xA;- run: pixi run cowpy &#34;Thanks for using pixi&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://pixi.sh/latest/advanced/github_actions&#34;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;contributing&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing üòç&lt;/h2&gt; &#xA;&lt;p&gt;We would absolutely love for you to contribute to &lt;code&gt;pixi&lt;/code&gt;! Whether you want to start an issue, fix a bug you encountered, or suggest an improvement, every contribution is greatly appreciated.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re just getting started with our project or stepping into the Rust ecosystem for the first time, we&#39;ve got your back! We recommend beginning with issues labeled as &lt;code&gt;good first issue&lt;/code&gt;. These are carefully chosen tasks that provide a smooth entry point into contributing.These issues are typically more straightforward and are a great way to get familiar with the project.&lt;/p&gt; &#xA;&lt;p&gt;Got questions or ideas, or just want to chat? Join our lively conversations on Discord. We&#39;re very active and would be happy to welcome you to our community. &lt;a href=&#34;https://discord.gg/kKV8ZxyzY4&#34;&gt;Join our discord server today!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;pixibuilt&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Built using pixi&lt;/h2&gt; &#xA;&lt;p&gt;To see what&#39;s being built with &lt;code&gt;pixi&lt;/code&gt; check out the &lt;a href=&#34;https://raw.githubusercontent.com/prefix-dev/pixi/main/docs/Community.md&#34;&gt;Community&lt;/a&gt; page.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dandavison/delta</title>
    <updated>2024-09-01T01:55:35Z</updated>
    <id>tag:github.com,2024-09-01:/dandavison/delta</id>
    <link href="https://github.com/dandavison/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A syntax-highlighting pager for git, diff, grep, and blame output&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;400px&#34; src=&#34;https://user-images.githubusercontent.com/52205/147996902-9829bd3f-cd33-466e-833e-49a6f3ebd623.png&#34; alt=&#34;image&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/dandavison/delta/actions&#34;&gt; &lt;img src=&#34;https://github.com/dandavison/delta/workflows/Continuous%20Integration/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/dandavison/delta?branch=main&#34;&gt; &lt;img src=&#34;https://coveralls.io/repos/github/dandavison/delta/badge.svg?branch=main&#34; alt=&#34;Coverage Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://gitter.im/dandavison-delta/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt; &lt;img src=&#34;https://badges.gitter.im/dandavison-delta/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dandavison.github.io/delta/installation.html&#34;&gt;Install it&lt;/a&gt; (the package is called &#34;git-delta&#34; in most package managers, but the executable is just &lt;code&gt;delta&lt;/code&gt;) and add this to your &lt;code&gt;~/.gitconfig&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-gitconfig&#34;&gt;[core]&#xA;    pager = delta&#xA;&#xA;[interactive]&#xA;    diffFilter = delta --color-only&#xA;&#xA;[delta]&#xA;    navigate = true    # use n and N to move between diff sections&#xA;&#xA;    # delta detects terminal colors automatically; set one of these to disable auto-detection&#xA;    # dark = true&#xA;    # light = true&#xA;&#xA;[merge]&#xA;    conflictstyle = diff3&#xA;&#xA;[diff]&#xA;    colorMoved = default&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Delta has many features and is very customizable; please see the &lt;a href=&#34;https://dandavison.github.io/delta/&#34;&gt;user manual&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Language syntax highlighting with the same syntax-highlighting themes as &lt;a href=&#34;https://github.com/sharkdp/bat#readme&#34;&gt;bat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Word-level diff highlighting using a Levenshtein edit inference algorithm&lt;/li&gt; &#xA; &lt;li&gt;Side-by-side view with line-wrapping&lt;/li&gt; &#xA; &lt;li&gt;Line numbering&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;n&lt;/code&gt; and &lt;code&gt;N&lt;/code&gt; keybindings to move between files in large diffs, and between diffs in &lt;code&gt;log -p&lt;/code&gt; views (&lt;code&gt;--navigate&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Improved merge conflict display&lt;/li&gt; &#xA; &lt;li&gt;Improved &lt;code&gt;git blame&lt;/code&gt; display (syntax highlighting; &lt;code&gt;--hyperlinks&lt;/code&gt; formats commits as links to hosting provider etc. Supported hosting providers are: GitHub, GitLab, SourceHut, Codeberg)&lt;/li&gt; &#xA; &lt;li&gt;Syntax-highlights grep output from &lt;code&gt;rg&lt;/code&gt;, &lt;code&gt;git grep&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, etc&lt;/li&gt; &#xA; &lt;li&gt;Support for Git&#39;s &lt;code&gt;--color-moved&lt;/code&gt; feature.&lt;/li&gt; &#xA; &lt;li&gt;Code can be copied directly from the diff (&lt;code&gt;-/+&lt;/code&gt; markers are removed by default).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;diff-highlight&lt;/code&gt; and &lt;code&gt;diff-so-fancy&lt;/code&gt; emulation modes&lt;/li&gt; &#xA; &lt;li&gt;Commit hashes can be formatted as terminal &lt;a href=&#34;https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda&#34;&gt;hyperlinks&lt;/a&gt; to the hosting provider page (&lt;code&gt;--hyperlinks&lt;/code&gt;). File paths can also be formatted as hyperlinks for opening in your OS.&lt;/li&gt; &#xA; &lt;li&gt;Stylable box/line decorations to draw attention to commit, file and hunk header sections.&lt;/li&gt; &#xA; &lt;li&gt;Style strings (foreground color, background color, font attributes) are supported for &amp;gt;20 stylable elements, using the same color/style language as git&lt;/li&gt; &#xA; &lt;li&gt;Handles traditional unified diff output in addition to git output&lt;/li&gt; &#xA; &lt;li&gt;Automatic detection of light/dark terminal background&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;A syntax-highlighting pager for git, diff, and grep output&lt;/h2&gt; &#xA;&lt;p&gt;Code evolves, and we all spend time studying diffs. Delta aims to make this both efficient and enjoyable: it allows you to make extensive changes to the layout and styling of diffs, as well as allowing you to stay arbitrarily close to the default git/diff output.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img width=&#34;400px&#34; src=&#34;https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png&#34; alt=&#34;image&#34;&gt; &lt;br&gt; &lt;sub&gt;delta with &lt;code&gt;line-numbers&lt;/code&gt; activated&lt;/sub&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img width=&#34;800px&#34; src=&#34;https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png&#34; alt=&#34;image&#34;&gt; &lt;br&gt; &lt;sub&gt;delta with &lt;code&gt;side-by-side&lt;/code&gt; and &lt;code&gt;line-numbers&lt;/code&gt; activated&lt;/sub&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here&#39;s what &lt;code&gt;git show&lt;/code&gt; can look like with git configured to use delta:&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img width=&#34;500px&#34; style=&#34;border: 1px solid black&#34; src=&#34;https://user-images.githubusercontent.com/52205/81058545-a5725f80-8e9c-11ea-912e-d21954586a44.png&#34; alt=&#34;image&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img width=&#34;500px&#34; style=&#34;border: 1px solid black&#34; src=&#34;https://user-images.githubusercontent.com/52205/81058911-6abcf700-8e9d-11ea-93be-e212824ec03d.png&#34; alt=&#34;image&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#34;Dracula&#34; theme &lt;/td&gt; &#xA;   &lt;td&gt; &#34;GitHub&#34; theme &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Syntax-highlighting themes&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;All the syntax-highlighting color themes that are available with &lt;a href=&#34;https://github.com/sharkdp/bat/&#34;&gt;bat&lt;/a&gt; are available with delta:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img width=&#34;400px&#34; style=&#34;border: 1px solid black&#34; src=&#34;https://user-images.githubusercontent.com/52205/149431273-e3ad049d-771e-4186-869d-0e57967958a6.png&#34; alt=&#34;image&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img width=&#34;400px&#34; style=&#34;border: 1px solid black&#34; src=&#34;https://user-images.githubusercontent.com/52205/149431419-48836001-2afc-4fd0-97ad-561a69b71db7.png&#34; alt=&#34;image&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;code&gt;delta --show-syntax-themes --dark&lt;/code&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;code&gt;delta --show-syntax-themes --light&lt;/code&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Side-by-side view&lt;/h3&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://dandavison.github.io/delta/side-by-side-view.html&#34;&gt;User manual&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-gitconfig&#34;&gt;[delta]&#xA;    side-by-side = true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, side-by-side view has line-numbers activated, and has syntax highlighting in both the left and right panels: [&lt;a href=&#34;https://raw.githubusercontent.com/dandavison/delta/main/#side-by-side-view-1&#34;&gt;config&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img width=&#34;800px&#34; src=&#34;https://user-images.githubusercontent.com/52205/87230973-412eb900-c381-11ea-8aec-cc200290bd1b.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Side-by-side view wraps long lines automatically:&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img width=&#34;600px&#34; src=&#34;https://user-images.githubusercontent.com/52205/139064537-f8479504-16d3-429a-b4f6-d0122438adaa.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Line numbers&lt;/h3&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://dandavison.github.io/delta/line-numbers.html&#34;&gt;User manual&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-gitconfig&#34;&gt;[delta]&#xA;    line-numbers = true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img width=&#34;400px&#34; src=&#34;https://user-images.githubusercontent.com/52205/86275526-76792100-bba1-11ea-9e78-6be9baa80b29.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Merge conflicts&lt;/h3&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://dandavison.github.io/delta/merge-conflicts.html&#34;&gt;User manual&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img width=&#34;500px&#34; src=&#34;https://user-images.githubusercontent.com/52205/144783121-bb549100-69d8-41b8-ac62-1704f1f7b43e.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Git blame&lt;/h3&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://dandavison.github.io/delta/git-blame.html&#34;&gt;User manual&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img width=&#34;600px&#34; src=&#34;https://user-images.githubusercontent.com/52205/141891376-1fdb87dc-1d9c-4ad6-9d72-eeb19a8aeb0b.png&#34; alt=&#34;image&#34;&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Ripgrep, git grep&lt;/h3&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://dandavison.github.io/delta/grep.html&#34;&gt;User manual&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;img width=&#34;600px&#34; alt=&#34;image&#34; src=&#34;https://github.com/dandavison/open-in-editor/assets/52205/d203d380-5acb-4296-aeb9-e38c73d6c27f&#34;&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation and usage&lt;/h3&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://dandavison.github.io/delta/&#34;&gt;user manual&lt;/a&gt; and &lt;code&gt;delta --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Maintainers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dandavison&#34;&gt;@dandavison&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/th1000s&#34;&gt;@th1000s&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>tracel-ai/burn</title>
    <updated>2024-09-01T01:55:35Z</updated>
    <id>tag:github.com,2024-09-01:/tracel-ai/burn</id>
    <link href="https://github.com/tracel-ai/burn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Burn is a new comprehensive dynamic Deep Learning Framework built using Rust with extreme flexibility, compute efficiency and portability as its primary goals.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp&#34; width=&#34;350px&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/uPEBbYYDB6&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;amp;&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/burn&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/burn.svg?sanitize=true&#34; alt=&#34;Current Crates.io Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/burn&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/msrv/burn&#34; alt=&#34;Minimum Supported Rust Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://burn.dev/docs/burn&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tracel-ai/burn/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;Test Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/tracel-ai/burn&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/tracel-ai/burn/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;CodeCov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://runblaze.dev&#34;&gt;&lt;img src=&#34;https://runblaze.dev/gh/114041730602611213183421653564341667516/badge.svg?sanitize=true&#34; alt=&#34;Blaze&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://shields.io/badge/license-MIT%2FApache--2.0-blue&#34; alt=&#34;license&#34;&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;strong&gt;Burn is a new comprehensive dynamic Deep Learning Framework built using Rust &lt;br&gt; with extreme flexibility, compute efficiency and portability as its primary goals.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;h2&gt;Performance&lt;/h2&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-blazingly-fast.png&#34; height=&#34;96px&#34;&gt; &#xA;  &lt;p&gt;Because we believe the goal of a deep learning framework is to convert computation into useful intelligence, we have made performance a core pillar of Burn. We strive to achieve top efficiency by leveraging multiple optimization techniques described below.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;strong&gt;Click on each section for more details&lt;/strong&gt; üëá&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Automatic kernel fusion üí• &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Using Burn means having your models optimized on any backend. When possible, we provide a way to automatically and dynamically create custom kernels that minimize data relocation between different memory spaces, extremely useful when moving memory is the bottleneck.&lt;/p&gt; &#xA;  &lt;p&gt;As an example, you could write your own GELU activation function with the high level tensor api (see Rust code snippet below).&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn gelu_custom&amp;lt;B: Backend, const D: usize&amp;gt;(x: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {&#xA;    let x = x.clone() * ((x / SQRT_2).erf() + 1);&#xA;    x / 2&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Then, at runtime, a custom low-level kernel will be automatically created for your specific implementation and will rival a handcrafted GPU implementation. The kernel consists of about 60 lines of WGSL &lt;a href=&#34;%22https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/%22&#34;&gt;WebGPU Shading Language&lt;/a&gt;, an extremely verbose lower level shader language you probably don&#39;t want to program your deep learning models in!&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;As of now, our fusion strategy is only implemented for our own WGPU backend and supports only a subset of operations. We plan to add more operations very soon and extend this technique to other future in-house backends.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Asynchronous execution ‚ù§Ô∏è‚Äçüî• &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;For &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/#backends&#34;&gt;backends developed from scratch by the Burn team&lt;/a&gt;, an asynchronous execution style is used, which allows to perform various optimizations, such as the previously mentioned automatic kernel fusion.&lt;/p&gt; &#xA;  &lt;p&gt;Asynchronous execution also ensures that the normal execution of the framework does not block the model computations, which implies that the framework overhead won&#39;t impact the speed of execution significantly. Conversely, the intense computations in the model do not interfere with the responsiveness of the framework. For more information about our asynchronous backends, see &lt;a href=&#34;https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute&#34;&gt;this blog post&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Thread-safe building blocks ü¶û &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Burn emphasizes thread safety by leveraging the &lt;a href=&#34;https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html&#34;&gt;ownership system of Rust&lt;/a&gt;. With Burn, each module is the owner of its weights. It is therefore possible to send a module to another thread for computing the gradients, then send the gradients to the main thread that can aggregate them, and &lt;em&gt;voil√†&lt;/em&gt;, you get multi-device training.&lt;/p&gt; &#xA;  &lt;p&gt;This is a very different approach from what PyTorch does, where backpropagation actually mutates the &lt;em&gt;grad&lt;/em&gt; attribute of each tensor parameter. This is not a thread-safe operation and therefore requires lower level synchronization primitives, see &lt;a href=&#34;https://pytorch.org/docs/stable/distributed.html&#34;&gt;distributed training&lt;/a&gt; for reference. Note that this is still very fast, but not compatible across different backends and quite hard to implement.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Intelligent memory management ü¶Ä &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;One of the main roles of a deep learning framework is to reduce the amount of memory necessary to run models. The naive way of handling memory is that each tensor has its own memory space, which is allocated when the tensor is created then deallocated as the tensor gets out of scope. However, allocating and deallocating data is very costly, so a memory pool is often required to achieve good throughput. Burn offers an infrastructure that allows for easily creating and selecting memory management strategies for backends. For more details on memory management in Burn, see &lt;a href=&#34;https://burn.dev/blog/creating-high-performance-asynchronous-backends-with-burn-compute&#34;&gt;this blog post&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;Another very important memory optimization of Burn is that we keep track of when a tensor can be mutated in-place just by using the ownership system well. Even though it is a rather small memory optimization on its own, it adds up considerably when training or running inference with larger models and contributes to reduce the memory usage even more. For more information, see &lt;a href=&#34;https://burn.dev/blog/burn-rusty-approach-to-tensor-handling&#34;&gt;this blog post about tensor handling&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Automatic kernel selection üéØ &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;A good deep learning framework should ensure that models run smoothly on all hardware. However, not all hardware share the same behavior in terms of execution speed. For instance, a matrix multiplication kernel can be launched with many different parameters, which are highly sensitive to the size of the matrices and the hardware. Using the wrong configuration could reduce the speed of execution by a large factor (10 times or even more in extreme cases), so choosing the right kernels becomes a priority.&lt;/p&gt; &#xA;  &lt;p&gt;With our home-made backends, we run benchmarks automatically and choose the best configuration for the current hardware and matrix sizes with a reasonable caching strategy.&lt;/p&gt; &#xA;  &lt;p&gt;This adds a small overhead by increasing the warmup execution time, but stabilizes quickly after a few forward and backward passes, saving lots of time in the long run. Note that this feature isn&#39;t mandatory, and can be disabled when cold starts are a priority over optimized throughput.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Hardware specific features üî• &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;It is no secret that deep learning is mostly relying on matrix multiplication as its core operation, since this is how fully-connected neural networks are modeled.&lt;/p&gt; &#xA;  &lt;p&gt;More and more, hardware manufacturers optimize their chips specifically for matrix multiplication workloads. For instance, Nvidia has its &lt;em&gt;Tensor Cores&lt;/em&gt; and today most cellphones have AI specialized chips. As of this moment, we support Tensor Cores with our LibTorch and Candle backends, but not other accelerators yet. We hope &lt;a href=&#34;https://github.com/gpuweb/gpuweb/issues/4195&#34;&gt;this issue&lt;/a&gt; gets resolved at some point to bring support to our WGPU backend.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Custom Backend Extension üéí &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Burn aims to be the most flexible deep learning framework. While it&#39;s crucial to maintain compatibility with a wide variety of backends, Burn also provides the ability to extend the functionalities of a backend implementation to suit your personal modeling requirements.&lt;/p&gt; &#xA;  &lt;p&gt;This versatility is advantageous in numerous ways, such as supporting custom operations like flash attention or manually writing your own kernel for a specific backend to enhance performance. See &lt;a href=&#34;https://burn.dev/book/advanced/backend-extension/index.html&#34;&gt;this section&lt;/a&gt; in the Burn Book üî• for more details.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;br&gt; &#xA; &lt;h2&gt;Training &amp;amp; Inference&lt;/h2&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png&#34; height=&#34;96px&#34;&gt; &#xA;  &lt;p&gt;The whole deep learning workflow is made easy with Burn, as you can monitor your training progress with an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU clusters.&lt;/p&gt; &#xA;  &lt;p&gt;Burn was built from the ground up with training and inference in mind. It&#39;s also worth noting how Burn, in comparison to frameworks like PyTorch, simplifies the transition from training to deployment, eliminating the need for code changes.&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://www.youtube.com/watch?v=N9RM5CQbNQc&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png&#34; alt=&#34;Burn Train TUI&#34; width=&#34;75%&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;Click on the following sections to expand üëá&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Training Dashboard üìà &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;As you can see in the previous video (click on the picture!), a new terminal UI dashboard based on the &lt;a href=&#34;https://github.com/ratatui-org/ratatui&#34;&gt;Ratatui&lt;/a&gt; crate allows users to follow their training with ease without having to connect to any external application.&lt;/p&gt; &#xA;  &lt;p&gt;You can visualize your training and validation metrics updating in real-time and analyze the lifelong progression or recent history of any registered metrics using only the arrow keys. Break from the training loop without crashing, allowing potential checkpoints to be fully written or important pieces of code to complete without interruption üõ°&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; ONNX Support üê´ &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;ONNX (Open Neural Network Exchange) is an open-standard format that exports both the architecture and the weights of a deep learning model.&lt;/p&gt; &#xA;  &lt;p&gt;Burn supports the importation of models that follow the ONNX standard so you can easily port a model you have written in another framework like TensorFlow or PyTorch to Burn to benefit from all the advantages our framework offers.&lt;/p&gt; &#xA;  &lt;p&gt;Our ONNX support is further described in &lt;a href=&#34;https://burn.dev/book/import/onnx-model.html&#34;&gt;this section of the Burn Book üî•&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This crate is in active development and currently supports a &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-import/SUPPORTED-ONNX-OPS.md&#34;&gt;limited set of ONNX operators&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Importing PyTorch Models üöö &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Support for loading of PyTorch model weights into Burn‚Äôs native model architecture, ensuring seamless integration. See &lt;a href=&#34;https://burn.dev/book/import/pytorch-model.html&#34;&gt;Burn Book üî• section on importing PyTorch&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Inference in the Browser üåê &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Several of our backends can compile to Web Assembly: Candle and NdArray for CPU, and WGPU for GPU. This means that you can run inference directly within a browser. We provide several examples of this:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web&#34;&gt;MNIST&lt;/a&gt; where you can draw digits and a small convnet tries to find which one it is! 2Ô∏è‚É£ 7Ô∏è‚É£ üò∞&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web&#34;&gt;Image Classification&lt;/a&gt; where you can upload images and classify them! üåÑ&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Embedded: &lt;i&gt;no_std&lt;/i&gt; support ‚öôÔ∏è &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Burn&#39;s core components support &lt;a href=&#34;https://docs.rust-embedded.org/book/intro/no-std.html&#34;&gt;no_std&lt;/a&gt;. This means it can run in bare metal environment such as embedded devices without an operating system.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;As of now, only the NdArray backend can be used in a &lt;em&gt;no_std&lt;/em&gt; environment.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA; &lt;/details&gt; &#xA; &lt;br&gt; &#xA; &lt;h2&gt;Backends&lt;/h2&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png&#34; height=&#34;96px&#34;&gt; Burn strives to be as fast as possible on as many hardwares as possible, with robust implementations. We believe this flexibility is crucial for modern needs where you may train your models in the cloud, then deploy on customer hardwares, which vary from user to user. &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;Compared to other frameworks, Burn has a very different approach to supporting many backends. By design, most code is generic over the Backend trait, which allows us to build Burn with swappable backends. This makes composing backend possible, augmenting them with additional functionalities such as autodifferentiation and automatic kernel fusion.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;We already have many backends implemented, all listed below üëá&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; WGPU (WebGPU): Cross-Platform GPU Backend üåê &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;&lt;strong&gt;The go-to backend for running on any GPU.&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;p&gt;Based on the most popular and well-supported Rust graphics library, &lt;a href=&#34;https://wgpu.rs&#34;&gt;WGPU&lt;/a&gt;, this backend automatically targets Vulkan, OpenGL, Metal, Direct X11/12, and WebGPU, by using the WebGPU shading language &lt;a href=&#34;https://www.w3.org/TR/WGSL/https://www.w3.org/TR/WGSL/&#34;&gt;WGSL&lt;/a&gt;. It can also be compiled to Web Assembly to run in the browser while leveraging the GPU, see &lt;a href=&#34;https://antimora.github.io/image-classification/&#34;&gt;this demo&lt;/a&gt;. For more information on the benefits of this backend, see &lt;a href=&#34;https://burn.dev/blog/cross-platform-gpu-backend&#34;&gt;this blog&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;The WGPU backend is our first &#34;in-house backend&#34;, which means we have complete control over its implementation details. It is fully optimized with the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/#performance&#34;&gt;performance characteristics mentioned earlier&lt;/a&gt;, as it serves as our research playground for a variety of optimizations.&lt;/p&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-wgpu/README.md&#34;&gt;WGPU Backend README&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Candle: Backend using the Candle bindings üïØ &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Based on &lt;a href=&#34;https://github.com/huggingface/candle&#34;&gt;Candle by Hugging Face&lt;/a&gt;, a minimalist ML framework for Rust with a focus on performance and ease of use, this backend can run on CPU with support for Web Assembly or on Nvidia GPUs using CUDA.&lt;/p&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-candle/README.md&#34;&gt;Candle Backend README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;em&gt;Disclaimer:&lt;/em&gt; This backend is not fully completed yet, but can work in some contexts like inference.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; LibTorch: Backend using the LibTorch bindings üéÜ &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;PyTorch doesn&#39;t need an introduction in the realm of deep learning. This backend leverages &lt;a href=&#34;https://github.com/LaurentMazare/tch-rs&#34;&gt;PyTorch Rust bindings&lt;/a&gt;, enabling you to use LibTorch C++ kernels on CPU, CUDA and Metal.&lt;/p&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-tch/README.md&#34;&gt;LibTorch Backend README&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; NdArray: Backend using the NdArray primitive as data structure ü¶ê &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;This CPU backend is admittedly not our fastest backend, but offers extreme portability.&lt;/p&gt; &#xA;  &lt;p&gt;It is our only backend supporting &lt;em&gt;no_std&lt;/em&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-ndarray/README.md&#34;&gt;NdArray Backend README&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Autodiff: Backend decorator that brings backpropagation to any backend üîÑ &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Contrary to the aforementioned backends, Autodiff is actually a backend &lt;em&gt;decorator&lt;/em&gt;. This means that it cannot exist by itself; it must encapsulate another backend.&lt;/p&gt; &#xA;  &lt;p&gt;The simple act of wrapping a base backend with Autodiff transparently equips it with autodifferentiation support, making it possible to call backward on your model.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::backend::{Autodiff, Wgpu};&#xA;use burn::tensor::{Distribution, Tensor};&#xA;&#xA;fn main() {&#xA;    type Backend = Autodiff&amp;lt;Wgpu&amp;gt;;&#xA;&#xA;    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default);&#xA;    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default).require_grad();&#xA;&#xA;    let tmp = x.clone() + y.clone();&#xA;    let tmp = tmp.matmul(x);&#xA;    let tmp = tmp.exp();&#xA;&#xA;    let grads = tmp.backward();&#xA;    let y_grad = y.grad(&amp;amp;grads).unwrap();&#xA;    println!(&#34;{y_grad}&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Of note, it is impossible to make the mistake of calling backward on a model that runs on a backend that does not support autodiff (for inference), as this method is only offered by an Autodiff backend.&lt;/p&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-autodiff/README.md&#34;&gt;Autodiff Backend README&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Fusion: Backend decorator that brings kernel fusion to backends that support it üí• &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;This backend decorator enhances a backend with kernel fusion, provided that the inner backend supports it. Note that you can compose this backend with other backend decorators such as Autodiff. For now, only the WGPU backend has support for fused kernels.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::backend::{Autodiff, Fusion, Wgpu};&#xA;use burn::tensor::{Distribution, Tensor};&#xA;&#xA;fn main() {&#xA;    type Backend = Autodiff&amp;lt;Fusion&amp;lt;Wgpu&amp;gt;&amp;gt;;&#xA;&#xA;    let x: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default);&#xA;    let y: Tensor&amp;lt;Backend, 2&amp;gt; = Tensor::random([32, 32], Distribution::Default).require_grad();&#xA;&#xA;    let tmp = x.clone() + y.clone();&#xA;    let tmp = tmp.matmul(x);&#xA;    let tmp = tmp.exp();&#xA;&#xA;    let grads = tmp.backward();&#xA;    let y_grad = y.grad(&amp;amp;grads).unwrap();&#xA;    println!(&#34;{y_grad}&#34;);&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Of note, we plan to implement automatic gradient checkpointing based on compute bound and memory bound operations, which will work gracefully with the fusion backend to make your code run even faster during training, see &lt;a href=&#34;https://github.com/tracel-ai/burn/issues/936&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/crates/burn-fusion/README.md&#34;&gt;Fusion Backend README&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;br&gt; &#xA; &lt;h2&gt;Getting Started&lt;/h2&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png&#34; height=&#34;96px&#34;&gt; &#xA;  &lt;p&gt;Just heard of Burn? You are at the right place! Just continue reading this section and we hope you can get on board really quickly.&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; The Burn Book üî• &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;To begin working effectively with Burn, it is crucial to understand its key components and philosophy. This is why we highly recommend new users to read the first sections of &lt;a href=&#34;https://burn.dev/book/&#34;&gt;The Burn Book üî•&lt;/a&gt;. It provides detailed examples and explanations covering every facet of the framework, including building blocks like tensors, modules, and optimizers, all the way to advanced usage, like coding your own GPU kernels.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;The project is constantly evolving, and we try as much as possible to keep the book up to date with new additions. However, we might miss some details sometimes, so if you see something weird, let us know! We also gladly accept Pull Requests üòÑ&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Examples üôè &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Let&#39;s start with a code snippet that shows how intuitive the framework is to use! In the following, we declare a neural network module with some parameters along with its forward pass.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::nn;&#xA;use burn::module::Module;&#xA;use burn::tensor::backend::Backend;&#xA;&#xA;#[derive(Module, Debug)]&#xA;pub struct PositionWiseFeedForward&amp;lt;B: Backend&amp;gt; {&#xA;    linear_inner: nn::Linear&amp;lt;B&amp;gt;,&#xA;    linear_outer: nn::Linear&amp;lt;B&amp;gt;,&#xA;    dropout: nn::Dropout,&#xA;    gelu: nn::Gelu,&#xA;}&#xA;&#xA;impl&amp;lt;B: Backend&amp;gt; PositionWiseFeedForward&amp;lt;B&amp;gt; {&#xA;    pub fn forward&amp;lt;const D: usize&amp;gt;(&amp;amp;self, input: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {&#xA;        let x = self.linear_inner.forward(input);&#xA;        let x = self.gelu.forward(x);&#xA;        let x = self.dropout.forward(x);&#xA;&#xA;        self.linear_outer.forward(x)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;We have a somewhat large amount of &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples&#34;&gt;examples&lt;/a&gt; in the repository that shows how to use the framework in different scenarios.&lt;/p&gt; &#xA;  &lt;p&gt;Following &lt;a href=&#34;https://burn.dev/book/&#34;&gt;the book&lt;/a&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/guide&#34;&gt;Basic Workflow&lt;/a&gt; : Creates a custom CNN &lt;code&gt;Module&lt;/code&gt; to train on the MNIST dataset and use for inference.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-training-loop&#34;&gt;Custom Training Loop&lt;/a&gt; : Implements a basic training loop instead of using the &lt;code&gt;Learner&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-wgpu-kernel&#34;&gt;Custom WGPU Kernel&lt;/a&gt; : Learn how to create your own custom operation with the WGPU backend.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;Additional examples:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-csv-dataset&#34;&gt;Custom CSV Dataset&lt;/a&gt; : Implements a dataset to parse CSV data for a regression task.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/simple-regression&#34;&gt;Regression&lt;/a&gt; : Trains a simple MLP on the CSV dataset for the regression task.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-image-dataset&#34;&gt;Custom Image Dataset&lt;/a&gt; : Trains a simple CNN on custom image dataset following a simple folder structure.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/custom-renderer&#34;&gt;Custom Renderer&lt;/a&gt; : Implements a custom renderer to display the &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/building-blocks/learner.md&#34;&gt;&lt;code&gt;Learner&lt;/code&gt;&lt;/a&gt; progress.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/image-classification-web&#34;&gt;Image Classification Web&lt;/a&gt; : Image classification web browser demo using Burn, WGPU and WebAssembly.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist-inference-web&#34;&gt;MNIST Inference on Web&lt;/a&gt; : An interactive MNIST inference demo in the browser. The demo is available &lt;a href=&#34;https://burn.dev/demo/&#34;&gt;online&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/mnist&#34;&gt;MNIST Training&lt;/a&gt; : Demonstrates how to train a custom &lt;code&gt;Module&lt;/code&gt; (MLP) with the &lt;code&gt;Learner&lt;/code&gt; configured to log metrics and keep training checkpoints.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/named-tensor&#34;&gt;Named Tensor&lt;/a&gt; : Performs operations with the experimental &lt;code&gt;NamedTensor&lt;/code&gt; feature.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/onnx-inference&#34;&gt;ONNX Import Inference&lt;/a&gt; : Imports an ONNX model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/pytorch-import&#34;&gt;PyTorch Import Inference&lt;/a&gt; : Imports a PyTorch model pre-trained on MNIST to perform inference on a sample image with Burn.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-classification&#34;&gt;Text Classification&lt;/a&gt; : Trains a text classification transformer model on the AG News or DbPedia dataset. The trained model can then be used to classify a text sample.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/examples/text-generation&#34;&gt;Text Generation&lt;/a&gt; : Trains a text generation transformer model on the DbPedia dataset.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;For more practical insights, you can clone the repository and run any of them directly on your computer!&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Pre-trained Models ü§ñ &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;We keep an updated and curated list of models and examples built with Burn, see the &lt;a href=&#34;https://github.com/tracel-ai/models&#34;&gt;tracel-ai/models repository&lt;/a&gt; for more details.&lt;/p&gt; &#xA;  &lt;p&gt;Don&#39;t see the model you want? Don&#39;t hesitate to open an issue, and we may prioritize it. Built a model using Burn and want to share it? You can also open a Pull Request and add your model under the community section!&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; Why use Rust for Deep Learning? ü¶Ä &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;Deep Learning is a special form of software where you need very high level abstractions as well as extremely fast execution time. Rust is the perfect candidate for that use case since it provides zero-cost abstractions to easily create neural network modules, and fine-grained control over memory to optimize every detail.&lt;/p&gt; &#xA;  &lt;p&gt;It&#39;s important that a framework be easy to use at a high level so that its users can focus on innovating in the AI field. However, since running models relies so heavily on computations, performance can&#39;t be neglected.&lt;/p&gt; &#xA;  &lt;p&gt;To this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on bindings to low-level languages such as C/C++. This reduces portability, increases complexity and creates frictions between researchers and engineers. We feel like Rust&#39;s approach to abstractions makes it versatile enough to tackle this two languages dichotomy.&lt;/p&gt; &#xA;  &lt;p&gt;Rust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and deploy from any environment, which is usually a pain in Python.&lt;/p&gt; &#xA;  &lt;p&gt;Although Rust has the reputation of being a difficult language at first, we strongly believe it leads to more reliable, bug-free solutions built faster (after some practice üòÖ)!&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;br&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;strong&gt;Deprecation Note&lt;/strong&gt;&lt;br&gt;Since &lt;code&gt;0.14.0&lt;/code&gt;, the internal structure for tensor data has changed. The previous &lt;code&gt;Data&lt;/code&gt; struct is being deprecated in favor of the new &lt;code&gt;TensorData&lt;/code&gt; struct, which allows for more flexibility by storing the underlying data as bytes and keeping the data type as a field. If you are using &lt;code&gt;Data&lt;/code&gt; in your code, make sure to switch to &lt;code&gt;TensorData&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;!-- &gt;&#xA;&gt; In the event that you are trying to load a model record saved in a previous version, make sure to&#xA;&gt; enable the `record-backward-compat` feature. Otherwise, the record won&#39;t be deserialized correctly&#xA;&gt; and you will get an error message (which will also point you to the backward compatible feature&#xA;&gt; flag). The backward compatibility is maintained for deserialization (loading), so as soon as you&#xA;&gt; have saved the record again it will be saved according to the new structure and you won&#39;t need the&#xA;&gt; backward compatible feature flag anymore. Please note that binary formats are not backward&#xA;&gt; compatible. Thus, you will need to load your record in a previous version and save it to another&#xA;&gt; of the self-describing record formats before using the new version with the&#xA;&gt; `record-backward-compat` feature flag. --&gt; &#xA; &lt;details id=&#34;deprecation&#34;&gt; &#xA;  &lt;summary&gt; Loading Model Records From Previous Versions ‚ö†Ô∏è &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;p&gt;In the event that you are trying to load a model record saved in a previous version, make sure to enable the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code&gt;features = [..., &#34;record-backward-compat&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Otherwise, the record won&#39;t be deserialized correctly and you will get an error message. This error will also point you to the backward compatible feature flag.&lt;/p&gt; &#xA;  &lt;p&gt;The backward compatibility is maintained for deserialization when loading records. Therefore, as soon as you have saved the record again it will be saved according to the new structure and you won&#39;t need the backward compatible feature flag anymore.&lt;/p&gt; &#xA;  &lt;p&gt;Please note that binary formats are not backward compatible. Thus, you will need to load your record in a previous version and save it in any of the other self-describing record format (e.g., using the &lt;code&gt;NamedMpkFileRecorder&lt;/code&gt;) before using the new version with the &lt;code&gt;record-backward-compat&lt;/code&gt; feature flag.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;h2&gt;Community&lt;/h2&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png&#34; height=&#34;96px&#34;&gt; &#xA;  &lt;p&gt;If you are excited about the project, don&#39;t hesitate to join our &lt;a href=&#34;https://discord.gg/uPEBbYYDB6&#34;&gt;Discord&lt;/a&gt;! We try to be as welcoming as possible to everybody from any background. You can ask your questions and share what you built with the community!&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Before contributing, please take a moment to review our &lt;a href=&#34;https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. It&#39;s also highly recommended to read the &lt;a href=&#34;https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture&#34;&gt;architecture overview&lt;/a&gt;, which explains some of our architectural decisions. Refer to our &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;h2&gt;Status&lt;/h2&gt; &#xA; &lt;p&gt;Burn is currently in active development, and there will be breaking changes. While any resulting issues are likely to be easy to fix, there are no guarantees at this stage.&lt;/p&gt; &#xA; &lt;h2&gt;License&lt;/h2&gt; &#xA; &lt;p&gt;Burn is distributed under the terms of both the MIT license and the Apache License (Version 2.0). See &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/tracel-ai/burn/main/LICENSE-MIT&#34;&gt;LICENSE-MIT&lt;/a&gt; for details. Opening a pull request is assumed to signal agreement with these licensing terms.&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>