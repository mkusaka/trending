<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T02:02:02Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rustformers/llama-rs</title>
    <updated>2023-03-26T02:02:02Z</updated>
    <id>tag:github.com,2023-03-26:/rustformers/llama-rs</id>
    <link href="https://github.com/rustformers/llama-rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run LLaMA inference on CPU, with Rust ü¶ÄüöÄü¶ô&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA-rs&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Do the LLaMA thing, but now in Rust ü¶ÄüöÄü¶ô&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/logo2.png&#34; alt=&#34;A llama riding a crab, AI-generated&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Image by &lt;a href=&#34;https://github.com/darthdeus/&#34;&gt;@darthdeus&lt;/a&gt;, using Stable Diffusion&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/F1F8DNO5D&#34;&gt;&lt;img src=&#34;https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true&#34; alt=&#34;ko-fi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/llama_rs&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/llama-rs.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT&#34;&gt; &lt;a href=&#34;https://discord.gg/YB9WaXYAWU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1085885067601137734&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/llama_gif.gif&#34; alt=&#34;Gif showcasing language generation using llama-rs&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLaMA-rs&lt;/strong&gt; is a Rust port of the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project. This allows running inference for Facebook&#39;s &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; model on a CPU with good performance using full precision, f16 or 4-bit quantized versions of the model.&lt;/p&gt; &#xA;&lt;p&gt;Just like its C++ counterpart, it is powered by the &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;&lt;code&gt;ggml&lt;/code&gt;&lt;/a&gt; tensor library, achieving the same performance as the original code.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have a rust toolchain set up.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get a copy of the model&#39;s weights[^1]&lt;/li&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA; &lt;li&gt;Build (&lt;code&gt;cargo build --release&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Run with &lt;code&gt;cargo run --release -- &amp;lt;ARGS&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: For best results, make sure to build and run in release mode. Debug builds are going to be very slow.&lt;/p&gt; &#xA;&lt;p&gt;For example, you try the following prompt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo run --release -- -m /data/Llama/LLaMA/7B/ggml-model-q4_0.bin -p &#34;Tell me how cool the Rust programming language is:&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some additional things to try:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;--help&lt;/code&gt; to see a list of available options.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have the &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;alpaca-lora&lt;/a&gt; weights, try &lt;code&gt;--repl&lt;/code&gt; mode! &lt;code&gt;cargo run --release -- -m &amp;lt;path&amp;gt;/ggml-alpaca-7b-q4.bin -f examples/alpaca_prompt.txt --repl&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/alpaca_repl_screencap.gif&#34; alt=&#34;Gif showcasing alpaca repl mode&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prompt files can be precomputed to speed up processing using the &lt;code&gt;--cache-prompt&lt;/code&gt; and &lt;code&gt;--restore-prompt&lt;/code&gt; flags so you can save processing time for lengthy prompts.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rustformers/llama-rs/main/doc/resources/prompt_caching_screencap.gif&#34; alt=&#34;Gif showcasing prompt caching&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: The only legal source to get the weights at the time of writing is &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/README.md#llama&#34;&gt;this repository&lt;/a&gt;. The choice of words also may or may not hint at the existence of other kinds of sources.&lt;/p&gt; &#xA;&lt;h2&gt;Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q: Why did you do this?&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It was not my choice. Ferris appeared to me in my dreams and asked me to rewrite this in the name of the Holy crab.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q: Seriously now&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Come on! I don&#39;t want to get into a flame war. You know how it goes, &lt;em&gt;something something&lt;/em&gt; memory &lt;em&gt;something something&lt;/em&gt; cargo is nice, don&#39;t make me say it, everybody knows this already.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q: I insist.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;em&gt;Sheesh! Okaaay&lt;/em&gt;. After seeing the huge potential for &lt;strong&gt;llama.cpp&lt;/strong&gt;, the first thing I did was to see how hard would it be to turn it into a library to embed in my projects. I started digging into the code, and realized the heavy lifting is done by &lt;code&gt;ggml&lt;/code&gt; (a C library, easy to bind to Rust) and the whole project was just around ~2k lines of C++ code (not so easy to bind). After a couple of (failed) attempts to build an HTTP server into the tool, I realized I&#39;d be much more productive if I just ported the code to Rust, where I&#39;m more comfortable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Q: Is this the real reason?&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Haha. Of course &lt;em&gt;not&lt;/em&gt;. I just like collecting imaginary internet points, in the form of little stars, that people seem to give to me whenever I embark on pointless quests for &lt;em&gt;rewriting X thing, but in Rust&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known issues / To-dos&lt;/h2&gt; &#xA;&lt;p&gt;Contributions welcome! Here&#39;s a few pressing issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; The quantization code has not been ported (yet). You can still use the quantized models with llama.cpp.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; No crates.io release. The name &lt;code&gt;llama-rs&lt;/code&gt; is reserved and I plan to do this soon-ish.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Any improvements from the original C++ code. (See &lt;a href=&#34;https://github.com/setzer22/llama-rs/issues/15&#34;&gt;https://github.com/setzer22/llama-rs/issues/15&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Debug builds are currently broken.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; The code needs to be &#34;library&#34;-fied. It is nice as a showcase binary, but the real potential for this tool is to allow embedding in other services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; The code only sets the right CFLAGS on Linux. The &lt;code&gt;build.rs&lt;/code&gt; script in &lt;code&gt;ggml_raw&lt;/code&gt; needs to be fixed, so inference &lt;em&gt;will be very slow on every other OS&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-opendal</title>
    <updated>2023-03-26T02:02:02Z</updated>
    <id>tag:github.com,2023-03-26:/apache/incubator-opendal</id>
    <link href="https://github.com/apache/incubator-opendal" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache OpenDAL: Access data freely, painlessly, and efficiently.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenDAL ‚ÄÉ &lt;a href=&#34;https://github.com/apache/incubator-opendal/actions?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/apache/incubator-opendal/ci.yml?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/XQy8yGR2dg&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1081052318650339399&#34; alt=&#34;chat&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open&lt;/strong&gt; &lt;strong&gt;D&lt;/strong&gt;ata &lt;strong&gt;A&lt;/strong&gt;ccess &lt;strong&gt;L&lt;/strong&gt;ayer: Access data freely, painlessly, and efficiently&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5351546/222356748-14276998-501b-4d2a-9b09-b8cff3018204.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/core/README.md&#34;&gt;core&lt;/a&gt;: OpenDAL Rust Core &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Documentation: &lt;a href=&#34;https://docs.rs/opendal/&#34;&gt;stable&lt;/a&gt; | &lt;a href=&#34;https://opendal.apache.org/docs/rust/opendal/&#34;&gt;main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/bindings/c&#34;&gt;binding-c&lt;/a&gt;: OpenDAL C Binding (working on)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/bindings/java&#34;&gt;binding-java&lt;/a&gt;: OpenDAL Java Binding (working on)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/bindings/nodejs/README.md&#34;&gt;binding-nodejs&lt;/a&gt;: OpenDAL Node.js Binding &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Documentation: &lt;a href=&#34;https://opendal.apache.org/docs/nodejs/&#34;&gt;main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/bindings/python/README.md&#34;&gt;binding-python&lt;/a&gt;: OpenDAL Python Binding &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Documentation: &lt;a href=&#34;https://opendal.apache.org/docs/python/&#34;&gt;main&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/bindings/ruby&#34;&gt;binding-ruby&lt;/a&gt;: OpenDAL Ruby Binding (working on)&lt;/li&gt; &#xA; &lt;li&gt;bin &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-opendal/main/bin/oli&#34;&gt;oli&lt;/a&gt;: OpenDAL Command Line Interface&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Rust&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use opendal::Result;&#xA;use opendal::layers::LoggingLayer;&#xA;use opendal::services;&#xA;use opendal::Operator;&#xA;&#xA;#[tokio::main]&#xA;async fn main() -&amp;gt; Result&amp;lt;()&amp;gt; {&#xA;    // Pick a builder and configure it.&#xA;    let mut builder = services::S3::default();&#xA;    builder.bucket(&#34;test&#34;);&#xA;&#xA;    // Init an operator&#xA;    let op = Operator::new(builder)?&#xA;        // Init with logging layer enabled.&#xA;        .layer(LoggingLayer::default())&#xA;        .finish();&#xA;&#xA;    // Write data&#xA;    op.write(&#34;hello.txt&#34;, &#34;Hello, World!&#34;).await?;&#xA;&#xA;    // Read data&#xA;    let bs = op.read(&#34;hello.txt&#34;).await?;&#xA;&#xA;    // Fetch metadata&#xA;    let meta = op.stat(&#34;hello.txt&#34;).await?;&#xA;    let mode = meta.mode();&#xA;    let length = meta.content_length();&#xA;&#xA;    // Delete&#xA;    op.delete(&#34;hello.txt&#34;).await?;&#xA;&#xA;    Ok(())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;&#xA;async def main():&#xA;    op = opendal.AsyncOperator(&#34;fs&#34;, root=&#34;/tmp&#34;)&#xA;    await op.write(&#34;test.txt&#34;, b&#34;Hello World&#34;)&#xA;    print(await op.read(&#34;test.txt&#34;))&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Node.js&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;import { Operator } from &#34;opendal&#34;;&#xA;&#xA;async function main() {&#xA;  const op = new Operator(&#34;fs&#34;, { root: &#34;/tmp&#34; });&#xA;  await op.write(&#34;test&#34;, &#34;Hello, World!&#34;);&#xA;  const bs = await op.read(&#34;test&#34;);&#xA;  console.log(new TextDecoder().decode(bs));&#xA;  const meta = await op.stat(&#34;test&#34;);&#xA;  console.log(`contentLength: ${meta.contentLength}`);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/datafuselabs/databend/&#34;&gt;Databend&lt;/a&gt;: A modern Elasticity and Performance cloud data warehouse.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GreptimeTeam/greptimedb&#34;&gt;GreptimeDB&lt;/a&gt;: An open-source, cloud-native, distributed time-series database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepeth/mars&#34;&gt;deepeth/mars&lt;/a&gt;: The powerful analysis platform to explore and visualize data from blockchain.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mozilla/sccache/&#34;&gt;mozilla/sccache&lt;/a&gt;: sccache is ccache with cloud storage&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/risingwavelabs/risingwave&#34;&gt;risingwave&lt;/a&gt;: A Distributed SQL Database for Stream Processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vectordotdev/vector&#34;&gt;Vector&lt;/a&gt;: A high-performance observability data pipeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting help&lt;/h2&gt; &#xA;&lt;p&gt;Submit &lt;a href=&#34;https://github.com/apache/incubator-opendal/issues/new&#34;&gt;issues&lt;/a&gt; for bug report or asking questions in the &lt;a href=&#34;https://github.com/apache/incubator-opendal/discussions/new?category=q-a&#34;&gt;Discussions forum&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Talk to develops at &lt;a href=&#34;https://discord.gg/XQy8yGR2dg&#34;&gt;discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sigoden/aichat</title>
    <updated>2023-03-26T02:02:02Z</updated>
    <id>tag:github.com,2023-03-26:/sigoden/aichat</id>
    <link href="https://github.com/sigoden/aichat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Using ChatGPT/GPT-3.5/GPT-4 in the terminal.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIChat&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sigoden/aichat/actions/workflows/ci.yaml&#34;&gt;&lt;img src=&#34;https://github.com/sigoden/aichat/actions/workflows/ci.yaml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/aichat&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/aichat.svg?sanitize=true&#34; alt=&#34;Crates&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using ChatGPT/GPT-3.5/GPT-4 in the terminal.&lt;/p&gt; &#xA;&lt;p&gt;AIChat in chat mode:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4012553/226499667-4c6b261a-d897-41c7-956b-979b69da5982.gif&#34; alt=&#34;chat mode&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;AIChat in command mode:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4012553/226499595-0b536c82-b039-4571-a077-0c40ad57f7db.png&#34; alt=&#34;command mode&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;With cargo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo install --force aichat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Binaries on macOS, Linux, Windows&lt;/h3&gt; &#xA;&lt;p&gt;Download it from &lt;a href=&#34;https://github.com/sigoden/aichat/releases&#34;&gt;Github Releases&lt;/a&gt;, unzip and add aichat to your $PATH.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support chat and command modes&lt;/li&gt; &#xA; &lt;li&gt;Predefine AI &lt;a href=&#34;https://raw.githubusercontent.com/sigoden/aichat/main/#roles&#34;&gt;roles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use GPT prompt easily&lt;/li&gt; &#xA; &lt;li&gt;Powerful &lt;a href=&#34;https://raw.githubusercontent.com/sigoden/aichat/main/#chat-repl&#34;&gt;Chat REPL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Context-ware conversation&lt;/li&gt; &#xA; &lt;li&gt;syntax highlighting markdown and other 200 languages&lt;/li&gt; &#xA; &lt;li&gt;Stream output with hand typing effect&lt;/li&gt; &#xA; &lt;li&gt;Support multiple models&lt;/li&gt; &#xA; &lt;li&gt;Support proxy&lt;/li&gt; &#xA; &lt;li&gt;Support dark/light theme&lt;/li&gt; &#xA; &lt;li&gt;Save chat messages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Config&lt;/h2&gt; &#xA;&lt;p&gt;On first launch, aichat will guide you through configuration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; No config file, create a new one? Yes&#xA;&amp;gt; Openai API Key: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&amp;gt; Use proxy? Yes&#xA;&amp;gt; Set proxy: socks5://127.0.0.1:1080&#xA;&amp;gt; Save chat messages Yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After setting, it will automatically create the configuration file. Of course, you can also manually set the configuration file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;api_key: &#34;&amp;lt;YOUR SECRET API KEY&amp;gt;&#34;  # Request via https://platform.openai.com/account/api-keys&#xA;organization_id: &#34;org-xxx&#34;        # optional, set organization id&#xA;model: &#34;gpt-3.5-turbo&#34;            # optional, choose a model&#xA;temperature: 1.0                  # optional, see https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature&#xA;save: true                        # optional, If set true, aichat will save chat messages to message.md&#xA;highlight: true                   # optional, Set false to turn highlight&#xA;proxy: &#34;socks5://127.0.0.1:1080&#34;  # optional, set proxy server. e.g. http://127.0.0.1:8080 or socks5://127.0.0.1:1080&#xA;conversation_first: false         # optional, If set true, start a conversation immediately upon repl&#xA;light_theme: false                # optional, If set true, use light theme&#xA;connect_timeout: 10               # optional, Set a timeout in seconds for connect to gpt.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can use &lt;code&gt;.info&lt;/code&gt; to view the current configuration file path and roles file path.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can use &lt;a href=&#34;https://github.com/sigoden/aichat/wiki/Environment-Variables&#34;&gt;Enviroment Variables&lt;/a&gt; to customize certain configuration items.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Roles&lt;/h3&gt; &#xA;&lt;p&gt;We can let ChatGPT play a certain role through &lt;code&gt;prompt&lt;/code&gt; to make it better generate what we want.&lt;/p&gt; &#xA;&lt;p&gt;We can predefine a batch of roles in &lt;code&gt;roles.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We can get the location of &lt;code&gt;roles.yaml&lt;/code&gt; through the repl&#39;s &lt;code&gt;.info&lt;/code&gt; command or cli&#39;s &lt;code&gt;--info&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For example, we define a role&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- name: shell&#xA;  prompt: &amp;gt;&#xA;    I want you to act as a linux shell expert.&#xA;    I want you to answer only with bash code.&#xA;    Do not provide explanations.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let ChatGPT answer questions in the role of a linux shell expert.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ.role shell&#xA;&#xA;shell„Äâ extract encrypted zipfile app.zip to /tmp/app&#xA;mkdir /tmp/app&#xA;unzip -P PASSWORD app.zip -d /tmp/app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have provided many awesome &lt;a href=&#34;https://github.com/sigoden/aichat/wiki/Role-Examples&#34;&gt;Role Examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Chat REPL&lt;/h2&gt; &#xA;&lt;p&gt;aichat has a powerful Chat REPL.&lt;/p&gt; &#xA;&lt;p&gt;The Chat REPL supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Emacs keybinding&lt;/li&gt; &#xA; &lt;li&gt;Command autocompletion&lt;/li&gt; &#xA; &lt;li&gt;History search&lt;/li&gt; &#xA; &lt;li&gt;Fish-style history autosuggestion hints&lt;/li&gt; &#xA; &lt;li&gt;Edit/past multiline input&lt;/li&gt; &#xA; &lt;li&gt;Undo support&lt;/li&gt; &#xA; &lt;li&gt;Clipboard integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;multi-line editing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Type &lt;code&gt;{&lt;/code&gt; or &lt;code&gt;(&lt;/code&gt; at the beginning of the line to enter the multi-line editing mode.&lt;/strong&gt; In this mode you can type or paste multiple lines of text. Type the corresponding &lt;code&gt;}&lt;/code&gt; or &lt;code&gt;)&lt;/code&gt; at the end of the line to exit the mode and submit the content.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ{ convert json below to toml&#xA;{&#xA;  &#34;an&#34;: [&#xA;    &#34;arbitrarily&#34;,&#xA;    &#34;nested&#34;&#xA;  ],&#xA;  &#34;data&#34;: &#34;structure&#34;&#xA;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.help&lt;/code&gt; - Print help message&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ.help&#xA;.info                    Print the information&#xA;.set                     Modify the configuration temporarily&#xA;.model                   Choose a model&#xA;.prompt                  Add a GPT prompt&#xA;.role                    Select a role&#xA;.clear role              Clear the currently selected role&#xA;.conversation            Start a conversation.&#xA;.clear conversation      End current conversation.&#xA;.history                 Print the history&#xA;.clear history           Clear the history&#xA;.help                    Print this help message&#xA;.exit                    Exit the REPL&#xA;&#xA;Type `{` to enter the multi-line editing mode, type &#39;}&#39; to exit the mode.&#xA;Press Ctrl+C to abort readline, Ctrl+D to exit the REPL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.info&lt;/code&gt; - view current configuration information.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ.info&#xA;config_file         /home/alice/.config/aichat/config.yaml&#xA;roles_file          /home/alice/.config/aichat/roles.yaml&#xA;messages_file       /home/alice/.config/aichat/messages.md&#xA;api_key             sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;organization_id     -&#xA;model               gpt-3.5-turbo&#xA;temperature         -&#xA;save                true&#xA;highlight           true&#xA;proxy               -&#xA;conversation_first  false&#xA;light_theme         false&#xA;connect_timeout     10&#xA;dry_run             false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.set&lt;/code&gt; - modify the configuration temporarily&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ.set dry_run true&#xA;„Äâ.set highlight false&#xA;„Äâ.set save false&#xA;„Äâ.set temperature 1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.model&lt;/code&gt; - choose a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; .model gpt-4&#xA;&amp;gt; .model gpt-4-32k&#xA;&amp;gt; .model gpt-3.5-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.prompt&lt;/code&gt; - use GPT prompt&lt;/h3&gt; &#xA;&lt;p&gt;When you set up a prompt, every message sent later will carry the prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ{ .prompt&#xA;I want you to translate the sentences I wrote into emojis.&#xA;I will write the sentence, and you will express it with emojis.&#xA;I just want you to express it with emojis.&#xA;I want you to reply only with emojis.&#xA;}&#xA;Done&#xA;&#xA;Ôº∞„ÄâYou are a genius&#xA;üëâüß†üí°üë®‚Äçüéì&#xA;&#xA;Ôº∞„ÄâI&#39;m embarrassed&#xA;üôàüò≥&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;.prompt&lt;/code&gt; actually creates a temporary role internally, so &lt;strong&gt;run &lt;code&gt;.clear role&lt;/code&gt; to clear the prompt&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you are satisfied with the prompt, add it to &lt;code&gt;roles.yaml&lt;/code&gt; for later use.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;.role&lt;/code&gt; - let the ai play a role&lt;/h3&gt; &#xA;&lt;p&gt;Select a role.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ.role emoji&#xA;name: emoji&#xA;prompt: I want you to translate the sentences I wrote into emojis. I will write the sentence, and you will express it with emojis. I just want you to express it with emojis. I don&#39;t want you to reply with anything but emoji. When I need to tell you something in English, I will do it by wrapping it in curly brackets like {like this}.&#xA;temperature: null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;AI play the role we specified&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;emoji„Äâhello&#xA;üëã&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clear current selected role&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;emoji„Äâ.clear role&#xA;&#xA;„Äâhello &#xA;Hello there! How can I assist you today?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.conversation&lt;/code&gt; - start a context-aware conversation&lt;/h3&gt; &#xA;&lt;p&gt;By default, aichat behaves in a one-off request/response manner.&lt;/p&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;.conversation&lt;/code&gt; to enter context-aware mode, or set &lt;code&gt;config.conversation_first&lt;/code&gt; true to start a conversation immediately upon repl.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;„Äâ.conversation&#xA;&#xA;ÔºÑlist 1 to 5, one per line                                                              4089&#xA;1&#xA;2&#xA;3&#xA;4&#xA;5&#xA;&#xA;ÔºÑreverse the list                                                                       4065&#xA;5&#xA;4&#xA;3&#xA;2&#xA;1&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When enter conversation mode, prompt &lt;code&gt;„Äâ&lt;/code&gt; will change to &lt;code&gt;ÔºÑ&lt;/code&gt;, A number will appear on the right, which means how many tokens left to use. Once the number becomes zero, you need to start a new conversation.&lt;/p&gt; &#xA;&lt;p&gt;Exit conversation mode&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ÔºÑ.clear conversation                                                                    4043&#xA;&#xA;„Äâ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) 2023 aichat-developers.&lt;/p&gt; &#xA;&lt;p&gt;aichat is made available under the terms of either the MIT License or the Apache License 2.0, at your option.&lt;/p&gt; &#xA;&lt;p&gt;See the LICENSE-APACHE and LICENSE-MIT files for license details.&lt;/p&gt;</summary>
  </entry>
</feed>