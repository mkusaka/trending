<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-13T02:00:09Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>burn-rs/burn</title>
    <updated>2023-08-13T02:00:09Z</updated>
    <id>tag:github.com,2023-08-13:/burn-rs/burn</id>
    <link href="https://github.com/burn-rs/burn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Burn - A Flexible and Comprehensive Deep Learning Framework in Rust&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/assets/logo-burn-full.png&#34; width=&#34;200px&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/uPEBbYYDB6&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1038839012602941528.svg?color=7289da&amp;amp;&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/burn-rs/burn/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/burn-rs/burn/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;Test Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/burn&#34;&gt;&lt;img src=&#34;https://docs.rs/burn/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/burn&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/burn.svg?sanitize=true&#34; alt=&#34;Current Crates.io Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://releases.rs/docs/1.65.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Rust-1.65.0+-blue&#34; alt=&#34;Rust Version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://shields.io/badge/license-MIT%2FApache--2.0-blue&#34; alt=&#34;license&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;This library strives to serve as a comprehensive &lt;strong&gt;deep learning framework&lt;/strong&gt;, offering exceptional flexibility and written in Rust. Our objective is to cater to both researchers and practitioners by simplifying the process of experimenting, training, and deploying models.&lt;/p&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;h2&gt;Features&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Customizable, user-friendly neural network &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#module&#34;&gt;module&lt;/a&gt; üî•&lt;/li&gt; &#xA;   &lt;li&gt;Comprehensive &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#learner&#34;&gt;training&lt;/a&gt; tools, inclusive of &lt;code&gt;metrics&lt;/code&gt;, &lt;code&gt;logging&lt;/code&gt;, and &lt;code&gt;checkpointing&lt;/code&gt; üìà&lt;/li&gt; &#xA;   &lt;li&gt;Versatile &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#tensor&#34;&gt;Tensor&lt;/a&gt; crate equipped with pluggable backends üîß &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-tch&#34;&gt;Torch&lt;/a&gt; backend, supporting both CPU and GPU üöÄ&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-ndarray&#34;&gt;Ndarray&lt;/a&gt; backend with &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#support-for-no_std&#34;&gt;&lt;code&gt;no_std&lt;/code&gt;&lt;/a&gt; compatibility, ensuring universal platform adaptability üëå&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-wgpu&#34;&gt;WebGPU&lt;/a&gt; backend, offering cross-platform, browser-inclusive, GPU-based computations üåê&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-autodiff&#34;&gt;Autodiff&lt;/a&gt; backend that enables differentiability across all backends üåü&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-dataset&#34;&gt;Dataset&lt;/a&gt; crate containing a diverse range of utilities and sources üìö&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-import&#34;&gt;Import&lt;/a&gt; crate that simplifies the integration of pretrained models üì¶&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;Supported Platforms&lt;/h2&gt; &#xA;  &lt;h3&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-ndarray&#34;&gt;Burn-ndarray&lt;/a&gt; Backend&lt;/h3&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Option&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;CPU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MacOS&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Android&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;iOS&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;WASM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Pure Rust&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Accelerate&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Netlib&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Openblas&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;h3&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-tch&#34;&gt;Burn-tch&lt;/a&gt; Backend&lt;/h3&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Option&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;CPU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MacOS&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Android&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;iOS&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;WASM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;CPU&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;CUDA&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MPS&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Vulkan&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;h3&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-wgpu&#34;&gt;Burn-wgpu&lt;/a&gt; Backend&lt;/h3&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Option&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;CPU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;MacOS&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;Android&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;iOS&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;WASM&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Metal&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Vulkan&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;OpenGL&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;WebGpu&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Dx11/Dx12&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;h2&gt;Pre-trained Models&lt;/h2&gt; &#xA;  &lt;p&gt;We keep an updated and curated list of models and examples built with Burn, see the &lt;a href=&#34;https://github.com/burn-rs/models&#34;&gt;burn-rs/models&lt;/a&gt; repository for more details.&lt;/p&gt; &#xA;  &lt;h2&gt;Get Started&lt;/h2&gt; &#xA;  &lt;p&gt;The best way to get started with &lt;code&gt;burn&lt;/code&gt; is to clone the repo and play with the &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#examples&#34;&gt;examples&lt;/a&gt;. This may also be a good idea to take a look the main &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#components&#34;&gt;components&lt;/a&gt; of &lt;code&gt;burn&lt;/code&gt; to get a quick overview of the fundamental building blocks. If you&#39;re interested in how the framework works, you can read our &lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/ARCHITECTURE.md&#34;&gt;architecture document&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h3&gt;Examples&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/examples/mnist&#34;&gt;MNIST&lt;/a&gt; train a model on CPU/GPU using different backends.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/examples/mnist-inference-web&#34;&gt;MNIST Inference Web&lt;/a&gt; run trained model in the browser for inference.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/examples/text-classification&#34;&gt;Text Classification&lt;/a&gt; train a transformer encoder from scratch on GPU.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/examples/text-generation&#34;&gt;Text Generation&lt;/a&gt; train an autoregressive transformer from scratch on GPU.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h3&gt;Components&lt;/h3&gt; &#xA;  &lt;p&gt;Understanding the key components and philosophy of &lt;code&gt;burn&lt;/code&gt; can greatly help when beginning to work with the framework.&lt;/p&gt; &#xA;  &lt;h4&gt;Backend&lt;/h4&gt; &#xA;  &lt;p&gt;Nearly everything in &lt;code&gt;burn&lt;/code&gt; is based on the &lt;code&gt;Backend&lt;/code&gt; trait, which enables you to run tensor operations using different implementations without having to modify your code. While a backend may not necessarily have autodiff capabilities, the &lt;code&gt;ADBackend&lt;/code&gt; trait specifies when autodiff is needed. This trait not only abstracts operations but also tensor, device and element types, providing each backend the flexibility they need. It&#39;s worth noting that the trait assumes eager mode since &lt;code&gt;burn&lt;/code&gt; fully supports dynamic graphs. However, we may create another API to assist with integrating graph-based backends, without requiring any changes to the user&#39;s code.&lt;/p&gt; &#xA;  &lt;h4&gt;Tensor&lt;/h4&gt; &#xA;  &lt;p&gt;At the core of burn lies the &lt;code&gt;Tensor&lt;/code&gt; struct, which encompasses multiple types of tensors, including &lt;code&gt;Float&lt;/code&gt;, &lt;code&gt;Int&lt;/code&gt;, and &lt;code&gt;Bool&lt;/code&gt;. The element types of these tensors are specified by the backend and are usually designated as a generic argument (e.g., &lt;code&gt;NdArrayBackend&amp;lt;f32&amp;gt;&lt;/code&gt;). Although the same struct is used for all tensors, the available methods differ depending on the tensor kind. You can specify the desired tensor kind by setting the third generic argument, which defaults to &lt;code&gt;Float&lt;/code&gt;. The first generic argument specifies the backend, while the second specifies the number of dimensions.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::tensor::backend::Backend;&#xA;use burn::tensor::{Tensor, Int};&#xA;&#xA;fn function&amp;lt;B: Backend&amp;gt;(tensor_float: Tensor&amp;lt;B, 2&amp;gt;) {&#xA;    let _tensor_bool = tensor_float.clone().equal_elem(2.0); // Tensor&amp;lt;B, 2, Bool&amp;gt;&#xA;    let _tensor_int = tensor_float.argmax(1); // Tensor&amp;lt;B, 2, Int&amp;gt;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;As demonstrated in the previous example, nearly all operations require owned tensors as parameters, which means that calling &lt;code&gt;Clone&lt;/code&gt; explicitly is necessary when reusing the same tensor multiple times. However, there&#39;s no need to worry since the tensor&#39;s data won&#39;t be copied, it will be flagged as readonly when multiple tensors use the same allocated memory. This enables backends to reuse tensor data when possible, similar to a copy-on-write pattern, while remaining completely transparent to the user.&lt;/p&gt; &#xA;  &lt;h4&gt;Autodiff&lt;/h4&gt; &#xA;  &lt;p&gt;The &#39;Backend&#39; trait is highly flexible, enabling backpropagation to be implemented using a simple backend decorator, which makes any backend differentiable.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::tensor::backend::{ADBackend, Backend};&#xA;use burn::tensor::{Distribution, Tensor};&#xA;use burn_autodiff::ADBackendDecorator;&#xA;use burn_ndarray::NdArrayBackend;&#xA;&#xA;fn linear&amp;lt;B: Backend&amp;gt;(x: Tensor&amp;lt;B, 2&amp;gt;, weight: Tensor&amp;lt;B, 2&amp;gt;, bias: Tensor&amp;lt;B, 2&amp;gt;) -&amp;gt; Tensor&amp;lt;B, 2&amp;gt; {&#xA;    x.matmul(weight) + bias&#xA;}&#xA;&#xA;fn main() {&#xA;    type Backend = NdArrayBackend&amp;lt;f32&amp;gt;;&#xA;&#xA;    let weight = Tensor::random([3, 3], Distribution::Standard);&#xA;    let bias = Tensor::zeros([1, 3]);&#xA;    let x = Tensor::random([3, 3], Distribution::Standard);&#xA;&#xA;    let y = linear::&amp;lt;Backend&amp;gt;(x.clone(), weight.clone(), bias.clone());&#xA;    // y.backward() // Method backward doesn&#39;t exist&#xA;&#xA;    let y = linear::&amp;lt;ADBackendDecorator&amp;lt;Backend&amp;gt;&amp;gt;(&#xA;        Tensor::from_inner(x),&#xA;        Tensor::from_inner(weight).require_grad(),&#xA;        Tensor::from_inner(bias).require_grad(),&#xA;    );&#xA;    let grads = y.backward(); // Method exists&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h4&gt;Module&lt;/h4&gt; &#xA;  &lt;p&gt;The &lt;code&gt;Module&lt;/code&gt; derive allows you to create your own neural network modules, similar to PyTorch. The derive function only generates the necessary methods to essentially act as a parameter container for your type, it makes no assumptions about how the forward pass is declared.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::nn;&#xA;use burn::module::Module;&#xA;use burn::tensor::backend::Backend;&#xA;&#xA;#[derive(Module, Debug)]&#xA;pub struct PositionWiseFeedForward&amp;lt;B: Backend&amp;gt; {&#xA;    linear_inner: Linear&amp;lt;B&amp;gt;,&#xA;    linear_outer: Linear&amp;lt;B&amp;gt;,&#xA;    dropout: Dropout,&#xA;    gelu: GELU,&#xA;}&#xA;&#xA;impl&amp;lt;B: Backend&amp;gt; PositionWiseFeedForward&amp;lt;B&amp;gt; {&#xA;    pub fn forward&amp;lt;const D: usize&amp;gt;(&amp;amp;self, input: Tensor&amp;lt;B, D&amp;gt;) -&amp;gt; Tensor&amp;lt;B, D&amp;gt; {&#xA;        let x = self.linear_inner.forward(input);&#xA;        let x = self.gelu.forward(x);&#xA;        let x = self.dropout.forward(x);&#xA;&#xA;        self.linear_outer.forward(x)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Note that all fields declared in the struct must also implement the &lt;code&gt;Module&lt;/code&gt; trait. The &lt;code&gt;Tensor&lt;/code&gt; struct doesn&#39;t implement &lt;code&gt;Module&lt;/code&gt;, but &lt;code&gt;Param&amp;lt;Tensor&amp;lt;B, D&amp;gt;&amp;gt;&lt;/code&gt; does.&lt;/p&gt; &#xA;  &lt;h4&gt;Config&lt;/h4&gt; &#xA;  &lt;p&gt;The &lt;code&gt;Config&lt;/code&gt; derive lets you define serializable and deserializable configurations or hyper-parameters for your &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/#module&#34;&gt;modules&lt;/a&gt; or any components.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn::config::Config;&#xA;&#xA;#[derive(Config)]&#xA;pub struct PositionWiseFeedForwardConfig {&#xA;    pub d_model: usize,&#xA;    pub d_ff: usize,&#xA;    #[config(default = 0.1)]&#xA;    pub dropout: f64,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The derive also adds useful methods to your config, similar to a builder pattern.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn main() {&#xA;    let config = PositionWiseFeedForwardConfig::new(512, 2048);&#xA;    println!(&#34;{}&#34;, config.d_model); // 512&#xA;    println!(&#34;{}&#34;, config.d_ff); // 2048&#xA;    println!(&#34;{}&#34;, config.dropout); // 0.1&#xA;    let config =  config.with_dropout(0.2);&#xA;    println!(&#34;{}&#34;, config.dropout); // 0.2&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h4&gt;Learner&lt;/h4&gt; &#xA;  &lt;p&gt;The &lt;code&gt;Learner&lt;/code&gt; is the main &lt;code&gt;struct&lt;/code&gt; that let you train a neural network with support for &lt;code&gt;logging&lt;/code&gt;, &lt;code&gt;metric&lt;/code&gt;, &lt;code&gt;checkpointing&lt;/code&gt; and more. In order to create a learner, you must use the &lt;code&gt;LearnerBuilder&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use burn:&lt;span&gt;üöã&lt;/span&gt;:LearnerBuilder;&#xA;use burn:&lt;span&gt;üöã&lt;/span&gt;:metric::{AccuracyMetric, LossMetric};&#xA;use burn::record::DefaultRecordSettings;&#xA;&#xA;fn main() {&#xA;    let dataloader_train = ...;&#xA;    let dataloader_valid = ...;&#xA;&#xA;    let model = ...;&#xA;    let optim = ...;&#xA;&#xA;    let learner = LearnerBuilder::new(&#34;/tmp/artifact_dir&#34;)&#xA;        .metric_train_plot(AccuracyMetric::new())&#xA;        .metric_valid_plot(AccuracyMetric::new())&#xA;        .metric_train(LossMetric::new())&#xA;        .metric_valid(LossMetric::new())&#xA;        .with_file_checkpointer::&amp;lt;DefaultRecordSettings&amp;gt;(2)&#xA;        .num_epochs(10)&#xA;        .build(model, optim);&#xA;&#xA;    let _model_trained = learner.fit(dataloader_train, dataloader_valid);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;See this &lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/examples/mnist&#34;&gt;example&lt;/a&gt; for a real usage.&lt;/p&gt; &#xA;  &lt;h2&gt;Support for &lt;code&gt;no_std&lt;/code&gt;&lt;/h2&gt; &#xA;  &lt;p&gt;Burn, including its &lt;code&gt;burn-ndarray&lt;/code&gt; backend, can work in a &lt;code&gt;no_std&lt;/code&gt; environment, provided &lt;code&gt;alloc&lt;/code&gt; is available for the inference mode. To accomplish this, simply turn off the default features in &lt;code&gt;burn&lt;/code&gt; and &lt;code&gt;burn-ndarray&lt;/code&gt; (which is the minimum requirement for running the inference mode). You can find a reference example in &lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/burn-no-std-tests&#34;&gt;burn-no-std-tests&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;The &lt;code&gt;burn-core&lt;/code&gt; and &lt;code&gt;burn-tensor&lt;/code&gt; crates also support &lt;code&gt;no_std&lt;/code&gt; with &lt;code&gt;alloc&lt;/code&gt;. These crates can be directly added as dependencies if necessary, as they are reexported by the &lt;code&gt;burn&lt;/code&gt; crate.&lt;/p&gt; &#xA;  &lt;p&gt;Please be aware that when using the &lt;code&gt;no_std&lt;/code&gt; mode, a random seed will be generated at build time if one hasn&#39;t been set using the &lt;code&gt;Backend::seed&lt;/code&gt; method. Also, the &lt;a href=&#34;https://docs.rs/spin/latest/spin/mutex/struct.Mutex.html&#34;&gt;spin::mutex::Mutex&lt;/a&gt; is used instead of &lt;a href=&#34;https://doc.rust-lang.org/std/sync/struct.Mutex.html&#34;&gt;std::sync::Mutex&lt;/a&gt; in this mode.&lt;/p&gt; &#xA;  &lt;h2&gt;Contributing&lt;/h2&gt; &#xA;  &lt;p&gt;Before contributing, please take a moment to review our &lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/CODE-OF-CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. It&#39;s also highly recommended to read our &lt;a href=&#34;https://github.com/burn-rs/burn/tree/main/ARCHITECTURE.md&#34;&gt;architecture document&lt;/a&gt;, which explains our architectural decisions. Please see more details in our &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;CI&lt;/h2&gt; &#xA;  &lt;h3&gt;Publish crates&lt;/h3&gt; &#xA;  &lt;p&gt;Compile &lt;code&gt;scripts/publish.rs&lt;/code&gt; using this command:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code&gt;rustc scripts/publish.rs --crate-type bin --out-dir scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;  &lt;p&gt;Burn is currently in active development, and there will be breaking changes. While any resulting issues are likely to be easy to fix, there are no guarantees at this stage.&lt;/p&gt; &#xA;  &lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;  &lt;p&gt;You can sponsor the founder of Burn from his &lt;a href=&#34;https://github.com/sponsors/nathanielsimard&#34;&gt;GitHub Sponsors profile&lt;/a&gt;. The Burn-rs organization doesn&#39;t yet have a fiscal entity, but other sponsor methods might become available as the project grows.&lt;/p&gt; &#xA;  &lt;p&gt;Thanks to all current sponsors üôè.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/smallstepman&#34;&gt;&lt;img src=&#34;https://github.com/smallstepman.png&#34; width=&#34;60px&#34; style=&#34;border-radius: 50%;&#34; alt=&#34;smallstepman&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/premAI-io&#34;&gt;&lt;img src=&#34;https://github.com/premAI-io.png&#34; width=&#34;60px&#34; style=&#34;border-radius: 50%;&#34; alt=&#34;premAI-io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h2&gt;License&lt;/h2&gt; &#xA;  &lt;p&gt;Burn is distributed under the terms of both the MIT license and the Apache License (Version 2.0). See &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/burn-rs/burn/main/LICENSE-MIT&#34;&gt;LICENSE-MIT&lt;/a&gt; for details. Opening a pull request is assumed to signal agreement with these licensing terms.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>srush/llama2.rs</title>
    <updated>2023-08-13T02:00:09Z</updated>
    <id>tag:github.com,2023-08-13:/srush/llama2.rs</id>
    <link href="https://github.com/srush/llama2.rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama2.rs&lt;/h1&gt; &#xA;&lt;p&gt;This is a one-file Rust implementation of Llama2.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for 4-bit GPT-Q Quantization&lt;/li&gt; &#xA; &lt;li&gt;SIMD support for fast CPU inference&lt;/li&gt; &#xA; &lt;li&gt;Support for Grouped Query Attention (needed for big Llamas)&lt;/li&gt; &#xA; &lt;li&gt;Memory mapping, loads 70B instantly.&lt;/li&gt; &#xA; &lt;li&gt;Static size checks, no pointers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Can run up on &lt;em&gt;1 tok/s&lt;/em&gt; 70B Llama2. (intel i9 with avx512)&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/srush/llama2.rs/assets/35882/dac9a285-b141-409f-bb46-c81a28516cd1&#34; width=&#34;300px&#34;&gt; &#xA;&lt;p&gt;To build (requires +nightly to use SIMD, get with rustup):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; rustup toolchain install nightly # to get nightly&#xA;&amp;gt; cargo +nightly build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you get a build error you may need to change .cargo/config to match your chipset.&lt;/p&gt; &#xA;&lt;p&gt;To get model (loads &lt;a href=&#34;https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ&#34;&gt;70B quantized&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torch transformers auto-gptq&#xA;python export.py llama2-70b-q.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; target/release/llama2_rs llama2-70b-q.bin 0.0 11 &#34;The only thing&#34;                                                                                                                                 &#xA;The only thing that I can think of is that the          &#xA;achieved tok/s: 0.89155835&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Honestly, not so bad for running on my GPU machine, significantly faster than llama.c.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a run of 13B quantized:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; target/release/llama2_rs llama2_7b.bin 0.0 11 &#34;One thing is that&#34;&#xA;One thing is that the 100% of the people who are in the 1%&#xA;achieved tok/s: 4.027984&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a run of 7B quantized:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; target/release/llama2_rs llama2_7b.bin 0.0 11 &#34;The only thing&#34;&#xA;The only thing that is certain in life is change.&#xA;achieved tok/s: 7.9735823&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;In order to make the model as fast as possible, you need to compile a new version to adapt to other Llama versions. Currently in &lt;code&gt;.cargo/config&lt;/code&gt;. The model will fail if these disagree with the binary model that is being loaded.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;// Configuration for Llama 70B. Others in config.txt                                                                                          &#xA;const DIM: usize = 8192;                                                                                                                      &#xA;const HIDDEN_DIM: usize = 28672;                                                                                                              &#xA;const ATTN_GROUPS: usize = 8;                                                                                                                 &#xA;const N_LAYERS: usize = 80;                                                                                                                   &#xA;const N_HEADS: usize = 64;                                                                                                                    &#xA;const SEQ_LEN: usize = 2048;                                                                                                                  &#xA;const VOCAB_SIZE: usize = 32000;                                                                                                              &#xA;                                                                                                                                              &#xA;// Grouped Query Attention                                                                                                                    &#xA;const KV_DIM: usize = DIM / ATTN_GROUPS;                                                                                                      &#xA;const N_KV_HEADS: usize = N_HEADS / ATTN_GROUPS;                                                                                              &#xA;const HEAD_SIZE: usize = DIM / N_HEADS;                                                                                                       &#xA;                                                                                                                                              &#xA;// Turn on GPT-Q Quantization.                                                                                                                &#xA;type TWeights = QTransformerWeights;                                                                                                          &#xA;const BITS: usize = 4;                                                                                                                        &#xA;const GROUPSIZE: usize = 128; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;See Also&lt;/h3&gt; &#xA;&lt;p&gt;Originally, a Rust port of Karpathy&#39;s &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; but now has a bunch more features to make it scale to 70B.&lt;/p&gt; &#xA;&lt;p&gt;Also check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gaxler/llama2.rs&#34;&gt;llama2.rs&lt;/a&gt; from @gaxler&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leo-du/llama2.rs&#34;&gt;llama2.rs&lt;/a&gt; from @leo-du&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LaurentMazare/candle&#34;&gt;candle&lt;/a&gt; and candle llama from @LaurentMazare&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How does it work?&lt;/h3&gt; &#xA;&lt;p&gt;Started as a port of the original code, with extra type information to make it easier to extend.&lt;/p&gt; &#xA;&lt;p&gt;There are two dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;memmap2&lt;/code&gt;for memory mapping&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rayon&lt;/code&gt; for parallel computation.&lt;/li&gt; &#xA; &lt;li&gt;SIMD enabled support with +nightly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why?&lt;/h3&gt; &#xA;&lt;p&gt;Mostly this was an exercise in learning some Rust. Was curious how you port over things like memory mapping, parallel processing, and some of the mathematical tricks.&lt;/p&gt; &#xA;&lt;p&gt;This is my first Rust project, so if you are an expert I would love a code review!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>astral-sh/ruff</title>
    <updated>2023-08-13T02:00:09Z</updated>
    <id>tag:github.com,2023-08-13:/astral-sh/ruff</id>
    <link href="https://github.com/astral-sh/ruff" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An extremely fast Python linter, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ruff&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/astral-sh/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&#34; alt=&#34;Ruff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/ruff.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/ruff.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/ruff&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/ruff.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/astral-sh/ruff/actions&#34;&gt;&lt;img src=&#34;https://github.com/astral-sh/ruff/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/c9MhzV8aU5&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://beta.ruff.rs/docs/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://play.ruff.rs/&#34;&gt;&lt;strong&gt;Playground&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An extremely fast Python linter, written in Rust.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture align=&#34;center&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://user-images.githubusercontent.com/1309177/232603514-c95e9b0f-6b31-43de-9a80-9e844173fd6a.svg&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg&#34;&gt; &#xA;  &lt;img alt=&#34;Shows a bar chart with benchmark results.&#34; src=&#34;https://user-images.githubusercontent.com/1309177/232603516-4fb4892d-585c-4b20-b810-3db9161831e4.svg?sanitize=true&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;Linting the CPython codebase from scratch.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ö°Ô∏è 10-100x faster than existing linters&lt;/li&gt; &#xA; &lt;li&gt;üêç Installable via &lt;code&gt;pip&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;code&gt;pyproject.toml&lt;/code&gt; support&lt;/li&gt; &#xA; &lt;li&gt;ü§ù Python 3.11 compatibility&lt;/li&gt; &#xA; &lt;li&gt;üì¶ Built-in caching, to avoid re-analyzing unchanged files&lt;/li&gt; &#xA; &lt;li&gt;üîß Autofix support, for automatic error correction (e.g., automatically remove unused imports)&lt;/li&gt; &#xA; &lt;li&gt;üìè Over &lt;a href=&#34;https://beta.ruff.rs/docs/rules/&#34;&gt;500 built-in rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚öñÔ∏è &lt;a href=&#34;https://beta.ruff.rs/docs/faq/#how-does-ruff-compare-to-flake8&#34;&gt;Near-parity&lt;/a&gt; with the built-in Flake8 rule set&lt;/li&gt; &#xA; &lt;li&gt;üîå Native re-implementations of dozens of Flake8 plugins, like flake8-bugbear&lt;/li&gt; &#xA; &lt;li&gt;‚å®Ô∏è First-party &lt;a href=&#34;https://beta.ruff.rs/docs/editor-integrations/&#34;&gt;editor integrations&lt;/a&gt; for &lt;a href=&#34;https://github.com/astral-sh/ruff-vscode&#34;&gt;VS Code&lt;/a&gt; and &lt;a href=&#34;https://github.com/astral-sh/ruff-lsp&#34;&gt;more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üåé Monorepo-friendly, with &lt;a href=&#34;https://beta.ruff.rs/docs/configuration/#pyprojecttoml-discovery&#34;&gt;hierarchical and cascading configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ruff aims to be orders of magnitude faster than alternative tools while integrating more functionality behind a single, common interface.&lt;/p&gt; &#xA;&lt;p&gt;Ruff can be used to replace &lt;a href=&#34;https://pypi.org/project/flake8/&#34;&gt;Flake8&lt;/a&gt; (plus dozens of plugins), &lt;a href=&#34;https://pypi.org/project/isort/&#34;&gt;isort&lt;/a&gt;, &lt;a href=&#34;https://pypi.org/project/pydocstyle/&#34;&gt;pydocstyle&lt;/a&gt;, &lt;a href=&#34;https://github.com/asottile/yesqa&#34;&gt;yesqa&lt;/a&gt;, &lt;a href=&#34;https://pypi.org/project/eradicate/&#34;&gt;eradicate&lt;/a&gt;, &lt;a href=&#34;https://pypi.org/project/pyupgrade/&#34;&gt;pyupgrade&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/autoflake/&#34;&gt;autoflake&lt;/a&gt;, all while executing tens or hundreds of times faster than any individual tool.&lt;/p&gt; &#xA;&lt;p&gt;Ruff is extremely actively developed and used in major open-source projects like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow&#34;&gt;Apache Airflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tiangolo/fastapi&#34;&gt;FastAPI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pandas-dev/pandas&#34;&gt;Pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scipy/scipy&#34;&gt;SciPy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;...and many more.&lt;/p&gt; &#xA;&lt;p&gt;Ruff is backed by &lt;a href=&#34;https://astral.sh&#34;&gt;Astral&lt;/a&gt;. Read the &lt;a href=&#34;https://astral.sh/blog/announcing-astral-the-company-behind-ruff&#34;&gt;launch post&lt;/a&gt;, or the original &lt;a href=&#34;https://notes.crmarsh.com/python-tooling-could-be-much-much-faster&#34;&gt;project announcement&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Testimonials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/tiangolo/status/1591912354882764802&#34;&gt;&lt;strong&gt;Sebasti√°n Ram√≠rez&lt;/strong&gt;&lt;/a&gt;, creator of &lt;a href=&#34;https://github.com/tiangolo/fastapi&#34;&gt;FastAPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ruff is so fast that sometimes I add an intentional bug in the code just to confirm it&#39;s actually running and checking the code.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/schrockn/status/1612615862904827904&#34;&gt;&lt;strong&gt;Nick Schrock&lt;/strong&gt;&lt;/a&gt;, founder of &lt;a href=&#34;https://www.elementl.com/&#34;&gt;Elementl&lt;/a&gt;, co-creator of &lt;a href=&#34;https://graphql.org/&#34;&gt;GraphQL&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Why is Ruff a gamechanger? Primarily because it is nearly 1000x faster. Literally. Not a typo. On our largest module (dagster itself, 250k LOC) pylint takes about 2.5 minutes, parallelized across 4 cores on my M1. Running ruff against our &lt;em&gt;entire&lt;/em&gt; codebase takes .4 seconds.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bokeh/bokeh/pull/12605&#34;&gt;&lt;strong&gt;Bryan Van de Ven&lt;/strong&gt;&lt;/a&gt;, co-creator of &lt;a href=&#34;https://github.com/bokeh/bokeh/&#34;&gt;Bokeh&lt;/a&gt;, original author of &lt;a href=&#34;https://docs.conda.io/en/latest/&#34;&gt;Conda&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ruff is ~150-200x faster than flake8 on my machine, scanning the whole repo takes ~0.2s instead of ~20s. This is an enormous quality of life improvement for local dev. It&#39;s fast enough that I added it as an actual commit hook, which is terrific.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/timothycrosley/status/1606420868514877440&#34;&gt;&lt;strong&gt;Timothy Crosley&lt;/strong&gt;&lt;/a&gt;, creator of &lt;a href=&#34;https://github.com/PyCQA/isort&#34;&gt;isort&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Just switched my first project to Ruff. Only one downside so far: it&#39;s so fast I couldn&#39;t believe it was working till I intentionally introduced some errors.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/astral-sh/ruff/issues/465#issuecomment-1317400028&#34;&gt;&lt;strong&gt;Tim Abbott&lt;/strong&gt;&lt;/a&gt;, lead developer of &lt;a href=&#34;https://github.com/zulip/zulip&#34;&gt;Zulip&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This is just ridiculously fast... &lt;code&gt;ruff&lt;/code&gt; is amazing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- End section: Overview --&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;p&gt;For more, see the &lt;a href=&#34;https://beta.ruff.rs/docs/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#rules&#34;&gt;Rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#whos-using-ruff&#34;&gt;Who&#39;s Using Ruff?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;For more, see the &lt;a href=&#34;https://beta.ruff.rs/docs/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Ruff is available as &lt;a href=&#34;https://pypi.org/project/ruff/&#34;&gt;&lt;code&gt;ruff&lt;/code&gt;&lt;/a&gt; on PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install ruff&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also install Ruff via &lt;a href=&#34;https://formulae.brew.sh/formula/ruff&#34;&gt;Homebrew&lt;/a&gt;, &lt;a href=&#34;https://anaconda.org/conda-forge/ruff&#34;&gt;Conda&lt;/a&gt;, and with &lt;a href=&#34;https://beta.ruff.rs/docs/installation/&#34;&gt;a variety of other package managers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;To run Ruff, try any of the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ruff check .                        # Lint all files in the current directory (and any subdirectories)&#xA;ruff check path/to/code/            # Lint all files in `/path/to/code` (and any subdirectories)&#xA;ruff check path/to/code/*.py        # Lint all `.py` files in `/path/to/code`&#xA;ruff check path/to/code/to/file.py  # Lint `file.py`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ruff can also be used as a &lt;a href=&#34;https://pre-commit.com&#34;&gt;pre-commit&lt;/a&gt; hook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- repo: https://github.com/astral-sh/ruff-pre-commit&#xA;  # Ruff version.&#xA;  rev: v0.0.284&#xA;  hooks:&#xA;    - id: ruff&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ruff can also be used as a &lt;a href=&#34;https://github.com/astral-sh/ruff-vscode&#34;&gt;VS Code extension&lt;/a&gt; or alongside any other editor through the &lt;a href=&#34;https://github.com/astral-sh/ruff-lsp&#34;&gt;Ruff LSP&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ruff can also be used as a &lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Action&lt;/a&gt; via &lt;a href=&#34;https://github.com/chartboost/ruff-action&#34;&gt;&lt;code&gt;ruff-action&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;name: Ruff&#xA;on: [ push, pull_request ]&#xA;jobs:&#xA;  ruff:&#xA;    runs-on: ubuntu-latest&#xA;    steps:&#xA;      - uses: actions/checkout@v3&#xA;      - uses: chartboost/ruff-action@v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Ruff can be configured through a &lt;code&gt;pyproject.toml&lt;/code&gt;, &lt;code&gt;ruff.toml&lt;/code&gt;, or &lt;code&gt;.ruff.toml&lt;/code&gt; file (see: &lt;a href=&#34;https://beta.ruff.rs/docs/configuration/&#34;&gt;&lt;em&gt;Configuration&lt;/em&gt;&lt;/a&gt;, or &lt;a href=&#34;https://beta.ruff.rs/docs/settings/&#34;&gt;&lt;em&gt;Settings&lt;/em&gt;&lt;/a&gt; for a complete list of all configuration options).&lt;/p&gt; &#xA;&lt;p&gt;If left unspecified, the default configuration is equivalent to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[tool.ruff]&#xA;# Enable pycodestyle (`E`) and Pyflakes (`F`) codes by default.&#xA;select = [&#34;E&#34;, &#34;F&#34;]&#xA;ignore = []&#xA;&#xA;# Allow autofix for all enabled rules (when `--fix`) is provided.&#xA;fixable = [&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;, &#34;E&#34;, &#34;F&#34;, &#34;G&#34;, &#34;I&#34;, &#34;N&#34;, &#34;Q&#34;, &#34;S&#34;, &#34;T&#34;, &#34;W&#34;, &#34;ANN&#34;, &#34;ARG&#34;, &#34;BLE&#34;, &#34;COM&#34;, &#34;DJ&#34;, &#34;DTZ&#34;, &#34;EM&#34;, &#34;ERA&#34;, &#34;EXE&#34;, &#34;FBT&#34;, &#34;ICN&#34;, &#34;INP&#34;, &#34;ISC&#34;, &#34;NPY&#34;, &#34;PD&#34;, &#34;PGH&#34;, &#34;PIE&#34;, &#34;PL&#34;, &#34;PT&#34;, &#34;PTH&#34;, &#34;PYI&#34;, &#34;RET&#34;, &#34;RSE&#34;, &#34;RUF&#34;, &#34;SIM&#34;, &#34;SLF&#34;, &#34;TCH&#34;, &#34;TID&#34;, &#34;TRY&#34;, &#34;UP&#34;, &#34;YTT&#34;]&#xA;unfixable = []&#xA;&#xA;# Exclude a variety of commonly ignored directories.&#xA;exclude = [&#xA;    &#34;.bzr&#34;,&#xA;    &#34;.direnv&#34;,&#xA;    &#34;.eggs&#34;,&#xA;    &#34;.git&#34;,&#xA;    &#34;.git-rewrite&#34;,&#xA;    &#34;.hg&#34;,&#xA;    &#34;.mypy_cache&#34;,&#xA;    &#34;.nox&#34;,&#xA;    &#34;.pants.d&#34;,&#xA;    &#34;.pytype&#34;,&#xA;    &#34;.ruff_cache&#34;,&#xA;    &#34;.svn&#34;,&#xA;    &#34;.tox&#34;,&#xA;    &#34;.venv&#34;,&#xA;    &#34;__pypackages__&#34;,&#xA;    &#34;_build&#34;,&#xA;    &#34;buck-out&#34;,&#xA;    &#34;build&#34;,&#xA;    &#34;dist&#34;,&#xA;    &#34;node_modules&#34;,&#xA;    &#34;venv&#34;,&#xA;]&#xA;&#xA;# Same as Black.&#xA;line-length = 88&#xA;&#xA;# Allow unused variables when underscore-prefixed.&#xA;dummy-variable-rgx = &#34;^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$&#34;&#xA;&#xA;# Assume Python 3.8&#xA;target-version = &#34;py38&#34;&#xA;&#xA;[tool.ruff.mccabe]&#xA;# Unlike Flake8, default to a complexity level of 10.&#xA;max-complexity = 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some configuration options can be provided via the command-line, such as those related to rule enablement and disablement, file discovery, logging level, and more:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ruff check path/to/code/ --select F401 --select F403 --quiet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;ruff help&lt;/code&gt; for more on Ruff&#39;s top-level commands, or &lt;code&gt;ruff help check&lt;/code&gt; for more on the linting command.&lt;/p&gt; &#xA;&lt;h2&gt;Rules&lt;/h2&gt; &#xA;&lt;!-- Begin section: Rules --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ruff supports over 500 lint rules&lt;/strong&gt;, many of which are inspired by popular tools like Flake8, isort, pyupgrade, and others. Regardless of the rule&#39;s origin, Ruff re-implements every rule in Rust as a first-party feature.&lt;/p&gt; &#xA;&lt;p&gt;By default, Ruff enables Flake8&#39;s &lt;code&gt;E&lt;/code&gt; and &lt;code&gt;F&lt;/code&gt; rules. Ruff supports all rules from the &lt;code&gt;F&lt;/code&gt; category, and a &lt;a href=&#34;https://beta.ruff.rs/docs/rules/#error-e&#34;&gt;subset&lt;/a&gt; of the &lt;code&gt;E&lt;/code&gt; category, omitting those stylistic rules made obsolete by the use of an autoformatter, like &lt;a href=&#34;https://github.com/psf/black&#34;&gt;Black&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re just getting started with Ruff, &lt;strong&gt;the default rule set is a great place to start&lt;/strong&gt;: it catches a wide variety of common errors (like unused imports) with zero configuration.&lt;/p&gt; &#xA;&lt;!-- End section: Rules --&gt; &#xA;&lt;p&gt;Beyond the defaults, Ruff re-implements some of the most popular Flake8 plugins and related code quality tools, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/autoflake/&#34;&gt;autoflake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/eradicate/&#34;&gt;eradicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-2020/&#34;&gt;flake8-2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-annotations/&#34;&gt;flake8-annotations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-async&#34;&gt;flake8-async&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-bandit/&#34;&gt;flake8-bandit&lt;/a&gt; (&lt;a href=&#34;https://github.com/astral-sh/ruff/issues/1646&#34;&gt;#1646&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-blind-except/&#34;&gt;flake8-blind-except&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-boolean-trap/&#34;&gt;flake8-boolean-trap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-bugbear/&#34;&gt;flake8-bugbear&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-builtins/&#34;&gt;flake8-builtins&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-commas/&#34;&gt;flake8-commas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-comprehensions/&#34;&gt;flake8-comprehensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-copyright/&#34;&gt;flake8-copyright&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-datetimez/&#34;&gt;flake8-datetimez&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-debugger/&#34;&gt;flake8-debugger&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-django/&#34;&gt;flake8-django&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-docstrings/&#34;&gt;flake8-docstrings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-eradicate/&#34;&gt;flake8-eradicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-errmsg/&#34;&gt;flake8-errmsg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-executable/&#34;&gt;flake8-executable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-future-annotations/&#34;&gt;flake8-future-annotations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-gettext/&#34;&gt;flake8-gettext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-implicit-str-concat/&#34;&gt;flake8-implicit-str-concat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joaopalmeiro/flake8-import-conventions&#34;&gt;flake8-import-conventions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-logging-format/&#34;&gt;flake8-logging-format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-no-pep420&#34;&gt;flake8-no-pep420&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-pie/&#34;&gt;flake8-pie&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-print/&#34;&gt;flake8-print&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-pyi/&#34;&gt;flake8-pyi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-pytest-style/&#34;&gt;flake8-pytest-style&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-quotes/&#34;&gt;flake8-quotes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-raise/&#34;&gt;flake8-raise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-return/&#34;&gt;flake8-return&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-self/&#34;&gt;flake8-self&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-simplify/&#34;&gt;flake8-simplify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-slots/&#34;&gt;flake8-slots&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-super/&#34;&gt;flake8-super&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-tidy-imports/&#34;&gt;flake8-tidy-imports&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-todos/&#34;&gt;flake8-todos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-type-checking/&#34;&gt;flake8-type-checking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flake8-use-pathlib/&#34;&gt;flake8-use-pathlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/flynt/&#34;&gt;flynt&lt;/a&gt; (&lt;a href=&#34;https://github.com/astral-sh/ruff/issues/2102&#34;&gt;#2102&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/isort/&#34;&gt;isort&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/mccabe/&#34;&gt;mccabe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pandas-vet/&#34;&gt;pandas-vet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pep8-naming/&#34;&gt;pep8-naming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pydocstyle/&#34;&gt;pydocstyle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pre-commit/pygrep-hooks&#34;&gt;pygrep-hooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pylint-airflow/&#34;&gt;pylint-airflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pyupgrade/&#34;&gt;pyupgrade&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/tryceratops/&#34;&gt;tryceratops&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/yesqa/&#34;&gt;yesqa&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a complete enumeration of the supported rules, see &lt;a href=&#34;https://beta.ruff.rs/docs/rules/&#34;&gt;&lt;em&gt;Rules&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome and highly appreciated. To get started, check out the &lt;a href=&#34;https://beta.ruff.rs/docs/contributing/&#34;&gt;&lt;strong&gt;contributing guidelines&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also join us on &lt;a href=&#34;https://discord.gg/c9MhzV8aU5&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Having trouble? Check out the existing issues on &lt;a href=&#34;https://github.com/astral-sh/ruff/issues&#34;&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt;, or feel free to &lt;a href=&#34;https://github.com/astral-sh/ruff/issues/new&#34;&gt;&lt;strong&gt;open a new one&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also ask for help on &lt;a href=&#34;https://discord.gg/c9MhzV8aU5&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Ruff&#39;s linter draws on both the APIs and implementation details of many other tools in the Python ecosystem, especially &lt;a href=&#34;https://github.com/PyCQA/flake8&#34;&gt;Flake8&lt;/a&gt;, &lt;a href=&#34;https://github.com/PyCQA/pyflakes&#34;&gt;Pyflakes&lt;/a&gt;, &lt;a href=&#34;https://github.com/PyCQA/pycodestyle&#34;&gt;pycodestyle&lt;/a&gt;, &lt;a href=&#34;https://github.com/PyCQA/pydocstyle&#34;&gt;pydocstyle&lt;/a&gt;, &lt;a href=&#34;https://github.com/asottile/pyupgrade&#34;&gt;pyupgrade&lt;/a&gt;, and &lt;a href=&#34;https://github.com/PyCQA/isort&#34;&gt;isort&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In some cases, Ruff includes a &#34;direct&#34; Rust port of the corresponding tool. We&#39;re grateful to the maintainers of these tools for their work, and for all the value they&#39;ve provided to the Python community.&lt;/p&gt; &#xA;&lt;p&gt;Ruff&#39;s autoformatter is built on a fork of Rome&#39;s &lt;a href=&#34;https://github.com/rome/tools/tree/main/crates/rome_formatter&#34;&gt;&lt;code&gt;rome_formatter&lt;/code&gt;&lt;/a&gt;, and again draws on both API and implementation details from &lt;a href=&#34;https://github.com/rome/tools&#34;&gt;Rome&lt;/a&gt;, &lt;a href=&#34;https://github.com/prettier/prettier&#34;&gt;Prettier&lt;/a&gt;, and &lt;a href=&#34;https://github.com/psf/black&#34;&gt;Black&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ruff&#39;s import resolver is based on the import resolution algorithm from &lt;a href=&#34;https://github.com/microsoft/pyright&#34;&gt;Pyright&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ruff is also influenced by a number of tools outside the Python ecosystem, like &lt;a href=&#34;https://github.com/rust-lang/rust-clippy&#34;&gt;Clippy&lt;/a&gt; and &lt;a href=&#34;https://github.com/eslint/eslint&#34;&gt;ESLint&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ruff is the beneficiary of a large number of &lt;a href=&#34;https://github.com/astral-sh/ruff/graphs/contributors&#34;&gt;contributors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ruff is released under the MIT license.&lt;/p&gt; &#xA;&lt;h2&gt;Who&#39;s Using Ruff?&lt;/h2&gt; &#xA;&lt;p&gt;Ruff is used by a number of major open-source projects and companies, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amazon (&lt;a href=&#34;https://github.com/aws/serverless-application-model&#34;&gt;AWS SAM&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Anthropic (&lt;a href=&#34;https://github.com/anthropics/anthropic-sdk-python&#34;&gt;Python SDK&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apache/airflow&#34;&gt;Apache Airflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AstraZeneca (&lt;a href=&#34;https://github.com/AstraZeneca/magnus-core&#34;&gt;Magnus&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Benchling (&lt;a href=&#34;https://github.com/benchling/refac&#34;&gt;Refac&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-babel/babel&#34;&gt;Babel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bokeh/bokeh&#34;&gt;Bokeh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pyca/cryptography&#34;&gt;Cryptography (PyCA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iterative/dvc&#34;&gt;DVC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dagger/dagger&#34;&gt;Dagger&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dagster-io/dagster&#34;&gt;Dagster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Databricks (&lt;a href=&#34;https://github.com/mlflow/mlflow&#34;&gt;MLflow&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tiangolo/fastapi&#34;&gt;FastAPI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/great-expectations/great_expectations&#34;&gt;Great Expectations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/encode/httpx&#34;&gt;HTTPX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face (&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Datasets&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;Diffusers&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pypa/hatch&#34;&gt;Hatch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/home-assistant/core&#34;&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ING Bank (&lt;a href=&#34;https://github.com/ing-bank/popmon&#34;&gt;popmon&lt;/a&gt;, &lt;a href=&#34;https://github.com/ing-bank/probatus&#34;&gt;probatus&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ibis-project/ibis&#34;&gt;Ibis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jupyter-server/jupyter_server&#34;&gt;Jupyter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Matrix (&lt;a href=&#34;https://github.com/matrix-org/synapse&#34;&gt;Synapse&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oxsecurity/megalinter&#34;&gt;MegaLinter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Meltano (&lt;a href=&#34;https://github.com/meltano/meltano&#34;&gt;Meltano CLI&lt;/a&gt;, &lt;a href=&#34;https://github.com/meltano/sdk&#34;&gt;Singer SDK&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Microsoft (&lt;a href=&#34;https://github.com/microsoft/semantic-kernel&#34;&gt;Semantic Kernel&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNX Runtime&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/LightGBM&#34;&gt;LightGBM&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Modern Treasury (&lt;a href=&#34;https://github.com/Modern-Treasury/modern-treasury-python-sdk&#34;&gt;Python SDK&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mozilla (&lt;a href=&#34;https://github.com/mozilla/gecko-dev&#34;&gt;Firefox&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python/mypy&#34;&gt;Mypy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Netflix (&lt;a href=&#34;https://github.com/Netflix/dispatch&#34;&gt;Dispatch&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/neondatabase/neon&#34;&gt;Neon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/onnx&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenBB-finance/OpenBBTerminal&#34;&gt;OpenBB&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pdm-project/pdm&#34;&gt;PDM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pandas-dev/pandas&#34;&gt;Pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/python-poetry/poetry&#34;&gt;Poetry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pola-rs/polars&#34;&gt;Polars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PostHog/posthog&#34;&gt;PostHog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prefect (&lt;a href=&#34;https://github.com/PrefectHQ/prefect&#34;&gt;Python SDK&lt;/a&gt;, &lt;a href=&#34;https://github.com/PrefectHQ/marvin&#34;&gt;Marvin&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pyinstaller/pyinstaller&#34;&gt;PyInstaller&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pydantic/pydantic&#34;&gt;Pydantic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PyCQA/pylint&#34;&gt;Pylint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reflex-dev/reflex&#34;&gt;Reflex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sansyrox/robyn&#34;&gt;Robyn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scale AI (&lt;a href=&#34;https://github.com/scaleapi/launch-python-client&#34;&gt;Launch SDK&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Snowflake (&lt;a href=&#34;https://github.com/Snowflake-Labs/snowcli&#34;&gt;SnowCLI&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saleor/saleor&#34;&gt;Saleor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scipy/scipy&#34;&gt;SciPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sphinx-doc/sphinx&#34;&gt;Sphinx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt;Stable Baselines3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://litestar.dev/&#34;&gt;Litestar&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TheAlgorithms/Python&#34;&gt;The Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/altair-viz/altair&#34;&gt;Vega-Altair&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WordPress (&lt;a href=&#34;https://github.com/WordPress/openverse&#34;&gt;Openverse&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zenml-io/zenml&#34;&gt;ZenML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zulip/zulip&#34;&gt;Zulip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pypa/build&#34;&gt;build (PyPA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pypa/cibuildwheel&#34;&gt;cibuildwheel (PyPA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alteryx/featuretools&#34;&gt;featuretools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mesonbuild/meson-python&#34;&gt;meson-python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wntrblm/nox&#34;&gt;nox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pypa/pip&#34;&gt;pip&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Show Your Support&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re using Ruff, consider adding the Ruff badge to project&#39;s &lt;code&gt;README.md&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or &lt;code&gt;README.rst&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rst&#34;&gt;.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&#xA;    :target: https://github.com/astral-sh/ruff&#xA;    :alt: Ruff&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...or, as HTML:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;a href=&#34;https://github.com/astral-sh/ruff&#34;&amp;gt;&amp;lt;img src=&#34;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&#34; alt=&#34;Ruff&#34; style=&#34;max-width:100%;&#34;&amp;gt;&amp;lt;/a&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://astral.sh&#34; style=&#34;background:none&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/astral-sh/ruff/main/assets/svg/Astral.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>