<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-02T01:48:03Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>davidlattimore/wild</title>
    <updated>2025-02-02T01:48:03Z</updated>
    <id>tag:github.com,2025-02-02:/davidlattimore/wild</id>
    <link href="https://github.com/davidlattimore/wild" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A very fast linker for Linux&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Wild linker&lt;/h1&gt; &#xA;&lt;p&gt;Wild is a linker with the goal of being very fast for iterative development.&lt;/p&gt; &#xA;&lt;p&gt;The plan is to eventually make it incremental, however that isn&#39;t yet implemented. It is however already pretty fast even without incremental linking.&lt;/p&gt; &#xA;&lt;p&gt;For production builds, its recommended to use a more mature linker like GNU ld or LLD.&lt;/p&gt; &#xA;&lt;p&gt;During development, if you&#39;d like faster warm build times, then you could give Wild a try. It&#39;s at the point now where it should be usable for development purposes provided you&#39;re developing on x86-64 Linux. If you hit any issues, please file a bug report.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install a pre-built binary, you can copy and paste the command from the &lt;a href=&#34;https://github.com/davidlattimore/wild/releases&#34;&gt;releases page&lt;/a&gt;. Alternatively, you can download the tarball and manually copy the &lt;code&gt;wild&lt;/code&gt; binary somewhere on your path.&lt;/p&gt; &#xA;&lt;p&gt;To build and install, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cargo install --locked --bin wild --git https://github.com/davidlattimore/wild.git wild&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using as your default linker&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to use Wild as your default linker for building Rust code, you can put the following in &lt;code&gt;~/.cargo/config.toml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[target.x86_64-unknown-linux-gnu]&#xA;linker = &#34;clang&#34;&#xA;rustflags = [&#34;-C&#34;, &#34;link-arg=--ld-path=wild&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h3&gt;Why another linker?&lt;/h3&gt; &#xA;&lt;p&gt;Mold is already very fast, however it doesn&#39;t do incremental linking and the author has stated that they don&#39;t intend to. Wild doesn&#39;t do incremental linking yet, but that is the end-goal. By writing Wild in Rust, it&#39;s hoped that the complexity of incremental linking will be achievable.&lt;/p&gt; &#xA;&lt;h3&gt;What&#39;s working?&lt;/h3&gt; &#xA;&lt;p&gt;The following platforms / architectures are currently supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;x86-64 on Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following is working with the caveat that there may be bugs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Output to statically linked, non-relocatable binaries&lt;/li&gt; &#xA; &lt;li&gt;Output to statically linked, position-independent binaries (static-PIE)&lt;/li&gt; &#xA; &lt;li&gt;Output to dynamically linked binaries&lt;/li&gt; &#xA; &lt;li&gt;Output to shared objects (.so files)&lt;/li&gt; &#xA; &lt;li&gt;Rust proc-macros, when linked with Wild work&lt;/li&gt; &#xA; &lt;li&gt;Most of the top downloaded crates on crates.io have been tested with Wild and pass their tests&lt;/li&gt; &#xA; &lt;li&gt;Debug info&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What isn&#39;t yet supported?&lt;/h3&gt; &#xA;&lt;p&gt;Lots of stuff. Here are some of the larger things that aren&#39;t yet done, roughly sorted by current priority:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Incremental linking&lt;/li&gt; &#xA; &lt;li&gt;Support for architectures other than x86-64&lt;/li&gt; &#xA; &lt;li&gt;Support for a wider range of linker flags&lt;/li&gt; &#xA; &lt;li&gt;Linker scripts&lt;/li&gt; &#xA; &lt;li&gt;Mac support&lt;/li&gt; &#xA; &lt;li&gt;Windows support&lt;/li&gt; &#xA; &lt;li&gt;LTO&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How can I verify that Wild was used to link a binary?&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;code&gt;readelf&lt;/code&gt;, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;readelf  -p .comment my-executable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Look for a line like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Linker: Wild version 0.1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you don&#39;t want to install readelf, you can probably get away with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;strings my-executable | grep &#39;Linker:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Where did the name come from?&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s somewhat of a tradition for linkers to end with the letters &#34;ld&#34;. e.g. &#34;GNU ld, &#34;gold&#34;, &#34;lld&#34;, &#34;mold&#34;. Since the end-goal is for the linker to be incremental, an &#34;I&#34; is added. Let&#39;s say the &#34;W&#34; stands for &#34;Wild&#34;, since recursive acronyms are popular in open-source projects.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;The goal of Wild is to eventually be very fast via incremental linking. However, we also want to be as fast as we can be for non-incremental linking and for the initial link when incremental linking is enabled.&lt;/p&gt; &#xA;&lt;p&gt;These benchmark were run on David Lattimore&#39;s laptop (2020 model System76 Lemur pro), which has 4 cores (8 threads) and 42 GB of RAM.&lt;/p&gt; &#xA;&lt;p&gt;The following times are for linking rustc-driver, which is a shared object that contains most of the code of the Rust compiler. Linking was done with with &lt;code&gt;--strip-debug&lt;/code&gt; and &lt;code&gt;--build-id=none&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linker&lt;/th&gt; &#xA;   &lt;th&gt;Time (ms)&lt;/th&gt; &#xA;   &lt;th&gt;¬± Standard deviation (ms)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNU ld (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;20774&lt;/td&gt; &#xA;   &lt;td&gt;855&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gold (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;6796&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lld (18.1.8)&lt;/td&gt; &#xA;   &lt;td&gt;1601&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mold (2.34.1)&lt;/td&gt; &#xA;   &lt;td&gt;946&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wild (2024-11-30)&lt;/td&gt; &#xA;   &lt;td&gt;486&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The following times are for linking the C compiler, clang without debug info.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linker&lt;/th&gt; &#xA;   &lt;th&gt;Time (ms)&lt;/th&gt; &#xA;   &lt;th&gt;¬± Standard deviation (ms)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNU ld (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;8784&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gold (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;2528&lt;/td&gt; &#xA;   &lt;td&gt;37&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lld (18.1.8)&lt;/td&gt; &#xA;   &lt;td&gt;1679&lt;/td&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mold (2.34.1)&lt;/td&gt; &#xA;   &lt;td&gt;429&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wild (2024-11-30)&lt;/td&gt; &#xA;   &lt;td&gt;244&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Next, let&#39;s add debug info (remove &lt;code&gt;--strip-debug&lt;/code&gt;). First rustc-driver:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linker&lt;/th&gt; &#xA;   &lt;th&gt;Time (ms)&lt;/th&gt; &#xA;   &lt;th&gt;¬± Standard deviation (ms)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNU ld (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;23224&lt;/td&gt; &#xA;   &lt;td&gt;1030&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gold (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;8840&lt;/td&gt; &#xA;   &lt;td&gt;879&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lld (18.1.8)&lt;/td&gt; &#xA;   &lt;td&gt;2741&lt;/td&gt; &#xA;   &lt;td&gt;1403&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mold (2.34.1)&lt;/td&gt; &#xA;   &lt;td&gt;3514&lt;/td&gt; &#xA;   &lt;td&gt;2102&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wild (2024-11-30)&lt;/td&gt; &#xA;   &lt;td&gt;3158&lt;/td&gt; &#xA;   &lt;td&gt;1616&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Now clang with debug info:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linker&lt;/th&gt; &#xA;   &lt;th&gt;Time (ms)&lt;/th&gt; &#xA;   &lt;th&gt;¬± Standard deviation (ms)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNU ld (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;139985&lt;/td&gt; &#xA;   &lt;td&gt;9871&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gold (2.38)&lt;/td&gt; &#xA;   &lt;td&gt;92147&lt;/td&gt; &#xA;   &lt;td&gt;7287&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lld (18.1.8)&lt;/td&gt; &#xA;   &lt;td&gt;30549&lt;/td&gt; &#xA;   &lt;td&gt;9819&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mold (2.34.1)&lt;/td&gt; &#xA;   &lt;td&gt;16933&lt;/td&gt; &#xA;   &lt;td&gt;5359&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wild (2024-11-30)&lt;/td&gt; &#xA;   &lt;td&gt;31540&lt;/td&gt; &#xA;   &lt;td&gt;7133&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;So Wild performs pretty well without debug info, but with debug info, it&#39;s performing less well at the moment.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/davidlattimore/wild/main/BENCHMARKING.md&#34;&gt;BENCHMARKING.md&lt;/a&gt; for more details on benchmarking.&lt;/p&gt; &#xA;&lt;h2&gt;Linking Rust code&lt;/h2&gt; &#xA;&lt;p&gt;The following is a &lt;code&gt;cargo test&lt;/code&gt; command-line that can be used to build and test a crate using Wild. This has been run successfully on a few popular crates (e.g. ripgrep, serde, tokio, rand, bitflags). It assumes that the &#34;wild&#34; binary is on your path. It also depends on the Clang compiler being installed, since GCC doesn&#39;t allow using an arbitrary linker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;RUSTFLAGS=&#34;-Clinker=clang -Clink-args=--ld-path=wild&#34; cargo test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;For more information on contributing to &lt;code&gt;wild&lt;/code&gt; see &lt;a href=&#34;https://raw.githubusercontent.com/davidlattimore/wild/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sponsorship&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to &lt;a href=&#34;https://github.com/sponsors/davidlattimore&#34;&gt;sponsor this work&lt;/a&gt;, that would be very much appreciated. The more sponsorship I get the longer I can continue to work on this project full time.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under either of &lt;a href=&#34;https://raw.githubusercontent.com/davidlattimore/wild/main/LICENSE-APACHE&#34;&gt;Apache License, Version 2.0&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/davidlattimore/wild/main/LICENSE-MIT&#34;&gt;MIT license&lt;/a&gt; at your option.&lt;/p&gt; &#xA;&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Wild by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>BoundaryML/baml</title>
    <updated>2025-02-02T01:48:03Z</updated>
    <id>tag:github.com,2025-02-02:/BoundaryML/baml</id>
    <link href="https://github.com/BoundaryML/baml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;BAML is a language that helps you get structured data from LLMs, with the best DX possible. Works with all languages. Check out the promptfiddle.com playground&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://boundaryml.com?utm_source=github&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;fern/assets/baml-lamb-white.png&#34;&gt; &#xA;   &lt;img src=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/fern/assets/baml-lamb-white.png&#34; height=&#34;64&#34; id=&#34;top&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;BAML: Basically a Made-up Language&lt;/h2&gt; &#xA; &lt;p&gt;&lt;em&gt;or &#34;Basic-Ass Machine Learning&#34; if your boss isn&#39;t around&lt;/em&gt;&lt;/p&gt; &#xA; &lt;h3&gt; &lt;p&gt;&lt;a href=&#34;https://www.boundaryml.com/&#34;&gt;Homepage&lt;/a&gt; | &lt;a href=&#34;https://docs.boundaryml.com&#34;&gt;Docs&lt;/a&gt; | &lt;a href=&#34;https://www.boundaryml.com/chat&#34;&gt;BAML Chat&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/BTNBeXGuaS&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/boundaryml/baml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/boundaryml/baml&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache-green.svg?sanitize=true&#34; alt=&#34;License: Apache-2&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/baml-py/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/baml-py?color=006dad&amp;amp;label=BAML%20Version&#34; alt=&#34;BAML Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Try BAML&lt;/strong&gt;: &lt;a href=&#34;https://www.promptfiddle.com&#34;&gt;Prompt Fiddle&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://baml-examples.vercel.app/&#34;&gt;Examples&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/BoundaryML/baml-examples&#34;&gt;Example Source Code&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;5 minute quickstarts&lt;/strong&gt; &lt;a href=&#34;https://docs.boundaryml.com/guide/installation-language/python&#34;&gt;Python&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/guide/installation-language/typescript&#34;&gt;Typescript&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/guide/installation-language/next-js&#34;&gt;NextJS&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/guide/installation-language/ruby&#34;&gt;Ruby&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/guide/installation-language/rest-api-other-languages&#34;&gt;Others&lt;/a&gt; (Go, Java, C++, Rust, PHP, etc)&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;What is BAML?&lt;/td&gt; &#xA;   &lt;td&gt;BAML is a new programming language for builing AI applications.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Do I need to write my whole app in BAML?&lt;/td&gt; &#xA;   &lt;td&gt;Nope, only the AI parts, you can then use BAML with any existing language of your choice! &lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/python&#34;&gt;python&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/ts&#34;&gt;typescript&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/more&#34;&gt;more&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is BAML stable?&lt;/td&gt; &#xA;   &lt;td&gt;We are still not 1.0, and we ship updates weekly. We rarely, if at all do breaking changes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Why a new language?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/#why-a-new-programming-language&#34;&gt;Jump to section&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Why a lamb?&lt;/td&gt; &#xA;   &lt;td&gt;Baaaaa-ml. LAMB == BAML&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;The core BAML principle: LLM Prompts are functions&lt;/h2&gt; &#xA;&lt;p&gt;The fundamental building block in BAML is a function. Every prompt is a function that takes in parameters and returns a type.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-baml&#34;&gt;function ChatAgent(message: Message[], tone: &#34;happy&#34; | &#34;sad&#34;) -&amp;gt; string&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Every function additionally defines which models it uses and what its prompt is.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-baml&#34;&gt;function ChatAgent(message: Message[], tone: &#34;happy&#34; | &#34;sad&#34;) -&amp;gt; StopTool | ReplyTool {&#xA;    client &#34;openai/gpt-4o-mini&#34;&#xA;&#xA;    prompt #&#34;&#xA;        Be a {{ tone }} bot.&#xA;&#xA;        {{ ctx.output_format }}&#xA;&#xA;        {% for m in message %}&#xA;        {{ _.role(m.role) }}&#xA;        {{ m.content }}&#xA;        {% endfor %}&#xA;    &#34;#&#xA;}&#xA;&#xA;class Message {&#xA;    role string&#xA;    content string&#xA;}&#xA;&#xA;class ReplyTool {&#xA;  response string&#xA;}&#xA;&#xA;class StopTool {&#xA;  action &#34;stop&#34; @description(#&#34;&#xA;    when it might be a good time to end the conversation&#xA;  &#34;#)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in any language of your choice you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from baml_client import b&#xA;from baml_client.types import Message, StopTool&#xA;&#xA;messages = [Message(role=&#34;assistant&#34;, content=&#34;How can I help?&#34;)]&#xA;&#xA;while True:&#xA;  print(messages[-1].content)&#xA;  user_reply = input()&#xA;  messages.append(Message(role=&#34;user&#34;, content=user_reply))&#xA;  tool = b.ChatAgent(messages, &#34;happy&#34;)&#xA;  if isinstance(tool, StopTool):&#xA;    print(&#34;Goodbye!&#34;)&#xA;    break&#xA;  else:&#xA;    messages.append(Message(role=&#34;assistant&#34;, content=tool.reply))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Making prompts easy to find and read&lt;/h3&gt; &#xA;&lt;p&gt;Since every prompt is a function, we can build tools to find every prompt you&#39;ve written. But we&#39;ve taken BAML one step further and built native tooling for VSCode (jetbrains + neovim coming soon).&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can see the full prompt (including any multi-modal assets) &lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/02-multi-modal.gif&#34; alt=&#34;Multi Modal&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can see the exact network request we are making &lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/03-curl-token-preview.gif&#34; alt=&#34;Token Preview&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can see every function you&#39;ve ever written&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/04-functions-preview.png&#34; alt=&#34;Functions&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Swapping models: 1-line change&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s just 1 line (ok, maybe 2). &lt;a href=&#34;https://docs.boundaryml.com/guide/baml-basics/switching-llms&#34;&gt;Docs&lt;/a&gt; &lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/05-sorry-sam.png&#34; alt=&#34;Sorry Sam&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-strategies/retry-policy&#34;&gt;Retry policies&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-strategies/fallback&#34;&gt;fallbacks&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-strategies/round-robin&#34;&gt;model rotations&lt;/a&gt;. All statically defined. &lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/06-fallback-retry.gif&#34; alt=&#34;Fallback Retry&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Want to do pick models at runtime? Check out &lt;a href=&#34;https://docs.boundaryml.com/guide/baml-advanced/llm-client-registry&#34;&gt;Client Registry&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We currently support: &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/open-ai&#34;&gt;OpenAI&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/anthropic&#34;&gt;Anthropic&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/google-ai-gemini&#34;&gt;Gemini&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/google-vertex&#34;&gt;Vertex&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/aws-bedrock&#34;&gt;Bedrock&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/open-ai-from-azure&#34;&gt;Azure OpenAI&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/openai-generic&#34;&gt;Anything OpenAI Compatible&lt;/a&gt; (&lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-ollama&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-open-router&#34;&gt;OpenRouter&lt;/a&gt;, &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-v-llm&#34;&gt;VLLM&lt;/a&gt;, &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-lm-studio&#34;&gt;LMStudio&lt;/a&gt;, &lt;a href=&#34;https://docs.boundaryml.com/ref/llm-client-providers/openai-generic-together-ai&#34;&gt;TogetherAI&lt;/a&gt;, and more)&lt;/p&gt; &#xA;&lt;h3&gt;Hot-reloading for prompts&lt;/h3&gt; &#xA;&lt;p&gt;Using AI is all about iteration speed.&lt;/p&gt; &#xA;&lt;p&gt;If testing your pipeline takes 2 minutes, in 20 minutes, you can only test 10 ideas.&lt;/p&gt; &#xA;&lt;p&gt;If testing your pipeline took 5 seconds, in 20 minutes, you can test 240 ideas.&lt;/p&gt; &#xA;&lt;p&gt;Introducing testing, for prompts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/07-hotreload.gif&#34; alt=&#34;Hot Reload&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Structured outputs with any LLM&lt;/h3&gt; &#xA;&lt;p&gt;JSON is amazing for REST APIs, but way too strict and verbose for LLMs. LLMs need something flexible. We created the SAP (schema-aligned parsing) algorithm to support the flexible outputs LLMs can provide, like markdown within a json blob or chain-of-thought prior to answering.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/09-cot.gif&#34; alt=&#34;Chain of Thought&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;SAP works with any model on day-1, without depending on tool-use or function-calling APIs.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about SAP you can read this post: &lt;a href=&#34;https://www.boundaryml.com/blog/schema-aligned-parsing&#34;&gt;Schema Aligned Parsing&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See it in action with: &lt;a href=&#34;https://www.boundaryml.com/blog/2025-01-20-deepseek-r1&#34;&gt;Deepseek-R1&lt;/a&gt; and &lt;a href=&#34;https://www.boundaryml.com/blog/2025-12-06-o1-pro&#34;&gt;OpenAI O1&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Streaming (when it&#39;s a first class citizen)&lt;/h3&gt; &#xA;&lt;p&gt;Streaming is way harder than it should be. With our [python/typescript/ruby] generated code, streaming becomes natural and type-safe.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.boundaryml.com/blog/2025-01-24-ai-agents-need-a-new-syntax/10-streaming-client.gif#still&#34; alt=&#34;Streaming&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;No strings attached&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;100% open-source (Apache 2)&lt;/li&gt; &#xA; &lt;li&gt;100% private. AGI will not require an internet connection, neither will BAML &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;No network requests beyond model calls you explicitly set&lt;/li&gt; &#xA;   &lt;li&gt;Not stored or used for any training data&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;BAML files can be saved locally on your machine and checked into Github for easy diffs.&lt;/li&gt; &#xA; &lt;li&gt;Built in Rust. So fast, you can&#39;t even tell its there.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BAML&#39;s Design Philosophy&lt;/h2&gt; &#xA;&lt;p&gt;Everything is fair game when making new syntax. If you can code it, it can be yours. This is our design philosophy to help restrict ideas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;1:&lt;/strong&gt; Avoid invention when possible &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yes, prompts need versioning ‚Äî we have a great versioning tool: git&lt;/li&gt; &#xA;   &lt;li&gt;Yes, you need to save prompts ‚Äî we have a great storage tool: filesystems&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2:&lt;/strong&gt; Any file editor and any terminal should be enough to use it&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3:&lt;/strong&gt; Be fast&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;4:&lt;/strong&gt; A first year university student should be able to understand it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why a new programming language&lt;/h2&gt; &#xA;&lt;p&gt;We used to write websites like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def home():&#xA;    return &#34;&amp;lt;button onclick=\&#34;() =&amp;gt; alert(\\\&#34;hello!\\\&#34;)\&#34;&amp;gt;Click&amp;lt;/button&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And now we do this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsx&#34;&gt;function Home() {&#xA;  return &amp;lt;button onClick={() =&amp;gt; setCount(prev =&amp;gt; prev + 1)}&amp;gt;&#xA;          {count} clicks!&#xA;         &amp;lt;/button&amp;gt;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;New syntax can be incredible at expressing new ideas. Plus the idea of mainting hundreds of f-strings for prompts kind of disgusts us ü§Æ. Strings are bad for maintable codebases. We prefer structured strings.&lt;/p&gt; &#xA;&lt;p&gt;The goal of BAML is to give you the expressiveness of English, but the structure of code.&lt;/p&gt; &#xA;&lt;p&gt;Full &lt;a href=&#34;https://www.boundaryml.com/blog/ai-agents-need-new-syntax&#34;&gt;blog post&lt;/a&gt; by us.&lt;/p&gt; &#xA;&lt;h2&gt;Conclusion&lt;/h2&gt; &#xA;&lt;p&gt;As models get better, we&#39;ll continue expecting even more out of them. But what will never change is that we&#39;ll want a way to write maintainable code that uses those models. The current way we all just assemble strings is very reminiscent of the early days PHP/HTML soup in web development. We hope some of the ideas we shared today can make a tiny dent in helping us all shape the way we all code tomorrow.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Checkout our &lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/CONTRIBUTING.md&#34;&gt;guide on getting started&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Made with ‚ù§Ô∏è by Boundary&lt;/p&gt; &#xA;&lt;p&gt;HQ in Seattle, WA&lt;/p&gt; &#xA;&lt;p&gt;P.S. We&#39;re hiring for software engineers that love rust. &lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/founders@boundaryml.com&#34;&gt;Email us&lt;/a&gt; or reach out on &lt;a href=&#34;https://discord.gg/ENtBB6kkXH&#34;&gt;discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;div align=&#34;left&#34; style=&#34;align-items: left;&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/BoundaryML/baml/canary/#top&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Back%20to%20Top-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&#34; alt=&#34;Back to Top&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;img src=&#34;https://imgs.xkcd.com/comics/standards.png&#34; alt_text=&#34;hi&#34;&gt;</summary>
  </entry>
  <entry>
    <title>katanemo/archgw</title>
    <updated>2025-02-02T01:48:03Z</updated>
    <id>tag:github.com,2025-02-02:/katanemo/archgw</id>
    <link href="https://github.com/katanemo/archgw" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI-native (edge and LLM) proxy for agents. Engineered with fast ‚ö°Ô∏è LLMs for task routing, rich observability, and the seamless integration of prompts with your APIs for agentic tasks. Built by the contributors of Envoy proxy.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/docs/source/_static/img/arch-logo.png&#34; alt=&#34;Arch Logo&#34; width=&#34;75%&#34; heigh=&#34;auto&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.producthunt.com/posts/arch-3?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-arch-3&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=565761&amp;amp;theme=light&amp;amp;period=daily&#34; alt=&#34;Arch - Build fast, hyper-personalized agents with intelligent infra | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Arch is an &lt;strong&gt;intelligent (edge and LLM) proxy designed for agentic applications&lt;/strong&gt; - to help you protect, observe, and build agentic tasks by simply connecting (existing) APIs.&lt;/p&gt; &#xA;&lt;p&gt;Built by the contributors of &lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;Envoy Proxy&lt;/a&gt; with the belief that:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Prompts are nuanced and opaque user requests, which require the same capabilities as traditional HTTP requests including secure handling, intelligent routing, robust observability, and integration with backend (API) systems for personalization ‚Äì outside core business logic.*&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/katanemo/arch/actions/workflows/pre-commit.yml&#34;&gt;&lt;img src=&#34;https://github.com/katanemo/arch/actions/workflows/pre-commit.yml/badge.svg?sanitize=true&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/katanemo/arch/actions/workflows/rust_tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/katanemo/arch/actions/workflows/rust_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;rust tests (prompt and llm gateway)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/katanemo/arch/actions/workflows/e2e_tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/katanemo/arch/actions/workflows/e2e_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;e2e tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/katanemo/arch/actions/workflows/static.yml&#34;&gt;&lt;img src=&#34;https://github.com/katanemo/arch/actions/workflows/static.yml/badge.svg?sanitize=true&#34; alt=&#34;Build and Deploy Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Arch is engineered with purpose-built LLMs to handle critical but undifferentiated tasks related to the handling and processing of prompts. This includes detecting and rejecting &lt;a href=&#34;https://github.com/verazuo/jailbreak_llms&#34;&gt;jailbreak&lt;/a&gt; attempts, intelligent task routing for improved accuracy, mapping user request into &#34;backend&#34; functions, and managing the observability of prompts and LLM API calls in a centralized way.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Built on &lt;a href=&#34;https://envoyproxy.io&#34;&gt;Envoy&lt;/a&gt;: Arch runs alongside application servers as a separate containerized process, and builds on top of Envoy&#39;s proven HTTP management and scalability features to handle ingress and egress traffic related to prompts and LLMs.&lt;/li&gt; &#xA; &lt;li&gt;Task Routing &amp;amp; Fast Function Calling. Engineered with purpose-built &lt;a href=&#34;https://huggingface.co/collections/katanemo/arch-function-66f209a693ea8df14317ad68&#34;&gt;LLMs&lt;/a&gt; to handle fast, cost-effective, and accurate prompt-based tasks like function/API calling, and parameter extraction from prompts to build more task-accurate agentic applications.&lt;/li&gt; &#xA; &lt;li&gt;Prompt &lt;a href=&#34;https://huggingface.co/collections/katanemo/arch-guard-6702bdc08b889e4bce8f446d&#34;&gt;Guard&lt;/a&gt;: Arch centralizes guardrails to prevent jailbreak attempts and ensure safe user interactions without writing a single line of code.&lt;/li&gt; &#xA; &lt;li&gt;Routing &amp;amp; Traffic Management: Arch centralizes calls to LLMs used by your applications, offering smart retries, automatic cutover, and resilient upstream connections for continuous availability.&lt;/li&gt; &#xA; &lt;li&gt;Observability: Arch uses the W3C Trace Context standard to enable complete request tracing across applications, ensuring compatibility with observability tools, and provides metrics to monitor latency, token usage, and error rates, helping optimize AI application performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Level Sequence Diagram&lt;/strong&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/docs/source/_static/img/arch_network_diagram_high_level.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Jump to our &lt;a href=&#34;https://docs.archgw.com&#34;&gt;docs&lt;/a&gt;&lt;/strong&gt; to learn how you can use Arch to improve the speed, security and personalization of your GenAI apps.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Today, the function calling LLM (Arch-Function) designed for the agentic and RAG scenarios is hosted free of charge in the US-central region. To offer consistent latencies and throughput, and to manage our expenses, we will enable access to the hosted version via developers keys soon, and give you the option to run that LLM locally. For more details see this issue &lt;a href=&#34;https://github.com/katanemo/archgw/issues/258&#34;&gt;#258&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;To get in touch with us, please join our &lt;a href=&#34;https://discord.gg/pGZf2gcwEc&#34;&gt;discord server&lt;/a&gt;. We will be monitoring that actively and offering support there.&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/demos/weather_forecast/README.md&#34;&gt;Weather Forecast&lt;/a&gt; - Walk through of the core function calling capabilities of arch gateway using weather forecasting service&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/demos/insurance_agent/README.md&#34;&gt;Insurance Agent&lt;/a&gt; - Build a full insurance agent with Arch&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/demos/network_agent/README.md&#34;&gt;Network Agent&lt;/a&gt; - Build a networking co-pilot/agent agent with Arch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Follow this quickstart guide to use arch gateway to build a simple AI agent. Laster in the section we will see how you can Arch Gateway to manage access keys, provide unified access to upstream LLMs and to provide e2e observability.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Before you begin, ensure you have the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-started/get-docker/&#34;&gt;Docker System&lt;/a&gt; (v24)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker compose&lt;/a&gt; (v2.29)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; (v3.12)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Arch&#39;s CLI allows you to manage and interact with the Arch gateway efficiently. To install the CLI, simply run the following command:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] We recommend that developers create a new Python virtual environment to isolate dependencies before installing Arch. This ensures that archgw and its dependencies do not interfere with other packages on your system.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ python -m venv venv&#xA;$ source venv/bin/activate   # On Windows, use: venv\Scripts\activate&#xA;$ pip install archgw==0.2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build AI Agent with Arch Gateway&lt;/h3&gt; &#xA;&lt;p&gt;In following quickstart we will show you how easy it is to build AI agent with Arch gateway. We will build a currency exchange agent using following simple steps. For this demo we will use &lt;code&gt;https://api.frankfurter.dev/&lt;/code&gt; to fetch latest price for currencies and assume USD as base currency.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1. Create arch config file&lt;/h4&gt; &#xA;&lt;p&gt;Create &lt;code&gt;arch_config.yaml&lt;/code&gt; file with following content,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: v0.1&#xA;&#xA;listener:&#xA;  address: 0.0.0.0&#xA;  port: 10000&#xA;  message_format: huggingface&#xA;  connect_timeout: 0.005s&#xA;&#xA;llm_providers:&#xA;  - name: gpt-4o&#xA;    access_key: $OPENAI_API_KEY&#xA;    provider: openai&#xA;    model: gpt-4o&#xA;&#xA;system_prompt: |&#xA;  You are a helpful assistant.&#xA;&#xA;prompt_guards:&#xA;  input_guards:&#xA;    jailbreak:&#xA;      on_exception:&#xA;        message: Looks like you&#39;re curious about my abilities, but I can only provide assistance for currency exchange.&#xA;&#xA;prompt_targets:&#xA;  - name: currency_exchange&#xA;    description: Get currency exchange rate from USD to other currencies&#xA;    parameters:&#xA;      - name: currency_symbol&#xA;        description: the currency that needs conversion&#xA;        required: true&#xA;        type: str&#xA;        in_path: true&#xA;    endpoint:&#xA;      name: frankfurther_api&#xA;      path: /v1/latest?base=USD&amp;amp;symbols={currency_symbol}&#xA;    system_prompt: |&#xA;      You are a helpful assistant. Show me the currency symbol you want to convert from USD.&#xA;&#xA;  - name: get_supported_currencies&#xA;    description: Get list of supported currencies for conversion&#xA;    endpoint:&#xA;      name: frankfurther_api&#xA;      path: /v1/currencies&#xA;&#xA;endpoints:&#xA;  frankfurther_api:&#xA;    endpoint: api.frankfurter.dev:443&#xA;    protocol: https&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 2. Start arch gateway with currency conversion config&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;&#xA;$ archgw up arch_config.yaml&#xA;2024-12-05 16:56:27,979 - cli.main - INFO - Starting archgw cli version: 0.1.5&#xA;...&#xA;2024-12-05 16:56:28,485 - cli.utils - INFO - Schema validation successful!&#xA;2024-12-05 16:56:28,485 - cli.main - INFO - Starging arch model server and arch gateway&#xA;...&#xA;2024-12-05 16:56:51,647 - cli.core - INFO - Container is healthy!&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the gateway is up you can start interacting with at port 10000 using openai chat completion API.&lt;/p&gt; &#xA;&lt;p&gt;Some of the sample queries you can ask could be &lt;code&gt;what is currency rate for gbp?&lt;/code&gt; or &lt;code&gt;show me list of currencies for conversion&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Step 3. Interacting with gateway using curl command&lt;/h4&gt; &#xA;&lt;p&gt;Here is a sample curl command you can use to interact,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl --header &#39;Content-Type: application/json&#39; \&#xA;  --data &#39;{&#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;what is exchange rate for gbp&#34;}]}&#39; \&#xA;  http://localhost:10000/v1/chat/completions | jq &#34;.choices[0].message.content&#34;&#xA;&#xA;&#34;As of the date provided in your context, December 5, 2024, the exchange rate for GBP (British Pound) from USD (United States Dollar) is 0.78558. This means that 1 USD is equivalent to 0.78558 GBP.&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to get list of supported currencies,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl --header &#39;Content-Type: application/json&#39; \&#xA;  --data &#39;{&#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;show me list of currencies that are supported for conversion&#34;}]}&#39; \&#xA;  http://localhost:10000/v1/chat/completions | jq &#34;.choices[0].message.content&#34;&#xA;&#xA;&#34;Here is a list of the currencies that are supported for conversion from USD, along with their symbols:\n\n1. AUD - Australian Dollar\n2. BGN - Bulgarian Lev\n3. BRL - Brazilian Real\n4. CAD - Canadian Dollar\n5. CHF - Swiss Franc\n6. CNY - Chinese Renminbi Yuan\n7. CZK - Czech Koruna\n8. DKK - Danish Krone\n9. EUR - Euro\n10. GBP - British Pound\n11. HKD - Hong Kong Dollar\n12. HUF - Hungarian Forint\n13. IDR - Indonesian Rupiah\n14. ILS - Israeli New Sheqel\n15. INR - Indian Rupee\n16. ISK - Icelandic Kr√≥na\n17. JPY - Japanese Yen\n18. KRW - South Korean Won\n19. MXN - Mexican Peso\n20. MYR - Malaysian Ringgit\n21. NOK - Norwegian Krone\n22. NZD - New Zealand Dollar\n23. PHP - Philippine Peso\n24. PLN - Polish Z≈Çoty\n25. RON - Romanian Leu\n26. SEK - Swedish Krona\n27. SGD - Singapore Dollar\n28. THB - Thai Baht\n29. TRY - Turkish Lira\n30. USD - United States Dollar\n31. ZAR - South African Rand\n\nIf you want to convert USD to any of these currencies, you can select the one you are interested in.&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use Arch Gateway as LLM Router&lt;/h3&gt; &#xA;&lt;h4&gt;Step 1. Create arch config file&lt;/h4&gt; &#xA;&lt;p&gt;Arch operates based on a configuration file where you can define LLM providers, prompt targets, guardrails, etc. Below is an example configuration that defines openai and mistral LLM providers.&lt;/p&gt; &#xA;&lt;p&gt;Create &lt;code&gt;arch_config.yaml&lt;/code&gt; file with following content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: v0.1&#xA;&#xA;listener:&#xA;  address: 0.0.0.0&#xA;  port: 10000&#xA;  message_format: huggingface&#xA;  connect_timeout: 0.005s&#xA;&#xA;llm_providers:&#xA;  - name: gpt-4o&#xA;    access_key: $OPENAI_API_KEY&#xA;    provider: openai&#xA;    model: gpt-4o&#xA;    default: true&#xA;&#xA;  - name: ministral-3b&#xA;    access_key: $MISTRAL_API_KEY&#xA;    provider: openai&#xA;    model: ministral-3b-latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 2. Start arch gateway&lt;/h4&gt; &#xA;&lt;p&gt;Once the config file is created ensure that you have env vars setup for &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt; and &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (or these are defined in &lt;code&gt;.env&lt;/code&gt; file).&lt;/p&gt; &#xA;&lt;p&gt;Start arch gateway,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ archgw up arch_config.yaml&#xA;2024-12-05 11:24:51,288 - cli.main - INFO - Starting archgw cli version: 0.1.5&#xA;2024-12-05 11:24:51,825 - cli.utils - INFO - Schema validation successful!&#xA;2024-12-05 11:24:51,825 - cli.main - INFO - Starting arch model server and arch gateway&#xA;...&#xA;2024-12-05 11:25:16,131 - cli.core - INFO - Container is healthy!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3: Interact with LLM&lt;/h3&gt; &#xA;&lt;h4&gt;Step 3.1: Using OpenAI python client&lt;/h4&gt; &#xA;&lt;p&gt;Make outbound calls via Arch gateway&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;# Use the OpenAI client as usual&#xA;client = OpenAI(&#xA;  # No need to set a specific openai.api_key since it&#39;s configured in Arch&#39;s gateway&#xA;  api_key = &#39;--&#39;,&#xA;  # Set the OpenAI API base URL to the Arch gateway endpoint&#xA;  base_url = &#34;http://127.0.0.1:12000/v1&#34;&#xA;)&#xA;&#xA;response = client.chat.completions.create(&#xA;    # we select model from arch_config file&#xA;    model=&#34;--&#34;,&#xA;    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the capital of France?&#34;}],&#xA;)&#xA;&#xA;print(&#34;OpenAI Response:&#34;, response.choices[0].message.content)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 3.2: Using curl command&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ curl --header &#39;Content-Type: application/json&#39; \&#xA;  --data &#39;{&#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;What is the capital of France?&#34;}]}&#39; \&#xA;  http://localhost:12000/v1/chat/completions&#xA;&#xA;{&#xA;  ...&#xA;  &#34;model&#34;: &#34;gpt-4o-2024-08-06&#34;,&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      ...&#xA;      &#34;message&#34;: {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: &#34;The capital of France is Paris.&#34;,&#xA;      },&#xA;    }&#xA;  ],&#xA;...&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can override model selection using &lt;code&gt;x-arch-llm-provider-hint&lt;/code&gt; header. For example if you want to use mistral using following curl command,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ curl --header &#39;Content-Type: application/json&#39; \&#xA;  --header &#39;x-arch-llm-provider-hint: ministral-3b&#39; \&#xA;  --data &#39;{&#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;What is the capital of France?&#34;}]}&#39; \&#xA;  http://localhost:12000/v1/chat/completions&#xA;{&#xA;  ...&#xA;  &#34;model&#34;: &#34;ministral-3b-latest&#34;,&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      &#34;message&#34;: {&#xA;        &#34;role&#34;: &#34;assistant&#34;,&#xA;        &#34;content&#34;: &#34;The capital of France is Paris. It is the most populous city in France and is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris is also a major global center for art, fashion, gastronomy, and culture.&#34;,&#xA;      },&#xA;      ...&#xA;    }&#xA;  ],&#xA;  ...&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://docs.archgw.com/guides/observability/observability.html&#34;&gt;Observability&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Arch is designed to support best-in class observability by supporting open standards. Please read our &lt;a href=&#34;https://docs.archgw.com/guides/observability/observability.html&#34;&gt;docs&lt;/a&gt; on observability for more details on tracing, metrics, and logs. The screenshot below is from our integration with Signoz (among others)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/docs/source/_static/img/tracing.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We would love feedback on our &lt;a href=&#34;https://github.com/orgs/katanemo/projects/1&#34;&gt;Roadmap&lt;/a&gt; and we welcome contributions to &lt;strong&gt;Arch&lt;/strong&gt;! Whether you&#39;re fixing bugs, adding new features, improving documentation, or creating tutorials, your help is much appreciated. Please visit our &lt;a href=&#34;https://raw.githubusercontent.com/katanemo/archgw/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; for more details&lt;/p&gt;</summary>
  </entry>
</feed>