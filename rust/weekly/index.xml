<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-21T02:01:54Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Cormanz/smartgpt</title>
    <updated>2023-05-21T02:01:54Z</updated>
    <id>tag:github.com,2023-05-21:/Cormanz/smartgpt</id>
    <link href="https://github.com/Cormanz/smartgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A program that provides LLMs with the ability to complete complex tasks using plugins.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;SmartGPT&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/Cormanz/smartgpt/main/LICENSE.md&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/Cormanz/smartgpt?style=flat-square&#34;&gt; &lt;img alt=&#34;Stars&#34; src=&#34;https://img.shields.io/github/stars/Cormanz/smartgpt?style=flat-square&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/use-experimental-informational?style=flat-square&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;SmartGPT is an experimental program meant to provide LLMs (particularly GPT-3.5 and GPT-4) with the ability to complete complex tasks without user input by breaking them down into smaller problems, and collecting information using the internet and other external sources.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested in keeping up with the progress of SmartGPT, want to contribute to development, or have issues to discuss, &lt;a href=&#34;https://discord.gg/5uezFE2XES&#34;&gt;join the SmartGPT Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Cormanz/smartgpt/assets/32941017/53bdcf83-4b2e-4798-b3f2-1a233b43c0e1&#34;&gt;https://github.com/Cormanz/smartgpt/assets/32941017/53bdcf83-4b2e-4798-b3f2-1a233b43c0e1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;p&gt;There are many existing solutions to allowing LLMs to perform more complex tasks, such as &lt;a href=&#34;https://github.com/Torantulino/Auto-GPT&#34;&gt;Auto-GPT&lt;/a&gt; and &lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;BabyAGI&lt;/a&gt;. So, why SmartGPT?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: With first class plugin support and the ability to compose Autos for whatever your project requires, SmartGPT is incredibly modular.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: SmartGPT has one &lt;code&gt;config.yml&lt;/code&gt; file that is automatically generated where you can configure everything and anything.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planning and Reasoning&lt;/strong&gt;: SmartGPT has an advanced hierarchical system of managers and employees to recursively break down your tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: SmartGPT is incredibly easy to configure simply by using a simple &lt;code&gt;config.yml&lt;/code&gt; file both for users, and for developers.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are two main shortcomings, however.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ecosystem&lt;/strong&gt;: Due to its popularity, &lt;a href=&#34;https://github.com/Torantulino/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt; is a very polished and refined tool. It has many more commands and integrations with memory systems. To go with this, the codebase has been through large scrutiny, so it is generally less buggy and more tested than SmartGPT.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;: Due to the extreme youth of this project, there is only one simple but limited memory system. However, this will change with time.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supporting Development&lt;/h2&gt; &#xA;&lt;p&gt;Currently, testing with SmartGPT is primarily being done with GPT3.5, and occasionally with GPT4, due to the costs of more-expensive models. As this project matures, we&#39;re aiming to experiment both with &lt;strong&gt;multiple agents at once&lt;/strong&gt; and using &lt;strong&gt;GPT4&lt;/strong&gt; much more to unleash maximum capabilities out of LLMs. This is expensive though, and as the core maintainer of SmartGPT, I&#39;m still a high school student, and funding a project like this is difficult for me. If you&#39;re interest in helping push the boundaries of LLMs, &lt;a href=&#34;https://www.patreon.com/SmartGPT&#34;&gt;consider joining our Patreon.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;SmartGPT is an &lt;strong&gt;incredibly experimental&lt;/strong&gt; application. The goal is to unlock maximum potential out of LLMs, and stability is sacrificed for this. Backwards compatibility is a fever dream here. However, SmartGPT is also housing some of the most innovative ideas and experiments in the AutoGPT space right now, and although most are unsuccessful, a few hit the dart-board and stick.&lt;/p&gt; &#xA;&lt;h2&gt;Cargo Quickstart&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://doc.rust-lang.org/cargo/getting-started/installation.html&#34;&gt;&lt;code&gt;cargo&lt;/code&gt;&lt;/a&gt;, preferably the latest stable version.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository with &lt;code&gt;git clone https://github.com/Cormanz/smartgpt.git &amp;amp;&amp;amp; cd smartgpt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run it in release mode with &lt;code&gt;cargo run --release&lt;/code&gt;. This will create a &lt;code&gt;config.yml&lt;/code&gt; for you.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Adjust the config to your liking, and execute it once again.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you want more information, &lt;a href=&#34;https://corman.gitbook.io/smartgpt/installation&#34;&gt;read the documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;How SmartGPT Works&lt;/h1&gt; &#xA;&lt;h2&gt;Autos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Auto&lt;/strong&gt;s are the building blocks of SmartGPT. There are two types of Autos.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Runner&lt;/strong&gt;: A runner is given a single task, and is asked to complete it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Assistants&lt;/strong&gt;: An Assistant Auto can be conversed with, and will give you responses back, in context of the conversation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Assistants are highly experimental, so we recommend Runners.&lt;/p&gt; &#xA;&lt;p&gt;Autos have &lt;strong&gt;agents&lt;/strong&gt;. An agent is an LLM that handles planning, reasoning, and task execution. The Auto starts with your &lt;strong&gt;top manager&lt;/strong&gt;, and asks it to run the task. Then, that manager will delegate tasks all the way down to your &lt;strong&gt;employee&lt;/strong&gt;, which will run the tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Managers&lt;/h2&gt; &#xA;&lt;p&gt;Managers are a type of agent that plan and reason. They&#39;ll be given a task, and plan out that task into subtasks. Then, one subtask at a time, they&#39;ll delegate it down to their employee (a lower-level manager, or the task-running employee.)&lt;/p&gt; &#xA;&lt;h2&gt;Employee&lt;/h2&gt; &#xA;&lt;p&gt;Employees are the lowest agent in the hierarchy. They&#39;re given a task, and they execute it one command at a time. They&#39;re much like the core application of AutoGPT, but they have a much more compact thought-loop.&lt;/p&gt; &#xA;&lt;h2&gt;Memory&lt;/h2&gt; &#xA;&lt;p&gt;Agents all have &lt;strong&gt;memory&lt;/strong&gt;. After completing a task, the agent will save a list of all observations into long-term memory. Once it starts another task, it will pull all long-term memories related to the task (using a VectorDB for this.)&lt;/p&gt; &#xA;&lt;h2&gt;Plugin System&lt;/h2&gt; &#xA;&lt;p&gt;Autos can use a set of &lt;strong&gt;tools&lt;/strong&gt; such as &lt;code&gt;google_search&lt;/code&gt;, &lt;code&gt;browse_url&lt;/code&gt;, etc. You define these using plugins. Plugins define their own set of commands, and can have their own data.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;smartgpt&lt;/code&gt; is available under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT license&lt;/a&gt;. See &lt;a href=&#34;https://github.com/Cormanz/smartgpt/raw/main/LICENSE.md&#34;&gt;LICENSE&lt;/a&gt; for the full license text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rustls/rustls</title>
    <updated>2023-05-21T02:01:54Z</updated>
    <id>tag:github.com,2023-05-21:/rustls/rustls</id>
    <link href="https://github.com/rustls/rustls" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern TLS library in Rust&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;460&#34; height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/rustls/rustls/main/admin/rustls-logo-web.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Rustls is a modern TLS library written in Rust. It uses &lt;a href=&#34;https://github.com/briansmith/ring&#34;&gt;&lt;em&gt;ring&lt;/em&gt;&lt;/a&gt; for cryptography and &lt;a href=&#34;https://github.com/rustls/webpki&#34;&gt;webpki&lt;/a&gt; for certificate verification. &lt;/p&gt; &#xA;&lt;h1&gt;Status&lt;/h1&gt; &#xA;&lt;p&gt;Rustls is mature and widely used. While most of the API surface is stable, we expect the next few releases will make further changes as needed to accomodate new features or performance improvements.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to help out, please see &lt;a href=&#34;https://raw.githubusercontent.com/rustls/rustls/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rustls/rustls/actions/workflows/build.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/rustls/rustls/actions/workflows/build.yml/badge.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/rustls/rustls/&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/rustls/rustls/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage Status (codecov.io)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/rustls/&#34;&gt;&lt;img src=&#34;https://docs.rs/rustls/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/MCSB76RU96&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/976380008299917365?logo=discord&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release history&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release 0.21.1 (2023-05-01) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove &lt;code&gt;warn&lt;/code&gt;-level logging from code paths that also return a &lt;code&gt;rustls::Error&lt;/code&gt; with the same information.&lt;/li&gt; &#xA;   &lt;li&gt;Bug fix: ensure &lt;code&gt;ConnectionCommon::complete_io&lt;/code&gt; flushes pending writes.&lt;/li&gt; &#xA;   &lt;li&gt;Bug fix: correct encoding of acceptable issuer subjects when rustls operates as a server requesting client authentication. This was a regression introduced in 0.21.0.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Release 0.21.0 (2023-03-29) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support for connecting to peers named with IP addresses. This means rustls now depends on a fork of webpki - &lt;code&gt;rustls-webpki&lt;/code&gt; - with a suitably extended API.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: &lt;code&gt;StoresClientSessions&lt;/code&gt; trait renamed to &lt;code&gt;ClientSessionStore&lt;/code&gt; and reworked to allow storage of multiple TLS1.3 tickets and avoid reuse of them. This is a privacy improvement, see RFC8446 appendix C.4.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the &lt;code&gt;DistinguishedNames&lt;/code&gt; type alias no longer exists; the public API now exports a &lt;code&gt;DistinguishedName&lt;/code&gt; type, and the &lt;code&gt;ClientCertVerifier::client_auth_root_subjects()&lt;/code&gt; method now returns a &lt;code&gt;&amp;amp;[DistinguishedName]&lt;/code&gt; instead (with the lifetime constrained to the verifier&#39;s).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the &lt;code&gt;ClientCertVerifier&lt;/code&gt; methods &lt;code&gt;client_auth_mandatory()&lt;/code&gt; and &lt;code&gt;client_auth_root_subjects()&lt;/code&gt; no longer return an &lt;code&gt;Option&lt;/code&gt;. You can now use an &lt;code&gt;Acceptor&lt;/code&gt; to decide whether to accept the connection based on information from the &lt;code&gt;ClientHello&lt;/code&gt; (like server name).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: rework &lt;code&gt;rustls::Error&lt;/code&gt; to avoid String usage in &lt;code&gt;PeerMisbehavedError&lt;/code&gt;, &lt;code&gt;PeerIncompatibleError&lt;/code&gt; and certificate errors. Especially note that custom certificate verifiers should move to use the new certificate errors. &lt;code&gt;Error&lt;/code&gt; is now &lt;code&gt;non_exhaustive&lt;/code&gt;, and so are the inner enums used in its variants.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: replace &lt;code&gt;webpki::Error&lt;/code&gt; appearing in the public API in &lt;code&gt;RootCertStore::add&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The number of tickets sent by a TLS1.3 server is now configurable via &lt;code&gt;ServerConfig::send_tls13_tickets&lt;/code&gt;. Previously one ticket was sent, now the default is four.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: remove deprecated methods from &lt;code&gt;Acceptor&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: &lt;code&gt;AllowAnyAuthenticatedClient&lt;/code&gt; and &lt;code&gt;AllowAnyAnonymousOrAuthenticatedClient&lt;/code&gt; &lt;code&gt;new&lt;/code&gt; functions now return &lt;code&gt;Self&lt;/code&gt;. A &lt;code&gt;boxed&lt;/code&gt; function was added to both types to easily acquire an &lt;code&gt;Arc&amp;lt;dyn ClientCertVerifier&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: &lt;code&gt;NoClientAuth::new&lt;/code&gt; was renamed to &lt;code&gt;boxed&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the QUIC API has changed to provide QUIC-specific &lt;code&gt;ClientConnection&lt;/code&gt; and &lt;code&gt;ServerConnection&lt;/code&gt; types, instead of using an extension trait.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the QUIC &lt;code&gt;Secrets&lt;/code&gt; constructor was changed to take a &lt;code&gt;Side&lt;/code&gt; instead of &lt;code&gt;bool&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the &lt;code&gt;export_keying_material&lt;/code&gt; function on a &lt;code&gt;Connection&lt;/code&gt; was changed from returning &lt;code&gt;Result&amp;lt;(), Error&amp;gt;&lt;/code&gt; to &lt;code&gt;Result&amp;lt;T, Error&amp;gt;&lt;/code&gt; where &lt;code&gt;T: AsMut&amp;lt;[u8]&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the &lt;code&gt;sni_hostname&lt;/code&gt; function on a &lt;code&gt;Connection&lt;/code&gt; was renamed to &lt;code&gt;server_name&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: remove alternative type names deprecated in 0.20.0 (&lt;code&gt;RSASigningKey&lt;/code&gt; vs. &lt;code&gt;RsaSigningKey&lt;/code&gt; etc.)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Breaking change&lt;/em&gt;: the client config &lt;code&gt;session_storage&lt;/code&gt; and &lt;code&gt;enable_tickets&lt;/code&gt; fields have been replaced by a more misuse resistant &lt;code&gt;Resumption&lt;/code&gt; type that combines the two options.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/rustls/rustls/main/RELEASE_NOTES.md&#34;&gt;RELEASE_NOTES.md&lt;/a&gt; for further change history.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;Lives here: &lt;a href=&#34;https://docs.rs/rustls/&#34;&gt;https://docs.rs/rustls/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Approach&lt;/h1&gt; &#xA;&lt;p&gt;Rustls is a TLS library that aims to provide a good level of cryptographic security, requires no configuration to achieve that security, and provides no unsafe features or obsolete cryptography.&lt;/p&gt; &#xA;&lt;h2&gt;Current features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TLS1.2 and TLS1.3.&lt;/li&gt; &#xA; &lt;li&gt;ECDSA, Ed25519 or RSA server authentication by clients.&lt;/li&gt; &#xA; &lt;li&gt;ECDSA, Ed25519 or RSA server authentication by servers.&lt;/li&gt; &#xA; &lt;li&gt;Forward secrecy using ECDHE; with curve25519, nistp256 or nistp384 curves.&lt;/li&gt; &#xA; &lt;li&gt;AES128-GCM and AES256-GCM bulk encryption, with safe nonces.&lt;/li&gt; &#xA; &lt;li&gt;ChaCha20-Poly1305 bulk encryption (&lt;a href=&#34;https://tools.ietf.org/html/rfc7905&#34;&gt;RFC7905&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;ALPN support.&lt;/li&gt; &#xA; &lt;li&gt;SNI support.&lt;/li&gt; &#xA; &lt;li&gt;Tunable fragment size to make TLS messages match size of underlying transport.&lt;/li&gt; &#xA; &lt;li&gt;Optional use of vectored IO to minimise system calls.&lt;/li&gt; &#xA; &lt;li&gt;TLS1.2 session resumption.&lt;/li&gt; &#xA; &lt;li&gt;TLS1.2 resumption via tickets (&lt;a href=&#34;https://tools.ietf.org/html/rfc5077&#34;&gt;RFC5077&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;TLS1.3 resumption via tickets or session storage.&lt;/li&gt; &#xA; &lt;li&gt;TLS1.3 0-RTT data for clients.&lt;/li&gt; &#xA; &lt;li&gt;TLS1.3 0-RTT data for servers.&lt;/li&gt; &#xA; &lt;li&gt;Client authentication by clients.&lt;/li&gt; &#xA; &lt;li&gt;Client authentication by servers.&lt;/li&gt; &#xA; &lt;li&gt;Extended master secret support (&lt;a href=&#34;https://tools.ietf.org/html/rfc7627&#34;&gt;RFC7627&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Exporters (&lt;a href=&#34;https://tools.ietf.org/html/rfc5705&#34;&gt;RFC5705&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;OCSP stapling by servers.&lt;/li&gt; &#xA; &lt;li&gt;SCT stapling by servers.&lt;/li&gt; &#xA; &lt;li&gt;SCT verification by clients.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Possible future features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PSK support.&lt;/li&gt; &#xA; &lt;li&gt;OCSP verification by clients.&lt;/li&gt; &#xA; &lt;li&gt;Certificate pinning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Non-features&lt;/h2&gt; &#xA;&lt;p&gt;For reasons &lt;a href=&#34;https://docs.rs/rustls/latest/rustls/manual/_02_tls_vulnerabilities/index.html&#34;&gt;explained in the manual&lt;/a&gt;, rustls does not and will not support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SSL1, SSL2, SSL3, TLS1 or TLS1.1.&lt;/li&gt; &#xA; &lt;li&gt;RC4.&lt;/li&gt; &#xA; &lt;li&gt;DES or triple DES.&lt;/li&gt; &#xA; &lt;li&gt;EXPORT ciphersuites.&lt;/li&gt; &#xA; &lt;li&gt;MAC-then-encrypt ciphersuites.&lt;/li&gt; &#xA; &lt;li&gt;Ciphersuites without forward secrecy.&lt;/li&gt; &#xA; &lt;li&gt;Renegotiation.&lt;/li&gt; &#xA; &lt;li&gt;Kerberos.&lt;/li&gt; &#xA; &lt;li&gt;Compression.&lt;/li&gt; &#xA; &lt;li&gt;Discrete-log Diffie-Hellman.&lt;/li&gt; &#xA; &lt;li&gt;Automatic protocol version downgrade.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are plenty of other libraries that provide these features should you need them.&lt;/p&gt; &#xA;&lt;h3&gt;Platform support&lt;/h3&gt; &#xA;&lt;p&gt;While Rustls itself is platform independent it uses &lt;a href=&#34;https://crates.io/crates/ring&#34;&gt;&lt;code&gt;ring&lt;/code&gt;&lt;/a&gt; for implementing the cryptography in TLS. As a result, rustls only runs on platforms supported by &lt;code&gt;ring&lt;/code&gt;. At the time of writing this means x86, x86-64, armv7, and aarch64. For more information see &lt;a href=&#34;https://github.com/briansmith/ring/raw/9cc0d45f4d8521f467bb3a621e74b1535e118188/.github/workflows/ci.yml#L151-L167&#34;&gt;the supported &lt;code&gt;ring&lt;/code&gt; CI targets&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Rustls requires Rust 1.57 or later.&lt;/p&gt; &#xA;&lt;h1&gt;Example code&lt;/h1&gt; &#xA;&lt;p&gt;There are two example programs which use &lt;a href=&#34;https://github.com/carllerche/mio&#34;&gt;mio&lt;/a&gt; to do asynchronous IO.&lt;/p&gt; &#xA;&lt;h2&gt;Client example program&lt;/h2&gt; &#xA;&lt;p&gt;The client example program is named &lt;code&gt;tlsclient-mio&lt;/code&gt;. The interface looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tlsclient-mio&#34;&gt;Connects to the TLS server at hostname:PORT.  The default PORT&#xA;is 443.  By default, this reads a request from stdin (to EOF)&#xA;before making the connection.  --http replaces this with a&#xA;basic HTTP GET request for /.&#xA;&#xA;If --cafile is not supplied, a built-in set of CA certificates&#xA;are used from the webpki-roots crate.&#xA;&#xA;Usage:&#xA;  tlsclient-mio [options] [--suite SUITE ...] [--proto PROTO ...] [--protover PROTOVER ...] &amp;lt;hostname&amp;gt;&#xA;  tlsclient-mio (--version | -v)&#xA;  tlsclient-mio (--help | -h)&#xA;&#xA;Options:&#xA;    -p, --port PORT     Connect to PORT [default: 443].&#xA;    --http              Send a basic HTTP GET request for /.&#xA;    --cafile CAFILE     Read root certificates from CAFILE.&#xA;    --auth-key KEY      Read client authentication key from KEY.&#xA;    --auth-certs CERTS  Read client authentication certificates from CERTS.&#xA;                        CERTS must match up with KEY.&#xA;    --protover VERSION  Disable default TLS version list, and use&#xA;                        VERSION instead.  May be used multiple times.&#xA;    --suite SUITE       Disable default cipher suite list, and use&#xA;                        SUITE instead.  May be used multiple times.&#xA;    --proto PROTOCOL    Send ALPN extension containing PROTOCOL.&#xA;                        May be used multiple times to offer several protocols.&#xA;    --no-tickets        Disable session ticket support.&#xA;    --no-sni            Disable server name indication support.&#xA;    --insecure          Disable certificate verification.&#xA;    --verbose           Emit log output.&#xA;    --max-frag-size M   Limit outgoing messages to M bytes.&#xA;    --version, -v       Show tool version.&#xA;    --help, -h          Show this screen.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some sample runs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cargo run --bin tlsclient-mio -- --http mozilla-modern.badssl.com&#xA;HTTP/1.1 200 OK&#xA;Server: nginx/1.6.2 (Ubuntu)&#xA;Date: Wed, 01 Jun 2016 18:44:00 GMT&#xA;Content-Type: text/html&#xA;Content-Length: 644&#xA;(...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cargo run --bin tlsclient-mio -- --http expired.badssl.com&#xA;TLS error: InvalidCertificate(Expired)&#xA;Connection closed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Server example program&lt;/h2&gt; &#xA;&lt;p&gt;The server example program is named &lt;code&gt;tlsserver-mio&lt;/code&gt;. The interface looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tlsserver-mio&#34;&gt;Runs a TLS server on :PORT.  The default PORT is 443.&#xA;&#xA;`echo&#39; mode means the server echoes received data on each connection.&#xA;&#xA;`http&#39; mode means the server blindly sends a HTTP response on each&#xA;connection.&#xA;&#xA;`forward&#39; means the server forwards plaintext to a connection made to&#xA;localhost:fport.&#xA;&#xA;`--certs&#39; names the full certificate chain, `--key&#39; provides the&#xA;RSA private key.&#xA;&#xA;Usage:&#xA;  tlsserver-mio --certs CERTFILE --key KEYFILE [--suite SUITE ...] [--proto PROTO ...] [--protover PROTOVER ...] [options] echo&#xA;  tlsserver-mio --certs CERTFILE --key KEYFILE [--suite SUITE ...] [--proto PROTO ...] [--protover PROTOVER ...] [options] http&#xA;  tlsserver-mio --certs CERTFILE --key KEYFILE [--suite SUITE ...] [--proto PROTO ...] [--protover PROTOVER ...] [options] forward &amp;lt;fport&amp;gt;&#xA;  tlsserver-mio (--version | -v)&#xA;  tlsserver-mio (--help | -h)&#xA;&#xA;Options:&#xA;    -p, --port PORT     Listen on PORT [default: 443].&#xA;    --certs CERTFILE    Read server certificates from CERTFILE.&#xA;                        This should contain PEM-format certificates&#xA;                        in the right order (the first certificate should&#xA;                        certify KEYFILE, the last should be a root CA).&#xA;    --key KEYFILE       Read private key from KEYFILE.  This should be a RSA&#xA;                        private key or PKCS8-encoded private key, in PEM format.&#xA;    --ocsp OCSPFILE     Read DER-encoded OCSP response from OCSPFILE and staple&#xA;                        to certificate.  Optional.&#xA;    --auth CERTFILE     Enable client authentication, and accept certificates&#xA;                        signed by those roots provided in CERTFILE.&#xA;    --require-auth      Send a fatal alert if the client does not complete client&#xA;                        authentication.&#xA;    --resumption        Support session resumption.&#xA;    --tickets           Support tickets.&#xA;    --protover VERSION  Disable default TLS version list, and use&#xA;                        VERSION instead.  May be used multiple times.&#xA;    --suite SUITE       Disable default cipher suite list, and use&#xA;                        SUITE instead.  May be used multiple times.&#xA;    --proto PROTOCOL    Negotiate PROTOCOL using ALPN.&#xA;                        May be used multiple times.&#xA;    --verbose           Emit log output.&#xA;    --version, -v       Show tool version.&#xA;    --help, -h          Show this screen.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a sample run; we start a TLS echo server, then connect to it with &lt;code&gt;openssl&lt;/code&gt; and &lt;code&gt;tlsclient-mio&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cargo run --bin tlsserver-mio -- --certs test-ca/rsa/end.fullchain --key test-ca/rsa/end.rsa -p 8443 echo &amp;amp;&#xA;$ echo hello world | openssl s_client -ign_eof -quiet -connect localhost:8443&#xA;depth=2 CN = ponytown RSA CA&#xA;verify error:num=19:self signed certificate in certificate chain&#xA;hello world&#xA;^C&#xA;$ echo hello world | cargo run --bin tlsclient-mio -- --cafile test-ca/rsa/ca.cert -p 8443 localhost&#xA;hello world&#xA;^C&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Rustls is distributed under the following three licenses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apache License version 2.0.&lt;/li&gt; &#xA; &lt;li&gt;MIT license.&lt;/li&gt; &#xA; &lt;li&gt;ISC license.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These are included as LICENSE-APACHE, LICENSE-MIT and LICENSE-ISC respectively. You may use this software under the terms of any of these licenses, at your option.&lt;/p&gt; &#xA;&lt;h1&gt;Code of conduct&lt;/h1&gt; &#xA;&lt;p&gt;This project adopts the &lt;a href=&#34;https://www.rust-lang.org/policies/code-of-conduct&#34;&gt;Rust Code of Conduct&lt;/a&gt;. Please email &lt;a href=&#34;mailto:rustls-mod@googlegroups.com&#34;&gt;rustls-mod@googlegroups.com&lt;/a&gt; to report any instance of misconduct, or if you have any comments or questions on the Code of Conduct.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>postgresml/postgresml</title>
    <updated>2023-05-21T02:01:54Z</updated>
    <id>tag:github.com,2023-05-21:/postgresml/postgresml</id>
    <link href="https://github.com/postgresml/postgresml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PostgresML is an AI application database. Use open source models from Huggingface, or train your own, to create and index LLM embeddings, generate text, or make online predictions using only SQL.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://postgresml.org/&#34;&gt; &lt;img src=&#34;https://postgresml.org/static/images/owl_gradient.svg?sanitize=true&#34; width=&#34;175&#34; alt=&#34;PostgresML&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://postgresml.org/&#34;&gt; &#xA;  &lt;svg version=&#34;1.1&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34; width=&#34;200&#34; height=&#34;50&#34;&gt; &#xA;   &lt;text font-size=&#34;32&#34; x=&#34;20&#34; y=&#34;32&#34;&gt; &#xA;    &lt;tspan fill=&#34;white&#34; style=&#34;mix-blend-mode: difference;&#34;&gt;&#xA;     Postgres&#xA;    &lt;/tspan&gt;&#xA;    &lt;tspan fill=&#34;dodgerblue&#34;&gt;&#xA;     ML&#xA;    &lt;/tspan&gt; &#xA;   &lt;/text&gt; &#xA;  &lt;/svg&gt; &lt;/a&gt; &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Generative AI and Simple ML with &lt;a href=&#34;https://www.postgresql.org/&#34; target=&#34;_blank&#34;&gt;PostgreSQL&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;CI&#34; src=&#34;https://github.com/postgresml/postgresml/actions/workflows/ci.yml/badge.svg?sanitize=true&#34;&gt; &lt;a href=&#34;https://discord.gg/DmyJP3qJ7U&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1013868243036930099&#34; alt=&#34;Join our Discord!&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Table of contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#getting-started&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#nlp-tasks&#34;&gt;Natural Language Processing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#text-classification&#34;&gt;Text Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#zero-shot-classification&#34;&gt;Zero-Shot Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#token-classification&#34;&gt;Token Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#translation&#34;&gt;Translation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#summarization&#34;&gt;Summarization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#question-answering&#34;&gt;Question Answering&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#text-generation&#34;&gt;Text Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#text-to-text-generation&#34;&gt;Text-to-Text Generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#fill-mask&#34;&gt;Fill-Mask&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/#vector-database&#34;&gt;Vector Database&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - [Regression](#regression)&#xA;- [Classification](#classification) --&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;PostgresML is a machine learning extension to PostgreSQL that enables you to perform training and inference on text and tabular data using SQL queries. With PostgresML, you can seamlessly integrate machine learning models into your PostgreSQL database and harness the power of cutting-edge algorithms to process data efficiently.&lt;/p&gt; &#xA;&lt;h2&gt;Text Data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Perform natural language processing (NLP) tasks like sentiment analysis, question and answering, translation, summarization and text generation&lt;/li&gt; &#xA; &lt;li&gt;Access 1000s of state-of-the-art language models like GPT-2, GPT-J, GPT-Neo from &lt;span&gt;🤗&lt;/span&gt; HuggingFace model hub&lt;/li&gt; &#xA; &lt;li&gt;Fine tune large language models (LLMs) on your own text data for different tasks&lt;/li&gt; &#xA; &lt;li&gt;Use your existing PostgreSQL database as a vector database by generating embeddings from text stored in the database.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Translation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;SQL query&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    &#39;translation_en_to_fr&#39;,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Welcome to the future!&#39;,&#xA;        &#39;Where have you been all this time?&#39;&#xA;    ]&#xA;) AS french;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;                         french                                 &#xA;------------------------------------------------------------&#xA;&#xA;[&#xA;    {&#34;translation_text&#34;: &#34;Bienvenue à l&#39;avenir!&#34;},&#xA;    {&#34;translation_text&#34;: &#34;Où êtes-vous allé tout ce temps?&#34;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sentiment Analysis&lt;/strong&gt; &lt;em&gt;SQL query&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task   =&amp;gt; &#39;text-classification&#39;,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;I love how amazingly simple ML has become!&#39;, &#xA;        &#39;I hate doing mundane and thankless tasks. ☹️&#39;&#xA;    ]&#xA;) AS positivity;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;                    positivity&#xA;------------------------------------------------------&#xA;[&#xA;    {&#34;label&#34;: &#34;POSITIVE&#34;, &#34;score&#34;: 0.9995759129524232}, &#xA;    {&#34;label&#34;: &#34;NEGATIVE&#34;, &#34;score&#34;: 0.9903519749641418}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tabular data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://postgresml.org/docs/guides/training/algorithm_selection&#34;&gt;47+ classification and regression algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://postgresml.org/blog/postgresml-is-8x-faster-than-python-http-microservices&#34;&gt;8 - 40X faster inference than HTTP based model serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://postgresml.org/blog/scaling-postgresml-to-one-million-requests-per-second&#34;&gt;Millions of transactions per second&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/postgresml/pgcat&#34;&gt;Horizontal scalability&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training a classification model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Training&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM pgml.train(&#xA;    &#39;Handwritten Digit Image Classifier&#39;,&#xA;    algorithm =&amp;gt; &#39;xgboost&#39;,&#xA;    &#39;classification&#39;,&#xA;    &#39;pgml.digits&#39;,&#xA;    &#39;target&#39;&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Inference&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.predict(&#xA;    &#39;My Classification Project&#39;, &#xA;    ARRAY[0.1, 2.0, 5.0]&#xA;) AS prediction;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;PostgresML installation consists of three parts: PostgreSQL database, Postgres extension for machine learning and a dashboard app. The extension provides all the machine learning functionality and can be used independently using any SQL IDE. The dashboard app provides an easy to use interface for writing SQL notebooks, performing and tracking ML experiments and ML models.&lt;/p&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;Step 1: Clone this repository&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:postgresml/postgresml.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 2: Start dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running. You can find Docker installation instructions &lt;a href=&#34;https://docs.docker.com/desktop/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd postgresml&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step 3: Connect to Postgres using an SQL IDE or &lt;a href=&#34;https://www.postgresql.org/docs/current/app-psql.html&#34; target=&#34;_blank&#34;&gt;psql&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;postgres://postgres@localhost:5433/pgml_development&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Free trial&lt;/h2&gt; &#xA;&lt;p&gt;If you want to check out the functionality without the hassle of Docker, &lt;a href=&#34;https://postgresml.org/signup&#34;&gt;sign up for a free PostgresML account&lt;/a&gt;. We will provide 5GiB of storage for your data and demo notebooks to help you get started.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Option 1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;On local installation go to dashboard app at &lt;code&gt;http://localhost:8000/&lt;/code&gt; to use SQL notebooks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On the hosted console click on the &lt;strong&gt;Dashboard&lt;/strong&gt; button to connect to your instance with SQL notebooks. &lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/dashboard.png&#34; alt=&#34;dashboard&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Try one of the pre-built SQL notebooks &lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/notebooks.png&#34; alt=&#34;notebooks&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Option 2&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use any of these popular tools to connect to PostgresML and write SQL queries &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://superset.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Superset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://dbeaver.io/&#34; target=&#34;_blank&#34;&gt;DBeaver&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.jetbrains.com/datagrip/&#34; target=&#34;_blank&#34;&gt;Data Grip&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eggerapps.at/postico2/&#34; target=&#34;_blank&#34;&gt;Postico 2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://popsql.com/&#34; target=&#34;_blank&#34;&gt;Popsql&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.tableau.com/&#34; target=&#34;_blank&#34;&gt;Tableau&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://powerbi.microsoft.com/en-us/&#34; target=&#34;_blank&#34;&gt;PowerBI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34;&gt;Jupyter&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com/&#34; target=&#34;_blank&#34;&gt;VSCode&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Option 3&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Connect directly to the database with your favorite programming language &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;C++: &lt;a href=&#34;https://www.tutorialspoint.com/postgresql/postgresql_c_cpp.htm&#34; target=&#34;_blank&#34;&gt;libpqxx&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;C#: &lt;a href=&#34;https://github.com/npgsql/npgsql&#34; target=&#34;_blank&#34;&gt;Npgsql&lt;/a&gt;,&lt;a href=&#34;https://github.com/DapperLib/Dapper&#34; target=&#34;_blank&#34;&gt;Dapper&lt;/a&gt;, or &lt;a href=&#34;https://github.com/dotnet/efcore&#34; target=&#34;_blank&#34;&gt;Entity Framework Core&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Elixir: &lt;a href=&#34;https://github.com/elixir-ecto/ecto&#34; target=&#34;_blank&#34;&gt;ecto&lt;/a&gt; or &lt;a href=&#34;https://github.com/elixir-ecto/postgrex&#34; target=&#34;_blank&#34;&gt;Postgrex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Go: &lt;a href=&#34;https://github.com/jackc/pgx&#34; target=&#34;_blank&#34;&gt;pgx&lt;/a&gt;, &lt;a href=&#34;https://github.com/go-pg/pg&#34; target=&#34;_blank&#34;&gt;pg&lt;/a&gt; or &lt;a href=&#34;https://github.com/uptrace/bun&#34; target=&#34;_blank&#34;&gt;Bun&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Haskell: &lt;a href=&#34;https://hackage.haskell.org/package/postgresql-simple&#34; target=&#34;_blank&#34;&gt;postgresql-simple&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Java &amp;amp; Scala: &lt;a href=&#34;https://jdbc.postgresql.org/&#34; target=&#34;_blank&#34;&gt;JDBC&lt;/a&gt; or &lt;a href=&#34;https://github.com/slick/slick&#34; target=&#34;_blank&#34;&gt;Slick&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Julia: &lt;a href=&#34;https://github.com/iamed2/LibPQ.jl&#34; target=&#34;_blank&#34;&gt;LibPQ.jl&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Lua: &lt;a href=&#34;https://github.com/leafo/pgmoon&#34; target=&#34;_blank&#34;&gt;pgmoon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Node: &lt;a href=&#34;https://github.com/brianc/node-postgres&#34; target=&#34;_blank&#34;&gt;node-postgres&lt;/a&gt;, &lt;a href=&#34;https://github.com/vitaly-t/pg-promise&#34; target=&#34;_blank&#34;&gt;pg-promise&lt;/a&gt;, or &lt;a href=&#34;https://sequelize.org/&#34; target=&#34;_blank&#34;&gt;Sequelize&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Perl: &lt;a href=&#34;https://github.com/bucardo/dbdpg&#34; target=&#34;_blank&#34;&gt;DBD::Pg&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;PHP: &lt;a href=&#34;https://laravel.com/&#34; target=&#34;_blank&#34;&gt;Laravel&lt;/a&gt; or &lt;a href=&#34;https://www.php.net/manual/en/book.pgsql.php&#34; target=&#34;_blank&#34;&gt;PHP&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Python: &lt;a href=&#34;https://github.com/psycopg/psycopg2/&#34; target=&#34;_blank&#34;&gt;psycopg2&lt;/a&gt;, &lt;a href=&#34;https://www.sqlalchemy.org/&#34; target=&#34;_blank&#34;&gt;SQLAlchemy&lt;/a&gt;, or &lt;a href=&#34;https://www.djangoproject.com/&#34; target=&#34;_blank&#34;&gt;Django&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;R: &lt;a href=&#34;https://github.com/r-dbi/DBI&#34; target=&#34;_blank&#34;&gt;DBI&lt;/a&gt; or &lt;a href=&#34;https://github.com/ankane/dbx&#34; target=&#34;_blank&#34;&gt;dbx&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Ruby: &lt;a href=&#34;https://github.com/ged/ruby-pg&#34; target=&#34;_blank&#34;&gt;pg&lt;/a&gt; or &lt;a href=&#34;https://rubyonrails.org/&#34; target=&#34;_blank&#34;&gt;Rails&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Rust: &lt;a href=&#34;https://crates.io/crates/postgres&#34; target=&#34;_blank&#34;&gt;postgres&lt;/a&gt;, &lt;a href=&#34;https://github.com/launchbadge/sqlx&#34; target=&#34;_blank&#34;&gt;SQLx&lt;/a&gt; or &lt;a href=&#34;https://github.com/diesel-rs/diesel&#34; target=&#34;_blank&#34;&gt;Diesel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Swift: &lt;a href=&#34;https://github.com/vapor/postgres-nio&#34; target=&#34;_blank&#34;&gt;PostgresNIO&lt;/a&gt; or &lt;a href=&#34;https://github.com/codewinsdotcom/PostgresClientKit&#34; target=&#34;_blank&#34;&gt;PostgresClientKit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;... open a PR to add your favorite language and connector.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;NLP Tasks&lt;/h1&gt; &#xA;&lt;p&gt;PostgresML integrates 🤗 Hugging Face Transformers to bring state-of-the-art NLP models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw text in your database into useful results. Many state of the art deep learning architectures have been published and made available from Hugging Face &lt;a href=&#34;https://huggingface.co/models&#34; target=&#34;_blank&#34;&gt;model hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can call different NLP tasks and customize using them using the following SQL query.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task   =&amp;gt; TEXT OR JSONB,     -- Pipeline initializer arguments&#xA;    inputs =&amp;gt; TEXT[] OR BYTEA[], -- inputs for inference&#xA;    args   =&amp;gt; JSONB              -- (optional) arguments to the pipeline.&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text Classification&lt;/h2&gt; &#xA;&lt;p&gt;Text classification involves assigning a label or category to a given text. Common use cases include sentiment analysis, natural language inference, and the assessment of grammatical correctness.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/text-classification.png&#34; alt=&#34;text classification&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sentiment Analysis&lt;/h3&gt; &#xA;&lt;p&gt;Sentiment analysis is a type of natural language processing technique that involves analyzing a piece of text to determine the sentiment or emotion expressed within it. It can be used to classify a text as positive, negative, or neutral, and has a wide range of applications in fields such as marketing, customer service, and political analysis.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Basic usage&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task   =&amp;gt; &#39;text-classification&#39;,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;I love how amazingly simple ML has become!&#39;, &#xA;        &#39;I hate doing mundane and thankless tasks. ☹️&#39;&#xA;    ]&#xA;) AS positivity;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;POSITIVE&#34;, &#34;score&#34;: 0.9995759129524232}, &#xA;    {&#34;label&#34;: &#34;NEGATIVE&#34;, &#34;score&#34;: 0.9903519749641418}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default &lt;a href=&#34;https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english&#34; target=&#34;_blank&#34;&gt;model&lt;/a&gt; used for text classification is a fine-tuned version of DistilBERT-base-uncased that has been specifically optimized for the Stanford Sentiment Treebank dataset (sst2).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Using specific model&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use one of the over 19,000 models available on Hugging Face, include the name of the desired model and &lt;code&gt;text-classification&lt;/code&gt; task as a JSONB object in the SQL query. For example, if you want to use a RoBERTa &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-classification&#34; target=&#34;_blank&#34;&gt;model&lt;/a&gt; trained on around 40,000 English tweets and that has POS (positive), NEG (negative), and NEU (neutral) labels for its classes, include this information in the JSONB object when making your query.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;I love how amazingly simple ML has become!&#39;, &#xA;        &#39;I hate doing mundane and thankless tasks. ☹️&#39;&#xA;    ],&#xA;    task  =&amp;gt; &#39;{&#34;task&#34;: &#34;text-classification&#34;, &#xA;              &#34;model&#34;: &#34;finiteautomata/bertweet-base-sentiment-analysis&#34;&#xA;             }&#39;::JSONB&#xA;) AS positivity;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;POS&#34;, &#34;score&#34;: 0.992932200431826}, &#xA;    {&#34;label&#34;: &#34;NEG&#34;, &#34;score&#34;: 0.975599765777588}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Using industry specific model&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;By selecting a model that has been specifically designed for a particular industry, you can achieve more accurate and relevant text classification. An example of such a model is &lt;a href=&#34;https://huggingface.co/ProsusAI/finbert&#34; target=&#34;_blank&#34;&gt;FinBERT&lt;/a&gt;, a pre-trained NLP model that has been optimized for analyzing sentiment in financial text. FinBERT was created by training the BERT language model on a large financial corpus, and fine-tuning it to specifically classify financial sentiment. When using FinBERT, the model will provide softmax outputs for three different labels: positive, negative, or neutral.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Stocks rallied and the British pound gained.&#39;, &#xA;        &#39;Stocks making the biggest moves midday: Nvidia, Palantir and more&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;{&#34;task&#34;: &#34;text-classification&#34;, &#xA;              &#34;model&#34;: &#34;ProsusAI/finbert&#34;&#xA;             }&#39;::JSONB&#xA;) AS market_sentiment;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;positive&#34;, &#34;score&#34;: 0.8983612656593323}, &#xA;    {&#34;label&#34;: &#34;neutral&#34;, &#34;score&#34;: 0.8062630891799927}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Natural Language Inference (NLI)&lt;/h3&gt; &#xA;&lt;p&gt;NLI, or Natural Language Inference, is a type of model that determines the relationship between two texts. The model takes a premise and a hypothesis as inputs and returns a class, which can be one of three types:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Entailment: This means that the hypothesis is true based on the premise.&lt;/li&gt; &#xA; &lt;li&gt;Contradiction: This means that the hypothesis is false based on the premise.&lt;/li&gt; &#xA; &lt;li&gt;Neutral: This means that there is no relationship between the hypothesis and the premise.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The GLUE dataset is the benchmark dataset for evaluating NLI models. There are different variants of NLI models, such as Multi-Genre NLI, Question NLI, and Winograd NLI.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use an NLI model, you can find them on the &lt;span&gt;🤗&lt;/span&gt; Hugging Face model hub. Look for models with &#34;mnli&#34;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;A soccer game with multiple males playing. Some men are playing a sport.&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;{&#34;task&#34;: &#34;text-classification&#34;, &#xA;              &#34;model&#34;: &#34;roberta-large-mnli&#34;&#xA;             }&#39;::JSONB&#xA;) AS nli;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;ENTAILMENT&#34;, &#34;score&#34;: 0.98837411403656}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Question Natural Language Inference (QNLI)&lt;/h3&gt; &#xA;&lt;p&gt;The QNLI task involves determining whether a given question can be answered by the information in a provided document. If the answer can be found in the document, the label assigned is &#34;entailment&#34;. Conversely, if the answer cannot be found in the document, the label assigned is &#34;not entailment&#34;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use an QNLI model, you can find them on the &lt;span&gt;🤗&lt;/span&gt; Hugging Face model hub. Look for models with &#34;qnli&#34;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Where is the capital of France?, Paris is the capital of France.&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;{&#34;task&#34;: &#34;text-classification&#34;, &#xA;              &#34;model&#34;: &#34;cross-encoder/qnli-electra-base&#34;&#xA;             }&#39;::JSONB&#xA;) AS qnli;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;LABEL_0&#34;, &#34;score&#34;: 0.9978110194206238}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quora Question Pairs (QQP)&lt;/h3&gt; &#xA;&lt;p&gt;The Quora Question Pairs model is designed to evaluate whether two given questions are paraphrases of each other. This model takes the two questions and assigns a binary value as output. LABEL_0 indicates that the questions are paraphrases of each other and LABEL_1 indicates that the questions are not paraphrases. The benchmark dataset used for this task is the Quora Question Pairs dataset within the GLUE benchmark, which contains a collection of question pairs and their corresponding labels.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use an QQP model, you can find them on the &lt;span&gt;🤗&lt;/span&gt; Hugging Face model hub. Look for models with &lt;code&gt;qqp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Which city is the capital of France?, Where is the capital of France?&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;{&#34;task&#34;: &#34;text-classification&#34;, &#xA;              &#34;model&#34;: &#34;textattack/bert-base-uncased-QQP&#34;&#xA;             }&#39;::JSONB&#xA;) AS qqp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;LABEL_0&#34;, &#34;score&#34;: 0.9988721013069152}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Grammatical Correctness&lt;/h3&gt; &#xA;&lt;p&gt;Linguistic Acceptability is a task that involves evaluating the grammatical correctness of a sentence. The model used for this task assigns one of two classes to the sentence, either &#34;acceptable&#34; or &#34;unacceptable&#34;. LABEL_0 indicates acceptable and LABEL_1 indicates unacceptable. The benchmark dataset used for training and evaluating models for this task is the Corpus of Linguistic Acceptability (CoLA), which consists of a collection of texts along with their corresponding labels.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use a grammatical correctness model, you can find them on the &lt;span&gt;🤗&lt;/span&gt; Hugging Face model hub. Look for models with &lt;code&gt;cola&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;I will walk to home when I went through the bus.&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;{&#34;task&#34;: &#34;text-classification&#34;, &#xA;              &#34;model&#34;: &#34;textattack/distilbert-base-uncased-CoLA&#34;&#xA;             }&#39;::JSONB&#xA;) AS grammatical_correctness;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;label&#34;: &#34;LABEL_1&#34;, &#34;score&#34;: 0.9576480388641356}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Zero-Shot Classification&lt;/h2&gt; &#xA;&lt;p&gt;Zero Shot Classification is a task where the model predicts a class that it hasn&#39;t seen during the training phase. This task leverages a pre-trained language model and is a type of transfer learning. Transfer learning involves using a model that was initially trained for one task in a different application. Zero Shot Classification is especially helpful when there is a scarcity of labeled data available for the specific task at hand.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/zero-shot-classification.png&#34; alt=&#34;zero-shot classification&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the example provided below, we will demonstrate how to classify a given sentence into a class that the model has not encountered before. To achieve this, we make use of &lt;code&gt;args&lt;/code&gt; in the SQL query, which allows us to provide &lt;code&gt;candidate_labels&lt;/code&gt;. You can customize these labels to suit the context of your task. We will use &lt;code&gt;facebook/bart-large-mnli&lt;/code&gt; model.&lt;/p&gt; &#xA;&lt;p&gt;Look for models with &lt;code&gt;mnli&lt;/code&gt; to use a zero-shot classification model on the &lt;span&gt;🤗&lt;/span&gt; Hugging Face model hub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;I have a problem with my iphone that needs to be resolved asap!!&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;{&#xA;                &#34;task&#34;: &#34;zero-shot-classification&#34;, &#xA;                &#34;model&#34;: &#34;facebook/bart-large-mnli&#34;&#xA;             }&#39;::JSONB,&#xA;    args =&amp;gt; &#39;{&#xA;                &#34;candidate_labels&#34;: [&#34;urgent&#34;, &#34;not urgent&#34;, &#34;phone&#34;, &#34;tablet&#34;, &#34;computer&#34;]&#xA;             }&#39;::JSONB&#xA;) AS zero_shot;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#xA;        &#34;labels&#34;: [&#34;urgent&#34;, &#34;phone&#34;, &#34;computer&#34;, &#34;not urgent&#34;, &#34;tablet&#34;], &#xA;        &#34;scores&#34;: [0.503635, 0.47879, 0.012600, 0.002655, 0.002308], &#xA;        &#34;sequence&#34;: &#34;I have a problem with my iphone that needs to be resolved asap!!&#34;&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Token Classification&lt;/h2&gt; &#xA;&lt;p&gt;Token classification is a task in natural language understanding, where labels are assigned to certain tokens in a text. Some popular subtasks of token classification include Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models can be trained to identify specific entities in a text, such as individuals, places, and dates. PoS tagging, on the other hand, is used to identify the different parts of speech in a text, such as nouns, verbs, and punctuation marks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/token-classification.png&#34; alt=&#34;token classification&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Named Entity Recognition&lt;/h3&gt; &#xA;&lt;p&gt;Named Entity Recognition (NER) is a task that involves identifying named entities in a text. These entities can include the names of people, locations, or organizations. The task is completed by labeling each token with a class for each named entity and a class named &#34;0&#34; for tokens that don&#39;t contain any entities. In this task, the input is text, and the output is the annotated text with named entities.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;I am Omar and I live in New York City.&#39;&#xA;    ],&#xA;    task =&amp;gt; &#39;token-classification&#39;&#xA;) as ner;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[[&#xA;    {&#34;end&#34;: 9,  &#34;word&#34;: &#34;Omar&#34;, &#34;index&#34;: 3,  &#34;score&#34;: 0.997110, &#34;start&#34;: 5,  &#34;entity&#34;: &#34;I-PER&#34;}, &#xA;    {&#34;end&#34;: 27, &#34;word&#34;: &#34;New&#34;,  &#34;index&#34;: 8,  &#34;score&#34;: 0.999372, &#34;start&#34;: 24, &#34;entity&#34;: &#34;I-LOC&#34;}, &#xA;    {&#34;end&#34;: 32, &#34;word&#34;: &#34;York&#34;, &#34;index&#34;: 9,  &#34;score&#34;: 0.999355, &#34;start&#34;: 28, &#34;entity&#34;: &#34;I-LOC&#34;}, &#xA;    {&#34;end&#34;: 37, &#34;word&#34;: &#34;City&#34;, &#34;index&#34;: 10, &#34;score&#34;: 0.999431, &#34;start&#34;: 33, &#34;entity&#34;: &#34;I-LOC&#34;}&#xA;]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Part-of-Speech (PoS) Tagging&lt;/h3&gt; &#xA;&lt;p&gt;PoS tagging is a task that involves identifying the parts of speech, such as nouns, pronouns, adjectives, or verbs, in a given text. In this task, the model labels each word with a specific part of speech.&lt;/p&gt; &#xA;&lt;p&gt;Look for models with &lt;code&gt;pos&lt;/code&gt; to use a zero-shot classification model on the &lt;span&gt;🤗&lt;/span&gt; Hugging Face model hub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select pgml.transform(&#xA;&#x9;inputs =&amp;gt; array [&#xA;  &#x9;&#39;I live in Amsterdam.&#39;&#xA;&#x9;],&#xA;&#x9;task =&amp;gt; &#39;{&#34;task&#34;: &#34;token-classification&#34;, &#xA;              &#34;model&#34;: &#34;vblagoje/bert-english-uncased-finetuned-pos&#34;&#xA;    }&#39;::JSONB&#xA;) as pos;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[[&#xA;    {&#34;end&#34;: 1,  &#34;word&#34;: &#34;i&#34;,         &#34;index&#34;: 1, &#34;score&#34;: 0.999, &#34;start&#34;: 0,  &#34;entity&#34;: &#34;PRON&#34;},&#xA;    {&#34;end&#34;: 6,  &#34;word&#34;: &#34;live&#34;,      &#34;index&#34;: 2, &#34;score&#34;: 0.998, &#34;start&#34;: 2,  &#34;entity&#34;: &#34;VERB&#34;},&#xA;    {&#34;end&#34;: 9,  &#34;word&#34;: &#34;in&#34;,        &#34;index&#34;: 3, &#34;score&#34;: 0.999, &#34;start&#34;: 7,  &#34;entity&#34;: &#34;ADP&#34;},&#xA;    {&#34;end&#34;: 19, &#34;word&#34;: &#34;amsterdam&#34;, &#34;index&#34;: 4, &#34;score&#34;: 0.998, &#34;start&#34;: 10, &#34;entity&#34;: &#34;PROPN&#34;}, &#xA;    {&#34;end&#34;: 20, &#34;word&#34;: &#34;.&#34;,         &#34;index&#34;: 5, &#34;score&#34;: 0.999, &#34;start&#34;: 19, &#34;entity&#34;: &#34;PUNCT&#34;}&#xA;]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Translation&lt;/h2&gt; &#xA;&lt;p&gt;Translation is the task of converting text written in one language into another language.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/translation.png&#34; alt=&#34;translation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You have the option to select from over 2000 models available on the Hugging Face &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=translation&#34; target=&#34;_blank&#34;&gt;hub&lt;/a&gt; for translation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select pgml.transform(&#xA;    inputs =&amp;gt; array[&#xA;            &#x9;&#39;How are you?&#39;&#xA;    ],&#xA;&#x9;task =&amp;gt; &#39;{&#34;task&#34;: &#34;translation&#34;, &#xA;              &#34;model&#34;: &#34;Helsinki-NLP/opus-mt-en-fr&#34;&#xA;    }&#39;::JSONB&#x9;&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;translation_text&#34;: &#34;Comment allez-vous ?&#34;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Summarization&lt;/h2&gt; &#xA;&lt;p&gt;Summarization involves creating a condensed version of a document that includes the important information while reducing its length. Different models can be used for this task, with some models extracting the most relevant text from the original document, while other models generate completely new text that captures the essence of the original content.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/summarization.png&#34; alt=&#34;summarization&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select pgml.transform(&#xA;&#x9;task =&amp;gt; &#39;{&#34;task&#34;: &#34;summarization&#34;, &#xA;              &#34;model&#34;: &#34;sshleifer/distilbart-cnn-12-6&#34;&#xA;    }&#39;::JSONB,&#xA;&#x9;inputs =&amp;gt; array[&#xA;&#x9;&#39;Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles). The City of Paris is the centre and seat of government of the region and province of Île-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.&#39;&#xA;&#x9;]&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;summary_text&#34;: &#34; Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018 . The city is the centre and seat of government of the region and province of Île-de-France, or Paris Region . Paris Region has an estimated 18 percent of the population of France as of 2017 .&#34;}&#xA;    ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can control the length of summary_text by passing &lt;code&gt;min_length&lt;/code&gt; and &lt;code&gt;max_length&lt;/code&gt; as arguments to the SQL query.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select pgml.transform(&#xA;&#x9;task =&amp;gt; &#39;{&#34;task&#34;: &#34;summarization&#34;, &#xA;              &#34;model&#34;: &#34;sshleifer/distilbart-cnn-12-6&#34;&#xA;    }&#39;::JSONB,&#xA;&#x9;inputs =&amp;gt; array[&#xA;&#x9;&#39;Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles). The City of Paris is the centre and seat of government of the region and province of Île-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.&#39;&#xA;&#x9;],&#xA;&#x9;args =&amp;gt; &#39;{&#xA;            &#34;min_length&#34; : 20,&#xA;            &#34;max_length&#34; : 70&#xA;&#x9;}&#39;::JSONB&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;summary_text&#34;: &#34; Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018 . City of Paris is centre and seat of government of the region and province of Île-de-France, or Paris Region, which has an estimated 12,174,880, or about 18 percent&#34;&#xA;    }  &#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Question Answering&lt;/h2&gt; &#xA;&lt;p&gt;Question Answering models are designed to retrieve the answer to a question from a given text, which can be particularly useful for searching for information within a document. It&#39;s worth noting that some question answering models are capable of generating answers even without any contextual information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/question-answering.png&#34; alt=&#34;question answering&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    &#39;question-answering&#39;,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;{&#xA;            &#34;question&#34;: &#34;Where do I live?&#34;,&#xA;            &#34;context&#34;: &#34;My name is Merve and I live in İstanbul.&#34;&#xA;        }&#39;&#xA;    ]&#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;end&#34;   :  39, &#xA;    &#34;score&#34; :  0.9538117051124572, &#xA;    &#34;start&#34; :  31, &#xA;    &#34;answer&#34;: &#34;İstanbul&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- ## Table Question Answering&#xA;![table question answering](pgml-docs/docs/images/table-question-answering.png) --&gt; &#xA;&lt;h2&gt;Text Generation&lt;/h2&gt; &#xA;&lt;p&gt;Text generation is the task of producing new text, such as filling in incomplete sentences or paraphrasing existing text. It has various use cases, including code generation and story generation. Completion generation models can predict the next word in a text sequence, while text-to-text generation models are trained to learn the mapping between pairs of texts, such as translating between languages. Popular models for text generation include GPT-based models, T5, T0, and BART. These models can be trained to accomplish a wide range of tasks, including text classification, summarization, and translation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/text-generation.png&#34; alt=&#34;text generation&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;text-generation&#39;,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ]&#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    [&#xA;        {&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and eight for the Dragon-lords in their halls of blood.\n\nEach of the guild-building systems is one-man&#34;}&#xA;    ]&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use a specific model from &lt;span&gt;🤗&lt;/span&gt; model hub, pass the model name along with task name in task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text-generation&#34;,&#xA;        &#34;model&#34; : &#34;gpt2-medium&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ]&#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    [{&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone.\n\nThis place has a deep connection to the lore of ancient Elven civilization. It is home to the most ancient of artifacts,&#34;}]&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To make the generated text longer, you can include the argument &lt;code&gt;max_length&lt;/code&gt; and specify the desired maximum length of the text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text-generation&#34;,&#xA;        &#34;model&#34; : &#34;gpt2-medium&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ],&#xA;    args =&amp;gt; &#39;{&#xA;&#x9;&#x9;&#x9;&#34;max_length&#34; : 200&#xA;&#x9;&#x9;}&#39;::JSONB &#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    [{&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Three for the Dwarfs and the Elves, One for the Gnomes of the Mines, and Two for the Elves of Dross.\&#34;\n\nHobbits: The Fellowship is the first book of J.R.R. Tolkien&#39;s story-cycle, and began with his second novel - The Two Towers - and ends in The Lord of the Rings.\n\n\nIt is a non-fiction novel, so there is no copyright claim on some parts of the story but the actual text of the book is copyrighted by author J.R.R. Tolkien.\n\n\nThe book has been classified into two types: fantasy novels and children&#39;s books\n\nHobbits: The Fellowship is the first book of J.R.R. Tolkien&#39;s story-cycle, and began with his second novel - The Two Towers - and ends in The Lord of the Rings.It&#34;}]&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want the model to generate more than one output, you can specify the number of desired output sequences by including the argument &lt;code&gt;num_return_sequences&lt;/code&gt; in the arguments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text-generation&#34;,&#xA;        &#34;model&#34; : &#34;gpt2-medium&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ],&#xA;    args =&amp;gt; &#39;{&#xA;&#x9;&#x9;&#x9;&#34;num_return_sequences&#34; : 3&#xA;&#x9;&#x9;}&#39;::JSONB &#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    [&#xA;        {&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and Thirteen for the human-men in their hall of fire.\n\nAll of us, our families, and our people&#34;}, &#xA;        {&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and the tenth for a King! As each of these has its own special story, so I have written them into the game.&#34;}, &#xA;        {&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone… What&#39;s left in the end is your heart&#39;s desire after all!\n\nHans: (Trying to be brave)&#34;}&#xA;    ]&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Text generation typically utilizes a greedy search algorithm that selects the word with the highest probability as the next word in the sequence. However, an alternative method called beam search can be used, which aims to minimize the possibility of overlooking hidden high probability word combinations. Beam search achieves this by retaining the num_beams most likely hypotheses at each step and ultimately selecting the hypothesis with the highest overall probability. We set &lt;code&gt;num_beams &amp;gt; 1&lt;/code&gt; and &lt;code&gt;early_stopping=True&lt;/code&gt; so that generation is finished when all beam hypotheses reached the EOS token.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text-generation&#34;,&#xA;        &#34;model&#34; : &#34;gpt2-medium&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ],&#xA;    args =&amp;gt; &#39;{&#xA;&#x9;&#x9;&#x9;&#34;num_beams&#34; : 5,&#xA;&#x9;&#x9;&#x9;&#34;early_stopping&#34; : true&#xA;&#x9;&#x9;}&#39;::JSONB &#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[[&#xA;    {&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Nine for the Dwarves in their caverns of ice, Ten for the Elves in their caverns of fire, Eleven for the&#34;}&#xA;]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sampling methods involve selecting the next word or sequence of words at random from the set of possible candidates, weighted by their probabilities according to the language model. This can result in more diverse and creative text, as well as avoiding repetitive patterns. In its most basic form, sampling means randomly picking the next word $w_t$ according to its conditional probability distribution: $$ w_t \approx P(w_t|w_{1:t-1})$$&lt;/p&gt; &#xA;&lt;p&gt;However, the randomness of the sampling method can also result in less coherent or inconsistent text, depending on the quality of the model and the chosen sampling parameters such as temperature, top-k, or top-p. Therefore, choosing an appropriate sampling method and parameters is crucial for achieving the desired balance between creativity and coherence in generated text.&lt;/p&gt; &#xA;&lt;p&gt;You can pass &lt;code&gt;do_sample = True&lt;/code&gt; in the arguments to use sampling methods. It is recommended to alter &lt;code&gt;temperature&lt;/code&gt; or &lt;code&gt;top_p&lt;/code&gt; but not both.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Temperature&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text-generation&#34;,&#xA;        &#34;model&#34; : &#34;gpt2-medium&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ],&#xA;    args =&amp;gt; &#39;{&#xA;&#x9;&#x9;&#x9;&#34;do_sample&#34; : true,&#xA;&#x9;&#x9;&#x9;&#34;temperature&#34; : 0.9&#xA;&#x9;&#x9;}&#39;::JSONB &#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[[{&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and Thirteen for the Giants and Men of S.A.\n\nThe First Seven-Year Time-Traveling Trilogy is&#34;}]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Top p&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text-generation&#34;,&#xA;        &#34;model&#34; : &#34;gpt2-medium&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone&#39;&#xA;    ],&#xA;    args =&amp;gt; &#39;{&#xA;&#x9;&#x9;&#x9;&#34;do_sample&#34; : true,&#xA;&#x9;&#x9;&#x9;&#34;top_p&#34; : 0.8&#xA;&#x9;&#x9;}&#39;::JSONB &#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[[{&#34;generated_text&#34;: &#34;Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Four for the Elves of the forests and fields, and Three for the Dwarfs and their warriors.\&#34; ―Lord Rohan [src&#34;}]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text-to-Text Generation&lt;/h2&gt; &#xA;&lt;p&gt;Text-to-text generation methods, such as T5, are neural network architectures designed to perform various natural language processing tasks, including summarization, translation, and question answering. T5 is a transformer-based architecture pre-trained on a large corpus of text data using denoising autoencoding. This pre-training process enables the model to learn general language patterns and relationships between different tasks, which can be fine-tuned for specific downstream tasks. During fine-tuning, the T5 model is trained on a task-specific dataset to learn how to perform the specific task. &lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/text-to-text-generation.png&#34; alt=&#34;text-to-text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Translation&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text2text-generation&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;translate from English to French: I&#39;&#39;m very happy&#39;&#xA;    ]&#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;generated_text&#34;: &#34;Je suis très heureux&#34;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similar to other tasks, we can specify a model for text-to-text generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;text2text-generation&#34;,&#xA;        &#34;model&#34; : &#34;bigscience/T0&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Is the word &#39;&#39;table&#39;&#39; used in the same meaning in the two previous sentences? Sentence A: you can leave the books on the table over there. Sentence B: the tables in this book are very hard to read.&#39;&#xA;&#xA;    ]&#xA;) AS answer;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fill-Mask&lt;/h2&gt; &#xA;&lt;p&gt;Fill-mask refers to a task where certain words in a sentence are hidden or &#34;masked&#34;, and the objective is to predict what words should fill in those masked positions. Such models are valuable when we want to gain statistical insights about the language used to train the model. &lt;img src=&#34;https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-docs/docs/images/fill-mask.png&#34; alt=&#34;fill mask&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.transform(&#xA;    task =&amp;gt; &#39;{&#xA;        &#34;task&#34; : &#34;fill-mask&#34;&#xA;    }&#39;::JSONB,&#xA;    inputs =&amp;gt; ARRAY[&#xA;        &#39;Paris is the &amp;lt;mask&amp;gt; of France.&#39;&#xA;&#xA;    ]&#xA;) AS answer;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;    {&#34;score&#34;: 0.679, &#34;token&#34;: 812,   &#34;sequence&#34;: &#34;Paris is the capital of France.&#34;,    &#34;token_str&#34;: &#34; capital&#34;}, &#xA;    {&#34;score&#34;: 0.051, &#34;token&#34;: 32357, &#34;sequence&#34;: &#34;Paris is the birthplace of France.&#34;, &#34;token_str&#34;: &#34; birthplace&#34;}, &#xA;    {&#34;score&#34;: 0.038, &#34;token&#34;: 1144,  &#34;sequence&#34;: &#34;Paris is the heart of France.&#34;,      &#34;token_str&#34;: &#34; heart&#34;}, &#xA;    {&#34;score&#34;: 0.024, &#34;token&#34;: 29778, &#34;sequence&#34;: &#34;Paris is the envy of France.&#34;,       &#34;token_str&#34;: &#34; envy&#34;}, &#xA;    {&#34;score&#34;: 0.022, &#34;token&#34;: 1867,  &#34;sequence&#34;: &#34;Paris is the Capital of France.&#34;,    &#34;token_str&#34;: &#34; Capital&#34;}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Vector Database&lt;/h1&gt; &#xA;&lt;p&gt;A vector database is a type of database that stores and manages vectors, which are mathematical representations of data points in a multi-dimensional space. Vectors can be used to represent a wide range of data types, including images, text, audio, and numerical data. It is designed to support efficient searching and retrieval of vectors, using methods such as nearest neighbor search, clustering, and indexing. These methods enable applications to find vectors that are similar to a given query vector, which is useful for tasks such as image search, recommendation systems, and natural language processing.&lt;/p&gt; &#xA;&lt;p&gt;PostgresML enhances your existing PostgreSQL database to be used as a vector database by generating embeddings from text stored in your tables. To generate embeddings, you can use the &lt;code&gt;pgml.embed&lt;/code&gt; function, which takes a transformer name and a text value as input. This function automatically downloads and caches the transformer for future reuse, which saves time and resources.&lt;/p&gt; &#xA;&lt;p&gt;Using a vector database involves three key steps: creating embeddings, indexing your embeddings using different algorithms, and querying the index using embeddings for your queries. Let&#39;s break down each step in more detail.&lt;/p&gt; &#xA;&lt;h2&gt;Step 1: Creating embeddings using transformers&lt;/h2&gt; &#xA;&lt;p&gt;To create embeddings for your data, you first need to choose a transformer that can generate embeddings from your input data. Some popular transformer options include BERT, GPT-2, and T5. Once you&#39;ve selected a transformer, you can use it to generate embeddings for your data.&lt;/p&gt; &#xA;&lt;p&gt;In the following section, we will demonstrate how to use PostgresML to generate embeddings for a dataset of tweets commonly used in sentiment analysis. To generate the embeddings, we will use the &lt;code&gt;pgml.embed&lt;/code&gt; function, which will generate an embedding for each tweet in the dataset. These embeddings will then be inserted into a table called tweet_embeddings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT pgml.load_dataset(&#39;tweet_eval&#39;, &#39;sentiment&#39;);&#xA;&#xA;SELECT * &#xA;FROM pgml.tweet_eval&#xA;LIMIT 10;&#xA;&#xA;CREATE TABLE tweet_embeddings AS&#xA;SELECT text, pgml.embed(&#39;distilbert-base-uncased&#39;, text) AS embedding &#xA;FROM pgml.tweet_eval;&#xA;&#xA;SELECT * from tweet_embeddings limit 2;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;text&lt;/th&gt; &#xA;   &lt;th&gt;embedding&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin&#34;&lt;/td&gt; &#xA;   &lt;td&gt;{-0.1567948312,-0.3149209619,0.2163394839,..}&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ&#34;&lt;/td&gt; &#xA;   &lt;td&gt;{-0.0701668188,-0.012231146,0.1304316372,.. }&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Step 2: Indexing your embeddings using different algorithms&lt;/h2&gt; &#xA;&lt;p&gt;After you&#39;ve created embeddings for your data, you need to index them using one or more indexing algorithms. There are several different types of indexing algorithms available, including B-trees, k-nearest neighbors (KNN), and approximate nearest neighbors (ANN). The specific type of indexing algorithm you choose will depend on your use case and performance requirements. For example, B-trees are a good choice for range queries, while KNN and ANN algorithms are more efficient for similarity searches.&lt;/p&gt; &#xA;&lt;p&gt;On small datasets (&amp;lt;100k rows), a linear search that compares every row to the query will give sub-second results, which may be fast enough for your use case. For larger datasets, you may want to consider various indexing strategies offered by additional extensions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.postgresql.org/docs/current/cube.html&#34; target=&#34;_blank&#34;&gt;Cube&lt;/a&gt; is a built-in extension that provides a fast indexing strategy for finding similar vectors. By default it has an arbitrary limit of 100 dimensions, unless Postgres is compiled with a larger size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pgvector/pgvector&#34; target=&#34;_blank&#34;&gt;PgVector&lt;/a&gt; supports embeddings up to 2000 dimensions out of the box, and provides a fast indexing strategy for finding similar vectors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When indexing your embeddings, it&#39;s important to consider the trade-offs between accuracy and speed. Exact indexing algorithms like B-trees can provide precise results, but may not be as fast as approximate indexing algorithms like KNN and ANN. Similarly, some indexing algorithms may require more memory or disk space than others.&lt;/p&gt; &#xA;&lt;p&gt;In the following, we are creating an index on the tweet_embeddings table using the ivfflat algorithm for indexing. The ivfflat algorithm is a type of hybrid index that combines an Inverted File (IVF) index with a Flat (FLAT) index.&lt;/p&gt; &#xA;&lt;p&gt;The index is being created on the embedding column in the tweet_embeddings table, which contains vector embeddings generated from the original tweet dataset. The &lt;code&gt;vector_cosine_ops&lt;/code&gt; argument specifies the indexing operation to use for the embeddings. In this case, it&#39;s using the &lt;code&gt;cosine similarity&lt;/code&gt; operation, which is a common method for measuring similarity between vectors.&lt;/p&gt; &#xA;&lt;p&gt;By creating an index on the embedding column, the database can quickly search for and retrieve records that are similar to a given query vector. This can be useful for a variety of machine learning applications, such as similarity search or recommendation systems.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE INDEX ON tweet_embeddings USING ivfflat (embedding vector_cosine_ops);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Step 3: Querying the index using embeddings for your queries&lt;/h2&gt; &#xA;&lt;p&gt;Once your embeddings have been indexed, you can use them to perform queries against your database. To do this, you&#39;ll need to provide a query embedding that represents the query you want to perform. The index will then return the closest matching embeddings from your database, based on the similarity between the query embedding and the stored embeddings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;WITH query AS (&#xA;    SELECT pgml.embed(&#39;distilbert-base-uncased&#39;, &#39;Star Wars christmas special is on Disney&#39;)::vector AS embedding&#xA;)&#xA;SELECT * FROM items, query ORDER BY items.embedding &amp;lt;-&amp;gt; query.embedding LIMIT 5;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;text&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Happy Friday with Batman animated Series 90S forever!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;Fri Oct 17, Sonic Highways is on HBO tonight, Also new episode of Girl Meets World on Disney&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tfw the 2nd The Hunger Games movie is on Amazon Prime but not the 1st one I didn&#39;t watch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5 RT&#39;s if you want the next episode of twilight princess tomorrow&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jurassic Park is BACK! New Trailer for the 4th Movie, Jurassic World -&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- ## Sentence Similarity&#xA;Sentence Similarity involves determining the degree of similarity between two texts. To accomplish this, Sentence similarity models convert the input texts into vectors (embeddings) that encapsulate semantic information, and then measure the proximity (or similarity) between the vectors. This task is especially beneficial for tasks such as information retrieval and clustering/grouping.&#xA;![sentence similarity](pgml-docs/docs/images/sentence-similarity.png)&#xA;&#xA;&lt;!-- ## Conversational --&gt; &#xA;&lt;!-- # Regression&#xA;# Classification --&gt;</summary>
  </entry>
</feed>