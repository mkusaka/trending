<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-10T02:00:35Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apache/arrow-rs</title>
    <updated>2024-03-10T02:00:35Z</updated>
    <id>tag:github.com,2024-03-10:/apache/arrow-rs</id>
    <link href="https://github.com/apache/arrow-rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Rust implementation of Apache Arrow&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Native Rust implementation of Apache Arrow and Parquet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/apache/arrow-rs?branch=master&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/arrow-rs/rust/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the implementation of Arrow, the popular in-memory columnar format, in &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repo contains the following main components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Crate&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Latest API Docs&lt;/th&gt; &#xA;   &lt;th&gt;README&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arrow&lt;/td&gt; &#xA;   &lt;td&gt;Core functionality (memory layout, arrays, low level computations)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.rs/arrow/latest&#34;&gt;docs.rs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/arrow-rs/master/arrow/README.md&#34;&gt;(README)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;parquet&lt;/td&gt; &#xA;   &lt;td&gt;Support for Parquet columnar file format&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.rs/parquet/latest&#34;&gt;docs.rs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/arrow-rs/master/parquet/README.md&#34;&gt;(README)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arrow-flight&lt;/td&gt; &#xA;   &lt;td&gt;Support for Arrow-Flight IPC protocol&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.rs/arrow-flight/latest&#34;&gt;docs.rs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/arrow-rs/master/arrow-flight/README.md&#34;&gt;(README)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;object-store&lt;/td&gt; &#xA;   &lt;td&gt;Support for object store interactions (aws, azure, gcp, local, in-memory)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.rs/object_store/latest&#34;&gt;docs.rs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/arrow-rs/master/object_store/README.md&#34;&gt;(README)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The current development version the API documentation in this repo can be found &lt;a href=&#34;https://arrow.apache.org/rust&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are two related crates in a different repository&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Crate&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DataFusion&lt;/td&gt; &#xA;   &lt;td&gt;In-memory query engine with SQL support&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/apache/arrow-datafusion/raw/main/README.md&#34;&gt;(README)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ballista&lt;/td&gt; &#xA;   &lt;td&gt;Distributed query execution&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/apache/arrow-ballista/raw/main/README.md&#34;&gt;(README)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Collectively, these crates support a vast array of functionality for analytic computations in Rust.&lt;/p&gt; &#xA;&lt;p&gt;For example, you can write an SQL query or a &lt;code&gt;DataFrame&lt;/code&gt; (using the &lt;code&gt;datafusion&lt;/code&gt; crate), run it against a parquet file (using the &lt;code&gt;parquet&lt;/code&gt; crate), evaluate it in-memory using Arrow&#39;s columnar format (using the &lt;code&gt;arrow&lt;/code&gt; crate), and send to another process (using the &lt;code&gt;arrow-flight&lt;/code&gt; crate).&lt;/p&gt; &#xA;&lt;p&gt;Generally speaking, the &lt;code&gt;arrow&lt;/code&gt; crate offers functionality for using Arrow arrays, and &lt;code&gt;datafusion&lt;/code&gt; offers most operations typically found in SQL, including &lt;code&gt;join&lt;/code&gt;s and window functions.&lt;/p&gt; &#xA;&lt;p&gt;You can find more details about each crate in their respective READMEs.&lt;/p&gt; &#xA;&lt;h2&gt;Arrow Rust Community&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;dev@arrow.apache.org&lt;/code&gt; mailing list serves as the core communication channel for the Arrow community. Instructions for signing up and links to the archives can be found at the &lt;a href=&#34;https://arrow.apache.org/community/&#34;&gt;Arrow Community&lt;/a&gt; page. All major announcements and communications happen there.&lt;/p&gt; &#xA;&lt;p&gt;The Rust Arrow community also uses the official &lt;a href=&#34;https://s.apache.org/slack-invite&#34;&gt;ASF Slack&lt;/a&gt; for informal discussions and coordination. This is a great place to meet other contributors and get guidance on where to contribute. Join us in the &lt;code&gt;#arrow-rust&lt;/code&gt; channel and feel free to ask for an invite via:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;the &lt;code&gt;dev@arrow.apache.org&lt;/code&gt; mailing list&lt;/li&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://github.com/apache/arrow-rs/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://discord.gg/YAb2TdazKQ&#34;&gt;Discord channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The Rust implementation uses &lt;a href=&#34;https://github.com/apache/arrow-rs/issues&#34;&gt;GitHub issues&lt;/a&gt; as the system of record for new features and bug fixes and this plays a critical role in the release process.&lt;/p&gt; &#xA;&lt;p&gt;For design discussions we generally collaborate on Google documents and file a GitHub issue linking to the document.&lt;/p&gt; &#xA;&lt;p&gt;There is more information in the &lt;a href=&#34;https://raw.githubusercontent.com/apache/arrow-rs/master/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; guide.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/text-embeddings-inference</title>
    <updated>2024-03-10T02:00:35Z</updated>
    <id>tag:github.com,2024-03-10:/huggingface/text-embeddings-inference</id>
    <link href="https://github.com/huggingface/text-embeddings-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A blazing fast inference solution for text embeddings models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Text Embeddings Inference&lt;/h1&gt; &#xA; &lt;a href=&#34;https://github.com/huggingface/text-embeddings-inference&#34;&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/huggingface/text-embeddings-inference?style=social&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.github.io/text-embeddings-inference&#34;&gt; &lt;img alt=&#34;Swagger API documentation&#34; src=&#34;https://img.shields.io/badge/API-Swagger-informational&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;A blazing fast inference solution for text embeddings models.&lt;/p&gt; &#xA; &lt;p&gt;Benchmark for &lt;a href=&#34;https://huggingface.co/BAAI/bge-base-en-v1.5&#34;&gt;BAAI/bge-base-en-v1.5&lt;/a&gt; on an Nvidia A10 with a sequence length of 512 tokens:&lt;/p&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs1-lat.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs1-tp.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs32-lat.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/assets/bs32-tp.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#docker-images&#34;&gt;Docker Images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-a-private-or-gated-model&#34;&gt;Using a private or gated model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-re-rankers-models&#34;&gt;Using Re-rankers models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-sequence-classification-models&#34;&gt;Using Sequence Classification models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#using-splade-pooling&#34;&gt;Using SPLADE pooling&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#distributed-tracing&#34;&gt;Distributed Tracing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#grpc&#34;&gt;gRPC&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#local-install&#34;&gt;Local Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#docker-build&#34;&gt;Docker Build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-embeddings-inference/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Text Embeddings Inference (TEI) is a toolkit for deploying and serving open source text embeddings and sequence classification models. TEI enables high-performance extraction for the most popular models, including FlagEmbedding, Ember, GTE and E5. TEI implements many features such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No model graph compilation step&lt;/li&gt; &#xA; &lt;li&gt;Metal support for local execution on Macs&lt;/li&gt; &#xA; &lt;li&gt;Small docker images and fast boot times. Get ready for true serverless!&lt;/li&gt; &#xA; &lt;li&gt;Token based dynamic batching&lt;/li&gt; &#xA; &lt;li&gt;Optimized transformers code for inference using &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;Flash Attention&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/candle&#34;&gt;Candle&lt;/a&gt; and &lt;a href=&#34;https://docs.nvidia.com/cuda/cublas/#using-the-cublaslt-api&#34;&gt;cuBLASLt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;Safetensors&lt;/a&gt; weight loading&lt;/li&gt; &#xA; &lt;li&gt;Production ready (distributed tracing with Open Telemetry, Prometheus metrics)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Supported Models&lt;/h3&gt; &#xA;&lt;h4&gt;Text Embeddings&lt;/h4&gt; &#xA;&lt;p&gt;You can use any JinaBERT model with Alibi or absolute positions or any BERT, CamemBERT, RoBERTa, or XLM-RoBERTa model with absolute positions in &lt;code&gt;text-embeddings-inference&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Support for other model types will be added in the future.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Examples of supported models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;MTEB Rank&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Model ID&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;Bert&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hf.co/WhereIsAI/UAE-Large-V1&#34;&gt;WhereIsAI/UAE-Large-V1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hf.co/intfloat/multilingual-e5-large-instruct&#34;&gt;intfloat/multilingual-e5-large-instruct&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;NomicBert&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hf.co/nomic-ai/nomic-embed-text-v1&#34;&gt;nomic-ai/nomic-embed-text-v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;NomicBert&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hf.co/nomic-ai/nomic-embed-text-v1.5&#34;&gt;nomic-ai/nomic-embed-text-v1.5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;JinaBERT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hf.co/jinaai/jina-embeddings-v2-base-en&#34;&gt;jinaai/jina-embeddings-v2-base-en&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can explore the list of best performing text embeddings models &lt;a href=&#34;https://huggingface.co/spaces/mteb/leaderboard&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Sequence Classification and Re-Ranking&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; v0.4.0 added support for Bert, CamemBERT, RoBERTa and XLM-RoBERTa Sequence Classification models.&lt;/p&gt; &#xA;&lt;p&gt;Example of supported sequence classification models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Model ID&lt;/th&gt; &#xA;   &lt;th&gt;Revision&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Re-Ranking&lt;/td&gt; &#xA;   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-reranker-large&#34;&gt;BAAI/bge-reranker-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;refs/pr/4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Re-Ranking&lt;/td&gt; &#xA;   &lt;td&gt;XLM-RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-reranker-base&#34;&gt;BAAI/bge-reranker-base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;refs/pr/5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sentiment Analysis&lt;/td&gt; &#xA;   &lt;td&gt;RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/SamLowe/roberta-base-go_emotions&#34;&gt;SamLowe/roberta-base-go_emotions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=BAAI/bge-large-en-v1.5&#xA;revision=refs/pr/5&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.1 --model-id $model --revision $revision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then you can make requests like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl 127.0.0.1:8080/embed \&#xA;    -X POST \&#xA;    -d &#39;{&#34;inputs&#34;:&#34;What is Deep Learning?&#34;}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To use GPUs, you need to install the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;. NVIDIA drivers on your machine need to be compatible with CUDA version 12.2 or higher.&lt;/p&gt; &#xA;&lt;p&gt;To see all options to serve your models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;text-embeddings-router --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Usage: text-embeddings-router [OPTIONS]&#xA;&#xA;Options:&#xA;      --model-id &amp;lt;MODEL_ID&amp;gt;&#xA;          The name of the model to load. Can be a MODEL_ID as listed on &amp;lt;https://hf.co/models&amp;gt; like `thenlper/gte-base`.&#xA;          Or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of&#xA;          transformers&#xA;&#xA;          [env: MODEL_ID=]&#xA;          [default: thenlper/gte-base]&#xA;&#xA;      --revision &amp;lt;REVISION&amp;gt;&#xA;          The actual revision of the model if you&#39;re referring to a model on the hub. You can use a specific commit id&#xA;          or a branch like `refs/pr/2`&#xA;&#xA;          [env: REVISION=]&#xA;&#xA;      --tokenization-workers &amp;lt;TOKENIZATION_WORKERS&amp;gt;&#xA;          Optionally control the number of tokenizer workers used for payload tokenization, validation and truncation.&#xA;          Default to the number of CPU cores on the machine&#xA;&#xA;          [env: TOKENIZATION_WORKERS=]&#xA;&#xA;      --dtype &amp;lt;DTYPE&amp;gt;&#xA;          The dtype to be forced upon the model&#xA;&#xA;          [env: DTYPE=]&#xA;          [possible values: float16, float32]&#xA;&#xA;      --pooling &amp;lt;POOLING&amp;gt;&#xA;          Optionally control the pooling method for embedding models.&#xA;&#xA;          If `pooling` is not set, the pooling configuration will be parsed from the model `1_Pooling/config.json` configuration.&#xA;&#xA;          If `pooling` is set, it will override the model pooling configuration&#xA;&#xA;          [env: POOLING=]&#xA;&#xA;          Possible values:&#xA;          - cls:    Select the CLS token as embedding&#xA;          - mean:   Apply Mean pooling to the model embeddings&#xA;          - splade: Apply SPLADE (Sparse Lexical and Expansion) to the model embeddings. This option is only available if the loaded model is a `ForMaskedLM` Transformer model&#xA;&#xA;      --max-concurrent-requests &amp;lt;MAX_CONCURRENT_REQUESTS&amp;gt;&#xA;          The maximum amount of concurrent requests for this particular deployment.&#xA;          Having a low limit will refuse clients requests instead of having them wait for too long and is usually good&#xA;          to handle backpressure correctly&#xA;&#xA;          [env: MAX_CONCURRENT_REQUESTS=]&#xA;          [default: 512]&#xA;&#xA;      --max-batch-tokens &amp;lt;MAX_BATCH_TOKENS&amp;gt;&#xA;          **IMPORTANT** This is one critical control to allow maximum usage of the available hardware.&#xA;&#xA;          This represents the total amount of potential tokens within a batch.&#xA;&#xA;          For `max_batch_tokens=1000`, you could fit `10` queries of `total_tokens=100` or a single query of `1000` tokens.&#xA;&#xA;          Overall this number should be the largest possible until the model is compute bound. Since the actual memory&#xA;          overhead depends on the model implementation, text-embeddings-inference cannot infer this number automatically.&#xA;&#xA;          [env: MAX_BATCH_TOKENS=]&#xA;          [default: 16384]&#xA;&#xA;      --max-batch-requests &amp;lt;MAX_BATCH_REQUESTS&amp;gt;&#xA;          Optionally control the maximum number of individual requests in a batch&#xA;&#xA;          [env: MAX_BATCH_REQUESTS=]&#xA;&#xA;      --max-client-batch-size &amp;lt;MAX_CLIENT_BATCH_SIZE&amp;gt;&#xA;          Control the maximum number of inputs that a client can send in a single request&#xA;&#xA;          [env: MAX_CLIENT_BATCH_SIZE=]&#xA;          [default: 32]&#xA;&#xA;      --hf-api-token &amp;lt;HF_API_TOKEN&amp;gt;&#xA;          Your HuggingFace hub token&#xA;&#xA;          [env: HF_API_TOKEN=]&#xA;&#xA;      --hostname &amp;lt;HOSTNAME&amp;gt;&#xA;          The IP address to listen on&#xA;&#xA;          [env: HOSTNAME=]&#xA;          [default: 0.0.0.0]&#xA;&#xA;  -p, --port &amp;lt;PORT&amp;gt;&#xA;          The port to listen on&#xA;&#xA;          [env: PORT=]&#xA;          [default: 3000]&#xA;&#xA;      --uds-path &amp;lt;UDS_PATH&amp;gt;&#xA;          The name of the unix socket some text-embeddings-inference backends will use as they communicate internally&#xA;          with gRPC&#xA;&#xA;          [env: UDS_PATH=]&#xA;          [default: /tmp/text-embeddings-inference-server]&#xA;&#xA;      --huggingface-hub-cache &amp;lt;HUGGINGFACE_HUB_CACHE&amp;gt;&#xA;          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk&#xA;          for instance&#xA;&#xA;          [env: HUGGINGFACE_HUB_CACHE=/data]&#xA;&#xA;      --json-output&#xA;          Outputs the logs in JSON format (useful for telemetry)&#xA;&#xA;          [env: JSON_OUTPUT=]&#xA;&#xA;      --otlp-endpoint &amp;lt;OTLP_ENDPOINT&amp;gt;&#xA;          The grpc endpoint for opentelemetry. Telemetry is sent to this endpoint as OTLP over gRPC.&#xA;          e.g. `http://localhost:4317`&#xA;          [env: OTLP_ENDPOINT=]&#xA;&#xA;      --cors-allow-origin &amp;lt;CORS_ALLOW_ORIGIN&amp;gt;&#xA;          [env: CORS_ALLOW_ORIGIN=]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Images&lt;/h3&gt; &#xA;&lt;p&gt;Text Embeddings Inference ships with multiple Docker images that you can use to target a specific backend:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:cpu-1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Volta&lt;/td&gt; &#xA;   &lt;td&gt;NOT SUPPORTED&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Turing (T4, RTX 2000 series, ...)&lt;/td&gt; &#xA;   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:turing-1.1 (experimental)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ampere 80 (A100, A30)&lt;/td&gt; &#xA;   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ampere 86 (A10, A40, ...)&lt;/td&gt; &#xA;   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:86-1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ada Lovelace (RTX 4000 series, ...)&lt;/td&gt; &#xA;   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:89-1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hopper (H100)&lt;/td&gt; &#xA;   &lt;td&gt;ghcr.io/huggingface/text-embeddings-inference:hopper-1.1 (experimental)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Flash Attention is turned off by default for the Turing image as it suffers from precision issues. You can turn Flash Attention v1 ON by using the &lt;code&gt;USE_FLASH_ATTENTION=True&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h3&gt;API documentation&lt;/h3&gt; &#xA;&lt;p&gt;You can consult the OpenAPI documentation of the &lt;code&gt;text-embeddings-inference&lt;/code&gt; REST API using the &lt;code&gt;/docs&lt;/code&gt; route. The Swagger UI is also available at: &lt;a href=&#34;https://huggingface.github.io/text-embeddings-inference&#34;&gt;https://huggingface.github.io/text-embeddings-inference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using a private or gated model&lt;/h3&gt; &#xA;&lt;p&gt;You have the option to utilize the &lt;code&gt;HUGGING_FACE_HUB_TOKEN&lt;/code&gt; environment variable for configuring the token employed by &lt;code&gt;text-embeddings-inference&lt;/code&gt;. This allows you to gain access to protected resources.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy your cli READ token&lt;/li&gt; &#xA; &lt;li&gt;Export &lt;code&gt;HUGGING_FACE_HUB_TOKEN=&amp;lt;your cli READ token&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;or with Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=&amp;lt;your private model&amp;gt;&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;token=&amp;lt;your cli READ token&amp;gt;&#xA;&#xA;docker run --gpus all -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.1 --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Re-rankers models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; v0.4.0 added support for CamemBERT, RoBERTa and XLM-RoBERTa Sequence Classification models. Re-rankers models are Sequence Classification cross-encoders models with a single class that scores the similarity between a query and a text.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83&#34;&gt;this blogpost&lt;/a&gt; by the LlamaIndex team to understand how you can use re-rankers models in your RAG pipeline to improve downstream performance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=BAAI/bge-reranker-large&#xA;revision=refs/pr/4&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.1 --model-id $model --revision $revision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then you can rank the similarity between a query and a list of texts with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl 127.0.0.1:8080/rerank \&#xA;    -X POST \&#xA;    -d &#39;{&#34;query&#34;:&#34;What is Deep Learning?&#34;, &#34;texts&#34;: [&#34;Deep Learning is not...&#34;, &#34;Deep learning is...&#34;]}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Sequence Classification models&lt;/h3&gt; &#xA;&lt;p&gt;You can also use classic Sequence Classification models like &lt;code&gt;SamLowe/roberta-base-go_emotions&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=SamLowe/roberta-base-go_emotions&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.1 --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you have deployed the model you can use the &lt;code&gt;predict&lt;/code&gt; endpoint to get the emotions most associated with an input:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl 127.0.0.1:8080/predict \&#xA;    -X POST \&#xA;    -d &#39;{&#34;inputs&#34;:&#34;I like you.&#34;}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using SPLADE pooling&lt;/h3&gt; &#xA;&lt;p&gt;You can choose to activate SPLADE pooling for Bert and Distilbert MaskedLM architectures:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=naver/efficient-splade-VI-BT-large-query&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.1 --model-id $model --pooling splade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you have deployed the model you can use the &lt;code&gt;/embed_sparse&lt;/code&gt; endpoint to get the sparse embedding:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl 127.0.0.1:8080/embed_sparse \&#xA;    -X POST \&#xA;    -d &#39;{&#34;inputs&#34;:&#34;I like you.&#34;}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Distributed Tracing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; is instrumented with distributed tracing using OpenTelemetry. You can use this feature by setting the address to an OTLP collector with the &lt;code&gt;--otlp-endpoint&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h3&gt;gRPC&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;text-embeddings-inference&lt;/code&gt; offers a gRPC API as an alternative to the default HTTP API for high performance deployments. The API protobuf definition can be found &lt;a href=&#34;https://github.com/huggingface/text-embeddings-inference/raw/main/proto/tei.proto&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can use the gRPC API by adding the &lt;code&gt;-grpc&lt;/code&gt; tag to any TEI Docker image. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=BAAI/bge-large-en-v1.5&#xA;revision=refs/pr/5&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.1-grpc --model-id $model --revision $revision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;grpcurl -d &#39;{&#34;inputs&#34;: &#34;What is Deep Learning&#34;}&#39; -plaintext 0.0.0.0:8080 tei.v1.Embed/Embed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Local install&lt;/h2&gt; &#xA;&lt;h3&gt;CPU&lt;/h3&gt; &#xA;&lt;p&gt;You can also opt to install &lt;code&gt;text-embeddings-inference&lt;/code&gt; locally.&lt;/p&gt; &#xA;&lt;p&gt;First &lt;a href=&#34;https://rustup.rs/&#34;&gt;install Rust&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# On x86&#xA;cargo install --path router -F candle -F mkl&#xA;# On M1 or M2&#xA;cargo install --path router -F candle -F metal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now launch Text Embeddings Inference on CPU with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=BAAI/bge-large-en-v1.5&#xA;revision=refs/pr/5&#xA;&#xA;text-embeddings-router --model-id $model --revision $revision --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; on some machines, you may also need the OpenSSL libraries and gcc. On Linux machines, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install libssl-dev gcc -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Cuda&lt;/h3&gt; &#xA;&lt;p&gt;GPUs with Cuda compute capabilities &amp;lt; 7.5 are not supported (V100, Titan V, GTX 1000 series, ...).&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have Cuda and the nvidia drivers installed. NVIDIA drivers on your device need to be compatible with CUDA version 12.2 or higher. You also need to add the nvidia binaries to your path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export PATH=$PATH:/usr/local/cuda/bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# This can take a while as we need to compile a lot of cuda kernels&#xA;&#xA;# On Turing GPUs (T4, RTX 2000 series ... )&#xA;cargo install --path router -F candle-cuda-turing --no-default-features&#xA;&#xA;# On Ampere and Hopper&#xA;cargo install --path router -F candle-cuda --no-default-features&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now launch Text Embeddings Inference on GPU with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=BAAI/bge-large-en-v1.5&#xA;revision=refs/pr/5&#xA;&#xA;text-embeddings-router --model-id $model --revision $revision --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker build&lt;/h2&gt; &#xA;&lt;p&gt;You can build the CPU container with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build the Cuda containers, you need to know the compute cap of the GPU you will be using at runtime.&lt;/p&gt; &#xA;&lt;p&gt;Then you can build the container with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Example for Turing (T4, RTX 2000 series, ...)&#xA;runtime_compute_cap=75&#xA;&#xA;# Example for A100&#xA;runtime_compute_cap=80&#xA;&#xA;# Example for A10&#xA;runtime_compute_cap=86&#xA;&#xA;# Example for Ada Lovelace (RTX 4000 series, ...)&#xA;runtime_compute_cap=89&#xA;&#xA;# Example for H100&#xA;runtime_compute_cap=90&#xA;&#xA;docker build . -f Dockerfile-cuda --build-arg CUDA_COMPUTE_CAP=$runtime_compute_cap&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/learn/cookbook/automatic_embedding_tei_inference_endpoints&#34;&gt;Set up an Inference Endpoint with TEI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plaggy/rag-containers&#34;&gt;RAG containers with TEI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>rapiz1/rathole</title>
    <updated>2024-03-10T02:00:35Z</updated>
    <id>tag:github.com,2024-03-10:/rapiz1/rathole</id>
    <link href="https://github.com/rapiz1/rathole" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A lightweight and high-performance reverse proxy for NAT traversal, written in Rust. An alternative to frp and ngrok.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;rathole&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/img/rathole-logo.png&#34; alt=&#34;rathole-logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rapiz1/rathole/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rapiz1/rathole&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rapiz1/rathole/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/rapiz1/rathole&#34; alt=&#34;GitHub release (latest SemVer)&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/rapiz1/rathole/rust.yml?branch=main&#34; alt=&#34;GitHub Workflow Status (branch)&#34;&gt; &lt;a href=&#34;https://github.com/rapiz1/rathole/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/rapiz1/rathole/total&#34; alt=&#34;GitHub all releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/rapiz1/rathole&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/rapiz1/rathole&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/rapiz1/rathole?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/rapiz1/rathole.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/rapiz1/rathole&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/README-zh.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A secure, stable and high-performance reverse proxy for NAT traversal, written in Rust&lt;/p&gt; &#xA;&lt;p&gt;rathole, like &lt;a href=&#34;https://github.com/fatedier/frp&#34;&gt;frp&lt;/a&gt; and &lt;a href=&#34;https://github.com/inconshreveable/ngrok&#34;&gt;ngrok&lt;/a&gt;, can help to expose the service on the device behind the NAT to the Internet, via a server with a public IP.&lt;/p&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#rathole&#34;&gt;rathole&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#configuration&#34;&gt;Configuration&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#tuning&#34;&gt;Tuning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#planning&#34;&gt;Planning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt; Much higher throughput can be achieved than frp, and more stable when handling a large volume of connections. See &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low Resource Consumption&lt;/strong&gt; Consumes much fewer memory than similar tools. See &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/build-guide.md&#34;&gt;The binary can be&lt;/a&gt; &lt;strong&gt;as small as ~500KiB&lt;/strong&gt; to fit the constraints of devices, like embedded devices as routers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; Tokens of services are mandatory and service-wise. The server and clients are responsible for their own configs. With the optional Noise Protocol, encryption can be configured at ease. No need to create a self-signed certificate! TLS is also supported.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hot Reload&lt;/strong&gt; Services can be added or removed dynamically by hot-reloading the configuration file. HTTP API is WIP.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;A full-powered &lt;code&gt;rathole&lt;/code&gt; can be obtained from the &lt;a href=&#34;https://github.com/rapiz1/rathole/releases&#34;&gt;release&lt;/a&gt; page. Or &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/build-guide.md&#34;&gt;build from source&lt;/a&gt; &lt;strong&gt;for other platforms and minimizing the binary&lt;/strong&gt;. A &lt;a href=&#34;https://hub.docker.com/r/rapiz1/rathole&#34;&gt;Docker image&lt;/a&gt; is also available.&lt;/p&gt; &#xA;&lt;p&gt;The usage of &lt;code&gt;rathole&lt;/code&gt; is very similar to frp. If you have experience with the latter, then the configuration is very easy for you. The only difference is that configuration of a service is split into the client side and the server side, and a token is mandatory.&lt;/p&gt; &#xA;&lt;p&gt;To use &lt;code&gt;rathole&lt;/code&gt;, you need a server with a public IP, and a device behind the NAT, where some services that need to be exposed to the Internet.&lt;/p&gt; &#xA;&lt;p&gt;Assuming you have a NAS at home behind the NAT, and want to expose its ssh service to the Internet:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;On the server which has a public IP&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create &lt;code&gt;server.toml&lt;/code&gt; with the following content and accommodate it to your needs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# server.toml&#xA;[server]&#xA;bind_addr = &#34;0.0.0.0:2333&#34; # `2333` specifies the port that rathole listens for clients&#xA;&#xA;[server.services.my_nas_ssh]&#xA;token = &#34;use_a_secret_that_only_you_know&#34; # Token that is used to authenticate the client for the service. Change to an arbitrary value.&#xA;bind_addr = &#34;0.0.0.0:5202&#34; # `5202` specifies the port that exposes `my_nas_ssh` to the Internet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./rathole server.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;On the host which is behind the NAT (your NAS)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create &lt;code&gt;client.toml&lt;/code&gt; with the following content and accommodate it to your needs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# client.toml&#xA;[client]&#xA;remote_addr = &#34;myserver.com:2333&#34; # The address of the server. The port must be the same with the port in `server.bind_addr`&#xA;&#xA;[client.services.my_nas_ssh]&#xA;token = &#34;use_a_secret_that_only_you_know&#34; # Must be the same with the server to pass the validation&#xA;local_addr = &#34;127.0.0.1:22&#34; # The address of the service that needs to be forwarded&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./rathole client.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Now the client will try to connect to the server &lt;code&gt;myserver.com&lt;/code&gt; on port &lt;code&gt;2333&lt;/code&gt;, and any traffic to &lt;code&gt;myserver.com:5202&lt;/code&gt; will be forwarded to the client&#39;s port &lt;code&gt;22&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;So you can &lt;code&gt;ssh myserver.com:5202&lt;/code&gt; to ssh to your NAS.&lt;/p&gt; &#xA;&lt;p&gt;To run &lt;code&gt;rathole&lt;/code&gt; run as a background service on Linux, checkout the &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/examples/systemd&#34;&gt;systemd examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;rathole&lt;/code&gt; can automatically determine to run in the server mode or the client mode, according to the content of the configuration file, if only one of &lt;code&gt;[server]&lt;/code&gt; and &lt;code&gt;[client]&lt;/code&gt; block is present, like the example in &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;But the &lt;code&gt;[client]&lt;/code&gt; and &lt;code&gt;[server]&lt;/code&gt; block can also be put in one file. Then on the server side, run &lt;code&gt;rathole --server config.toml&lt;/code&gt; and on the client side, run &lt;code&gt;rathole --client config.toml&lt;/code&gt; to explicitly tell &lt;code&gt;rathole&lt;/code&gt; the running mode.&lt;/p&gt; &#xA;&lt;p&gt;Before heading to the full configuration specification, it&#39;s recommend to skim &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/examples&#34;&gt;the configuration examples&lt;/a&gt; to get a feeling of the configuration format.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/transport.md&#34;&gt;Transport&lt;/a&gt; for more details about encryption and the &lt;code&gt;transport&lt;/code&gt; block.&lt;/p&gt; &#xA;&lt;p&gt;Here is the full configuration specification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[client]&#xA;remote_addr = &#34;example.com:2333&#34; # Necessary. The address of the server&#xA;default_token = &#34;default_token_if_not_specify&#34; # Optional. The default token of services, if they don&#39;t define their own ones&#xA;heartbeat_timeout = 40 # Optional. Set to 0 to disable the application-layer heartbeat test. The value must be greater than `server.heartbeat_interval`. Default: 40 seconds&#xA;retry_interval = 1 # Optional. The interval between retry to connect to the server. Default: 1 second&#xA;&#xA;[client.transport] # The whole block is optional. Specify which transport to use&#xA;type = &#34;tcp&#34; # Optional. Possible values: [&#34;tcp&#34;, &#34;tls&#34;, &#34;noise&#34;]. Default: &#34;tcp&#34;&#xA;&#xA;[client.transport.tcp] # Optional. Also affects `noise` and `tls`&#xA;proxy = &#34;socks5://user:passwd@127.0.0.1:1080&#34; # Optional. The proxy used to connect to the server. `http` and `socks5` is supported.&#xA;nodelay = true # Optional. Determine whether to enable TCP_NODELAY, if applicable, to improve the latency but decrease the bandwidth. Default: true&#xA;keepalive_secs = 20 # Optional. Specify `tcp_keepalive_time` in `tcp(7)`, if applicable. Default: 20 seconds&#xA;keepalive_interval = 8 # Optional. Specify `tcp_keepalive_intvl` in `tcp(7)`, if applicable. Default: 8 seconds&#xA;&#xA;[client.transport.tls] # Necessary if `type` is &#34;tls&#34;&#xA;trusted_root = &#34;ca.pem&#34; # Necessary. The certificate of CA that signed the server&#39;s certificate&#xA;hostname = &#34;example.com&#34; # Optional. The hostname that the client uses to validate the certificate. If not set, fallback to `client.remote_addr`&#xA;&#xA;[client.transport.noise] # Noise protocol. See `docs/transport.md` for further explanation&#xA;pattern = &#34;Noise_NK_25519_ChaChaPoly_BLAKE2s&#34; # Optional. Default value as shown&#xA;local_private_key = &#34;key_encoded_in_base64&#34; # Optional&#xA;remote_public_key = &#34;key_encoded_in_base64&#34; # Optional&#xA;&#xA;[client.transport.websocket] # Necessary if `type` is &#34;websocket&#34;&#xA;tls = true # If `true` then it will use settings in `client.transport.tls`&#xA;&#xA;[client.services.service1] # A service that needs forwarding. The name `service1` can change arbitrarily, as long as identical to the name in the server&#39;s configuration&#xA;type = &#34;tcp&#34; # Optional. The protocol that needs forwarding. Possible values: [&#34;tcp&#34;, &#34;udp&#34;]. Default: &#34;tcp&#34;&#xA;token = &#34;whatever&#34; # Necessary if `client.default_token` not set&#xA;local_addr = &#34;127.0.0.1:1081&#34; # Necessary. The address of the service that needs to be forwarded&#xA;nodelay = true # Optional. Override the `client.transport.nodelay` per service&#xA;retry_interval = 1 # Optional. The interval between retry to connect to the server. Default: inherits the global config&#xA;&#xA;[client.services.service2] # Multiple services can be defined&#xA;local_addr = &#34;127.0.0.1:1082&#34;&#xA;&#xA;[server]&#xA;bind_addr = &#34;0.0.0.0:2333&#34; # Necessary. The address that the server listens for clients. Generally only the port needs to be change.&#xA;default_token = &#34;default_token_if_not_specify&#34; # Optional&#xA;heartbeat_interval = 30 # Optional. The interval between two application-layer heartbeat. Set to 0 to disable sending heartbeat. Default: 30 seconds&#xA;&#xA;[server.transport] # Same as `[client.transport]`&#xA;type = &#34;tcp&#34;&#xA;&#xA;[server.transport.tcp] # Same as the client&#xA;nodelay = true&#xA;keepalive_secs = 20&#xA;keepalive_interval = 8&#xA;&#xA;[server.transport.tls] # Necessary if `type` is &#34;tls&#34;&#xA;pkcs12 = &#34;identify.pfx&#34; # Necessary. pkcs12 file of server&#39;s certificate and private key&#xA;pkcs12_password = &#34;password&#34; # Necessary. Password of the pkcs12 file&#xA;&#xA;[server.transport.noise] # Same as `[client.transport.noise]`&#xA;pattern = &#34;Noise_NK_25519_ChaChaPoly_BLAKE2s&#34;&#xA;local_private_key = &#34;key_encoded_in_base64&#34;&#xA;remote_public_key = &#34;key_encoded_in_base64&#34;&#xA;&#xA;[server.transport.websocket] # Necessary if `type` is &#34;websocket&#34;&#xA;tls = true # If `true` then it will use settings in `server.transport.tls`&#xA;&#xA;[server.services.service1] # The service name must be identical to the client side&#xA;type = &#34;tcp&#34; # Optional. Same as the client `[client.services.X.type]&#xA;token = &#34;whatever&#34; # Necessary if `server.default_token` not set&#xA;bind_addr = &#34;0.0.0.0:8081&#34; # Necessary. The address of the service is exposed at. Generally only the port needs to be change.&#xA;nodelay = true # Optional. Same as the client&#xA;&#xA;[server.services.service2]&#xA;bind_addr = &#34;0.0.0.1:8082&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;rathole&lt;/code&gt;, like many other Rust programs, use environment variables to control the logging level. &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;warn&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;, &lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;trace&lt;/code&gt; are available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;RUST_LOG=error ./rathole config.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run &lt;code&gt;rathole&lt;/code&gt; with only error level logging.&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;RUST_LOG&lt;/code&gt; is not present, the default logging level is &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Tuning&lt;/h3&gt; &#xA;&lt;p&gt;From v0.4.7, rathole enables TCP_NODELAY by default, which should benefit the latency and interactive applications like rdp, Minecraft servers. However, it slightly decreases the bandwidth.&lt;/p&gt; &#xA;&lt;p&gt;If the bandwidth is more important, TCP_NODELAY can be opted out with &lt;code&gt;nodelay = false&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;rathole has similar latency to &lt;a href=&#34;https://github.com/fatedier/frp&#34;&gt;frp&lt;/a&gt;, but can handle a more connections, provide larger bandwidth, with less memory usage.&lt;/p&gt; &#xA;&lt;p&gt;For more details, see the separate page &lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/benchmark.md&#34;&gt;Benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;However, don&#39;t take it from here that &lt;code&gt;rathole&lt;/code&gt; can magically make your forwarded service faster several times than before.&lt;/strong&gt; The benchmark is done on local loopback, indicating the performance when the task is cpu-bounded. One can gain quite a improvement if the network is not the bottleneck. Unfortunately, that&#39;s not true for many users. In that case, the main benefit is lower resource consumption, while the bandwidth and the latency may not improved significantly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/img/http_throughput.svg?sanitize=true&#34; alt=&#34;http_throughput&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/img/tcp_bitrate.svg?sanitize=true&#34; alt=&#34;tcp_bitrate&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/img/udp_bitrate.svg?sanitize=true&#34; alt=&#34;udp_bitrate&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/img/mem-graph.png&#34; alt=&#34;mem&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Planning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; HTTP APIs for configuration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rapiz1/rathole/main/docs/out-of-scope.md&#34;&gt;Out of Scope&lt;/a&gt; lists features that are not planned to be implemented and why.&lt;/p&gt;</summary>
  </entry>
</feed>