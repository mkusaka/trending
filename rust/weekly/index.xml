<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-10T06:08:44Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cisagov/thorium</title>
    <updated>2025-08-10T06:08:44Z</updated>
    <id>tag:github.com,2025-08-10:/cisagov/thorium</id>
    <link href="https://github.com/cisagov/thorium" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A scalable file analysis and data generation platform that allows users to easily orchestrate arbitrary docker/vm/shell tools at scale.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./api/docs/src/static_resources/logo_dark.svg&#34; /&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;./api/docs/src/static_resources/logo_light.svg&#34; /&gt; &#xA;  &lt;img alt=&#34;Thorium&#34; src=&#34;https://raw.githubusercontent.com/cisagov/thorium/main/api/docs/src/static_resources/logo_dark.svg?sanitize=true&#34; width=&#34;50%&#34; /&gt; &#xA; &lt;/picture&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://cisagov.github.io/thorium/intro.html&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://cisagov.github.io/thorium/admins/deploy/deploy.html&#34;&gt;Install Documentation&lt;/a&gt; | &lt;a href=&#34;https://github.com/cisagov/thorium/raw/main/minithor/README.md&#34;&gt;Single Node Install Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;A scalable file analysis and data generation platform that allows users to easily orchestrate arbitrary docker/vm/shell tools at scale.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;hr /&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Highly scalable analysis of arbitrary files/repos&lt;/li&gt; &#xA; &lt;li&gt;Near zero-cost analysis tool integration&lt;/li&gt; &#xA; &lt;li&gt;Static and dynamic analysis sandboxes&lt;/li&gt; &#xA; &lt;li&gt;User friendly interfaces: GUI + CLI&lt;/li&gt; &#xA; &lt;li&gt;RESTful API for automated access to data&lt;/li&gt; &#xA; &lt;li&gt;Multi-tenant friendly permission system&lt;/li&gt; &#xA; &lt;li&gt;Full-text search of analysis results&lt;/li&gt; &#xA; &lt;li&gt;Key/Value tags for labeling data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;FAQ&lt;/h3&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h5&gt;What Does Thorium actually do?&lt;/h5&gt; &#xA;&lt;p&gt;Thorium allows for developers or analysts to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easily scale up and orchestrate docker or baremetal based tools&lt;/li&gt; &#xA; &lt;li&gt;Easily search and use results from tools&lt;/li&gt; &#xA; &lt;li&gt;Comment and share files, tags, and results between users/analysts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Are any tools included?&lt;/h5&gt; &#xA;&lt;p&gt;Currently no there are no tools included in Thorium but we plan on releasing some soon. We hope to continue to release tools as well as allow for curated contributions from the community.&lt;/p&gt; &#xA;&lt;h5&gt;What do I need to deploy Thorium?&lt;/h5&gt; &#xA;&lt;p&gt;Thorium was built and intended to run in a K8s cluster but it can also run on a laptop using minikube. However it is important to note the single node deployment was not intended for production and reliability/stability may be less then stellar. To deploy a single node instance follow the Minithor docs &lt;a href=&#34;https://github.com/cisagov/thorium/raw/main/minithor/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For full production deployment on a cluster you will need the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Block store provider&lt;/li&gt; &#xA; &lt;li&gt;S3 storage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For on prem deployments we recommend CEPH. To deploy Thorium on a cluster follow the docs &lt;a href=&#34;https://cisagov.github.io/thorium/admins/deploy/deploy.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;How scalable is Thorium?&lt;/h5&gt; &#xA;&lt;p&gt;Thorium is almost infinitely scalable if you have the compute/storage. Thorium has been tested to support billions of samples and utilization of large amounts of compute. Thorium partitions data temporally and allows admins to configure how scalable they want the system to be.&lt;/p&gt; &#xA;&lt;h5&gt;Does Thorium call home or send any telemetry out?&lt;/h5&gt; &#xA;&lt;p&gt;No, Thorium does not send any telemetry out.&lt;/p&gt; &#xA;&lt;h5&gt;How large of files/repos does Thorium support?&lt;/h5&gt; &#xA;&lt;p&gt;Currently, Thorium supports up to ~50 GiB per file/repo. This is after compression and so it is a fuzzy limit but it should never be significantly smaller then 50 GiB. We do have plans to increase this limit in the future please create an issue if you feel this would be useful.&lt;/p&gt; &#xA;&lt;h5&gt;Can you add more documentation and examples of using Thorium and what it can do?&lt;/h5&gt; &#xA;&lt;p&gt;Yes, we plan to continue to expand our documentation and examples soon.&lt;/p&gt; &#xA;&lt;h3&gt;Funded by&lt;/h3&gt; &#xA;&lt;hr /&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./api/docs/src/static_resources/funders_dark.png&#34; /&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;./api/docs/src/static_resources/funders.png&#34; /&gt; &#xA;  &lt;img alt=&#34;Thorium&#34; src=&#34;https://raw.githubusercontent.com/cisagov/thorium/main/api/docs/src/static_resources/funders_dark.png&#34; &lt; picture /&gt; &#xA; &lt;/picture&gt;&#xA;&lt;/div&gt; &#xA;&lt;p&gt;Contact us at &lt;a href=&#34;mailto:thorium@sandia.gov&#34;&gt;thorium@sandia.gov&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rathole-org/rathole</title>
    <updated>2025-08-10T06:08:44Z</updated>
    <id>tag:github.com,2025-08-10:/rathole-org/rathole</id>
    <link href="https://github.com/rathole-org/rathole" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A lightweight and high-performance reverse proxy for NAT traversal, written in Rust. An alternative to frp and ngrok.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;rathole&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/rathole-logo.png&#34; alt=&#34;rathole-logo&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rapiz1/rathole/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rapiz1/rathole&#34; alt=&#34;GitHub stars&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rapiz1/rathole/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/rapiz1/rathole&#34; alt=&#34;GitHub release (latest SemVer)&#34; /&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/rapiz1/rathole/rust.yml?branch=main&#34; alt=&#34;GitHub Workflow Status (branch)&#34; /&gt; &lt;a href=&#34;https://github.com/rapiz1/rathole/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/rapiz1/rathole/total&#34; alt=&#34;GitHub all releases&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/rapiz1/rathole&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/rapiz1/rathole&#34; alt=&#34;Docker Pulls&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/rapiz1/rathole?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/rapiz1/rathole.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/rapiz1/rathole&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/README-zh.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A secure, stable and high-performance reverse proxy for NAT traversal, written in Rust&lt;/p&gt; &#xA;&lt;p&gt;rathole, like &lt;a href=&#34;https://github.com/fatedier/frp&#34;&gt;frp&lt;/a&gt; and &lt;a href=&#34;https://github.com/inconshreveable/ngrok&#34;&gt;ngrok&lt;/a&gt;, can help to expose the service on the device behind the NAT to the Internet, via a server with a public IP.&lt;/p&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#rathole&#34;&gt;rathole&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#configuration&#34;&gt;Configuration&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#tuning&#34;&gt;Tuning&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#planning&#34;&gt;Planning&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt; Much higher throughput can be achieved than frp, and more stable when handling a large volume of connections. See &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low Resource Consumption&lt;/strong&gt; Consumes much fewer memory than similar tools. See &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/build-guide.md&#34;&gt;The binary can be&lt;/a&gt; &lt;strong&gt;as small as ~500KiB&lt;/strong&gt; to fit the constraints of devices, like embedded devices as routers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; Tokens of services are mandatory and service-wise. The server and clients are responsible for their own configs. With the optional Noise Protocol, encryption can be configured at ease. No need to create a self-signed certificate! TLS is also supported.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hot Reload&lt;/strong&gt; Services can be added or removed dynamically by hot-reloading the configuration file. HTTP API is WIP.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;A full-powered &lt;code&gt;rathole&lt;/code&gt; can be obtained from the &lt;a href=&#34;https://github.com/rapiz1/rathole/releases&#34;&gt;release&lt;/a&gt; page. Or &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/build-guide.md&#34;&gt;build from source&lt;/a&gt; &lt;strong&gt;for other platforms and minimizing the binary&lt;/strong&gt;. A &lt;a href=&#34;https://hub.docker.com/r/rapiz1/rathole&#34;&gt;Docker image&lt;/a&gt; is also available.&lt;/p&gt; &#xA;&lt;p&gt;The usage of &lt;code&gt;rathole&lt;/code&gt; is very similar to frp. If you have experience with the latter, then the configuration is very easy for you. The only difference is that configuration of a service is split into the client side and the server side, and a token is mandatory.&lt;/p&gt; &#xA;&lt;p&gt;To use &lt;code&gt;rathole&lt;/code&gt;, you need a server with a public IP, and a device behind the NAT, where some services that need to be exposed to the Internet.&lt;/p&gt; &#xA;&lt;p&gt;Assuming you have a NAS at home behind the NAT, and want to expose its ssh service to the Internet:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;On the server which has a public IP&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create &lt;code&gt;server.toml&lt;/code&gt; with the following content and accommodate it to your needs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# server.toml&#xA;[server]&#xA;bind_addr = &#34;0.0.0.0:2333&#34; # `2333` specifies the port that rathole listens for clients&#xA;&#xA;[server.services.my_nas_ssh]&#xA;token = &#34;use_a_secret_that_only_you_know&#34; # Token that is used to authenticate the client for the service. Change to an arbitrary value.&#xA;bind_addr = &#34;0.0.0.0:5202&#34; # `5202` specifies the port that exposes `my_nas_ssh` to the Internet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./rathole server.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;On the host which is behind the NAT (your NAS)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create &lt;code&gt;client.toml&lt;/code&gt; with the following content and accommodate it to your needs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# client.toml&#xA;[client]&#xA;remote_addr = &#34;myserver.com:2333&#34; # The address of the server. The port must be the same with the port in `server.bind_addr`&#xA;&#xA;[client.services.my_nas_ssh]&#xA;token = &#34;use_a_secret_that_only_you_know&#34; # Must be the same with the server to pass the validation&#xA;local_addr = &#34;127.0.0.1:22&#34; # The address of the service that needs to be forwarded&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./rathole client.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Now the client will try to connect to the server &lt;code&gt;myserver.com&lt;/code&gt; on port &lt;code&gt;2333&lt;/code&gt;, and any traffic to &lt;code&gt;myserver.com:5202&lt;/code&gt; will be forwarded to the client&#39;s port &lt;code&gt;22&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;So you can &lt;code&gt;ssh myserver.com:5202&lt;/code&gt; to ssh to your NAS.&lt;/p&gt; &#xA;&lt;p&gt;To run &lt;code&gt;rathole&lt;/code&gt; run as a background service on Linux, checkout the &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/examples/systemd&#34;&gt;systemd examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;rathole&lt;/code&gt; can automatically determine to run in the server mode or the client mode, according to the content of the configuration file, if only one of &lt;code&gt;[server]&lt;/code&gt; and &lt;code&gt;[client]&lt;/code&gt; block is present, like the example in &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;But the &lt;code&gt;[client]&lt;/code&gt; and &lt;code&gt;[server]&lt;/code&gt; block can also be put in one file. Then on the server side, run &lt;code&gt;rathole --server config.toml&lt;/code&gt; and on the client side, run &lt;code&gt;rathole --client config.toml&lt;/code&gt; to explicitly tell &lt;code&gt;rathole&lt;/code&gt; the running mode.&lt;/p&gt; &#xA;&lt;p&gt;Before heading to the full configuration specification, it&#39;s recommend to skim &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/examples&#34;&gt;the configuration examples&lt;/a&gt; to get a feeling of the configuration format.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/transport.md&#34;&gt;Transport&lt;/a&gt; for more details about encryption and the &lt;code&gt;transport&lt;/code&gt; block.&lt;/p&gt; &#xA;&lt;p&gt;Here is the full configuration specification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[client]&#xA;remote_addr = &#34;example.com:2333&#34; # Necessary. The address of the server&#xA;default_token = &#34;default_token_if_not_specify&#34; # Optional. The default token of services, if they don&#39;t define their own ones&#xA;heartbeat_timeout = 40 # Optional. Set to 0 to disable the application-layer heartbeat test. The value must be greater than `server.heartbeat_interval`. Default: 40 seconds&#xA;retry_interval = 1 # Optional. The interval between retry to connect to the server. Default: 1 second&#xA;&#xA;[client.transport] # The whole block is optional. Specify which transport to use&#xA;type = &#34;tcp&#34; # Optional. Possible values: [&#34;tcp&#34;, &#34;tls&#34;, &#34;noise&#34;]. Default: &#34;tcp&#34;&#xA;&#xA;[client.transport.tcp] # Optional. Also affects `noise` and `tls`&#xA;proxy = &#34;socks5://user:passwd@127.0.0.1:1080&#34; # Optional. The proxy used to connect to the server. `http` and `socks5` is supported.&#xA;nodelay = true # Optional. Determine whether to enable TCP_NODELAY, if applicable, to improve the latency but decrease the bandwidth. Default: true&#xA;keepalive_secs = 20 # Optional. Specify `tcp_keepalive_time` in `tcp(7)`, if applicable. Default: 20 seconds&#xA;keepalive_interval = 8 # Optional. Specify `tcp_keepalive_intvl` in `tcp(7)`, if applicable. Default: 8 seconds&#xA;&#xA;[client.transport.tls] # Necessary if `type` is &#34;tls&#34;&#xA;trusted_root = &#34;ca.pem&#34; # Necessary. The certificate of CA that signed the server&#39;s certificate&#xA;hostname = &#34;example.com&#34; # Optional. The hostname that the client uses to validate the certificate. If not set, fallback to `client.remote_addr`&#xA;&#xA;[client.transport.noise] # Noise protocol. See `docs/transport.md` for further explanation&#xA;pattern = &#34;Noise_NK_25519_ChaChaPoly_BLAKE2s&#34; # Optional. Default value as shown&#xA;local_private_key = &#34;key_encoded_in_base64&#34; # Optional&#xA;remote_public_key = &#34;key_encoded_in_base64&#34; # Optional&#xA;&#xA;[client.transport.websocket] # Necessary if `type` is &#34;websocket&#34;&#xA;tls = true # If `true` then it will use settings in `client.transport.tls`&#xA;&#xA;[client.services.service1] # A service that needs forwarding. The name `service1` can change arbitrarily, as long as identical to the name in the server&#39;s configuration&#xA;type = &#34;tcp&#34; # Optional. The protocol that needs forwarding. Possible values: [&#34;tcp&#34;, &#34;udp&#34;]. Default: &#34;tcp&#34;&#xA;token = &#34;whatever&#34; # Necessary if `client.default_token` not set&#xA;local_addr = &#34;127.0.0.1:1081&#34; # Necessary. The address of the service that needs to be forwarded&#xA;nodelay = true # Optional. Override the `client.transport.nodelay` per service&#xA;retry_interval = 1 # Optional. The interval between retry to connect to the server. Default: inherits the global config&#xA;&#xA;[client.services.service2] # Multiple services can be defined&#xA;local_addr = &#34;127.0.0.1:1082&#34;&#xA;&#xA;[server]&#xA;bind_addr = &#34;0.0.0.0:2333&#34; # Necessary. The address that the server listens for clients. Generally only the port needs to be change.&#xA;default_token = &#34;default_token_if_not_specify&#34; # Optional&#xA;heartbeat_interval = 30 # Optional. The interval between two application-layer heartbeat. Set to 0 to disable sending heartbeat. Default: 30 seconds&#xA;&#xA;[server.transport] # Same as `[client.transport]`&#xA;type = &#34;tcp&#34;&#xA;&#xA;[server.transport.tcp] # Same as the client&#xA;nodelay = true&#xA;keepalive_secs = 20&#xA;keepalive_interval = 8&#xA;&#xA;[server.transport.tls] # Necessary if `type` is &#34;tls&#34;&#xA;pkcs12 = &#34;identify.pfx&#34; # Necessary. pkcs12 file of server&#39;s certificate and private key&#xA;pkcs12_password = &#34;password&#34; # Necessary. Password of the pkcs12 file&#xA;&#xA;[server.transport.noise] # Same as `[client.transport.noise]`&#xA;pattern = &#34;Noise_NK_25519_ChaChaPoly_BLAKE2s&#34;&#xA;local_private_key = &#34;key_encoded_in_base64&#34;&#xA;remote_public_key = &#34;key_encoded_in_base64&#34;&#xA;&#xA;[server.transport.websocket] # Necessary if `type` is &#34;websocket&#34;&#xA;tls = true # If `true` then it will use settings in `server.transport.tls`&#xA;&#xA;[server.services.service1] # The service name must be identical to the client side&#xA;type = &#34;tcp&#34; # Optional. Same as the client `[client.services.X.type]&#xA;token = &#34;whatever&#34; # Necessary if `server.default_token` not set&#xA;bind_addr = &#34;0.0.0.0:8081&#34; # Necessary. The address of the service is exposed at. Generally only the port needs to be change.&#xA;nodelay = true # Optional. Same as the client&#xA;&#xA;[server.services.service2]&#xA;bind_addr = &#34;0.0.0.1:8082&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;rathole&lt;/code&gt;, like many other Rust programs, use environment variables to control the logging level. &lt;code&gt;info&lt;/code&gt;, &lt;code&gt;warn&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt;, &lt;code&gt;debug&lt;/code&gt;, &lt;code&gt;trace&lt;/code&gt; are available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;RUST_LOG=error ./rathole config.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run &lt;code&gt;rathole&lt;/code&gt; with only error level logging.&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;RUST_LOG&lt;/code&gt; is not present, the default logging level is &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Tuning&lt;/h3&gt; &#xA;&lt;p&gt;From v0.4.7, rathole enables TCP_NODELAY by default, which should benefit the latency and interactive applications like rdp, Minecraft servers. However, it slightly decreases the bandwidth.&lt;/p&gt; &#xA;&lt;p&gt;If the bandwidth is more important, TCP_NODELAY can be opted out with &lt;code&gt;nodelay = false&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;rathole has similar latency to &lt;a href=&#34;https://github.com/fatedier/frp&#34;&gt;frp&lt;/a&gt;, but can handle a more connections, provide larger bandwidth, with less memory usage.&lt;/p&gt; &#xA;&lt;p&gt;For more details, see the separate page &lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/benchmark.md&#34;&gt;Benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;However, don&#39;t take it from here that &lt;code&gt;rathole&lt;/code&gt; can magically make your forwarded service faster several times than before.&lt;/strong&gt; The benchmark is done on local loopback, indicating the performance when the task is cpu-bounded. One can gain quite a improvement if the network is not the bottleneck. Unfortunately, that&#39;s not true for many users. In that case, the main benefit is lower resource consumption, while the bandwidth and the latency may not improved significantly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/http_throughput.svg?sanitize=true&#34; alt=&#34;http_throughput&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/tcp_bitrate.svg?sanitize=true&#34; alt=&#34;tcp_bitrate&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/udp_bitrate.svg?sanitize=true&#34; alt=&#34;udp_bitrate&#34; /&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/img/mem-graph.png&#34; alt=&#34;mem&#34; /&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Planning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled /&gt; HTTP APIs for configuration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rathole-org/rathole/main/docs/out-of-scope.md&#34;&gt;Out of Scope&lt;/a&gt; lists features that are not planned to be implemented and why.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/codex</title>
    <updated>2025-08-10T06:08:44Z</updated>
    <id>tag:github.com,2025-08-10:/openai/codex</id>
    <link href="https://github.com/openai/codex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;OpenAI Codex CLI&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.&lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, see &lt;a href=&#34;https://chatgpt.com/codex&#34;&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png&#34; alt=&#34;Codex CLI splash&#34; width=&#34;50%&#34; /&gt; &lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;!-- Begin ToC --&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#installing-and-running-codex-cli&#34;&gt;Installing and running Codex CLI&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#using-codex-with-your-chatgpt-plan&#34;&gt;Using Codex with your ChatGPT plan&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#connecting-through-vps-or-remote&#34;&gt;Connecting through VPS or remote&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#usage-based-billing-alternative-use-an-openai-api-key&#34;&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#choosing-codexs-level-of-autonomy&#34;&gt;Choosing Codex&#39;s level of autonomy&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#1-readwrite&#34;&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#2-read-only&#34;&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#3-advanced-configuration&#34;&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#can-i-run-without-any-approvals&#34;&gt;Can I run without ANY approvals?&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#fine-tuning-in-configtoml&#34;&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#example-prompts&#34;&gt;Example prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#running-with-a-prompt-as-input&#34;&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#using-open-source-models&#34;&gt;Using Open Source Models&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#platform-sandboxing-details&#34;&gt;Platform sandboxing details&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#experimental-technology-disclaimer&#34;&gt;Experimental technology disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#system-requirements&#34;&gt;System requirements&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#cli-reference&#34;&gt;CLI reference&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#memory--project-docs&#34;&gt;Memory &amp;amp; project docs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#non-interactive--ci-mode&#34;&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#model-context-protocol-mcp&#34;&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#tracing--verbose-logging&#34;&gt;Tracing / verbose logging&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#dotslash&#34;&gt;DotSlash&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#zero-data-retention-zdr-usage&#34;&gt;Zero data retention (ZDR) usage&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#codex-open-source-fund&#34;&gt;Codex open source fund&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#contributing&#34;&gt;Contributing&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#development-workflow&#34;&gt;Development workflow&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#writing-high-impact-code-changes&#34;&gt;Writing high-impact code changes&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#opening-a-pull-request&#34;&gt;Opening a pull request&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#review-process&#34;&gt;Review process&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#community-values&#34;&gt;Community values&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#getting-help&#34;&gt;Getting help&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#contributor-license-agreement-cla&#34;&gt;Contributor license agreement (CLA)&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#quick-fixes&#34;&gt;Quick fixes&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#releasing-codex&#34;&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#security--responsible-ai&#34;&gt;Security &amp;amp; responsible AI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;!-- End ToC --&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; &#xA;&lt;p&gt;Install globally with your preferred package manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install -g @openai/codex  # Alternatively: `brew install codex`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;codex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;You can also go to the &lt;a href=&#34;https://github.com/openai/codex/releases/latest&#34;&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; &#xA; &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;macOS &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Linux &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png&#34; alt=&#34;Codex CLI login&#34; width=&#34;50%&#34; /&gt; &lt;/p&gt; &#xA;&lt;p&gt;After you run &lt;code&gt;codex&lt;/code&gt; select Sign in with ChatGPT. You&#39;ll need a Plus, Pro, or Team ChatGPT account, and will get access to our latest models, including &lt;code&gt;gpt-5&lt;/code&gt;, at no extra cost to your plan. (Enterprise is coming soon.)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Important: If you&#39;ve used the Codex CLI before, you&#39;ll need to follow these steps to migrate from usage-based billing with your API key:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Update the CLI with &lt;code&gt;codex update&lt;/code&gt; and ensure &lt;code&gt;codex --version&lt;/code&gt; is greater than 0.13&lt;/li&gt; &#xA;  &lt;li&gt;Ensure that there is no &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable set. (Check that &lt;code&gt;env | grep &#39;OPENAI_API_KEY&#39;&lt;/code&gt; returns empty)&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;codex login&lt;/code&gt; again&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you encounter problems with the login flow, please comment on &lt;a href=&#34;https://github.com/openai/codex/issues/1243&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Connecting through VPS or remote&lt;/h3&gt; &#xA;&lt;p&gt;If you run Codex on a remote machine (VPS/server) without a local browser, the login helper starts a server on &lt;code&gt;localhost:1455&lt;/code&gt; on the remote host. To complete login in your local browser, forward that port to your machine before starting the login flow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From your local machine&#xA;ssh -L 1455:localhost:1455 &amp;lt;user&amp;gt;@&amp;lt;remote-host&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, in that SSH session, run &lt;code&gt;codex&lt;/code&gt; and select &#34;Sign in with ChatGPT&#34;. When prompted, open the printed URL (it will be &lt;code&gt;http://localhost:1455/...&lt;/code&gt;) in your local browser. The traffic will be tunneled to the remote server.&lt;/p&gt; &#xA;&lt;h3&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/h3&gt; &#xA;&lt;p&gt;If you prefer to pay-as-you-go, you can still authenticate with your OpenAI API key by setting it as an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export OPENAI_API_KEY=&#34;your-api-key-here&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This command only sets the key for your current terminal session, which we recommend. To set it for all future sessions, you can also add the &lt;code&gt;export&lt;/code&gt; line to your shell&#39;s configuration file (e.g., &lt;code&gt;~/.zshrc&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;If you have signed in with ChatGPT, Codex will default to using your ChatGPT credits. If you wish to use your API key, use the &lt;code&gt;/logout&lt;/code&gt; command to clear your ChatGPT authentication.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Choosing Codex&#39;s level of autonomy&lt;/h3&gt; &#xA;&lt;p&gt;We always recommend running Codex in its default sandbox that gives you strong guardrails around what the agent can do. The default sandbox prevents it from editing files outside its workspace, or from accessing the network.&lt;/p&gt; &#xA;&lt;p&gt;When you launch Codex in a new folder, it detects whether the folder is version controlled and recommends one of two levels of autonomy:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Codex can run commands and write files in the workspace without approval.&lt;/li&gt; &#xA; &lt;li&gt;To write files in other folders, access network, update git or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; &#xA; &lt;li&gt;By default, the workspace includes the current directory, as well as temporary directories like &lt;code&gt;/tmp&lt;/code&gt;. You can see what directories are in the workspace with the &lt;code&gt;/status&lt;/code&gt; command. See the docs for how to customize this behavior.&lt;/li&gt; &#xA; &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox workspace-write --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;This is the recommended default for version-controlled folders.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Codex can run read-only commands without approval.&lt;/li&gt; &#xA; &lt;li&gt;To edit files, access network, or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; &#xA; &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox read-only --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;This is the recommended default non-version-controlled folders.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Codex gives you fine-grained control over the sandbox with the &lt;code&gt;--sandbox&lt;/code&gt; option, and over when it requests approval with the &lt;code&gt;--ask-for-approval&lt;/code&gt; option. Run &lt;code&gt;codex help&lt;/code&gt; for more on these options.&lt;/p&gt; &#xA;&lt;h4&gt;Can I run without ANY approvals?&lt;/h4&gt; &#xA;&lt;p&gt;Yes, run codex non-interactively with &lt;code&gt;--ask-for-approval never&lt;/code&gt;. This option works with all &lt;code&gt;--sandbox&lt;/code&gt; options, so you still have full control over Codex&#39;s level of autonomy. It will make its best attempt with whatever contrainsts you provide. For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox read-only&lt;/code&gt; when you are running many agents to answer questions in parallel in the same workspace.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox workspace-write&lt;/code&gt; when you want the agent to non-interactively take time to produce the best outcome, with strong guardrails around its behavior.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox danger-full-access&lt;/code&gt; to dangerously give the agent full autonomy. Because this disables important safety mechanisms, we recommend against using this unless running Codex in an isolated environment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# approval mode&#xA;approval_policy = &#34;untrusted&#34;&#xA;sandbox_mode    = &#34;read-only&#34;&#xA;&#xA;# full-auto mode&#xA;approval_policy = &#34;on-request&#34;&#xA;sandbox_mode    = &#34;workspace-write&#34;&#xA;&#xA;# Optional: allow network in workspace-write mode&#xA;[sandbox_workspace_write]&#xA;network_access = true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also save presets as &lt;strong&gt;profiles&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[profiles.full_auto]&#xA;approval_policy = &#34;on-request&#34;&#xA;sandbox_mode    = &#34;workspace-write&#34;&#xA;&#xA;[profiles.readonly_quiet]&#xA;approval_policy = &#34;never&#34;&#xA;sandbox_mode    = &#34;read-only&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example prompts&lt;/h3&gt; &#xA;&lt;p&gt;Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the &lt;a href=&#34;https://github.com/openai/codex/raw/main/codex-cli/examples/prompting_guide.md&#34;&gt;prompting guide&lt;/a&gt; for more tips and usage patterns.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;✨&lt;/th&gt; &#xA;   &lt;th&gt;What you type&lt;/th&gt; &#xA;   &lt;th&gt;What happens&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Refactor the Dashboard component to React Hooks&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Codex rewrites the class component, runs &lt;code&gt;npm test&lt;/code&gt;, and shows the diff.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Generate SQL migrations for adding a users table&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Infers your ORM, creates migration files, and runs them in a sandboxed DB.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Write unit tests for utils/date.ts&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generates tests, executes them, and iterates until they pass.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Bulk-rename *.jpeg -&amp;gt; *.jpg with git mv&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Safely renames files and updates imports/usages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Explain what this regex does: ^(?=.*[A-Z]).{8,}$&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Outputs a step-by-step human explanation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Carefully review this repo, and propose 3 high impact well-scoped PRs&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Suggests impactful PRs in the current codebase.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;Look for vulnerabilities and create a security review report&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Finds and explains security bugs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Running with a prompt as input&lt;/h2&gt; &#xA;&lt;p&gt;You can also run Codex CLI with a prompt as input:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;codex &#34;explain this codebase to me&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;codex --full-auto &#34;create the fanciest todo-list app&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it - Codex will scaffold a file, run it inside a sandbox, install any missing dependencies, and show you the live result. Approve the changes and they&#39;ll be committed to your working directory.&lt;/p&gt; &#xA;&lt;h2&gt;Using Open Source Models&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Use &lt;code&gt;--profile&lt;/code&gt; to use other models&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Codex also allows you to use other providers that support the OpenAI Chat Completions (or Responses) API.&lt;/p&gt; &#xA; &lt;p&gt;To do so, you must first define custom &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/config.md#model_providers&#34;&gt;providers&lt;/a&gt; in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For example, the provider for a standard Ollama setup would be defined as follows:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[model_providers.ollama]&#xA;name = &#34;Ollama&#34;&#xA;base_url = &#34;http://localhost:11434/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The &lt;code&gt;base_url&lt;/code&gt; will have &lt;code&gt;/chat/completions&lt;/code&gt; appended to it to build the full URL for the request.&lt;/p&gt; &#xA; &lt;p&gt;For providers that also require an &lt;code&gt;Authorization&lt;/code&gt; header of the form &lt;code&gt;Bearer: SECRET&lt;/code&gt;, an &lt;code&gt;env_key&lt;/code&gt; can be specified, which indicates the environment variable to read to use as the value of &lt;code&gt;SECRET&lt;/code&gt; when making a request:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[model_providers.openrouter]&#xA;name = &#34;OpenRouter&#34;&#xA;base_url = &#34;https://openrouter.ai/api/v1&#34;&#xA;env_key = &#34;OPENROUTER_API_KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Providers that speak the Responses API are also supported by adding &lt;code&gt;wire_api = &#34;responses&#34;&lt;/code&gt; as part of the definition. Accessing OpenAI models via Azure is an example of such a provider, though it also requires specifying additional &lt;code&gt;query_params&lt;/code&gt; that need to be appended to the request URL:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[model_providers.azure]&#xA;name = &#34;Azure&#34;&#xA;# Make sure you set the appropriate subdomain for this URL.&#xA;base_url = &#34;https://YOUR_PROJECT_NAME.openai.azure.com/openai&#34;&#xA;env_key = &#34;AZURE_OPENAI_API_KEY&#34;  # Or &#34;OPENAI_API_KEY&#34;, whichever you use.&#xA;# Newer versions appear to support the responses API, see https://github.com/openai/codex/pull/1321&#xA;query_params = { api-version = &#34;2025-04-01-preview&#34; }&#xA;wire_api = &#34;responses&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Once you have defined a provider you wish to use, you can configure it as your default provider as follows:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;model_provider = &#34;azure&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;[!TIP] If you find yourself experimenting with a variety of models and providers, then you likely want to invest in defining a &lt;em&gt;profile&lt;/em&gt; for each configuration like so:&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[profiles.o3]&#xA;model_provider = &#34;azure&#34;&#xA;model = &#34;o3&#34;&#xA;&#xA;[profiles.mistral]&#xA;model_provider = &#34;ollama&#34;&#xA;model = &#34;mistral&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This way, you can specify one command-line argument (.e.g., &lt;code&gt;--profile o3&lt;/code&gt;, &lt;code&gt;--profile mistral&lt;/code&gt;) to override multiple settings together.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the &lt;code&gt;--oss&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interactive UI: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;codex --oss&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Non-interactive (programmatic) mode: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;echo &#34;Refactor utils&#34; | codex exec --oss&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Model selection when using &lt;code&gt;--oss&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you omit &lt;code&gt;-m/--model&lt;/code&gt;, Codex defaults to -m gpt-oss:20b and will verify it exists locally (downloading if needed).&lt;/li&gt; &#xA; &lt;li&gt;To pick a different size, pass one of: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;-m &#34;gpt-oss:20b&#34;&lt;/li&gt; &#xA;   &lt;li&gt;-m &#34;gpt-oss:120b&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Point Codex at your own OSS host:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;By default, &lt;code&gt;--oss&lt;/code&gt; talks to &lt;a href=&#34;http://localhost:11434/v1&#34;&gt;http://localhost:11434/v1&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To use a different host, set one of these environment variables before running Codex: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CODEX_OSS_BASE_URL, for example: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CODEX_OSS_BASE_URL=&#34;&lt;a href=&#34;http://my-ollama.example.com:11434/v1&#34;&gt;http://my-ollama.example.com:11434/v1&lt;/a&gt;&#34; codex --oss -m gpt-oss:20b&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;or CODEX_OSS_PORT (when the host is localhost): &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CODEX_OSS_PORT=11434 codex --oss&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Advanced: you can persist this in your config instead of environment variables by overriding the built-in &lt;code&gt;oss&lt;/code&gt; provider in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[model_providers.oss]&#xA;name = &#34;Open Source&#34;&#xA;base_url = &#34;http://my-ollama.example.com:11434/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h3&gt;Platform sandboxing details&lt;/h3&gt; &#xA;&lt;p&gt;The mechanism Codex uses to implement the sandbox policy depends on your OS:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;macOS 12+&lt;/strong&gt; uses &lt;strong&gt;Apple Seatbelt&lt;/strong&gt; and runs commands using &lt;code&gt;sandbox-exec&lt;/code&gt; with a profile (&lt;code&gt;-p&lt;/code&gt;) that corresponds to the &lt;code&gt;--sandbox&lt;/code&gt; that was specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; uses a combination of Landlock/seccomp APIs to enforce the &lt;code&gt;sandbox&lt;/code&gt; configuration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--sandbox danger-full-access&lt;/code&gt; (or, more simply, the &lt;code&gt;--dangerously-bypass-approvals-and-sandbox&lt;/code&gt; flag) within your container.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Experimental technology disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We&#39;re building it in the open with the community and welcome:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bug reports&lt;/li&gt; &#xA; &lt;li&gt;Feature requests&lt;/li&gt; &#xA; &lt;li&gt;Pull requests&lt;/li&gt; &#xA; &lt;li&gt;Good vibes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Requirement&lt;/th&gt; &#xA;   &lt;th&gt;Details&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operating systems&lt;/td&gt; &#xA;   &lt;td&gt;macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 &lt;strong&gt;via WSL2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Git (optional, recommended)&lt;/td&gt; &#xA;   &lt;td&gt;2.23+ for built-in PR helpers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RAM&lt;/td&gt; &#xA;   &lt;td&gt;4-GB minimum (8-GB recommended)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;CLI reference&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interactive TUI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;...&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Initial prompt for interactive TUI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex &#34;fix lint errors&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex exec &#34;...&#34;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Non-interactive &#34;automation mode&#34;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;codex exec &#34;explain utils.ts&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Key flags: &lt;code&gt;--model/-m&lt;/code&gt;, &lt;code&gt;--ask-for-approval/-a&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Memory &amp;amp; project docs&lt;/h2&gt; &#xA;&lt;p&gt;You can give Codex extra instructions and guidance using &lt;code&gt;AGENTS.md&lt;/code&gt; files. Codex looks for &lt;code&gt;AGENTS.md&lt;/code&gt; files in the following places, and merges them top-down:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;~/.codex/AGENTS.md&lt;/code&gt; - personal global guidance&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; at repo root - shared project notes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; in the current working directory - sub-folder/feature specifics&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Non-interactive / CI mode&lt;/h2&gt; &#xA;&lt;p&gt;Run Codex head-less in pipelines. Example GitHub Action step:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- name: Update changelog via Codex&#xA;  run: |&#xA;    npm install -g @openai/codex&#xA;    export OPENAI_API_KEY=&#34;${{ secrets.OPENAI_KEY }}&#34;&#xA;    codex exec --full-auto &#34;update CHANGELOG for next release&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; &#xA;&lt;p&gt;The Codex CLI can be configured to leverage MCP servers by defining an &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#mcp_servers&#34;&gt;&lt;code&gt;mcp_servers&lt;/code&gt;&lt;/a&gt; section in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. It is intended to mirror how tools such as Claude and Cursor define &lt;code&gt;mcpServers&lt;/code&gt; in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.&#xA;[mcp_servers.server-name]&#xA;command = &#34;npx&#34;&#xA;args = [&#34;-y&#34;, &#34;mcp-server&#34;]&#xA;env = { &#34;API_KEY&#34; = &#34;value&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] It is somewhat experimental, but the Codex CLI can also be run as an MCP &lt;em&gt;server&lt;/em&gt; via &lt;code&gt;codex mcp&lt;/code&gt;. If you launch it with an MCP client such as &lt;code&gt;npx @modelcontextprotocol/inspector codex mcp&lt;/code&gt; and send it a &lt;code&gt;tools/list&lt;/code&gt; request, you will see that there is only one tool, &lt;code&gt;codex&lt;/code&gt;, that accepts a grab-bag of inputs, including a catch-all &lt;code&gt;config&lt;/code&gt; map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Tracing / verbose logging&lt;/h2&gt; &#xA;&lt;p&gt;Because Codex is written in Rust, it honors the &lt;code&gt;RUST_LOG&lt;/code&gt; environment variable to configure its logging behavior.&lt;/p&gt; &#xA;&lt;p&gt;The TUI defaults to &lt;code&gt;RUST_LOG=codex_core=info,codex_tui=info&lt;/code&gt; and log messages are written to &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt;, so you can leave the following running in a separate terminal to monitor log messages as they are written:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tail -F ~/.codex/log/codex-tui.log&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By comparison, the non-interactive mode (&lt;code&gt;codex exec&lt;/code&gt;) defaults to &lt;code&gt;RUST_LOG=error&lt;/code&gt;, but messages are printed inline, so there is no need to monitor a separate file.&lt;/p&gt; &#xA;&lt;p&gt;See the Rust documentation on &lt;a href=&#34;https://docs.rs/env_logger/latest/env_logger/#enabling-logging&#34;&gt;&lt;code&gt;RUST_LOG&lt;/code&gt;&lt;/a&gt; for more information on the configuration options.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h3&gt;DotSlash&lt;/h3&gt; &#xA;&lt;p&gt;The GitHub Release also contains a &lt;a href=&#34;https://dotslash-cli.com/&#34;&gt;DotSlash&lt;/a&gt; file for the Codex CLI named &lt;code&gt;codex&lt;/code&gt;. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.&lt;/p&gt;  &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Build from source&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository and navigate to the root of the Cargo workspace.&#xA;git clone https://github.com/openai/codex.git&#xA;cd codex/codex-rs&#xA;&#xA;# Install the Rust toolchain, if necessary.&#xA;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y&#xA;source &#34;$HOME/.cargo/env&#34;&#xA;rustup component add rustfmt&#xA;rustup component add clippy&#xA;&#xA;# Build Codex.&#xA;cargo build&#xA;&#xA;# Launch the TUI with a sample prompt.&#xA;cargo run --bin codex -- &#34;explain this codebase to me&#34;&#xA;&#xA;# After making changes, ensure the code is clean.&#xA;cargo fmt -- --config imports_granularity=Item&#xA;cargo clippy --tests&#xA;&#xA;# Run the tests.&#xA;cargo test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Codex supports a rich set of configuration options documented in &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md&#34;&gt;&lt;code&gt;codex-rs/config.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default, Codex loads its configuration from &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Though &lt;code&gt;--config&lt;/code&gt; can be used to set/override ad-hoc config values for individual invocations of &lt;code&gt;codex&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OpenAI released a model called Codex in 2021 - is this related?&lt;/summary&gt; &#xA; &lt;p&gt;In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Which models are supported?&lt;/summary&gt; &#xA; &lt;p&gt;Any model available with &lt;a href=&#34;https://platform.openai.com/docs/api-reference/responses&#34;&gt;Responses API&lt;/a&gt;. The default is &lt;code&gt;o4-mini&lt;/code&gt;, but pass &lt;code&gt;--model gpt-4.1&lt;/code&gt; or set &lt;code&gt;model: gpt-4.1&lt;/code&gt; in your config file to override.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Why does &lt;code&gt;o3&lt;/code&gt; or &lt;code&gt;o4-mini&lt;/code&gt; not work for me?&lt;/summary&gt; &#xA; &lt;p&gt;It&#39;s possible that your &lt;a href=&#34;https://help.openai.com/en/articles/10910291-api-organization-verification&#34;&gt;API account needs to be verified&lt;/a&gt; in order to start streaming responses and seeing chain of thought summaries from the API. If you&#39;re still running into issues, please let us know!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;How do I stop Codex from editing my files?&lt;/summary&gt; &#xA; &lt;p&gt;Codex runs model-generated commands in a sandbox. If a proposed command or file change doesn&#39;t look right, you can simply type &lt;strong&gt;n&lt;/strong&gt; to deny the command or give the model feedback.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Does it work on Windows?&lt;/summary&gt; &#xA; &lt;p&gt;Not directly. It requires &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;Windows Subsystem for Linux (WSL2)&lt;/a&gt; - Codex has been tested on macOS and Linux with Node 22.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Zero data retention (ZDR) usage&lt;/h2&gt; &#xA;&lt;p&gt;Codex CLI &lt;strong&gt;does&lt;/strong&gt; support OpenAI organizations with &lt;a href=&#34;https://platform.openai.com/docs/guides/your-data#zero-data-retention&#34;&gt;Zero Data Retention (ZDR)&lt;/a&gt; enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure you are running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--config disable_response_storage=true&lt;/code&gt; or add this line to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to avoid specifying the command line option each time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;disable_response_storage = true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#disable_response_storage&#34;&gt;the configuration documentation on &lt;code&gt;disable_response_storage&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Codex open source fund&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re excited to launch a &lt;strong&gt;$1 million initiative&lt;/strong&gt; supporting open source projects that use Codex CLI and other OpenAI models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Grants are awarded up to &lt;strong&gt;$25,000&lt;/strong&gt; API credits.&lt;/li&gt; &#xA; &lt;li&gt;Applications are reviewed &lt;strong&gt;on a rolling basis&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interested? &lt;a href=&#34;https://openai.com/form/codex-open-source-fund/&#34;&gt;Apply here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project is under active development and the code will likely change pretty significantly. We&#39;ll update this message once that&#39;s complete!&lt;/p&gt; &#xA;&lt;p&gt;More broadly we welcome contributions - whether you are opening your very first pull request or you&#39;re a seasoned maintainer. At the same time we care about reliability and long-term maintainability, so the bar for merging code is intentionally &lt;strong&gt;high&lt;/strong&gt;. The guidelines below spell out what &#34;high-quality&#34; means in practice and should make the whole process transparent and friendly.&lt;/p&gt; &#xA;&lt;h3&gt;Development workflow&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;em&gt;topic branch&lt;/em&gt; from &lt;code&gt;main&lt;/code&gt; - e.g. &lt;code&gt;feat/interactive-prompt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.&lt;/li&gt; &#xA; &lt;li&gt;Following the &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/#development-workflow&#34;&gt;development setup&lt;/a&gt; instructions above, ensure your change is free of lint warnings and test failures.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Writing high-impact code changes&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start with an issue.&lt;/strong&gt; Open a new one or comment on an existing discussion so we can agree on the solution before code is written.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add or update tests.&lt;/strong&gt; Every new feature or bug-fix should come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document behaviour.&lt;/strong&gt; If your change affects user-facing behaviour, update the README, inline help (&lt;code&gt;codex --help&lt;/code&gt;), or relevant example projects.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Keep commits atomic.&lt;/strong&gt; Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Opening a pull request&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fill in the PR template (or include similar information) - &lt;strong&gt;What? Why? How?&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;strong&gt;all&lt;/strong&gt; checks locally (&lt;code&gt;cargo test &amp;amp;&amp;amp; cargo clippy --tests &amp;amp;&amp;amp; cargo fmt -- --config imports_granularity=Item&lt;/code&gt;). CI failures that could have been caught locally slow down the process.&lt;/li&gt; &#xA; &lt;li&gt;Make sure your branch is up-to-date with &lt;code&gt;main&lt;/code&gt; and that you have resolved merge conflicts.&lt;/li&gt; &#xA; &lt;li&gt;Mark the PR as &lt;strong&gt;Ready for review&lt;/strong&gt; only when you believe it is in a merge-able state.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Review process&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;One maintainer will be assigned as a primary reviewer.&lt;/li&gt; &#xA; &lt;li&gt;We may ask for changes - please do not take this personally. We value the work, we just also value consistency and long-term maintainability.&lt;/li&gt; &#xA; &lt;li&gt;When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Community values&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Be kind and inclusive.&lt;/strong&gt; Treat others with respect; we follow the &lt;a href=&#34;https://www.contributor-covenant.org/&#34;&gt;Contributor Covenant&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Assume good intent.&lt;/strong&gt; Written communication is hard - err on the side of generosity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Teach &amp;amp; learn.&lt;/strong&gt; If you spot something confusing, open an issue or PR with improvements.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting help&lt;/h3&gt; &#xA;&lt;p&gt;If you run into problems setting up the project, would like feedback on an idea, or just want to say &lt;em&gt;hi&lt;/em&gt; - please open a Discussion or jump into the relevant issue. We are happy to help.&lt;/p&gt; &#xA;&lt;p&gt;Together we can make Codex CLI an incredible tool. &lt;strong&gt;Happy hacking!&lt;/strong&gt; &lt;span&gt;🚀&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Contributor license agreement (CLA)&lt;/h3&gt; &#xA;&lt;p&gt;All contributors &lt;strong&gt;must&lt;/strong&gt; accept the CLA. The process is lightweight:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open your pull request.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Paste the following comment (or reply &lt;code&gt;recheck&lt;/code&gt; if you&#39;ve signed before):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;I have read the CLA Document and I hereby sign the CLA&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The CLA-Assistant bot records your signature in the repo and marks the status check as passed.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;No special Git commands, email attachments, or commit footers required.&lt;/p&gt; &#xA;&lt;h4&gt;Quick fixes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scenario&lt;/th&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Amend last commit&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;git commit --amend -s --no-edit &amp;amp;&amp;amp; git push -f&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The &lt;strong&gt;DCO check&lt;/strong&gt; blocks merges until every commit in the PR carries the footer (with squash this is just the one).&lt;/p&gt; &#xA;&lt;h3&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;For admins only.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure you are on &lt;code&gt;main&lt;/code&gt; and have no local changes. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;VERSION=0.2.0  # Can also be 0.2.0-alpha.1 or any valid Rust version.&#xA;./codex-rs/scripts/create_github_release.sh &#34;$VERSION&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will make a local commit on top of &lt;code&gt;main&lt;/code&gt; with &lt;code&gt;version&lt;/code&gt; set to &lt;code&gt;$VERSION&lt;/code&gt; in &lt;code&gt;codex-rs/Cargo.toml&lt;/code&gt; (note that on &lt;code&gt;main&lt;/code&gt;, we leave the version as &lt;code&gt;version = &#34;0.0.0&#34;&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;This will push the commit using the tag &lt;code&gt;rust-v${VERSION}&lt;/code&gt;, which in turn kicks off &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/.github/workflows/rust-release.yml&#34;&gt;the release workflow&lt;/a&gt;. This will create a new GitHub Release named &lt;code&gt;$VERSION&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If everything looks good in the generated GitHub Release, uncheck the &lt;strong&gt;pre-release&lt;/strong&gt; box so it is the latest release.&lt;/p&gt; &#xA;&lt;p&gt;Create a PR to update &lt;a href=&#34;https://github.com/Homebrew/homebrew-core/raw/main/Formula/c/codex.rb&#34;&gt;&lt;code&gt;Formula/c/codex.rb&lt;/code&gt;&lt;/a&gt; on Homebrew.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Security &amp;amp; responsible AI&lt;/h2&gt; &#xA;&lt;p&gt;Have you discovered a vulnerability or have concerns about model output? Please e-mail &lt;strong&gt;&lt;a href=&#34;mailto:security@openai.com&#34;&gt;security@openai.com&lt;/a&gt;&lt;/strong&gt; and we will respond promptly.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/openai/codex/main/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>