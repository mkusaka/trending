<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-16T01:44:20Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alvr-org/ALVR</title>
    <updated>2024-06-16T01:44:20Z</updated>
    <id>tag:github.com,2024-06-16:/alvr-org/ALVR</id>
    <link href="https://github.com/alvr-org/ALVR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stream VR games from your PC to your headset via Wi-Fi&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/alvr-org/ALVR/master/resources/alvr_combined_logo_hq.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;ALVR - Air Light VR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ALVR&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/720612397580025886?style=for-the-badge&amp;amp;logo=discord&amp;amp;color=5865F2&#34; alt=&#34;badge-discord&#34; title=&#34;Join us on Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#alvr:ckie.dev?via=ckie.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=chat&amp;amp;message=%23alvr&amp;amp;style=for-the-badge&amp;amp;logo=matrix&amp;amp;color=blueviolet&#34; alt=&#34;badge-matrix&#34; title=&#34;Join us on Matrix&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/alvr&#34;&gt;&lt;img src=&#34;https://img.shields.io/opencollective/all/alvr?style=for-the-badge&amp;amp;logo=opencollective&amp;amp;color=79a3e6&#34; alt=&#34;badge-opencollective&#34; title=&#34;Donate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stream VR games from your PC to your headset via Wi-Fi.&lt;br&gt; ALVR uses technologies like &lt;a href=&#34;https://developer.oculus.com/documentation/native/android/mobile-timewarp-overview&#34;&gt;Asynchronous Timewarp&lt;/a&gt; and &lt;a href=&#34;https://developer.oculus.com/documentation/native/android/mobile-ffr&#34;&gt;Fixed Foveated Rendering&lt;/a&gt; for a smoother experience.&lt;br&gt; Most of the games that run on SteamVR or Oculus Software (using Revive) should work with ALVR.&lt;br&gt; This is a fork of &lt;a href=&#34;https://github.com/polygraphene/ALVR&#34;&gt;ALVR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VR Headset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Support&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Quest 1/2/3/Pro&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pico 4/Neo 3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vive Focus 3/XR Elite&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;YVR 1/2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Lynx R1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Apple Vision Pro&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Smartphone/Monado&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ö†&lt;/span&gt; *&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google Cardboard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ö†&lt;/span&gt; * (&lt;a href=&#34;https://github.com/PhoneVR-Developers/PhoneVR&#34;&gt;PhoneVR&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GearVR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;üöß&lt;/span&gt; (maybe)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Oculus Go&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt; **&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;* : Only works on some smartphones, not enough testing.&lt;br&gt; ** : Oculus Go support was dropped, the minimum supported OS is Android 8. Download the last compatible version &lt;a href=&#34;https://github.com/alvr-org/ALVR/releases/tag/v18.2.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PC OS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Support&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Windows 8/10/11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Windows 7/XP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ubuntu/Arch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ö†&lt;/span&gt; ***&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Other linux distros&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùî&lt;/span&gt; ***&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;macOS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*** : Linux support is still in beta. To be able to make audio work or run ALVR at all you may need advanced knowledge of your distro for debugging or building from source.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;A supported standalone VR headset (see compatibility table above)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;SteamVR&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High-end gaming PC&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See OS compatibility table above.&lt;/li&gt; &#xA;   &lt;li&gt;NVIDIA GPU that supports NVENC (1000 GTX Series or higher) (or with an AMD GPU that supports AMF VCE) with the latest driver.&lt;/li&gt; &#xA;   &lt;li&gt;Laptops with an onboard (Intel HD, AMD iGPU) and an additional dedicated GPU (NVidia GTX/RTX, AMD HD/R5/R7): you should assign the dedicated GPU or &#34;high performance graphics adapter&#34; to the applications ALVR, SteamVR for best performance and compatibility. (NVidia: Nvidia control panel-&amp;gt;3d settings-&amp;gt;application settings; AMD: similiar way)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;802.11ac 5Ghz wireless or ethernet wired connection&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It is recommended to use 802.11ac 5Ghz for the headset and ethernet for PC&lt;/li&gt; &#xA;   &lt;li&gt;You need to connect both the PC and the headset to same router (or use a routed connection as described &lt;a href=&#34;https://github.com/alvr-org/ALVR/wiki/ALVR-v14-and-Above&#34;&gt;here&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Follow the installation guide &lt;a href=&#34;https://github.com/alvr-org/ALVR/wiki/Installation-guide&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please check the &lt;a href=&#34;https://github.com/alvr-org/ALVR/wiki/Troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt; page, and also &lt;a href=&#34;https://github.com/alvr-org/ALVR/wiki/Linux-Troubleshooting&#34;&gt;Linux Troubleshooting&lt;/a&gt; if applicable.&lt;/li&gt; &#xA; &lt;li&gt;Configuration recommendations and information may be found &lt;a href=&#34;https://github.com/alvr-org/ALVR/wiki/Information-and-Recommendations&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Uninstall&lt;/h2&gt; &#xA;&lt;p&gt;Open &lt;code&gt;ALVR Dashboard.exe&lt;/code&gt;, go to &lt;code&gt;Installation&lt;/code&gt; tab then press &lt;code&gt;Remove firewall rules&lt;/code&gt;. Close ALVR window and delete the ALVR folder.&lt;/p&gt; &#xA;&lt;h2&gt;Build from source&lt;/h2&gt; &#xA;&lt;p&gt;You can follow the guide &lt;a href=&#34;https://github.com/alvr-org/ALVR/wiki/Building-From-Source&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;ALVR is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/alvr-org/ALVR/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Privacy policy&lt;/h2&gt; &#xA;&lt;p&gt;ALVR apps do not directly collect any kind of data.&lt;/p&gt; &#xA;&lt;h2&gt;Donate&lt;/h2&gt; &#xA;&lt;p&gt;If you want to support this project you can make a donation to our &lt;a href=&#34;https://opencollective.com/alvr&#34;&gt;Open Source Collective account&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also donate to the original author of ALVR using Paypal (&lt;a href=&#34;mailto:polygraphene@gmail.com&#34;&gt;polygraphene@gmail.com&lt;/a&gt;) or with bitcoin (1FCbmFVSjsmpnAj6oLx2EhnzQzzhyxTLEv).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>EricLBuehler/mistral.rs</title>
    <updated>2024-06-16T01:44:20Z</updated>
    <id>tag:github.com,2024-06-16:/EricLBuehler/mistral.rs</id>
    <link href="https://github.com/EricLBuehler/mistral.rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Blazingly fast LLM inference.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; mistral.rs &lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Blazingly fast LLM inference. &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://ericlbuehler.github.io/mistral.rs/mistralrs/&#34;&gt;&lt;b&gt;Rust Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/raw/master/mistralrs-pyo3/API.md&#34;&gt;&lt;b&gt;Python Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/SZrecqK8qw&#34;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p&gt;Mistral.rs is a fast LLM inference platform supporting inference on a variety of devices, quantization, and easy-to-use application with an Open-AI API compatible HTTP server and Python bindings.&lt;/p&gt; &#xA;&lt;p&gt;Please submit requests for new models &lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/issues/156&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get started fast üöÄ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#installation-and-build&#34;&gt;Install&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#getting-models&#34;&gt;Get models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy with our easy to use APIs&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs/examples&#34;&gt;Rust&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/http.md&#34;&gt;OpenAI compatible HTTP server&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quick examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ü¶ô Run the Llama 3 model&lt;/p&gt; &lt;p&gt;&lt;em&gt;After following installation instructions&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./mistralrs_server -i plain -m meta-llama/Meta-Llama-3-8B-Instruct -a llama&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;œÜ¬≥ Run the Phi 3 model with 128K context window&lt;/p&gt; &lt;p&gt;&lt;em&gt;After following installation instructions&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./mistralrs_server -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;œÜ¬≥ üì∑ Run the Phi 3 vision model: &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/PHI3V.md&#34;&gt;documentation and guide here&lt;/a&gt;&lt;/p&gt; &lt;img src=&#34;https://www.nhmagazine.com/content/uploads/2019/05/mtwashingtonFranconia-2-19-18-108-Edit-Edit.jpg&#34; alt=&#34;Mount Washington&#34; width=&#34;400&#34; height=&#34;267&#34;&gt; &lt;h6&gt;&lt;a href=&#34;https://www.nhmagazine.com/mount-washington/&#34;&gt;Credit&lt;/a&gt;&lt;/h6&gt; &lt;p&gt;&lt;em&gt;After following installation instructions&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./mistralrs_server --port 1234 vision-plain -m microsoft/Phi-3-vision-128k-instruct -a phi3v&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Other models: &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#supported-models&#34;&gt;see supported models&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#run-with-the-cli&#34;&gt;how to run them&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quantized model support: 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit for faster inference and optimized memory usage.&lt;/li&gt; &#xA; &lt;li&gt;Continuous batching.&lt;/li&gt; &#xA; &lt;li&gt;Prefix caching.&lt;/li&gt; &#xA; &lt;li&gt;Device mapping: load and run some layers on the device and the rest on the CPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerator support&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple silicon support with the Metal framework.&lt;/li&gt; &#xA; &lt;li&gt;CPU inference with &lt;code&gt;mkl&lt;/code&gt;, &lt;code&gt;accelerate&lt;/code&gt; support and optimized backend.&lt;/li&gt; &#xA; &lt;li&gt;CUDA support with flash attention and cuDNN.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Easy&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightweight OpenAI API compatible HTTP server.&lt;/li&gt; &#xA; &lt;li&gt;Python API.&lt;/li&gt; &#xA; &lt;li&gt;Grammar support with Regex and Yacc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/ISQ.md&#34;&gt;ISQ&lt;/a&gt; (In situ quantization): run &lt;code&gt;.safetensors&lt;/code&gt; models directly from Hugging Face Hub by quantizing them after loading instead of creating a GGUF file. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This loads the ISQ-able weights on CPU before quantizing with ISQ and then moving to the device to avoid memory spikes.&lt;/li&gt; &#xA;   &lt;li&gt;Provides methods to further reduce memory spikes. &lt;strong&gt;Powerful&lt;/strong&gt;:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast LoRA support with weight merging.&lt;/li&gt; &#xA; &lt;li&gt;First X-LoRA inference platform with first class support.&lt;/li&gt; &#xA; &lt;li&gt;Speculative Decoding: Mix supported models as the draft model or the target model&lt;/li&gt; &#xA; &lt;li&gt;Dynamic LoRA adapter swapping at runtime with adapter preloading: &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/ADAPTER_MODELS.md#adapter-model-dynamic-adapter-activation&#34;&gt;examples and docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is a demo of interactive mode with streaming running Mistral GGUF:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/assets/65165915/3396abcd-8d44-4bf7-95e6-aa532db09415&#34;&gt;https://github.com/EricLBuehler/mistral.rs/assets/65165915/3396abcd-8d44-4bf7-95e6-aa532db09415&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported models:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mistral 7B (v0.1 and v0.2)&lt;/li&gt; &#xA; &lt;li&gt;Gemma&lt;/li&gt; &#xA; &lt;li&gt;Llama, including Llama 3&lt;/li&gt; &#xA; &lt;li&gt;Mixtral 8x7B&lt;/li&gt; &#xA; &lt;li&gt;Phi 2&lt;/li&gt; &#xA; &lt;li&gt;Phi 3&lt;/li&gt; &#xA; &lt;li&gt;Qwen 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#supported-models&#34;&gt;this section&lt;/a&gt; for details on quantization and LoRA support.&lt;/p&gt; &#xA;&lt;h2&gt;APIs and Integrations&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Rust Crate&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Rust multithreaded/async API for easy integration into any application.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://ericlbuehler.github.io/mistral.rs/mistralrs/&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs/examples/&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;To install: Add &lt;code&gt;mistralrs = { git = &#34;https://github.com/EricLBuehler/mistral.rs.git&#34; }&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Python API&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Python API for mistral.rs.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs-pyo3/README.md&#34;&gt;Installation including PyPI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs-pyo3/API.md&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python/python_api.py&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python/cookbook.ipynb&#34;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mistralrs import Runner, Which, ChatCompletionRequest&#xA;&#xA;runner = Runner(&#xA;    which=Which.GGUF(&#xA;        tok_model_id=&#34;mistralai/Mistral-7B-Instruct-v0.1&#34;,&#xA;        quantized_model_id=&#34;TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;,&#xA;        quantized_filename=&#34;mistral-7b-instruct-v0.1.Q4_K_M.gguf&#34;,&#xA;        tokenizer_json=None,&#xA;        repeat_last_n=64,&#xA;    )&#xA;)&#xA;&#xA;res = runner.send_chat_completion_request(&#xA;    ChatCompletionRequest(&#xA;        model=&#34;mistral&#34;,&#xA;        messages=[{&#34;role&#34;:&#34;user&#34;, &#34;content&#34;:&#34;Tell me a story about the Rust type system.&#34;}],&#xA;        max_tokens=256,&#xA;        presence_penalty=1.0,&#xA;        top_p=0.1,&#xA;        temperature=0.1,&#xA;    )&#xA;)&#xA;print(res.choices[0].message.content)&#xA;print(res.usage)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;HTTP Server&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;OpenAI API compatible API server&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/http.md&#34;&gt;API Docs&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/README.md#run&#34;&gt;Running&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/server/chat.py&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Llama Index integration&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Docs: &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/examples/llm/mistral_rs/&#34;&gt;https://docs.llamaindex.ai/en/stable/examples/llm/mistral_rs/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported accelerators&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable with &lt;code&gt;cuda&lt;/code&gt; feature: &lt;code&gt;--features cuda&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Flash attention support with &lt;code&gt;flash-attn&lt;/code&gt; feature, only applicable to non-quantized models: &lt;code&gt;--features flash-attn&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;cuDNNsupport with &lt;code&gt;cudnn&lt;/code&gt; feature: &lt;code&gt;--features cudnn&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Metal: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable with &lt;code&gt;metal&lt;/code&gt; feature: &lt;code&gt;--features metal&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CPU: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intel MKL with &lt;code&gt;mkl&lt;/code&gt; feature: &lt;code&gt;--features mkl&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Apple Accelerate with &lt;code&gt;accelerate&lt;/code&gt; feature: &lt;code&gt;--features accelerate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Enabling features is done by passing &lt;code&gt;--features ...&lt;/code&gt; to the build system. When using &lt;code&gt;cargo run&lt;/code&gt; or &lt;code&gt;maturin develop&lt;/code&gt;, pass the &lt;code&gt;--features&lt;/code&gt; flag before the &lt;code&gt;--&lt;/code&gt; separating build flags from runtime flags.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To enable a single feature like &lt;code&gt;metal&lt;/code&gt;: &lt;code&gt;cargo build --release --features metal&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To enable multiple features, specify them in quotes: &lt;code&gt;cargo build --release --features &#34;cuda flash-attn cudnn&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;Mistral.rs Completion T/s&lt;/th&gt; &#xA;   &lt;th&gt;Llama.cpp Completion T/s&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Quant&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A10 GPU, CUDA&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4_K_M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon 8358 CPU, AVX&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4_K_M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Raspberry Pi 5 (8GB), Neon&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2_K&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A100 GPU, CUDA&lt;/td&gt; &#xA;   &lt;td&gt;119&lt;/td&gt; &#xA;   &lt;td&gt;119&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4_K_M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please submit more benchmarks via raising an issue!&lt;/p&gt; &#xA;&lt;h2&gt;Installation and Build&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install required packages&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;openssl&lt;/code&gt; (&lt;em&gt;Example on Ubuntu:&lt;/em&gt; &lt;code&gt;sudo apt install libssl-dev&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pkg-config&lt;/code&gt; (&lt;em&gt;Example on Ubuntu:&lt;/em&gt; &lt;code&gt;sudo apt install pkg-config&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Rust: &lt;a href=&#34;https://rustup.rs/&#34;&gt;https://rustup.rs/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example on Ubuntu:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;source $HOME/.cargo/env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set HF token correctly (skip if already set or your model is not gated, or if you want to use the &lt;code&gt;token_source&lt;/code&gt; parameters in Python or the command line.)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Note: you can install &lt;code&gt;huggingface-cli&lt;/code&gt; as documented &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/en/installation&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the code&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/EricLBuehler/mistral.rs.git&#xA;cd mistral.rs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build or install&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Base build command&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with CUDA support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with CUDA and Flash Attention V2 support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features &#34;cuda flash-attn&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with Metal support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features metal&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with Accelerate support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with MKL support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features mkl&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Install with &lt;code&gt;cargo install&lt;/code&gt; for easy command line usage&lt;/p&gt; &lt;p&gt;Pass the same values to &lt;code&gt;--features&lt;/code&gt; as you would for &lt;code&gt;cargo build&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo install --path mistralrs-server --features cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The build process will output a binary &lt;code&gt;misralrs-server&lt;/code&gt; at &lt;code&gt;./target/release/mistralrs-server&lt;/code&gt; which may be copied into the working directory with the following command:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Example on Ubuntu:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cp ./target/release/mistralrs-server ./mistralrs_server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Installing Python support&lt;/p&gt; &lt;p&gt;You can install Python support by following the guide &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs-pyo3/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting models&lt;/h2&gt; &#xA;&lt;p&gt;There are 2 ways to run a model with mistral.rs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;From Hugging Face Hub (easiest)&lt;/li&gt; &#xA; &lt;li&gt;From local files &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Running a GGUF model fully locally&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting models from Hugging Face Hub&lt;/h3&gt; &#xA;&lt;p&gt;Mistral.rs can automatically download models from HF Hub. To access gated models, you should provide a token source. They may be one of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;literal:&amp;lt;value&amp;gt;&lt;/code&gt;: Load from a specified literal&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;env:&amp;lt;value&amp;gt;&lt;/code&gt;: Load from a specified environment variable&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;path:&amp;lt;value&amp;gt;&lt;/code&gt;: Load from a specified file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cache&lt;/code&gt;: &lt;strong&gt;default&lt;/strong&gt;: Load from the HF token at ~/.cache/huggingface/token or equivalent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Use no HF token&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is passed in the following ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Command line:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --token-source none -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python/token_source.py&#34;&gt;Here&lt;/a&gt; is an example of setting the token source.&lt;/p&gt; &#xA;&lt;p&gt;If token cannot be loaded, no token will be used (i.e. effectively using &lt;code&gt;none&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Loading models from local files:&lt;/h3&gt; &#xA;&lt;p&gt;You can also instruct mistral.rs to load models fully locally by modifying the &lt;code&gt;*_model_id&lt;/code&gt; arguments or options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 plain -m . -a mistral&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Throughout mistral.rs, any model ID argument or option may be a local path and should contain the following files for each model ID option:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model-id&lt;/code&gt; (server) or &lt;code&gt;model_id&lt;/code&gt; (python/rust) or &lt;code&gt;--tok-model-id&lt;/code&gt; (server) or &lt;code&gt;tok_model_id&lt;/code&gt; (python/rust): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer.json&lt;/code&gt; (if not specified separately)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;.safetensors&lt;/code&gt; files.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--quantized-model-id&lt;/code&gt; (server) or &lt;code&gt;quantized_model_id&lt;/code&gt; (python/rust): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specified &lt;code&gt;.gguf&lt;/code&gt; or &lt;code&gt;.ggml&lt;/code&gt; file.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--x-lora-model-id&lt;/code&gt; (server) or &lt;code&gt;xlora_model_id&lt;/code&gt; (python/rust): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;xlora_classifier.safetensors&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;xlora_config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Adapters &lt;code&gt;.safetensors&lt;/code&gt; and &lt;code&gt;adapter_config.json&lt;/code&gt; files in their respective directories&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--adapters-model-id&lt;/code&gt; (server) or &lt;code&gt;adapters_model_id&lt;/code&gt; (python/rust): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Adapters &lt;code&gt;.safetensors&lt;/code&gt; and &lt;code&gt;adapter_config.json&lt;/code&gt; files in their respective directories&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running GGUF models locally&lt;/h3&gt; &#xA;&lt;p&gt;To run GGUF models fully locally, the only mandatory arguments are the quantized model ID and the quantized filename.&lt;/p&gt; &#xA;&lt;h4&gt;Chat template&lt;/h4&gt; &#xA;&lt;p&gt;The chat template can be automatically detected and loaded from the GGUF file if no other chat template source is specified including the tokenizer model ID.&lt;/p&gt; &#xA;&lt;p&gt;you do not need to specify the tokenizer model ID argument and instead should pass a path to the chat template JSON file (examples &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/chat_templates&#34;&gt;here&lt;/a&gt;, you will need to create your own by specifying the chat template and &lt;code&gt;bos&lt;/code&gt;/&lt;code&gt;eos&lt;/code&gt; tokens) as well as specifying a local model ID. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs-server --chat-template &amp;lt;chat_template&amp;gt; gguf -m . -f Phi-3-mini-128k-instruct-q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not specify a chat template, then the &lt;code&gt;--tok-model-id&lt;/code&gt;/&lt;code&gt;-t&lt;/code&gt; tokenizer model ID argument is expected where the &lt;code&gt;tokenizer_config.json&lt;/code&gt; file should be provided. If that model ID contains a &lt;code&gt;tokenizer.json&lt;/code&gt;, then that will be used over the GGUF tokenizer.&lt;/p&gt; &#xA;&lt;h4&gt;Tokenizer&lt;/h4&gt; &#xA;&lt;p&gt;The following tokenizer model types are currently supported. If you would like one to be added, please raise an issue. Otherwise, please consider using the method demonstrated in examples below, where the tokenizer is sourced from Hugging Face.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported GGUF tokenizer types&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;llama&lt;/code&gt; (sentencepiece)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt2&lt;/code&gt; (BPE)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run with the CLI&lt;/h2&gt; &#xA;&lt;p&gt;Mistral.rs uses subcommands to control the model type. They are generally of format &lt;code&gt;&amp;lt;XLORA/LORA&amp;gt;-&amp;lt;QUANTIZATION&amp;gt;&lt;/code&gt;. Please run &lt;code&gt;./mistralrs_server --help&lt;/code&gt; to see the subcommands.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, for models without quantization, the model architecture should be provided as the &lt;code&gt;--arch&lt;/code&gt; or &lt;code&gt;-a&lt;/code&gt; argument in contrast to GGUF models which encode the architecture in the file.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture for plain models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: for plain models, you can specify the data type to load and run in. This must be one of &lt;code&gt;f32&lt;/code&gt;, &lt;code&gt;f16&lt;/code&gt;, &lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;auto&lt;/code&gt; to choose based on the device. This is specified in the &lt;code&gt;--dype&lt;/code&gt;/&lt;code&gt;-d&lt;/code&gt; parameter after the model architecture (&lt;code&gt;plain&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mistral&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gemma&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mixtral&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llama&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;qwen2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Architecture for vision models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: for vision models, you can specify the data type to load and run in. This must be one of &lt;code&gt;f32&lt;/code&gt;, &lt;code&gt;f16&lt;/code&gt;, &lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;auto&lt;/code&gt; to choose based on the device. This is specified in the &lt;code&gt;--dype&lt;/code&gt;/&lt;code&gt;-d&lt;/code&gt; parameter after the model architecture (&lt;code&gt;vision-plain&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;phi3v&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interactive mode:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can launch interactive mode, a simple chat application running in the terminal, by passing &lt;code&gt;-i&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More quick examples:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;X-LoRA with no quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start an X-LoRA server with the exactly as presented in &lt;a href=&#34;https://arxiv.org/abs/2402.07148&#34;&gt;the paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 x-lora-plain -o orderings/xlora-paper-ordering.json -x lamm-mit/x-lora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LoRA with a model from GGUF&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start an LoRA server with adapters from the X-LoRA paper (you should modify the ordering file to use only one adapter, as the adapter static scalings are all 1 and so the signal will become distorted):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 lora-gguf -o orderings/xlora-paper-ordering.json -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q8_0.gguf -a lamm-mit/x-lora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Normally with a LoRA model you would use a custom ordering file. However, for this example we use the ordering from the X-LoRA paper because we are using the adapters from the X-LoRA paper.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With a model from GGUF&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start a server running Mistral from GGUF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 gguf -t mistralai/Mistral-7B-Instruct-v0.1 -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -f mistral-7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With a model from GGML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start a server running Llama from GGML:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 ggml -t meta-llama/Llama-2-13b-chat-hf -m TheBloke/Llama-2-13B-chat-GGML -f llama-2-13b-chat.ggmlv3.q4_K_M.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain model from safetensors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start a server running Mistral from safetensors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 plain -m mistralai/Mistral-7B-Instruct-v0.1 -a mistral&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Structured selection with a &lt;code&gt;.toml&lt;/code&gt; file&lt;/h3&gt; &#xA;&lt;p&gt;We provide a method to select models with a &lt;code&gt;.toml&lt;/code&gt; file. The keys are the same as the command line, with &lt;code&gt;no_kv_cache&lt;/code&gt; and &lt;code&gt;tokenizer_json&lt;/code&gt; being &#34;global&#34; keys.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 toml -f toml-selectors/gguf.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported models&lt;/h2&gt; &#xA;&lt;p&gt;Mistal.rs supports several model categories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;text&lt;/li&gt; &#xA; &lt;li&gt;vision (see &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/VISION_MODELS.md&#34;&gt;the docs&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quantization support&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;GGUF&lt;/th&gt; &#xA;   &lt;th&gt;GGML&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral 7B&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral 8x7B&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 2&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3 Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Device mapping support&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Supported&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Plain&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GGML&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vision Plain&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-LoRA and LoRA support&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;X-LoRA&lt;/th&gt; &#xA;   &lt;th&gt;X-LoRA+GGUF&lt;/th&gt; &#xA;   &lt;th&gt;X-LoRA+GGML&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral 7B&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral 8x7B&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 2&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3 Vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Using derivative model&lt;/h3&gt; &#xA;&lt;p&gt;To use a derivative model, select the model architecture using the correct subcommand. To see what can be passed for the architecture, pass &lt;code&gt;--help&lt;/code&gt; after the subcommand. For example, when using a different model than the default, specify the following for the following types of models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plain&lt;/strong&gt;: Model id&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quantized&lt;/strong&gt;: Quantized model id, quantized filename, and tokenizer id&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;X-LoRA&lt;/strong&gt;: Model id, X-LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;X-LoRA quantized&lt;/strong&gt;: Quantized model id, quantized filename, tokenizer id, and X-LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: Model id, LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA quantized&lt;/strong&gt;: Quantized model id, quantized filename, tokenizer id, and LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vision Plain&lt;/strong&gt;: Model id&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#adapter-ordering-file&#34;&gt;this&lt;/a&gt; section to determine if it is necessary to prepare an X-LoRA/LoRA ordering file, it is always necessary if the target modules or architecture changed, or if the adapter order changed.&lt;/p&gt; &#xA;&lt;p&gt;It is also important to check the chat template style of the model. If the HF hub repo has a &lt;code&gt;tokenizer_config.json&lt;/code&gt; file, it is not necessary to specify. Otherwise, templates can be found in &lt;code&gt;chat_templates&lt;/code&gt; and should be passed before the subcommand. If the model is not instruction tuned, no chat template will be found and the APIs will only accept a prompt, no messages.&lt;/p&gt; &#xA;&lt;p&gt;For example, when using a Zephyr model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./mistralrs_server --port 1234 --log output.txt gguf -t HuggingFaceH4/zephyr-7b-beta -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q5_0.gguf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Adapter model support: X-LoRA and LoRA&lt;/h3&gt; &#xA;&lt;p&gt;An adapter model is a model with X-LoRA or LoRA. X-LoRA support is provided by selecting the &lt;code&gt;x-lora-*&lt;/code&gt; architecture, and LoRA support by selecting the &lt;code&gt;lora-*&lt;/code&gt; architecture. Please find docs for adapter models &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/ADAPTER_MODELS.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chat Templates and Tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;Mistral.rs will attempt to automatically load a chat template and tokenizer. This enables high flexibility across models and ensures accurate and flexible chat templating. However, this behavior can be customized. Please find detailed documentation &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/CHAT_TOK.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for contributing! If you have any problems or want to contribute something, please raise an issue or pull request. If you want to add a new model, please contact us via an issue and we can coordinate how to do this.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Debugging with the environment variable &lt;code&gt;MISTRALRS_DEBUG=1&lt;/code&gt; causes the following things &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If loading a GGUF or GGML model, this will output a file containing the names, shapes, and types of each tensor. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;mistralrs_gguf_tensors.txt&lt;/code&gt; or &lt;code&gt;mistralrs_ggml_tensors.txt&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;More logging.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Setting the CUDA compiler path: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set the &lt;code&gt;NVCC_CCBIN&lt;/code&gt; environment variable during build.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Error: &lt;code&gt;recompile with -fPIE&lt;/code&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Some Linux distributions require compiling with &lt;code&gt;-fPIE&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Set the &lt;code&gt;CUDA_NVCC_FLAGS&lt;/code&gt; environment variable to &lt;code&gt;-fPIE&lt;/code&gt; during build: &lt;code&gt;CUDA_NVCC_FLAGS=-fPIE&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Error &lt;code&gt;CUDA_ERROR_NOT_FOUND&lt;/code&gt; or symbol not found when using a normal or vison model: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For non-quantized models, you can specify the data type to load and run in. This must be one of &lt;code&gt;f32&lt;/code&gt;, &lt;code&gt;f16&lt;/code&gt;, &lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;auto&lt;/code&gt; to choose based on the device.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This project would not be possible without the excellent work at &lt;a href=&#34;https://github.com/huggingface/candle&#34;&gt;&lt;code&gt;candle&lt;/code&gt;&lt;/a&gt;. Additionally, thank you to all contributors! Contributing can range from raising an issue or suggesting a feature to adding some new functionality.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/windows-rs</title>
    <updated>2024-06-16T01:44:20Z</updated>
    <id>tag:github.com,2024-06-16:/microsoft/windows-rs</id>
    <link href="https://github.com/microsoft/windows-rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Rust for Windows&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Rust for Windows&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://crates.io/crates/windows&#34;&gt;windows&lt;/a&gt; and &lt;a href=&#34;https://crates.io/crates/windows-sys&#34;&gt;windows-sys&lt;/a&gt; crates let you call any Windows API past, present, and future using code generated on the fly directly from the &lt;a href=&#34;https://github.com/microsoft/windows-rs/tree/master/crates/libs/bindgen/default&#34;&gt;metadata describing the API&lt;/a&gt; and right into your Rust package where you can call them as if they were just another Rust module. The Rust language projection follows in the tradition established by &lt;a href=&#34;https://github.com/microsoft/cppwinrt&#34;&gt;C++/WinRT&lt;/a&gt; of building language projections for Windows using standard languages and compilers, providing a natural and idiomatic way for Rust developers to call Windows APIs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kennykerr.ca/rust-getting-started/&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/windows-rs/tree/master/crates/samples&#34;&gt;Samples&lt;/a&gt; &#xA;  &lt;!-- link to latest samples on repo --&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/windows-rs/releases&#34;&gt;Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/windows-rs/features/#/master&#34;&gt;Feature search&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repo is the home of the following crates (and other supporting crates):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows&#34;&gt;windows&lt;/a&gt; - Safer bindings including C-style APIs as well as COM and WinRT APIs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-bindgen&#34;&gt;windows-bindgen&lt;/a&gt; - Windows metadata compiler library.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-core&#34;&gt;windows-core&lt;/a&gt; - Type support for the &lt;code&gt;windows&lt;/code&gt; crate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-implement&#34;&gt;windows-implement&lt;/a&gt; - The &lt;code&gt;implement&lt;/code&gt; macro for the &lt;code&gt;windows&lt;/code&gt; crate, for implementing COM interfaces.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-interface&#34;&gt;windows-interface&lt;/a&gt; - The &lt;code&gt;interface&lt;/code&gt; macro for the &lt;code&gt;windows&lt;/code&gt; crate, for declaring COM interfaces.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-metadata&#34;&gt;windows-metadata&lt;/a&gt; - Windows metadata reader.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-registry&#34;&gt;windows-registry&lt;/a&gt; - Windows registry.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-result&#34;&gt;windows-result&lt;/a&gt; - Windows error handling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-sys&#34;&gt;windows-sys&lt;/a&gt; - Raw bindings for C-style Windows APIs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-targets&#34;&gt;windows-targets&lt;/a&gt; - Import libs for Windows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/windows-version&#34;&gt;windows-version&lt;/a&gt; - Windows version information.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/cppwinrt&#34;&gt;cppwinrt&lt;/a&gt; - Bundles the C++/WinRT compiler for use in Rust.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://crates.io/crates/riddle&#34;&gt;riddle&lt;/a&gt; - Windows metadata compiler tool.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>