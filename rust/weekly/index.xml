<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Rust Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-26T01:57:48Z</updated>
  <subtitle>Weekly Trending of Rust in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>loro-dev/loro</title>
    <updated>2023-11-26T01:57:48Z</updated>
    <id>tag:github.com,2023-11-26:/loro-dev/loro</id>
    <link href="https://github.com/loro-dev/loro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reimagine state management with CRDTs. Easily enable collaboration and time-travel on your app.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://loro.dev&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;img src=&#34;https://raw.githubusercontent.com/loro-dev/loro/main/docs/Loro.svg?sanitize=true&#34; width=&#34;200&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://loro.dev&#34; alt=&#34;loro-site&#34;&gt;Loro&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Reimagine state management with CRDTs ü¶ú&lt;/b&gt;&lt;br&gt; Make your app state synchronized and collaborative effortlessly. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a aria-label=&#34;X&#34; href=&#34;https://x.com/loro_dev&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://img.shields.io/badge/Twitter-black?style=for-the-badge&amp;amp;logo=Twitter&#34;&gt; &lt;/a&gt; &lt;a aria-label=&#34;Discord-Link&#34; href=&#34;https://discord.gg/tUsBSVfqzf&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://img.shields.io/badge/Discord-black?style=for-the-badge&amp;amp;logo=discord&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.postimg.cc/nr2SLHQB/202311120101-2.gif&#34; alt=&#34;Rich text example&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Notice&lt;/strong&gt;: The current API and encoding schema of Loro are &lt;strong&gt;experimental&lt;/strong&gt; and &lt;strong&gt;subject to change&lt;/strong&gt;. You should not use it in production.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Loro is a CRDTs(Conflict-free Replicated Data Types) library that makes building &lt;a href=&#34;https://www.inkandswitch.com/local-first/&#34;&gt;local-first apps&lt;/a&gt; easier.&lt;/p&gt; &#xA;&lt;p&gt;Explore our vision for the local-first development paradigm in our blog post: &lt;a href=&#34;https://loro.dev/blog/loro-now-open-source&#34;&gt;&lt;strong&gt;Reimagine State Management with CRDTs&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;h2&gt;Supported CRDT Algorithms&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Common Data Structures&lt;/strong&gt;: Support for &lt;code&gt;List&lt;/code&gt; for ordered collections, LWW(Last Write Win) &lt;code&gt;Map&lt;/code&gt; for key-value pairs, &lt;code&gt;Tree&lt;/code&gt; for hierarchical data, and &lt;code&gt;Text&lt;/code&gt; for rich text manipulation, enabling various applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text Editing with Fugue&lt;/strong&gt;: Loro integrates &lt;a href=&#34;https://arxiv.org/abs/2305.00583&#34;&gt;Fugue&lt;/a&gt;, a CRDT algorithm designed to minimize interleaving anomalies in concurrent text editing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Peritext-like Rich Text CRDT&lt;/strong&gt;: Drawing inspiration from &lt;a href=&#34;https://www.inkandswitch.com/peritext/&#34;&gt;Peritext&lt;/a&gt;, Loro manages rich text CRDTs that excel at merging concurrent rich text style edits, maintaining the original intent of users input as much as possible. Details on this will be explored further in an upcoming blog post.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Moveable Tree&lt;/strong&gt;: For applications requiring directory-like data manipulation, Loro utilizes the algorithm from &lt;a href=&#34;https://ieeexplore.ieee.org/document/9563274&#34;&gt;&lt;em&gt;A Highly-Available Move Operation for Replicated Trees&lt;/em&gt;&lt;/a&gt;, which simplifies the process of moving hierarchical data structures.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Advanced Features in Loro&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Preserve Editing History&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;With Loro, you can track changes effortlessly as it records the editing history with low overhead.&lt;/li&gt; &#xA;   &lt;li&gt;This feature is essential for audit trails, undo/redo functionality, and understanding the evolution of your data over time.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Time Travel Through History&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It allows users to compare and merge manually when needed, although CRDTs typically resolve conflicts well.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.loro.dev/docs/performance&#34;&gt;See benchmarks&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Build time travel feature easily for large documents&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/loro-dev/loro/assets/18425020/ec2d20a3-3d8c-4483-a601-b200243c9792&#34;&gt;https://github.com/loro-dev/loro/assets/18425020/ec2d20a3-3d8c-4483-a601-b200243c9792&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features Provided by CRDTs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Decentralized Synchronization&lt;/strong&gt;: Loro allows your app&#39;s state synced via p2p connections.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Merging&lt;/strong&gt;: CRDTs guarantee strong eventual consistency by automating the merging of concurrent changes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Availability&lt;/strong&gt;: Data can be persisted on users&#39; devices, supporting offline functionality and real-time responsiveness.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Effortlessly scale your application horizontally thanks to the inherently distributed nature of CRDTs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Delta Updates&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;h3&gt;Development Environment Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rust&lt;/strong&gt;: Install from the official Rust website.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deno&lt;/strong&gt;: Download and install from Deno&#39;s website.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Node&lt;/strong&gt;: Install from the Node.js website.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;pnpm&lt;/strong&gt;: Run &lt;code&gt;npm i -g pnpm&lt;/code&gt; for global installation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rust Target&lt;/strong&gt;: Add with &lt;code&gt;rustup target add wasm32-unknown-unknown&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;wasm-bindgen-cli&lt;/strong&gt;: Install version 0.2.86 via &lt;code&gt;cargo install wasm-bindgen-cli --version 0.2.86&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;wasm-opt&lt;/strong&gt;: Install using &lt;code&gt;cargo install wasm-opt --locked&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;cargo-nextest&lt;/strong&gt;: Install using &lt;code&gt;cargo install cargo-nextest --locked&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;cargo-fuzz&lt;/strong&gt;: Run &lt;code&gt;cargo install cargo-fuzz&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deno task test&#xA;&#xA;# Build and test WASM&#xA;deno task test-wasm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;Loro draws inspiration from the innovative work of the following projects and individuals:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://inkandswitch.com/&#34;&gt;Ink &amp;amp; Switch&lt;/a&gt;: The principles of Local-first Software have greatly influenced this project. The &lt;a href=&#34;https://www.inkandswitch.com/peritext/&#34;&gt;Peritext&lt;/a&gt; project has also shaped our approach to rich text CRDTs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/josephg/diamond-types&#34;&gt;Diamond-types&lt;/a&gt;: The ingenious OT-like merging algorithm from @josephg has been adapted to reduce the computation and space usage of CRDTs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/automerge/automerge&#34;&gt;Automerge&lt;/a&gt;: Their use of columnar encoding for CRDTs has informed our strategies for efficient data encoding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yjs/yjs&#34;&gt;Yjs&lt;/a&gt;: We have incorporated a similar algorithm for effectively merging collaborative editing operations, thanks to their pioneering contributions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mattweidner.com/&#34;&gt;Matthew Weidner&lt;/a&gt;: His work on the &lt;a href=&#34;https://arxiv.org/abs/2305.00583&#34;&gt;Fugue&lt;/a&gt; algorithm has been invaluable, enhancing our text editing capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/candle</title>
    <updated>2023-11-26T01:57:48Z</updated>
    <id>tag:github.com,2023-11-26:/huggingface/candle</id>
    <link href="https://github.com/huggingface/candle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimalist ML framework for Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;candle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/hugging-face-879548962464493619&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/hugging-face-879548962464493619&#34; alt=&#34;discord server&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/candle-core&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/candle-core.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/candle-core&#34;&gt;&lt;img src=&#34;https://docs.rs/candle-core/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/crates/l/candle-core.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use. Try our online demos: &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;LLaMA2&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm&#34;&gt;T5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-yolo&#34;&gt;yolo&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/radames/candle-segment-anything-wasm&#34;&gt;Segment Anything&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;p&gt;Make sure that you have &lt;a href=&#34;https://github.com/huggingface/candle/tree/main/candle-core&#34;&gt;&lt;code&gt;candle-core&lt;/code&gt;&lt;/a&gt; correctly installed as described in &lt;a href=&#34;https://huggingface.github.io/candle/guide/installation.html&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s see how to run a simple matrix multiplication. Write the following to your &lt;code&gt;myapp/src/main.rs&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use candle_core::{Device, Tensor};&#xA;&#xA;fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {&#xA;    let device = Device::Cpu;&#xA;&#xA;    let a = Tensor::randn(0f32, 1., (2, 3), &amp;amp;device)?;&#xA;    let b = Tensor::randn(0f32, 1., (3, 4), &amp;amp;device)?;&#xA;&#xA;    let c = a.matmul(&amp;amp;b)?;&#xA;    println!(&#34;{c}&#34;);&#xA;    Ok(())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;cargo run&lt;/code&gt; should display a tensor of shape &lt;code&gt;Tensor[[2, 4], f32]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Having installed &lt;code&gt;candle&lt;/code&gt; with Cuda support, simply define the &lt;code&gt;device&lt;/code&gt; to be on GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- let device = Device::Cpu;&#xA;+ let device = Device::new_cuda(0)?;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced examples, please have a look at the following section.&lt;/p&gt; &#xA;&lt;h2&gt;Check out our examples&lt;/h2&gt; &#xA;&lt;p&gt;These online demos run entirely in your browser:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-yolo&#34;&gt;yolo&lt;/a&gt;: pose estimation and object recognition.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;: speech recognition.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;LLaMA2&lt;/a&gt;: text generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm&#34;&gt;T5&lt;/a&gt;: text generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm&#34;&gt;Phi-v1.5&lt;/a&gt;: text generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/radames/candle-segment-anything-wasm&#34;&gt;Segment Anything Model&lt;/a&gt;: Image segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/radames/Candle-BLIP-Image-Captioning&#34;&gt;BLIP&lt;/a&gt;: image captioning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also provide a some command line based examples using state of the art models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/llama/&#34;&gt;LLaMA and LLaMA-v2&lt;/a&gt;: general LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/falcon/&#34;&gt;Falcon&lt;/a&gt;: general LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/phi/&#34;&gt;Phi-v1 and Phi-v1.5&lt;/a&gt;: a 1.3b general LLM with performance on par with LLaMA-v2 7b.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/stable-lm/&#34;&gt;StableLM-3B-4E1T&lt;/a&gt;: a 3b general LLM pre-trained on 1T tokens of English and code datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/mistral/&#34;&gt;Mistral7b-v0.1&lt;/a&gt;: a 7b general LLM with performance larger than all publicly available 13b models as of 2023-09-28.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bigcode/&#34;&gt;StarCoder&lt;/a&gt;: LLM specialized to code generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/replit-code/&#34;&gt;Replit-code-v1.5&lt;/a&gt;: a 3.3b LLM specialized for code completion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/yi/&#34;&gt;Yi-6B / Yi-34B&lt;/a&gt;: two bilingual (English/Chinese) general LLMs with 6b and 34b parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/quantized/&#34;&gt;Quantized LLaMA&lt;/a&gt;: quantized version of the LLaMA model using the same quantization techniques as &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/huggingface/candle/raw/main/candle-examples/examples/quantized/assets/aoc.gif&#34; width=&#34;600&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/stable-diffusion/&#34;&gt;Stable Diffusion&lt;/a&gt;: text to image generative model, support for the 1.5, 2.1, and SDXL 1.0 versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/huggingface/candle/raw/main/candle-examples/examples/stable-diffusion/assets/stable-diffusion-xl.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/wuerstchen/&#34;&gt;Wuerstchen&lt;/a&gt;: another text to image generative model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/huggingface/candle/raw/main/candle-examples/examples/wuerstchen/assets/cat.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/yolo-v3/&#34;&gt;yolo-v3&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/yolo-v8/&#34;&gt;yolo-v8&lt;/a&gt;: object detection and pose estimation models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.od.jpg&#34; width=&#34;200&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.pose.jpg&#34; width=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/segment-anything/&#34;&gt;segment-anything&lt;/a&gt;: image segmentation model with prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/huggingface/candle/raw/main/candle-examples/examples/segment-anything/assets/sam_merged.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/whisper/&#34;&gt;Whisper&lt;/a&gt;: speech recognition model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/t5&#34;&gt;T5&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bert/&#34;&gt;Bert&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/jina-bert/&#34;&gt;JinaBert&lt;/a&gt; : useful for sentence embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/dinov2/&#34;&gt;DINOv2&lt;/a&gt;: computer vision model trained using self-supervision (can be used for imagenet classification, depth evaluation, segmentation).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/blip/&#34;&gt;BLIP&lt;/a&gt;: image to text model, can be used to generate captions for an image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/marian-mt/&#34;&gt;Marian-MT&lt;/a&gt;: neural machine translation model, generates the translated text from the input text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run them using commands like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --example quantized --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to use &lt;strong&gt;CUDA&lt;/strong&gt; add &lt;code&gt;--features cuda&lt;/code&gt; to the example command line. If you have cuDNN installed, use &lt;code&gt;--features cudnn&lt;/code&gt; for even more speedups.&lt;/p&gt; &#xA;&lt;p&gt;There are also some wasm examples for whisper and &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;. You can either build them with &lt;code&gt;trunk&lt;/code&gt; or try them online: &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;llama2&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm&#34;&gt;T5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm&#34;&gt;Phi-v1.5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/radames/candle-segment-anything-wasm&#34;&gt;Segment Anything Model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For LLaMA2, run the following command to retrieve the weight files and start a test server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd candle-wasm-examples/llama2-c&#xA;wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/model.bin&#xA;wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/tokenizer.json&#xA;trunk serve --release --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then head over to &lt;a href=&#34;http://localhost:8081/&#34;&gt;http://localhost:8081/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ANCHOR: useful_libraries ---&gt; &#xA;&lt;h2&gt;Useful External Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ToluClassics/candle-tutorial&#34;&gt;&lt;code&gt;candle-tutorial&lt;/code&gt;&lt;/a&gt;: A very detailed tutorial showing how to convert a PyTorch model to Candle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EricLBuehler/candle-lora&#34;&gt;&lt;code&gt;candle-lora&lt;/code&gt;&lt;/a&gt;: Efficient and ergonomic LoRA implemenation for Candle. &lt;code&gt;candle-lora&lt;/code&gt; has&lt;br&gt; out-of-the-box LoRA support for many models from Candle, which can be found &lt;a href=&#34;https://github.com/EricLBuehler/candle-lora/tree/master/candle-lora-transformers/examples&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KGrewal1/optimisers&#34;&gt;&lt;code&gt;optimisers&lt;/code&gt;&lt;/a&gt;: A collection of optimisers including SGD with momentum, AdaGrad, AdaDelta, AdaMax, NAdam, RAdam, and RMSprop.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EricLBuehler/candle-vllm&#34;&gt;&lt;code&gt;candle-vllm&lt;/code&gt;&lt;/a&gt;: Efficient platform for inference and serving local LLMs including an OpenAI compatible API server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mokeyish/candle-ext&#34;&gt;&lt;code&gt;candle-ext&lt;/code&gt;&lt;/a&gt;: An extension library to Candle that provides PyTorch functions not currently available in Candle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/floneum/floneum/tree/master/interfaces/kalosm&#34;&gt;&lt;code&gt;kalosm&lt;/code&gt;&lt;/a&gt;: A multi-modal meta-framework in Rust for interfacing with local pre-trained models with support for controlled generation, custom samplers, in-memory vector databases, audio transcription, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EricLBuehler/candle-sampling&#34;&gt;&lt;code&gt;candle-sampling&lt;/code&gt;&lt;/a&gt;: Sampling techniques for Candle.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have an addition to this list, please submit a pull request.&lt;/p&gt; &#xA;&lt;!-- ANCHOR_END: useful_libraries ---&gt; &#xA;&lt;!-- ANCHOR: features ---&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple syntax, looks and feels like PyTorch. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Model training.&lt;/li&gt; &#xA;   &lt;li&gt;Embed user-defined ops/kernels, such as &lt;a href=&#34;https://github.com/huggingface/candle/raw/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152&#34;&gt;flash-attention v2&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Backends. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optimized CPU backend with optional MKL support for x86 and Accelerate for macs.&lt;/li&gt; &#xA;   &lt;li&gt;CUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL.&lt;/li&gt; &#xA;   &lt;li&gt;WASM support, run your models in a browser.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Included models. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Language Models. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;LLaMA v1 and v2.&lt;/li&gt; &#xA;     &lt;li&gt;Falcon.&lt;/li&gt; &#xA;     &lt;li&gt;StarCoder.&lt;/li&gt; &#xA;     &lt;li&gt;Phi v1.5.&lt;/li&gt; &#xA;     &lt;li&gt;Mistral 7b v0.1.&lt;/li&gt; &#xA;     &lt;li&gt;StableLM-3B-4E1T.&lt;/li&gt; &#xA;     &lt;li&gt;Replit-code-v1.5-3B.&lt;/li&gt; &#xA;     &lt;li&gt;Bert.&lt;/li&gt; &#xA;     &lt;li&gt;Yi-6B and Yi-34B.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Quantized LLMs. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Llama 7b, 13b, 70b, as well as the chat and code variants.&lt;/li&gt; &#xA;     &lt;li&gt;Mistral 7b, and 7b instruct.&lt;/li&gt; &#xA;     &lt;li&gt;Zephyr 7b a and b (Mistral based).&lt;/li&gt; &#xA;     &lt;li&gt;OpenChat 3.5 (Mistral based).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Text to text. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;T5 and its variants: FlanT5, UL2, MADLAD400 (translation), CoEdit (Grammar correction).&lt;/li&gt; &#xA;     &lt;li&gt;Marian MT (Machine Translation).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Whisper (multi-lingual support).&lt;/li&gt; &#xA;   &lt;li&gt;Text to image. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Stable Diffusion v1.5, v2.1, XL v1.0.&lt;/li&gt; &#xA;     &lt;li&gt;Wurstchen v2.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Image to text. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;BLIP.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Computer Vision Models. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;DINOv2, ConvMixer, EfficientNet, ResNet, ViT.&lt;/li&gt; &#xA;     &lt;li&gt;yolo-v3, yolo-v8.&lt;/li&gt; &#xA;     &lt;li&gt;Segment-Anything Model (SAM).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;File formats: load models from safetensors, npz, ggml, or PyTorch files.&lt;/li&gt; &#xA; &lt;li&gt;Serverless (on CPU), small and fast deployments.&lt;/li&gt; &#xA; &lt;li&gt;Quantization support using the llama.cpp quantized types.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ANCHOR_END: features ---&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;!-- ANCHOR: cheatsheet ---&gt; &#xA;&lt;p&gt;Cheatsheet:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Using PyTorch&lt;/th&gt; &#xA;   &lt;th&gt;Using Candle&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Creation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.Tensor([[1, 2], [3, 4]])&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Tensor::new(&amp;amp;[[1f32, 2.], [3., 4.]], &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Creation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.zeros((2, 2))&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Tensor::zeros((2, 2), DType::F32, &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Indexing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor[:, :4]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.i((.., ..4))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.view((2, 2))&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.reshape((2, 2))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a.matmul(b)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a.matmul(&amp;amp;b)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arithmetic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a + b&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;amp;a + &amp;amp;b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Device&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to(device=&#34;cuda&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to_device(&amp;amp;Device::new_cuda(0)?)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dtype&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to(dtype=torch.float16)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to_dtype(&amp;amp;DType::F16)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Saving&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.save({&#34;A&#34;: A}, &#34;model.bin&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;candle::safetensors::save(&amp;amp;HashMap::from([(&#34;A&#34;, A)]), &#34;model.safetensors&#34;)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Loading&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;weights = torch.load(&#34;model.bin&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;candle::safetensors::load(&#34;model.safetensors&#34;, &amp;amp;device)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- ANCHOR_END: cheatsheet ---&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-core&#34;&gt;candle-core&lt;/a&gt;: Core ops, devices, and &lt;code&gt;Tensor&lt;/code&gt; struct definition&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-nn/&#34;&gt;candle-nn&lt;/a&gt;: Tools to build real models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/&#34;&gt;candle-examples&lt;/a&gt;: Examples of using the library in realistic settings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-kernels/&#34;&gt;candle-kernels&lt;/a&gt;: CUDA custom kernels&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-datasets/&#34;&gt;candle-datasets&lt;/a&gt;: Datasets and data loaders.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-transformers&#34;&gt;candle-transformers&lt;/a&gt;: transformers-related utilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-flash-attn&#34;&gt;candle-flash-attn&lt;/a&gt;: Flash attention v2 layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-onnx/&#34;&gt;candle-onnx&lt;/a&gt;: ONNX model evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Why should I use Candle?&lt;/h3&gt; &#xA;&lt;p&gt;Candle&#39;s core goal is to &lt;em&gt;make serverless inference possible&lt;/em&gt;. Full machine learning frameworks like PyTorch are very large, which makes creating instances on a cluster slow. Candle allows deployment of lightweight binaries.&lt;/p&gt; &#xA;&lt;p&gt;Secondly, Candle lets you &lt;em&gt;remove Python&lt;/em&gt; from production workloads. Python overhead can seriously hurt performance, and the &lt;a href=&#34;https://www.backblaze.com/blog/the-python-gil-past-present-and-future/&#34;&gt;GIL&lt;/a&gt; is a notorious source of headaches.&lt;/p&gt; &#xA;&lt;p&gt;Finally, Rust is cool! A lot of the HF ecosystem already has Rust crates, like &lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;safetensors&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;tokenizers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other ML frameworks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/coreylowman/dfdx&#34;&gt;dfdx&lt;/a&gt; is a formidable crate, with shapes being included in types. This prevents a lot of headaches by getting the compiler to complain about shape mismatches right off the bat. However, we found that some features still require nightly, and writing code can be a bit daunting for non rust experts.&lt;/p&gt; &lt;p&gt;We&#39;re leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each other.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/burn-rs/burn&#34;&gt;burn&lt;/a&gt; is a general crate that can leverage multiple backends so you can choose the best engine for your workload.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/LaurentMazare/tch-rs.git&#34;&gt;tch-rs&lt;/a&gt; Bindings to the torch library in Rust. Extremely versatile, but they bring in the entire torch library into the runtime. The main contributor of &lt;code&gt;tch-rs&lt;/code&gt; is also involved in the development of &lt;code&gt;candle&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Common Errors&lt;/h3&gt; &#xA;&lt;h4&gt;Missing symbols when compiling with the mkl feature.&lt;/h4&gt; &#xA;&lt;p&gt;If you get some missing symbols when compiling binaries/tests using the mkl or accelerate features, e.g. for mkl you get:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#39;:&#xA;          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#39; collect2: error: ld returned 1 exit status&#xA;&#xA;  = note: some `extern` functions couldn&#39;t be found; some native libraries may need to be installed or have their path specified&#xA;  = note: use the `-l` flag to specify native libraries to link&#xA;  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for accelerate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Undefined symbols for architecture arm64:&#xA;            &#34;_dgemm_&#34;, referenced from:&#xA;                candle_core::accelerate::dgemm::h1b71a038552bcabe in libcandle_core...&#xA;            &#34;_sgemm_&#34;, referenced from:&#xA;                candle_core::accelerate::sgemm::h2cf21c592cba3c47 in libcandle_core...&#xA;          ld: symbol(s) not found for architecture arm64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is likely due to a missing linker flag that was needed to enable the mkl library. You can try adding the following for mkl at the top of your binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate intel_mkl_src;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for accelerate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate accelerate_src;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Cannot run the LLaMA examples: access to source requires login credentials&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error: request error: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is likely because you&#39;re not permissioned for the LLaMA-v2 model. To fix this, you have to register on the huggingface-hub, accept the &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;LLaMA-v2 model conditions&lt;/a&gt;, and set up your authentication token. See issue &lt;a href=&#34;https://github.com/huggingface/candle/issues/350&#34;&gt;#350&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h4&gt;Missing cute/cutlass headers when compiling flash-attn&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;  In file included from kernels/flash_fwd_launch_template.h:11:0,&#xA;                   from kernels/flash_fwd_hdim224_fp16_sm80.cu:5:&#xA;  kernels/flash_fwd_kernel.h:8:10: fatal error: cute/algorithm/copy.hpp: No such file or directory&#xA;   #include &amp;lt;cute/algorithm/copy.hpp&amp;gt;&#xA;            ^~~~~~~~~~~~~~~~~~~~~~~~~&#xA;  compilation terminated.&#xA;  Error: nvcc error while compiling:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/cutlass&#34;&gt;cutlass&lt;/a&gt; is provided as a git submodule so you may want to run the following command to check it in properly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule update --init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Compiling with flash-attention fails&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‚Äò...‚Äô:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is a bug in gcc-11 triggered by the Cuda compiler. To fix this, install a different, supported gcc version - for example gcc-10, and specify the path to the compiler in the CANDLE_NVCC_CCBIN environment variable.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;env CANDLE_NVCC_CCBIN=/usr/lib/gcc/x86_64-linux-gnu/10 cargo ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Linking error on windows when running rustdoc or mdbook tests&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;Couldn&#39;t compile the test.&#xA;---- .\candle-book\src\inference\hub.md - Using_the_hub::Using_in_a_real_model_ (line 50) stdout ----&#xA;error: linking with `link.exe` failed: exit code: 1181&#xA;//very long chain of linking&#xA; = note: LINK : fatal error LNK1181: cannot open input file &#39;windows.0.48.5.lib&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure you link all native libraries that might be located outside a project target, e.g., to run mdbook tests, you should run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mdbook test candle-book -L .\target\debug\deps\ `&#xA;-L native=$env:USERPROFILE\.cargo\registry\src\index.crates.io-6f17d22bba15001f\windows_x86_64_msvc-0.42.2\lib `&#xA;-L native=$env:USERPROFILE\.cargo\registry\src\index.crates.io-6f17d22bba15001f\windows_x86_64_msvc-0.48.5\lib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extremely slow model load time with WSL&lt;/h4&gt; &#xA;&lt;p&gt;This may be caused by the models being loaded from &lt;code&gt;/mnt/c&lt;/code&gt;, more details on &lt;a href=&#34;https://stackoverflow.com/questions/68972448/why-is-wsl-extremely-slow-when-compared-with-native-windows-npm-yarn-processing&#34;&gt;stackoverflow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Tracking down errors&lt;/h4&gt; &#xA;&lt;p&gt;You can set &lt;code&gt;RUST_BACKTRACE=1&lt;/code&gt; to be provided with backtraces when a candle error is generated.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rwf2/Rocket</title>
    <updated>2023-11-26T01:57:48Z</updated>
    <id>tag:github.com,2023-11-26:/rwf2/Rocket</id>
    <link href="https://github.com/rwf2/Rocket" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A web framework for Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rocket&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rwf2/Rocket/actions&#34;&gt;&lt;img src=&#34;https://github.com/rwf2/Rocket/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://rocket.rs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/web-rocket.rs-red.svg?style=flat&amp;amp;label=https&amp;amp;colorB=d33847&#34; alt=&#34;Rocket Homepage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crates.io/crates/rocket&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/rocket.svg?sanitize=true&#34; alt=&#34;Current Crates.io Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://chat.mozilla.org/#/room/#rocket:mozilla.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/style-%23rocket:mozilla.org-blue.svg?style=flat&amp;amp;label=%5Bm%5D&#34; alt=&#34;Matrix: #rocket:mozilla.org&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kiwiirc.com/client/irc.libera.chat/#rocket&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/style-%23rocket-blue.svg?style=flat&amp;amp;label=Libera.Chat&#34; alt=&#34;IRC: #rocket on irc.libera.chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rocket is an async web framework for Rust with a focus on usability, security, extensibility, and speed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;#[macro_use] extern crate rocket;&#xA;&#xA;#[get(&#34;/&amp;lt;name&amp;gt;/&amp;lt;age&amp;gt;&#34;)]&#xA;fn hello(name: &amp;amp;str, age: u8) -&amp;gt; String {&#xA;    format!(&#34;Hello, {} year old named {}!&#34;, age, name)&#xA;}&#xA;&#xA;#[launch]&#xA;fn rocket() -&amp;gt; _ {&#xA;    rocket::build().mount(&#34;/hello&#34;, routes![hello])&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visiting &lt;code&gt;localhost:8000/hello/John/58&lt;/code&gt;, for example, will trigger the &lt;code&gt;hello&lt;/code&gt; route resulting in the string &lt;code&gt;Hello, 58 year old named John!&lt;/code&gt; being sent to the browser. If an &lt;code&gt;&amp;lt;age&amp;gt;&lt;/code&gt; string was passed in that can&#39;t be parsed as a &lt;code&gt;u8&lt;/code&gt;, the route won&#39;t get called, resulting in a 404 error.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Rocket is extensively documented:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocket.rs/overview/&#34;&gt;Overview&lt;/a&gt;: A brief look at what makes Rocket special.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocket.rs/guide/quickstart&#34;&gt;Quickstart&lt;/a&gt;: How to get started as quickly as possible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocket.rs/guide/getting-started&#34;&gt;Getting Started&lt;/a&gt;: How to start your first Rocket project.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocket.rs/guide/&#34;&gt;Guide&lt;/a&gt;: A detailed guide and reference to Rocket.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://api.rocket.rs/rocket/&#34;&gt;API Documentation&lt;/a&gt;: The &#34;rustdocs&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The official community support channels are &lt;a href=&#34;https://chat.mozilla.org/#/room/#rocket:mozilla.org&#34;&gt;&lt;code&gt;#rocket:mozilla.org&lt;/code&gt;&lt;/a&gt; on Matrix and the bridged &lt;a href=&#34;https://kiwiirc.com/client/irc.libera.chat/#rocket&#34;&gt;&lt;code&gt;#rocket&lt;/code&gt;&lt;/a&gt; IRC channel on Libera.Chat at &lt;code&gt;irc.libera.chat&lt;/code&gt;. We recommend joining us on &lt;a href=&#34;https://chat.mozilla.org/#/room/#rocket:mozilla.org&#34;&gt;Matrix via Element&lt;/a&gt;. If your prefer IRC, you can join via the &lt;a href=&#34;https://kiwiirc.com/client/irc.libera.chat/#rocket&#34;&gt;Kiwi IRC client&lt;/a&gt; or a client of your own.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;An extensive number of examples are provided in the &lt;code&gt;examples/&lt;/code&gt; directory. Each example can be compiled and run with Cargo. For instance, the following sequence of commands builds and runs the &lt;code&gt;Hello, world!&lt;/code&gt; example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd examples/hello&#xA;cargo run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see &lt;code&gt;Hello, world!&lt;/code&gt; by visiting &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building and Testing&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;core&lt;/code&gt; directory contains the three core libraries: &lt;code&gt;lib&lt;/code&gt;, &lt;code&gt;codegen&lt;/code&gt;, and &lt;code&gt;http&lt;/code&gt; published as &lt;code&gt;rocket&lt;/code&gt;, &lt;code&gt;rocket_codegen&lt;/code&gt; and &lt;code&gt;rocket_http&lt;/code&gt;, respectively. The latter two are implementations details and are reexported from &lt;code&gt;rocket&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;Rocket&#39;s complete test suite can be run with &lt;code&gt;./scripts/test.sh&lt;/code&gt; from the root of the source tree. The script builds and tests all libraries and examples in all configurations. It accepts the following flags:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--examples&lt;/code&gt;: tests all examples in &lt;code&gt;examples/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--contrib&lt;/code&gt;: tests each &lt;code&gt;contrib&lt;/code&gt; library and feature individually&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--core&lt;/code&gt;: tests each &lt;code&gt;core/lib&lt;/code&gt; feature individually&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--benchmarks&lt;/code&gt;: runs all benchmarks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--all&lt;/code&gt;: runs all tests in all configurations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, a &lt;code&gt;+${toolchain}&lt;/code&gt; flag, where &lt;code&gt;${toolchain}&lt;/code&gt; is a valid &lt;code&gt;rustup&lt;/code&gt; toolchain string, can be passed as the first parameter. The flag is forwarded to &lt;code&gt;cargo&lt;/code&gt; commands. Any other extra parameters are passed directly to &lt;code&gt;cargo&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To test crates individually, simply run &lt;code&gt;cargo test --all-features&lt;/code&gt; in the crate&#39;s directory.&lt;/p&gt; &#xA;&lt;h3&gt;Codegen Testing&lt;/h3&gt; &#xA;&lt;p&gt;Code generation diagnostics are tested using &lt;a href=&#34;https://docs.rs/trybuild/1&#34;&gt;&lt;code&gt;trybuild&lt;/code&gt;&lt;/a&gt;; tests can be found in the &lt;code&gt;codegen/tests/ui-fail&lt;/code&gt; directories of respective &lt;code&gt;codegen&lt;/code&gt; crates. Each test is symlinked into sibling &lt;code&gt;ui-fail-stable&lt;/code&gt; and &lt;code&gt;ui-fail-nightly&lt;/code&gt; directories which contain the expected error output for stable and nightly compilers, respectively. To update codegen test UI output, run a codegen test suite with &lt;code&gt;TRYBUILD=overwrite&lt;/code&gt; and inspect the &lt;code&gt;diff&lt;/code&gt; of &lt;code&gt;.std*&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;h3&gt;API Docs&lt;/h3&gt; &#xA;&lt;p&gt;API documentation is built with &lt;code&gt;./scripts/mk-docs.sh&lt;/code&gt;. The resulting assets are uploaded to &lt;a href=&#34;https://api.rocket.rs/&#34;&gt;api.rocket.rs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Documentation for a released version &lt;code&gt;${x}&lt;/code&gt; can be found at &lt;code&gt;https://api.rocket.rs/v${x}&lt;/code&gt; and &lt;code&gt;https://rocket.rs/v${x}&lt;/code&gt;. For instance, the documentation for &lt;code&gt;0.4&lt;/code&gt; can be found at &lt;a href=&#34;https://api.rocket.rs/v0.4&#34;&gt;https://api.rocket.rs/v0.4&lt;/a&gt; and &lt;a href=&#34;https://rocket.rs/v0.4&#34;&gt;https://rocket.rs/v0.4&lt;/a&gt;. Documentation for unreleased versions in branch &lt;code&gt;${branch}&lt;/code&gt; be found at &lt;code&gt;https://api.rocket.rs/${branch}&lt;/code&gt; and &lt;code&gt;https://rocket.rs/${branch}&lt;/code&gt;. For instance, the documentation for the &lt;code&gt;master&lt;/code&gt; branch can be found at &lt;a href=&#34;https://api.rocket.rs/master&#34;&gt;https://api.rocket.rs/master&lt;/a&gt; and &lt;a href=&#34;https://rocket.rs/master&#34;&gt;https://rocket.rs/master&lt;/a&gt;. Documentation for unreleased branches is updated periodically.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are absolutely, positively welcome and encouraged! Contributions come in many forms. You could:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Submit a feature request or bug report as an &lt;a href=&#34;https://github.com/rwf2/Rocket/issues&#34;&gt;issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ask for improved documentation as an &lt;a href=&#34;https://github.com/rwf2/Rocket/issues&#34;&gt;issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Comment on &lt;a href=&#34;https://github.com/rwf2/Rocket/issues?q=is%3Aissue+is%3Aopen+label%3A%22feedback+wanted%22&#34;&gt;issues that require feedback&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Contribute code via &lt;a href=&#34;https://github.com/rwf2/Rocket/pulls&#34;&gt;pull requests&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We aim to keep Rocket&#39;s code quality at the highest level. This means that any code you contribute must be:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Commented:&lt;/strong&gt; Complex and non-obvious functionality must be properly commented.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Documented:&lt;/strong&gt; Public items &lt;em&gt;must&lt;/em&gt; have doc comments with examples, if applicable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Styled:&lt;/strong&gt; Your code&#39;s style should match the existing and surrounding code style.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple:&lt;/strong&gt; Your code should accomplish its task as simply and idiomatically as possible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tested:&lt;/strong&gt; You must write (and pass) convincing tests for any new functionality.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focused:&lt;/strong&gt; Your code should do what it&#39;s supposed to and nothing more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All pull requests are code reviewed and tested by the CI. Note that unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Rocket by you shall be dual licensed under the MIT License and Apache License, Version 2.0, without any additional terms or conditions.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Rocket is licensed under either of the following, at your option:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apache License, Version 2.0, (&lt;a href=&#34;https://raw.githubusercontent.com/rwf2/Rocket/master/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;MIT License (&lt;a href=&#34;https://raw.githubusercontent.com/rwf2/Rocket/master/LICENSE-MIT&#34;&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;https://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Rocket website source is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/rwf2/Rocket/master/site#license&#34;&gt;separate terms&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>