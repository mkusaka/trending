<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-01T01:44:24Z</updated>
  <subtitle>Monthly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zackriya-Solutions/meeting-minutes</title>
    <updated>2025-05-01T01:44:24Z</updated>
    <id>tag:github.com,2025-05-01:/Zackriya-Solutions/meeting-minutes</id>
    <link href="https://github.com/Zackriya-Solutions/meeting-minutes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A free and open source, self hosted Ai based live meeting note taker and minutes summary generator that can completely run in your Local device (Mac OS and windows OS Support added. Working on adding linux support soon) https://meetily.zackriya.com/&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; style=&#34;border-bottom: none&#34;&gt; &#xA; &lt;h1&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zackriya-Solutions/meeting-minutes/main/docs/6.png&#34; width=&#34;400&#34; style=&#34;border-radius: 10px;&#34;&gt; &lt;br&gt; Meetily - AI-Powered Meeting Assistant &lt;/h1&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Pre_Release-v0.0.3-brightgreen&#34; alt=&#34;Pre-Release&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Stars-1000+-red&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Supported_OS-macOS,_Windows-yellow&#34; alt=&#34;Supported OS&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;h3&gt; &lt;br&gt; Open source Ai Assistant for taking meeting notes &lt;/h3&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://meetily.zackriya.com&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://in.linkedin.com/company/zackriya-solutions&#34;&gt;&lt;b&gt;Author&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/crRymMQBFH&#34;&gt;&lt;b&gt;Discord Channel&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;An AI-Powered Meeting Assistant that captures live meeting audio, transcribes it in real-time, and generates summaries while ensuring user privacy. Perfect for teams who want to focus on discussions while automatically capturing and organizing meeting content without the need for external servers or complex infrastructure.&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zackriya-Solutions/meeting-minutes/main/docs/demo_small.gif&#34; width=&#34;650&#34; alt=&#34;Meetily Demo&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://youtu.be/5k_Q5Wlahuk&#34;&gt;View full Demo Video&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;An AI-powered meeting assistant that captures live meeting audio, transcribes it in real-time, and generates summaries while ensuring user privacy. Perfect for teams who want to focus on discussions while automatically capturing and organizing meeting content.&lt;/p&gt; &#xA;&lt;h3&gt;Why?&lt;/h3&gt; &#xA;&lt;p&gt;While there are many meeting transcription tools available, this solution stands out by offering:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Privacy First&lt;/strong&gt;: All processing happens locally on your device&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost Effective&lt;/strong&gt;: Uses open-source AI models instead of expensive APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Works offline, supports multiple meeting platforms&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Self-host and modify for your specific needs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intelligent&lt;/strong&gt;: Built-in knowledge graph for semantic search across meetings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;✅ Modern, responsive UI with real-time updates&lt;/p&gt; &#xA;&lt;p&gt;✅ Real-time audio capture (microphone + system audio)&lt;/p&gt; &#xA;&lt;p&gt;✅ Live transcription using Whisper.cpp&lt;/p&gt; &#xA;&lt;p&gt;🚧 Speaker diarization&lt;/p&gt; &#xA;&lt;p&gt;✅ Local processing for privacy&lt;/p&gt; &#xA;&lt;p&gt;✅ Packaged the app for macOS and Windows&lt;/p&gt; &#xA;&lt;p&gt;🚧 Export to Markdown/PDF&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We have a Rust-based implementation that explores better performance and native integration. It currently implements:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;✅ Real-time audio capture from both microphone and system audio&lt;/li&gt; &#xA;  &lt;li&gt;✅ Live transcription using locally-running Whisper&lt;/li&gt; &#xA;  &lt;li&gt;✅ Speaker diarization&lt;/li&gt; &#xA;  &lt;li&gt;✅ Rich text editor for notes&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We are currently working on:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;✅ Export to Markdown/PDF&lt;/li&gt; &#xA;  &lt;li&gt;✅ Export to HTML&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Release 0.0.3&lt;/h2&gt; &#xA;&lt;p&gt;A new release is available!&lt;/p&gt; &#xA;&lt;p&gt;Please check out the release &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What&#39;s New&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows Support&lt;/strong&gt;: Fixed audio capture issues on Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improved Error Handling&lt;/strong&gt;: Better error handling and logging for audio devices&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced Device Detection&lt;/strong&gt;: More robust audio device detection across platforms&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows Installers&lt;/strong&gt;: Added both .exe and .msi installers for Windows&lt;/li&gt; &#xA; &lt;li&gt;Transcription quality is improved&lt;/li&gt; &#xA; &lt;li&gt;Bug fixes and improvements for frontend&lt;/li&gt; &#xA; &lt;li&gt;Better backend app build process&lt;/li&gt; &#xA; &lt;li&gt;Improved documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What would be next?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Database connection to save meeting minutes&lt;/li&gt; &#xA; &lt;li&gt;Improve summarization quality for smaller LLM models&lt;/li&gt; &#xA; &lt;li&gt;Add download options for meeting transcriptions&lt;/li&gt; &#xA; &lt;li&gt;Add download option for summary&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Smaller LLMs can hallucinate, making summarization quality poor; Please use model above 32B parameter size&lt;/li&gt; &#xA; &lt;li&gt;Backend build process requires CMake, C++ compiler, etc. Making it harder to build&lt;/li&gt; &#xA; &lt;li&gt;Backend build process requires Python 3.10 or newer&lt;/li&gt; &#xA; &lt;li&gt;Frontend build process requires Node.js&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Integration&lt;/h2&gt; &#xA;&lt;p&gt;The backend supports multiple LLM providers through a unified interface. Current implementations include:&lt;/p&gt; &#xA;&lt;h3&gt;Supported Providers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt; (Claude models)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq&lt;/strong&gt; (Llama3.2 90 B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; (Local models that supports function calling)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Create &lt;code&gt;.env&lt;/code&gt; file with your API keys:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;# Required for Anthropic&#xA;ANTHROPIC_API_KEY=your_key_here  &#xA;&#xA;# Required for Groq &#xA;GROQ_API_KEY=your_key_here&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;System Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zackriya-Solutions/meeting-minutes/main/docs/HighLevel.jpg&#34; alt=&#34;High Level Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Core Components&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audio Capture Service&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Real-time microphone/system audio capture&lt;/li&gt; &#xA;   &lt;li&gt;Audio preprocessing pipeline&lt;/li&gt; &#xA;   &lt;li&gt;Built with Rust (experimental) and Python&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transcription Engine&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Whisper.cpp for local transcription&lt;/li&gt; &#xA;   &lt;li&gt;Supports multiple model sizes (tiny-&amp;gt;large)&lt;/li&gt; &#xA;   &lt;li&gt;GPU-accelerated processing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Orchestrator&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unified interface for multiple providers&lt;/li&gt; &#xA;   &lt;li&gt;Automatic fallback handling&lt;/li&gt; &#xA;   &lt;li&gt;Chunk processing with overlap&lt;/li&gt; &#xA;   &lt;li&gt;Model configuration:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Services&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ChromaDB&lt;/strong&gt;: Vector store for transcript embeddings&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;SQLite&lt;/strong&gt;: Process tracking and metadata storage&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;API Layer&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FastAPI endpoints: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;POST /upload&lt;/li&gt; &#xA;     &lt;li&gt;POST /process&lt;/li&gt; &#xA;     &lt;li&gt;GET /summary/{id}&lt;/li&gt; &#xA;     &lt;li&gt;DELETE /summary/{id}&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deployment Architecture&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Tauri app + Next.js (packaged executables)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python FastAPI: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Transcript workers&lt;/li&gt; &#xA;   &lt;li&gt;LLM inference&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.js 18+&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10+&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg&lt;/li&gt; &#xA; &lt;li&gt;Rust 1.65+ (for experimental features)&lt;/li&gt; &#xA; &lt;li&gt;Cmake 3.22+ (for building the frontend)&lt;/li&gt; &#xA; &lt;li&gt;For Windows: Visual Studio Build Tools with C++ development workload&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;1. Frontend Setup&lt;/h3&gt; &#xA;&lt;h4&gt;Run packaged version&lt;/h4&gt; &#xA;&lt;p&gt;Go to the &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases&#34;&gt;releases page&lt;/a&gt; and download the latest version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For Windows:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download either the &lt;code&gt;.exe&lt;/code&gt; installer or &lt;code&gt;.msi&lt;/code&gt; package&lt;/li&gt; &#xA; &lt;li&gt;Once the installer is downloaded, double-click the executable file to run it&lt;/li&gt; &#xA; &lt;li&gt;Windows will ask if you want to run untrusted apps, click &#34;More info&#34; and choose &#34;Run anyway&#34;&lt;/li&gt; &#xA; &lt;li&gt;Follow the installation wizard to complete the setup&lt;/li&gt; &#xA; &lt;li&gt;The application will be installed and available on your desktop&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the &lt;code&gt;dmg_darwin_arch64.zip&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Extract the file&lt;/li&gt; &#xA; &lt;li&gt;Double-click the &lt;code&gt;.dmg&lt;/code&gt; file inside the extracted folder&lt;/li&gt; &#xA; &lt;li&gt;Drag the application to your Applications folder&lt;/li&gt; &#xA; &lt;li&gt;Execute the following command in terminal to remove the quarantine attribute: &lt;pre&gt;&lt;code&gt;xattr -c /Applications/meeting-minutes-frontend.app&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Provide necessary permissions for audio capture and microphone access.&lt;/p&gt; &#xA;&lt;h4&gt;Dev run&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Navigate to frontend directory&#xA;cd frontend&#xA;&#xA;# Give execute permissions to clean_build.sh&#xA;chmod +x clean_build.sh&#xA;&#xA;# run clean_build.sh&#xA;./clean_build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Backend Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/Zackriya-Solutions/meeting-minutes.git&#xA;cd meeting-minutes/backend&#xA;&#xA;# Create and activate virtual environment&#xA;# On macOS/Linux:&#xA;python -m venv venv&#xA;source venv/bin/activate&#xA;&#xA;# On Windows:&#xA;python -m venv venv&#xA;.\venv\Scripts\activate&#xA;&#xA;# Install dependencies&#xA;pip install -r requirements.txt&#xA;&#xA;# Add environment file with API keys&#xA;# On macOS/Linux:&#xA;echo -e &#34;ANTHROPIC_API_KEY=your_api_key\nGROQ_API_KEY=your_api_key&#34; | tee .env&#xA;&#xA;# On Windows (PowerShell):&#xA;&#34;ANTHROPIC_API_KEY=your_api_key`nGROQ_API_KEY=your_api_key&#34; | Out-File -FilePath .env -Encoding utf8&#xA;&#xA;# Configure environment variables for Groq&#xA;# On macOS/Linux:&#xA;export GROQ_API_KEY=your_groq_api_key&#xA;&#xA;# On Windows (PowerShell):&#xA;$env:GROQ_API_KEY=&#34;your_groq_api_key&#34;&#xA;&#xA;# Build dependencies&#xA;# On macOS/Linux:&#xA;chmod +x build_whisper.sh&#xA;./build_whisper.sh&#xA;&#xA;# On Windows:&#xA;.\build_whisper.bat&#xA;&#xA;# Start backend servers&#xA;# On macOS/Linux:&#xA;./clean_start_backend.sh&#xA;&#xA;# On Windows:&#xA;.\start_with_output.ps1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development Guidelines&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the established project structure&lt;/li&gt; &#xA; &lt;li&gt;Write tests for new features&lt;/li&gt; &#xA; &lt;li&gt;Document API changes&lt;/li&gt; &#xA; &lt;li&gt;Use type hints in Python code&lt;/li&gt; &#xA; &lt;li&gt;Follow ESLint configuration for JavaScript/TypeScript&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository&lt;/li&gt; &#xA; &lt;li&gt;Create a feature branch&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License - Feel free to use this project for your own purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Introducing Subscription&lt;/h2&gt; &#xA;&lt;p&gt;We are planning to add a subscription option so that you don&#39;t have to run the backend on your own server. This will help you scale better and run the service 24/7. This is based on a few requests we received. If you are interested, please fill out the form &lt;a href=&#34;http://zackriya.com/aimeeting/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Last updated: March 3, 2025&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Zackriya-Solutions/meeting-minutes&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Zackriya-Solutions/meeting-minutes&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/perfetto</title>
    <updated>2025-05-01T01:44:24Z</updated>
    <id>tag:github.com,2025-05-01:/google/perfetto</id>
    <link href="https://github.com/google/perfetto" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Performance instrumentation and tracing for Android, Linux and Chrome&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Perfetto - System profiling, app tracing and trace analysis&lt;/h1&gt; &#xA;&lt;p&gt;Perfetto is a production-grade open-source stack for performance instrumentation and trace analysis. It offers services and libraries and for recording system-level and app-level traces, native + java heap profiling, a library for analyzing traces using SQL and a web-based UI to visualize and explore multi-GB traces.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://perfetto.dev/docs&#34;&gt;https://perfetto.dev/docs&lt;/a&gt; or the /docs/ directory for documentation.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/BitNet</title>
    <updated>2025-05-01T01:44:24Z</updated>
    <id>tag:github.com,2025-05-01:/microsoft/BitNet</id>
    <link href="https://github.com/microsoft/BitNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/version-1.0-blue&#34; alt=&#34;version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png&#34; alt=&#34;BitNet Model on Hugging Face&#34; width=&#34;800&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try it out via this &lt;a href=&#34;https://bitnet-demo.azurewebsites.net/&#34;&gt;demo&lt;/a&gt;, or &lt;a href=&#34;https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source&#34;&gt;build and run&lt;/a&gt; it on your own CPU.&lt;/p&gt; &#xA;&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU (with NPU and GPU support coming next).&lt;/p&gt; &#xA;&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href=&#34;https://arxiv.org/abs/2410.16144&#34;&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg&#34; alt=&#34;m2_performance&#34; width=&#34;800&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg&#34; alt=&#34;m2_performance&#34; width=&#34;800&#34;&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&#34;&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;04/14/2025 &lt;a href=&#34;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&#34;&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/NEW-red&#34; alt=&#34;NEW&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;02/18/2025 &lt;a href=&#34;https://arxiv.org/abs/2502.11880&#34;&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;11/08/2024 &lt;a href=&#34;https://arxiv.org/abs/2411.04965&#34;&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/21/2024 &lt;a href=&#34;https://arxiv.org/abs/2410.16144&#34;&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; &#xA; &lt;li&gt;03/21/2024 &lt;a href=&#34;https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf&#34;&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02/27/2024 &lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/17/2023 &lt;a href=&#34;https://arxiv.org/abs/2310.11453&#34;&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#39;s kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href=&#34;https://github.com/microsoft/T-MAC/&#34;&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; &#xA;&lt;h2&gt;Official Models&lt;/h2&gt; &#xA;&lt;table&gt;  &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;Kernel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;I2_S&lt;/th&gt; &#xA;   &lt;th&gt;TL1&lt;/th&gt; &#xA;   &lt;th&gt;TL2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&#34;&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;2.4B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;❗️&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt;  &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;Kernel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;I2_S&lt;/th&gt; &#xA;   &lt;th&gt;TL1&lt;/th&gt; &#xA;   &lt;th&gt;TL2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-large&#34;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;0.7B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&#34;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;3.3B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&#34;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;8.0B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&#34;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;1B-10B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; &#xA; &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; &#xA; &lt;li&gt;clang&amp;gt;=18 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Desktop-development with C++&lt;/li&gt; &#xA;     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; &#xA;     &lt;li&gt;Git for Windows&lt;/li&gt; &#xA;     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; &#xA;     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href=&#34;https://apt.llvm.org/&#34;&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c &#34;$(wget -O - https://apt.llvm.org/llvm.sh)&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;conda (highly recommend)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/microsoft/BitNet.git&#xA;cd BitNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# (Recommended) Create a new conda environment&#xA;conda create -n bitnet-cpp python=3.9&#xA;conda activate bitnet-cpp&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Build the project&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Manually download the model and run with local path&#xA;huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T&#xA;python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&#xA;usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]&#xA;                    [--use-pretuned]&#xA;&#xA;Setup the environment for running inference&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}&#xA;                        Model used for inference&#xA;  --model-dir MODEL_DIR, -md MODEL_DIR&#xA;                        Directory to save/load the model&#xA;  --log-dir LOG_DIR, -ld LOG_DIR&#xA;                        Directory to save the logging info&#xA;  --quant-type {i2_s,tl1}, -q {i2_s,tl1}&#xA;                        Quantization type&#xA;  --quant-embd          Quantize the embeddings to f16&#xA;  --use-pretuned, -p    Use the pretuned kernel parameters&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Basic usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run inference with the quantized model&#xA;python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &#34;You are a helpful assistant&#34; -cnv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&#xA;usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]&#xA;&#xA;Run inference&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  -m MODEL, --model MODEL&#xA;                        Path to model file&#xA;  -n N_PREDICT, --n-predict N_PREDICT&#xA;                        Number of tokens to predict when generating text&#xA;  -p PROMPT, --prompt PROMPT&#xA;                        Prompt to generate text from&#xA;  -t THREADS, --threads THREADS&#xA;                        Number of threads to use&#xA;  -c CTX_SIZE, --ctx-size CTX_SIZE&#xA;                        Size of the prompt context&#xA;  -temp TEMPERATURE, --temperature TEMPERATURE&#xA;                        Temperature, a hyperparameter that controls the randomness of the generated text&#xA;  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)&#xA;                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)&#xA;&lt;/pre&gt; &#xA;&lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  &#xA;   &#xA;Setup the environment for running the inference  &#xA;   &#xA;required arguments:  &#xA;  -m MODEL, --model MODEL  &#xA;                        Path to the model file. &#xA;   &#xA;optional arguments:  &#xA;  -h, --help  &#xA;                        Show this help message and exit. &#xA;  -n N_TOKEN, --n-token N_TOKEN  &#xA;                        Number of generated tokens. &#xA;  -p N_PROMPT, --n-prompt N_PROMPT  &#xA;                        Prompt to generate text from. &#xA;  -t THREADS, --threads THREADS  &#xA;                        Number of threads to use. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a brief explanation of each argument:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; &#xA;&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M&#xA;&#xA;# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate&#xA;python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;FAQ (Frequently Asked Questions)📌&lt;/h3&gt; &#xA;&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href=&#34;https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323&#34;&gt;commit&lt;/a&gt; in the &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python/issues/1942&#34;&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; &#xA;&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;clang -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;clang&#39; is not recognized as an internal or external command, operable program or batch file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; &#xA;&lt;p&gt;• If you are using Command Prompt, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat&#34; -startdir=none -arch=x64 -host_arch=x64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;• If you are using Windows PowerShell, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Import-Module &#34;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&#34; Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments &#34;-arch=x64 -host_arch=x64&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</summary>
  </entry>
</feed>