<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-02T01:30:16Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dail8859/NotepadNext</title>
    <updated>2022-07-02T01:30:16Z</updated>
    <id>tag:github.com,2022-07-02:/dail8859/NotepadNext</id>
    <link href="https://github.com/dail8859/NotepadNext" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A cross-platform, reimplementation of Notepad++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Notepad Next&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/dail8859/NotepadNext/workflows/Build%20Notepad%20Next/badge.svg?sanitize=true&#34; alt=&#34;Build Notepad Next&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A cross-platform, reimplementation of Notepad++.&lt;/p&gt; &#xA;&lt;p&gt;Though the application overall is stable and usable, it should not be considered safe for critically important work.&lt;/p&gt; &#xA;&lt;p&gt;There are numerous bugs and half working implementations. Pull requests are greatly appreciated.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dail8859/NotepadNext/master/doc/screenshot.png&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Packages are available for Windows, Linux, and MacOS.&lt;/p&gt; &#xA;&lt;p&gt;Windows packages are available as an installer or a stand-alone zip file on the &lt;a href=&#34;https://github.com/dail8859/NotepadNext/releases&#34;&gt;release&lt;/a&gt; page. The installer provides additional components such as an auto-updater and Windows context menu integration.&lt;/p&gt; &#xA;&lt;p&gt;Linux packages can be obtained by downloading the stand-alone AppImage on the &lt;a href=&#34;https://github.com/dail8859/NotepadNext/releases&#34;&gt;release&lt;/a&gt; page or by installing the &lt;a href=&#34;https://flathub.org/apps/details/com.github.dail8859.NotepadNext&#34;&gt;flatpak&lt;/a&gt; by executing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;flatpak install flathub com.github.dail8859.NotepadNext&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MacOS disk images can be downloaded from the &lt;a href=&#34;https://github.com/dail8859/NotepadNext/releases&#34;&gt;release&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;Current development is done using Visual Studio 2019 and Qt v5.15 on Windows. This is also known to build successfully on various Linux distributions and macOS. Other platforms/compilers should be usable with minor modifications.&lt;/p&gt; &#xA;&lt;p&gt;If you are familiar with building C++ Qt desktop applications with Qt Creator, then this should be as simple as opening &lt;code&gt;src/NotepadNext.pro&lt;/code&gt; and build/run the project.&lt;/p&gt; &#xA;&lt;p&gt;If you are new to building C++ Qt desktop applications, there is a more detailed guide &lt;a href=&#34;https://raw.githubusercontent.com/dail8859/NotepadNext/master/doc/Building.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This code is released under the &lt;a href=&#34;http://www.gnu.org/licenses/gpl-3.0.txt&#34;&gt;GNU General Public License version 3&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alibaba/MNN</title>
    <updated>2022-07-02T01:30:16Z</updated>
    <id>tag:github.com,2022-07-02:/alibaba/MNN</id>
    <link href="https://github.com/alibaba/MNN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/banner.png&#34; alt=&#34;MNN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/README_CN.md&#34;&gt;中文版本&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.mnn.zone&#34;&gt;MNN Homepage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;MNN is a highly efficient and lightweight deep learning framework. It supports inference and training of deep learning models, and has industry leading performance for inference and training on-device. At present, MNN has been integrated in more than 30 apps of Alibaba Inc, such as Taobao, Tmall, Youku, Dingtalk, Xianyu and etc., covering more than 70 usage scenarios such as live broadcast, short video capture, search recommendation, product searching by image, interactive marketing, equity distribution, security risk control. In addition, MNN is also used on embedded devices, such as IoT.&lt;/p&gt; &#xA;&lt;p&gt;The design principles and performance data of MNN has been published in an MLSys 2020 paper &lt;a href=&#34;https://arxiv.org/pdf/2002.12418.pdf&#34;&gt;here&lt;/a&gt;. Please cite MNN in your publications if it helps your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{alibaba2020mnn,&#xA;  author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},&#xA;  title = {MNN: A Universal and Efficient Inference Engine},&#xA;  booktitle = {MLSys},&#xA;  year = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/workflow.png&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and Workbench&lt;/h2&gt; &#xA;&lt;p&gt;MNN&#39;s docs are in placed in &lt;a href=&#34;https://www.yuque.com/mnn/en&#34;&gt;Yuque docs here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MNN Workbench could be downloaded from &lt;a href=&#34;http://www.mnn.zone&#34;&gt;MNN&#39;s homepage&lt;/a&gt;, which provides pretrained models, visualized training tools, and one-click deployment of models to devices.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;Lightweight&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimized for devices, no dependencies, can be easily deployed to mobile devices and a variety of embedded devices.&lt;/li&gt; &#xA; &lt;li&gt;iOS platform: static library size will full option for armv7+arm64 platforms is about 12MB, size increase of linked executables is about 2M.&lt;/li&gt; &#xA; &lt;li&gt;Android platform: core so size is about 800KB (armv7a - c++_shared).&lt;/li&gt; &#xA; &lt;li&gt;Use MNN_BUILD_MINI can reduce package size about 25% , with limit of fix model input size&lt;/li&gt; &#xA; &lt;li&gt;Support FP16 / Int8 qunatize, can reduce model size 50%-70%&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Versatility&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports &lt;code&gt;Tensorflow&lt;/code&gt;, &lt;code&gt;Caffe&lt;/code&gt;, &lt;code&gt;ONNX&lt;/code&gt;,&lt;code&gt;Torchscripts&lt;/code&gt; and supports common neural networks such as &lt;code&gt;CNN&lt;/code&gt;, &lt;code&gt;RNN&lt;/code&gt;, &lt;code&gt;GAN&lt;/code&gt;, &lt;code&gt;Transformork&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports AI model with multi-inputs or multi-outputs, every kind of dimenstion format, dynamic inputs, controlflow.&lt;/li&gt; &#xA; &lt;li&gt;MNN supports approximate full OPs used for AI Model. The converter supports 178 &lt;code&gt;Tensorflow&lt;/code&gt; OPs, 52 &lt;code&gt;Caffe&lt;/code&gt; OPs, 163 &lt;code&gt;Torchscripts&lt;/code&gt; OPs, 158 &lt;code&gt;ONNX&lt;/code&gt; OPs.&lt;/li&gt; &#xA; &lt;li&gt;Supports iOS 8.0+, Android 4.3+ and embedded devices with POSIX interface.&lt;/li&gt; &#xA; &lt;li&gt;Supports hybrid computing on multiple devices. Currently supports CPU and GPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;High performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implements core computing with lots of optimized assembly code to make full use of the ARM / x64 CPU.&lt;/li&gt; &#xA; &lt;li&gt;Use Metal / OpenCL / Vulkan to support GPU inference on mobile.&lt;/li&gt; &#xA; &lt;li&gt;Use CUDA and tensorcore to support NVIDIA GPU for better performance&lt;/li&gt; &#xA; &lt;li&gt;Convolution and transposition convolution algorithms are efficient and stable. The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3,4x4,5x5,6x6,7x7.&lt;/li&gt; &#xA; &lt;li&gt;Twice speed increase for the new architecture ARM v8.2 with FP16 half-precision calculation support. 2.5 faster to use sdot for ARM v8.2 and VNNI.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ease of use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Support use MNN&#39;s OP to do numerical calculating like numpy.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Support lightweight image process module like OpenCV, which is only 100k.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Support build model and train it on PC / mobile.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MNN Python API helps ML engineers to easily use MNN to inference, train, process image, without dipping their toes in C++ code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;S ：Support and work well, deeply optimized, recommend to use&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A ：Support and work well, can use&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;B ：Support but has bug or not optimized, no recommend to use&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;C ：Not Support&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Architecture / Precision&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Normal&lt;/th&gt; &#xA;   &lt;th&gt;FP16&lt;/th&gt; &#xA;   &lt;th&gt;BF16&lt;/th&gt; &#xA;   &lt;th&gt;Int8&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;Native&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;x86/x64-SSE4.1&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;x86/x64-AVX2&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;x86/x64-AVX512&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARMv7a&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;S (ARMv8.2)&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARMv8&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;S (ARMv8.2)&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;   &lt;td&gt;OpenCL&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vulkan&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Metal&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;A&lt;/td&gt; &#xA;   &lt;td&gt;S&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NPU&lt;/td&gt; &#xA;   &lt;td&gt;CoreML&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HIAI&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;   &lt;td&gt;B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;MNN can be divided into two parts: Inference Engine and Tools.&lt;/p&gt; &#xA;&lt;h3&gt;Inference Engine&lt;/h3&gt; &#xA;&lt;p&gt;The input of Inference Engine, AI model is a Directed Acyclic Graph(DAG), each node in model is an operator, which describe a kind of tensor compute function. Inference Engine will load and execute the graph. It can seperate into schedule and execute: &lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/runflow.png&#34; alt=&#34;runflow.png&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Schedule: Load Graph and Pretreat it &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Decompose OP, reduce kinds of OPs&lt;/li&gt; &#xA;   &lt;li&gt;Search best compute stratagy&lt;/li&gt; &#xA;   &lt;li&gt;Find best resource allocation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Execute: Implete OP, use algorithm and hardware feature to optimize &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Algorithm: Winograd Convolution, Strassen Matrix Multiply, Low Precision Compute&lt;/li&gt; &#xA;   &lt;li&gt;Hardware: SIMD for CPU (SSE/NEON/AVX), GPU API (OpenCL / CUDA / Metal)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MNN-Converter: Convert other model to MNN model, such as Tensorflow(lite), Caffe, ONNX, Torchscripts. And do graph optimization to reduce computation.&lt;/li&gt; &#xA; &lt;li&gt;MNN-Compress: Compress model to reduce size and increase performance / speed&lt;/li&gt; &#xA; &lt;li&gt;MNN-Express: Support model with controlflow, use MNN&#39;s OP to do general-purpose compute.&lt;/li&gt; &#xA; &lt;li&gt;MNN-CV: A OpenCV liked library, but based on MNN and then much more lightweight.&lt;/li&gt; &#xA; &lt;li&gt;MNN-Train: Support train MNN model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Discuss and Get Help From MNN Community&lt;/h2&gt; &#xA;&lt;p&gt;The group discussions are predominantly Chinese. But we welcome and will help English speakers.&lt;/p&gt; &#xA;&lt;p&gt;Dingtalk discussion groups:&lt;/p&gt; &#xA;&lt;p&gt;Group #1 (Full): 23329087&lt;/p&gt; &#xA;&lt;p&gt;Group #2 (Full): 23350225&lt;/p&gt; &#xA;&lt;p&gt;Group #3: &lt;a href=&#34;https://h5.dingtalk.com/circle/healthCheckin.html?dtaction=os&amp;amp;corpId=ding8989a1d6ae6ef130b177420cc0e366ea&amp;amp;f0c81=1b93a&amp;amp;cbdbhh=qwertyuiop&#34;&gt;https://h5.dingtalk.com/circle/healthCheckin.html?dtaction=os&amp;amp;corpId=ding8989a1d6ae6ef130b177420cc0e366ea&amp;amp;f0c81=1b93a&amp;amp;cbdbhh=qwertyuiop&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache 2.0&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MNN participants: Taobao Technology Department, Search Engineering Team, DAMO Team, Youku and other Alibaba Group employees.&lt;/p&gt; &#xA;&lt;p&gt;MNN refers to the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BVLC/caffe&#34;&gt;Caffe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/flatbuffers&#34;&gt;flatbuffer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/gemmlowp&#34;&gt;gemmlowp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.github.com/googlesamples/android-vulkan-tutorials&#34;&gt;Google Vulkan demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halide/Halide&#34;&gt;Halide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XiaoMi/mace&#34;&gt;Mace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/onnx&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf&#34;&gt;protobuffer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/skia&#34;&gt;skia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;ncnn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/paddle-mobile&#34;&gt;paddle-mobile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nothings/stb&#34;&gt;stb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/rapidjson&#34;&gt;rapidjson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pybind/pybind11&#34;&gt;pybind11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huawei-noah/bolt&#34;&gt;bolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chromium.googlesource.com/libyuv/libyuv&#34;&gt;libyuv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/libjpeg-turbo/libjpeg-turbo&#34;&gt;libjpeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opencv/opencv&#34;&gt;opencv&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>prusa3d/PrusaSlicer</title>
    <updated>2022-07-02T01:30:16Z</updated>
    <id>tag:github.com,2022-07-02:/prusa3d/PrusaSlicer</id>
    <link href="https://github.com/prusa3d/PrusaSlicer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;G-code generator for 3D printers (RepRap, Makerbot, Ultimaker etc.)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prusa3d/PrusaSlicer/master/resources/icons/PrusaSlicer.png?raw=true&#34; alt=&#34;PrusaSlicer logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;PrusaSlicer&lt;/h1&gt; &#xA;&lt;p&gt;You may want to check the &lt;a href=&#34;https://www.prusa3d.com/prusaslicer/&#34;&gt;PrusaSlicer project page&lt;/a&gt;. Prebuilt Windows, OSX and Linux binaries are available through the &lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer/releases&#34;&gt;git releases page&lt;/a&gt; or from the &lt;a href=&#34;https://www.prusa3d.com/drivers/&#34;&gt;Prusa3D downloads page&lt;/a&gt;. There are also &lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer/wiki/PrusaSlicer-on-Linux---binary-distributions&#34;&gt;3rd party Linux builds available&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;PrusaSlicer takes 3D models (STL, OBJ, AMF) and converts them into G-code instructions for FFF printers or PNG layers for mSLA 3D printers. It&#39;s compatible with any modern printer based on the RepRap toolchain, including all those based on the Marlin, Prusa, Sprinter and Repetier firmware. It also works with Mach3, LinuxCNC and Machinekit controllers.&lt;/p&gt; &#xA;&lt;p&gt;PrusaSlicer is based on &lt;a href=&#34;https://github.com/Slic3r/Slic3r&#34;&gt;Slic3r&lt;/a&gt; by Alessandro Ranellucci and the RepRap community.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.prusa3d.com/slic3r-prusa-edition/&#34;&gt;project homepage&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/PrusaSlicer/master/doc/&#34;&gt;documentation directory&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;What language is it written in?&lt;/h3&gt; &#xA;&lt;p&gt;All user facing code is written in C++, and some legacy code as well as unit tests are written in Perl. Perl is not required for either development or use of PrusaSlicer.&lt;/p&gt; &#xA;&lt;p&gt;The slicing core is the &lt;code&gt;libslic3r&lt;/code&gt; library, which can be built and used in a standalone way. The command line interface is a thin wrapper over &lt;code&gt;libslic3r&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What are PrusaSlicer&#39;s main features?&lt;/h3&gt; &#xA;&lt;p&gt;Key features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;multi-platform&lt;/strong&gt; (Linux/Mac/Win) and packaged as standalone-app with no dependencies required&lt;/li&gt; &#xA; &lt;li&gt;complete &lt;strong&gt;command-line interface&lt;/strong&gt; to use it with no GUI&lt;/li&gt; &#xA; &lt;li&gt;multi-material &lt;strong&gt;(multiple extruders)&lt;/strong&gt; object printing&lt;/li&gt; &#xA; &lt;li&gt;multiple G-code flavors supported (RepRap, Makerbot, Mach3, Machinekit etc.)&lt;/li&gt; &#xA; &lt;li&gt;ability to plate &lt;strong&gt;multiple objects having distinct print settings&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;multithread&lt;/strong&gt; processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;STL auto-repair&lt;/strong&gt; (tolerance for broken models)&lt;/li&gt; &#xA; &lt;li&gt;wide automated unit testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other major features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;combine infill every &#39;n&#39; perimeters layer to speed up printing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D preview&lt;/strong&gt; (including multi-material files)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;multiple layer heights&lt;/strong&gt; in a single print&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;spiral vase&lt;/strong&gt; mode for bumpless vases&lt;/li&gt; &#xA; &lt;li&gt;fine-grained configuration of speed, acceleration, extrusion width&lt;/li&gt; &#xA; &lt;li&gt;several infill patterns including honeycomb, spirals, Hilbert curves&lt;/li&gt; &#xA; &lt;li&gt;support material, raft, brim, skirt&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;standby temperature&lt;/strong&gt; and automatic wiping for multi-extruder printing&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer/wiki/Slic3r-Prusa-Edition-Macro-Language&#34;&gt;customizable &lt;strong&gt;G-code macros&lt;/strong&gt;&lt;/a&gt; and output filename with variable placeholders&lt;/li&gt; &#xA; &lt;li&gt;support for &lt;strong&gt;post-processing scripts&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;cooling logic&lt;/strong&gt; controlling fan speed and dynamic print speed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;p&gt;If you want to compile the source yourself, follow the instructions on one of these documentation pages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/PrusaSlicer/master/doc/How%20to%20build%20-%20Linux%20et%20al.md&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/PrusaSlicer/master/doc/How%20to%20build%20-%20Mac%20OS.md&#34;&gt;macOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prusa3d/PrusaSlicer/master/doc/How%20to%20build%20-%20Windows.md&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Can I help?&lt;/h3&gt; &#xA;&lt;p&gt;Sure! You can do the following to find things that are available to help with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add an &lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer/issues&#34;&gt;issue&lt;/a&gt; to the github tracker if it isn&#39;t already present.&lt;/li&gt; &#xA; &lt;li&gt;Look at &lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Aissue+label%3A%22volunteer+needed%22&#34;&gt;issues labeled &#34;volunteer needed&#34;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What&#39;s PrusaSlicer license?&lt;/h3&gt; &#xA;&lt;p&gt;PrusaSlicer is licensed under the &lt;em&gt;GNU Affero General Public License, version 3&lt;/em&gt;. The PrusaSlicer is originally based on Slic3r by Alessandro Ranellucci.&lt;/p&gt; &#xA;&lt;h3&gt;How can I use PrusaSlicer from the command line?&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer/wiki/Command-Line-Interface&#34;&gt;Command Line Interface&lt;/a&gt; wiki page.&lt;/p&gt;</summary>
  </entry>
</feed>