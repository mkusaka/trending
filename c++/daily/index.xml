<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-13T01:31:51Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>adnanaziz/EPIJudge</title>
    <updated>2022-08-13T01:31:51Z</updated>
    <id>tag:github.com,2022-08-13:/adnanaziz/EPIJudge</id>
    <link href="https://github.com/adnanaziz/EPIJudge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EPI Judge - Preview Release&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EPI Judge&lt;/h1&gt; &#xA;&lt;h2&gt;Beta 5&lt;/h2&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;EPI Judge is meant to serve as a companion to our book Elements of Programming Interviews. Specifically, this project consists of the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stub programs&lt;/strong&gt; for each problem in our book in Python, Java, and C++&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test-cases&lt;/strong&gt; that cover common corner-case and performance bugs&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;framework&lt;/strong&gt; for running these tests on your implementation on your machine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how to download the judge:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/adnanaziz/EPIJudge.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not have &lt;code&gt;git&lt;/code&gt;, here&#39;s a good &lt;a href=&#34;https://www.atlassian.com/git/tutorials/install-git&#34;&gt;tutorial&lt;/a&gt; on installing git itself.&lt;/p&gt; &#xA;&lt;h2&gt;Running the judge using IDEs&lt;/h2&gt; &#xA;&lt;p&gt;Check out these one minute videos to see how easy it is to get started with the judge.&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/ImD_iI-uGYo&#34;&gt;PyCharm&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/rZ1qqwEXwQY&#34;&gt;Eclipse&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/Z41jW1TyZwY&#34;&gt;NetBeans&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Java&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/1BzHUpluQHM&#34;&gt;IntelliJ IDEA&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/i9uz9Zazo0A&#34;&gt;Eclipse&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;C++&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/aHPDApyyYEg&#34;&gt;CLion&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/hgd8IIQpBEE&#34;&gt;Visual Studio 2017&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running the judge from the command line&lt;/h2&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 &amp;lt;program_name&amp;gt;.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Java&lt;/h4&gt; &#xA;&lt;p&gt;Use the &lt;a href=&#34;https://github.com/adnanaziz/EPIJudge/raw/master/epi_judge_java/Makefile&#34;&gt;&lt;code&gt;Makefile&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Compile and run a specific program:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make &amp;lt;program_name&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make Anagrams&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Compile and run the last program that you edited:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;C++&lt;/h3&gt; &#xA;&lt;p&gt;You can manually compile and run all programs by directly invoking GCC and Clang.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ g++ -pthread -std=c++14 -O3 -o anagrams anagrams.cc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the provided Makefile: &lt;code&gt;make &amp;lt;program_name&amp;gt;&lt;/code&gt;. You can also use CMake with the provided CMakeLists.txt file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default Makefile target is the last edited file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make anagrams&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How can I contact the authors?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please feel free to send us questions and feedback - &lt;code&gt;adnan.aziz@gmail.com&lt;/code&gt; and &lt;code&gt;tsung.hsien.lee@gmail.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Help, my EPIJudge is not working, what should I do?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you do have issues, e.g., with install or with buggy tests, feel free to reach out to us via email. Please be as detailed as you can: the ideal is if you can upload a screencast video of the issue to youtube; failing that, please upload screenshots. The more detailed the description of the problem and your environment (OS, language version, IDE and version), the easier it‚Äôll be for us to help you.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I&#39;m new to programming, and don&#39;t have any kind of development environment, what should I do?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The IntelliJ Integrated Development environments described above are best-in-class, and have free versions that will work fine for the EPI Judge. They do not include the compilers. You can get the Java development environment from &lt;a href=&#34;http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html&#34;&gt;Oracle&lt;/a&gt;, and the Python development environment from &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python.org&lt;/a&gt;. For C++, you have multiple options. The simplest is to install &lt;a href=&#34;https://code.visualstudio.com/download&#34;&gt;VisualStudio&lt;/a&gt;, which includes both the IDE and the compiler. Google is a good resource for installation help.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;What compilers are supported for judge?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;C++ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Linux &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;GCC&lt;/strong&gt; 5.4.1 and newer&lt;/li&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Clang&lt;/strong&gt; 4.0 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;OS X &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Apple LLVM Clang&lt;/strong&gt; 9.0.0 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Windows &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Visual Studio&lt;/strong&gt; 2017 and newer&lt;/li&gt; &#xA;       &lt;li&gt;&lt;strong&gt;MinGW&lt;/strong&gt; GCC 5.4.0 and newer&lt;/li&gt; &#xA;       &lt;li&gt;&lt;strong&gt;LXSS&lt;/strong&gt; (Windows Subsystem for Linux) GCC 5.4.0 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Java &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Java&lt;/strong&gt; 9 and newer&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Python &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; 3.7 and newer&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;What compilers are supported for solutions?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;C++ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Linux &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;GCC&lt;/strong&gt; 7.0.0 and newer&lt;/li&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Clang&lt;/strong&gt; 5.0 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;OS X &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Apple LLVM Clang&lt;/strong&gt; 9.0.0 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Windows&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Visual Studio&lt;/strong&gt; 2017 and newer &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;MinGW&lt;/strong&gt; GCC 7.2.0 and newer&lt;/li&gt; &#xA;       &lt;li&gt;&lt;strong&gt;LXSS&lt;/strong&gt; (Windows Subsystem for Linux) GCC 7.2.0 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Java &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Java&lt;/strong&gt; 9 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Python &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; 3.6 and newer&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let us know if you managed to compile with an older version.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What does the UI look like?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Take a look at this screenshot.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://elementsofprogramminginterviews.com/img/judge-ide-example.png&#34; width=&#34;600px&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How can I understand the test framework better?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The judge harness is fairly complex (but does not use nonstandard language features or libraries). You are welcome to study it, but we‚Äôd advise you against making changes to it (since it will lead to nasty merge conflicts when you update).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How do I import the C++ project?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to import the project into your favourite IDE, you probably need to create IDE project with &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; (no need to do it for CLion, it supports CMake out-of-the-box).&lt;/p&gt; &#xA;&lt;p&gt;Here is an example recipe for generationg Visual Studio project (&lt;a href=&#34;https://cmake.org/cmake/help/v3.10/manual/cmake-generators.7.html&#34;&gt;list&lt;/a&gt; of all CMake supported IDEs). After installing CMake, open your terminal, go to &lt;code&gt;epi_judge_cpp&lt;/code&gt; folder and run following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir vs&#xA;cd vs&#xA;cmake -G &#34;Visual Studio 15 2017&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then just open &lt;code&gt;epi_judge_cpp/vs/epi_judge_cpp.sln&lt;/code&gt; solution with Visual Studio and it will load all EPI programs.&lt;/p&gt; &#xA;&lt;h2&gt;Tracking your progress&lt;/h2&gt; &#xA;&lt;p&gt;The file &lt;a href=&#34;https://github.com/adnanaziz/EPIJudge/raw/master/index.html&#34;&gt;index.html&lt;/a&gt; in the root of this project tracks your progress through the problems. Specifically, there&#39;s an expanding tab for each chapter. Click on it, and you will see your progress, e.g., as below. This file gets updated each time you execute a program. You can &lt;strong&gt;use this file to map book problems to stub programs&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xjf7Z32.png&#34; width=&#34;600px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;A big shout-out to the hundreds of users who tried out the release over the past couple of months. As always, we never fail to be impressed by the enthusiasm and commitment our readers have; it has served to bring out the best in us. We all thank &lt;a href=&#34;https://github.com/metopa&#34;&gt;Viacheslav Kroilov&lt;/a&gt;, for applying his exceptional software engineering skills to make EPI Judge a reality.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>libcpr/cpr</title>
    <updated>2022-08-13T01:31:51Z</updated>
    <id>tag:github.com,2022-08-13:/libcpr/cpr</id>
    <link href="https://github.com/libcpr/cpr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ Requests: Curl for People, a spiritual port of Python Requests.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;C++ Requests: Curl for People &lt;img align=&#34;right&#34; height=&#34;40&#34; src=&#34;http://i.imgur.com/d9Xtyts.png&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.libcpr.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-online-informational?style=flat&amp;amp;link=https://docs.libcpr.org/&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/libcpr/cpr/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://gitter.im/libcpr/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/libcpr/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Announcements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Like you probably have noticed, &lt;code&gt;cpr&lt;/code&gt; moved to a new home from &lt;a href=&#34;https://github.com/whoshuu/cpr&#34;&gt;https://github.com/whoshuu/cpr&lt;/a&gt; to &lt;a href=&#34;https://github.com/libcpr/cpr&#34;&gt;https://github.com/libcpr/cpr&lt;/a&gt;. Read more &lt;a href=&#34;https://github.com/libcpr/cpr/issues/636&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This project is being maintained by &lt;a href=&#34;https://github.com/com8&#34;&gt;Fabian Sauter&lt;/a&gt; and &lt;a href=&#34;https://github.com/KingKili&#34;&gt;Kilian Traub&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For quick help, and discussion libcpr also offer a &lt;a href=&#34;https://gitter.im/libcpr/community?utm_source=share-link&amp;amp;utm_medium=link&amp;amp;utm_campaign=share-link&#34;&gt;gitter&lt;/a&gt; chat.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TLDR&lt;/h2&gt; &#xA;&lt;p&gt;C++ Requests is a simple wrapper around &lt;a href=&#34;http://curl.haxx.se/libcurl&#34;&gt;libcurl&lt;/a&gt; inspired by the excellent &lt;a href=&#34;https://github.com/kennethreitz/requests&#34;&gt;Python Requests&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;Despite its name, libcurl&#39;s easy interface is anything but, and making mistakes misusing it is a common source of error and frustration. Using the more expressive language facilities of C++11, this library captures the essence of making network calls into a few concise idioms.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a quick GET request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;cpr/cpr.h&amp;gt;&#xA;&#xA;int main(int argc, char** argv) {&#xA;    cpr::Response r = cpr::Get(cpr::Url{&#34;https://api.github.com/repos/whoshuu/cpr/contributors&#34;},&#xA;                      cpr::Authentication{&#34;user&#34;, &#34;pass&#34;, cpr::AuthMode::BASIC},&#xA;                      cpr::Parameters{{&#34;anon&#34;, &#34;true&#34;}, {&#34;key&#34;, &#34;value&#34;}});&#xA;    r.status_code;                  // 200&#xA;    r.header[&#34;content-type&#34;];       // application/json; charset=utf-8&#xA;    r.text;                         // JSON text string&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here&#39;s &lt;a href=&#34;https://gist.github.com/whoshuu/2dc858b8730079602044&#34;&gt;less functional, more complicated code, without cpr&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.libcpr.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-online-informational?style=for-the-badge&amp;amp;link=https://docs.libcpr.org/&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;br&gt; You can find the latest documentation &lt;a href=&#34;https://docs.libcpr.org/&#34;&gt;here&lt;/a&gt;. It&#39;s a work in progress, but it should give you a better idea of how to use the library than the &lt;a href=&#34;https://github.com/libcpr/cpr/tree/master/test&#34;&gt;tests&lt;/a&gt; currently do.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;C++ Requests currently supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Custom headers&lt;/li&gt; &#xA; &lt;li&gt;Url encoded parameters&lt;/li&gt; &#xA; &lt;li&gt;Url encoded POST values&lt;/li&gt; &#xA; &lt;li&gt;Multipart form POST upload&lt;/li&gt; &#xA; &lt;li&gt;File POST upload&lt;/li&gt; &#xA; &lt;li&gt;Basic authentication&lt;/li&gt; &#xA; &lt;li&gt;Bearer authentication&lt;/li&gt; &#xA; &lt;li&gt;Digest authentication&lt;/li&gt; &#xA; &lt;li&gt;NTLM authentication&lt;/li&gt; &#xA; &lt;li&gt;Connection and request timeout specification&lt;/li&gt; &#xA; &lt;li&gt;Timeout for low speed connection&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous requests&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üç™&lt;/span&gt; support!&lt;/li&gt; &#xA; &lt;li&gt;Proxy support&lt;/li&gt; &#xA; &lt;li&gt;Callback interfaces&lt;/li&gt; &#xA; &lt;li&gt;PUT methods&lt;/li&gt; &#xA; &lt;li&gt;DELETE methods&lt;/li&gt; &#xA; &lt;li&gt;HEAD methods&lt;/li&gt; &#xA; &lt;li&gt;OPTIONS methods&lt;/li&gt; &#xA; &lt;li&gt;PATCH methods&lt;/li&gt; &#xA; &lt;li&gt;Thread Safe access to &lt;a href=&#34;https://curl.haxx.se/libcurl/c/threadsafe.html&#34;&gt;libCurl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenSSL and WinSSL support for HTTPS requests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Planned&lt;/h2&gt; &#xA;&lt;p&gt;For a quick overview about the planed features, have a look at the next &lt;a href=&#34;https://github.com/libcpr/cpr/milestones&#34;&gt;Milestones&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;CMake&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a CMake project you need to integrate C++ Requests with, the primary way is to use &lt;code&gt;fetch_content&lt;/code&gt;. Add the following to your &lt;code&gt;CMakeLists.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;include(FetchContent)&#xA;FetchContent_Declare(cpr GIT_REPOSITORY https://github.com/libcpr/cpr.git&#xA;                         GIT_TAG 871ed52d350214a034f6ef8a3b8f51c5ce1bd400) # The commit hash for 1.9.0. Replace with the latest from: https://github.com/libcpr/cpr/releases&#xA;FetchContent_MakeAvailable(cpr)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will produce the target &lt;code&gt;cpr::cpr&lt;/code&gt; which you can link against the typical way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;target_link_libraries(your_target_name PRIVATE cpr::cpr)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That should do it! There&#39;s no need to handle &lt;code&gt;libcurl&lt;/code&gt; yourself. All dependencies are taken care of for you.&lt;br&gt; All of this can be found in an example &lt;a href=&#34;https://github.com/libcpr/example-cmake-fetch-content&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Packages for Linux Distributions&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you may install a package specific to your Linux distribution. Since so few distributions currently have a package for cpr, most users will not be able to run your program with this approach.&lt;/p&gt; &#xA;&lt;p&gt;Currently, we are aware of packages for the following distributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aur.archlinux.org/packages/cpr&#34;&gt;Arch Linux (AUR)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If there&#39;s no package for your distribution, try making one! If you do, and it is added to your distribution&#39;s repositories, please submit a pull request to add it to the list above. However, please only do this if you plan to actively maintain the package.&lt;/p&gt; &#xA;&lt;h3&gt;NuGet Package&lt;/h3&gt; &#xA;&lt;p&gt;For Windows there is also a libcpr NuGet package available. Currently x86 and x64 builds are supported with release and debug configuration.&lt;/p&gt; &#xA;&lt;p&gt;The package can be found here: &lt;a href=&#34;https://www.nuget.org/packages/libcpr/&#34;&gt;NuGet.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The only explicit requirements are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a &lt;code&gt;C++11&lt;/code&gt; compatible compiler such as Clang or GCC. The minimum required version of GCC is unknown, so if anyone has trouble building this library with a specific version of GCC, do let me know&lt;/li&gt; &#xA; &lt;li&gt;If you would like to perform https requests &lt;code&gt;OpenSSL&lt;/code&gt; and its development libraries are required.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building cpr - Using vcpkg&lt;/h2&gt; &#xA;&lt;p&gt;You can download and install cpr using the &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;git clone https://github.com/Microsoft/vcpkg.git&#xA;cd vcpkg&#xA;./bootstrap-vcpkg.sh&#xA;./vcpkg integrate install&#xA;./vcpkg install cpr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;cpr&lt;/code&gt; port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h2&gt;Building cpr - Using Conan&lt;/h2&gt; &#xA;&lt;p&gt;You can download and install &lt;code&gt;cpr&lt;/code&gt; using the &lt;a href=&#34;https://conan.io/&#34;&gt;Conan&lt;/a&gt; package manager. Setup your CMakeLists.txt (see &lt;a href=&#34;https://docs.conan.io/en/latest/integrations/build_system.html&#34;&gt;Conan documentation&lt;/a&gt; on how to use MSBuild, Meson and others). An example can be found &lt;a href=&#34;https://github.com/libcpr/example-cmake-conan&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;cpr&lt;/code&gt; package in Conan is kept up to date by Conan contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/conan-io/conan-center-index&#34;&gt;create an issue or pull request&lt;/a&gt; on the &lt;code&gt;conan-center-index&lt;/code&gt; repository.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DefTruth/lite.ai.toolkit</title>
    <updated>2022-08-13T01:31:51Z</updated>
    <id>tag:github.com,2022-08-13:/DefTruth/lite.ai.toolkit</id>
    <link href="https://github.com/DefTruth/lite.ai.toolkit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üõ† A lite C++ toolkit of awesome AI models with ONNXRuntime, NCNN, MNN and TNN. YOLOX, YOLOP, YOLOv6, YOLOR, MODNet, YOLOX, YOLOv7, YOLOv5. MNN, NCNN, TNN, ONNXRuntime.&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;lite.ai.toolkit-Introduction&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/logo-v3.png&#34; alt=&#34;logo-v3&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/mac|linux|win-pass-brightgreen.svg&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/device-GPU/CPU-yellow.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/license-GPLv3-blue.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/onnxruntime-1.10.0-turquoise.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/mnn-1.2.0-hotpink.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/ncnn-1.0.21-orange.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/tnn-0.3.0-blue.svg?sanitize=true&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üõ†&lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt;: A lite C++ toolkit of awesome AI models, such as &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-object-detection&#34;&gt;Object Detection&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-face-detection&#34;&gt;Face Detection&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-face-recognition&#34;&gt;Face Recognition&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-segmentation&#34;&gt;Segmentation&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-matting&#34;&gt;Matting&lt;/a&gt;, etc. See &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Model-Zoo&#34;&gt;Model Zoo&lt;/a&gt; and &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;ONNX Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md&#34;&gt;MNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md&#34;&gt;TNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md&#34;&gt;NCNN Hub&lt;/a&gt;. [‚ù§Ô∏è Star üåüüëÜüèª this repo to support me if it does any helps to you, thanks ~ ]&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_yolov5_1.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/efficientdet_d0.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/street.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd_2.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_ultraface.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_deeplabv3_resnet101.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/teslai.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/tesla.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3i.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/yolop1.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/yolop2.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg_mask.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/face_parsing.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/face_parsing_merge.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/cartoon.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/facemesh0.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/facemesh1.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_mosaic.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_candy.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/README.zh.md&#34;&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-MacOS&#34;&gt;MacOS&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Linux&#34;&gt;Linux&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Windows&#34;&gt;Windows&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Core Features üëèüëã&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Core-Features&#34;&gt;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simply and User friendly.&lt;/strong&gt; Simply and Consistent syntax like &lt;strong&gt;lite::cv::Type::Class&lt;/strong&gt;, see &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minimum Dependencies.&lt;/strong&gt; Only &lt;strong&gt;OpenCV&lt;/strong&gt; and &lt;strong&gt;ONNXRuntime&lt;/strong&gt; are required by default, see &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Lite.AI.ToolKit&#34;&gt;build&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lots of Algorithm Modules.&lt;/strong&gt; Contains almost &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;300+&lt;/a&gt;&lt;/strong&gt; C++ re-implementations and &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;500+&lt;/a&gt;&lt;/strong&gt; weights.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations üéâüéâ&lt;/h2&gt; &#xA;&lt;p&gt;Consider to cite it as follows if you use &lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; in your projects.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{lite.ai.toolkit2021,&#xA;  title={lite.ai.toolkit: A lite C++ toolkit of awesome AI models.},&#xA;  url={https://github.com/DefTruth/lite.ai.toolkit},&#xA;  note={Open-source software available at https://github.com/DefTruth/lite.ai.toolkit},&#xA;  author={Yan Jun},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;About Training ü§ìüëÄ&lt;/h2&gt; &#xA;&lt;p&gt;A high level Training and Evaluating Toolkit for Face Landmarks Detection is available at &lt;a href=&#34;https://github.com/DefTruth/torchlm&#34;&gt;torchlm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloads &amp;amp; RoadMap ‚úÖ&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-RoadMap&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/lite.ai.toolkit-roadmap-v0.1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some prebuilt lite.ai.toolkit libs for MacOS(x64) and Linux(x64) are available, you can download the libs from the release links. Further, prebuilt libs for Windows(x64) and Android will be coming soon ~ Please, see &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/48&#34;&gt;issues#48&lt;/a&gt; for more details of the prebuilt plan and refer to &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases&#34;&gt;releases&lt;/a&gt; for more available prebuilt libs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&#34;&gt;lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&#34;&gt;lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&#34;&gt;lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&#34;&gt;lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&#34;&gt;lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&#34;&gt;lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In Linux, in order to link the prebuilt libs, you need to export &lt;code&gt;lite.ai.toolkit/lib&lt;/code&gt; to LD_LIBRARY_PATH first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export LD_LIBRARY_PATH=YOUR-PATH-TO/lite.ai.toolkit/lib:$LD_LIBRARY_PATH&#xA;export LIBRARY_PATH=YOUR-PATH-TO/lite.ai.toolkit/lib:$LIBRARY_PATH  # (may need)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Setup üëÄ&lt;/h2&gt; &#xA;&lt;p&gt;To quickly setup &lt;code&gt;lite.ai.toolkit&lt;/code&gt;, you can follow the &lt;code&gt;CMakeLists.txt&lt;/code&gt; listed as belows. üëáüëÄ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)&#xA;include_directories(${LITE_AI_DIR}/include)&#xA;link_directories(${LITE_AI_DIR}/lib})&#xA;set(TOOLKIT_LIBS lite.ai.toolkit onnxruntime)&#xA;set(OpenCV_LIBS opencv_core opencv_imgcodecs opencv_imgproc opencv_video opencv_videoio)&#xA;&#xA;add_executable(lite_yolov5 examples/test_lite_yolov5.cpp)&#xA;target_link_libraries(lite_yolov5 ${TOOLKIT_LIBS} ${OpenCV_LIBS})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contents üìñüí°&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Core-Features&#34;&gt;Core Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Quick-Start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-RoadMap&#34;&gt;RoadMap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Important-Updates&#34;&gt;Important Updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Supported-Models-Matrix&#34;&gt;Supported Models Matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Lite.AI.ToolKit&#34;&gt;Build Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Model-Zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-References&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Contribute&#34;&gt;Contribute&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. Quick Start üåüüåü&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Quick-Start&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example0: Object Detection using &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/yolov5s.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_yolov5_1.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_yolov5_1.jpg&#34;;&#xA;&#xA;  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); &#xA;  std::vector&amp;lt;lite::types::Boxf&amp;gt; detected_boxes;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  yolov5-&amp;gt;detect(img_bgr, detected_boxes);&#xA;  &#xA;  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);&#xA;  cv::imwrite(save_img_path, img_bgr);  &#xA;  &#xA;  delete yolov5;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Important Updates üÜï&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Important-Updates&#34;&gt;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click here to see details of Important Updates! &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;C++&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Awesome&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2022/04/03„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_modnet.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.11961.pdf&#34;&gt;AAAI 2022&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ZHKKKe/MODNet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;matting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2022/03/23„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNtet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet98.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.03771&#34;&gt;CVPR 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/jhb86253817/PIPNet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;face::align&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2022/01/19„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLO5Face&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolo5face.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.12931&#34;&gt;arXiv 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/deepcam-cn/yolov5-face.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;face::detect&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2022/01/07„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;SCRFD&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_scrfd.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.04714&#34;&gt;CVPR 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/deepinsight/insightface.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;face::detect&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/12/27„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDetPlus&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet_plus.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/449912627&#34;&gt;blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/12/08„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;MGMatting&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mg_matting.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.06722&#34;&gt;CVPR 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yucornetto/MGMatting.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;matting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/11/11„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases/tag/v6.0&#34;&gt;YoloV5_V_6_0&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zenodo.org/record/5563715#.YbXffH1Bzfs&#34;&gt;doi&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases/tag/v6.0&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ultralytics/yolov5.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/10/26„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/tag/0.1.1rc0&#34;&gt;YoloX_V_0_1_1&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.08430&#34;&gt;arXiv 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Megvii-BaseDetection/YOLOX.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/10/02„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/306530300&#34;&gt;blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/09/20„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_rvm.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11515&#34;&gt;WACV 2022&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PeterL1n/RobustVideoMatting.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;matting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;„Äê2021/09/02„Äë&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;YOLOP&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolop.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11250&#34;&gt;arXiv 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hustvl/YOLOP.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;3. Supported Models Matrix&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Supported-Models-Matrix&#34;&gt;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/ = not supported now.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ = known work and official supported now.&lt;/li&gt; &#xA; &lt;li&gt;‚úîÔ∏è = known work, but unofficial supported now.&lt;/li&gt; &#xA; &lt;li&gt;‚ùî = in my plan, but not coming soon, maybe a few months later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ONNXRuntime&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NCNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MacOS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Android&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YoloV5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/yolov3&#34;&gt;YoloV3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;236M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov3.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/tiny-yolov3&#34;&gt;TinyYoloV3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tiny_yolov3.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/argusswift/YOLOv4-pytorch&#34;&gt;YoloV4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;176M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov4.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/ssd&#34;&gt;SSD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ssd.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/ssd-mobilenetv1&#34;&gt;SSDMobileNetV1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ssd_mobilenetv1.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YoloX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolox.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bubbliiiing/yolov4-tiny-pytorch&#34;&gt;TinyYoloV4VOC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tiny_yolov4_voc.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bubbliiiing/yolov4-tiny-pytorch&#34;&gt;TinyYoloV4COCO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tiny_yolov4_coco.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;YoloR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolor.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;ScaledYoloV4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;270M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_scaled_yolov4.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch&#34;&gt;EfficientDet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientdet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch&#34;&gt;EfficientDetD7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;220M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientdet_d7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch&#34;&gt;EfficientDetD8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;322M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientdet_d8.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;YOLOP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolop.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDetPlus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet_plus.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDetEffi...&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet_efficientnet_lite.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YoloX_V_0_1_1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YoloV5_V_6_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch&#34;&gt;GlintArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_glint_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch&#34;&gt;GlintCosFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_glint_cosface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc&#34;&gt;GlintPartialFC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;170M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_glint_partial_fc.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/timesler/facenet-pytorch&#34;&gt;FaceNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_facenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZhaoJ9014/face.evoLVe.PyTorch&#34;&gt;FocalArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;166M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_focal_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZhaoJ9014/face.evoLVe.PyTorch&#34;&gt;FocalAsiaArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;166M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_focal_asia_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Tencent/TFace/tree/master/tasks/distfc&#34;&gt;TencentCurricularFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;249M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tencent_curricular_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Tencent/TFace/tree/master/tasks/cifp&#34;&gt;TencentCifpFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;130M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tencent_cifp_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/louis-she/center-loss.pytorch&#34;&gt;CenterLossFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;280M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_center_loss_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/clcarwin/sphereface_pytorch&#34;&gt;SphereFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_sphere_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/penincillin/DREAM&#34;&gt;PoseRobustFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pose_robust_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/penincillin/DREAM&#34;&gt;NaivePoseRobustFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_naive_pose_robust_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Xiaoccer/MobileFaceNet_Pytorch&#34;&gt;MobileFaceNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobile_facenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch&#34;&gt;CavaGhostArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_cava_ghost_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch&#34;&gt;CavaCombinedFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;250M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_cava_combined_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/grib0ed0v/face_recognition.pytorch&#34;&gt;MobileSEFocalFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilese_focal_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_rvm.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;MGMatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;113M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mg_matting.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_modnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNetDyn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_modnet_dyn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/BackgroundMattingV2&#34;&gt;BackgroundMattingV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_backgroundmattingv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/BackgroundMattingV2&#34;&gt;BackgroundMattingV2Dyn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_backgroundmattingv2_dyn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34;&gt;UltraFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ultraface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biubug6/Pytorch_Retinaface&#34;&gt;RetinaFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.6M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_retinaface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zisianw/FaceBoxes.PyTorch&#34;&gt;FaceBoxes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_faceboxes.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/FaceBoxesV2&#34;&gt;FaceBoxesV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_faceboxesv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;SCRFD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_scrfd.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLO5Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolo5face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Hsintao/pfld_106_face_landmarks&#34;&gt;PFLD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pfld.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/polarisZhao/PFLD-pytorch&#34;&gt;PFLD98&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pfld98.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;MobileNetV268&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilenetv2_68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;MobileNetV2SE68&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilenetv2_se_68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;PFLD68&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pfld68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Single430/FaceLandmark1000&#34;&gt;FaceLandmark1000&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_face_landmarks_1000.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet98&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet98.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet68&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet29&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet29.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet19&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet19.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/omasaht/headpose-fsanet-pytorch&#34;&gt;FSANet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::pose&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fsanet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;AgeGoogleNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_age_googlenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;GenderGoogleNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_gender_googlenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/body_analysis/emotion_ferplus&#34;&gt;EmotionFerPlus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_emotion_ferplus.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;VGG16Age&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;514M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_vgg16_age.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;VGG16Gender&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_vgg16_gender.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/oukohou/SSR_Net_Pytorch&#34;&gt;SSRNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;190K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ssrnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;EfficientEmotion7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficient_emotion7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;EfficientEmotion8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficient_emotion8.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;MobileEmotion7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobile_emotion7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;ReXNetEmotion7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_rexnet_emotion7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/classification/efficientnet-lite4&#34;&gt;EfficientNetLite4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientnet_lite4.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/classification/shufflenet&#34;&gt;ShuffleNetV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_shufflenetv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_densenet/&#34;&gt;DenseNet121&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_densenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_ghostnet/&#34;&gt;GhostNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ghostnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_hardnet//&#34;&gt;HdrDNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_hardnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_ibnnet/&#34;&gt;IBNNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ibnnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_mobilenet_v2/&#34;&gt;MobileNetV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilenetv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_resnet/&#34;&gt;ResNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_resnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_resnext/&#34;&gt;ResNeXt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_resnext.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&#34;&gt;DeepLabV3ResNet101&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;232M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_deeplabv3_resnet101.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_fcn_resnet101/&#34;&gt;FCNResNet101&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;207M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fcn_resnet101.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/style_transfer/fast_neural_style&#34;&gt;FastStyleTransfer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;style&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fast_style_transfer.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/richzhang/colorization&#34;&gt;Colorizer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;123M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;colorization&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_colorizer.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/niazwazir/SUB_PIXEL_CNN&#34;&gt;SubPixelCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;234K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;resolution&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_subpixel_cnn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/niazwazir/SUB_PIXEL_CNN&#34;&gt;SubPixelCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;234K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;resolution&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_subpixel_cnn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/quarrying/quarrying-insect-id&#34;&gt;InsectDet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_insectdet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/quarrying/quarrying-insect-id&#34;&gt;InsectID&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_insectid.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/quarrying/quarrying-plant-id&#34;&gt;PlantID&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_plantid.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLOv5BlazeFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_blazeface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases/tag/v6.1&#34;&gt;YoloV5_V_6_1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_v6.1.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/minivision-ai/photo2cartoon&#34;&gt;HeadSeg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_head_seg.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/minivision-ai/photo2cartoon&#34;&gt;FemalePhoto2Cartoon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;style&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_female_photo2cartoon.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/YexingWan/Fast-Portrait-Segmentation&#34;&gt;FastPortraitSeg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;400k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fast_portrait_seg.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/clovaai/ext_portrait_segmentation&#34;&gt;PortraitSegSINet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;380k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_portrait_seg_sinet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/clovaai/ext_portrait_segmentation&#34;&gt;PortraitSegExtremeC3Net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;180k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_portrait_seg_extremec3net.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/kampta/face-seg&#34;&gt;FaceHairSeg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_face_hair_seg.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/akirasosa/mobile-semantic-segmentation&#34;&gt;HairSeg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_hair_seg.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lizhengwei1992/mobile_phone_human_matting&#34;&gt;MobileHumanMatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobile_human_matting.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/wonbeomjang/mobile-hair-segmentation-pytorch&#34;&gt;MobileHairSeg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobile_hair_seg.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6&#34;&gt;YOLOv6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov6.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;FaceParsingBiSeNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_face_parsing_bisenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;FaceParsingBiSeNetDyn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_face_parsing_bisenet_dyn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úîÔ∏è&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;4. Build Docs.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-MacOS&#34;&gt;&lt;/div&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-Lite.AI.ToolKit&#34;&gt;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS: Build the shared lib of &lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; for &lt;strong&gt;MacOS&lt;/strong&gt; from sources. Note that Lite.Ai.ToolKit uses &lt;strong&gt;onnxruntime&lt;/strong&gt; as default backend, for the reason that onnxruntime supports the most of onnx&#39;s operators.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    git clone --depth=1 https://github.com/DefTruth/lite.ai.toolkit.git  # latest&#xA;    cd lite.ai.toolkit &amp;amp;&amp;amp; sh ./build.sh  # On MacOS, you can use the built OpenCV, ONNXRuntime, MNN, NCNN and TNN libs in this repo.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-Linux&#34;&gt;&lt;/div&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-Windows&#34;&gt;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üí° Linux and Windows. &lt;/summary&gt; &#xA; &lt;h3&gt;Linux and Windows.&lt;/h3&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; is not directly support Linux and Windows now. For Linux and Windows, you need to build or download(if have official builts) the shared libs of &lt;strong&gt;OpenCV&lt;/strong&gt;„ÄÅ&lt;strong&gt;ONNXRuntime&lt;/strong&gt; and any other Engines(like MNN, NCNN, TNN) firstly, then put the headers into the specific directories or just let these directories unchange(use the headers offer by this repo, the header file of the dependent library of this project is directly copied from the corresponding official library). However, the dynamic libraries under different operating systems need to be recompiled or downloaded. MacOS users can directly use the dynamic libraries of each dependent library provided by this project:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/opencv2&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-opencv/include/opencv4/opencv2 lite.ai.toolkit/opencv2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/onnxruntime&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-onnxruntime/include/onnxruntime lite.ai.toolkit/onnxruntime&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/MNN&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-MNN/include/MNN lite.ai.toolkit/MNN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/ncnn&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-ncnn/include/ncnn lite.ai.toolkit/ncnn&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/tnn&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-TNN/include/tnn lite.ai.toolkit/tnn&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;and put the libs into &lt;strong&gt;lite.ai.toolkit/lib/(linux|windows)&lt;/strong&gt; directory. Please reference the build-docs&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; for &lt;strong&gt;third_party&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/lib/(linux|windows)&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp you-path-to-downloaded-or-built-opencv/lib/*opencv* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-onnxruntime/lib/*onnxruntime* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-MNN/lib/*MNN* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-ncnn/lib/*ncnn* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-TNN/lib/*TNN* lite.ai.toolkit/lib/(linux|windows)/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note, your also need to install ffmpeg(&amp;lt;=4.2.2) in Linux to support the opencv videoio module. See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/6&#34;&gt;issue#203&lt;/a&gt;. In MacOS, ffmpeg4.2.2 was been package into lite.ai.toolkit, thus, no installation need in OSX. In Windows, ffmpeg was been package into opencv dll prebuilt by the team of opencv. Please make sure -DWITH_FFMPEG=ON and check the configuration info when building opencv.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;first, build ffmpeg(&amp;lt;=4.2.2) from source.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone --depth=1 https://git.ffmpeg.org/ffmpeg.git -b n4.2.2&#xA;cd ffmpeg&#xA;./configure --enable-shared --disable-x86asm --prefix=/usr/local/opt/ffmpeg --disable-static&#xA;make -j8&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;then, build opencv with -DWITH_FFMPEG=ON, just like&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash&#xA;&#xA;mkdir build&#xA;cd build&#xA;&#xA;cmake .. \&#xA;  -D CMAKE_BUILD_TYPE=Release \&#xA;  -D CMAKE_INSTALL_PREFIX=your-path-to-custom-dir \&#xA;  -D BUILD_TESTS=OFF \&#xA;  -D BUILD_PERF_TESTS=OFF \&#xA;  -D BUILD_opencv_python3=OFF \&#xA;  -D BUILD_opencv_python2=OFF \&#xA;  -D BUILD_SHARED_LIBS=ON \&#xA;  -D BUILD_opencv_apps=OFF \&#xA;  -D WITH_FFMPEG=ON &#xA;  &#xA;make -j8&#xA;make install&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;after built opencv, you can follow the steps to build lite.ai.toolkit.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Windows: You can reference to &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/6&#34;&gt;issue#6&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Linux: The Docs and Docker image for Linux will be coming soon ~ &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/2&#34;&gt;issue#2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Happy News !!! : üöÄ You can download the latest &lt;strong&gt;ONNXRuntime&lt;/strong&gt; official built libs of Windows, Linux, MacOS and Arm !!! Both CPU and GPU versions are available. No more attentions needed pay to build it from source. Download the official built libs from &lt;a href=&#34;https://github.com/microsoft/onnxruntime/releases&#34;&gt;v1.8.1&lt;/a&gt;. I have used version 1.7.0 for Lite.Ai.ToolKit now, you can download it from &lt;a href=&#34;https://github.com/microsoft/onnxruntime/releases/tag/v1.7.0&#34;&gt;v1.7.0&lt;/a&gt;, but version 1.8.1 should also work, I guess ~ üôÉü§™üçÄ. For &lt;strong&gt;OpenCV&lt;/strong&gt;, try to build from source(Linux) or down load the official built(Windows) from &lt;a href=&#34;https://github.com/opencv/opencv/releases&#34;&gt;OpenCV 4.5.3&lt;/a&gt;. Then put the includes and libs into specific directory of Lite.Ai.ToolKit.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;GPU Compatibility for Windows: See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/10&#34;&gt;issue#10&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;GPU Compatibility for Linux: See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/97&#34;&gt;issue#97&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üîëÔ∏è How to link Lite.Ai.ToolKit?&lt;/summary&gt; * To link Lite.Ai.ToolKit, you can follow the CMakeLists.txt listed belows. &#xA; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;cmake_minimum_required(VERSION 3.10)&#xA;project(lite.ai.toolkit.demo)&#xA;&#xA;set(CMAKE_CXX_STANDARD 11)&#xA;&#xA;# setting up lite.ai.toolkit&#xA;set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)&#xA;set(LITE_AI_INCLUDE_DIR ${LITE_AI_DIR}/include)&#xA;set(LITE_AI_LIBRARY_DIR ${LITE_AI_DIR}/lib)&#xA;include_directories(${LITE_AI_INCLUDE_DIR})&#xA;link_directories(${LITE_AI_LIBRARY_DIR})&#xA;&#xA;set(OpenCV_LIBS&#xA;        opencv_highgui&#xA;        opencv_core&#xA;        opencv_imgcodecs&#xA;        opencv_imgproc&#xA;        opencv_video&#xA;        opencv_videoio&#xA;        )&#xA;# add your executable&#xA;set(EXECUTABLE_OUTPUT_PATH ${CMAKE_SOURCE_DIR}/examples/build)&#xA;&#xA;add_executable(lite_rvm examples/test_lite_rvm.cpp)&#xA;target_link_libraries(lite_rvm&#xA;        lite.ai.toolkit&#xA;        onnxruntime&#xA;        MNN  # need, if built lite.ai.toolkit with ENABLE_MNN=ON,  default OFF&#xA;        ncnn # need, if built lite.ai.toolkit with ENABLE_NCNN=ON, default OFF &#xA;        TNN  # need, if built lite.ai.toolkit with ENABLE_TNN=ON,  default OFF &#xA;        ${OpenCV_LIBS})  # link lite.ai.toolkit &amp;amp; other libs.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ./build/lite.ai.toolkit/lib &amp;amp;&amp;amp; otool -L liblite.ai.toolkit.0.0.1.dylib &#xA;liblite.ai.toolkit.0.0.1.dylib:&#xA;        @rpath/liblite.ai.toolkit.0.0.1.dylib (compatibility version 0.0.1, current version 0.0.1)&#xA;        @rpath/libopencv_highgui.4.5.dylib (compatibility version 4.5.0, current version 4.5.2)&#xA;        @rpath/libonnxruntime.1.7.0.dylib (compatibility version 0.0.0, current version 1.7.0)&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ../ &amp;amp;&amp;amp; tree .&#xA;‚îú‚îÄ‚îÄ bin&#xA;‚îú‚îÄ‚îÄ include&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ lite&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ backend.h&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ config.h&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ lite.h&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ ort&#xA;‚îî‚îÄ‚îÄ lib&#xA;    ‚îî‚îÄ‚îÄ liblite.ai.toolkit.0.0.1.dylib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the built examples:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ./build/lite.ai.toolkit/bin &amp;amp;&amp;amp; ls -lh | grep lite&#xA;-rwxr-xr-x  1 root  staff   301K Jun 26 23:10 liblite.ai.toolkit.0.0.1.dylib&#xA;...&#xA;-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov4&#xA;-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov5&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./lite_yolov5&#xA;LITEORT_DEBUG LogId: ../../../hub/onnx/cv/yolov5s.onnx&#xA;=============== Input-Dims ==============&#xA;...&#xA;detected num_anchors: 25200&#xA;generate_bboxes num: 66&#xA;Default Version Detected Boxes Num: 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To link &lt;code&gt;lite.ai.toolkit&lt;/code&gt; shared lib. You need to make sure that &lt;code&gt;OpenCV&lt;/code&gt; and &lt;code&gt;onnxruntime&lt;/code&gt; are linked correctly. A minimum example to show you how to link the shared lib of Lite.AI.ToolKit correctly for your own project can be found at &lt;a href=&#34;https://github.com/DefTruth/RobustVideoMatting-ncnn-mnn-tnn-onnxruntime/raw/main/CMakeLists.txt&#34;&gt;CMakeLists.txt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;5. Model Zoo.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Model-Zoo&#34;&gt;&lt;/div&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-2&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; contains almost &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;100+&lt;/a&gt;&lt;/strong&gt; AI models with &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;500+&lt;/a&gt;&lt;/strong&gt; frozen pretrained files now. Most of the files are converted by myself. You can use it through &lt;strong&gt;lite::cv::Type::Class&lt;/strong&gt; syntax, such as &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-object-detection&#34;&gt;lite::cv::detection::YoloV5&lt;/a&gt;&lt;/strong&gt;. More details can be found at &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;Examples for Lite.Ai.ToolKit&lt;/a&gt;. Note, for Google Drive, I can not upload all the *.onnx files because of the storage limitation (15G).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;File&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Baidu Drive&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Google Drive&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Docker Hub&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hub (Docs)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ONNX&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1elUGcx7CZkkjEoYhTMwTRQ&#34;&gt;Baidu Drive&lt;/a&gt; code: 8gin&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1p6uBcxGeyS1exc-T61vL8YRhwjYL4iD2?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-onnx-hub/tags&#34;&gt;ONNX Docker v0.1.22.01.08 (28G), v0.1.22.02.02 (400M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;ONNX Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1KyO-bCYUv6qPq2M8BH_Okg&#34;&gt;Baidu Drive&lt;/a&gt; code: 9v63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-mnn-hub/tags&#34;&gt;MNN Docker v0.1.22.01.08 (11G), v0.1.22.02.02 (213M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md&#34;&gt;MNN Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NCNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1hlnqyNsFbMseGFWscgVhgQ&#34;&gt;Baidu Drive&lt;/a&gt; code: sc7f&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-ncnn-hub/tags&#34;&gt;NCNN Docker v0.1.22.01.08 (9G), v0.1.22.02.02 (197M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md&#34;&gt;NCNN Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw&#34;&gt;Baidu Drive&lt;/a&gt; code: 6o6k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùî&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-tnn-hub/tags&#34;&gt;TNN Docker v0.1.22.01.08 (11G), v0.1.22.02.02 (217M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md&#34;&gt;TNN Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.01.08  # (28G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08   # (11G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.01.08  # (9G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.01.08   # (11G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.02.02  # (400M) + YOLO5Face&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.02.02   # (213M) + YOLO5Face&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.02.02  # (197M) + YOLO5Face&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.02.02   # (217M) + YOLO5Face&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; ‚ùáÔ∏è Lite.Ai.ToolKit modules.&lt;/summary&gt; &#xA; &lt;h3&gt;Namespace and Lite.Ai.ToolKit modules.&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Namespace&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Details&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::detection&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Object Detection. one-stage and anchor-free detectors, YoloV5, YoloV4, SSD, etc. ‚úÖ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::classification&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Image Classification. DensNet, ShuffleNet, ResNet, IBNNet, GhostNet, etc. ‚úÖ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::faceid&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Recognition. ArcFace, CosFace, CurricularFace, etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Analysis. &lt;em&gt;detect&lt;/em&gt;, &lt;em&gt;align&lt;/em&gt;, &lt;em&gt;pose&lt;/em&gt;, &lt;em&gt;attr&lt;/em&gt;, etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::detect&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Detection. UltraFace, RetinaFace, FaceBoxes, PyramidBox, etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::align&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Alignment. PFLD(106), FaceLandmark1000(1000 landmarks), PRNet, etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::align3d&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;3D Face Alignment. FaceMesh(468 3D landmarks), IrisLandmark(71+5 3D landmarks), etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::pose&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Head Pose Estimation. FSANet, etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::attr&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Attributes. Emotion, Age, Gender. EmotionFerPlus, VGG16Age, etc. ‚ùáÔ∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::segmentation&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Object Segmentation. Such as FCN, DeepLabV3, etc. ‚ùáÔ∏è Ô∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::style&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Style Transfer. Contains neural style transfer now, such as FastStyleTransfer. ‚ö†Ô∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::matting&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Image Matting. Object and Human matting. ‚ùáÔ∏è Ô∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::colorization&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Colorization. Make Gray image become RGB. ‚ö†Ô∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::resolution&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Super Resolution. ‚ö†Ô∏è&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Lite.Ai.ToolKit&#39;s Classes and Pretrained Files.&lt;/h3&gt; &#xA; &lt;p&gt;Correspondence between the classes in &lt;strong&gt;Lite.AI.ToolKit&lt;/strong&gt; and pretrained model files can be found at &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;lite.ai.toolkit.hub.onnx.md&lt;/a&gt;. For examples, the pretrained model files for &lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt; and &lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt; are listed as follows.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Pretrained ONNX Files&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rename or Converted From (Repo)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5l.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (üî•üî•üí•‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;188Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5m.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (üî•üî•üí•‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;85Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5s.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (üî•üî•üí•‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5x.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (üî•üî•üí•‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;351Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_x.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•!!‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;378Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_l.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•!!‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;207Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_m.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•!!‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;97Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_s.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•!!‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;34Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_tiny.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•!!‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;19Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_nano.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•!!‚Üë)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.5Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;It means that you can load the the any one &lt;code&gt;yolov5*.onnx&lt;/code&gt; and &lt;code&gt;yolox_*.onnx&lt;/code&gt; according to your application through the same Lite.AI.ToolKit&#39;s classes, such as &lt;em&gt;YoloV5&lt;/em&gt;, &lt;em&gt;YoloX&lt;/em&gt;, etc.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5x.onnx&#34;);  // for server&#xA;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5l.onnx&#34;); &#xA;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5m.onnx&#34;);  &#xA;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5s.onnx&#34;);  // for mobile device &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_x.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_l.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_m.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_s.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_tiny.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_nano.onnx&#34;);  // 3.5Mb only !&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; üîëÔ∏è How to download Model Zoo from Docker Hub?&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Firstly, pull the image from docker hub. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08 # (11G)&#xA;docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.01.08 # (9G)&#xA;docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.01.08 # (11G)&#xA;docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.01.08 # (28G)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Secondly, run the container with local &lt;code&gt;share&lt;/code&gt; dir using &lt;code&gt;docker run -idt xxx&lt;/code&gt;. A minimum example will show you as follows. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;make a &lt;code&gt;share&lt;/code&gt; dir in your local device.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir share # any name is ok.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;write &lt;code&gt;run_mnn_docker_hub.sh&lt;/code&gt; script like:&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash  &#xA;PORT1=6072&#xA;PORT2=6084&#xA;SERVICE_DIR=/Users/xxx/Desktop/your-path-to/share&#xA;CONRAINER_DIR=/home/hub/share&#xA;CONRAINER_NAME=mnn_docker_hub_d&#xA;&#xA;docker run -idt -p ${PORT2}:${PORT1} -v ${SERVICE_DIR}:${CONRAINER_DIR} --shm-size=16gb --name ${CONRAINER_NAME} qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Finally, copy the model weights from &lt;code&gt;/home/hub/mnn/cv&lt;/code&gt; to your local &lt;code&gt;share&lt;/code&gt; dir. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# activate mnn docker.&#xA;sh ./run_mnn_docker_hub.sh&#xA;docker exec -it mnn_docker_hub_d /bin/bash&#xA;# copy the models to the share dir.&#xA;cd /home/hub &#xA;cp -rf mnn/cv share/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Model Hubs&lt;/h3&gt; &#xA;&lt;p&gt;The pretrained and converted ONNX files provide by lite.ai.toolkit are listed as follows. Also, see &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Model-Zoo&#34;&gt;Model Zoo&lt;/a&gt; and &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;ONNX Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md&#34;&gt;MNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md&#34;&gt;TNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md&#34;&gt;NCNN Hub&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;6. Examples.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;More examples can be found at &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/examples/lite/cv&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-object-detection&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example0: Object Detection using &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/yolov5s.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_yolov5_1.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_yolov5_1.jpg&#34;;&#xA;&#xA;  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); &#xA;  std::vector&amp;lt;lite::types::Boxf&amp;gt; detected_boxes;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  yolov5-&amp;gt;detect(img_bgr, detected_boxes);&#xA;  &#xA;  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);&#xA;  cv::imwrite(save_img_path, img_bgr);  &#xA;  &#xA;  delete yolov5;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_yolov5_1.jpg&#34; height=&#34;256px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_yolov5_2.jpg&#34; height=&#34;256px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Or you can use Newest üî•üî• ! YOLO series&#39;s detector &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; or &lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;YoloR&lt;/a&gt;. They got the similar results.&lt;/p&gt; &#xA;&lt;p&gt;More classes for general object detection (80 classes, COCO).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *detector = new lite::cv::detection::YoloX(onnx_path);  // Newest YOLO detector !!! 2021-07&#xA;auto *detector = new lite::cv::detection::YoloV4(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloV3(onnx_path); &#xA;auto *detector = new lite::cv::detection::TinyYoloV3(onnx_path); &#xA;auto *detector = new lite::cv::detection::SSD(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloV5(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloR(onnx_path);  // Newest YOLO detector !!! 2021-05&#xA;auto *detector = new lite::cv::detection::TinyYoloV4VOC(onnx_path); &#xA;auto *detector = new lite::cv::detection::TinyYoloV4COCO(onnx_path); &#xA;auto *detector = new lite::cv::detection::ScaledYoloV4(onnx_path); &#xA;auto *detector = new lite::cv::detection::EfficientDet(onnx_path); &#xA;auto *detector = new lite::cv::detection::EfficientDetD7(onnx_path); &#xA;auto *detector = new lite::cv::detection::EfficientDetD8(onnx_path); &#xA;auto *detector = new lite::cv::detection::YOLOP(onnx_path);&#xA;auto *detector = new lite::cv::detection::NanoDet(onnx_path); // Super fast and tiny!&#xA;auto *detector = new lite::cv::detection::NanoDetPlus(onnx_path); // Super fast and tiny! 2021/12/25&#xA;auto *detector = new lite::cv::detection::NanoDetEfficientNetLite(onnx_path); // Super fast and tiny!&#xA;auto *detector = new lite::cv::detection::YoloV5_V_6_0(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloV5_V_6_1(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloX_V_0_1_1(onnx_path);  // Newest YOLO detector !!! 2021-07&#xA;auto *detector = new lite::cv::detection::YOLOv6(onnx_path);  // Newest 2022 YOLO detector !!!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-matting&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example1: Video Matting using &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting2021üî•üî•üî•&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/rvm_mobilenetv3_fp32.onnx&#34;;&#xA;  std::string video_path = &#34;../../../examples/lite/resources/test_lite_rvm_0.mp4&#34;;&#xA;  std::string output_path = &#34;../../../logs/test_lite_rvm_0.mp4&#34;;&#xA;  std::string background_path = &#34;../../../examples/lite/resources/test_lite_matting_bgr.jpg&#34;;&#xA;  &#xA;  auto *rvm = new lite::cv::matting::RobustVideoMatting(onnx_path, 16); // 16 threads&#xA;  std::vector&amp;lt;lite::types::MattingContent&amp;gt; contents;&#xA;  &#xA;  // 1. video matting.&#xA;  cv::Mat background = cv::imread(background_path);&#xA;  rvm-&amp;gt;detect_video(video_path, output_path, contents, false, 0.4f,&#xA;                    20, true, true, background);&#xA;  &#xA;  delete rvm;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/interviewi.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/interview.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3i.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/teslai.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/tesla.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/b5i.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/b5.gif&#34; height=&#34;150px&#34; width=&#34;150px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for matting (image matting, video matting, trimap/mask-free, trimap/mask-based)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *matting = new lite::cv::matting::RobustVideoMatting:(onnx_path);  //  WACV 2022.&#xA;auto *matting = new lite::cv::matting::MGMatting(onnx_path); // CVPR 2021&#xA;auto *matting = new lite::cv::matting::MODNet(onnx_path); // AAAI 2022&#xA;auto *matting = new lite::cv::matting::MODNetDyn(onnx_path); // AAAI 2022 Dynamic Shape Inference.&#xA;auto *matting = new lite::cv::matting::BackgroundMattingV2(onnx_path); // CVPR 2020 &#xA;auto *matting = new lite::cv::matting::BackgroundMattingV2Dyn(onnx_path); // CVPR 2020 Dynamic Shape Inference.&#xA;auto *matting = new lite::cv::matting::MobileHumanMatting(onnx_path); // 3Mb only !!!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-alignment&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example2: 1000 Facial Landmarks Detection using &lt;a href=&#34;https://github.com/Single430/FaceLandmark1000&#34;&gt;FaceLandmarks1000&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/FaceLandmark1000.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_face_landmarks_0.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_face_landmarks_1000.jpg&#34;;&#xA;    &#xA;  auto *face_landmarks_1000 = new lite::cv::face::align::FaceLandmark1000(onnx_path);&#xA;&#xA;  lite::types::Landmarks landmarks;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  face_landmarks_1000-&amp;gt;detect(img_bgr, landmarks);&#xA;  lite::utils::draw_landmarks_inplace(img_bgr, landmarks);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;  &#xA;  delete face_landmarks_1000;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000_0.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face alignment (68 points, 98 points, 106 points, 1000 points)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *align = new lite::cv::face::align::PFLD(onnx_path);  // 106 landmarks, 1.0Mb only!&#xA;auto *align = new lite::cv::face::align::PFLD98(onnx_path);  // 98 landmarks, 4.8Mb only!&#xA;auto *align = new lite::cv::face::align::PFLD68(onnx_path);  // 68 landmarks, 2.8Mb only!&#xA;auto *align = new lite::cv::face::align::MobileNetV268(onnx_path);  // 68 landmarks, 9.4Mb only!&#xA;auto *align = new lite::cv::face::align::MobileNetV2SE68(onnx_path);  // 68 landmarks, 11Mb only!&#xA;auto *align = new lite::cv::face::align::FaceLandmark1000(onnx_path);  // 1000 landmarks, 2.0Mb only!&#xA;auto *align = new lite::cv::face::align::PIPNet98(onnx_path);  // 98 landmarks, CVPR2021!&#xA;auto *align = new lite::cv::face::align::PIPNet68(onnx_path);  // 68 landmarks, CVPR2021!&#xA;auto *align = new lite::cv::face::align::PIPNet29(onnx_path);  // 29 landmarks, CVPR2021!&#xA;auto *align = new lite::cv::face::align::PIPNet19(onnx_path);  // 19 landmarks, CVPR2021!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-colorization&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example3: Colorization using &lt;a href=&#34;https://github.com/richzhang/colorization&#34;&gt;colorization&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/eccv16-colorizer.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_colorizer_1.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_eccv16_colorizer_1.jpg&#34;;&#xA;  &#xA;  auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);&#xA;  &#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  lite::types::ColorizeContent colorize_content;&#xA;  colorizer-&amp;gt;detect(img_bgr, colorize_content);&#xA;  &#xA;  if (colorize_content.flag) cv::imwrite(save_img_path, colorize_content.mat);&#xA;  delete colorizer;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_colorizer_1.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_colorizer_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_colorizer_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_siggraph17_colorizer_1.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_siggraph17_colorizer_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_siggraph17_colorizer_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for colorization (gray to rgb)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-recognition&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example4: Face Recognition using &lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch&#34;&gt;ArcFace&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/ms1mv3_arcface_r100.onnx&#34;;&#xA;  std::string test_img_path0 = &#34;../../../examples/lite/resources/test_lite_faceid_0.png&#34;;&#xA;  std::string test_img_path1 = &#34;../../../examples/lite/resources/test_lite_faceid_1.png&#34;;&#xA;  std::string test_img_path2 = &#34;../../../examples/lite/resources/test_lite_faceid_2.png&#34;;&#xA;&#xA;  auto *glint_arcface = new lite::cv::faceid::GlintArcFace(onnx_path);&#xA;&#xA;  lite::types::FaceContent face_content0, face_content1, face_content2;&#xA;  cv::Mat img_bgr0 = cv::imread(test_img_path0);&#xA;  cv::Mat img_bgr1 = cv::imread(test_img_path1);&#xA;  cv::Mat img_bgr2 = cv::imread(test_img_path2);&#xA;  glint_arcface-&amp;gt;detect(img_bgr0, face_content0);&#xA;  glint_arcface-&amp;gt;detect(img_bgr1, face_content1);&#xA;  glint_arcface-&amp;gt;detect(img_bgr2, face_content2);&#xA;&#xA;  if (face_content0.flag &amp;amp;&amp;amp; face_content1.flag &amp;amp;&amp;amp; face_content2.flag)&#xA;  {&#xA;    float sim01 = lite::utils::math::cosine_similarity&amp;lt;float&amp;gt;(&#xA;        face_content0.embedding, face_content1.embedding);&#xA;    float sim02 = lite::utils::math::cosine_similarity&amp;lt;float&amp;gt;(&#xA;        face_content0.embedding, face_content2.embedding);&#xA;    std::cout &amp;lt;&amp;lt; &#34;Detected Sim01: &#34; &amp;lt;&amp;lt; sim  &amp;lt;&amp;lt; &#34; Sim02: &#34; &amp;lt;&amp;lt; sim02 &amp;lt;&amp;lt; std::endl;&#xA;  }&#xA;&#xA;  delete glint_arcface;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_arcface_resnet_0.png&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_arcface_resnet_1.png&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_arcface_resnet_2.png&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Detected Sim01: 0.721159 Sim02: -0.0626267&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;More classes for face recognition (face id vector extract)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *recognition = new lite::cv::faceid::GlintCosFace(onnx_path);  // DeepGlint(insightface)&#xA;auto *recognition = new lite::cv::faceid::GlintArcFace(onnx_path);  // DeepGlint(insightface)&#xA;auto *recognition = new lite::cv::faceid::GlintPartialFC(onnx_path); // DeepGlint(insightface)&#xA;auto *recognition = new lite::cv::faceid::FaceNet(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::FocalArcFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::FocalAsiaArcFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::TencentCurricularFace(onnx_path); // Tencent(TFace)&#xA;auto *recognition = new lite::cv::faceid::TencentCifpFace(onnx_path); // Tencent(TFace)&#xA;auto *recognition = new lite::cv::faceid::CenterLossFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::SphereFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::PoseRobustFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::NaivePoseRobustFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::MobileFaceNet(onnx_path); // 3.8Mb only !&#xA;auto *recognition = new lite::cv::faceid::CavaGhostArcFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::CavaCombinedFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::MobileSEFocalFace(onnx_path); // 4.5Mb only !&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-detection&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example5: Face Detection using &lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;SCRFD 2021&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/scrfd_2.5g_bnkps_shape640x640.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_face_detector.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_scrfd.jpg&#34;;&#xA;  &#xA;  auto *scrfd = new lite::cv::face::detect::SCRFD(onnx_path);&#xA;  &#xA;  std::vector&amp;lt;lite::types::BoxfWithLandmarks&amp;gt; detected_boxes;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  scrfd-&amp;gt;detect(img_bgr, detected_boxes);&#xA;  &#xA;  lite::utils::draw_boxes_with_landmarks_inplace(img_bgr, detected_boxes);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;  &#xA;  delete scrfd;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face detection (super fast face detection)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *detector = new lite::face::detect::UltraFace(onnx_path);  // 1.1Mb only !&#xA;auto *detector = new lite::face::detect::FaceBoxes(onnx_path);  // 3.8Mb only ! &#xA;auto *detector = new lite::face::detect::FaceBoxesv2(onnx_path);  // 4.0Mb only ! &#xA;auto *detector = new lite::face::detect::RetinaFace(onnx_path);  // 1.6Mb only ! CVPR2020&#xA;auto *detector = new lite::face::detect::SCRFD(onnx_path);  // 2.5Mb only ! CVPR2021, Super fast and accurate!!&#xA;auto *detector = new lite::face::detect::YOLO5Face(onnx_path);  // 2021, Super fast and accurate!!&#xA;auto *detector = new lite::face::detect::YOLOv5BlazeFace(onnx_path);  // 2021, Super fast and accurate!!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-segmentation&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example6: Object Segmentation using &lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&#34;&gt;DeepLabV3ResNet101&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/deeplabv3_resnet101_coco.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_deeplabv3_resnet101.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_deeplabv3_resnet101.jpg&#34;;&#xA;&#xA;  auto *deeplabv3_resnet101 = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path, 16); // 16 threads&#xA;&#xA;  lite::types::SegmentContent content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  deeplabv3_resnet101-&amp;gt;detect(img_bgr, content);&#xA;&#xA;  if (content.flag)&#xA;  {&#xA;    cv::Mat out_img;&#xA;    cv::addWeighted(img_bgr, 0.2, content.color_mat, 0.8, 0., out_img);&#xA;    cv::imwrite(save_img_path, out_img);&#xA;    if (!content.names_map.empty())&#xA;    {&#xA;      for (auto it = content.names_map.begin(); it != content.names_map.end(); ++it)&#xA;      {&#xA;        std::cout &amp;lt;&amp;lt; it-&amp;gt;first &amp;lt;&amp;lt; &#34; Name: &#34; &amp;lt;&amp;lt; it-&amp;gt;second &amp;lt;&amp;lt; std::endl;&#xA;      }&#xA;    }&#xA;  }&#xA;  delete deeplabv3_resnet101;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_deeplabv3_resnet101.png&#34; height=&#34;256px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_deeplabv3_resnet101.jpg&#34; height=&#34;256px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for object segmentation (general objects segmentation)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *segment = new lite::cv::segmentation::FCNResNet101(onnx_path);&#xA;auto *segment = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-attributes-analysis&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example7: Age Estimation using &lt;a href=&#34;https://github.com/oukohou/SSR_Net_Pytorch&#34;&gt;SSRNet&lt;/a&gt; . Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/ssrnet.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_ssrnet.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_ssrnet.jpg&#34;;&#xA;&#xA;  auto *ssrnet = new lite::cv::face::attr::SSRNet(onnx_path);&#xA;&#xA;  lite::types::Age age;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  ssrnet-&amp;gt;detect(img_bgr, age);&#xA;  lite::utils::draw_age_inplace(img_bgr, age);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;&#xA;  delete ssrnet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_ssrnet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_gender_googlenet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_emotion_ferplus.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face attributes analysis (age, gender, emotion)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *attribute = new lite::cv::face::attr::AgeGoogleNet(onnx_path);  &#xA;auto *attribute = new lite::cv::face::attr::GenderGoogleNet(onnx_path); &#xA;auto *attribute = new lite::cv::face::attr::EmotionFerPlus(onnx_path);&#xA;auto *attribute = new lite::cv::face::attr::VGG16Age(onnx_path);&#xA;auto *attribute = new lite::cv::face::attr::VGG16Gender(onnx_path);&#xA;auto *attribute = new lite::cv::face::attr::EfficientEmotion7(onnx_path); // 7 emotions, 15Mb only!&#xA;auto *attribute = new lite::cv::face::attr::EfficientEmotion8(onnx_path); // 8 emotions, 15Mb only!&#xA;auto *attribute = new lite::cv::face::attr::MobileEmotion7(onnx_path); // 7 emotions, 13Mb only!&#xA;auto *attribute = new lite::cv::face::attr::ReXNetEmotion7(onnx_path); // 7 emotions&#xA;auto *attribute = new lite::cv::face::attr::SSRNet(onnx_path); // age estimation, 190kb only!!!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-image-classification&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example8: 1000 Classes Classification using &lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_densenet/&#34;&gt;DenseNet&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/densenet121.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_densenet.jpg&#34;;&#xA;&#xA;  auto *densenet = new lite::cv::classification::DenseNet(onnx_path);&#xA;&#xA;  lite::types::ImageNetContent content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  densenet-&amp;gt;detect(img_bgr, content);&#xA;  if (content.flag)&#xA;  {&#xA;    const unsigned int top_k = content.scores.size();&#xA;    if (top_k &amp;gt; 0)&#xA;    {&#xA;      for (unsigned int i = 0; i &amp;lt; top_k; ++i)&#xA;        std::cout &amp;lt;&amp;lt; i + 1&#xA;                  &amp;lt;&amp;lt; &#34;: &#34; &amp;lt;&amp;lt; content.labels.at(i)&#xA;                  &amp;lt;&amp;lt; &#34;: &#34; &amp;lt;&amp;lt; content.texts.at(i)&#xA;                  &amp;lt;&amp;lt; &#34;: &#34; &amp;lt;&amp;lt; content.scores.at(i)&#xA;                  &amp;lt;&amp;lt; std::endl;&#xA;    }&#xA;  }&#xA;  delete densenet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_densenet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_densenet.png&#34; height=&#34;224px&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for image classification (1000 classes)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *classifier = new lite::cv::classification::EfficientNetLite4(onnx_path);  &#xA;auto *classifier = new lite::cv::classification::ShuffleNetV2(onnx_path); // 8.7Mb only!&#xA;auto *classifier = new lite::cv::classification::GhostNet(onnx_path);&#xA;auto *classifier = new lite::cv::classification::HdrDNet(onnx_path);&#xA;auto *classifier = new lite::cv::classification::IBNNet(onnx_path);&#xA;auto *classifier = new lite::cv::classification::MobileNetV2(onnx_path); // 13Mb only!&#xA;auto *classifier = new lite::cv::classification::ResNet(onnx_path); &#xA;auto *classifier = new lite::cv::classification::ResNeXt(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-head-pose-estimation&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example9: Head Pose Estimation using &lt;a href=&#34;https://github.com/omasaht/headpose-fsanet-pytorch&#34;&gt;FSANet&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/fsanet-var.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_fsanet.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_fsanet.jpg&#34;;&#xA;&#xA;  auto *fsanet = new lite::cv::face::pose::FSANet(onnx_path);&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  lite::types::EulerAngles euler_angles;&#xA;  fsanet-&amp;gt;detect(img_bgr, euler_angles);&#xA;  &#xA;  if (euler_angles.flag)&#xA;  {&#xA;    lite::utils::draw_axis_inplace(img_bgr, euler_angles);&#xA;    cv::imwrite(save_img_path, img_bgr);&#xA;    std::cout &amp;lt;&amp;lt; &#34;yaw:&#34; &amp;lt;&amp;lt; euler_angles.yaw &amp;lt;&amp;lt; &#34; pitch:&#34; &amp;lt;&amp;lt; euler_angles.pitch &amp;lt;&amp;lt; &#34; row:&#34; &amp;lt;&amp;lt; euler_angles.roll &amp;lt;&amp;lt; std::endl;&#xA;  }&#xA;  delete fsanet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for head pose estimation (euler angle, yaw, pitch, roll)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *pose = new lite::cv::face::pose::FSANet(onnx_path); // 1.2Mb only!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-style-transfer&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example10: Style Transfer using &lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/style_transfer/fast_neural_style&#34;&gt;FastStyleTransfer&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/style-candy-8.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_fast_style_transfer.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_fast_style_transfer_candy.jpg&#34;;&#xA;  &#xA;  auto *fast_style_transfer = new lite::cv::style::FastStyleTransfer(onnx_path);&#xA; &#xA;  lite::types::StyleContent style_content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  fast_style_transfer-&amp;gt;detect(img_bgr, style_content);&#xA;&#xA;  if (style_content.flag) cv::imwrite(save_img_path, style_content.mat);&#xA;  delete fast_style_transfer;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_fast_style_transfer.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_candy.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_mosaic.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_pointilism.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_rain_princes.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_udnie.jpg&#34; height=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for style transfer (neural style transfer, others)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *transfer = new lite::cv::style::FastStyleTransfer(onnx_path); // 6.4Mb only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Example11: Human Head Segmentation using &lt;a href=&#34;https://github.com/minivision-ai/photo2cartoon&#34;&gt;HeadSeg&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/minivision_head_seg.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_head_seg.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_head_seg.jpg&#34;;&#xA;&#xA;  auto *head_seg = new lite::cv::segmentation::HeadSeg(onnx_path, 4); // 4 threads&#xA;&#xA;  lite::types::HeadSegContent content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  head_seg-&amp;gt;detect(img_bgr, content);&#xA;  if (content.flag) cv::imwrite(save_img_path, content.mask * 255.f);&#xA;&#xA;  delete head_seg;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg.png&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg_mask.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg_1.png&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg_1_mask.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for human segmentation (head, portrait, hair, others)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *segment = new lite::cv::segmentation::HeadSeg(onnx_path); // 31Mb&#xA;auto *segment = new lite::cv::segmentation::FastPortraitSeg(onnx_path); // &amp;lt;= 400Kb !!! &#xA;auto *segment = new lite::cv::segmentation::PortraitSegSINet(onnx_path); // &amp;lt;= 380Kb !!!&#xA;auto *segment = new lite::cv::segmentation::PortraitSegExtremeC3Net(onnx_path); // &amp;lt;= 180Kb !!! Extreme Tiny !!!&#xA;auto *segment = new lite::cv::segmentation::FaceHairSeg(onnx_path); // 18M&#xA;auto *segment = new lite::cv::segmentation::HairSeg(onnx_path); // 18M&#xA;auto *segment = new lite::cv::segmentation::MobileHairSeg(onnx_path); // 14M&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Example12: Photo transfer to Cartoon &lt;a href=&#34;https://github.com/minivision-ai/photo2cartoon&#34;&gt;Photo2Cartoon&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string head_seg_onnx_path = &#34;../../../hub/onnx/cv/minivision_head_seg.onnx&#34;;&#xA;  std::string cartoon_onnx_path = &#34;../../../hub/onnx/cv/minivision_female_photo2cartoon.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_female_photo2cartoon.jpg&#34;;&#xA;  std::string save_mask_path = &#34;../../../logs/test_lite_female_photo2cartoon_seg.jpg&#34;;&#xA;  std::string save_cartoon_path = &#34;../../../logs/test_lite_female_photo2cartoon_cartoon.jpg&#34;;&#xA;&#xA;  auto *head_seg = new lite::cv::segmentation::HeadSeg(head_seg_onnx_path, 4); // 4 threads&#xA;  auto *female_photo2cartoon = new lite::cv::style::FemalePhoto2Cartoon(cartoon_onnx_path, 4); // 4 threads&#xA;&#xA;  lite::types::HeadSegContent head_seg_content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  head_seg-&amp;gt;detect(img_bgr, head_seg_content);&#xA;&#xA;  if (head_seg_content.flag &amp;amp;&amp;amp; !head_seg_content.mask.empty())&#xA;  {&#xA;    cv::imwrite(save_mask_path, head_seg_content.mask * 255.f);&#xA;    // Female Photo2Cartoon Style Transfer&#xA;    lite::types::FemalePhoto2CartoonContent female_cartoon_content;&#xA;    female_photo2cartoon-&amp;gt;detect(img_bgr, head_seg_content.mask, female_cartoon_content);&#xA;    &#xA;    if (female_cartoon_content.flag &amp;amp;&amp;amp; !female_cartoon_content.cartoon.empty())&#xA;      cv::imwrite(save_cartoon_path, female_cartoon_content.cartoon);&#xA;  }&#xA;&#xA;  delete head_seg;&#xA;  delete female_photo2cartoon;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/head_seg.png&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/female_photo2cartoon_cartoon_out.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/female_photo2cartoon_1.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/female_photo2cartoon_cartoon_1_out.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for photo style transfer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *transfer = new lite::cv::style::FemalePhoto2Cartoon(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Example13: Face Parsing using &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;FaceParsing&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/face_parsing_512x512.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_face_parsing.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_face_parsing_bisenet.jpg&#34;;&#xA;&#xA;  auto *face_parsing_bisenet = new lite::cv::segmentation::FaceParsingBiSeNet(onnx_path, 8); // 8 threads&#xA;&#xA;  lite::types::FaceParsingContent content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  face_parsing_bisenet-&amp;gt;detect(img_bgr, content);&#xA;&#xA;  if (content.flag &amp;amp;&amp;amp; !content.merge.empty())&#xA;    cv::imwrite(save_img_path, content.merge);&#xA;  &#xA;  delete face_parsing_bisenet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/face_parsing.png&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/face_parsing_merge.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/face_parsing_1.png&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/face_parsing_1_merge.jpg&#34; height=&#34;180px&#34; width=&#34;180px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face parsing (hair, eyes, nose, mouth, others)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *segment = new lite::cv::segmentation::FaceParsingBiSeNet(onnx_path); // 50Mb&#xA;auto *segment = new lite::cv::segmentation::FaceParsingBiSeNetDyn(onnx_path); // Dynamic Shape Inference.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;7. License.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-License&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;The code of &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Introduction&#34;&gt;Lite.Ai.ToolKit&lt;/a&gt; is released under the GPL-3.0 License.&lt;/p&gt; &#xA;&lt;h2&gt;8. References.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-References&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;Many thanks to these following projects. All the Lite.AI.ToolKit&#39;s models are sourced from these repos.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting&lt;/a&gt; (üî•üî•üî•new!!‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;nanodet&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (üî•üî•üî•new!!‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;YOLOP&lt;/a&gt; (üî•üî•new!!‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;YOLOR&lt;/a&gt; (üî•üî•new!!‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;ScaledYOLOv4&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (üî•üî•üí•‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/TFace&#34;&gt;TFace&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/argusswift/YOLOv4-pytorch&#34;&gt;YOLOv4-pytorch&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34;&gt;Ultra-Light-Fast-Generic-Face-Detector-1MB&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Expand for More References.&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/omasaht/headpose-fsanet-pytorch&#34;&gt;headpose-fsanet-pytorch&lt;/a&gt; (üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Hsintao/pfld_106_face_landmarks&#34;&gt;pfld_106_face_landmarks&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/models&#34;&gt;onnx-models&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/oukohou/SSR_Net_Pytorch&#34;&gt;SSR_Net_Pytorch&lt;/a&gt; (üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/richzhang/colorization&#34;&gt;colorization&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/niazwazir/SUB_PIXEL_CNN&#34;&gt;SUB_PIXEL_CNN&lt;/a&gt; (üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/vision&#34;&gt;torchvision&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/timesler/facenet-pytorch&#34;&gt;facenet-pytorch&lt;/a&gt; (üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ZhaoJ9014/face.evoLVe.PyTorch&#34;&gt;face.evoLVe.PyTorch&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/louis-she/center-loss.pytorch&#34;&gt;center-loss.pytorch&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/clcarwin/sphereface_pytorch&#34;&gt;sphereface_pytorch&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/penincillin/DREAM&#34;&gt;DREAM&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Xiaoccer/MobileFaceNet_Pytorch&#34;&gt;MobileFaceNet_Pytorch&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch&#34;&gt;cavaface.pytorch&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/HuangYG123/CurricularFace&#34;&gt;CurricularFace&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;face-emotion-recognition&lt;/a&gt; (üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/grib0ed0v/face_recognition.pytorch&#34;&gt;face_recognition.pytorch&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/polarisZhao/PFLD-pytorch&#34;&gt;PFLD-pytorch&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;pytorch_face_landmark&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Single430/FaceLandmark1000&#34;&gt;FaceLandmark1000&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/biubug6/Pytorch_Retinaface&#34;&gt;Pytorch_Retinaface&lt;/a&gt; (üî•üî•üî•‚Üë)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/zisianw/FaceBoxes.PyTorch&#34;&gt;FaceBoxes&lt;/a&gt; (üî•üî•‚Üë)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;9. Compilation Options.&lt;/h2&gt; &#xA;&lt;p&gt;In addition, &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;NCNN&lt;/a&gt; and &lt;a href=&#34;https://github.com/Tencent/TNN&#34;&gt;TNN&lt;/a&gt; support for some models will be added in the future, but due to operator compatibility and some other reasons, it is impossible to ensure that all models supported by &lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNXRuntime C++&lt;/a&gt; can run through &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;NCNN&lt;/a&gt; and &lt;a href=&#34;https://github.com/Tencent/TNN&#34;&gt;TNN&lt;/a&gt;. So, if you want to use all the models supported by this repo and don&#39;t care about the performance gap of &lt;em&gt;1~2ms&lt;/em&gt;, just let &lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNXRuntime&lt;/a&gt; as default inference engine for this repo. However, you can follow the steps below if you want to build with &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;NCNN&lt;/a&gt; or &lt;a href=&#34;https://github.com/Tencent/TNN&#34;&gt;TNN&lt;/a&gt; support.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;change the &lt;code&gt;build.sh&lt;/code&gt; with &lt;code&gt;DENABLE_MNN=ON&lt;/code&gt;,&lt;code&gt;DENABLE_NCNN=ON&lt;/code&gt; or &lt;code&gt;DENABLE_TNN=ON&lt;/code&gt;, such as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd build &amp;amp;&amp;amp; cmake \&#xA;  -DCMAKE_BUILD_TYPE=MinSizeRel \&#xA;  -DINCLUDE_OPENCV=ON \   # Whether to package OpenCV into lite.ai.toolkit, default ON; otherwise, you need to setup OpenCV yourself.&#xA;  -DENABLE_MNN=ON \       # Whether to build with MNN,  default OFF, only some models are supported now.&#xA;  -DENABLE_NCNN=OFF \     # Whether to build with NCNN, default OFF, only some models are supported now.&#xA;  -DENABLE_TNN=OFF \      # Whether to build with TNN,  default OFF, only some models are supported now.&#xA;  .. &amp;amp;&amp;amp; make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;use the MNN, NCNN or TNN version interface, see &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet.cpp&#34;&gt;demo&lt;/a&gt;, such as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto *nanodet = new lite::mnn::cv::detection::NanoDet(mnn_path);&#xA;auto *nanodet = new lite::tnn::cv::detection::NanoDet(proto_path, model_path);&#xA;auto *nanodet = new lite::ncnn::cv::detection::NanoDet(param_path, bin_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;10. Contribute&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Contribute&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;How to add your own models and become a contributor? See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/191&#34;&gt;CONTRIBUTING.zh.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;11. Many Thanks !!! ü§óüéâüéâ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/207&#34;&gt;Windows10 VS2019 CUDA 11.1 Build Docs&lt;/a&gt; (&lt;a href=&#34;https://github.com/zhanghongyong123456&#34;&gt;@zhanghongyong123456&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/docs/build/Linux.zh.md&#34;&gt;Linux Build Docs&lt;/a&gt; (&lt;a href=&#34;https://github.com/lee1221ee&#34;&gt;@lee1221ee&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/pull/105&#34;&gt;Some Windows10 Bugs Fixed&lt;/a&gt; (&lt;a href=&#34;https://github.com/ysc3839&#34;&gt;@ysc3839&lt;/a&gt;, &lt;a href=&#34;https://github.com/AvenSun&#34;&gt;@AvenSun&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>