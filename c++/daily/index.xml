<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-08T01:29:48Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>janhq/nitro</title>
    <updated>2024-01-08T01:29:48Z</updated>
    <id>tag:github.com,2024-01-08:/janhq/nitro</id>
    <link href="https://github.com/janhq/nitro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast, lightweight, embeddable inference engine to supercharge your apps with local AI. OpenAI-compatible API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nitro - Embeddable AI&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;nitrologo&#34; src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/assets/Nitro%20README%20banner.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://nitro.jan.ai/docs&#34;&gt;Documentation&lt;/a&gt; - &lt;a href=&#34;https://nitro.jan.ai/api&#34;&gt;API Reference&lt;/a&gt; - &lt;a href=&#34;https://github.com/janhq/nitro/releases/&#34;&gt;Changelog&lt;/a&gt; - &lt;a href=&#34;https://github.com/janhq/nitro/issues&#34;&gt;Bug reports&lt;/a&gt; - &lt;a href=&#34;https://discord.gg/AsJ8krTT3N&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Nitro is currently in Development&lt;/strong&gt;: Expect breaking changes and bugs!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast Inference: Built on top of the cutting-edge inference library llama.cpp, modified to be production ready.&lt;/li&gt; &#xA; &lt;li&gt;Lightweight: Only 3MB, ideal for resource-sensitive environments.&lt;/li&gt; &#xA; &lt;li&gt;Easily Embeddable: Simple integration into existing applications, offering flexibility.&lt;/li&gt; &#xA; &lt;li&gt;Quick Setup: Approximately 10-second initialization for swift deployment.&lt;/li&gt; &#xA; &lt;li&gt;Enhanced Web Framework: Incorporates drogon cpp to boost web service efficiency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About Nitro&lt;/h2&gt; &#xA;&lt;p&gt;Nitro is a high-efficiency C++ inference engine for edge computing, powering &lt;a href=&#34;https://jan.ai/&#34;&gt;Jan&lt;/a&gt;. It is lightweight and embeddable, ideal for product integration.&lt;/p&gt; &#xA;&lt;p&gt;The binary of nitro after zipped is only ~3mb in size with none to minimal dependencies (if you use a GPU need CUDA for example) make it desirable for any edge/server deployment üëç.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Read more about Nitro at &lt;a href=&#34;https://nitro.jan.ai/&#34;&gt;https://nitro.jan.ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Repo Structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ controllers&#xA;‚îú‚îÄ‚îÄ docs &#xA;‚îú‚îÄ‚îÄ llama.cpp -&amp;gt; Upstream llama C++&#xA;‚îú‚îÄ‚îÄ nitro_deps -&amp;gt; Dependencies of the Nitro project as a sub-project&#xA;‚îî‚îÄ‚îÄ utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Install Nitro&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For Linux and MacOS&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sfL https://raw.githubusercontent.com/janhq/nitro/main/install.sh | sudo /bin/bash -&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For Windows&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;powershell -Command &#34;&amp;amp; { Invoke-WebRequest -Uri &#39;https://raw.githubusercontent.com/janhq/nitro/main/install.bat&#39; -OutFile &#39;install.bat&#39;; .\install.bat; Remove-Item -Path &#39;install.bat&#39; }&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Downloading a Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir model &amp;amp;&amp;amp; cd model&#xA;wget -O llama-2-7b-model.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf?download=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3: Run Nitro server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nitro&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 4: Load model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:3928/inferences/llamacpp/loadmodel \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;llama_model_path&#34;: &#34;/model/llama-2-7b-model.gguf&#34;,&#xA;    &#34;ctx_len&#34;: 512,&#xA;    &#34;ngl&#34;: 100,&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 5: Making an Inference&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:3928/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;Who won the world series in 2020?&#34;&#xA;      },&#xA;    ]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Table of parameters&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;llama_model_path&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;String&lt;/td&gt; &#xA;   &lt;td&gt;The file path to the LLaMA model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ngl&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of GPU layers to use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ctx_len&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Integer&lt;/td&gt; &#xA;   &lt;td&gt;The context length for the model operations.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;embedding&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Boolean&lt;/td&gt; &#xA;   &lt;td&gt;Whether to use embedding in the model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;n_parallel&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of parallel operations.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cont_batching&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Boolean&lt;/td&gt; &#xA;   &lt;td&gt;Whether to use continuous batching.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;user_prompt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;String&lt;/td&gt; &#xA;   &lt;td&gt;The prompt to use for the user.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ai_prompt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;String&lt;/td&gt; &#xA;   &lt;td&gt;The prompt to use for the AI assistant.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;system_prompt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;String&lt;/td&gt; &#xA;   &lt;td&gt;The prompt to use for system rules.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;pre_prompt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;String&lt;/td&gt; &#xA;   &lt;td&gt;The prompt to use for internal configuration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cpu_threads&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Integer&lt;/td&gt; &#xA;   &lt;td&gt;The number of threads to use for inferencing (CPU MODE ONLY)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;n_batch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Integer&lt;/td&gt; &#xA;   &lt;td&gt;The batch size for prompt eval step&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;caching_enabled&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Boolean&lt;/td&gt; &#xA;   &lt;td&gt;To enable prompt caching or not&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clean_cache_threshold&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Integer&lt;/td&gt; &#xA;   &lt;td&gt;Number of chats that will trigger clean cache action&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;OPTIONAL&lt;/strong&gt;&lt;/em&gt;: You can run Nitro on a different port like 5000 instead of 3928 by running it manually in terminal&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-zsh&#34;&gt;./nitro 1 127.0.0.1 5000 ([thread_num] [host] [port])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;thread_num : the number of thread that nitro webserver needs to have&lt;/li&gt; &#xA; &lt;li&gt;host : host value normally 127.0.0.1 or 0.0.0.0&lt;/li&gt; &#xA; &lt;li&gt;port : the port that nitro got deployed onto&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Nitro server is compatible with the OpenAI format, so you can expect the same output as the OpenAI ChatGPT API.&lt;/p&gt; &#xA;&lt;h2&gt;Compile from source&lt;/h2&gt; &#xA;&lt;p&gt;To compile nitro please visit &lt;a href=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/new/build-source.md&#34;&gt;Compile from source&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt;&lt;b&gt;Version Type&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;text-align:center&#34;&gt;&lt;b&gt;Windows&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;text-align:center&#34;&gt;&lt;b&gt;MacOS&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;text-align:center&#34;&gt;&lt;b&gt;Linux&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt;&lt;b&gt;Stable (Recommended)&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/releases/download/v0.2.5/nitro-0.2.5-win-amd64.tar.gz&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/static/img/windows.png&#34; style=&#34;height:15px; width: 15px&#34;&gt; &lt;b&gt;CPU&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/releases/download/v0.2.5/nitro-0.2.5-win-amd64-cuda.tar.gz&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/static/img/windows.png&#34; style=&#34;height:15px; width: 15px&#34;&gt; &lt;b&gt;CUDA&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/releases/download/v0.2.5/nitro-0.2.5-mac-amd64.tar.gz&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/static/img/mac.png&#34; style=&#34;height:15px; width: 15px&#34;&gt; &lt;b&gt;Intel&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/releases/download/v0.2.5/nitro-0.2.5-mac-arm64.tar.gz&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/static/img/mac.png&#34; style=&#34;height:15px; width: 15px&#34;&gt; &lt;b&gt;M1/M2&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/releases/download/v0.2.5/nitro-0.2.5-linux-amd64.tar.gz&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/static/img/linux.png&#34; style=&#34;height:15px; width: 15px&#34;&gt; &lt;b&gt;CPU&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/releases/download/v0.2.5/nitro-0.2.5-linux-amd64-cuda.tar.gz&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/janhq/nitro/main/docs/static/img/linux.png&#34; style=&#34;height:15px; width: 15px&#34;&gt; &lt;b&gt;CUDA&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;text-align: center&#34;&gt; &#xA;   &lt;td style=&#34;text-align:center&#34;&gt;&lt;b&gt;Experimental (Nighlty Build)&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center&#34; colspan=&#34;6&#34;&gt; &lt;a href=&#34;https://github.com/janhq/nitro/actions/runs/7440414324&#34;&gt; &lt;b&gt;Github action artifactory&lt;/b&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Download the latest version of Nitro at &lt;a href=&#34;https://nitro.jan.ai/&#34;&gt;https://nitro.jan.ai/&lt;/a&gt; or visit the &lt;strong&gt;&lt;a href=&#34;https://github.com/janhq/nitro/releases&#34;&gt;GitHub Releases&lt;/a&gt;&lt;/strong&gt; to download any previous release.&lt;/p&gt; &#xA;&lt;h2&gt;Nightly Build&lt;/h2&gt; &#xA;&lt;p&gt;Nightly build is a process where the software is built automatically every night. This helps in detecting and fixing bugs early in the development cycle. The process for this project is defined in &lt;a href=&#34;https://raw.githubusercontent.com/janhq/nitro/main/.github/workflows/build.yml&#34;&gt;&lt;code&gt;.github/workflows/build.yml&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can join our Discord server &lt;a href=&#34;https://discord.gg/FTk2MvZwJH&#34;&gt;here&lt;/a&gt; and go to channel &lt;a href=&#34;https://discordapp.com/channels/1107178041848909847/1151022176019939328&#34;&gt;github-nitro&lt;/a&gt; to monitor the build process.&lt;/p&gt; &#xA;&lt;p&gt;The nightly build is triggered at 2:00 AM UTC every day.&lt;/p&gt; &#xA;&lt;p&gt;The nightly build can be downloaded from the url notified in the Discord channel. Please access the url from the browser and download the build artifacts from there.&lt;/p&gt; &#xA;&lt;h2&gt;Manual Build&lt;/h2&gt; &#xA;&lt;p&gt;Manual build is a process where the software is built manually by the developers. This is usually done when a new feature is implemented or a bug is fixed. The process for this project is defined in &lt;a href=&#34;https://raw.githubusercontent.com/janhq/nitro/main/.github/workflows/build.yml&#34;&gt;&lt;code&gt;.github/workflows/build.yml&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It is similar to the nightly build process, except that it is triggered manually by the developers.&lt;/p&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For support, please file a GitHub ticket.&lt;/li&gt; &#xA; &lt;li&gt;For questions, join our Discord &lt;a href=&#34;https://discord.gg/FTk2MvZwJH&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For long-form inquiries, please email &lt;a href=&#34;mailto:hello@jan.ai&#34;&gt;hello@jan.ai&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#janhq/nitro&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=janhq/nitro&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>