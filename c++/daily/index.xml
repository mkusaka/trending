<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-27T01:28:55Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>es3n1n/no-defender</title>
    <updated>2024-05-27T01:28:55Z</updated>
    <id>tag:github.com,2024-05-27:/es3n1n/no-defender</id>
    <link href="https://github.com/es3n1n/no-defender" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A slightly more fun way to disable windows defender. (through the WSC api)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;no-defender&lt;/h1&gt; &#xA;&lt;p&gt;A slightly more fun way to disable windows defender.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/8qyJoBV.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a WSC (Windows Security Center) service in Windows which is used by antiviruses to let Windows know that there&#39;s some other antivirus in the hood and it should disable Windows Defender.&lt;br&gt; This WSC API is undocumented and furthermore requires people to sign an NDA with Microsoft to get its documentation, so I decided to take an interesting approach for such a thing and used an already existing antivirus called Avast. This AV engine includes a so-called &lt;code&gt;wsc_proxy.exe&lt;/code&gt; service, which essentially sets up the WSC API for Avast.&lt;br&gt; With a little bit of reverse engineering, I turned this service into a service that could add my own stuff there.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Sadly, to keep this WSC stuff even after the reboot, no-defender adds itself (not really itself but rather Avast&#39;s module) to the autorun. Thus, you would need to keep the no-defender binaries on your disk :(&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;Usage: no-defender-loader [--help] [--version] [--disable] [--name VAR]&#xA;&#xA;Optional arguments:&#xA;  -h, --help     shows help message and exits&#xA;  -v, --version  prints version information and exits&#xA;  --disable      disable the no-defender stuff&#xA;  --name         av name [default: &#34;github.com/es3n1n/no-defender&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;GPL-3.0&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pcg-mlp/KsanaLLM</title>
    <updated>2024-05-27T01:28:55Z</updated>
    <id>tag:github.com,2024-05-27:/pcg-mlp/KsanaLLM</id>
    <link href="https://github.com/pcg-mlp/KsanaLLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KsanaLLM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pcg-mlp/KsanaLLM/main/README.md&#34;&gt;English&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pcg-mlp/KsanaLLM/main/README_cn.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;KsanaLLM is a high performance and easy-to-use engine for LLM inference and serving.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Performance and Throughput:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Utilizes optimized CUDA kernels, including high performance kernels from &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vllm&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FastTransformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Efficient management of attention key and value memory with &lt;a href=&#34;https://arxiv.org/abs/2309.06180&#34;&gt;PagedAttention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Detailed optimization of task-scheduling and memory-uitlization for dynamic batching&lt;/li&gt; &#xA; &lt;li&gt;(Experimental) Prefix caching support&lt;/li&gt; &#xA; &lt;li&gt;Sufficient testing has been conducted on GPU cards such as A10, A100, etc&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flexibility and easy to use:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Seamless integration with popular Hugging Face models, and support multiple weight formats, such as pytorch and SafeTensors&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High-throughput serving with various decoding algorithms, including parallel sampling, beam search, and more&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enables multi-gpu tensor parallelism&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Streaming outputs&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenAI-compatible API server&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Support NVIDIA GPUs and Huawei Ascend NPU&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;KsanaLLM seamlessly supports many Hugging Face models, including the below models that have been verified:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLaMA 7B/13B &amp;amp; LLaMA-2 7B/13B &amp;amp; LLaMA3 8B/70B&lt;/li&gt; &#xA; &lt;li&gt;Baichuan1 7B/13B &amp;amp; Baichuan2 7B/13B&lt;/li&gt; &#xA; &lt;li&gt;Qwen 7B/14B &amp;amp; QWen1.5 7B/14B/72B&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Hardware&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nvidia GPUs: A10, A100&lt;/li&gt; &#xA; &lt;li&gt;Huawei Ascend NPUs: 910B&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;1. Create docker container and runtime environment&lt;/h3&gt; &#xA;&lt;h4&gt;1.1 For Nvidia GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# need install nvidia-docker from https://github.com/NVIDIA/nvidia-container-toolkit&#xA;sudo nvidia-docker run -itd --network host --privileged \&#xA;    nvcr.io/nvidia/pytorch:24.03-py3 bash&#xA;pip install -r requirements.txt&#xA;# for download huggingface model&#xA;apt update &amp;amp;&amp;amp; apt install git-lfs -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;1.2 For Huawei Ascend NPU&lt;/h4&gt; &#xA;&lt;p&gt;Note: Image for Huawei Cloud is in progress.&lt;/p&gt; &#xA;&lt;h3&gt;2. Clone source code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/pcg-mlp/KsanaLLM&#xA;export GIT_PROJECT_REPO_ROOT=`pwd`/KsanaLLM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Compile&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ${GIT_PROJECT_REPO_ROOT}&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3.1 For Nvidia&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# SM for A10 is 86， change it when using other gpus.&#xA;# refer to: https://developer.nvidia.cn/cuda-gpus&#xA;cmake -DSM=86 -DWITH_TESTING=ON .. &amp;amp;&amp;amp; make -j32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3.2 For Huawei Ascend NPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -DWITH_TESTING=ON -DWITH_CUDA=OFF -DWITH_ACL=ON .. &amp;amp;&amp;amp; make -j32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ${GIT_PROJECT_REPO_ROOT}/src/ksana_llm/python&#xA;ln -s ${GIT_PROJECT_REPO_ROOT}/build/lib .&#xA;&#xA;# download huggingface model for example:&#xA;git clone https://huggingface.co/NousResearch/Llama-2-7b-hf&#xA;&#xA;# change the model_dir in ${GIT_PROJECT_REPO_ROOT}/examples/ksana_llm2-7b.yaml if needed&#xA;&#xA;# set environment variable `NLLM_LOG_LEVEL=DEBUG` before run to get more log info&#xA;# the serving log locate in log/ksana_llm.log&#xA;&#xA;# ${GIT_PROJECT_REPO_ROOT}/examples/ksana_llm2-7b.yaml&#39;s tensor_para_size equal the GPUs/NPUs number&#xA;export CUDA_VISIBLE_DEVICES=xx&#xA;&#xA;# launch server&#xA;python serving_server.py \&#xA;    --config_file ${GIT_PROJECT_REPO_ROOT}/examples/ksana_llm2-7b.yaml \&#xA;    --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference test with one shot conversation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# open another session&#xA;cd ${GIT_PROJECT_REPO_ROOT}/examples/llama7b&#xA;python serving_client.py --port 8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Distribute&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ${GIT_PROJECT_REPO_ROOT}&#xA;&#xA;# for distribute wheel&#xA;python setup.py bdist_wheel&#xA;# install wheel&#xA;pip install dist/ksana_llm-0.1-*-linux_x86_64.whl&#xA;&#xA;# check install success&#xA;pip show -f ksana_llm&#xA;python -c &#34;import ksana_llm&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;6. Optional&lt;/h3&gt; &#xA;&lt;h4&gt;6.1 Model Weight Map&lt;/h4&gt; &#xA;&lt;p&gt;You can include an optional weight map JSON file for models that share the same structure as the Llama model but have different weight names. For more detailed information, please refer to the following link: &lt;a href=&#34;https://raw.githubusercontent.com/pcg-mlp/KsanaLLM/main/src/ksana_llm/python/weight_map/README.md&#34;&gt;Optional Weigth Map Guide&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>