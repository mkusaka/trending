<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-28T01:26:05Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ossamamehmood/Hacktoberfest2023</title>
    <updated>2023-09-28T01:26:05Z</updated>
    <id>tag:github.com,2023-09-28:/ossamamehmood/Hacktoberfest2023</id>
    <link href="https://github.com/ossamamehmood/Hacktoberfest2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hacktoberfest 2023 OPEN FIRST Pull Request - FREE T-SHIRTðŸŽ‰&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HacktoberFest 2023 &lt;code&gt;OPEN FIRST&lt;/code&gt; Pull Request - &lt;code&gt;FREE T-SHIRT&lt;/code&gt;ðŸŽ‰&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ossamamehmood/Hacktoberfest2023/raw/main/.github/logo.png&#34; alt=&#34;HacktoberFest 2023&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors of &lt;code&gt;Hacktoberfest 2023&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood/Hacktoberfest2023/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=ossamamehmood/Hacktoberfest2023&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;This Project Is Perfect For Your First Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ—£ &lt;strong&gt;Hacktoberfest encourages participation in the open-source community, which grows bigger every year. Complete the challenge and earn a limited edition T-shirt.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ“¢ &lt;strong&gt;Register &lt;a href=&#34;https://hacktoberfest.digitalocean.com&#34;&gt;here&lt;/a&gt; for Hacktoberfest and make four pull requests (PRs) between October 1st-31st to grab free SWAGS ðŸ”¥.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/hacktoberfest-2023-blueviolet&#34; alt=&#34;Hacktober Badge&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/static/v1?label=%F0%9F%8C%9F&amp;amp;message=If%20Useful&amp;amp;style=style=flat&amp;amp;color=BC4E99&#34; alt=&#34;Star Badge&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributions-welcome-violet.svg?style=flat&amp;amp;logo=git&#34; alt=&#34;Contributions&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/ossamamehmood/hacktoberfest2023&#34; alt=&#34;Pull Requests Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/graphs/contributors&#34;&gt;&lt;img alt=&#34;GitHub contributors&#34; src=&#34;https://img.shields.io/github/contributors/ossamamehmood/hacktoberfest2023?color=2b9348&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/ossamamehmood/hacktoberfest2023?color=2b9348&#34; alt=&#34;License Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;INSTRUCTIONS-&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this Repository using the button at the top on the right corner.&lt;/li&gt; &#xA; &lt;li&gt;Clone your forked repository to your PC ( git clone &#34;url from clone option.)&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your modifications (ie. &lt;code&gt;git branch new-user and check it out &lt;/code&gt;git checkout new-user&lt;code&gt;or simply do&lt;/code&gt;git checkout -b new-user`)&lt;/li&gt; &#xA; &lt;li&gt;Add your profile image in &lt;code&gt;static/images/&lt;/code&gt; ( use drag and drop option or upload by commands.)&lt;/li&gt; &#xA; &lt;li&gt;Add your profile data in &lt;code&gt;content/participant/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add your files (&lt;code&gt;git add -A&lt;/code&gt;), commit (&lt;code&gt;git commit -m &#34;added myself&#34;&lt;/code&gt;) and push (&lt;code&gt;git push origin new-user&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create a pull request&lt;/li&gt; &#xA; &lt;li&gt;Star this repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How To Make Your First Pull Request&lt;/h1&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;YOUR-USERNAME&amp;gt;&lt;/code&gt; with your GitHub username in this guide.&lt;/p&gt; &#xA;&lt;h2&gt;1. Add your profile picture to the folder&lt;/h2&gt; &#xA;&lt;p&gt;Add a picture of your choice in &lt;code&gt;static/images/&lt;/code&gt;. Accepted files are &lt;strong&gt;png&lt;/strong&gt; and &lt;strong&gt;jpg&lt;/strong&gt;, should be squared and minimum size 544x544 pixels. Ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;static/images/&amp;lt;YOUR-USERNAME&amp;gt;.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Add your profile information&lt;/h2&gt; &#xA;&lt;p&gt;Create a markdown file in your folder following the convention &lt;code&gt;content/participant/&amp;lt;YOUR-USERNAME&amp;gt;.md&lt;/code&gt;. Ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;content/participant/&amp;lt;YOUR-USERNAME&amp;gt;.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copy the next template into your file, delete the boilerplate data and fill the information with yours.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;---&#xA;name: YOURNAME&#xA;institution/company: INSTITUTION_NAME&#xA;github: USERNAME&#xA;---&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; OR &lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;3. Create / Upload Your Code / Algorithms&lt;/h2&gt; &#xA;&lt;p&gt;Create/Upload your code in the folder following the convention &lt;code&gt;Add Code Here&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Choose an extract language folder &lt;code&gt;drop your code&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Below is &lt;code&gt;an example&lt;/code&gt; to add file properly&lt;/li&gt; &#xA; &lt;li&gt;You can follow up &lt;code&gt;any languages&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add Code Here/PYTHON/&amp;lt;YOUR-FILERNAME&amp;gt;.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add Code Here/C++/&amp;lt;YOUR-FILERNAME&amp;gt;.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can follow any pathway a &lt;code&gt;code&lt;/code&gt; or &lt;code&gt;profile information&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;4. Wait for Pull Request to merge&lt;/h2&gt; &#xA;&lt;h2&gt;5. Celebrate - you&#39;ve done your first pull request!!&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;&#39;&#39;&#xA;Always make more than 4 pull requests.&#xA;Let&#39;s say you have made only 4 pull requests to different projects,&#xA;but one project is excluded from hackoctoberfest event then your pull request will not be counted and &#xA;then you have the remaining 3 valid pull requests if these projects are not excluded.&#xA;If you fail to make 4 pull requests then you can&#39;t get swags or t-shirts.&#xA;I will recommend you make a pull request to your own repo which is very very safe for you.&#xA;Keep in mind that the repo has hacktoberfest topic.&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;+ Follow Me : } Quick Approval of Pull Request&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;&#39;&#39;&#xA;To get approval of the pull request much quicker and faster (`Follow Me`)ðŸš€&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;kbd&gt;&lt;img src=&#34;https://avatars3.githubusercontent.com/ossamamehmood?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt;&lt;/kbd&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Ossama Mehmood&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>casper-hansen/AutoAWQ</title>
    <updated>2023-09-28T01:26:05Z</updated>
    <id>tag:github.com,2023-09-28:/casper-hansen/AutoAWQ</id>
    <link href="https://github.com/casper-hansen/AutoAWQ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AutoAWQ implements the AWQ algorithm for 4-bit quantization with a 2x speedup during inference.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoAWQ&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://github.com/casper-hansen/AutoAWQ/issues/32&#34;&gt;&lt;b&gt;Roadmap&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/casper-hansen/AutoAWQ/tree/main/examples&#34;&gt;&lt;b&gt;Examples&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/casper-hansen/AutoAWQ/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22&#34;&gt;&lt;b&gt;Issues: Help Wanted&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/models?search=awq&#34;&gt; &lt;img alt=&#34;Huggingface - Models&#34; src=&#34;https://img.shields.io/badge/ðŸ¤—_400+_models_available-8A2BE2&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/casper-hansen/AutoAWQ/releases&#34;&gt; &lt;img alt=&#34;GitHub - Releases&#34; src=&#34;https://img.shields.io/github/release/casper-hansen/AutoAWQ.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/autoawq/&#34;&gt; &lt;img alt=&#34;PyPI - Downloads&#34; src=&#34;https://img.shields.io/pypi/dd/autoawq&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;AutoAWQ is an easy-to-use package for 4-bit quantized models. AutoAWQ speeds up models by 2x while reducing memory requirements by 3x compared to FP16. AutoAWQ implements the Activation-aware Weight Quantization (AWQ) algorithm for quantizing LLMs. AutoAWQ was created and improved upon from the &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq&#34;&gt;original work&lt;/a&gt; from MIT.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; ðŸ”¥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/09] 1.6x-2.5x speed boost on fused models (now including MPT and Falcon).&lt;/li&gt; &#xA; &lt;li&gt;[2023/09] Multi-GPU support, bug fixes, and better benchmark scripts available&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] PyPi package released and AutoModel class available&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compute Capability 8.0 (sm80). Ampere and later architectures are supported.&lt;/li&gt; &#xA; &lt;li&gt;CUDA Toolkit 11.8 and later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Install:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use pip to install awq&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install autoawq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using conda&lt;/h3&gt; &#xA;&lt;p&gt;CUDA dependencies can be hard to manage sometimes. It is recommended to use conda with AutoAWQ:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name autoawq python=3.10 -y&#xA;conda activate autoawq&#xA;conda install pytorch=2.0.1 torchvision torchaudio cudatoolkit=11.8 -c pytorch -c nvidia&#xA;pip install autoawq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build source&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Build AutoAWQ from scratch&lt;/summary&gt; &#xA; &lt;p&gt;Build time can take 10 minutes. Download your model while you install AutoAWQ.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;git clone https://github.com/casper-hansen/AutoAWQ&#xA;cd AutoAWQ&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Supported models&lt;/h2&gt; &#xA;&lt;p&gt;The detailed support list:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Sizes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-2&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/70B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B/30B/65B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B/13B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MPT&lt;/td&gt; &#xA;   &lt;td&gt;7B/30B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon&lt;/td&gt; &#xA;   &lt;td&gt;7B/40B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;125m/1.3B/2.7B/6.7B/13B/30B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bloom&lt;/td&gt; &#xA;   &lt;td&gt;560m/3B/7B/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTJ&lt;/td&gt; &#xA;   &lt;td&gt;6.7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Under examples, you can find examples of how to quantize, run inference, and benchmark AutoAWQ models.&lt;/p&gt; &#xA;&lt;h3&gt;INT4 GEMM vs INT4 GEMV vs FP16&lt;/h3&gt; &#xA;&lt;p&gt;There are two versions of AWQ: GEMM and GEMV. Both names relate to how matrix multiplication runs under the hood. We suggest the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GEMV (quantized): Best for small context, batch size 1, highest number of tokens/s.&lt;/li&gt; &#xA; &lt;li&gt;GEMM (quantized): Best for larger context, up to batch size 8, faster than GEMV on batch size &amp;gt; 1, slower than GEMV on batch size = 1.&lt;/li&gt; &#xA; &lt;li&gt;FP16 (non-quantized): Best for large batch sizes of 8 or larger, highest throughput. We recommend &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;TGI&lt;/a&gt; or &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Quantization&lt;/summary&gt; &#xA; &lt;p&gt;Expect this to take 10-15 minutes on smaller 7B models, and around 1 hour for 70B models.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from awq import AutoAWQForCausalLM&#xA;from transformers import AutoTokenizer&#xA;&#xA;model_path = &#39;lmsys/vicuna-7b-v1.5&#39;&#xA;quant_path = &#39;vicuna-7b-v1.5-awq&#39;&#xA;quant_config = { &#34;zero_point&#34;: True, &#34;q_group_size&#34;: 128, &#34;w_bit&#34;: 4 }&#xA;&#xA;# Load model&#xA;model = AutoAWQForCausalLM.from_pretrained(model_path)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)&#xA;&#xA;# Quantize&#xA;model.quantize(tokenizer, quant_config=quant_config)&#xA;&#xA;# Save quantized model&#xA;model.save_quantized(quant_path)&#xA;tokenizer.save_pretrained(quant_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Inference&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from awq import AutoAWQForCausalLM&#xA;from transformers import AutoTokenizer, TextStreamer&#xA;&#xA;quant_path = &#34;casperhansen/vicuna-7b-v1.5-awq&#34;&#xA;quant_file = &#34;awq_model_w4_g128.pt&#34;&#xA;&#xA;# Load model&#xA;model = AutoAWQForCausalLM.from_quantized(quant_path, quant_file, fuse_layers=True)&#xA;tokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)&#xA;streamer = TextStreamer(tokenizer, skip_special_tokens=True)&#xA;&#xA;# Convert prompt to tokens&#xA;prompt_template = &#34;&#34;&#34;\&#xA;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions.&#xA;&#xA;USER: {prompt}&#xA;ASSISTANT:&#34;&#34;&#34;&#xA;&#xA;tokens = tokenizer(&#xA;    prompt_template.format(prompt=&#34;How are you today?&#34;), &#xA;    return_tensors=&#39;pt&#39;&#xA;).input_ids.cuda()&#xA;&#xA;# Generate output&#xA;generation_output = model.generate(&#xA;    tokens, &#xA;    streamer=streamer,&#xA;    max_new_tokens=512&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;AutoAWQForCausalLM.from_quantized&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;quant_path&lt;/code&gt;: Path to folder containing model files.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;quant_filename&lt;/code&gt;: The filename to model weights or &lt;code&gt;index.json&lt;/code&gt; file.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;max_new_tokens&lt;/code&gt;: The max sequence length, used to allocate kv-cache for fused models.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;fuse_layers&lt;/code&gt;: Whether or not to use fused layers.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: The batch size to initialize the AWQ model with.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;h3&gt;Vicuna 7B (LLaMa-2)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Blazing fast generation, slow context processing&lt;/li&gt; &#xA; &lt;li&gt;GPU: NVIDIA GeForce RTX 3090&lt;/li&gt; &#xA; &lt;li&gt;Version: GEMV&lt;/li&gt; &#xA; &lt;li&gt;Command: &lt;code&gt;python examples/benchmark.py --model_path casperhansen/vicuna-7b-v1.5-awq-gemv&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Memory (VRAM)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;231.393&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;153.632&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.66 GB (19.68%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;233.909&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;154.475&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.66 GB (19.68%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;233.145&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;152.133&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.66 GB (19.68%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;228.562&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;147.692&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.67 GB (19.72%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;228.914&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;139.179&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.80 GB (20.26%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;227.393&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;125.058&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.56 GB (23.48%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;225.736&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;123.228&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.08 GB (34.09%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Fast generation, fast context processing&lt;/li&gt; &#xA; &lt;li&gt;GPU: NVIDIA GeForce RTX 3090&lt;/li&gt; &#xA; &lt;li&gt;Version: GEMM&lt;/li&gt; &#xA; &lt;li&gt;Command: &lt;code&gt;python examples/benchmark.py --model_path casperhansen/vicuna-7b-v1.5-awq&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Memory (VRAM)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;521.444&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;126.51&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.55 GB (19.21%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2618.88&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;125.428&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.57 GB (19.31%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2808.09&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;123.865&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.61 GB (19.44%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2807.46&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;120.779&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.67 GB (19.72%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2769.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;115.08&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.80 GB (20.26%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2640.95&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;105.493&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.56 GB (23.48%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2341.36&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;104.188&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.08 GB (34.09%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MPT 7B&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Blazing fast generation, slow context processing&lt;/li&gt; &#xA; &lt;li&gt;GPU: NVIDIA GeForce RTX 3090&lt;/li&gt; &#xA; &lt;li&gt;Command: &lt;code&gt;python examples/benchmark.py --model_path casperhansen/mpt-7b-8k-chat-awq-gemv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Version: GEMV&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Memory (VRAM)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;187.332&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;136.765&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.65 GB (15.42%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;241.026&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;136.476&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.67 GB (15.48%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;239.44&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;137.599&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.70 GB (15.61%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;233.184&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;137.02&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.76 GB (15.88%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;233.082&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;135.633&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.89 GB (16.41%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;231.504&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;122.197&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.40 GB (18.57%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;228.307&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;121.468&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.92 GB (24.98%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Fast generation, fast context processing&lt;/li&gt; &#xA; &lt;li&gt;GPU: NVIDIA GeForce RTX 3090&lt;/li&gt; &#xA; &lt;li&gt;Version: GEMM&lt;/li&gt; &#xA; &lt;li&gt;Command: &lt;code&gt;python examples/benchmark.py --model_path casperhansen/mpt-7b-8k-chat-awq&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Memory (VRAM)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;557.714&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;118.567&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.65 GB (15.42%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2752.9&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;120.772&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.67 GB (15.48%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2982.67&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;119.52&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.70 GB (15.61%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3009.16&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;116.911&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.76 GB (15.88%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2901.91&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;111.607&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.95 GB (16.68%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2718.68&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;102.623&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.40 GB (18.57%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2363.61&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;101.368&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.92 GB (24.98%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Falcon 7B&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Fast generation, fast context processing&lt;/li&gt; &#xA; &lt;li&gt;GPU: NVIDIA GeForce RTX 3090&lt;/li&gt; &#xA; &lt;li&gt;Command: &lt;code&gt;python examples/benchmark.py --model_path casperhansen/falcon-7b-awq --quant_file awq_model_w4_g64.pt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Version: GEMM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode Length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Prefill tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Decode tokens/s&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Memory (VRAM)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;466.826&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;95.1413&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.47 GB (18.88%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1920.61&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;94.5963&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.48 GB (18.92%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2406.1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;94.793&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.48 GB (18.92%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2521.08&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;94.1144&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.48 GB (18.92%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;512&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2478.28&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;93.4123&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.48 GB (18.92%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1024&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2256.22&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;94.0237&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.69 GB (19.78%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1831.71&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;94.2032&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;6.83 GB (28.83%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;If you find AWQ useful or relevant to your research, you can cite their &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{lin2023awq,&#xA;  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},&#xA;  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},&#xA;  journal={arXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>