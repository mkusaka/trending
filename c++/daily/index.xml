<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-27T01:29:20Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ggml-org/ggml</title>
    <updated>2025-03-27T01:29:20Z</updated>
    <id>tag:github.com,2025-03-27:/ggml-org/ggml</id>
    <link href="https://github.com/ggml-org/ggml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensor library for machine learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ggml&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/users/ggerganov/projects/7&#34;&gt;Roadmap&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/discussions/205&#34;&gt;Manifesto&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tensor library for machine learning&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note that this project is under active development. &lt;br&gt; Some of the development is currently happening in the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt; repos&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Low-level cross-platform implementation&lt;/li&gt; &#xA; &lt;li&gt;Integer quantization support&lt;/li&gt; &#xA; &lt;li&gt;Broad hardware support&lt;/li&gt; &#xA; &lt;li&gt;Automatic differentiation&lt;/li&gt; &#xA; &lt;li&gt;ADAM and L-BFGS optimizers&lt;/li&gt; &#xA; &lt;li&gt;No third-party dependencies&lt;/li&gt; &#xA; &lt;li&gt;Zero memory allocations during runtime&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ggml-org/ggml&#xA;cd ggml&#xA;&#xA;# install python dependencies in a virtual environment&#xA;python3.10 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&#xA;# build the examples&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;cmake --build . --config Release -j 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GPT inference (example)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# run the GPT-2 small 117M model&#xA;../examples/gpt-2/download-ggml-model.sh 117M&#xA;./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p &#34;This is an example&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, checkout the corresponding programs in the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/ggml/master/examples&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Using CUDA&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# fix the path to point to your CUDA compiler&#xA;cmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using hipBLAS&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -DCMAKE_C_COMPILER=&#34;$(hipconfig -l)/clang&#34; -DCMAKE_CXX_COMPILER=&#34;$(hipconfig -l)/clang++&#34; -DGGML_HIP=ON&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using SYCL&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# linux&#xA;source /opt/intel/oneapi/setvars.sh&#xA;cmake -G &#34;Ninja&#34; -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON ..&#xA;&#xA;# windows&#xA;&#34;C:\Program Files (x86)\Intel\oneAPI\setvars.bat&#34;&#xA;cmake -G &#34;Ninja&#34; -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiling for Android&lt;/h2&gt; &#xA;&lt;p&gt;Download and unzip the NDK from this download &lt;a href=&#34;https://developer.android.com/ndk/downloads&#34;&gt;page&lt;/a&gt;. Set the NDK_ROOT_PATH environment variable or provide the absolute path to the CMAKE_ANDROID_NDK in the command below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake .. \&#xA;   -DCMAKE_SYSTEM_NAME=Android \&#xA;   -DCMAKE_SYSTEM_VERSION=33 \&#xA;   -DCMAKE_ANDROID_ARCH_ABI=arm64-v8a \&#xA;   -DCMAKE_ANDROID_NDK=$NDK_ROOT_PATH&#xA;   -DCMAKE_ANDROID_STL_TYPE=c++_shared&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create directories&#xA;adb shell &#39;mkdir /data/local/tmp/bin&#39;&#xA;adb shell &#39;mkdir /data/local/tmp/models&#39;&#xA;&#xA;# push the compiled binaries to the folder&#xA;adb push bin/* /data/local/tmp/bin/&#xA;&#xA;# push the ggml library&#xA;adb push src/libggml.so /data/local/tmp/&#xA;&#xA;# push model files&#xA;adb push models/gpt-2-117M/ggml-model.bin /data/local/tmp/models/&#xA;&#xA;adb shell&#xA;cd /data/local/tmp&#xA;export LD_LIBRARY_PATH=/data/local/tmp&#xA;./bin/gpt-2-backend -m models/ggml-model.bin -p &#34;this is an example&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/introduction-to-ggml&#34;&gt;Introduction to ggml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/ggml/raw/master/docs/gguf.md&#34;&gt;The GGUF file format&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>