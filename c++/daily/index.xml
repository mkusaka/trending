<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-08T01:34:23Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bytedance/sonic-cpp</title>
    <updated>2022-12-08T01:34:23Z</updated>
    <id>tag:github.com,2022-12-08:/bytedance/sonic-cpp</id>
    <link href="https://github.com/bytedance/sonic-cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast JSON serializing &amp; deserializing library, accelerated by SIMD.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sonic-Cpp&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;A fast JSON serializing &amp;amp; deserializing library, accelerated by SIMD.&lt;/p&gt; &#xA;&lt;h2&gt;Requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c++11 or above&lt;/li&gt; &#xA; &lt;li&gt;X86 platform and AVX2 instruction support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Complete APIs for JSON value manipulation&lt;/li&gt; &#xA; &lt;li&gt;Fast on JSON serializing and parsing&lt;/li&gt; &#xA; &lt;li&gt;Support parse ondemand&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;use CMake&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake -S . -B build -DBUILD_BENCH=ON&#xA;cmake --build build --target bench -j&#xA;./build/benchmark/bench&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;use bazel&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bazel run :benchmark --compilation_mode=opt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Performance by sonic benchmark&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# build by bazel&#xA;python3 ./scripts/tools/draw-decode-encode.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Result&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Parsing Performance &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/docs/images/compare_Decode.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Serializing performance &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/docs/images/compare_Encode.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Performance by third-party benchmark&lt;/h3&gt; &#xA;&lt;p&gt;Below data is test by &lt;a href=&#34;https://github.com/miloyip/nativejson-benchmark&#34;&gt;https://github.com/miloyip/nativejson-benchmark&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Parsing Performance &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/docs/images/parse.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Serializing Performance &lt;img src=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/docs/images/serialize.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Design&lt;/h2&gt; &#xA;&lt;p&gt;Sonic-cpp parses JSON into a compact document tree. The document structure is as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/docs/images/dom.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are many optimizations in parsing as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;using SIMD to accelerate skipping white space.&lt;/li&gt; &#xA; &lt;li&gt;using SIMD to find escaped chars when parsing strings.&lt;/li&gt; &#xA; &lt;li&gt;using the STOA float pointing algorithm.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sonic-cpp serializes a document to JSON. When serializing JSON strings, we should check the escaped characters first. So, we use SIMD instructions(AVX2/SSE) to find the escaped char for long JSON string.&lt;/p&gt; &#xA;&lt;p&gt;Sonic-cpp also supports ParseOnDemand if the user knows the target key at compile time. ParseOndemand also used SIMD and bit manipulation to skip the unwanted values fastly.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;include&lt;/h3&gt; &#xA;&lt;p&gt;Sonic-Cpp is header-only library, you only need to include the directory of Sonic-Cpp header files, such as adding &lt;code&gt;-I/path/to/sonic/include/&lt;/code&gt; to your compiler.&lt;/p&gt; &#xA;&lt;h3&gt;Parsing and Serializing&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;sonic/sonic.h&#34;&#xA;&#xA;#include &amp;lt;string&amp;gt;&#xA;#include &amp;lt;iostream&amp;gt;&#xA;&#xA;int main()&#xA;{&#xA;  std::string json = R&#34;(&#xA;    {&#xA;      &#34;a&#34;: 1,&#xA;      &#34;b&#34;: 2&#xA;    }&#xA;  )&#34;;&#xA;&#xA;  sonic_json::Document doc;&#xA;  doc.Parse(json);&#xA;&#xA;  sonic_json::WriteBuffer wb;&#xA;  doc.Serialize(wb);&#xA;  std::cout &amp;lt;&amp;lt; wb.ToString() &amp;lt;&amp;lt; std::endl;&#xA;}&#xA;// g++ -I./include/ -march=haswell --std=c++11 -O3 example/parse_and_serialize.cpp -o example/parse_and_serialize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checking parse result&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;sonic/sonic.h&#34;&#xA;&#xA;#include &amp;lt;string&amp;gt;&#xA;#include &amp;lt;iostream&amp;gt;&#xA;&#xA;int main()&#xA;{&#xA;  std::string json = R&#34;(&#xA;    {&#xA;      &#34;a&#34;: 1,&#xA;      &#34;b&#34;: 2&#xA;    }&#xA;  )&#34;;&#xA;&#xA;  sonic_json::Document doc;&#xA;  doc.Parse(json);&#xA;  if (doc.HasParseError()) {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Parse failed!\n&#34;;&#xA;  } else {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Parse successful!\n&#34;;&#xA;  }&#xA;  return 0;&#xA;}&#xA;// g++ -I./include/ -march=haswell --std=c++11 -O3 example/check_parse_result.cpp -o example/check_parse_result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Getting and Setting&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;sonic/sonic.h&#34;&#xA;&#xA;#include &amp;lt;string&amp;gt;&#xA;#include &amp;lt;iostream&amp;gt;&#xA;&#xA;using member_itr_type = typename sonic_json::Document::MemberIterator;&#xA;&#xA;void print_member(member_itr_type m) {&#xA;  sonic_json::Node&amp;amp; key = m-&amp;gt;name;&#xA;  sonic_json::Node&amp;amp; value = m-&amp;gt;value;&#xA;  if (key.IsString()) {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Key is: &#34;&#xA;              &amp;lt;&amp;lt; key.GetString()&#xA;              &amp;lt;&amp;lt; std::endl;&#xA;  } else {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Incoreect key type!\n&#34;;&#xA;    return;&#xA;  }&#xA;  if (value.IsInt64()) {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Value is &#34; &amp;lt;&amp;lt; value.GetInt64() &amp;lt;&amp;lt; std::endl;&#xA;  }&#xA;&#xA;  return;&#xA;}&#xA;&#xA;void set_new_value(member_itr_type m) {&#xA;  sonic_json::Node&amp;amp; value = m-&amp;gt;value;&#xA;  value.SetInt64(2);&#xA;  return;&#xA;}&#xA;&#xA;int main()&#xA;{&#xA;  std::string json = R&#34;(&#xA;    {&#xA;      &#34;a&#34;: 1,&#xA;      &#34;b&#34;: 2&#xA;    }&#xA;  )&#34;;&#xA;&#xA;  sonic_json::Document doc;&#xA;  doc.Parse(json);&#xA;&#xA;  if (doc.HasParseError()) {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Parse failed!\n&#34;;&#xA;    return -1;&#xA;  }&#xA;&#xA;  // Find member by key&#xA;  if (!doc.IsObject()) { // Check JSON value type.&#xA;    std::cout &amp;lt;&amp;lt; &#34;Incorrect doc type!\n&#34;;&#xA;    return -1;&#xA;  }&#xA;  auto m = doc.FindMember(&#34;a&#34;);&#xA;  if (m != doc.MemberEnd()) {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Before Setting new value:\n&#34;;&#xA;    print_member(m);&#xA;    std::cout &amp;lt;&amp;lt; &#34;After Setting value:\n&#34;;&#xA;    set_new_value(m);&#xA;    print_member(m);&#xA;  } else {&#xA;    std::cout &amp;lt;&amp;lt; &#34;Find key doesn&#39;t exist!\n&#34;;&#xA;  }&#xA;  return 0;&#xA;}&#xA;// g++ -I./include/ -march=haswell --std=c++11 -O3 example/get_and_set.cpp -o example/get_and_set&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following Is*, Get* and Set* methods are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IsNull(), SetNull()&lt;/li&gt; &#xA; &lt;li&gt;IsBoo(), GetBool(), SetBool(bool)&lt;/li&gt; &#xA; &lt;li&gt;IsString(), GetString(), GetStringLength(), SetString(const char*, size_t)&lt;/li&gt; &#xA; &lt;li&gt;IsNumber()&lt;/li&gt; &#xA; &lt;li&gt;IsArray(), SetArray()&lt;/li&gt; &#xA; &lt;li&gt;IsObject(), SetObject()&lt;/li&gt; &#xA; &lt;li&gt;IsTrue(), IsFalse()&lt;/li&gt; &#xA; &lt;li&gt;IsDouble(), GetDouble(), SetDouble(double)&lt;/li&gt; &#xA; &lt;li&gt;IsInt64(), GetInt64(), SetInt64(int64_t)&lt;/li&gt; &#xA; &lt;li&gt;IsUint64(), GetUint64(), SetUint64_t(uint64_t)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;More &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/docs/usage.md&#34;&gt;usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;RoadMap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;code&gt;RawNumber&lt;/code&gt; for JSON parsing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://datatracker.ietf.org/wg/jsonpath/about/&#34;&gt;&lt;code&gt;JSON Path&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://www.rfc-editor.org/rfc/rfc7396&#34;&gt;&lt;code&gt;JSON Merge Patch&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support &lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc6901&#34;&gt;&lt;code&gt;JSON Pointer&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more platforms, e.g. ARM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support struct bind.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support validating UTF-8.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/sonic-cpp/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for information on contributing to sonic-cpp.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ethz-asl/maplab</title>
    <updated>2022-12-08T01:34:23Z</updated>
    <id>tag:github.com,2022-12-08:/ethz-asl/maplab</id>
    <link href="https://github.com/ethz-asl/maplab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open visual-inertial mapping framework.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/logos/maplab_new.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;Ubuntu 18.04+ROS melodic&lt;/em&gt;: &lt;a href=&#34;https://jenkins.asl.ethz.ch/job/maplab_nightly&#34;&gt;&lt;img src=&#34;https://jenkins.asl.ethz.ch/buildStatus/icon?job=maplab_nightly&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jenkins.asl.ethz.ch/job/maplab_docs&#34;&gt;&lt;img src=&#34;https://jenkins.asl.ethz.ch/buildStatus/icon?job=maplab_docs&amp;amp;subject=docs&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;May 2018:&lt;/strong&gt; maplab was presented at &lt;a href=&#34;https://icra2018.org/&#34;&gt;ICRA&lt;/a&gt; in Brisbane. &lt;a href=&#34;https://arxiv.org/abs/1711.10250&#34;&gt;Paper&lt;/a&gt; / &lt;a href=&#34;https://github.com/ethz-asl/maplab/releases/tag/initial_release&#34;&gt;Initial Release&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;July 2018:&lt;/strong&gt; Check out our release candidate with improved localization and lots of new features! &lt;a href=&#34;https://github.com/ethz-asl/maplab/releases/tag/1.3&#34;&gt;Release 1.3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;November 2022:&lt;/strong&gt; maplab 2.0 initial release with new features and sensors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains &lt;strong&gt;maplab 2.0&lt;/strong&gt;, an open research-oriented mapping framework, written in C++, for multi-session and multi-robot mapping. For the original maplab release from 2018 the source code and documentation is available &lt;a href=&#34;https://github.com/ethz-asl/maplab/releases/tag/1.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For documentation, tutorials and datasets, please visit the &lt;a href=&#34;https://maplab.asl.ethz.ch/index.html&#34;&gt;wiki&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Robust visual-inertial odometry with localization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/rovio_stairs.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/rviz_cla_vs.gif&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Large-scale multisession mapping and optimization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/largescale.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/cla.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dense reconstruction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/stereo.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/pmvs.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;A research platform extensively tested on real robots&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/topomap.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://github.com/ethz-asl/maplab/wiki/readme_images/robots.jpg&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation and getting started&lt;/h2&gt; &#xA;&lt;p&gt;The following articles help you with getting started with maplab and ROVIOLI:&lt;/p&gt; &#xA;&lt;!-- TODO(floriantschopp): Update links to new wiki --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maplab.asl.ethz.ch/pages/installation/A_Installation-Ubuntu.html&#34;&gt;Installation on Ubuntu 18.04 or 20.04&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maplab.asl.ethz.ch/pages/overview_and_introduction/A_Introduction-to-the-Maplab-Framework.html&#34;&gt;Introduction to the maplab framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maplab.asl.ethz.ch/pages/overview_and_introduction/B_Structure-of-the-framework.html&#34;&gt;Structure of the framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maplab.asl.ethz.ch/pages/tutorials-rovioli/B_Running-ROVIOLI-in-VIO-mode.html&#34;&gt;Running ROVIOLI in VIO mode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maplab.asl.ethz.ch/pages/tutorials-maplab/basics/A_Basic-Console-Usage.html&#34;&gt;Basic console usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://maplab.asl.ethz.ch/pages/tutorials-maplab/basics/C_Console-map-management.html&#34;&gt;Console map management&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;More detailed information can be found in the &lt;a href=&#34;https://maplab.asl.ethz.ch/docs/develop/index.html&#34;&gt;wiki pages&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Research Results&lt;/h2&gt; &#xA;&lt;p&gt;The maplab framework has been used as an experimental platform for numerous scientific publications. For a complete list of publications please refer to &lt;a href=&#34;https://maplab.asl.ethz.ch/pages/overview_and_introduction/C_Related-Research.html&#34;&gt;Research based on maplab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the &lt;a href=&#34;https://arxiv.org/abs/1711.10250&#34;&gt;following paper&lt;/a&gt; when using maplab for your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{schneider2018maplab,&#xA;  title={maplab: An Open Framework for Research in Visual-inertial Mapping and Localization},&#xA;  author={T. Schneider and M. T. Dymczyk and M. Fehr and K. Egger and S. Lynen and I. Gilitschenski and R. Siegwart},&#xA;  journal={IEEE Robotics and Automation Letters},&#xA;  year={2018},&#xA;  doi={10.1109/LRA.2018.2800113}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional Citations&lt;/h3&gt; &#xA;&lt;p&gt;Certain components of maplab are directly using the code of the following publications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Localization: &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{lynen2015get,&#xA;  title={Get Out of My Lab: Large-scale, Real-Time Visual-Inertial Localization.},&#xA;  author={Lynen, Simon and Sattler, Torsten and Bosse, Michael and Hesch, Joel A and Pollefeys, Marc and Siegwart, Roland},&#xA;  booktitle={Robotics: Science and Systems},&#xA;  year={2015}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;ROVIOLI which is composed of ROVIO + maplab for map building and localization: &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{bloesch2017iterated,&#xA;  title={Iterated extended Kalman filter based visual-inertial odometry using direct photometric feedback},&#xA;  author={Bloesch, Michael and Burri, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},&#xA;  journal={The International Journal of Robotics Research},&#xA;  volume={36},&#xA;  number={10},&#xA;  pages={1053--1072},&#xA;  year={2017},&#xA;  publisher={SAGE Publications Sage UK: London, England}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thomas Schneider&lt;/li&gt; &#xA; &lt;li&gt;Marcin Dymczyk&lt;/li&gt; &#xA; &lt;li&gt;Marius Fehr&lt;/li&gt; &#xA; &lt;li&gt;Kevin Egger&lt;/li&gt; &#xA; &lt;li&gt;Simon Lynen&lt;/li&gt; &#xA; &lt;li&gt;Mathias BÃ¼rki&lt;/li&gt; &#xA; &lt;li&gt;Titus Cieslewski&lt;/li&gt; &#xA; &lt;li&gt;Timo Hinzmann&lt;/li&gt; &#xA; &lt;li&gt;Mathias Gehrig&lt;/li&gt; &#xA; &lt;li&gt;Florian Tschopp&lt;/li&gt; &#xA; &lt;li&gt;Andrei Cramariuc&lt;/li&gt; &#xA; &lt;li&gt;Lukas Bernreiter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a complete list of contributors, have a look at &lt;a href=&#34;https://github.com/ethz-asl/maplab/raw/master/CONTRIBUTORS.md&#34;&gt;CONTRIBUTORS.md&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>onnx/onnx-tensorrt</title>
    <updated>2022-12-08T01:34:23Z</updated>
    <id>tag:github.com,2022-12-08:/onnx/onnx-tensorrt</id>
    <link href="https://github.com/onnx/onnx-tensorrt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ONNX-TensorRT: TensorRT backend for ONNX&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorRT Backend For ONNX&lt;/h1&gt; &#xA;&lt;p&gt;Parses ONNX models for execution with &lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See also the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/sdk/#inference&#34;&gt;TensorRT documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the list of recent changes, see the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx-tensorrt/main/docs/Changelog.md&#34;&gt;changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a list of commonly seen issues and questions, see the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx-tensorrt/main/docs/faq.md&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please contact &lt;a href=&#34;mailto:researchinquiries@nvidia.com&#34;&gt;researchinquiries@nvidia.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For press and other inquiries, please contact Hector Marinez at &lt;a href=&#34;mailto:hmarinez@nvidia.com&#34;&gt;hmarinez@nvidia.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported TensorRT Versions&lt;/h2&gt; &#xA;&lt;p&gt;Development on the &lt;code&gt;main&lt;/code&gt; branch is for the latest version of &lt;a href=&#34;https://developer.nvidia.com/nvidia-tensorrt-download&#34;&gt;TensorRT 8.5.1&lt;/a&gt; with full-dimensions and dynamic shape support.&lt;/p&gt; &#xA;&lt;p&gt;For previous versions of TensorRT, refer to their respective branches.&lt;/p&gt; &#xA;&lt;h2&gt;Full Dimensions + Dynamic Shapes&lt;/h2&gt; &#xA;&lt;p&gt;Building INetwork objects in full dimensions mode with dynamic shape support requires calling the following API:&lt;/p&gt; &#xA;&lt;p&gt;C++&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;const auto explicitBatch = 1U &amp;lt;&amp;lt; static_cast&amp;lt;uint32_t&amp;gt;(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);&#xA;builder-&amp;gt;createNetworkV2(explicitBatch)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import tensorrt&#xA;explicit_batch = 1 &amp;lt;&amp;lt; (int)(tensorrt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)&#xA;builder.create_network(explicit_batch)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For examples of usage of these APIs see:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleOnnxMNIST&#34;&gt;sampleONNXMNIST&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleDynamicReshape&#34;&gt;sampleDynamicReshape&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Operators&lt;/h2&gt; &#xA;&lt;p&gt;Current supported ONNX operators are found in the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx-tensorrt/main/docs/operators.md&#34;&gt;operator support matrix&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/protobuf/releases&#34;&gt;Protobuf &amp;gt;= 3.0.x&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT 8.5.1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT/&#34;&gt;TensorRT 8.5.1 open source libaries (main branch)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;For building within docker, we recommend using and setting up the docker containers as instructed in the main &lt;a href=&#34;https://github.com/NVIDIA/TensorRT#setting-up-the-build-environment&#34;&gt;TensorRT repository&lt;/a&gt; to build the onnx-tensorrt library.&lt;/p&gt; &#xA;&lt;p&gt;Once you have cloned the repository, you can build the parser libraries and executables by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd onnx-tensorrt&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake .. -DTENSORRT_ROOT=&amp;lt;path_to_trt&amp;gt; &amp;amp;&amp;amp; make -j&#xA;// Ensure that you update your LD_LIBRARY_PATH to pick up the location of the newly built library:&#xA;export LD_LIBRARY_PATH=$PWD:$LD_LIBRARY_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this project has a dependency on CUDA. By default the build will look in &lt;code&gt;/usr/local/cuda&lt;/code&gt; for the CUDA toolkit installation. If your CUDA path is different, overwrite the default path by providing &lt;code&gt;-DCUDA_TOOLKIT_ROOT_DIR=&amp;lt;path_to_cuda_install&amp;gt;&lt;/code&gt; in the CMake command.&lt;/p&gt; &#xA;&lt;h3&gt;Experimental Ops&lt;/h3&gt; &#xA;&lt;p&gt;All experimental operators will be considered unsupported by the ONNX-TRT&#39;s &lt;code&gt;supportsModel()&lt;/code&gt; function.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;NonMaxSuppression&lt;/code&gt; is available as an experimental operator in TensorRT 8. It has the limitation that the output shape is always padded to length [&lt;code&gt;max_output_boxes_per_class&lt;/code&gt;, 3], therefore some post processing is required to extract the valid indices.&lt;/p&gt; &#xA;&lt;h2&gt;Executable Usage&lt;/h2&gt; &#xA;&lt;p&gt;There are currently two officially supported tools for users to quickly check if an ONNX model can parse and build into a TensorRT engine from an ONNX file.&lt;/p&gt; &#xA;&lt;p&gt;For C++ users, there is the &lt;a href=&#34;https://github.com/NVIDIA/TensorRT/tree/main/samples/opensource/trtexec&#34;&gt;trtexec&lt;/a&gt; binary that is typically found in the &lt;code&gt;&amp;lt;tensorrt_root_dir&amp;gt;/bin&lt;/code&gt; directory. The basic command of running an ONNX model is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;trtexec --onnx=model.onnx&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to the link or run &lt;code&gt;trtexec -h&lt;/code&gt; for more information on CLI options.&lt;/p&gt; &#xA;&lt;p&gt;For Python users, there is the &lt;a href=&#34;https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy&#34;&gt;polygraphy&lt;/a&gt; tool. The basic command for running an onnx model is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;polygraphy run model.onnx --trt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to the link or run &lt;code&gt;polygraphy run -h&lt;/code&gt; for more information on CLI options.&lt;/p&gt; &#xA;&lt;h3&gt;Python Modules&lt;/h3&gt; &#xA;&lt;p&gt;Python bindings for the ONNX-TensorRT parser are packaged in the shipped &lt;code&gt;.whl&lt;/code&gt; files. Install them with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install &amp;lt;tensorrt_install_dir&amp;gt;/python/tensorrt-8.x.x.x-cp&amp;lt;python_ver&amp;gt;-none-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;TensorRT 8.5.1 supports ONNX release 1.12.0. Install it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install onnx==1.12.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The ONNX-TensorRT backend can be installed by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ONNX-TensorRT Python Backend Usage&lt;/h2&gt; &#xA;&lt;p&gt;The TensorRT backend for ONNX can be used in Python as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import onnx&#xA;import onnx_tensorrt.backend as backend&#xA;import numpy as np&#xA;&#xA;model = onnx.load(&#34;/path/to/model.onnx&#34;)&#xA;engine = backend.prepare(model, device=&#39;CUDA:1&#39;)&#xA;input_data = np.random.random(size=(32, 3, 224, 224)).astype(np.float32)&#xA;output_data = engine.run(input_data)[0]&#xA;print(output_data)&#xA;print(output_data.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;C++ Library Usage&lt;/h2&gt; &#xA;&lt;p&gt;The model parser library, libnvonnxparser.so, has its C++ API declared in this header:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;NvOnnxParser.h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tests&lt;/h3&gt; &#xA;&lt;p&gt;After installation (or inside the Docker container), ONNX backend tests can be run as follows:&lt;/p&gt; &#xA;&lt;p&gt;Real model tests only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python onnx_backend_test.py OnnxBackendRealModelTest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python onnx_backend_test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;-v&lt;/code&gt; flag to make output more verbose.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-trained Models&lt;/h2&gt; &#xA;&lt;p&gt;Pre-trained models in ONNX format can be found at the &lt;a href=&#34;https://github.com/onnx/models&#34;&gt;ONNX Model Zoo&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>