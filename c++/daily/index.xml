<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-06T01:30:28Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ComodoSecurity/openedr</title>
    <updated>2023-01-06T01:30:28Z</updated>
    <id>tag:github.com,2023-01-06:/ComodoSecurity/openedr</id>
    <link href="https://github.com/ComodoSecurity/openedr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open EDR public repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenEDR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openedr.com/&#34;&gt;&lt;img src=&#34;https://techtalk.comodo.com/wp-content/uploads/2020/09/logo_small.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openedr.com/register/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-join-blue.svg?sanitize=true&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;mailto:register@openedr.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/email-join-blue.svg?sanitize=true&#34; alt=&#34;Email&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=lfo_fyinvYs&#34; title=&#34;OpenEDR - Getting Started&#34;&gt;&lt;img src=&#34;https://techtalk.comodo.com/wp-content/uploads/2022/10/Screenshot_3.jpg&#34; alt=&#34;OpenEDR - Getting Started&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We at OpenEDR believe in creating a cybersecurity platform with its source code openly available to the public, where products and services can be provisioned and managed together. EDR is our starting point. OpenEDR is a full-blown EDR capability. It is one of the most sophisticated, effective EDR code base in the world and with the community’s help, it will become even better.&lt;/p&gt; &#xA;&lt;p&gt;OpenEDR is free and its source code is open to the public. OpenEDR allows you to analyze what’s happening across your entire environment at the base-security-event level. This granularity enables accurate root-causes analysis needed for faster and more effective remediation. Proven to be the best way to convey this type of information, process hierarchy tracking provides more than just data, they offer actionable knowledge. It collects all the details on endpoints, hashes, and base and advanced events. You get detailed file and device trajectory information and can navigate single events to uncover a larger issue that may be compromising your system.&lt;/p&gt; &#xA;&lt;p&gt;OpenEDR’s security architecture simplifies &lt;em&gt;breach detection, protection, and visibility&lt;/em&gt; by working for all threat vectors without requiring any other agent or solution. The agent records all telemetry information locally and then will send the data to locally hosted or cloud-hosted ElasticSearch deployments. Real-time visibility and continuous analysis are vital elements of the entire endpoint security concept. OpenEDR enables you to perform analysis into what&#39;s happening across your environment at base event level granularity. This allows accurate root cause analysis leading to better remediation of your compromises. Integrated Security Architecture of OpenEDR delivers Full Attack Vector Visibility including MITRE Framework.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The community response to OpenEDR has been absolutely amazing! Thank you. We had a lot of requests from people who want to deploy and use OpenEDR easily and quickly. We have a roadmap to achieve all these. However in the meanwhile, we have decided to use the Comodo Dragon Enterprise platform with OpenEDR to achieve that. By simply opening an account, you will be able to use OpenEDR. No custom installation, no log forwarding configuration, or worrying about storing telemetry data. All of that is handled by the Comodo Dragon Platform. This is only a short-term solution until all the easy-to-use packages for OpenEDR is finalized. In the meanwhile do take advantage of this by emailing &lt;a href=&#34;mailto:quick-start@openedr.com&#34;&gt;quick-start@openedr.com&lt;/a&gt; to get you up and running!&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;The Open EDR consists of the following components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Runtime components &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Core Library – the basic framework;&lt;/li&gt; &#xA;   &lt;li&gt;Service – service application;&lt;/li&gt; &#xA;   &lt;li&gt;Process Monitor – components for per-process monitoring; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Injected DLL – the library which is injected into different processes and hooks API calls;&lt;/li&gt; &#xA;     &lt;li&gt;Loader for Injected DLL – the driver component which loads injected DLL into each new process&lt;/li&gt; &#xA;     &lt;li&gt;Controller for Injected DLL – service component for interaction with Injected DLL;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;System Monitor – the genetic container for different kernel-mode components;&lt;/li&gt; &#xA;   &lt;li&gt;File-system mini-filter – the kernel component that hooks I/O requests file system;&lt;/li&gt; &#xA;   &lt;li&gt;Low-level process monitoring component – monitors processes creation/deletion using system callbacks&lt;/li&gt; &#xA;   &lt;li&gt;Low-level registry monitoring component – monitors registry access using system callbacks&lt;/li&gt; &#xA;   &lt;li&gt;Self-protection provider – prevents EDR components and configuration from unauthorized changes&lt;/li&gt; &#xA;   &lt;li&gt;Network monitor – network filter for monitoring the network activity;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Installer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Generic high-level interaction diagram for runtime components&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://techtalk.comodo.com/wp-content/uploads/2020/09/image.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For details, you can refer here: &lt;a href=&#34;https://techtalk.comodo.com/2020/09/19/open-edr-components/&#34;&gt;https://techtalk.comodo.com/2020/09/19/open-edr-components/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Community Forums: &lt;a href=&#34;https://community.openedr.com/&#34;&gt;https://community.openedr.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join Slack &lt;a href=&#34;https://openedr.com/register/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-join-blue.svg?sanitize=true&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Registration &lt;a href=&#34;mailto:register@openedr.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/email-join-blue.svg?sanitize=true&#34; alt=&#34;Email&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Please refer here for project roadmap : &lt;a href=&#34;https://github.com/ComodoSecurity/openedr_roadmap/projects/1&#34;&gt;https://github.com/ComodoSecurity/openedr_roadmap/projects/1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please take a look at the following documents.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/InstallationInstructions.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/BuildInstructions.md&#34;&gt;Build Instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/DockerInstallation.md&#34;&gt;Docker Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/SettingELK.md&#34;&gt;Setting up Elasticsearch Kibana and Logstash&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/SettingFileBeat.md&#34;&gt;Setting up Openedr and File beat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/EditingAlertingPolicies.md&#34;&gt;Editing Alerting Policies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ComodoSecurity/openedr/main/getting-started/SettingKibana.md&#34;&gt;Setting Up Kibana&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Releases&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ComodoSecurity/openedr/releases/tag/2.5.0.0&#34;&gt;https://github.com/ComodoSecurity/openedr/releases/tag/2.5.0.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Screenshots&lt;/h1&gt; &#xA;&lt;p&gt;How OpenEDR integration with a platform looks like and also a showcase for openedr capabilities&lt;/p&gt; &#xA;&lt;p&gt;Detection / Alerting &lt;a href=&#34;https://enterprise.comodo.com/dragon/&#34;&gt;&lt;img src=&#34;https://github.com/ComodoSecurity/openedr/raw/main/docs/screenshots/Screenshot_1.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Event Details &lt;a href=&#34;https://enterprise.comodo.com/dragon/&#34;&gt;&lt;img src=&#34;https://github.com/ComodoSecurity/openedr/raw/main/docs/screenshots/Screenshot_2.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dashboard &lt;a href=&#34;https://enterprise.comodo.com/dragon/&#34;&gt;&lt;img src=&#34;https://github.com/ComodoSecurity/openedr/raw/main/docs/screenshots/Screenshot_3.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Process Timeline &lt;a href=&#34;https://enterprise.comodo.com/dragon/&#34;&gt;&lt;img src=&#34;https://github.com/ComodoSecurity/openedr/raw/main/docs/screenshots/Screenshot_4.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Process Treeview &lt;a href=&#34;https://enterprise.comodo.com/dragon/&#34;&gt;&lt;img src=&#34;https://github.com/ComodoSecurity/openedr/raw/main/docs/screenshots/Screenshot_5.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Event Search &lt;a href=&#34;https://enterprise.comodo.com/dragon/&#34;&gt;&lt;img src=&#34;https://github.com/ComodoSecurity/openedr/raw/main/docs/screenshots/Screenshot_6.jpg&#34; alt=&#34;OpenEDR&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/nccl</title>
    <updated>2023-01-06T01:30:28Z</updated>
    <id>tag:github.com,2023-01-06:/NVIDIA/nccl</id>
    <link href="https://github.com/NVIDIA/nccl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Optimized primitives for collective multi-GPU communication&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NCCL&lt;/h1&gt; &#xA;&lt;p&gt;Optimized primitives for inter-GPU communication.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NCCL (pronounced &#34;Nickel&#34;) is a stand-alone library of standard communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, as well as any send/receive based communication pattern. It has been optimized to achieve high bandwidth on platforms using PCIe, NVLink, NVswitch, as well as networking using InfiniBand Verbs or TCP/IP sockets. NCCL supports an arbitrary number of GPUs installed in a single node or across multiple nodes, and can be used in either single- or multi-process (e.g., MPI) applications.&lt;/p&gt; &#xA;&lt;p&gt;For more information on NCCL usage, please refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/index.html&#34;&gt;NCCL documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Note: the official and tested builds of NCCL can be downloaded from: &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;https://developer.nvidia.com/nccl&lt;/a&gt;. You can skip the following build steps if you choose to use the official builds.&lt;/p&gt; &#xA;&lt;p&gt;To build the library :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd nccl&#xA;$ make -j src.build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If CUDA is not installed in the default /usr/local/cuda path, you can define the CUDA path with :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ make src.build CUDA_HOME=&amp;lt;path to cuda install&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NCCL will be compiled and installed in &lt;code&gt;build/&lt;/code&gt; unless &lt;code&gt;BUILDDIR&lt;/code&gt; is set.&lt;/p&gt; &#xA;&lt;p&gt;By default, NCCL is compiled for all supported architectures. To accelerate the compilation and reduce the binary size, consider redefining &lt;code&gt;NVCC_GENCODE&lt;/code&gt; (defined in &lt;code&gt;makefiles/common.mk&lt;/code&gt;) to only include the architecture of the target platform :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ make -j src.build NVCC_GENCODE=&#34;-gencode=arch=compute_70,code=sm_70&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;To install NCCL on the system, create a package then install it as root.&lt;/p&gt; &#xA;&lt;p&gt;Debian/Ubuntu :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Install tools to create debian packages&#xA;$ sudo apt install build-essential devscripts debhelper fakeroot&#xA;$ # Build NCCL deb package&#xA;$ make pkg.debian.build&#xA;$ ls build/pkg/deb/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;RedHat/CentOS :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Install tools to create rpm packages&#xA;$ sudo yum install rpm-build rpmdevtools&#xA;$ # Build NCCL rpm package&#xA;$ make pkg.redhat.build&#xA;$ ls build/pkg/rpm/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OS-agnostic tarball :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ make pkg.txz.build&#xA;$ ls build/pkg/txz/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;Tests for NCCL are maintained separately at &lt;a href=&#34;https://github.com/nvidia/nccl-tests&#34;&gt;https://github.com/nvidia/nccl-tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/NVIDIA/nccl-tests.git&#xA;$ cd nccl-tests&#xA;$ make&#xA;$ ./build/all_reduce_perf -b 8 -e 256M -f 2 -g &amp;lt;ngpus&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Copyright&lt;/h2&gt; &#xA;&lt;p&gt;All source code and accompanying documentation is copyright (c) 2015-2020, NVIDIA CORPORATION. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>triton-inference-server/client</title>
    <updated>2023-01-06T01:30:28Z</updated>
    <id>tag:github.com,2023-01-06:/triton-inference-server/client</id>
    <link href="https://github.com/triton-inference-server/client" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Triton Python, C++ and Java client libraries, and GRPC-generated client examples for go, java and scala.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/BSD-3-Clause&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-BSD3-lightgrey.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Triton Client Libraries and Examples&lt;/h1&gt; &#xA;&lt;p&gt;To simplify communication with Triton, the Triton project provides several client libraries and examples of how to use those libraries. Ask questions or report problems in the main Triton &lt;a href=&#34;https://github.com/triton-inference-server/server/issues&#34;&gt;issues page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The provided client libraries are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#client-library-apis&#34;&gt;C++ and Python APIs&lt;/a&gt; that make it easy to communicate with Triton from your C++ or Python application. Using these libraries you can send either HTTP/REST or GRPC requests to Triton to access all its capabilities: inferencing, status and health, statistics and metrics, model repository management, etc. These libraries also support using system and CUDA shared memory for passing inputs to and receiving outputs from Triton.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#client-library-apis&#34;&gt;Java API&lt;/a&gt; (contributed by Alibaba Cloud PAI Team) that makes it easy to communicate with Triton from your Java application using HTTP/REST requests. For now, only a limited feature subset is supported.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;protoc compiler&lt;/a&gt; can generate a GRPC API in a large number of programming languages.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/grpc_generated/go&#34;&gt;src/grpc_generated/go&lt;/a&gt; for an example for the &lt;a href=&#34;https://golang.org/&#34;&gt;Go programming language&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/grpc_generated/java&#34;&gt;src/grpc_generated/java&lt;/a&gt; for an example for the Java and Scala programming languages.&lt;/li&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/grpc_generated/javascript&#34;&gt;src/grpc_generated/javascript&lt;/a&gt; for an example with JavaScript programming language.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are also many example applications that show how to use these libraries. Many of these examples use models from the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/getting_started/quickstart.md#create-a-model-repository&#34;&gt;example model repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;C++ and Python versions of &lt;em&gt;image_client&lt;/em&gt;, an example application that uses the C++ or Python client library to execute image classification models on Triton. See &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#image-classification-example&#34;&gt;Image Classification Example&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Several simple &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples&#34;&gt;C++ examples&lt;/a&gt; show how to use the C++ library to communicate with Triton to perform inferencing and other task. The C++ examples demonstrating the HTTP/REST client are named with a &lt;em&gt;simple_http_&lt;/em&gt; prefix and the examples demonstrating the GRPC client are named with a &lt;em&gt;simple_grpc_&lt;/em&gt; prefix. See &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#simple-example-applications&#34;&gt;Simple Example Applications&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Several simple &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples&#34;&gt;Python examples&lt;/a&gt; show how to use the Python library to communicate with Triton to perform inferencing and other task. The Python examples demonstrating the HTTP/REST client are named with a &lt;em&gt;simple_http_&lt;/em&gt; prefix and the examples demonstrating the GRPC client are named with a &lt;em&gt;simple_grpc_&lt;/em&gt; prefix. See &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#simple-example-applications&#34;&gt;Simple Example Applications&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Several simple &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/java/src/main/java/triton/client/examples&#34;&gt;Java examples&lt;/a&gt; show how to use the Java API to communicate with Triton to perform inferencing and other task.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A couple of &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples&#34;&gt;Python examples that communicate with Triton using a Python GRPC API&lt;/a&gt; generated by the &lt;a href=&#34;https://grpc.io/docs/guides/&#34;&gt;protoc compiler&lt;/a&gt;. &lt;em&gt;grpc_client.py&lt;/em&gt; is a simple example that shows simple API usage. &lt;em&gt;grpc_image_client.py&lt;/em&gt; is functionally equivalent to &lt;em&gt;image_client&lt;/em&gt; but that uses a generated GRPC client stub to communicate with Triton.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting the Client Libraries And Examples&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get the Python client library is to &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#download-using-python-package-installer-pip&#34;&gt;use pip to install the tritonclient module&lt;/a&gt;. You can also download the C++, Python and Java client libraries from &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#download-from-github&#34;&gt;Triton GitHub release&lt;/a&gt;, or &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#download-docker-image-from-ngc&#34;&gt;download a pre-built Docker image containing the client libraries&lt;/a&gt; from &lt;a href=&#34;https://ngc.nvidia.com&#34;&gt;NVIDIA GPU Cloud (NGC)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is also possible to build the client libraries with &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/#build-using-cmake&#34;&gt;cmake&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Download Using Python Package Installer (pip)&lt;/h3&gt; &#xA;&lt;p&gt;The GRPC and HTTP client libraries are available as a Python package that can be installed using a recent version of pip.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tritonclient[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using &lt;em&gt;all&lt;/em&gt; installs both the HTTP/REST and GRPC client libraries. There are two optional packages available, &lt;em&gt;grpc&lt;/em&gt; and &lt;em&gt;http&lt;/em&gt; that can be used to install support specifically for the protocol. For example, to install only the HTTP/REST client library use,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tritonclient[http]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The components of the install packages are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;http&lt;/li&gt; &#xA; &lt;li&gt;grpc [ &lt;code&gt;service_pb2&lt;/code&gt;, &lt;code&gt;service_pb2_grpc&lt;/code&gt;, &lt;code&gt;model_config_pb2&lt;/code&gt; ]&lt;/li&gt; &#xA; &lt;li&gt;utils [ linux distribution will include &lt;code&gt;shared_memory&lt;/code&gt; and &lt;code&gt;cuda_shared_memory&lt;/code&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Linux version of the package also includes the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/user_guide/perf_analyzer.md&#34;&gt;perf_analyzer&lt;/a&gt; binary. The perf_analyzer binary is built on Ubuntu 20.04 and may not run on other Linux distributions. To run the perf_analyzer the following dependency must be installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt update&#xA;$ sudo apt install libb64-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reiterate, the installation on windows will not include perf_analyzer nor shared_memory/cuda_shared_memory components.&lt;/p&gt; &#xA;&lt;h3&gt;Download From GitHub&lt;/h3&gt; &#xA;&lt;p&gt;The client libraries and the perf_analyzer executable can be downloaded from the &lt;a href=&#34;https://github.com/triton-inference-server/server/releases&#34;&gt;Triton GitHub release page&lt;/a&gt; corresponding to the release you are interested in. The client libraries are found in the &#34;Assets&#34; section of the release page in a tar file named after the version of the release and the OS, for example, v2.3.0_ubuntu2004.clients.tar.gz.&lt;/p&gt; &#xA;&lt;p&gt;The pre-built libraries can be used on the corresponding host system or you can install them into the Triton container to have both the clients and server in the same container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir clients&#xA;$ cd clients&#xA;$ wget https://github.com/triton-inference-server/server/releases/download/&amp;lt;tarfile_path&amp;gt;&#xA;$ tar xzf &amp;lt;tarfile_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installing, the libraries can be found in lib/, the headers in include/, the Python wheel files in python/, and the jar files in java/. The bin/ and python/ directories contain the built examples that you can learn more about below.&lt;/p&gt; &#xA;&lt;p&gt;The perf_analyzer binary is built on Ubuntu 20.04 and may not run on other Linux distributions. To use the C++ libraries or perf_analyzer executable you must install some dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ apt-get update&#xA;$ apt-get install curl libcurl4-openssl-dev libb64-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Docker Image From NGC&lt;/h3&gt; &#xA;&lt;p&gt;A Docker image containing the client libraries and examples is available from &lt;a href=&#34;https://ngc.nvidia.com&#34;&gt;NVIDIA GPU Cloud (NGC)&lt;/a&gt;. Before attempting to pull the container ensure you have access to NGC. For step-by-step instructions, see the &lt;a href=&#34;http://docs.nvidia.com/ngc/ngc-getting-started-guide/index.html&#34;&gt;NGC Getting Started Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use docker pull to get the client libraries and examples container from NGC.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker pull nvcr.io/nvidia/tritonserver:&amp;lt;xx.yy&amp;gt;-py3-sdk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &amp;lt;xx.yy&amp;gt; is the version that you want to pull. Within the container the client libraries are in /workspace/install/lib, the corresponding headers in /workspace/install/include, and the Python wheel files in /workspace/install/python. The image will also contain the built client examples.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; When running either the server or the client using Docker containers and using the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/protocol/extension_shared_memory.md#cuda-shared-memory&#34;&gt;CUDA shared memory feature&lt;/a&gt; you need to add &lt;code&gt;--pid host&lt;/code&gt; flag when launching the containers. The reason is that CUDA IPC APIs require the PID of the source and destination of the exported pointer to be different. Otherwise, Docker enables PID namespace which may result in equality between the source and destination PIDs. The error will be always observed when both of the containers are started in the non-interactive mode.&lt;/p&gt; &#xA;&lt;h3&gt;Build Using CMake&lt;/h3&gt; &#xA;&lt;p&gt;The client library build is performed using CMake. To build the client libraries and examples with all features, first change directory to the root of this repo and checkout the release version of the branch that you want to build (or the &lt;em&gt;main&lt;/em&gt; branch if you want to build the under-development version).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git checkout main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If building the Java client you must first install Maven and a JDK appropriate for your OS. For example, for Ubuntu you should install the &lt;code&gt;default-jdk&lt;/code&gt; package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ apt-get install default-jdk maven&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Building on Windows vs. non-Windows requires different invocations because Triton on Windows does not yet support all the build options.&lt;/p&gt; &#xA;&lt;h4&gt;Non-Windows&lt;/h4&gt; &#xA;&lt;p&gt;Use &lt;em&gt;cmake&lt;/em&gt; to configure the build. You should adjust the flags depending on the components of Triton Client you are working and would like to build. For example, if you want to build Perf Analyzer with Triton C API, you can use &lt;br&gt; &lt;code&gt;-DTRITON_ENABLE_PERF_ANALYZER=ON -DTRITON_ENABLE_PERF_ANALYZER_C_API=ON&lt;/code&gt;. You can also use &lt;code&gt;TRITON_ENABLE_PERF_ANALYZER_TFS&lt;/code&gt; and &lt;code&gt;TRITON_ENABLE_PERF_ANALYZER_TS&lt;/code&gt; flags to enable/disable support for TensorFlow Serving and TorchServe backend respectively in perf analyzer. &lt;br&gt; The following command demonstrate how to build client with all the features:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ mkdir build&#xA;$ cd build&#xA;$ cmake -DCMAKE_INSTALL_PREFIX=`pwd`/install -DTRITON_ENABLE_CC_HTTP=ON -DTRITON_ENABLE_CC_GRPC=ON -DTRITON_ENABLE_PERF_ANALYZER=ON -DTRITON_ENABLE_PERF_ANALYZER_C_API=ON -DTRITON_ENABLE_PERF_ANALYZER_TFS=ON -DTRITON_ENABLE_PERF_ANALYZER_TS=ON -DTRITON_ENABLE_PYTHON_HTTP=ON -DTRITON_ENABLE_PYTHON_GRPC=ON -DTRITON_ENABLE_JAVA_HTTP=ON -DTRITON_ENABLE_GPU=ON -DTRITON_ENABLE_EXAMPLES=ON -DTRITON_ENABLE_TESTS=ON ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are building on a release branch (or on a development branch that is based off of a release branch), then you must also use additional cmake arguments to point to that release branch for repos that the client build depends on. For example, if you are building the r21.10 client branch then you need to use the following additional cmake flags:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-DTRITON_COMMON_REPO_TAG=r21.10&#xA;-DTRITON_THIRD_PARTY_REPO_TAG=r21.10&#xA;-DTRITON_CORE_REPO_TAG=r21.10&#xA;-DTRITON_BACKEND_REPO_TAG=r21.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use &lt;em&gt;make&lt;/em&gt; to build the clients and examples.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make cc-clients python-clients java-clients&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the build completes the libraries and examples can be found in the install directory.&lt;/p&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;To build the clients you must install an appropriate C++ compiler and other dependencies required for the build. The easiest way to do this is to create the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/customization_guide/build.md#windows-10-min-container&#34;&gt;Windows min Docker image&lt;/a&gt; and the perform the build within a container launched from that image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; docker run  -it --rm win10-py3-min powershell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is not necessary to use Docker or the win10-py3-min container for the build, but if you do not you must install the appropriate dependencies onto your host system.&lt;/p&gt; &#xA;&lt;p&gt;Next use &lt;em&gt;cmake&lt;/em&gt; to configure the build. If you are not building within the win10-py3-min container then you will likely need to adjust the CMAKE_TOOLCHAIN_FILE location in the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ mkdir build&#xA;$ cd build&#xA;$ cmake -DVCPKG_TARGET_TRIPLET=x64-windows -DCMAKE_TOOLCHAIN_FILE=&#39;/vcpkg/scripts/buildsystems/vcpkg.cmake&#39; -DCMAKE_INSTALL_PREFIX=install -DTRITON_ENABLE_CC_GRPC=ON -DTRITON_ENABLE_PYTHON_GRPC=ON -DTRITON_ENABLE_GPU=OFF -DTRITON_ENABLE_EXAMPLES=ON -DTRITON_ENABLE_TESTS=ON ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are building on a release branch (or on a development branch that is based off of a release branch), then you must also use additional cmake arguments to point to that release branch for repos that the client build depends on. For example, if you are building the r21.10 client branch then you need to use the following additional cmake flags:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-DTRITON_COMMON_REPO_TAG=r21.10&#xA;-DTRITON_THIRD_PARTY_REPO_TAG=r21.10&#xA;-DTRITON_CORE_REPO_TAG=r21.10&#xA;-DTRITON_BACKEND_REPO_TAG=r21.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use msbuild.exe to build.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ msbuild.exe cc-clients.vcxproj -p:Configuration=Release -clp:ErrorsOnly&#xA;$ msbuild.exe python-clients.vcxproj -p:Configuration=Release -clp:ErrorsOnly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the build completes the libraries and examples can be found in the install directory.&lt;/p&gt; &#xA;&lt;h2&gt;Client Library APIs&lt;/h2&gt; &#xA;&lt;p&gt;The C++ client API exposes a class-based interface. The commented interface is available in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/grpc_client.h&#34;&gt;grpc_client.h&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/http_client.h&#34;&gt;http_client.h&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/common.h&#34;&gt;common.h&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Python client API provides similar capabilities as the C++ API. The commented interface is available in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/grpc/__init__.py&#34;&gt;grpc&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/http/__init__.py&#34;&gt;http&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Java client API provides similar capabilities as the Python API with similar classes and methods. For more information please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/java&#34;&gt;Java client directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;HTTP Options&lt;/h3&gt; &#xA;&lt;h4&gt;SSL/TLS&lt;/h4&gt; &#xA;&lt;p&gt;The client library allows communication across a secured channel using HTTPS protocol. Just setting these SSL options do not ensure the secure communication. Triton server should be running behind &lt;code&gt;https://&lt;/code&gt; proxy such as nginx. The client can then establish a secure channel to the proxy. The &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/qa/L0_https/test.sh&#34;&gt;&lt;code&gt;qa/L0_https&lt;/code&gt;&lt;/a&gt; in the server repository demonstrates how this can be achieved.&lt;/p&gt; &#xA;&lt;p&gt;For C++ client, see &lt;code&gt;HttpSslOptions&lt;/code&gt; struct that encapsulates these options in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/http_client.h&#34;&gt;http_client.h&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Python client, look for the following options in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/http/__init__.py&#34;&gt;http/__init__.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ssl&lt;/li&gt; &#xA; &lt;li&gt;ssl_options&lt;/li&gt; &#xA; &lt;li&gt;ssl_context_factory&lt;/li&gt; &#xA; &lt;li&gt;insecure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/simple_http_infer_client.cc&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_http_infer_client.py&#34;&gt;Python&lt;/a&gt; examples demonstrates how to use SSL/TLS settings on client side.&lt;/p&gt; &#xA;&lt;h4&gt;Compression&lt;/h4&gt; &#xA;&lt;p&gt;The client library enables on-wire compression for HTTP transactions.&lt;/p&gt; &#xA;&lt;p&gt;For C++ client, see &lt;code&gt;request_compression_algorithm&lt;/code&gt; and &lt;code&gt;response_compression_algorithm&lt;/code&gt; parameters in the &lt;code&gt;Infer&lt;/code&gt; and &lt;code&gt;AsyncInfer&lt;/code&gt; functions in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/http_client.h&#34;&gt;http_client.h&lt;/a&gt;. By default, the parameter is set as &lt;code&gt;CompressionType::NONE&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Similarly, for Python client, see &lt;code&gt;request_compression_algorithm&lt;/code&gt; and &lt;code&gt;response_compression_algorithm&lt;/code&gt; parameters in &lt;code&gt;infer&lt;/code&gt; and &lt;code&gt;async_infer&lt;/code&gt; functions in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/http/__init__.py&#34;&gt;http/__init__.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/simple_http_infer_client.cc&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_http_infer_client.py&#34;&gt;Python&lt;/a&gt; examples demonstrates how to use compression options.&lt;/p&gt; &#xA;&lt;h4&gt;Python AsyncIO Support (Beta)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;This feature is currently in beta and may be subject to change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Advanced users may call the Python client via &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; syntax. The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_http_aio_infer_client.py&#34;&gt;infer&lt;/a&gt; example demonstrates how to infer with AsyncIO.&lt;/p&gt; &#xA;&lt;p&gt;If using SSL/TLS with AsyncIO, look for the &lt;code&gt;ssl&lt;/code&gt; and &lt;code&gt;ssl_context&lt;/code&gt; options in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/http/aio/__init__.py&#34;&gt;http/aio/__init__.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GRPC Options&lt;/h3&gt; &#xA;&lt;h4&gt;SSL/TLS&lt;/h4&gt; &#xA;&lt;p&gt;The client library allows communication across a secured channel using gRPC protocol.&lt;/p&gt; &#xA;&lt;p&gt;For C++ client, see &lt;code&gt;SslOptions&lt;/code&gt; struct that encapsulates these options in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/grpc_client.h&#34;&gt;grpc_client.h&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Python client, look for the following options in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/grpc/__init__.py&#34;&gt;grpc/__init__.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ssl&lt;/li&gt; &#xA; &lt;li&gt;root_certificates&lt;/li&gt; &#xA; &lt;li&gt;private_key&lt;/li&gt; &#xA; &lt;li&gt;certificate_chain&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/simple_grpc_infer_client.cc&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_grpc_infer_client.py&#34;&gt;Python&lt;/a&gt; examples demonstrates how to use SSL/TLS settings on client side. For information on the corresponding server-side parameters, refer to the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/customization_guide/inference_protocols.md#ssltls&#34;&gt;server documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Compression&lt;/h4&gt; &#xA;&lt;p&gt;The client library also exposes options to use on-wire compression for gRPC transactions.&lt;/p&gt; &#xA;&lt;p&gt;For C++ client, see &lt;code&gt;compression_algorithm&lt;/code&gt; parameter in the &lt;code&gt;Infer&lt;/code&gt;, &lt;code&gt;AsyncInfer&lt;/code&gt; and &lt;code&gt;StartStream&lt;/code&gt; functions in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/grpc_client.h&#34;&gt;grpc_client.h&lt;/a&gt;. By default, the parameter is set as &lt;code&gt;GRPC_COMPRESS_NONE&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Similarly, for Python client, see &lt;code&gt;compression_algorithm&lt;/code&gt; parameter in &lt;code&gt;infer&lt;/code&gt;, &lt;code&gt;async_infer&lt;/code&gt; and &lt;code&gt;start_stream&lt;/code&gt; functions in &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/grpc/__init__.py&#34;&gt;grpc/__init__.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/simple_grpc_infer_client.cc&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_grpc_infer_client.py&#34;&gt;Python&lt;/a&gt; examples demonstrates how to configure compression for clients. For information on the corresponding server-side parameters, refer to the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/customization_guide/inference_protocols.md#compression&#34;&gt;server documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;GRPC KeepAlive&lt;/h4&gt; &#xA;&lt;p&gt;Triton exposes GRPC KeepAlive parameters with the default values for both client and server described &lt;a href=&#34;https://github.com/grpc/grpc/raw/master/doc/keepalive.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can find a &lt;code&gt;KeepAliveOptions&lt;/code&gt; struct/class that encapsulates these parameters in both the &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/library/grpc_client.h&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/grpc/__init__.py&#34;&gt;Python&lt;/a&gt; client libraries.&lt;/p&gt; &#xA;&lt;p&gt;There is also a &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/simple_grpc_keepalive_client.cc&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_grpc_keepalive_client.py&#34;&gt;Python&lt;/a&gt; example demonstrating how to setup these parameters on the client-side. For information on the corresponding server-side parameters, refer to the &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/customization_guide/inference_protocols.md#grpc-keepalive&#34;&gt;server documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Custom GRPC Channel Arguments&lt;/h4&gt; &#xA;&lt;p&gt;Advanced users may require specific client-side GRPC Channel Arguments that are not currently exposed by Triton through direct means. To support this, Triton allows users to pass custom channel arguments upon creating a GRPC client. When using this option, it is up to the user to pass a valid combination of arguments for their use case; Triton cannot feasibly test every possible combination of channel arguments.&lt;/p&gt; &#xA;&lt;p&gt;There is a &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/simple_grpc_custom_args_client.cc&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_grpc_custom_args_client.py&#34;&gt;Python&lt;/a&gt; example demonstrating how to construct and pass these custom arguments upon creating a GRPC client.&lt;/p&gt; &#xA;&lt;p&gt;You can find a comprehensive list of possible GRPC Channel Arguments &lt;a href=&#34;https://grpc.github.io/grpc/core/group__grpc__arg__keys.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Python AsyncIO Support (Beta)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;This feature is currently in beta and may be subject to change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Advanced users may call the Python client via &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; syntax. The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_grpc_aio_infer_client.py&#34;&gt;infer&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/simple_grpc_aio_sequence_stream_infer_client.py&#34;&gt;stream&lt;/a&gt; examples demonstrate how to infer with AsyncIO.&lt;/p&gt; &#xA;&lt;h2&gt;Simple Example Applications&lt;/h2&gt; &#xA;&lt;p&gt;This section describes several of the simple example applications and the features that they illustrate.&lt;/p&gt; &#xA;&lt;h3&gt;Bytes/String Datatype&lt;/h3&gt; &#xA;&lt;p&gt;Some frameworks support tensors where each element in the tensor is variable-length binary data. Each element can hold a string or an arbitrary sequence of bytes. On the client this datatype is BYTES (see &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/user_guide/model_configuration.md#datatypes&#34;&gt;Datatypes&lt;/a&gt; for information on supported datatypes).&lt;/p&gt; &#xA;&lt;p&gt;The Python client library uses numpy to represent input and output tensors. For BYTES tensors the dtype of the numpy array should be &#39;np.object_&#39; as shown in the examples. For backwards compatibility with previous versions of the client library, &#39;np.bytes_&#39; can also be used for BYTES tensors. However, using &#39;np.bytes_&#39; is not recommended because using this dtype will cause numpy to remove all trailing zeros from each array element. As a result, binary sequences ending in zero(s) will not be represented correctly.&lt;/p&gt; &#xA;&lt;p&gt;BYTES tensors are demonstrated in the C++ example applications simple_http_string_infer_client.cc and simple_grpc_string_infer_client.cc. String tensors are demonstrated in the Python example application simple_http_string_infer_client.py and simple_grpc_string_infer_client.py.&lt;/p&gt; &#xA;&lt;h3&gt;System Shared Memory&lt;/h3&gt; &#xA;&lt;p&gt;Using system shared memory to communicate tensors between the client library and Triton can significantly improve performance in some cases.&lt;/p&gt; &#xA;&lt;p&gt;Using system shared memory is demonstrated in the C++ example applications simple_http_shm_client.cc and simple_grpc_shm_client.cc. Using system shared memory is demonstrated in the Python example application simple_http_shm_client.py and simple_grpc_shm_client.py.&lt;/p&gt; &#xA;&lt;p&gt;Python does not have a standard way of allocating and accessing shared memory so as an example a simple &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/utils/shared_memory&#34;&gt;system shared memory module&lt;/a&gt; is provided that can be used with the Python client library to create, set and destroy system shared memory.&lt;/p&gt; &#xA;&lt;h3&gt;CUDA Shared Memory&lt;/h3&gt; &#xA;&lt;p&gt;Using CUDA shared memory to communicate tensors between the client library and Triton can significantly improve performance in some cases.&lt;/p&gt; &#xA;&lt;p&gt;Using CUDA shared memory is demonstrated in the C++ example applications simple_http_cudashm_client.cc and simple_grpc_cudashm_client.cc. Using CUDA shared memory is demonstrated in the Python example application simple_http_cudashm_client.py and simple_grpc_cudashm_client.py.&lt;/p&gt; &#xA;&lt;p&gt;Python does not have a standard way of allocating and accessing shared memory so as an example a simple &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/library/tritonclient/utils/cuda_shared_memory&#34;&gt;CUDA shared memory module&lt;/a&gt; is provided that can be used with the Python client library to create, set and destroy CUDA shared memory.&lt;/p&gt; &#xA;&lt;h3&gt;Client API for Stateful Models&lt;/h3&gt; &#xA;&lt;p&gt;When performing inference using a &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/user_guide/architecture.md#stateful-models&#34;&gt;stateful model&lt;/a&gt;, a client must identify which inference requests belong to the same sequence and also when a sequence starts and ends.&lt;/p&gt; &#xA;&lt;p&gt;Each sequence is identified with a sequence ID that is provided when an inference request is made. It is up to the clients to create a unique sequence ID. For each sequence the first inference request should be marked as the start of the sequence and the last inference requests should be marked as the end of the sequence.&lt;/p&gt; &#xA;&lt;p&gt;The use of sequence ID and start and end flags are demonstrated in the C++ example applications simple_http_sequence_stream_infer_client.cc and simple_grpc_sequence_stream_infer_client.cc. The use of sequence ID and start and end flags are demonstrated in the Python example application simple_http_sequence_stream_infer_client.py and simple_grpc_sequence_stream_infer_client.py.&lt;/p&gt; &#xA;&lt;h2&gt;Image Classification Example&lt;/h2&gt; &#xA;&lt;p&gt;The image classification example that uses the C++ client API is available at &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/c%2B%2B/examples/image_client.cc&#34;&gt;src/c++/examples/image_client.cc&lt;/a&gt;. The Python version of the image classification client is available at &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/image_client.py&#34;&gt;src/python/examples/image_client.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use image_client (or image_client.py) you must first have a running Triton that is serving one or more image classification models. The image_client application requires that the model have a single image input and produce a single classification output. If you don&#39;t have a model repository with image classification models see &lt;a href=&#34;https://github.com/triton-inference-server/server/raw/main/docs/getting_started/quickstart.md&#34;&gt;QuickStart&lt;/a&gt; for instructions on how to create one.&lt;/p&gt; &#xA;&lt;p&gt;Once Triton is running you can use the image_client application to send inference requests. You can specify a single image or a directory holding images. Here we send a request for the inception_graphdef model for an image from the &lt;a href=&#34;https://github.com/triton-inference-server/server/tree/main/qa/images&#34;&gt;qa/images&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ image_client -m inception_graphdef -s INCEPTION qa/images/mug.jpg&#xA;Request 0, batch size 1&#xA;Image &#39;qa/images/mug.jpg&#39;:&#xA;    0.754130 (505) = COFFEE MUG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Python version of the application accepts the same command-line arguments.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python image_client.py -m inception_graphdef -s INCEPTION qa/images/mug.jpg&#xA;Request 0, batch size 1&#xA;Image &#39;qa/images/mug.jpg&#39;:&#xA;     0.826384 (505) = COFFEE MUG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The image_client and image_client.py applications use the client libraries to talk to Triton. By default image_client instructs the client library to use HTTP/REST protocol, but you can use the GRPC protocol by providing the -i flag. You must also use the -u flag to point at the GRPC endpoint on Triton.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ image_client -i grpc -u localhost:8001 -m inception_graphdef -s INCEPTION qa/images/mug.jpg&#xA;Request 0, batch size 1&#xA;Image &#39;qa/images/mug.jpg&#39;:&#xA;    0.754130 (505) = COFFEE MUG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default the client prints the most probable classification for the image. Use the -c flag to see more classifications.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ image_client -m inception_graphdef -s INCEPTION -c 3 qa/images/mug.jpg&#xA;Request 0, batch size 1&#xA;Image &#39;qa/images/mug.jpg&#39;:&#xA;    0.754130 (505) = COFFEE MUG&#xA;    0.157077 (969) = CUP&#xA;    0.002880 (968) = ESPRESSO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The -b flag allows you to send a batch of images for inferencing. The image_client application will form the batch from the image or images that you specified. If the batch is bigger than the number of images then image_client will just repeat the images to fill the batch.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ image_client -m inception_graphdef -s INCEPTION -c 3 -b 2 qa/images/mug.jpg&#xA;Request 0, batch size 2&#xA;Image &#39;qa/images/mug.jpg&#39;:&#xA;    0.754130 (505) = COFFEE MUG&#xA;    0.157077 (969) = CUP&#xA;    0.002880 (968) = ESPRESSO&#xA;Image &#39;qa/images/mug.jpg&#39;:&#xA;    0.754130 (505) = COFFEE MUG&#xA;    0.157077 (969) = CUP&#xA;    0.002880 (968) = ESPRESSO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Provide a directory instead of a single image to perform inferencing on all images in the directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ image_client -m inception_graphdef -s INCEPTION -c 3 -b 2 qa/images&#xA;Request 0, batch size 2&#xA;Image &#39;/opt/tritonserver/qa/images/car.jpg&#39;:&#xA;    0.819196 (818) = SPORTS CAR&#xA;    0.033457 (437) = BEACH WAGON&#xA;    0.031232 (480) = CAR WHEEL&#xA;Image &#39;/opt/tritonserver/qa/images/mug.jpg&#39;:&#xA;    0.754130 (505) = COFFEE MUG&#xA;    0.157077 (969) = CUP&#xA;    0.002880 (968) = ESPRESSO&#xA;Request 1, batch size 2&#xA;Image &#39;/opt/tritonserver/qa/images/vulture.jpeg&#39;:&#xA;    0.977632 (24) = VULTURE&#xA;    0.000613 (9) = HEN&#xA;    0.000560 (137) = EUROPEAN GALLINULE&#xA;Image &#39;/opt/tritonserver/qa/images/car.jpg&#39;:&#xA;    0.819196 (818) = SPORTS CAR&#xA;    0.033457 (437) = BEACH WAGON&#xA;    0.031232 (480) = CAR WHEEL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/triton-inference-server/client/main/src/python/examples/grpc_image_client.py&#34;&gt;grpc_image_client.py&lt;/a&gt; application behaves the same as the image_client except that instead of using the client library it uses the GRPC generated library to communicate with Triton.&lt;/p&gt; &#xA;&lt;h2&gt;Ensemble Image Classification Example Application&lt;/h2&gt; &#xA;&lt;p&gt;In comparison to the image classification example above, this example uses an ensemble of an image-preprocessing model implemented as a &lt;a href=&#34;https://github.com/triton-inference-server/dali_backend&#34;&gt;DALI backend&lt;/a&gt; and a TensorFlow Inception model. The ensemble model allows you to send the raw image binaries in the request and receive classification results without preprocessing the images on the client.&lt;/p&gt; &#xA;&lt;p&gt;To try this example you should follow the &lt;a href=&#34;https://github.com/triton-inference-server/dali_backend/tree/main/docs/examples/inception_ensemble&#34;&gt;DALI ensemble example instructions&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>