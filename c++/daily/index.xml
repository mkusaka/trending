<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-24T01:22:18Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google/gemma.cpp</title>
    <updated>2024-02-24T01:22:18Z</updated>
    <id>tag:github.com,2024-02-24:/google/gemma.cpp</id>
    <link href="https://github.com/google/gemma.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;lightweight, standalone C++ inference engine for Google&#39;s Gemma models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gemma.cpp&lt;/h1&gt; &#xA;&lt;p&gt;gemma.cpp is a lightweight, standalone C++ inference engine for the Gemma foundation models from Google.&lt;/p&gt; &#xA;&lt;p&gt;For additional information about Gemma, see &lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;ai.google.dev/gemma&lt;/a&gt;. Model weights, including gemma.cpp specific artifacts, are &lt;a href=&#34;https://www.kaggle.com/models/google/gemma&#34;&gt;available on kaggle&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Who is this project for?&lt;/h2&gt; &#xA;&lt;p&gt;Modern LLM inference engines are sophisticated systems, often with bespoke capabilities extending beyond traditional neural network runtimes. With this comes opportunities for research and innovation through co-design of high level algorithms and low-level computation. However, there is a gap between deployment-oriented C++ inference runtimes, which are not designed for experimentation, and Python-centric ML research frameworks, which abstract away low-level computation through compilation.&lt;/p&gt; &#xA;&lt;p&gt;gemma.cpp provides a minimalist implementation of Gemma 2B and 7B models, focusing on simplicity and directness rather than full generality. This is inspired by vertically-integrated model implementations such as &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;, &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama.c&lt;/a&gt;, and &lt;a href=&#34;https://github.com/srush/llama2.rs&#34;&gt;llama.rs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;gemma.cpp targets experimentation and research use cases. It is intended to be straightforward to embed in other projects with minimal dependencies and also easily modifiable with a small ~2K LoC core implementation (along with ~4K LoC of supporting utilities). We use the &lt;a href=&#34;https://github.com/google/highway&#34;&gt;Google Highway&lt;/a&gt; Library to take advantage of portable SIMD for CPU inference.&lt;/p&gt; &#xA;&lt;p&gt;For production-oriented edge deployments we recommend standard deployment pathways using Python frameworks like JAX, Keras, PyTorch, and Transformers (&lt;a href=&#34;https://www.kaggle.com/models/google/gemma&#34;&gt;all model variations here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Community contributions large and small are welcome. This project follows &lt;a href=&#34;https://opensource.google.com/conduct/&#34;&gt;Google&#39;s Open Source Community Guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Active development is currently done on the &lt;code&gt;dev&lt;/code&gt; branch. Please open pull requests targeting &lt;code&gt;dev&lt;/code&gt; branch instead of &lt;code&gt;main&lt;/code&gt;, which is intended to be more stable.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;System requirements&lt;/h3&gt; &#xA;&lt;p&gt;Before starting, you should have installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clang.llvm.org/get_started.html&#34;&gt;Clang C++ compiler&lt;/a&gt;, supporting at least C++17.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tar&lt;/code&gt; for extracting archives from Kaggle.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 1: Obtain model weights and tokenizer from Kaggle&lt;/h3&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://www.kaggle.com/models/google/gemma&#34;&gt;the Gemma model page on Kaggle&lt;/a&gt; and select &lt;code&gt;Model Variations |&amp;gt; Gemma C++&lt;/code&gt;. On this tab, the &lt;code&gt;Variation&lt;/code&gt; dropdown includes the options below. Note bfloat16 weights are higher fidelity, while 8-bit switched floating point weights enable faster inference. In general, we recommend starting with the &lt;code&gt;-sfp&lt;/code&gt; checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;2B instruction-tuned (&lt;code&gt;it&lt;/code&gt;) and pre-trained (&lt;code&gt;pt&lt;/code&gt;) models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;2b-it&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 billion parameter instruction-tuned model, bfloat16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;2b-it-sfp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 billion parameter instruction-tuned model, 8-bit switched floating point&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;2b-pt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 billion parameter pre-trained model, bfloat16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;2b-pt-sfp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 billion parameter pre-trained model, 8-bit switched floating point&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;7B instruction-tuned (&lt;code&gt;it&lt;/code&gt;) and pre-trained (&lt;code&gt;pt&lt;/code&gt;) models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;7b-it&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7 billion parameter instruction-tuned model, bfloat16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;7b-it-sfp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7 billion parameter instruction-tuned model, 8-bit switched floating point&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;7b-pt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7 billion parameter pre-trained model, bfloat16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;7b-pt-sfp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7 billion parameter pre-trained model, 8-bit switched floating point&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &lt;strong&gt;Important&lt;/strong&gt;: We strongly recommend starting off with the &lt;code&gt;2b-it-sfp&lt;/code&gt; model to get up and running.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 2: Extract Files&lt;/h3&gt; &#xA;&lt;p&gt;After filling out the consent form, the download should proceed to retrieve a tar archive file &lt;code&gt;archive.tar.gz&lt;/code&gt;. Extract files from &lt;code&gt;archive.tar.gz&lt;/code&gt; (this can take a few minutes):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tar -xf archive.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should produce a file containing model weights such as &lt;code&gt;2b-it-sfp.sbs&lt;/code&gt; and a tokenizer file (&lt;code&gt;tokenizer.spm&lt;/code&gt;). You may want to move these files to a convenient directory location (e.g. the &lt;code&gt;build/&lt;/code&gt; directory in this repo).&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Build&lt;/h3&gt; &#xA;&lt;p&gt;The build system uses &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt;. To build the gemma inference runtime, create a build directory and generate the build files using &lt;code&gt;cmake&lt;/code&gt; from the top-level project directory. For the 8-bit switched floating point weights (sfp), run cmake with no options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake -B build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; if you downloaded bfloat16 weights (any model &lt;em&gt;without&lt;/em&gt; &lt;code&gt;-sfp&lt;/code&gt; in the name), instead of running cmake with no options as above, run cmake with WEIGHT_TYPE set to &lt;a href=&#34;https://github.com/google/highway&#34;&gt;highway&#39;s&lt;/a&gt; &lt;code&gt;hwy::bfloat16_t&lt;/code&gt; type (this will be simplified in the future, we recommend using &lt;code&gt;-sfp&lt;/code&gt; weights instead of bfloat16 for faster inference):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake -B build -DWEIGHT_TYPE=hwy::bfloat16_t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running whichever of the above &lt;code&gt;cmake&lt;/code&gt; invocations that is appropriate for your weights, you can enter the &lt;code&gt;build/&lt;/code&gt; directory and run &lt;code&gt;make&lt;/code&gt; to build the &lt;code&gt;./gemma&lt;/code&gt; executable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd build&#xA;make -j [number of parallel threads to use] gemma&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;[number of parallel threads to use]&lt;/code&gt; with a number - the number of cores available on your system is a reasonable heuristic.&lt;/p&gt; &#xA;&lt;p&gt;For example, &lt;code&gt;make -j4 gemma&lt;/code&gt; will build using 4 threads. If this is successful, you should now have a &lt;code&gt;gemma&lt;/code&gt; executable in the &lt;code&gt;build/&lt;/code&gt; directory. If the &lt;code&gt;nproc&lt;/code&gt; command is available, you can use &lt;code&gt;make -j$(nproc) gemma&lt;/code&gt; as a reasonable default for the number of threads.&lt;/p&gt; &#xA;&lt;p&gt;If you aren&#39;t sure of the right value for the &lt;code&gt;-j&lt;/code&gt; flag, you can simply run &lt;code&gt;make gemma&lt;/code&gt; instead and it should still build the &lt;code&gt;./gemma&lt;/code&gt; executable.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] On Windows Subsystem for Linux (WSL) users should set the number of parallel threads to 1. Using a larger number may result in errors.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 4: Run&lt;/h3&gt; &#xA;&lt;p&gt;You can now run &lt;code&gt;gemma&lt;/code&gt; from inside the &lt;code&gt;build/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;gemma&lt;/code&gt; has the following required arguments:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Argument&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The model type.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2b-it&lt;/code&gt;, &lt;code&gt;2b-pt&lt;/code&gt;, &lt;code&gt;7b-it&lt;/code&gt;, &lt;code&gt;7b-pt&lt;/code&gt;, ... (see above)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compressed_weights&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The compressed weights file.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2b-it-sfp.sbs&lt;/code&gt;, ... (see above)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--tokenizer&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The tokenizer file.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tokenizer.spm&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;gemma&lt;/code&gt; is invoked as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gemma \&#xA;--tokenizer [tokenizer file] \&#xA;--compressed_weights [compressed weights file] \&#xA;--model [2b-it or 2b-pt or 7b-it or 7b-pt or ...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example invocation for the following configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compressed weights file &lt;code&gt;2b-it-sfp.sbs&lt;/code&gt; (2B instruction-tuned model, 8-bit switched floating point).&lt;/li&gt; &#xA; &lt;li&gt;Tokenizer file &lt;code&gt;tokenizer.spm&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gemma \&#xA;--tokenizer tokenizer.spm \&#xA;--compressed_weights 2b-it-sfp.sbs \&#xA;--model 2b-it&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Troubleshooting and FAQs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running &lt;code&gt;./gemma&lt;/code&gt; fails with &#34;Failed to read cache gating_ein_0 (error 294) ...&#34;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The most common problem is that &lt;code&gt;cmake&lt;/code&gt; was built with the wrong weight type and &lt;code&gt;gemma&lt;/code&gt; is attempting to load &lt;code&gt;bfloat16&lt;/code&gt; weights (&lt;code&gt;2b-it&lt;/code&gt;, &lt;code&gt;2b-pt&lt;/code&gt;, &lt;code&gt;7b-it&lt;/code&gt;, &lt;code&gt;7b-pt&lt;/code&gt;) using the default switched floating point (sfp) or vice versa. Revisit step #3 and check that the &lt;code&gt;cmake&lt;/code&gt; command used to build &lt;code&gt;gemma&lt;/code&gt; was correct for the weights that you downloaded.&lt;/p&gt; &#xA;&lt;p&gt;In the future we will handle model format handling from compile time to runtime to simplify this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Problems building in Windows / Visual Studio&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently if you&#39;re using Windows, we recommend building in WSL (Windows Subsystem for Linux). We are exploring options to enable other build configurations, see issues for active discussion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Model does not respond to instructions and produces strange output&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A common issue is that you are using a pre-trained model, which is not instruction-tuned and thus does not respond to instructions. Make sure you are using an instruction-tuned model (&lt;code&gt;2b-it-sfp&lt;/code&gt;, &lt;code&gt;2b-it&lt;/code&gt;, &lt;code&gt;7b-it-sfp&lt;/code&gt;, &lt;code&gt;7b-it&lt;/code&gt;) and not a pre-trained model (any model with a &lt;code&gt;-pt&lt;/code&gt; suffix).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I convert my fine-tune to a &lt;code&gt;.sbs&lt;/code&gt; compressed model file?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re working on a python script to convert a standard model format to &lt;code&gt;.sbs&lt;/code&gt;, and hope have it available in the next week or so. Follow &lt;a href=&#34;https://github.com/google/gemma.cpp/issues/11&#34;&gt;this issue&lt;/a&gt; for updates.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;gemma&lt;/code&gt; has different usage modes, controlled by the verbosity flag.&lt;/p&gt; &#xA;&lt;p&gt;All usage modes are currently interactive, triggering text generation upon newline input.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Verbosity&lt;/th&gt; &#xA;   &lt;th&gt;Usage mode&lt;/th&gt; &#xA;   &lt;th&gt;Details&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbosity 0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal&lt;/td&gt; &#xA;   &lt;td&gt;Only prints generation output. Suitable as a CLI tool.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbosity 1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Default&lt;/td&gt; &#xA;   &lt;td&gt;Standard user-facing terminal UI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbosity 2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Detailed&lt;/td&gt; &#xA;   &lt;td&gt;Shows additional developer and debug info.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Interactive Terminal App&lt;/h3&gt; &#xA;&lt;p&gt;By default, verbosity is set to 1, bringing up a terminal-based interactive interface when &lt;code&gt;gemma&lt;/code&gt; is invoked:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ ./gemma [...]&#xA;  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __&#xA; / _` |/ _ \ &#39;_ ` _ \| &#39;_ ` _ \ / _` | / __| &#39;_ \| &#39;_ \&#xA;| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |&#xA; \__, |\___|_| |_| |_|_| |_| |_|\__,_(_)___| .__/| .__/&#xA;  __/ |                                    | |   | |&#xA; |___/                                     |_|   |_|&#xA;&#xA;tokenizer                     : tokenizer.spm&#xA;compressed_weights            : 2b-it-sfp.sbs&#xA;model                         : 2b-it&#xA;weights                       : [no path specified]&#xA;max_tokens                    : 3072&#xA;max_generated_tokens          : 2048&#xA;&#xA;*Usage*&#xA;  Enter an instruction and press enter (%Q quits).&#xA;&#xA;*Examples*&#xA;  - Write an email to grandma thanking her for the cookies.&#xA;  - What are some historical attractions to visit around Massachusetts?&#xA;  - Compute the nth fibonacci number in javascript.&#xA;  - Write a standup comedy bit about WebGPU programming.&#xA;&#xA;&amp;gt; What are some outdoorsy places to visit around Boston?&#xA;&#xA;[ Reading prompt ] .....................&#xA;&#xA;&#xA;**Boston Harbor and Islands:**&#xA;&#xA;* **Boston Harbor Islands National and State Park:** Explore pristine beaches, wildlife, and maritime history.&#xA;* **Charles River Esplanade:** Enjoy scenic views of the harbor and city skyline.&#xA;* **Boston Harbor Cruise Company:** Take a relaxing harbor cruise and admire the city from a different perspective.&#xA;* **Seaport Village:** Visit a charming waterfront area with shops, restaurants, and a seaport museum.&#xA;&#xA;**Forest and Nature:**&#xA;&#xA;* **Forest Park:** Hike through a scenic forest with diverse wildlife.&#xA;* **Quabbin Reservoir:** Enjoy boating, fishing, and hiking in a scenic setting.&#xA;* **Mount Forest:** Explore a mountain with breathtaking views of the city and surrounding landscape.&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a Command Line Tool&lt;/h3&gt; &#xA;&lt;p&gt;For using the &lt;code&gt;gemma&lt;/code&gt; executable as a command line tool, it may be useful to create an alias for gemma.cpp with arguments fully specified:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;alias gemma2b=&#34;~/gemma.cpp/build/gemma -- --tokenizer ~/gemma.cpp/build/tokenizer.spm --compressed_weights ~/gemma.cpp/build/2b-it-sfp.sbs --model 2b-it --verbosity 0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace the above paths with your own paths to the model and tokenizer paths from the download.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of prompting &lt;code&gt;gemma&lt;/code&gt; with a truncated input file (using a &lt;code&gt;gemma2b&lt;/code&gt; alias like defined above):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cat configs.h | tail -35 | tr &#39;\n&#39; &#39; &#39; | xargs -0 echo &#34;What does this C++ code do: &#34; | gemma2b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] CLI usage of gemma.cpp is experimental and should take context length limitations into account.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The output of the above command should look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ cat configs.h | tail -35 | tr &#39;\n&#39; &#39; &#39; | xargs -0 echo &#34;What does this C++ code do: &#34; | gemma2b&#xA;[ Reading prompt ] ......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................&#xA;The code defines two C++ structs, `ConfigGemma7B` and `ConfigGemma2B`, which are used for configuring a deep learning model.&#xA;&#xA;**ConfigGemma7B**:&#xA;&#xA;* `seq_len`: Stores the length of the sequence to be processed. It&#39;s set to 7168.&#xA;* `vocab_size`: Stores the size of the vocabulary, which is 256128.&#xA;* `n_layers`: Number of layers in the deep learning model. It&#39;s set to 28.&#xA;* `dim_model`: Dimension of the model&#39;s internal representation. It&#39;s set to 3072.&#xA;* `dim_ffw_hidden`: Dimension of the feedforward and recurrent layers&#39; hidden representations. It&#39;s set to 16 * 3072 / 2.&#xA;&#xA;**ConfigGemma2B**:&#xA;&#xA;* `seq_len`: Stores the length of the sequence to be processed. It&#39;s also set to 7168.&#xA;* `vocab_size`: Size of the vocabulary, which is 256128.&#xA;* `n_layers`: Number of layers in the deep learning model. It&#39;s set to 18.&#xA;* `dim_model`: Dimension of the model&#39;s internal representation. It&#39;s set to 2048.&#xA;* `dim_ffw_hidden`: Dimension of the feedforward and recurrent layers&#39; hidden representations. It&#39;s set to 16 * 2048 / 2.&#xA;&#xA;These structs are used to configure a deep learning model with specific parameters for either Gemma7B or Gemma2B architecture.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Incorporating gemma.cpp as a Library in your Project&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to incorporate gemma.cpp in your own project is to pull in gemma.cpp and dependencies using &lt;code&gt;FetchContent&lt;/code&gt;. You can add the following to your CMakeLists.txt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;include(FetchContent)&#xA;&#xA;FetchContent_Declare(sentencepiece GIT_REPOSITORY https://github.com/google/sentencepiece GIT_TAG 53de76561cfc149d3c01037f0595669ad32a5e7c)&#xA;FetchContent_MakeAvailable(sentencepiece)&#xA;&#xA;FetchContent_Declare(gemma GIT_REPOSITORY https://github.com/google/gemma.cpp GIT_TAG origin/main)&#xA;FetchContent_MakeAvailable(gemma)&#xA;&#xA;FetchContent_Declare(highway GIT_REPOSITORY https://github.com/google/highway.git GIT_TAG da250571a45826b21eebbddc1e50d0c1137dee5f)&#xA;FetchContent_MakeAvailable(highway)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note for the gemma.cpp &lt;code&gt;GIT_TAG&lt;/code&gt;, you may replace &lt;code&gt;origin/main&lt;/code&gt; for a specific commit hash if you would like to pin the library version.&lt;/p&gt; &#xA;&lt;p&gt;After your executable is defined (substitute your executable name for &lt;code&gt;[Executable Name]&lt;/code&gt; below):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;target_link_libraries([Executable Name] libgemma hwy hwy_contrib sentencepiece)&#xA;FetchContent_GetProperties(gemma)&#xA;FetchContent_GetProperties(sentencepiece)&#xA;target_include_directories([Executable Name] PRIVATE ${gemma_SOURCE_DIR})&#xA;target_include_directories([Executable Name] PRIVATE ${sentencepiece_SOURCE_DIR})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building gemma.cpp as a Library&lt;/h3&gt; &#xA;&lt;p&gt;gemma.cpp can also be used as a library dependency in your own project. The shared library artifact can be built by modifying the make invocation to build the &lt;code&gt;libgemma&lt;/code&gt; target instead of &lt;code&gt;gemma&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] If you are using gemma.cpp in your own project with the &lt;code&gt;FetchContent&lt;/code&gt; steps in the previous section, building the library is done automatically by &lt;code&gt;cmake&lt;/code&gt; and this section can be skipped.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, run &lt;code&gt;cmake&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake -B build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run &lt;code&gt;make&lt;/code&gt; with the &lt;code&gt;libgemma&lt;/code&gt; target:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd build&#xA;make -j [number of parallel threads to use] libgemma&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If this is successful, you should now have a &lt;code&gt;libgemma&lt;/code&gt; library file in the &lt;code&gt;build/&lt;/code&gt; directory. On Unix platforms, the filename is &lt;code&gt;libgemma.a&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements and Contacts&lt;/h2&gt; &#xA;&lt;p&gt;gemma.cpp was started in fall 2023 by &lt;a href=&#34;mailto:austinvhuang@google.com&#34;&gt;Austin Huang&lt;/a&gt; and &lt;a href=&#34;mailto:janwas@google.com&#34;&gt;Jan Wassenberg&lt;/a&gt;, and subsequently released February 2024 thanks to contributions from Phil Culliton, Paul Chang, and Dan Zheng.&lt;/p&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt;</summary>
  </entry>
</feed>