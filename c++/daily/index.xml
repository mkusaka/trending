<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-02T01:25:07Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ckcr4lyf/EvilAppleJuice-ESP32</title>
    <updated>2023-11-02T01:25:07Z</updated>
    <id>tag:github.com,2023-11-02:/ckcr4lyf/EvilAppleJuice-ESP32</id>
    <link href="https://github.com/ckcr4lyf/EvilAppleJuice-ESP32" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spam Apple Proximity Messages via an ESP32&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EvilAppleJuice ESP32&lt;/h1&gt; &#xA;&lt;p&gt;Spam BLE advertisements on iPhones!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;iPhone 15s (latest)&lt;/th&gt; &#xA;   &lt;th&gt;Older iPhones&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;video controls width=&#34;250&#34; src=&#34;https://user-images.githubusercontent.com/6680615/274864225-53ed6d7c-0569-4f22-b55b-bc9973c4bc93.mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;video controls width=&#34;250&#34; src=&#34;https://user-images.githubusercontent.com/6680615/274864287-c6e871fd-9fdf-4507-ae21-a566beead5cc.mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Based off of the work of &lt;a href=&#34;https://github.com/ronaldstoner&#34;&gt;ronaldstoner&lt;/a&gt; in the &lt;a href=&#34;https://github.com/ECTO-1A/AppleJuice/raw/e6a61f6a199075f5bb5b1a00768e317571d25bb9/ESP32-Arduino/applejuice.ino&#34;&gt;AppleJuice repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With the randomization optimizations it can render an iPhone almost useless with a single ESP32 (a new notification as soon as you close the old one).&lt;/p&gt; &#xA;&lt;p&gt;Confirmed on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iPhone 14 Pro (running iOS 16.6.1)&lt;/li&gt; &#xA; &lt;li&gt;iPhone 13 Pro (TBD which iOS)&lt;/li&gt; &#xA; &lt;li&gt;iPhone 11 (running iOS 16.6.1)&lt;/li&gt; &#xA; &lt;li&gt;iPhone X (running iOS 14.8 (18H17)) - only &#34;AppleTV Keyboard&#34;, &#34;TV Color Balance&#34;, &#34;AppleTV Setup&#34;, &#34;AppleTV Homekit Setup&#34;, &#34;AppleTV New User&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Not working on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iPhone 4S (running iOS 10.3 (14E277))&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other observations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Doesn&#39;t seem to spawn notifications if Keyboard is open / Camera is open&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Demo&lt;/h3&gt; &#xA;&lt;p&gt;Single ESP32 vs. iPhone 14 Pro @ iOS 16.6.1&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ECTO-1A/AppleJuice/assets/6680615/47466ed6-03c9-43b2-a0d0-aac2e2aaa228&#34;&gt;https://github.com/ECTO-1A/AppleJuice/assets/6680615/47466ed6-03c9-43b2-a0d0-aac2e2aaa228&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notable Differences&lt;/h2&gt; &#xA;&lt;p&gt;This implementation makes the following changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Random source MAC address (including &lt;code&gt;BLE_ADDR_TYPE_RANDOM&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Randomly pick BLE Advertisement Type (&lt;a href=&#34;https://github.com/ECTO-1A/AppleJuice/pull/25&#34;&gt;this may lead to more success&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Randomly pick one of the possible devices&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And it makes these random choices every time it runs (default re-advertise every second).&lt;/p&gt; &#xA;&lt;p&gt;Given the 29 devices and the 3 advertisement types, there are a total of 87 unique possible advertisements (ignoring the random source MAC) possible, of which one is broadcast every second.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repo, and easiest would be to use VS Code w/ PlatformIO to upload it to your ESP32.&lt;/p&gt; &#xA;&lt;p&gt;This project has been tested on an &lt;a href=&#34;https://wiki.luatos.com/chips/esp32c3/board.html&#34;&gt;ESP32-C3 from AirM2M&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>intel/intel-extension-for-transformers</title>
    <updated>2023-11-02T01:25:07Z</updated>
    <id>tag:github.com,2023-11-02:/intel/intel-extension-for-transformers</id>
    <link href="https://github.com/intel/intel-extension-for-transformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ö° Build your chatbot within minutes on your favorite device; offer SOTA compression techniques for LLMs; run LLMs efficiently on Intel Platforms‚ö°&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Intel¬Æ Extension for Transformers&lt;/h1&gt; &#xA; &lt;h3&gt;An Innovative Transformer-based Toolkit to Accelerate GenAI/LLM Everywhere&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/Wxk3J3ZJkU&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/Wxk3J3ZJkU?compact=true&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/intel/intel-extension-for-transformers&#34; alt=&#34;Release Notes&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/architecture.md&#34;&gt;üè≠Architecture&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat&#34;&gt;üí¨NeuralChat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;üòÉInference&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/examples.md&#34;&gt;üíªExamples&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html&#34;&gt;üìñDocumentations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üöÄLatest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/10] LLM runtime, an Intel-optimized &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; compatible runtime, demonstrates &lt;strong&gt;up to 15x performance gain in 1st token generation and 1.5x in other token generation&lt;/strong&gt; over the default &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/10] LLM runtime now supports LLM inference with &lt;strong&gt;infinite-length inputs up to 4 million tokens&lt;/strong&gt;, inspired from &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;StreamingLLM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/09] NeuralChat has been showcased in &lt;a href=&#34;https://www.youtube.com/watch?v=RbKRELWP9y8&amp;amp;t=2954s&#34;&gt;&lt;strong&gt;Intel Innovation‚Äô23 Keynote&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next-23&#34;&gt;Google Cloud Next&#39;23&lt;/a&gt; to demonstrate GenAI/LLM capabilities on Intel Xeon Scalable Processors.&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] NeuralChat supports &lt;strong&gt;custom chatbot development and deployment within minutes&lt;/strong&gt; on broad Intel HWs such as Xeon Scalable Processors, Gaudi2,&amp;nbsp;Xeon CPU Max Series,&amp;nbsp;Data Center GPU Max Series, Arc Series, and Core Processors. Check out &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&#34;&gt;Notebooks&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07] LLM runtime extends Hugging Face Transformers API to provide seamless low precision inference for popular LLMs, supporting low precision data types such as INT3/INT4/FP4/NF4/INT5/INT8/FP8.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;h2&gt;üèÉInstallation&lt;/h2&gt; &#xA; &lt;h3&gt;Quick Install from Pypi&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install intel-extension-for-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;For more installation methods, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/installation.md&#34;&gt;Installation Page&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h2&gt;üåüIntroduction&lt;/h2&gt; &#xA; &lt;p&gt;Intel¬Æ Extension for Transformers is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular, effective on 4th Intel Xeon Scalable processor&amp;nbsp;Sapphire Rapids (codenamed &lt;a href=&#34;https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html&#34;&gt;Sapphire Rapids&lt;/a&gt;). The toolkit provides the below key features and examples:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Seamless user experience of model compressions on Transformer-based models by extending &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face transformers&lt;/a&gt;&amp;nbsp;APIs and leveraging &lt;a href=&#34;https://github.com/intel/neural-compressor&#34;&gt;Intel¬Æ Neural Compressor&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Advanced software optimizations and unique compression-aware runtime (released with NeurIPS 2022&#39;s paper &lt;a href=&#34;https://arxiv.org/abs/2211.07715&#34;&gt;Fast Distilbert on CPUs&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2210.17114&#34;&gt;QuaLA-MiniLM: a Quantized Length Adaptive MiniLM&lt;/a&gt;, and NeurIPS 2021&#39;s paper &lt;a href=&#34;https://arxiv.org/abs/2111.05754&#34;&gt;Prune Once for All: Sparse Pre-Trained Language Models&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Optimized Transformer-based model packages such as &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/text-generation/deployment&#34;&gt;GPT-J-6B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/language-modeling/quantization#2-validated-model-list&#34;&gt;GPT-NEOX&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/language-modeling/inference#BLOOM-176B&#34;&gt;BLOOM-176B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/summarization/quantization#2-validated-model-list&#34;&gt;T5&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/summarization/quantization#2-validated-model-list&#34;&gt;Flan-T5&lt;/a&gt;, and end-to-end workflows such as &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb&#34;&gt;SetFit-based text classification&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/workflows/dlsa&#34;&gt;document level sentiment analysis (DLSA)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat&#34;&gt;NeuralChat&lt;/a&gt;, a customizable chatbot framework to create your own chatbot within minutes by leveraging a rich set of plugins &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md&#34;&gt;Knowledge Retrieval&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/audio/README.md&#34;&gt;Speech Interaction&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/caching/README.md&#34;&gt;Query Caching&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/security/README.md&#34;&gt;Security Guardrail&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;Inference&lt;/a&gt; of Large Language Model (LLM) in pure C/C++ with weight-only quantization kernels, supporting &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/gptneox&#34;&gt;GPT-NEOX&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/llama&#34;&gt;LLAMA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/mpt&#34;&gt;MPT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/falcon&#34;&gt;FALCON&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/bloom&#34;&gt;BLOOM-7B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/opt&#34;&gt;OPT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/chatglm&#34;&gt;ChatGLM2-6B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/gptj&#34;&gt;GPT-J-6B&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/gptneox&#34;&gt;Dolly-v2-3B&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;üå±Getting Started&lt;/h2&gt; &#xA; &lt;p&gt;Below is the sample code to enable the chatbot. See more &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Chatbot&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pip install intel-extension-for-transformers&#xA;from intel_extension_for_transformers.neural_chat import build_chatbot&#xA;chatbot = build_chatbot()&#xA;response = chatbot.predict(&#34;Tell me about Intel Xeon Scalable Processors.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Below is the sample code to enable weight-only INT4/INT8 inference. See more &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;INT4 Inference&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig&#xA;&#xA;model_name = &#34;Intel/neural-chat-7b-v1-1&#34;     # Hugging Face model_id or local model&#xA;config = WeightOnlyQuantConfig(compute_dtype=&#34;int8&#34;, weight_dtype=&#34;int4&#34;)&#xA;prompt = &#34;Once upon a time, a little girl&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&#xA;inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)&#xA;gen_tokens = model.generate(inputs, max_new_tokens=300)&#xA;outputs = tokenizer.batch_decode(gen_tokens)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;INT8 Inference&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig&#xA;&#xA;model_name = &#34;Intel/neural-chat-7b-v1-1&#34;     # Hugging Face model_id or local model&#xA;config = WeightOnlyQuantConfig(compute_dtype=&#34;bf16&#34;, weight_dtype=&#34;int8&#34;)&#xA;prompt = &#34;Once upon a time, a little girl&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&#xA;inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)&#xA;gen_tokens = model.generate(inputs, max_new_tokens=300)&#xA;outputs = tokenizer.batch_decode(gen_tokens)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h2&gt;üéØValidated Models&lt;/h2&gt; &#xA; &lt;p&gt;You can access the latest int4 performance and accuracy at &lt;a href=&#34;https://medium.com/@NeuralCompressor/llm-performance-of-intel-extension-for-transformers-f7d061556176&#34;&gt;int4 blog&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Additionally, we are preparing to introduce Baichuan, Mistral, and other models into &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;LLM Runtime (Intel Optimized llamacpp)&lt;/a&gt;. For comprehensive accuracy and performance data, though not the most up-to-date, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/release_data.md&#34;&gt;Release data&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h2&gt;üìñDocumentation&lt;/h2&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;OVERVIEW&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat&#34;&gt;NeuralChat&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;LLM Runtime&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;NEURALCHAT&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_spr.ipynb&#34;&gt;Chatbot on Intel CPU&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_xpu.ipynb&#34;&gt;Chatbot on Intel GPU&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_habana_gaudi.ipynb&#34;&gt;Chatbot on Gaudi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/examples/talkingbot_pc/build_talkingbot_on_pc.ipynb&#34;&gt;Chatbot on Client&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&#34;&gt;More Notebooks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;LLM RUNTIME&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/README.md&#34;&gt;LLM Runtime&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/README.md#2-run-llm-with-python-api&#34;&gt;Streaming LLM&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/core/README.md&#34;&gt;Low Precision Kernels&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/tensor_parallelism.md&#34;&gt;Tensor Parallelism&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;LLM COMPRESSION&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/smoothquant.md&#34;&gt;SmoothQuant (INT8)&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/weightonlyquant.md&#34;&gt;Weight-only Quantization (INT4/FP4/NF4/INT8)&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/qloracpu.md&#34;&gt;QLoRA on CPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;GENERAL COMPRESSION&lt;/th&gt; &#xA;   &lt;/tr&gt;&#xA;   &lt;tr&gt; &#xA;   &lt;/tr&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/quantization.md&#34;&gt;Quantization&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/pruning.md&#34;&gt;Pruning&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/distillation.md&#34;&gt;Distillation&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/text-classification/orchestrate_optimizations/README.md&#34;&gt;Orchestration&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/language-modeling/nas/README.md&#34;&gt;Neural Architecture Search&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/export.md&#34;&gt;Export&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/metrics.md&#34;&gt;Metrics&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/objectives.md&#34;&gt;Objectives&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/pipeline.md&#34;&gt;Pipeline&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/question-answering/dynamic/README.md&#34;&gt;Length Adaptive&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/examples.md#early-exit&#34;&gt;Early Exit&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/data_augmentation.md&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;TUTORIALS &amp;amp; RESULTS&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/tutorials/pytorch&#34;&gt;Tutorials&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph#supported-models&#34;&gt;LLM List&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/examples.md&#34;&gt;General Model List&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.md&#34;&gt;Model Performance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h2&gt;üôåDemo&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Infinite inference (up to 4M tokens)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/assets/109187816/1698dcda-c9ec-4f44-b159-f4e9d67ab15b&#34;&gt;https://github.com/intel/intel-extension-for-transformers/assets/109187816/1698dcda-c9ec-4f44-b159-f4e9d67ab15b&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h2&gt;üìÉSelected Publications/Events&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Video on YouTube:&lt;a href=&#34;https://www.youtube.com/watch?v=KWT6yKfu4n0&#34;&gt;Build Your Own ChatBot with Neural Chat | Intel Software&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/@NeuralCompressor/high-performance-low-bit-layer-wise-weight-only-quantization-on-a-laptop-712580899396&#34;&gt;Layer-wise Low-bit Weight Only Quantization on a Laptop&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/@NeuralCompressor/llm-performance-of-intel-extension-for-transformers-f7d061556176&#34;&gt;Intel-Optimized Llama.CPP in Intel Extension for Transformers&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/intel-analytics-software/reduce-large-language-model-carbon-footprint-with-intel-neural-compressor-and-intel-extension-for-dfadec3af76a&#34;&gt;Reduce the Carbon Footprint of Large Language Models&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/intel-tech/empower-applications-with-optimized-llms-performance-cost-and-beyond-59c6e79cceb4&#34;&gt;Empower Applications with Optimized LLMs: Performance, Cost, and Beyond&lt;/a&gt; (Sep 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/@NeuralCompressor/neuralchat-simplifying-supervised-instruction-fine-tuning-and-reinforcement-aligning-for-chatbots-d034bca44f69&#34;&gt;NeuralChat: Simplifying Supervised Instruction Fine-tuning and Reinforcement Aligning for Chatbots&lt;/a&gt; (Sep 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Intel Innovation&#39;23 Keynote: &lt;a href=&#34;https://www.youtube.com/watch?v=RbKRELWP9y8&amp;amp;t=2954s&#34;&gt;Intel Innovation 2023 Keynote by Greg Lavender&lt;/a&gt; (Sep 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/intel-analytics-software/make-your-own-chatbot-within-a-few-minutes-with-neuralchat-a-customizable-chatbot-framework-139b4bdec8d1&#34;&gt;NeuralChat: A Customizable Chatbot Framework&lt;/a&gt; (Sep 2023)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;View &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/publication.md&#34;&gt;Full Publication List&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h2&gt;Additional Content&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/release.md&#34;&gt;Release Information&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/contributions.md&#34;&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/legal.md&#34;&gt;Legal Information&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/SECURITY.md&#34;&gt;Security Policy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Excellent open-source projects: &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;, &lt;a href=&#34;https://github.com/IntelLabs/fastRAG&#34;&gt;fastRAG&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;, &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;gptq&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evauation-harness&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;peft&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;trl&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;streamingllm&lt;/a&gt; and many others.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Thanks to all the &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/contributors.md&#34;&gt;contributors&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;üíÅCollaborations&lt;/h2&gt; &#xA; &lt;p&gt;Welcome to raise any interesting ideas on model compression techniques and LLM-based chatbot development! Feel free to reach &lt;a href=&#34;mailto:itrex.maintainers@intel.com&#34;&gt;us&lt;/a&gt;, and we look forward to our collaborations on Intel Extension for Transformers!&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>