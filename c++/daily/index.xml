<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-03T01:24:29Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>city-super/Octree-GS</title>
    <updated>2024-04-03T01:24:29Z</updated>
    <id>tag:github.com,2024-04-03:/city-super/Octree-GS</id>
    <link href="https://github.com/city-super/Octree-GS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;em&gt;Octree&lt;/em&gt;-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://city-super.github.io/octree-gs/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2403.17898&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1BEcAvM98HpchubODF249X3NGoKoC7SuQ/view?usp=sharing&#34;&gt;Viewers for Windows&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tongji-rkr&#34;&gt;Kerui Ren*&lt;/a&gt;, &lt;a href=&#34;https://jianglh-whu.github.io/&#34;&gt;Lihan Jiang*&lt;/a&gt;, &lt;a href=&#34;https://github.com/inspirelt&#34;&gt;Tao Lu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=w0Od3hQAAAAJ&#34;&gt;Mulin Yu&lt;/a&gt;, &lt;a href=&#34;https://eveneveno.github.io/lnxu&#34;&gt;Linning Xu&lt;/a&gt;, &lt;a href=&#34;https://eezkni.github.io/&#34;&gt;Zhangkai Ni&lt;/a&gt;, &lt;a href=&#34;https://daibo.info/&#34;&gt;Bo Dai&lt;/a&gt; ✉️ &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.04.01]&lt;/strong&gt; 🎈👀 The &lt;a href=&#34;https://github.com/city-super/Octree-GS/tree/main/SIBR_viewers&#34;&gt;viewer&lt;/a&gt; for Octree-GS is available now.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.04.01]&lt;/strong&gt; We release the code.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/city-super/Octree-GS/main/assets/pipeline.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; class=&#34;center&#34;&gt; &lt;/p&gt; Inspired by the Level-of-Detail (LOD) techniques, we introduce \modelname, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results. &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/city-super/Octree-GS/main/assets/teaser_big.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; class=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We tested on a server configured with Ubuntu 18.04, cuda 11.6 and gcc 9.4.0. Other similar configurations should also work, but we have not verified each one individually.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repo:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/city-super/Octree-GS --recursive&#xA;cd Octree-GS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;SET DISTUTILS_USE_SDK=1 # Windows only&#xA;conda env create --file environment.yml&#xA;conda activate octree_gs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;First, create a &lt;code&gt;data/&lt;/code&gt; folder inside the project path by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The data structure will be organised as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;data/&#xA;├── dataset_name&#xA;│&amp;nbsp;&amp;nbsp; ├── scene1/&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── images&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── IMG_0.jpg&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── IMG_1.jpg&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── ...&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── sparse/&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp;     └──0/&#xA;│&amp;nbsp;&amp;nbsp; ├── scene2/&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── images&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── IMG_0.jpg&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── IMG_1.jpg&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── ...&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── sparse/&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp;     └──0/&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Public Data&lt;/h3&gt; &#xA;&lt;p&gt;The MatrixCity dataset can be downloaded from &lt;a href=&#34;https://huggingface.co/datasets/BoDai/MatrixCity/tree/main&#34;&gt;Hugging Face&lt;/a&gt;/&lt;a href=&#34;https://openxlab.org.cn/datasets/bdaibdai/MatrixCity&#34;&gt;Openxlab&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/share/init?surl=87P0e5p1hz9t5mgdJXjL1g&#34;&gt;百度网盘[提取码:hqnn]&lt;/a&gt;. The BungeeNeRF dataset is available in &lt;a href=&#34;https://drive.google.com/file/d/1nBLcf9Jrr6sdxKa1Hbd47IArQQ_X8lww/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1AUYUJojhhICSKO2JrmOnCA&#34;&gt;百度网盘[提取码:4whv]&lt;/a&gt;. The MipNeRF360 scenes are provided by the paper author &lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;here&lt;/a&gt;. The SfM data sets for Tanks&amp;amp;Temples and Deep Blending are hosted by 3D-Gaussian-Splatting &lt;a href=&#34;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/datasets/input/tandt_db.zip&#34;&gt;here&lt;/a&gt;. Download and uncompress them into the &lt;code&gt;data/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Custom Data&lt;/h3&gt; &#xA;&lt;p&gt;For custom data, you should process the image sequences with &lt;a href=&#34;https://colmap.github.io/&#34;&gt;Colmap&lt;/a&gt; to obtain the SfM points and camera poses. Then, place the results into &lt;code&gt;data/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Training multiple scenes&lt;/h3&gt; &#xA;&lt;p&gt;To train multiple scenes in parallel, we provide batch training scripts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tanks&amp;amp;Temples: &lt;code&gt;train_tandt.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;MipNeRF360: &lt;code&gt;train_mipnerf360.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;BungeeNeRF: &lt;code&gt;train_bungeenerf.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deep Blending: &lt;code&gt;train_db.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;run them with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash train_xxx.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice 1: Make sure you have enough GPU cards and memories to run these scenes at the same time.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice 2: Each process occupies many cpu cores, which may slow down the training process. Set &lt;code&gt;torch.set_num_threads(32)&lt;/code&gt; accordingly in the &lt;code&gt;train.py&lt;/code&gt; to alleviate it.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Training a single scene&lt;/h3&gt; &#xA;&lt;p&gt;For training a single scene, modify the path and configurations in &lt;code&gt;single_train.sh&lt;/code&gt; accordingly and run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash single_train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;scene: scene name with a format of &lt;code&gt;dataset_name/scene_name/&lt;/code&gt; or &lt;code&gt;scene_name/&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;exp_name: user-defined experiment name;&lt;/li&gt; &#xA; &lt;li&gt;gpu: specify the GPU id to run the code. &#39;-1&#39; denotes using the most idle GPU.&lt;/li&gt; &#xA; &lt;li&gt;ratio: sampling interval of the SfM point cloud at initialization&lt;/li&gt; &#xA; &lt;li&gt;appearance_dim: dimensions of appearance embedding&lt;/li&gt; &#xA; &lt;li&gt;fork: proportion of subdivisions between LOD levels&lt;/li&gt; &#xA; &lt;li&gt;base_layer: the coarsest layer of the octree, corresponding to LOD 0, &#39;&amp;lt;0&#39; means scene-based setting&lt;/li&gt; &#xA; &lt;li&gt;visible_threshold: the threshold ratio of anchor points with low training frequency&lt;/li&gt; &#xA; &lt;li&gt;dist2level: the way floating-point values map to integers when estimating the LOD level&lt;/li&gt; &#xA; &lt;li&gt;update_ratio: the threshold ratio of anchor growing&lt;/li&gt; &#xA; &lt;li&gt;progressive: whether to use progressive learning&lt;/li&gt; &#xA; &lt;li&gt;levels: The number of LOD levels, &#39;&amp;lt;0&#39; means scene-based setting&lt;/li&gt; &#xA; &lt;li&gt;init_level: initial level of progressive learning&lt;/li&gt; &#xA; &lt;li&gt;extra_ratio: the threshold ratio of LOD bias&lt;/li&gt; &#xA; &lt;li&gt;extra_up: Increment of LOD bias per time&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For these public datasets, the configurations of &#39;voxel_size&#39; and &#39;fork&#39; can refer to the above batch training script.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This script will store the log (with running-time code) into &lt;code&gt;outputs/dataset_name/scene_name/exp_name/cur_time&lt;/code&gt; automatically.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve integrated the rendering and metrics calculation process into the training code. So, when completing training, the &lt;code&gt;rendering results&lt;/code&gt;, &lt;code&gt;fps&lt;/code&gt; and &lt;code&gt;quality metrics&lt;/code&gt; will be printed automatically. And the rendering results will be save in the log dir. Mind that the &lt;code&gt;fps&lt;/code&gt; is roughly estimated by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.cuda.synchronize();t_start=time.time()&#xA;rendering...&#xA;torch.cuda.synchronize();t_end=time.time()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which may differ somewhat from the original 3D-GS, but it does not affect the analysis.&lt;/p&gt; &#xA;&lt;p&gt;Meanwhile, we keep the manual rendering function with a similar usage of the counterpart in &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D-GS&lt;/a&gt;, one can run it by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python render.py -m &amp;lt;path to trained model&amp;gt; # Generate renderings&#xA;python metrics.py -m &amp;lt;path to trained model&amp;gt; # Compute error metrics on renderings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Viewer&lt;/h2&gt; &#xA;&lt;p&gt;The viewer for Octree-GS is available now.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kerui Ren: &lt;a href=&#34;mailto:renkerui@pjlab.org.cn&#34;&gt;renkerui@pjlab.org.cn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Lihan Jiang: &lt;a href=&#34;mailto:mr.lhjiang@gmail.com&#34;&gt;mr.lhjiang@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the LICENSE of &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D-GS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank all authors from &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D-GS&lt;/a&gt; and &lt;a href=&#34;https://github.com/city-super/Scaffold-GS&#34;&gt;Scaffold-GS&lt;/a&gt; for presenting such an excellent work.&lt;/p&gt;</summary>
  </entry>
</feed>