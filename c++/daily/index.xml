<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-07T01:30:42Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dusty-nv/jetson-inference</title>
    <updated>2022-08-07T01:30:42Z</updated>
    <id>tag:github.com,2022-08-07:/dusty-nv/jetson-inference</id>
    <link href="https://github.com/dusty-nv/jetson-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-header.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h1&gt;Deploying Deep Learning&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to our instructional guide for inference and realtime &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#api-reference&#34;&gt;DNN vision&lt;/a&gt; library for NVIDIA &lt;strong&gt;&lt;a href=&#34;http://www.nvidia.com/object/embedded-systems.html&#34;&gt;Jetson Nano/TX1/TX2/Xavier NX/AGX Xavier/AGX Orin&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repo uses NVIDIA &lt;strong&gt;&lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT&lt;/a&gt;&lt;/strong&gt; for efficiently deploying neural networks onto the embedded Jetson platform, improving performance and power efficiency using graph optimizations, kernel fusion, and FP16/INT8 precision.&lt;/p&gt; &#xA;&lt;p&gt;Vision primitives, such as &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console-2.md&#34;&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt; for image recognition, &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console-2.md&#34;&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt; for object detection, &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console-2.md&#34;&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt; for semantic segmentation, and &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/posenet.md&#34;&gt;&lt;code&gt;poseNet&lt;/code&gt;&lt;/a&gt; for pose estimation inherit from the shared &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/c/tensorNet.h&#34;&gt;&lt;code&gt;tensorNet&lt;/code&gt;&lt;/a&gt; object. Examples are provided for streaming from live camera feed and processing images. See the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#api-reference&#34;&gt;API Reference&lt;/a&gt;&lt;/strong&gt; section for detailed reference documentation of the C++ and Python libraries.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/dev/docs/images/deep-vision-primitives.jpg&#34;&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#hello-ai-world&#34;&gt;Hello AI World&lt;/a&gt; tutorial for running inference and transfer learning onboard your Jetson, including collecting your own datasets and training your own models. It covers image classification, object detection, semantic segmentation, pose estimation, and mono depth.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#hello-ai-world&#34;&gt;Hello AI World&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#video-walkthroughs&#34;&gt;Video Walkthroughs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#api-reference&#34;&gt;API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#code-examples&#34;&gt;Code Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#pre-trained-models&#34;&gt;Pre-Trained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#recommended-system-requirements&#34;&gt;System Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/CHANGELOG.md&#34;&gt;Change Log&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&amp;gt; &amp;nbsp; JetPack 5.0 is now supported, along with &lt;a href=&#34;https://developer.nvidia.com/embedded/jetson-agx-orin-developer-kit&#34;&gt;Jetson AGX Orin&lt;/a&gt;. &lt;br&gt; &amp;gt; &amp;nbsp; Try the new &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/posenet.md&#34;&gt;Pose Estimation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/depthnet.md&#34;&gt;Mono Depth&lt;/a&gt; tutorials! &lt;br&gt; &amp;gt; &amp;nbsp; See the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/CHANGELOG.md&#34;&gt;Change Log&lt;/a&gt; for the latest updates and new features. &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Hello AI World&lt;/h2&gt; &#xA;&lt;p&gt;Hello AI World can be run completely onboard your Jetson, including inferencing with TensorRT and transfer learning with PyTorch. The inference portion of Hello AI World - which includes coding your own image classification and object detection applications for Python or C++, and live camera demos - can be run on your Jetson in roughly two hours or less, while transfer learning is best left to leave running overnight.&lt;/p&gt; &#xA;&lt;h4&gt;System Setup&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/jetpack-setup-2.md&#34;&gt;Setting up Jetson with JetPack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-docker.md&#34;&gt;Running the Docker Container&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo-2.md&#34;&gt;Building the Project from Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console-2.md&#34;&gt;Classifying Images with ImageNet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console-2.md&#34;&gt;Using the ImageNet Program on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-python-2.md&#34;&gt;Coding Your Own Image Recognition Program (Python)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-2.md&#34;&gt;Coding Your Own Image Recognition Program (C++)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-camera-2.md&#34;&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console-2.md&#34;&gt;Locating Objects with DetectNet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console-2.md#detecting-objects-from-the-command-line&#34;&gt;Detecting Objects from Images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-camera-2.md&#34;&gt;Running the Live Camera Detection Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-example-2.md&#34;&gt;Coding Your Own Object Detection Program&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console-2.md&#34;&gt;Semantic Segmentation with SegNet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console-2.md#segmenting-images-from-the-command-line&#34;&gt;Segmenting Images from the Command Line&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-camera-2.md&#34;&gt;Running the Live Camera Segmentation Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/posenet.md&#34;&gt;Pose Estimation with PoseNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/depthnet.md&#34;&gt;Monocular Depth with DepthNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-transfer-learning.md&#34;&gt;Transfer Learning with PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Classification/Recognition (ResNet-18) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-cat-dog.md&#34;&gt;Re-training on the Cat/Dog Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-plants.md&#34;&gt;Re-training on the PlantCLEF Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-collect.md&#34;&gt;Collecting your own Classification Datasets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Object Detection (SSD-Mobilenet) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-ssd.md&#34;&gt;Re-training SSD-Mobilenet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-collect-detection.md&#34;&gt;Collecting your own Detection Datasets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Appendix&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-streaming.md&#34;&gt;Camera Streaming and Multimedia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-image.md&#34;&gt;Image Manipulation with CUDA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dusty-nv/ros_deep_learning&#34;&gt;Deep Learning Nodes for ROS/ROS2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video Walkthroughs&lt;/h2&gt; &#xA;&lt;p&gt;Below are screencasts of Hello AI World that were recorded for the &lt;a href=&#34;https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs&#34;&gt;Jetson AI Certification&lt;/a&gt; course:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Video&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QXIwdsyK7Rw&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=9&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Hello AI World Setup&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Download and run the Hello AI World container on Jetson Nano, test your camera feed, and see how to stream it over the network via RTP.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QXIwdsyK7Rw&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=9&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_setup.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QatH8iF0Efk&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=10&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Image Classification Inference&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Code your own Python program for image classification using Jetson Nano and deep learning, then experiment with realtime classification on a live camera stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QatH8iF0Efk&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=10&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_imagenet.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sN6aT9TpltU&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=11&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Training Image Classification Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Learn how to train image classification models with PyTorch onboard Jetson Nano, and collect your own classification datasets to create custom models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sN6aT9TpltU&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=11&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_imagenet_training.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=obt60r8ZeB0&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=12&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Object Detection Inference&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Code your own Python program for object detection using Jetson Nano and deep learning, then experiment with realtime detection on a live camera stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=obt60r8ZeB0&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=12&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_detectnet.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2XMkPW_sIGg&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=13&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Training Object Detection Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Learn how to train object detection models with PyTorch onboard Jetson Nano, and collect your own detection datasets to create custom models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2XMkPW_sIGg&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=13&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_detectnet_training.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AQhkMLaB_fY&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=14&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Semantic Segmentation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Experiment with fully-convolutional semantic segmentation networks on Jetson Nano, and run realtime segmentation on a live camera stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AQhkMLaB_fY&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=14&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_segnet.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;API Reference&lt;/h2&gt; &#xA;&lt;p&gt;Below are links to reference documentation for the &lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/index.html&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.html&#34;&gt;Python&lt;/a&gt; libraries from the repo:&lt;/p&gt; &#xA;&lt;h4&gt;jetson-inference&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/group__deepVision.html&#34;&gt;C++&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html&#34;&gt;Python&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classimageNet.html&#34;&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#imageNet&#34;&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classdetectNet.html&#34;&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#detectNet&#34;&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classsegNet.html&#34;&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/pytorch/docs/html/python/jetson.inference.html#segNet&#34;&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose Estimation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classposeNet.html&#34;&gt;&lt;code&gt;poseNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#poseNet&#34;&gt;&lt;code&gt;poseNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Monocular Depth&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classdepthNet.html&#34;&gt;&lt;code&gt;depthNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#depthNet&#34;&gt;&lt;code&gt;depthNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;jetson-utils&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/group__util.html&#34;&gt;C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.utils.html&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These libraries are able to be used in external projects by linking to &lt;code&gt;libjetson-inference&lt;/code&gt; and &lt;code&gt;libjetson-utils&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code Examples&lt;/h2&gt; &#xA;&lt;p&gt;Introductory code walkthroughs of using the library are covered during these steps of the Hello AI World tutorial:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-python-2.md&#34;&gt;Coding Your Own Image Recognition Program (Python)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-2.md&#34;&gt;Coding Your Own Image Recognition Program (C++)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additional C++ and Python samples for running the networks on static images and live camera streams can be found here:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;C++&lt;/th&gt; &#xA;   &lt;th&gt;Python&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Image Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/imagenet/imagenet.cpp&#34;&gt;&lt;code&gt;imagenet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/imagenet.py&#34;&gt;&lt;code&gt;imagenet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/detectnet/detectnet.cpp&#34;&gt;&lt;code&gt;detectnet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/detectnet.py&#34;&gt;&lt;code&gt;detectnet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/segnet/segnet.cpp&#34;&gt;&lt;code&gt;segnet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/segnet.py&#34;&gt;&lt;code&gt;segnet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Pose Estimation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/posenet/posenet.cpp&#34;&gt;&lt;code&gt;posenet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/posenet.py&#34;&gt;&lt;code&gt;posenet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Monocular Depth&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/depthnet/segnet.cpp&#34;&gt;&lt;code&gt;depthnet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/depthnet.py&#34;&gt;&lt;code&gt;depthnet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: for working with numpy arrays, see &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-image.md#converting-to-numpy-arrays&#34;&gt;Converting to Numpy Arrays&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-image.md#converting-from-numpy-arrays&#34;&gt;Converting from Numpy Arrays&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;These examples will automatically be compiled while &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo-2.md&#34;&gt;Building the Project from Source&lt;/a&gt;, and are able to run the pre-trained models listed below in addition to custom models provided by the user. Launch each example with &lt;code&gt;--help&lt;/code&gt; for usage info.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-Trained Models&lt;/h2&gt; &#xA;&lt;p&gt;The project comes with a number of pre-trained models that are available through the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo-2.md#downloading-models&#34;&gt;&lt;strong&gt;Model Downloader&lt;/strong&gt;&lt;/a&gt; tool:&lt;/p&gt; &#xA;&lt;h4&gt;Image Recognition&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Network&lt;/th&gt; &#xA;   &lt;th&gt;CLI argument&lt;/th&gt; &#xA;   &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AlexNet&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;alexnet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ALEXNET&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GoogleNet&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;googlenet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GOOGLENET&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GoogleNet-12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;googlenet-12&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GOOGLENET_12&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-18&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_18&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-50&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_50&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-101&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_101&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-152&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-152&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_152&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VGG-16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;vgg-16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VGG-16&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VGG-19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;vgg-19&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VGG-19&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Inception-v4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;inception-v4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;INCEPTION_V4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Object Detection&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Network&lt;/th&gt; &#xA;   &lt;th&gt;CLI argument&lt;/th&gt; &#xA;   &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;   &lt;th&gt;Object classes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD-Mobilenet-v1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ssd-mobilenet-v1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SSD_MOBILENET_V1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91 (&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/data/networks/ssd_coco_labels.txt&#34;&gt;COCO classes&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD-Mobilenet-v2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ssd-mobilenet-v2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SSD_MOBILENET_V2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91 (&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/data/networks/ssd_coco_labels.txt&#34;&gt;COCO classes&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD-Inception-v2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ssd-inception-v2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SSD_INCEPTION_V2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91 (&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/data/networks/ssd_coco_labels.txt&#34;&gt;COCO classes&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Dog&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-dog&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_DOG&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;dogs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Bottle&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-bottle&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_BOTTLE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bottles&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Chair&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-chair&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_CHAIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;chairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Airplane&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-airplane&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_AIRPLANE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;airplanes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ped-100&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pednet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PEDNET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pedestrians&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiped-500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;multiped&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PEDNET_MULTI&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pedestrians, luggage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;facenet-120&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;facenet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;FACENET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;faces&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Semantic Segmentation&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;CLI Argument&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Jetson Nano&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Jetson Xavier&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x256&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-512x256&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024x512&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-1024x512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;175 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048x1024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-2048x1024&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.6%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34;&gt;DeepScene&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;576x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-deepscene-576x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.4%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;360 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34;&gt;DeepScene&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;864x480&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-deepscene-864x480&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.9%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;190 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lv-mhp.github.io/&#34;&gt;Multi-Human&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-mhp-512x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;370 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lv-mhp.github.io/&#34;&gt;Multi-Human&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x360&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-mhp-512x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;325 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://host.robots.ox.ac.uk/pascal/VOC/&#34;&gt;Pascal VOC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;320x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-voc-320x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.9%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;508 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://host.robots.ox.ac.uk/pascal/VOC/&#34;&gt;Pascal VOC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-voc-512x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;375 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://rgbd.cs.princeton.edu/&#34;&gt;SUN RGB-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-sun-512x400&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;340 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://rgbd.cs.princeton.edu/&#34;&gt;SUN RGB-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x512&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-sun-640x512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If the resolution is omitted from the CLI argument, the lowest resolution model is loaded&lt;/li&gt; &#xA; &lt;li&gt;Accuracy indicates the pixel classification accuracy across the model&#39;s validation dataset&lt;/li&gt; &#xA; &lt;li&gt;Performance is measured for GPU FP16 mode with JetPack 4.2.1, &lt;code&gt;nvpmodel 0&lt;/code&gt; (MAX-N)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Legacy Segmentation Models&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Network&lt;/th&gt; &#xA;    &lt;th&gt;CLI Argument&lt;/th&gt; &#xA;    &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;    &lt;th&gt;Classes&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Cityscapes (2048x2048)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-cityscapes-hd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_CITYSCAPES_HD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Cityscapes (1024x1024)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-cityscapes-sd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_CITYSCAPES_SD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Pascal VOC (500x356)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-pascal-voc&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_PASCAL_VOC&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Synthia (CVPR16)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-cvpr&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_CVPR&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Synthia (Summer-HD)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-summer-hd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_SUMMER_HD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Synthia (Summer-SD)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-summer-sd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_SUMMER_SD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Aerial-FPV (1280x720)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-aerial-fpv-720p&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_AERIAL_FPV_720p&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Pose Estimation&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;CLI argument&lt;/th&gt; &#xA;   &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;   &lt;th&gt;Keypoints&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose-ResNet18-Body&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet18-body&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET18_BODY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose-ResNet18-Hand&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet18-hand&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET18_HAND&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose-DenseNet121-Body&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;densenet121-body&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;DENSENET121_BODY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Recommended System Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Jetson Nano Developer Kit with JetPack 4.2 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson Nano 2GB Developer Kit with JetPack 4.4.1 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson Xavier NX Developer Kit with JetPack 4.4 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson AGX Xavier Developer Kit with JetPack 4.0 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson TX2 Developer Kit with JetPack 3.0 or newer (Ubuntu 16.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson TX1 Developer Kit with JetPack 2.3 or newer (Ubuntu 16.04 aarch64).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#training&#34;&gt;Transfer Learning with PyTorch&lt;/a&gt; section of the tutorial speaks from the perspective of running PyTorch onboard Jetson for training DNNs, however the same PyTorch code can be used on a PC, server, or cloud instance with an NVIDIA discrete GPU for faster training.&lt;/p&gt; &#xA;&lt;h2&gt;Extra Resources&lt;/h2&gt; &#xA;&lt;p&gt;In this area, links and resources for deep learning are listed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.github.com/dusty-nv/ros_deep_learning&#34;&gt;ros_deep_learning&lt;/a&gt; - TensorRT inference ROS nodes&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA-AI-IOT&#34;&gt;NVIDIA AI IoT&lt;/a&gt; - NVIDIA Jetson GitHub repositories&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eLinux.org/Jetson&#34;&gt;Jetson eLinux Wiki&lt;/a&gt; - Jetson eLinux Wiki&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Two Days to a Demo (DIGITS)&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; the DIGITS/Caffe tutorial from below is deprecated. It&#39;s recommended to follow the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#training&#34;&gt;Transfer Learning with PyTorch&lt;/a&gt; tutorial from Hello AI World.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Expand this section to see original DIGITS tutorial (deprecated)&lt;/summary&gt; &#xA; &lt;br&gt; The DIGITS tutorial includes training DNN&#39;s in the cloud or PC, and inference on the Jetson with TensorRT, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your GPU. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/digits-workflow.md&#34;&gt;DIGITS Workflow&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/digits-setup.md&#34;&gt;DIGITS System Setup&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/jetpack-setup.md&#34;&gt;Setting up Jetson with JetPack&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo.md&#34;&gt;Building the Project from Source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console.md&#34;&gt;Classifying Images with ImageNet&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console.md#using-the-console-program-on-jetson&#34;&gt;Using the Console Program on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example.md&#34;&gt;Coding Your Own Image Recognition Program&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-camera.md&#34;&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md&#34;&gt;Re-Training the Network with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#downloading-image-recognition-dataset&#34;&gt;Downloading Image Recognition Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#customizing-the-object-classes&#34;&gt;Customizing the Object Classes&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#importing-classification-dataset-into-digits&#34;&gt;Importing Classification Dataset into DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#creating-image-classification-model-with-digits&#34;&gt;Creating Image Classification Model with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#testing-classification-model-in-digits&#34;&gt;Testing Classification Model in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-snapshot.md&#34;&gt;Downloading Model Snapshot to Jetson&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-custom.md&#34;&gt;Loading Custom Models on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md&#34;&gt;Locating Objects with DetectNet&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#detection-data-formatting-in-digits&#34;&gt;Detection Data Formatting in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#downloading-the-detection-dataset&#34;&gt;Downloading the Detection Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#importing-the-detection-dataset-into-digits&#34;&gt;Importing the Detection Dataset into DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#creating-detectnet-model-with-digits&#34;&gt;Creating DetectNet Model with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#testing-detectnet-model-inference-in-digits&#34;&gt;Testing DetectNet Model Inference in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-snapshot.md&#34;&gt;Downloading the Detection Model to Jetson&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-snapshot.md#detectnet-patches-for-tensorrt&#34;&gt;DetectNet Patches for TensorRT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console.md&#34;&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console.md#multi-class-object-detection-models&#34;&gt;Multi-class Object Detection Models&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-camera.md&#34;&gt;Running the Live Camera Detection Demo on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-dataset.md&#34;&gt;Semantic Segmentation with SegNet&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-dataset.md#downloading-aerial-drone-dataset&#34;&gt;Downloading Aerial Drone Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-dataset.md#importing-the-aerial-dataset-into-digits&#34;&gt;Importing the Aerial Dataset into DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-pretrained.md&#34;&gt;Generating Pretrained FCN-Alexnet&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-training.md&#34;&gt;Training FCN-Alexnet with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-training.md#testing-inference-model-in-digits&#34;&gt;Testing Inference Model in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-patches.md&#34;&gt;FCN-Alexnet Patches for TensorRT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console.md&#34;&gt;Running Segmentation Models on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;sup&gt;© 2016-2019 NVIDIA | &lt;/sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#deploying-deep-learning&#34;&gt;&lt;sup&gt;Table of Contents&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/puerts</title>
    <updated>2022-08-07T01:30:42Z</updated>
    <id>tag:github.com,2022-08-07:/Tencent/puerts</id>
    <link href="https://github.com/Tencent/puerts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Write your game with TypeScript in UE or Unity. Puerts can be pronounced as pu-erh TS（普洱TS）&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/pic/puerts_logo.png&#34; alt=&#34;Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/puerts/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD_3_Clause-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/puerts/releases/tag/Unreal_v1.0.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/unreal-v1.0.1-blue.svg?sanitize=true&#34; alt=&#34;unreal&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/install.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/unity-v1.3.6-blue.svg?sanitize=true&#34; alt=&#34;unity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/puerts/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/Tencent/puerts/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What？&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;in English&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;puerts is a TypeScript programming solution within game engines.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;provides a JavaScript Runtime&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;allows TypeScript to access the host engine（module-binding on the JavaScript level and generating TypeScript declarations）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;说中文&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;puerts是游戏引擎下的TypeScript编程解决方案&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;提供了一个JavaScript运行时&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;提供通过TypeScript访问宿主引擎的能力（JavaScript层面的绑定以及TypeScript声明生成）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;in English&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Facililates game building processes by combining JavaScript packages and toolchains with the rendering power of professional game engines&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In contrast to lua script, TypeScript supports static type checking, which significantly improves code robustness and maintainability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High efficiency: supports reflection binding throughout the platform (engine) - no extra steps (code generation) needed for development.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;High performance：supports static binding throughout the platform (engine) - takes care of complex scenes&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;说中文&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;JavaScript生态有众多的库和工具链，结合专业商业引擎的渲染能力，快速打造游戏&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;相比游戏领域常用的lua脚本，TypeScript的静态类型检查有助于编写更健壮，可维护性更好的程序&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;高效：全引擎，全平台支持反射Binding，无需额外（生成代码）步骤即可开发&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;高性能：全引擎，全平台支持静态Binding，兼顾了高性能的场景&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How to Install | 最新版本安装&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/install.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/install.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changelog&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/changelog.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/changelog.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known issues | 已知问题与解决办法&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/bugs.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/bugs.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code Sample | 编程样例&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unity&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import {UnityEngine} from &#39;csharp&#39;&#xA;&#xA;UnityEngine.Debug.Log(&#39;hello world&#39;);&#xA;let gameObject = new UnityEngine.GameObject(&#34;testobject&#34;);&#xA;console.log(gameObject.name);&#xA;gameObject.transform.position = new UnityEngine.Vector3(1, 2, 3);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unreal&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import * as UE from &#39;ue&#39;&#xA;import {argv} from &#39;puerts&#39;;&#xA;let world = argv.getByName(&#34;World&#34;) as UE.World;&#xA;let actor = world.SpawnActor(UE.MainActor.StaticClass(),&#xA;    undefined, UE.ESpawnActorCollisionHandlingMethod.Undefined, undefined, undefined) as UE.MainActor;&#xA;console.log(actor.GetName());&#xA;console.log(actor.K2_GetActorLocation().ToString());&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Manual | 参考文档&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unreal&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/unreal/README.md&#34;&gt;Unreal Readme&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/manual.md&#34;&gt;Unreal Manual&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/vscode_debug.md&#34;&gt;Unreal debugging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/unreal/interact_with_uclass.md&#34;&gt;TypeScript and unreal engine interaction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/unreal/template_binding.md&#34;&gt;Template-based static binding&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unity&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/unity/README.md&#34;&gt;Unity Readme&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/manual.md&#34;&gt;Unity Manual&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/vscode_debug.md&#34;&gt;Unity debugging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo#more-example--%E6%9B%B4%E5%A4%9A%E7%9A%84%E7%A4%BA%E4%BE%8B%E6%88%96%E8%80%85%E6%95%99%E7%A8%8B&#34;&gt;More Framework for Unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sample Projects | 示例项目&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unreal&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unreal_demo/raw/master/TsProj/QuickStart.ts&#34;&gt;QuickStart.ts&lt;/a&gt; ： 演示TypeScript和UE4引擎互相调用&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unreal_demo/raw/master/TsProj/NewContainer.ts&#34;&gt;NewContainer.ts&lt;/a&gt; ： 演示容器的创建&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unreal_demo/raw/master/TsProj/AsyncTest.ts&#34;&gt;AsyncTest.ts&lt;/a&gt; ： 将异步加载蓝图，Delay封装成async/await&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unreal_demo/raw/master/TsProj/UsingWidget.ts&#34;&gt;UsingWidget.ts&lt;/a&gt; ： UI加载，绑定事件，获取数据的演示&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unreal_demo/raw/master/TsProj/UsingMixin.ts&#34;&gt;UsingMixin.ts&lt;/a&gt;：演示mixin功能的使用&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_fps_demo&#34;&gt;FPS demo&lt;/a&gt; ： 以一个FPS游戏例子演示如何使用Puerts的“继承引擎类功能”，该功能的介绍见&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/manual.md&#34;&gt;unreal手册&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unity&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo&#34;&gt;Basic_Demo&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo/Assets/Examples/01_JsCallCs&#34;&gt;01_JsCallCs&lt;/a&gt; ： js调用c#&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo/Assets/Examples/02_Require&#34;&gt;02_Require&lt;/a&gt; ： 加载js文件&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo/Assets/Examples/03_Callback&#34;&gt;03_Callback&lt;/a&gt; ： 回调基本演示&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo/Assets/Examples/04_JsBehaviour&#34;&gt;04_JsBehaviour&lt;/a&gt; ： 用js模拟MonoBehaviour&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo/Assets/Examples/05_Typescript&#34;&gt;05_Typescript&lt;/a&gt; ： 包含了大部分TypeScript和C#互相调用的演示&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/0_Basic_Demo/Assets/Examples/06_UIEvent&#34;&gt;06_UIEvent&lt;/a&gt; ：UI事件的演示&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/chexiongsheng/puerts_unity_demo/tree/master/projects/1_Start_Template&#34;&gt;Start_Template&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ | 常见问题&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/faq.md&#34;&gt;general faq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/faq.md&#34;&gt;unreal faq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/faq.md&#34;&gt;unity faq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Avaliable on these Engine | 引擎&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;unreal engine 4.22 ~ latest&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;unity 5 ~ latest&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Available on these Platform | 平台&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;iOS，Android，Windows，Macos&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;任意.net环境 | Any .net project&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ask for help | 技术支持&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/puerts/discussions&#34;&gt;Github Discussion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;QQ群：942696334&lt;/p&gt; &#xA;&lt;p&gt;UE4专属群：689643903&lt;/p&gt; &#xA;&lt;h2&gt;开发博客&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/column/c_1355534112468402176&#34;&gt;知乎专栏&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/calculator</title>
    <updated>2022-08-07T01:30:42Z</updated>
    <id>tag:github.com,2022-08-07:/microsoft/calculator</id>
    <link href="https://github.com/microsoft/calculator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Windows Calculator: A simple yet powerful calculator that ships with Windows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Calculator&lt;/h1&gt; &#xA;&lt;p&gt;The Windows Calculator app is a modern Windows app written in C++ and C# that ships pre-installed with Windows. The app provides standard, scientific, and programmer calculator functionality, as well as a set of converters between various units of measurement and currencies.&lt;/p&gt; &#xA;&lt;p&gt;Calculator ships regularly with new features and bug fixes. You can get the latest version of Calculator in the &lt;a href=&#34;https://www.microsoft.com/store/apps/9WZDNCRFHVN5&#34;&gt;Microsoft Store&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/ms/calculator/_build/latest?definitionId=57&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/ms/calculator/_apis/build/status/Calculator-CI?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/docs/Images/CalculatorScreenshot.png&#34; alt=&#34;Calculator Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standard Calculator functionality which offers basic operations and evaluates commands immediately as they are entered.&lt;/li&gt; &#xA; &lt;li&gt;Scientific Calculator functionality which offers expanded operations and evaluates commands using order of operations.&lt;/li&gt; &#xA; &lt;li&gt;Programmer Calculator functionality which offers common mathematical operations for developers including conversion between common bases.&lt;/li&gt; &#xA; &lt;li&gt;Date Calculation functionality which offers the difference between two dates, as well as the ability to add/subtract years, months and/or days to/from a given input date.&lt;/li&gt; &#xA; &lt;li&gt;Calculation history and memory capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Conversion between many units of measurement.&lt;/li&gt; &#xA; &lt;li&gt;Currency conversion based on data retrieved from &lt;a href=&#34;https://www.bing.com&#34;&gt;Bing&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic&#34;&gt;Infinite precision&lt;/a&gt; for basic arithmetic operations (addition, subtraction, multiplication, division) so that calculations never lose precision.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Prerequisites:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Your computer must be running Windows 10, version 1809 or newer. Windows 11 is recommended.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the latest version of &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/downloads&#34;&gt;Visual Studio&lt;/a&gt; (the free community edition is sufficient).&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install the &#34;Universal Windows Platform Development&#34; workload.&lt;/li&gt; &#xA;   &lt;li&gt;Install the optional &#34;C++ Universal Windows Platform tools&#34; component.&lt;/li&gt; &#xA;   &lt;li&gt;Install the latest Windows 11 SDK.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/docs/Images/VSInstallationScreenshot.png&#34; alt=&#34;Visual Studio Installation Screenshot&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=TeamXavalon.XAMLStyler&#34;&gt;XAML Styler&lt;/a&gt; Visual Studio extension.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Microsoft/calculator.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/src/Calculator.sln&#34;&gt;src\Calculator.sln&lt;/a&gt; in Visual Studio to build and run the Calculator app.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For a general description of the Calculator project architecture see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/docs/ApplicationArchitecture.md&#34;&gt;ApplicationArchitecture.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To run the UI Tests, you need to make sure that &lt;a href=&#34;https://github.com/microsoft/WinAppDriver/releases/latest&#34;&gt;Windows Application Driver (WinAppDriver)&lt;/a&gt; is installed.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Want to contribute? The team encourages community feedback and contributions. Please follow our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If Calculator is not working properly, please file a report in the &lt;a href=&#34;https://insider.windows.com/en-us/fb/?contextid=130&#34;&gt;Feedback Hub&lt;/a&gt;. We also welcome &lt;a href=&#34;https://github.com/Microsoft/calculator/issues&#34;&gt;issues submitted on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;For information regarding Windows Calculator plans and release schedule, please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/docs/Roadmap.md&#34;&gt;Windows Calculator Roadmap&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Graphing Mode&lt;/h3&gt; &#xA;&lt;p&gt;Adding graphing calculator functionality &lt;a href=&#34;https://github.com/Microsoft/calculator/issues/338&#34;&gt;is on the project roadmap&lt;/a&gt; and we hope that this project can create a great end-user experience around graphing. To that end, the UI from the official in-box Windows Calculator is currently part of this repository, although the proprietary Microsoft-built graphing engine, which also drives graphing in Microsoft Mathematics and OneNote, is not. Community members can still be involved in the creation of the UI, however developer builds will not have graphing functionality due to the use of a &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/src/GraphingImpl/Mocks&#34;&gt;mock implementation of the engine&lt;/a&gt; built on top of a &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/src/GraphingInterfaces&#34;&gt;common graphing API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diagnostic Data&lt;/h2&gt; &#xA;&lt;p&gt;This project collects usage data and sends it to Microsoft to help improve our products and services. Read our &lt;a href=&#34;https://go.microsoft.com/fwlink/?LinkId=521839&#34;&gt;privacy statement&lt;/a&gt; to learn more. Diagnostic data is disabled in development builds by default, and can be enabled with the &lt;code&gt;SEND_DIAGNOSTICS&lt;/code&gt; build flag.&lt;/p&gt; &#xA;&lt;h2&gt;Currency Converter&lt;/h2&gt; &#xA;&lt;p&gt;Windows Calculator includes a currency converter feature that uses mock data in developer builds. The data that Microsoft uses for the currency converter feature (e.g., in the retail version of the application) is not licensed for your use. The mock data will be clearly identifiable as it references planets instead of countries, and remains static regardless of selected inputs.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting Security Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/SECURITY.md&#34;&gt;SECURITY.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright (c) Microsoft Corporation. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/calculator/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>