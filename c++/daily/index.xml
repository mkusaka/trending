<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-08T01:27:49Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bytedance/flux</title>
    <updated>2025-03-08T01:27:49Z</updated>
    <id>tag:github.com,2025-03-08:/bytedance/flux</id>
    <link href="https://github.com/bytedance/flux" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast communication-overlapping library for tensor/expert parallelism on GPUs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Flux&lt;/h1&gt; &#xA;&lt;p&gt;Flux is a communication-overlapping library for dense/MoE models on GPUs, providing high-performance and pluggable kernels to support various parallelisms in model training/inference.&lt;/p&gt; &#xA;&lt;p&gt;Flux&#39;s efficient kernels are compatible with Pytorch and can be integrated into existing frameworks easily, supporting various Nvidia GPU architectures and data types.&lt;/p&gt; &#xA;&lt;p&gt;Welcome to join the &lt;a href=&#34;https://github.com/bytedance/flux/raw/main/docs/assets/comet_wechat_group.JPG&#34;&gt;Wechat&lt;/a&gt; group and stay tuned!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Install Flux either from source or from PyPI.&lt;/p&gt; &#xA;&lt;h3&gt;Install from Source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/bytedance/flux.git &amp;amp;&amp;amp; cd flux&#xA;&#xA;# Install dependencies&#xA;bash ./install_deps.sh&#xA;&#xA;# For Ampere(sm80) GPU&#xA;./build.sh --arch 80 --nvshmem&#xA;# For Ada Lovelace(sm89) GPU&#xA;./build.sh --arch 89 --nvshmem&#xA;# For Hopper(sm90) GPU&#xA;./build.sh --arch 90 --nvshmem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install in a virtual environment&lt;/h4&gt; &#xA;&lt;p&gt;Here is a snippet to install Flux in a virtual environment. Let&#39;s finish the installation in an virtual environment with CUDA 12.4, torch 2.6.0 and python 3.11.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n flux python=3.11&#xA;conda activate flux&#xA;pip3 install packaging&#xA;pip3 install ninja&#xA;pip3 install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124&#xA;&#xA;./build.sh --clean-all&#xA;./build.sh --arch &#34;80;89;90&#34; --nvshmem --package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you would expect a wheel package under &lt;code&gt;dist/&lt;/code&gt; folder that is suitable for your virtual environment.&lt;/p&gt; &#xA;&lt;h3&gt;Install from PyPI&lt;/h3&gt; &#xA;&lt;p&gt;We also provide some pre-built wheels for Flux, and you can directly install with pip if your wanted version is available. Currently we provide wheels for the following configurations: torch(2.4.0, 2.5.0, 2.6.0), python(3.10, 3.11), cuda(12.4).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Make sure that PyTorch is installed.&#xA;pip install byte-flux&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customized Installation&lt;/h3&gt; &#xA;&lt;h4&gt;Build options for source installation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--nvshmem&lt;/code&gt; to build Flux with NVSHMEM support. It is essential for the MoE kernels.&lt;/li&gt; &#xA; &lt;li&gt;If you are tired of the cmake process, you can set environment variable &lt;code&gt;FLUX_BUILD_SKIP_CMAKE&lt;/code&gt; to 1 to skip cmake if &lt;code&gt;build/CMakeCache.txt&lt;/code&gt; already exists.&lt;/li&gt; &#xA; &lt;li&gt;If you want to build a wheel package, add &lt;code&gt;--package&lt;/code&gt; to the build command. find the output wheel file under dist/&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;The core dependencies of Flux are NCCL, CUTLASS, and NVSHMEM, which are located under the 3rdparty folder.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;NCCL: Managed by git submodule automatically.&lt;/li&gt; &#xA; &lt;li&gt;NVSHMEM: Downloaded from &lt;a href=&#34;https://developer.nvidia.com/nvshmem&#34;&gt;https://developer.nvidia.com/nvshmem&lt;/a&gt;. The current version is 3.2.5-1.&lt;/li&gt; &#xA; &lt;li&gt;CUTLASS: Flux leverages CUTLASS to generate high-performance GEMM kernels. We currently use CUTLASS 3.7.0 and a tiny patch should be applied to CUTLASS.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Below are commands to run some basic demos once you have installed Flux successfully.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# gemm only&#xA;python3 test/python/gemm_only/test_gemm_only.py 4096 12288 6144 --dtype=float16&#xA;&#xA;# all-gather fused with gemm (dense MLP layer0)&#xA;./launch.sh test/python/ag_gemm/test_ag_kernel.py 4096 49152 12288 --dtype=float16 --iters=10&#xA;&#xA;# gemm fused with reduce-scatter (dense MLP layer1)&#xA;./launch.sh test/python/gemm_rs/test_gemm_rs.py 4096 12288 49152 --dtype=float16 --iters=10&#xA;&#xA;# all-gather fused with grouped gemm (MoE MLP layer0)&#xA;./launch.sh test/python/moe_ag_scatter/test_moe_ag.py&#xA;&#xA;# grouped gemm fused with reduce-scatter (MoE MLP layer1)&#xA;./launch.sh test/python/moe_gather_rs/test_moe_gather_rs.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check out the documentations for more details!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For a more detailed usage on MoE kernels, please refer to &lt;a href=&#34;https://github.com/bytedance/flux/raw/main/docs/moe_usage.md&#34;&gt;Flux MoE Usage&lt;/a&gt;. Try some &lt;a href=&#34;https://github.com/bytedance/flux/raw/main/examples&#34;&gt;examples&lt;/a&gt; as a quick start!&lt;/li&gt; &#xA; &lt;li&gt;For some performance numbers, please refer to &lt;a href=&#34;https://github.com/bytedance/flux/raw/main/docs/performance.md&#34;&gt;Performance Doc&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To learn more about the design principles of Flux, please refer to &lt;a href=&#34;https://github.com/bytedance/flux/raw/main/docs/design.md&#34;&gt;Design Doc&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you use Flux in a scientific publication, we encourage you to add the following reference to the related papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{chang2024flux,&#xA;      title={FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion},&#xA;      author={Li-Wen Chang and Wenlei Bao and Qi Hou and Chengquan Jiang and Ningxin Zheng and Yinmin Zhong and Xuanrun Zhang and Zuquan Song and Ziheng Jiang and Haibin Lin and Xin Jin and Xin Liu},&#xA;      year={2024},&#xA;      eprint={2406.06858},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&#xA;@misc{zhang2025comet,&#xA;      title={Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts},&#xA;      author={Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen and Xin Liu},&#xA;      year={2025},&#xA;      eprint={2502.19811},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.DC}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/2406.06858&#34;&gt;ArXiv Paper (Flux)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.19811&#34;&gt;ArXiv Paper (Comet)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytedance/flux/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The Flux Project is under the Apache License v2.0.&lt;/p&gt;</summary>
  </entry>
</feed>