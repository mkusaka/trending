<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-20T01:27:56Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>inferflow/inferflow</title>
    <updated>2024-01-20T01:27:56Z</updated>
    <id>tag:github.com,2024-01-20:/inferflow/inferflow</id>
    <link href="https://github.com/inferflow/inferflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inferflow is an efficient and highly configurable inference engine for large language models (LLMs).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Inferflow&lt;/h1&gt; &#xA;&lt;h4&gt; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Version-1.0-blue.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/inferflow/inferflow?color=yellow&#34; alt=&#34;Stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/inferflow/inferflow?color=red&#34; alt=&#34;Issues&#34;&gt;&lt;/p&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://inferflow.github.io/&#34;&gt;&lt;strong&gt;Inferflow&lt;/strong&gt;&lt;/a&gt; is an efficient and highly configurable inference engine for large language models (LLMs). With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code. Further details can be found in our &lt;a href=&#34;https://arxiv.org/abs/2401.08294&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Main Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensible and highly configurable&lt;/strong&gt;: A typical way of using Inferflow to serve a new model is editing a model specification file, but not adding/editing source codes. We implement in Inferflow a modular framework of atomic building-blocks and technologies, making it compositionally generalizable to new models. A new model can be served by Inferflow if the atomic building-blocks and technologies in this model have been &#34;known&#34; (to Inferflow).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3.5-bit quantization&lt;/strong&gt;: Inferflow implements 2-bit, 3-bit, 3.5-bit, 4-bit, 5-bit, 6-bit and 8-bit quantization. Among the quantization schemes, 3.5-bit quantization is a new one introduced by Inferflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hybrid model partition for multi-GPU inference&lt;/strong&gt;: Inferflow supports multi-GPU inference with three model partitioning strategies to choose from: partition-by-layer (pipeline parallelism), partition-by-tensor (tensor parallelism), and hybrid partitioning (hybrid parallelism). Hybrid partitioning is seldom supported by other inference engines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide file format support&lt;/strong&gt; (and safely loading pickle data): Inferflow supports loading models of multiple file formats directly, without reliance on an external converter. Supported formats include pickle, safetensors, llama.cpp gguf, etc. It is known that there are security issues to read pickle files using Python codes. By implementing a simplified pickle parser in C++, Inferflow supports safely loading models from pickle data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide network type support&lt;/strong&gt;: Supporting three types transformer models: decoder-only models, encoder-only models, and encoder-decoder models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU/CPU hybrid inference&lt;/strong&gt;: Supporting GPU-only, CPU-only, and GPU/CPU hybrid inference.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Below is a comparison between Inferflow and some other inference engines:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;New Model Support&lt;/th&gt; &#xA;   &lt;th&gt;Supported File Formats&lt;/th&gt; &#xA;   &lt;th&gt;Network Structures&lt;/th&gt; &#xA;   &lt;th&gt;Quantization Bits&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hybrid Parallelism for Multi-GPU Inference&lt;/th&gt; &#xA;   &lt;th&gt;Programming Languages&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;Huggingface Transformers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;pickle (unsafe), safetensors&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only, encoder-decoder, encoder-only&lt;/td&gt; &#xA;   &lt;td&gt;4b, 8b&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;pickle (unsafe), safetensors&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only&lt;/td&gt; &#xA;   &lt;td&gt;4b, 8b&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only, encoder-decoder, encoder-only&lt;/td&gt; &#xA;   &lt;td&gt;4b, 8b&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;C++, Python&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed-MII&#34;&gt;DeepSpeed-MII&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;pickle (unsafe), safetensors&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;gguf&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only&lt;/td&gt; &#xA;   &lt;td&gt;2b, 3b, 4b, 5b, 6b, 8b&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;C/C++&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;llama2.c&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/lmdeploy&#34;&gt;LMDeploy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adding/editing source codes&lt;/td&gt; &#xA;   &lt;td&gt;pickle (unsafe), TurboMind&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only&lt;/td&gt; &#xA;   &lt;td&gt;4b, 8b&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;C++, Python&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Inferflow&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Editing configuration files&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pickle (&lt;strong&gt;safe&lt;/strong&gt;), safetensors, gguf, llama2.c&lt;/td&gt; &#xA;   &lt;td&gt;decoder-only, encoder-decoder, encoder-only&lt;/td&gt; &#xA;   &lt;td&gt;2b, 3b, &lt;strong&gt;3.5b&lt;/strong&gt;, 4b, 5b, 6b, 8b&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;✔&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt; &#xA;   &lt;td&gt;C++&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Support Matrix&lt;/h2&gt; &#xA;&lt;h3&gt;Supported Model File Formats&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pickle (Inferflow reduces the security issue of most other inference engines in loading pickle-format files).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Safetensors&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; llama.cpp gguf&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; llama2.c&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Technologies, Modules, and Options&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Supported modules and technologies related to model definition:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Normalization functions: STD, RMS&lt;/li&gt; &#xA;   &lt;li&gt;Activation functions: RELU, GELU, SILU&lt;/li&gt; &#xA;   &lt;li&gt;Position embeddings: ALIBI, RoPE, Sinusoidal&lt;/li&gt; &#xA;   &lt;li&gt;Grouped-query attention&lt;/li&gt; &#xA;   &lt;li&gt;Parallel attention&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Supported technologies and options related to serving:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linear quantization of weights and KV cache elements: 2-bit, 3b, 3.5b, 4b, 5b, 6b, 8b&lt;/li&gt; &#xA;   &lt;li&gt;The option of moving part of all of the KV cache from VRAM to regular RAM&lt;/li&gt; &#xA;   &lt;li&gt;The option of placing the input embedding tensor(s) to regular RAM&lt;/li&gt; &#xA;   &lt;li&gt;Model partitioning strategies for multi-GPU inference: partition-by-layer, partition-by-tensor, hybrid partitioning&lt;/li&gt; &#xA;   &lt;li&gt;Dynamic batching&lt;/li&gt; &#xA;   &lt;li&gt;Decoding strategies: Greedy, top-k, top-p, FSD, typical, mirostat...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Transformer Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Decoder-only: Inferflow supports many types of decoder-only transformer models.&lt;/li&gt; &#xA; &lt;li&gt;Encoder-decoder: Some types of encoder-decoder models are supported.&lt;/li&gt; &#xA; &lt;li&gt;Encoder-only: Some types of encoder-only models are supported.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Models with Predefined Specification Files&lt;/h3&gt; &#xA;&lt;p&gt;Users can serve a model with Inferflow by editing a model specification file. We have built &lt;a href=&#34;https://raw.githubusercontent.com/inferflow/inferflow/main/data/models/&#34;&gt;predefined specification files&lt;/a&gt; for some popular or representative models. Below is a list of such models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Aquila (aquila_chat2_34b)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Baichuan (baichuan2_7b_chat, baichuan2_13b_chat)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; BERT (bert-base-multilingual-cased)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Bloom (bloomz_3b)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Facebook m2m100 (facebook_m2m100_418m)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Falcon (falcon_7b_instruct, falcon_40b_instruct)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Internlm (internlm-chat-20b)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLAMA2 (llama2_7b, llama2_7b_chat, llama2_13b_chat)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mistral (mistral_7b_instruct)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open LLAMA (open_llama_3b)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Phi-2 (phi_2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; XVERSE (xverse_13b_chat)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YI (yi_6b, yi_34b_chat)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/inferflow/inferflow&#xA;cd inferflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Build with cmake on Linux, Mac, and WSL (Windows Subsystem for Linux):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build the GPU version (that supports GPU/CPU hybrid inference):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build/gpu&#xA;cd build/gpu&#xA;cmake ../.. -DUSE_CUDA=1 -DCMAKE_CUDA_ARCHITECTURES=75&#xA;make install -j 8&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build the CPU-only version:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build/cpu&#xA;cd build/cpu&#xA;cmake ../.. -DUSE_CUDA=0&#xA;make install -j 8&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Upon a successful build, executables are generated and copied to &lt;code&gt;bin/release/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build with Visual Studio on Windows:&lt;/p&gt; &lt;p&gt;Open one of the following sln files in build/vs_projects:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;inferflow.sln&lt;/strong&gt;: The GPU version that supports GPU/CPU hybrid inference&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;inferflow_cpu.sln&lt;/strong&gt;: The CPU-only version&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For the Debug configuration, executables are generated to &lt;code&gt;bin/x64_Debug/&lt;/code&gt;; while the output directory for the Release configuration is &lt;code&gt;bin/x64_Release/&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run the Service and Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Example-1: Load a tiny model and perform inference&lt;/p&gt; &lt;p&gt;Step-1: Download the model&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd {inferflow-root-dir}/data/models/llama2.c/&#xA;bash download.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-2: Run the &lt;strong&gt;llm_inference&lt;/strong&gt; tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd {inferflow-root-dir}/bin/&#xA;release/llm_inference llm_inference.tiny.ini&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Example-2: Run the &lt;strong&gt;llm_inference&lt;/strong&gt; tool to load a larger model for inference&lt;/p&gt; &lt;p&gt;Step-1: Edit configuration file &lt;strong&gt;bin/llm_inference.ini&lt;/strong&gt; to choose a model&lt;/p&gt; &lt;p&gt;Step-2: Download the selected model&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd {inferflow-root-dir}/data/models/{model-name}/&#xA;bash download.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-3: Run the tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd {inferflow-root-dir}/bin/&#xA;release/llm_inference&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the Inferflow service:&lt;/p&gt; &lt;p&gt;Step-1: Edit the service configuration file (bin/inferflow_service.ini)&lt;/p&gt; &lt;p&gt;Step-2: Start the service:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd bin/release (on Windows: cd bin/x64_Release)&#xA;./inferflow_service&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the Inferflow client (for interacting with the Inferflow service via HTTP protocol to get inference results):&lt;/p&gt; &lt;p&gt;Step-1: Edit the configuration file (bin/inferflow_client.ini)&lt;/p&gt; &lt;p&gt;Step-2: Run the client tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd bin/release (on Windows: cd bin/x64_Release)&#xA;./inferflow_client&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use the CURL command to send a HTTP POST request to the Inferflow service and get inference results. Below is an example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -X POST -d &#39;{&#34;text&#34;: &#34;Write an article about the weather of Seattle.&#34;, &#34;res_prefix&#34;: &#34;&#34;, &#34;decoding_alg&#34;: &#34;sample.top_p&#34;, &#34;random_seed&#34;: 1, &#34;temperature&#34;: 0.7, &#34;is_streaming_mode&#34;: false}&#39; localhost:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reference&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in our work, please kindly cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@misc{shi2024inferflow,&#xA;    title={Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models},&#xA;    author={Shuming Shi and Enbo Zhao and Deng Cai and Leyang Cui and Xinting Huang and Huayang Li},&#xA;    year={2024},&#xA;    eprint={2401.08294},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;Inferflow is inspired by the awesome projects of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;. The CPU inference part of Inferflow is based on the &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; library. The FP16 data type in the CPU-only version of Inferflow is from the &lt;a href=&#34;https://half.sourceforge.net/&#34;&gt;Half-precision floating-point library&lt;/a&gt;. We express our sincere gratitude to the maintainers and implementers of these source codes and tools.&lt;/p&gt;</summary>
  </entry>
</feed>