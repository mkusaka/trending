<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-18T01:30:00Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>anhkgg/SuperRDP</title>
    <updated>2022-06-18T01:30:00Z</updated>
    <id>tag:github.com,2022-06-18:/anhkgg/SuperRDP</id>
    <link href="https://github.com/anhkgg/SuperRDP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Super RDPWrap&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SuperRDP&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anhkgg/SuperRDP/main/bin/SuperRDP2.zip&#34;&gt;SuperRDP2&lt;/a&gt; is coming.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic support for the latest version of Remote Desktop&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enjoy it! Remember to check the box of AutoSupport&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anhkgg/SuperRDP/main/superrdp2_help.md&#34;&gt;SuperRDP2 help&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=2_rBfUYNjxo&#34;&gt;video for usage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anhkgg/SuperRDP/main/superrdp2_help_cn.md&#34;&gt;国内帮助在这里&lt;/a&gt; 和 &lt;a href=&#34;https://www.bilibili.com/video/BV1T54y1Z7TB/&#34;&gt;简单操作视频&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anhkgg/SuperRDP/main/README_cn.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;SuperRDP is a new version rdpwarp base on the &lt;a href=&#34;https://github.com/stascorp/rdpwrap&#34;&gt;rdpwrap&lt;/a&gt; by &lt;a href=&#34;https://github.com/stascorp&#34;&gt;stascorp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks for &lt;a href=&#34;https://github.com/stascorp&#34;&gt;stascorp&lt;/a&gt; so much.&lt;/p&gt; &#xA;&lt;p&gt;SuperRDP is written in C, and rebuild the code about patch and hook.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m very dependent on the Remote Desktop service, so I will keep updating to support for the new version of termsrv.&lt;/p&gt; &#xA;&lt;h1&gt;update&lt;/h1&gt; &#xA;&lt;p&gt;If you found SuperRDP can&#39;t support your version, please upload c:\windows\system32\termsrv.dll to me by &lt;a href=&#34;https://github.com/anhkgg/SuperRDP/issues&#34;&gt;github issues&lt;/a&gt;, I will update it by soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/anhkgg/SuperRDP/main/update.md&#34;&gt;click me&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Support me&lt;/h1&gt; &#xA;&lt;p&gt;If you think it make some work, please star it and support me.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/anhkgg/SuperRDP/main/pay.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>oneapi-src/oneDNN</title>
    <updated>2022-06-18T01:30:00Z</updated>
    <id>tag:github.com,2022-06-18:/oneapi-src/oneDNN</id>
    <link href="https://github.com/oneapi-src/oneDNN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;oneAPI Deep Neural Network Library (oneDNN)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;oneAPI Deep Neural Network Library (oneDNN)&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This software was previously known as &lt;strong&gt;Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)&lt;/strong&gt; and &lt;strong&gt;Deep Neural Network Library (DNNL)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;img align=&#34;left&#34; src=&#34;https://spec.oneapi.io/oneapi-logo-white-scaled.jpg&#34; alt=&#34;oneAPI logo&#34;&gt; &#xA;&lt;p&gt;oneAPI Deep Neural Network Library (oneDNN) is an open-source cross-platform performance library of basic building blocks for deep learning applications. oneDNN is part of &lt;a href=&#34;https://oneapi.io&#34;&gt;oneAPI&lt;/a&gt;. The library is optimized for Intel(R) Architecture Processors, Intel Processor Graphics and Xe Architecture graphics. oneDNN has experimental support for the following architectures: Arm* 64-bit Architecture (AArch64), NVIDIA* GPU, OpenPOWER* Power ISA (PPC64), IBMz* (s390x), and RISC-V.&lt;/p&gt; &#xA;&lt;p&gt;oneDNN is intended for deep learning applications and framework developers interested in improving application performance on Intel CPUs and GPUs. Deep learning practitioners should use one of the &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#applications-enabled-with-onednn&#34;&gt;applications enabled with oneDNN&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#system-requirements&#34;&gt;System Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#applications-enabled-with-onednn&#34;&gt;Applications Enabled with oneDNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#security&#34;&gt;Security&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#trademark-information&#34;&gt;Trademark Information&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://oneapi-src.github.io/oneDNN&#34;&gt;Developer guide&lt;/a&gt; explains programming model, supported functionality, and implementation details, and includes annotated examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://oneapi-src.github.io/oneDNN/group_dnnl_api.html&#34;&gt;API reference&lt;/a&gt; provides a comprehensive reference of the library API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Binary distribution of this software is available in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://anaconda.org/conda-forge/onednn&#34;&gt;Anaconda&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/onednn&#34;&gt;Intel oneAPI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The packages do not include library dependencies and these need to be resolved in the application at build time. See the &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/#system-requirements&#34;&gt;System Requirements&lt;/a&gt; section below and the &lt;a href=&#34;https://oneapi-src.github.io/oneDNN/dev_guide_build_options.html&#34;&gt;Build Options&lt;/a&gt; section in the &lt;a href=&#34;https://oneapi-src.github.io/oneDNN&#34;&gt;developer guide&lt;/a&gt; for more details on CPU and GPU runtimes.&lt;/p&gt; &#xA;&lt;p&gt;If the configuration you need is not available, you can &lt;a href=&#34;https://oneapi-src.github.io/oneDNN/dev_guide_build.html&#34;&gt;build the library from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;System Requirements&lt;/h1&gt; &#xA;&lt;p&gt;oneDNN supports platforms based on the following architectures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/X86-64&#34;&gt;Intel 64 or AMD64&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.arm.com/architectures/cpu-architecture/a-profile&#34;&gt;Arm 64-bit Architecture (AArch64)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openpowerfoundation.org/&#34;&gt;OpenPOWER&lt;/a&gt; / &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_ISA&#34;&gt;IBM Power ISA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Z/Architecture&#34;&gt;IBMz z/Architecture (s390x)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/RISC-V&#34;&gt;RISC-V 64-bit (RV64)&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Arm 64-bit Architecture (AArch64), Power ISA (PPC64), IBMz (s390x), and RISC-V (RV64) support is &lt;strong&gt;experimental&lt;/strong&gt; with limited testing validation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The library is optimized for the following CPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intel Atom(R) processors (at least Intel SSE4.1 support is required)&lt;/li&gt; &#xA; &lt;li&gt;Intel Core(TM) processors (at least Intel SSE4.1 support is required)&lt;/li&gt; &#xA; &lt;li&gt;Intel Xeon(R) processor E3, E5, and E7 family (formerly Sandy Bridge, Ivy Bridge, Haswell, and Broadwell)&lt;/li&gt; &#xA; &lt;li&gt;Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, and Cooper Lake)&lt;/li&gt; &#xA; &lt;li&gt;future Intel Xeon Scalable processor (code name Sapphire Rapids)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;On a CPU based on Intel 64 or on AMD64 architecture, oneDNN detects the instruction set architecture (ISA) at runtime and uses just-in-time (JIT) code generation to deploy the code optimized for the latest supported ISA. Future ISAs may have initial support in the library disabled by default and require the use of run-time controls to enable them. See &lt;a href=&#34;https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html&#34;&gt;CPU dispatcher control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;On a CPU based on Arm AArch64 architecture, oneDNN can be built with Arm Compute Library integration. Compute Library is an open-source library for machine learning applications and provides AArch64 optimized implementations of core functions. This functionality currently requires that Compute Library is downloaded and built separately, see &lt;a href=&#34;https://oneapi-src.github.io/oneDNN/dev_guide_build.html&#34;&gt;Build from Source&lt;/a&gt;. oneDNN is only compatible with Compute Library versions 22.02 or later.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;On macOS, applications that use oneDNN may need to request special entitlements if they use the hardened runtime. See the &lt;a href=&#34;https://oneapi-src.github.io/oneDNN/dev_guide_link.html&#34;&gt;linking guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The library is optimized for the following GPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intel Processor Graphics based on Gen9, Gen9.5 and Gen11, and Gen12 architectures&lt;/li&gt; &#xA; &lt;li&gt;Intel Iris(R) Xe graphics (formerly DG1)&lt;/li&gt; &#xA; &lt;li&gt;future Intel Arc(TM) graphics (code name Alchemist and DG2)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements for Building from Source&lt;/h2&gt; &#xA;&lt;p&gt;oneDNN supports systems meeting the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Operating system with Intel 64 / Arm 64 / Power / IBMz architecture support&lt;/li&gt; &#xA; &lt;li&gt;C++ compiler with C++11 standard support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmake.org/download/&#34;&gt;CMake&lt;/a&gt; 2.8.12 or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/arm-software/ComputeLibrary&#34;&gt;Arm Compute Library&lt;/a&gt; for builds using Compute Library on AArch64.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following tools are required to build oneDNN documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.doxygen.nl/download.html#srcbin&#34;&gt;Doxygen&lt;/a&gt; 1.8.5 or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vovkos/doxyrest&#34;&gt;Doxyrest&lt;/a&gt; 2.1.2 or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sphinx-doc.org/en/master/usage/installation.html&#34;&gt;Sphinx&lt;/a&gt; 4.0.2 or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sphinx-book-theme.readthedocs.io/en/latest&#34;&gt;sphinx-book-theme&lt;/a&gt; 0.0.41 or later&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Configurations of CPU and GPU engines may introduce additional build time dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;CPU Engine&lt;/h3&gt; &#xA;&lt;p&gt;oneDNN CPU engine is used to execute primitives on Intel Architecture Processors, 64-bit Arm Architecture (AArch64) processors, 64-bit Power ISA (PPC64) processors, IBMz (s390x), and compatible devices.&lt;/p&gt; &#xA;&lt;p&gt;The CPU engine is built by default but can be disabled at build time by setting &lt;code&gt;DNNL_CPU_RUNTIME&lt;/code&gt; to &lt;code&gt;NONE&lt;/code&gt;. In this case, GPU engine must be enabled. The CPU engine can be configured to use the OpenMP, TBB or DPCPP runtime. The following additional requirements apply:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenMP runtime requires C++ compiler with OpenMP 2.0 or later standard support&lt;/li&gt; &#xA; &lt;li&gt;TBB runtime requires &lt;a href=&#34;https://www.threadingbuildingblocks.org/&#34;&gt;Threading Building Blocks (TBB)&lt;/a&gt; 2017 or later.&lt;/li&gt; &#xA; &lt;li&gt;DPCPP runtime requires &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/dpc-compiler&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.threadingbuildingblocks.org/&#34;&gt;Threading Building Blocks (TBB)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some implementations rely on OpenMP 4.0 SIMD extensions. For the best performance results on Intel Architecture Processors we recommend using the Intel C++ Compiler.&lt;/p&gt; &#xA;&lt;h3&gt;GPU Engine&lt;/h3&gt; &#xA;&lt;p&gt;Intel Processor Graphics and Xe Architecture graphics are supported by the oneDNN GPU engine. The GPU engine is disabled in the default build configuration. The following additional requirements apply when GPU engine is enabled:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenCL runtime requires &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenCL* runtime library (OpenCL version 1.2 or later)&lt;/li&gt; &#xA;   &lt;li&gt;OpenCL driver (with kernel language support for OpenCL C 2.0 or later) with Intel subgroups and USM extensions support&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DPCPP runtime requires &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/dpc-compiler&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;OpenCL runtime library (OpenCL version 1.2 or later)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/oneapi-src/level-zero&#34;&gt;oneAPI Level Zero&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DPCPP runtime with NVIDIA GPU support requires &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/intel/llvm&#34;&gt;oneAPI DPC++ Compiler&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;NVIDIA CUDA* driver&lt;/li&gt; &#xA;   &lt;li&gt;cuBLAS 10.1 or later&lt;/li&gt; &#xA;   &lt;li&gt;cuDNN 7.6 or later&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;NVIDIA GPU support is experimental. General information, build instructions and implementation limitations is available in &lt;a href=&#34;https://github.com/oneapi-src/oneDNN/raw/master/src/gpu/nvidia/README.md&#34;&gt;NVIDIA backend readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Runtime Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;When oneDNN is built from source, the library runtime dependencies and specific versions are defined by the build environment.&lt;/p&gt; &#xA;&lt;h4&gt;Linux&lt;/h4&gt; &#xA;&lt;p&gt;Common dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GNU C Library (&lt;code&gt;libc.so&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;GNU Standard C++ Library v3 (&lt;code&gt;libstdc++.so&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Linking Library (&lt;code&gt;libdl.so&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;C Math Library (&lt;code&gt;libm.so&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;POSIX Threads Library (&lt;code&gt;libpthread.so&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Runtime-specific dependencies:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Runtime configuration&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Dependency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=OMP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GCC&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GNU OpenMP runtime (&lt;code&gt;libgomp.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=OMP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel C/C++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel OpenMP runtime (&lt;code&gt;libiomp5.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=OMP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Clang&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel OpenMP runtime (&lt;code&gt;libiomp5.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=TBB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;any&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TBB (&lt;code&gt;libtbb.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=DPCPP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler runtime (&lt;code&gt;libsycl.so&lt;/code&gt;), TBB (&lt;code&gt;libtbb.so&lt;/code&gt;), OpenCL loader (&lt;code&gt;libOpenCL.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_GPU_RUNTIME=OCL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;any&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OpenCL loader (&lt;code&gt;libOpenCL.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_GPU_RUNTIME=DPCPP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler runtime (&lt;code&gt;libsycl.so&lt;/code&gt;), OpenCL loader (&lt;code&gt;libOpenCL.so&lt;/code&gt;), oneAPI Level Zero loader (&lt;code&gt;libze_loader.so&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;Common dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Microsoft Visual C++ Redistributable (&lt;code&gt;msvcrt.dll&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Runtime-specific dependencies:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Runtime configuration&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Dependency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=OMP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Microsoft Visual C++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;No additional requirements&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=OMP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel C/C++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel OpenMP runtime (&lt;code&gt;iomp5.dll&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=TBB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;any&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TBB (&lt;code&gt;tbb.dll&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=DPCPP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler runtime (&lt;code&gt;sycl.dll&lt;/code&gt;), TBB (&lt;code&gt;tbb.dll&lt;/code&gt;), OpenCL loader (&lt;code&gt;OpenCL.dll&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_GPU_RUNTIME=OCL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;any&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OpenCL loader (&lt;code&gt;OpenCL.dll&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_GPU_RUNTIME=DPCPP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel oneAPI DPC++ Compiler runtime (&lt;code&gt;sycl.dll&lt;/code&gt;), OpenCL loader (&lt;code&gt;OpenCL.dll&lt;/code&gt;), oneAPI Level Zero loader (&lt;code&gt;ze_loader.dll&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;macOS&lt;/h4&gt; &#xA;&lt;p&gt;Common dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System C/C++ runtime (&lt;code&gt;libc++.dylib&lt;/code&gt;, &lt;code&gt;libSystem.dylib&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Runtime-specific dependencies:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Runtime configuration&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Dependency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=OMP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel C/C++ Compiler&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Intel OpenMP runtime (&lt;code&gt;libiomp5.dylib&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DNNL_CPU_RUNTIME=TBB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;any&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TBB (&lt;code&gt;libtbb.dylib&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Validated Configurations&lt;/h3&gt; &#xA;&lt;p&gt;CPU engine was validated on RedHat* Enterprise Linux 7 with&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GNU Compiler Collection 4.8, 5.4, 6.1, 7.2, 8.1, and 9.1&lt;/li&gt; &#xA; &lt;li&gt;Clang* 3.8.1, 7.1, 8.0, and 9.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/parallel-studio-xe.html&#34;&gt;Intel C/C++ Compiler&lt;/a&gt; 19.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/dpc-compiler&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/a&gt; 2021.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;on Windows Server* 2016 with&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Microsoft Visual Studio 2015, 2017, and 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/parallel-studio-xe.html&#34;&gt;Intel C/C++ Compiler&lt;/a&gt; 19.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/dpc-compiler&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/a&gt; 2021.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;on macOS 10.13 (High Sierra) with&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple LLVM version 9.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/parallel-studio-xe.html&#34;&gt;Intel C/C++ Compiler&lt;/a&gt; 19.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;GPU engine was validated on Ubuntu* 20.04 with&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GNU Compiler Collection 7.2, 8.1, and 9.1&lt;/li&gt; &#xA; &lt;li&gt;Clang 3.8.1, 7.1, 8.0, and 9.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/parallel-studio-xe.html&#34;&gt;Intel C/C++ Compiler&lt;/a&gt; 19.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/dpc-compiler&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/a&gt; 2021.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dgpu-docs.intel.com/index.html&#34;&gt;Intel Software for General Purpose GPU capabilities&lt;/a&gt; latest stable version available at the time of release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;on Windows Server 2019 with&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Microsoft Visual Studio 2015, 2017, and 2019&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/parallel-studio-xe.html&#34;&gt;Intel C/C++ Compiler&lt;/a&gt; 19.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/oneapi/dpc-compiler&#34;&gt;Intel oneAPI DPC++ Compiler&lt;/a&gt; 2021.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://downloadcenter.intel.com/download/29808/Intel-Graphics-Windows-10-DCH-Drivers&#34;&gt;Intel Graphics - Windows 10 DCH Drivers&lt;/a&gt; latest stable version available at the time of release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements for Pre-built Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the README included in the corresponding binary package.&lt;/p&gt; &#xA;&lt;h1&gt;Applications Enabled with oneDNN&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mxnet.apache.org&#34;&gt;Apache* MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://singa.apache.org&#34;&gt;Apache* SINGA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deeplearning4j.org&#34;&gt;DeepLearning4J*&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/flashlight&#34;&gt;Flashlight*&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cselab/korali&#34;&gt;Korali&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mathworks.com/help/deeplearning/&#34;&gt;MATLAB* Deep Learning Toolbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://01.org/openvinotoolkit&#34;&gt;OpenVINO(TM) toolkit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.paddlepaddle.org&#34;&gt;PaddlePaddle*&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch*&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;Tensorflow*&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;Please submit your questions, feature requests, and bug reports on the &lt;a href=&#34;https://github.com/oneapi-src/oneDNN/issues&#34;&gt;GitHub issues&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;p&gt;You may reach out to project maintainers privately at &lt;a href=&#34;mailto:dnnl.maintainers@intel.com&#34;&gt;dnnl.maintainers@intel.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We welcome community contributions to oneDNN. If you have an idea on how to improve the library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For changes impacting the public API or library overall, such as adding new primitives or changes to the architecture, submit an &lt;a href=&#34;https://github.com/oneapi-src/oneDNN/tree/rfcs&#34;&gt;RFC pull request&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ensure that the changes are consistent with the &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/CONTRIBUTING.md#code-contribution-guidelines&#34;&gt;code contribution guidelines&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/CONTRIBUTING.md#coding-standards&#34;&gt;coding standards&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ensure that you can build the product and run all the examples with your patch.&lt;/li&gt; &#xA; &lt;li&gt;Submit a &lt;a href=&#34;https://github.com/oneapi-src/oneDNN/pulls&#34;&gt;pull request&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For additional details, see &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/CODE_OF_CONDUCT.md&#34;&gt;Contributor Covenant&lt;/a&gt; code of conduct.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;oneDNN is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt;. Refer to the &#34;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&#34; file for the full license text and copyright notice.&lt;/p&gt; &#xA;&lt;p&gt;This distribution includes third party software governed by separate license terms.&lt;/p&gt; &#xA;&lt;p&gt;3-clause BSD license:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/herumi/xbyak&#34;&gt;Xbyak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/googletest&#34;&gt;gtest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel/ittapi&#34;&gt;Instrumentation and Tracing Technology API (ITT API)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kitware/CMake&#34;&gt;CMake&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;2-clause BSD license:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sphinx-doc.org/&#34;&gt;Sphinx&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Apache License Version 2.0:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fujitsu/xbyak_aarch64&#34;&gt;Xbyak_aarch64&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Boost Software License, Version 1.0:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.boost.org/&#34;&gt;Boost C++ Libraries&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;MIT License:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel/compute-runtime&#34;&gt;Intel Graphics Compute Runtime for oneAPI Level Zero and OpenCL Driver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel/intel-graphics-compiler&#34;&gt;Intel Graphics Compiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oneapi-src/level-zero&#34;&gt;oneAPI Level Zero&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vovkos/doxyrest&#34;&gt;Doxyrest&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This third party software, even if included with the distribution of the Intel software, may be governed by separate license terms, including without limitation, third party license terms, other Intel software license terms, and open source software license terms. These separate license terms govern your use of the third party programs as set forth in the &#34;&lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/THIRD-PARTY-PROGRAMS&#34;&gt;THIRD-PARTY-PROGRAMS&lt;/a&gt;&#34; file.&lt;/p&gt; &#xA;&lt;h1&gt;Security&lt;/h1&gt; &#xA;&lt;p&gt;See Intel&#39;s &lt;a href=&#34;https://www.intel.com/content/www/us/en/security-center/default.html&#34;&gt;Security Center&lt;/a&gt; for information on how to report a potential security issue or vulnerability.&lt;/p&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://raw.githubusercontent.com/oneapi-src/oneDNN/master/SECURITY.md&#34;&gt;Security Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Trademark Information&lt;/h1&gt; &#xA;&lt;p&gt;Intel, the Intel logo, Arc, Intel Atom, Intel Core, Iris, OpenVINO, the OpenVINO logo, Pentium, VTune, and Xeon are trademarks of Intel Corporation or its subsidiaries.&lt;/p&gt; &#xA;&lt;p&gt;* Other names and brands may be claimed as the property of others.&lt;/p&gt; &#xA;&lt;p&gt;Microsoft, Windows, and the Windows logo are trademarks, or registered trademarks of Microsoft Corporation in the United States and/or other countries.&lt;/p&gt; &#xA;&lt;p&gt;OpenCL and the OpenCL logo are trademarks of Apple Inc. used by permission by Khronos.&lt;/p&gt; &#xA;&lt;p&gt;(C) Intel Corporation&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rui314/mold</title>
    <updated>2022-06-18T01:30:00Z</updated>
    <id>tag:github.com,2022-06-18:/rui314/mold</id>
    <link href="https://github.com/rui314/mold" rel="alternate"></link>
    <summary type="html">&lt;p&gt;mold: A Modern Linker&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;mold: A Modern Linker&lt;/h1&gt; &#xA;&lt;p&gt;mold is a faster drop-in replacement for existing Unix linkers. It is several times faster than the LLVM lld linker, the second-fastest open-source linker which I originally created a few years ago. mold is designed to increase developer productivity by reducing build time, especially in rapid debug-edit-rebuild cycles.&lt;/p&gt; &#xA;&lt;p&gt;Here is a performance comparison of GNU gold, LLVM lld, and mold for linking final debuginfo-enabled executables of major large programs on a simulated 8-core 16-threads machine.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rui314/mold/main/docs/comparison.png&#34; alt=&#34;Link speed comparison&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Program (linker output size)&lt;/th&gt; &#xA;   &lt;th&gt;GNU gold&lt;/th&gt; &#xA;   &lt;th&gt;LLVM lld&lt;/th&gt; &#xA;   &lt;th&gt;mold&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chrome 96 (1.89 GiB)&lt;/td&gt; &#xA;   &lt;td&gt;53.86s&lt;/td&gt; &#xA;   &lt;td&gt;11.74s&lt;/td&gt; &#xA;   &lt;td&gt;2.21s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Clang 13 (3.18 GiB)&lt;/td&gt; &#xA;   &lt;td&gt;64.12s&lt;/td&gt; &#xA;   &lt;td&gt;5.82s&lt;/td&gt; &#xA;   &lt;td&gt;2.90s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Firefox 89 libxul (1.64 GiB)&lt;/td&gt; &#xA;   &lt;td&gt;32.95s&lt;/td&gt; &#xA;   &lt;td&gt;6.80s&lt;/td&gt; &#xA;   &lt;td&gt;1.42s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;mold is so fast that it is only 2x &lt;em&gt;slower&lt;/em&gt; than &lt;code&gt;cp&lt;/code&gt; on the same machine. Feel free to &lt;a href=&#34;https://github.com/rui314/mold/issues&#34;&gt;file a bug&lt;/a&gt; if you find mold is not faster than other linkers.&lt;/p&gt; &#xA;&lt;p&gt;mold currently supports x86-64, i386, ARM32, ARM64 and 64-bit RISC-V.&lt;/p&gt; &#xA;&lt;h2&gt;Why does the speed of linking matter?&lt;/h2&gt; &#xA;&lt;p&gt;If you are using a compiled language such as C, C++ or Rust, a build consists of two phases. In the first phase, a compiler compiles source files into object files (&lt;code&gt;.o&lt;/code&gt; files). In the second phase, a linker takes all object files to combine them into a single executable or a shared library file.&lt;/p&gt; &#xA;&lt;p&gt;The second phase takes a long time if your build output is large. mold can make it faster, saving your time and keeping you from being distracted while waiting for a long build to finish. The difference is most noticeable when you are in rapid debug-edit-rebuild cycles.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Binary packages for the following systems are currently available.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repology.org/project/mold/versions&#34;&gt;&lt;img src=&#34;https://repology.org/badge/vertical-allrepos/mold.svg?sanitize=true&#34; alt=&#34;Packaging status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;p&gt;mold is written in C++20, so if you build mold yourself, you need a recent version of a C++ compiler and a C++ standard library. GCC 10.2 or Clang 12.0.0 (or later) as well as libstdc++ 10 or libc++ 7 (or later) are recommended.&lt;/p&gt; &#xA;&lt;h3&gt;Install dependencies&lt;/h3&gt; &#xA;&lt;p&gt;To install build dependencies, run &lt;code&gt;./install-build-deps.sh&lt;/code&gt; in this directory. It recognizes your Linux distribution and tries to install necessary packages. You may want to run it as root.&lt;/p&gt; &#xA;&lt;h3&gt;Compile mold&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/rui314/mold.git&#xA;cd mold&#xA;git checkout v1.2.1&#xA;make -j$(nproc) CXX=clang++&#xA;sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need to pass a C++20 compiler command name to &lt;code&gt;make&lt;/code&gt;. In the above case, &lt;code&gt;clang++&lt;/code&gt; is passed. If it doesn&#39;t work for you, try a specific version of a compiler such as &lt;code&gt;g++-10&lt;/code&gt; or &lt;code&gt;clang++-12&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;mold&lt;/code&gt; is installed to &lt;code&gt;/usr/local/bin&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t use a recent enough Linux distribution, or if for any reason &lt;code&gt;make&lt;/code&gt; in the above commands doesn&#39;t work for you, you can use Docker to build it in a Docker environment. To do so, just run &lt;code&gt;./dist.sh&lt;/code&gt; in this directory instead of running &lt;code&gt;make -j$(nproc)&lt;/code&gt;. The shell script pulls a Docker image, builds mold and auxiliary files inside it, and packs them into a single tar file &lt;code&gt;mold-$version-$arch-linux.tar.gz&lt;/code&gt;. You can extract the tar file anywhere and use &lt;code&gt;mold&lt;/code&gt; executable in it.&lt;/p&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;A classic way to use mold&lt;/summary&gt; &#xA; &lt;p&gt;On Unix, the linker command (which is usually &lt;code&gt;/usr/bin/ld&lt;/code&gt;) is invoked indirectly by the compiler driver (which is usually &lt;code&gt;cc&lt;/code&gt;, &lt;code&gt;gcc&lt;/code&gt; or &lt;code&gt;clang&lt;/code&gt;), which is typically in turn indirectly invoked by &lt;code&gt;make&lt;/code&gt; or some other build system command.&lt;/p&gt; &#xA; &lt;p&gt;If you can specify an additional command line option to your compiler driver by modifying build system&#39;s config files, add one of the following flags to use &lt;code&gt;mold&lt;/code&gt; instead of &lt;code&gt;/usr/bin/ld&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Clang: pass &lt;code&gt;-fuse-ld=mold&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;GCC 12.1.0 or later: pass &lt;code&gt;-fuse-ld=mold&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;GCC before 12.1.0: &lt;code&gt;-fuse-ld&lt;/code&gt; does not accept &lt;code&gt;mold&lt;/code&gt; as a valid argument, so you need to use &lt;code&gt;-B&lt;/code&gt; option instead. &lt;code&gt;-B&lt;/code&gt; is an option to tell GCC where to look for external commands such as &lt;code&gt;ld&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If you have installed mold with &lt;code&gt;make install&lt;/code&gt;, there should be a directory named &lt;code&gt;/usr/libexec/mold&lt;/code&gt; (or &lt;code&gt;/usr/local/libexec/mold&lt;/code&gt;, depending on your &lt;code&gt;$PREFIX&lt;/code&gt;), and &lt;code&gt;ld&lt;/code&gt; command should be there. The &lt;code&gt;ld&lt;/code&gt; is actually a symlink to &lt;code&gt;mold&lt;/code&gt;. So, all you need is to pass &lt;code&gt;-B/usr/libexec/mold&lt;/code&gt; (or &lt;code&gt;-B/usr/local/libexec/mold&lt;/code&gt;) to GCC.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;If you haven&#39;t installed &lt;code&gt;mold&lt;/code&gt; to any &lt;code&gt;$PATH&lt;/code&gt;, you can still pass &lt;code&gt;-fuse-ld=/absolute/path/to/mold&lt;/code&gt; to clang to use mold. GCC does not take an absolute path as an argument for &lt;code&gt;-fuse-ld&lt;/code&gt; though.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;If you are using Rust&lt;/summary&gt; &#xA; &lt;p&gt;Create &lt;code&gt;.cargo/config.toml&lt;/code&gt; in your project directory with the following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;[target.x86_64-unknown-linux-gnu]&#xA;linker = &#34;clang&#34;&#xA;rustflags = [&#34;-C&#34;, &#34;link-arg=-fuse-ld=/path/to/mold&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;where &lt;code&gt;/path/to/mold&lt;/code&gt; is an absolute path to &lt;code&gt;mold&lt;/code&gt; exectuable. Please make sure you have installed &lt;code&gt;clang&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;If you want to use mold for all projects, put the above snippet to &lt;code&gt;~/.cargo/config.toml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;mold -run&lt;/summary&gt; &#xA; &lt;p&gt;It is sometimes very hard to pass an appropriate command line option to &lt;code&gt;cc&lt;/code&gt; to specify an alternative linker. To deal with the situation, mold has a feature to intercept all invocations of &lt;code&gt;ld&lt;/code&gt;, &lt;code&gt;ld.lld&lt;/code&gt; or &lt;code&gt;ld.gold&lt;/code&gt; and redirect it to itself. To use the feature, run &lt;code&gt;make&lt;/code&gt; (or another build command) as a subcommand of mold as follows:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mold -run make &amp;lt;make-options-if-any&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Internally, mold invokes a given command with &lt;code&gt;LD_PRELOAD&lt;/code&gt; environment variable set to its companion shared object file. The shared object file intercepts all function calls to &lt;code&gt;exec(3)&lt;/code&gt;-family functions to replace &lt;code&gt;argv[0]&lt;/code&gt; with &lt;code&gt;mold&lt;/code&gt; if it is &lt;code&gt;ld&lt;/code&gt;, &lt;code&gt;ld.gold&lt;/code&gt; or &lt;code&gt;ld.lld&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;GitHub Actions&lt;/summary&gt; &#xA; &lt;p&gt;You can use our &lt;a href=&#34;https://github.com/rui314/setup-mold&#34;&gt;setup-mold&lt;/a&gt; GitHub Action to speed up GitHub-hosted continuous build. GitHub Actions runs on a two-core machine, but mold is still significantly faster than the default GNU linker there especially when a program being linked is large.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Verify that you are using mold&lt;/summary&gt; &#xA; &lt;p&gt;mold leaves its identification string in &lt;code&gt;.comment&lt;/code&gt; section in an output file. You can print it out to verify that you are actually using mold.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ readelf -p .comment &amp;lt;executable-file&amp;gt;&#xA;&#xA;String dump of section &#39;.comment&#39;:&#xA;  [     0]  GCC: (Ubuntu 10.2.0-5ubuntu1~20.04) 10.2.0&#xA;  [    2b]  mold 9a1679b47d9b22012ec7dfbda97c8983956716f7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If &lt;code&gt;mold&lt;/code&gt; is in &lt;code&gt;.comment&lt;/code&gt;, the file is created by mold.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Why is mold so fast?&lt;/h2&gt; &#xA;&lt;p&gt;One reason is because it simply uses faster algorithms and efficient data structures than other linkers do. The other reason is that the new linker is highly parallelized.&lt;/p&gt; &#xA;&lt;p&gt;Here is a side-by-side comparison of per-core CPU usage of lld (left) and mold (right). They are linking the same program, Chromium executable.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rui314/mold/main/docs/htop.gif&#34; alt=&#34;CPU usage comparison in htop animation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As you can see, mold uses all available cores throughout its execution and finishes quickly. On the other hand, lld failed to use available cores most of the time. In this demo, the maximum parallelism is artificially capped to 16 so that the bars fit in the GIF.&lt;/p&gt; &#xA;&lt;p&gt;For details, please read &lt;a href=&#34;https://raw.githubusercontent.com/rui314/mold/main/docs/design.md&#34;&gt;design notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;mold is available under AGPL. Note that that does not mean that you have to license your program under AGPL if you use mold to link your program. An output of the mold linker is a derived work of the object files and libraries you pass to the linker but not a derived work of the mold linker itself.&lt;/p&gt; &#xA;&lt;p&gt;Besides that, you can also buy a commercial, non-AGPL license with technical support from our company, Blue Whale Systems PTE LTD. If you are a big company, please consider obtaining it before making hundreds or thousands of developers of your company to depend on mold. mold is mostly a single-person open-source project, and just like other open-source projects, we are not legally obligated to keep maintaining it. A legally-binding commercial license contract addresses the concern. By purchasing a license, you are guaranteed that mold will be maintained for you. Please &lt;a href=&#34;mailto:contact@bluewhale.systems&#34;&gt;contact us&lt;/a&gt; for a commercial license inquiry.&lt;/p&gt;</summary>
  </entry>
</feed>