<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-11T01:32:13Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google/cdc-file-transfer</title>
    <updated>2023-01-11T01:32:13Z</updated>
    <id>tag:github.com,2023-01-11:/google/cdc-file-transfer</id>
    <link href="https://github.com/google/cdc-file-transfer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tools for synching and streaming files from Windows to Linux&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CDC File Transfer&lt;/h1&gt; &#xA;&lt;p&gt;Born from the ashes of Stadia, this repository contains tools for syncing and streaming files from Windows to Linux. They are based on Content Defined Chunking (CDC), in particular &lt;a href=&#34;https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia&#34;&gt;FastCDC&lt;/a&gt;, to split up files into chunks.&lt;/p&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;At Stadia, game developers had access to Linux cloud instances to run games. Most developers wrote their games on Windows, though. Therefore, they needed a way to make them available on the remote Linux instance.&lt;/p&gt; &#xA;&lt;p&gt;As developers had SSH access to those instances, they could use &lt;code&gt;scp&lt;/code&gt; to copy the game content. However, this was impractical, especially with the shift to working from home during the pandemic with sub-par internet connections. &lt;code&gt;scp&lt;/code&gt; always copies full files, there is no &#34;delta mode&#34; to copy only the things that changed, it is slow for many small files, and there is no fast compression.&lt;/p&gt; &#xA;&lt;p&gt;To help this situation, we developed two tools, &lt;code&gt;cdc_rsync&lt;/code&gt; and &lt;code&gt;cdc_stream&lt;/code&gt;, which enable developers to quickly iterate on their games without repeatedly incurring the cost of transmitting dozens of GBs.&lt;/p&gt; &#xA;&lt;h2&gt;CDC RSync&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; is a tool to sync files from a Windows machine to a Linux device, similar to the standard Linux &lt;a href=&#34;https://linux.die.net/man/1/rsync&#34;&gt;rsync&lt;/a&gt;. It is basically a copy tool, but optimized for the case where there is already an old version of the files available in the target directory.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It quickly skips files if timestamp and file size match.&lt;/li&gt; &#xA; &lt;li&gt;It uses fast compression for all data transfer.&lt;/li&gt; &#xA; &lt;li&gt;If a file changed, it determines which parts changed and only transfers the differences.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/cdc_rsync_recursive_upload_demo.gif&#34; alt=&#34;cdc_rsync demo&#34; width=&#34;688&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The remote diffing algorithm is based on CDC. In our tests, it is up to 30x faster than the one used in &lt;code&gt;rsync&lt;/code&gt; (1500 MB/s vs 50 MB/s).&lt;/p&gt; &#xA;&lt;p&gt;The following chart shows a comparison of &lt;code&gt;cdc_rsync&lt;/code&gt; and Linux &lt;code&gt;rsync&lt;/code&gt; running under Cygwin on Windows. The test data consists of 58 development builds of some game provided to us for evaluation purposes. The builds are 40-45 GB large. For this experiment, we uploaded the first build, then synced the second build with each of the two tools and measured the time. For example, syncing from build 1 to build 2 took 210 seconds with the Cygwin &lt;code&gt;rsync&lt;/code&gt;, but only 75 seconds with &lt;code&gt;cdc_rsync&lt;/code&gt;. The three outliers are probably feature drops from another development branch, where the delta was much higher. Overall, &lt;code&gt;cdc_rsync&lt;/code&gt; syncs files about &lt;strong&gt;3 times faster&lt;/strong&gt; than Cygwin &lt;code&gt;rsync&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/cdc_rsync_vs_cygwin_rsync.png&#34; alt=&#34;Comparison of cdc_rsync and Linux rsync running in Cygwin&#34; width=&#34;753&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We also ran the experiment with the native Linux &lt;code&gt;rsync&lt;/code&gt;, i.e syncing Linux to Linux, to rule out issues with Cygwin. Linux &lt;code&gt;rsync&lt;/code&gt; performed on average 35% worse than Cygwin &lt;code&gt;rsync&lt;/code&gt;, which can be attributed to CPU differences. We did not include it in the figure because of this, but you can find it &lt;a href=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/cdc_rsync_vs_cygwin_rsync_vs_linux_rsync.png&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How does it work and why is it faster?&lt;/h3&gt; &#xA;&lt;p&gt;The standard Linux &lt;code&gt;rsync&lt;/code&gt; splits a file into fixed-size chunks of typically several KB.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/fixed_size_chunks.png&#34; alt=&#34;Linux rsync uses fixed size chunks&#34; width=&#34;258&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If the file is modified in the middle, e.g. by inserting &lt;code&gt;xxxx&lt;/code&gt; after &lt;code&gt;567&lt;/code&gt;, this usually means that &lt;span style=&#34;color: red&#34;&gt;the modified chunks as well as all subsequent chunks&lt;/span&gt; change.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/fixed_size_chunks_inserted.png&#34; alt=&#34;Fixed size chunks after inserting data&#34; width=&#34;301&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The standard &lt;code&gt;rsync&lt;/code&gt; algorithm hashes the chunks of the remote &#34;old&#34; file and sends the hashes to the local device. The local device then figures out which parts of the &#34;new&#34; file matches known chunks.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/linux_rsync_animation.gif&#34; alt=&#34;Syncing a file with the standard Linux rsync&#34; width=&#34;855&#34;&gt; &lt;br&gt; Standard rsync algorithm &lt;/p&gt; &#xA;&lt;p&gt;This is a simplification. The actual algorithm is more complicated and uses two hashes, a weak rolling hash and a strong hash, see &lt;a href=&#34;https://rsync.samba.org/tech_report/&#34;&gt;here&lt;/a&gt; for a great overview. What makes &lt;code&gt;rsync&lt;/code&gt; relatively slow is the &#34;no match&#34; situation where the rolling hash does not match any remote hash, and the algorithm has to roll the hash forward and perform a hash map lookup for each byte. &lt;code&gt;rsync&lt;/code&gt; goes to &lt;a href=&#34;https://github.com/librsync/librsync/raw/master/src/hashtable.h&#34;&gt;great lengths&lt;/a&gt; optimizing lookups.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; does not use fixed-size chunks, but instead variable-size, content-defined chunks. That means, chunk boundaries are determined by the &lt;em&gt;local content&lt;/em&gt; of the file, in practice a 64 byte sliding window. For more details, see &lt;a href=&#34;https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia&#34;&gt;the FastCDC paper&lt;/a&gt; or take a look at &lt;a href=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/fastcdc/fastcdc.h&#34;&gt;our implementation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/variable_size_chunks.png&#34; alt=&#34;cdc_rsync uses variable, content-defined size chunks&#34; width=&#34;260&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If the file is modified in the middle, only &lt;span style=&#34;color: red&#34;&gt;the modified chunks&lt;/span&gt;, but not &lt;span style=&#34;color: #38761d&#34;&gt;subsequent chunks&lt;/span&gt; change (unless they are less than 64 bytes away from the modifications).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/variable_size_chunks_inserted.png&#34; alt=&#34;Content-defined chunks after inserting data&#34; width=&#34;314&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Computing the chunk boundaries is cheap and involves only a left-shift, a memory lookup, an &lt;code&gt;add&lt;/code&gt; and an &lt;code&gt;and&lt;/code&gt; operation for each input byte. This is cheaper than the hash map lookup for the standard &lt;code&gt;rsync&lt;/code&gt; algorithm.&lt;/p&gt; &#xA;&lt;p&gt;Because of this, the &lt;code&gt;cdc_rsync&lt;/code&gt; algorithm is faster than the standard &lt;code&gt;rsync&lt;/code&gt;. It is also simpler. Since chunk boundaries move along with insertions or deletions, the task to match local and remote hashes is a trivial set difference operation. It does not involve a per-byte hash map lookup.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/cdc_rsync_animation.gif&#34; alt=&#34;Syncing a file with cdc_rsync&#34; width=&#34;857&#34;&gt; &lt;br&gt; cdc_rsync algorithm &lt;/p&gt; &#xA;&lt;h2&gt;CDC Stream&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;cdc_stream&lt;/code&gt; is a tool to stream files and directories from a Windows machine to a Linux device. Conceptually, it is similar to &lt;a href=&#34;https://github.com/libfuse/sshfs&#34;&gt;sshfs&lt;/a&gt;, but it is optimized for read speed.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It caches streamed data on the Linux device.&lt;/li&gt; &#xA; &lt;li&gt;If a file is re-read on Linux after it changed on Windows, only the differences are streamed again. The rest is read from the cache.&lt;/li&gt; &#xA; &lt;li&gt;Stat operations are very fast since the directory metadata (filenames, permissions etc.) is provided in a streaming-friendly way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To efficiently determine which parts of a file changed, the tool uses the same CDC-based diffing algorithm as &lt;code&gt;cdc_rsync&lt;/code&gt;. Changes to Windows files are almost immediately reflected on Linux, with a delay of roughly (0.5s + 0.7s x total size of changed files in GB).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/cdc_stream_demo.gif&#34; alt=&#34;cdc_stream demo&#34; width=&#34;688&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The tool does not support writing files back from Linux to Windows; the Linux directory is readonly.&lt;/p&gt; &#xA;&lt;p&gt;The following chart compares times from starting a game to reaching the menu. In one case, the game is streamed via &lt;code&gt;sshfs&lt;/code&gt;, in the other case we use &lt;code&gt;cdc_stream&lt;/code&gt;. Overall, we see a &lt;strong&gt;2x to 5x speedup&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/cdc-file-transfer/main/docs/cdc_stream_vs_sshfs.png&#34; alt=&#34;Comparison of cdc_stream and sshfs&#34; width=&#34;752&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;Download the precompiled binaries from the &lt;a href=&#34;https://github.com/google/cdc-file-transfer/releases&#34;&gt;latest release&lt;/a&gt;. We currently provide Linux binaries compiled on &lt;a href=&#34;https://github.com/actions/runner-images&#34;&gt;Github&#39;s latest Ubuntu&lt;/a&gt; version. If the binaries work for you, you can skip the following two sections.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, the project can be built from source. Some binaries have to be built on Windows, some on Linux.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To build the tools from source, the following steps have to be executed on &lt;strong&gt;both Windows and Linux&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and install Bazel from &lt;a href=&#34;https://bazel.build/install&#34;&gt;here&lt;/a&gt;. See &lt;a href=&#34;https://github.com/google/cdc-file-transfer/actions&#34;&gt;workflow logs&lt;/a&gt; for the currently used version.&lt;/li&gt; &#xA; &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code&gt;git clone https://github.com/google/cdc-file-transfer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Initialize submodules. &lt;pre&gt;&lt;code&gt;cd cdc-file-transfer&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Finally, install an SSH client on the Windows device if not present. The file transfer tools require &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;scp.exe&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;The two tools can be built and used independently.&lt;/p&gt; &#xA;&lt;h3&gt;CDC RSync&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build Linux components &lt;pre&gt;&lt;code&gt;bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build Windows components &lt;pre&gt;&lt;code&gt;bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Copy the Linux build output file &lt;code&gt;cdc_rsync_server&lt;/code&gt; from &lt;code&gt;bazel-bin/cdc_rsync_server&lt;/code&gt; on the Linux system to &lt;code&gt;bazel-bin\cdc_rsync&lt;/code&gt; on the Windows machine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CDC Stream&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build Linux components &lt;pre&gt;&lt;code&gt;bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build Windows components &lt;pre&gt;&lt;code&gt;bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Copy the Linux build output files &lt;code&gt;cdc_fuse_fs&lt;/code&gt; and &lt;code&gt;libfuse.so&lt;/code&gt; from &lt;code&gt;bazel-bin/cdc_fuse_fs&lt;/code&gt; on the Linux system to &lt;code&gt;bazel-bin\cdc_stream&lt;/code&gt; on the Windows machine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The tools require a setup where you can use SSH and SCP from the Windows machine to the Linux device without entering a password, e.g. by using key-based authentication.&lt;/p&gt; &#xA;&lt;h3&gt;Configuring SSH and SCP&lt;/h3&gt; &#xA;&lt;p&gt;By default, the tools search &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;scp.exe&lt;/code&gt; from the path environment variable. If you can run the following commands in a Windows cmd without entering your password, you are all set:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ssh user@linux.device.com&#xA;scp somefile.txt user@linux.device.com:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, &lt;code&gt;user&lt;/code&gt; is the Linux user and &lt;code&gt;linux.device.com&lt;/code&gt; is the Linux host to SSH into or copy the file to.&lt;/p&gt; &#xA;&lt;p&gt;If additional arguments are required, it is recommended to provide an SSH config file. By default, both &lt;code&gt;ssh.exe&lt;/code&gt; and &lt;code&gt;scp.exe&lt;/code&gt; use the file at &lt;code&gt;%USERPROFILE%\.ssh\config&lt;/code&gt; on Windows, if it exists. A possible config file that sets a username, a port, an identity file and a known host file could look as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Host linux_device&#xA;&#x9;HostName linux.device.com&#xA;&#x9;User user&#xA;&#x9;Port 12345&#xA;&#x9;IdentityFile C:\path\to\id_rsa&#xA;&#x9;UserKnownHostsFile C:\path\to\known_hosts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If &lt;code&gt;ssh.exe&lt;/code&gt; or &lt;code&gt;scp.exe&lt;/code&gt; cannot be found, you can specify the full paths via the command line arguments &lt;code&gt;--ssh-command&lt;/code&gt; and &lt;code&gt;--scp-command&lt;/code&gt; for &lt;code&gt;cdc_rsync&lt;/code&gt; and &lt;code&gt;cdc_stream start&lt;/code&gt; (see below), or set the environment variables &lt;code&gt;CDC_SSH_COMMAND&lt;/code&gt; and &lt;code&gt;CDC_SCP_COMMAND&lt;/code&gt;, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;set CDC_SSH_COMMAND=&#34;C:\path with space\to\ssh.exe&#34;&#xA;set CDC_SCP_COMMAND=&#34;C:\path with space\to\scp.exe&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you can also specify SSH configuration via the environment variables instead of using a config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts&#xA;set CDC_SCP_COMMAND=C:\path\to\scp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the small &lt;code&gt;-p&lt;/code&gt; for &lt;code&gt;ssh.exe&lt;/code&gt; and the capital &lt;code&gt;-P&lt;/code&gt; for &lt;code&gt;scp.exe&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Google Specific&lt;/h4&gt; &#xA;&lt;p&gt;For Google internal usage, set the following environment variables to enable SSH authentication using a Google security key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe&#xA;set CDC_SCP_COMMAND=C:\gnubby\bin\scp.exe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you will have to touch the security key multiple times during the first run. Subsequent runs only require a single touch.&lt;/p&gt; &#xA;&lt;h3&gt;CDC RSync&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; is used similar to &lt;code&gt;scp&lt;/code&gt; or the Linux &lt;code&gt;rsync&lt;/code&gt; command. To sync a single Windows file &lt;code&gt;C:\path\to\file.txt&lt;/code&gt; to the home directory &lt;code&gt;~&lt;/code&gt; on the Linux device &lt;code&gt;linux.device.com&lt;/code&gt;, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_rsync C:\path\to\file.txt user@linux.device.com:~&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; understands the usual Windows wildcards &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;?&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_rsync C:\path\to\*.txt user@linux.device.com:~&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To sync the contents of the Windows directory &lt;code&gt;C:\path\to\assets&lt;/code&gt; recursively to &lt;code&gt;~/assets&lt;/code&gt; on the Linux device, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get per file progress, add &lt;code&gt;-v&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CDC Stream&lt;/h3&gt; &#xA;&lt;p&gt;To stream the Windows directory &lt;code&gt;C:\path\to\assets&lt;/code&gt; to &lt;code&gt;~/assets&lt;/code&gt; on the Linux device, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_stream start C:\path\to\assets user@linux.device.com:~/assets&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This makes all files and directories in &lt;code&gt;C:\path\to\assets&lt;/code&gt; available on &lt;code&gt;~/assets&lt;/code&gt; immediately, as if it were a local copy. However, data is streamed from Windows to Linux as files are accessed.&lt;/p&gt; &#xA;&lt;p&gt;To stop the streaming session, enter&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_stream stop user@linux.device.com:~/assets&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command also accepts wildcards. For instance,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_stream stop user@*:*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;stops all existing streaming sessions for the given user.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;On first run, &lt;code&gt;cdc_stream&lt;/code&gt; starts a background service, which does all the work. The &lt;code&gt;cdc_stream start&lt;/code&gt; and &lt;code&gt;cdc_stream stop&lt;/code&gt; commands are just RPC clients that talk to the service.&lt;/p&gt; &#xA;&lt;p&gt;The service logs to &lt;code&gt;%APPDATA%\cdc-file-transfer\logs&lt;/code&gt; by default. The logs are useful to investigate issues with asset streaming. To pass custom arguments, or to debug the service, create a JSON config file at &lt;code&gt;%APPDATA%\cdc-file-transfer\cdc_stream.json&lt;/code&gt; with command line flags. For instance,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{ &#34;verbosity&#34;:3 }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;instructs the service to log debug messages. Try &lt;code&gt;cdc_stream start-service -h&lt;/code&gt; for a list of available flags. Alternatively, run the service manually with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cdc_stream start-service&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and pass the flags as command line arguments. When you run the service manually, the flag &lt;code&gt;--log-to-stdout&lt;/code&gt; is particularly useful as it logs to the console instead of to the file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cdc_rsync&lt;/code&gt; always logs to the console. To increase log verbosity, pass &lt;code&gt;-vvv&lt;/code&gt; for debug logs or &lt;code&gt;-vvvv&lt;/code&gt; for verbose logs.&lt;/p&gt; &#xA;&lt;p&gt;For both sync and stream, the debug logs contain all SSH and SCP commands that are attempted to run, which is very useful for troubleshooting. If a command fails unexpectedly, copy it and run it in isolation. Pass &lt;code&gt;-vv&lt;/code&gt; or &lt;code&gt;-vvv&lt;/code&gt; for additional debug output.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sebastianstarke/AI4Animation</title>
    <updated>2023-01-11T01:32:13Z</updated>
    <id>tag:github.com,2023-01-11:/sebastianstarke/AI4Animation</id>
    <link href="https://github.com/sebastianstarke/AI4Animation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bringing Characters to Life with Computer Brains in Unity&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI4Animation: Deep Learning for Character Control&lt;/h1&gt; &#xA;&lt;p&gt;This project explores the opportunities of deep learning for character animation and control as part of my Ph.D. research at the University of Edinburgh in the School of Informatics, supervised by &lt;a href=&#34;http://homepages.inf.ed.ac.uk/tkomura&#34;&gt;Taku Komura&lt;/a&gt;. Over the last couple years, this project has become a comprehensive framework for data-driven character animation, including data processing, network training and runtime control, developed in Unity3D / Tensorflow / PyTorch. This repository demonstrates using neural networks for animating biped locomotion, quadruped locomotion, and character-scene interactions with objects and the environment, plus sports and fighting games. Further advances on this research will continue being added to this project.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=wNqpSk4FhSw&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/Other/ThesisFastForward.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2022&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;DeepPhase: Periodic Autoencoders for Learning Motion Phase Manifolds&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ian-mason-134197105/&#34;&gt;Ian Mason&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, ACM Trans. Graph. 41, 4, Article 136. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Learning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Manifolds.png&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=YhH4PYEkVnY&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2022/PyTorch&#34;&gt;PAE Code &amp;amp; Demo&lt;/a&gt; - Animation Code &amp;amp; Demo (coming soon) - &lt;a href=&#34;https://www.ianxmason.com/posts/PAE/&#34;&gt;Explanation and Addendum&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=YhH4PYEkVnY&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2022/Thumbnail.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2021&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Neural Animation Layering for Synthesizing Martial Arts Movements&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/evan-yiwei-zhao-18584a105/&#34;&gt;Yiwei Zhao&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/fabio-zinno-1a77331/&#34;&gt;Fabio Zinno&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, ACM Trans. Graph. 40, 4, Article 92. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Teaser.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Layering.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Interactively synthesizing novel combinations and variations of character movements from different motion skills is a key problem in computer animation. In this research, we propose a deep learning framework to produce a large variety of martial arts movements in a controllable manner from raw motion capture data. Our method imitates animation layering using neural networks with the aim to overcome typical challenges when mixing, blending and editing movements from unaligned motion sources. The system can be used for offline and online motion generation alike, provides an intuitive interface to integrate with animator workflows, and is relevant for real-time applications such as computer games. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=SkJNxLYNwN0&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=SkJNxLYNwN0&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2021/Thumbnail.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2020&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Local Motion Phases for Learning Multi-Contact Character Movements&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/evan-yiwei-zhao-18584a105/&#34;&gt;Yiwei Zhao&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kazizaman/&#34;&gt;Kazi Zaman&lt;/a&gt;. ACM Trans. Graph. 39, 4, Article 54. &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Not sure how to align complex character movements? Tired of phase labeling? Unclear how to squeeze everything into a single phase variable? Don&#39;t worry, a solution exists! &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Court.jpg&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Controlling characters to perform a large variety of dynamic, fast-paced and quickly changing movements is a key challenge in character animation. In this research, we present a deep learning framework to interactively synthesize such animations in high quality, both from unstructured motion data and without any manual labeling. We introduce the concept of local motion phases, and show our system being able to produce various motion skills, such as ball dribbling and professional maneuvers in basketball plays, shooting, catching, avoidance, multiple locomotion modes as well as different character and object interactions, all generated under a unified framework. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=Rzj3k3yerDk&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2020&#34;&gt;Code&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2020/Demo_Windows.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2020/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Rzj3k3yerDk&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2020/Thumbnail.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH Asia 2019&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Neural State Machine for Character-Scene Interactions&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/he-zhang-148467165/&#34;&gt;He Zhang&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jun-saito/&#34;&gt;Jun Saito&lt;/a&gt;. ACM Trans. Graph. 38, 6, Article 178. &lt;/sub&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(+Joint First Authors)&lt;/sup&gt; &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_Asia_2019/Teaser.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Animating characters can be an easy or difficult task - interacting with objects is one of the latter. In this research, we present the Neural State Machine, a data-driven deep learning framework for character-scene interactions. The difficulty in such animations is that they require complex planning of periodic as well as aperiodic movements to complete a given task. Creating them in a production-ready quality is not straightforward and often very time-consuming. Instead, our system can synthesize different movements and scene interactions from motion capture data, and allows the user to seamlessly control the character in real-time from simple control commands. Since our model directly learns from the geometry, the motions can naturally adapt to variations in the scene. We show that our system can generate a large variety of movements, icluding locomotion, sitting on chairs, carrying boxes, opening doors and avoiding obstacles, all from a single model. The model is responsive, compact and scalable, and is the first of such frameworks to handle scene interaction tasks for data-driven character animation. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=7c6oQP1u2eQ&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_Asia_2019/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_Asia_2019&#34;&gt;Code &amp;amp; Demo&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_Asia_2019/MotionCapture.zip&#34;&gt;Mocap Data&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_Asia_2019/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=7c6oQP1u2eQ&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_Asia_2019/Thumbnail.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2018&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Mode-Adaptive Neural Networks for Quadruped Motion Control&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/he-zhang-148467165/&#34;&gt;He Zhang&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sebastian-starke-b281a6148/&#34;&gt;Sebastian Starke&lt;/a&gt;&lt;sup&gt;+&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jun-saito/&#34;&gt;Jun Saito&lt;/a&gt;. ACM Trans. Graph. 37, 4, Article 145. &lt;/sub&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(+Joint First Authors)&lt;/sup&gt; &lt;sub&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2018/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Animating characters can be a pain, especially those four-legged monsters! This year, we will be presenting our recent research on quadruped animation and character control at the SIGGRAPH 2018 in Vancouver. The system can produce natural animations from real motion data using a novel neural network architecture, called Mode-Adaptive Neural Networks. Instead of optimising a fixed group of weights, the system learns to dynamically blend a group of weights into a further neural network, based on the current state of the character. That said, the system does not require labels for the phase or locomotion gaits, but can learn from unstructured motion capture data in an end-to-end fashion. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=uFJvRYtjQ4c&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2018/Paper.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2018&#34;&gt;Code&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2018/MotionCapture.zip&#34;&gt;Mocap Data&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2018/Demo_Windows.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2018/Demo_Linux.zip&#34;&gt;Linux Demo&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2018/Demo_Mac.zip&#34;&gt;Mac Demo&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2018/ReadMe.md&#34;&gt;ReadMe&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=uFJvRYtjQ4c&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2018/Thumbnail.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://github.com/pauzii/AnimationAuthoring&#34;&gt;Animation Authoring Tool&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/pauzii/AnimationAuthoring/raw/main/Media/Teaser.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;SIGGRAPH 2017&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Phase-Functioned Neural Networks for Character Control&lt;/strong&gt;&lt;br&gt; &lt;sub&gt; &lt;a href=&#34;https://www.linkedin.com/in/daniel-holden-300b871b/&#34;&gt;Daniel Holden&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/taku-komura-571b32b/&#34;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jun-saito/&#34;&gt;Jun Saito&lt;/a&gt;. ACM Trans. Graph. 36, 4, Article 42. &lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/SIGGRAPH_2017/Adam.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; This work continues the recent work on PFNN (Phase-Functioned Neural Networks) for character control. A demo in Unity3D using the original weights for terrain-adaptive locomotion is contained in the Assets/Demo/SIGGRAPH_2017/Original folder. Another demo on flat ground using the Adam character is contained in the Assets/Demo/SIGGRAPH_2017/Adam folder. In order to run them, you need to download the neural network weights from the link provided in the Link.txt file, extract them into the /NN folder, and store the parameters via the custom inspector button. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=Ul0Gilv5wvY&#34;&gt;Video&lt;/a&gt; - &lt;a href=&#34;http://theorangeduck.com/media/uploads/other_stuff/phasefunction.pdf&#34;&gt;Paper&lt;/a&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/AI4Animation/SIGGRAPH_2017&#34;&gt;Code (Unity)&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2017/Demo_Windows.zip&#34;&gt;Windows Demo&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2017/Demo_Linux.zip&#34;&gt;Linux Demo&lt;/a&gt; - &lt;a href=&#34;http://www.starke-consult.de/AI4Animation/SIGGRAPH_2017/Demo_Mac.zip&#34;&gt;Mac Demo&lt;/a&gt; - &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Ul0Gilv5wvY&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://img.youtube.com/vi/Ul0Gilv5wvY/0.jpg&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Processing Pipeline&lt;/h1&gt; &#xA;&lt;p&gt;In progress. More information will be added soon.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/sebastianstarke/AI4Animation/master/Media/ProcessingPipeline/Editor.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h1&gt;Copyright Information&lt;/h1&gt; &#xA;&lt;p&gt;This project is only for research or education purposes, and not freely available for commercial use or redistribution. The motion capture data is available only under the terms of the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/legalcode&#34;&gt;Attribution-NonCommercial 4.0 International&lt;/a&gt; (CC BY-NC 4.0) license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>D1rkMtr/FilelessNtdllReflection</title>
    <updated>2023-01-11T01:32:13Z</updated>
    <id>tag:github.com,2023-01-11:/D1rkMtr/FilelessNtdllReflection</id>
    <link href="https://github.com/D1rkMtr/FilelessNtdllReflection" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bypass Userland EDR hooks by Loading Reflective Ntdll in memory from a remote server based on Windows ReleaseID to avoid opening a handle to ntdll, and trigger exported API from the export table&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FilelessNtdllReflection&lt;/h1&gt; &#xA;&lt;p&gt;Bypass Userland EDR hooks by Loading Reflective Ntdll in memory from a remote server based on Windows ReleaseID to avoid opening a handle to ntdll , and trigger exported API from the export table&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/110354855/211152080-3b708135-19db-4e36-a15f-a0c3bbf1b787.png&#34; alt=&#34;ntdllFromServer&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/110354855/211152091-32d4ea37-a061-43ef-a957-ad8f5cfa5be3.png&#34; alt=&#34;ReflectiveNTDLL&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>