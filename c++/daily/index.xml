<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-09T01:30:01Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>WerWolv/ImHex</title>
    <updated>2022-07-09T01:30:01Z</updated>
    <id>tag:github.com,2022-07-09:/WerWolv/ImHex</id>
    <link href="https://github.com/WerWolv/ImHex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üîç A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://imhex.werwolv.net&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;a href=&#34;https://imhex.werwolv.net&#34;&gt;&lt;span&gt;üîç&lt;/span&gt; ImHex&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a title=&#34;&#39;Build&#39; workflow Status&#34; href=&#34;https://github.com/WerWolv/ImHex/actions?query=workflow%3ABuild&#34;&gt;&lt;img alt=&#34;&#39;Build&#39; workflow Status&#34; src=&#34;https://img.shields.io/github/workflow/status/WerWolv/ImHex/Build?longCache=true&amp;amp;style=for-the-badge&amp;amp;label=Build&amp;amp;logoColor=fff&amp;amp;logo=GitHub%20Actions&#34;&gt;&lt;/a&gt; &lt;a title=&#34;Discord Server&#34; href=&#34;https://discord.gg/X63jZ36xBY&#34;&gt;&lt;img alt=&#34;Discord Server&#34; src=&#34;https://img.shields.io/discord/789833418631675954?label=Discord&amp;amp;logo=Discord&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a title=&#34;Total Downloads&#34; href=&#34;https://github.com/WerWolv/ImHex/releases/latest&#34;&gt;&lt;img alt=&#34;Total Downloads&#34; src=&#34;https://img.shields.io/github/downloads/WerWolv/ImHex/total?longCache=true&amp;amp;style=for-the-badge&amp;amp;label=Downloads&amp;amp;logoColor=fff&amp;amp;logo=GitHub&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Supporting&lt;/h2&gt; &#xA;&lt;p&gt;If you like my work, please consider supporting me on GitHub Sponsors, Patreon or PayPal. Thanks a lot!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/sponsors/WerWolv&#34;&gt;&lt;img src=&#34;https://werwolv.net/assets/github_banner.png&#34; alt=&#34;GitHub donate button&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/werwolv&#34;&gt;&lt;img src=&#34;https://c5.patreon.com/external/logo/become_a_patron_button.png&#34; alt=&#34;Patreon donate button&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://werwolv.net/donate&#34;&gt;&lt;img src=&#34;https://werwolv.net/assets/paypal_banner.png&#34; alt=&#34;PayPal donate button&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10835354/139717326-8044769d-527b-4d88-8adf-2d4ecafdca1f.png&#34; alt=&#34;Hex editor, patterns and data information&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10835354/139717323-1f8c9d52-f7eb-4f43-9f11-097ac728ed6c.png&#34; alt=&#34;Bookmarks, disassembler and data processor&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Featureful hex view &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Byte patching&lt;/li&gt; &#xA;   &lt;li&gt;Patch management&lt;/li&gt; &#xA;   &lt;li&gt;Copy bytes as feature &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Bytes&lt;/li&gt; &#xA;     &lt;li&gt;Hex string&lt;/li&gt; &#xA;     &lt;li&gt;C, C++, C#, Rust, Python, Java &amp;amp; JavaScript array&lt;/li&gt; &#xA;     &lt;li&gt;ASCII-Art hex view&lt;/li&gt; &#xA;     &lt;li&gt;HTML self-contained div&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;String and hex search&lt;/li&gt; &#xA;   &lt;li&gt;Colorful highlighting&lt;/li&gt; &#xA;   &lt;li&gt;Goto from start, end and current cursor position&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Custom C++-like pattern language for parsing highlighting a file&#39;s content &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Automatic loading based on MIME type&lt;/li&gt; &#xA;   &lt;li&gt;arrays, pointers, structs, unions, enums, bitfields, namespaces, little and big endian support, conditionals and much more!&lt;/li&gt; &#xA;   &lt;li&gt;Useful error messages, syntax highlighting and error marking&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data importing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Base64 files&lt;/li&gt; &#xA;   &lt;li&gt;IPS and IPS32 patches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data exporting &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;IPS and IPS32 patches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data inspector allowing interpretation of data as many different types (little and big endian)&lt;/li&gt; &#xA; &lt;li&gt;Huge file support with fast and efficient loading&lt;/li&gt; &#xA; &lt;li&gt;String search &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Copying of strings&lt;/li&gt; &#xA;   &lt;li&gt;Copying of demangled strings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;File hashing support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CRC16 and CRC32 with custom initial values and polynomials&lt;/li&gt; &#xA;   &lt;li&gt;MD4, MD5&lt;/li&gt; &#xA;   &lt;li&gt;SHA-1, SHA-224, SHA-256, SHA-384, SHA-512&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Disassembler supporting many architectures (frontend for Capstone) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ARM32 (ARM, Thumb, Cortex-M, AArch32)&lt;/li&gt; &#xA;   &lt;li&gt;ARM64&lt;/li&gt; &#xA;   &lt;li&gt;MIPS (MIPS32, MIPS64, MIPS32R6, Micro)&lt;/li&gt; &#xA;   &lt;li&gt;x86 (16-bit, 32-bit, 64-bit)&lt;/li&gt; &#xA;   &lt;li&gt;PowerPC (32-bit, 64-bit)&lt;/li&gt; &#xA;   &lt;li&gt;SPARC&lt;/li&gt; &#xA;   &lt;li&gt;IBM SystemZ&lt;/li&gt; &#xA;   &lt;li&gt;xCORE&lt;/li&gt; &#xA;   &lt;li&gt;M68K&lt;/li&gt; &#xA;   &lt;li&gt;TMS320C64X&lt;/li&gt; &#xA;   &lt;li&gt;M680X&lt;/li&gt; &#xA;   &lt;li&gt;Ethereum&lt;/li&gt; &#xA;   &lt;li&gt;RISC-V&lt;/li&gt; &#xA;   &lt;li&gt;WebAssembly&lt;/li&gt; &#xA;   &lt;li&gt;MOS565XX&lt;/li&gt; &#xA;   &lt;li&gt;Berkeley Packet Filter&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Bookmarks &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Region highlighting&lt;/li&gt; &#xA;   &lt;li&gt;Comments&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data Analyzer &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;File magic-based file parser and MIME type database&lt;/li&gt; &#xA;   &lt;li&gt;Byte distribution graph&lt;/li&gt; &#xA;   &lt;li&gt;Entropy graph&lt;/li&gt; &#xA;   &lt;li&gt;Highest and average entropy&lt;/li&gt; &#xA;   &lt;li&gt;Encrypted / Compressed file detection&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Built-in Content Store &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Download all files found in the database directly from within ImHex&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Yara Rules support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Quickly scan a file for vulnerabilities with official yara rules&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Helpful tools &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Itanium and MSVC demangler&lt;/li&gt; &#xA;   &lt;li&gt;ASCII table&lt;/li&gt; &#xA;   &lt;li&gt;Regex replacer&lt;/li&gt; &#xA;   &lt;li&gt;Mathematical expression evaluator (Calculator)&lt;/li&gt; &#xA;   &lt;li&gt;Hexadecimal Color picker&lt;/li&gt; &#xA;   &lt;li&gt;Base converter&lt;/li&gt; &#xA;   &lt;li&gt;UNIX Permissions calculator&lt;/li&gt; &#xA;   &lt;li&gt;Anonfiles File upload tool&lt;/li&gt; &#xA;   &lt;li&gt;Wikipedia term definition finder&lt;/li&gt; &#xA;   &lt;li&gt;File utilities &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;File splitter&lt;/li&gt; &#xA;     &lt;li&gt;File combiner&lt;/li&gt; &#xA;     &lt;li&gt;File shredder&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Built-in cheat sheet for pattern language and Math evaluator&lt;/li&gt; &#xA; &lt;li&gt;Doesn&#39;t burn out your retinas when used in late-night sessions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pattern Language&lt;/h2&gt; &#xA;&lt;p&gt;The custom C-like Pattern Language developed and used by ImHex is easy to read, understand and learn. A guide with all features of the language can be found &lt;a href=&#34;http://imhex.werwolv.net/docs&#34;&gt;on the docs page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Database&lt;/h2&gt; &#xA;&lt;p&gt;For format patterns, includable libraries magic and constant files, check out the &lt;a href=&#34;https://github.com/WerWolv/ImHex-Patterns&#34;&gt;ImHex-Patterns&lt;/a&gt; repository. Feel free to PR your own files there as well!&lt;/p&gt; &#xA;&lt;h2&gt;Plugin development&lt;/h2&gt; &#xA;&lt;p&gt;To develop plugins for ImHex, use one of the following two templates projects to get started. You then have access to the entirety of libimhex as well as the ImHex API and the Content Registry to interact with ImHex or to add new content.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WerWolv/ImHex-Cpp-Plugin-Template&#34;&gt;C++ Plugin Template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WerWolv/ImHex-Rust-Plugin-Template&#34;&gt;Rust Plugin Template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Nightly builds&lt;/h2&gt; &#xA;&lt;p&gt;Nightlies are available via GitHub Actions &lt;a href=&#34;https://github.com/WerWolv/ImHex/actions?query=workflow%3ABuild&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows ‚Ä¢ &lt;strong&gt;x86_64&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Windows%20Installer.zip&#34;&gt;Installer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Windows%20Portable%20ZIP.zip&#34;&gt;Portable&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MacOS ‚Ä¢ &lt;strong&gt;x86_64&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/macOS%20DMG.zip&#34;&gt;DMG&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Linux ‚Ä¢ &lt;strong&gt;x86_64&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Linux%20DEB.zip&#34;&gt;DEB&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Linux%20AppImage.zip&#34;&gt;AppImage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/ArchLinux%20.pkg.tar.zst.zip&#34;&gt;Arch Package&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compiling&lt;/h2&gt; &#xA;&lt;p&gt;To compile ImHex, a C++20 compiler is required. Releases are all mainly built using GCC, however on macOS, clang is also required to compile some ObjC code.&lt;/p&gt; &#xA;&lt;p&gt;Many dependencies are bundled into the repository using submodules so make sure to clone it using the &lt;code&gt;--recurse-submodules&lt;/code&gt; option. All dependencies that aren&#39;t bundled, can be installed using the dependency installer scripts found in the &lt;code&gt;/dist&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Thog&#34;&gt;Mary&lt;/a&gt; for her immense help porting ImHex to MacOS and help during development&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Roblabla&#34;&gt;Roblabla&lt;/a&gt; for adding MSI Installer support to ImHex&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jam1garner&#34;&gt;jam1garner&lt;/a&gt; and &lt;a href=&#34;https://github.com/raytwo&#34;&gt;raytwo&lt;/a&gt; for their help with adding Rust support to plugins&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mailaender&#34;&gt;Mailaender&lt;/a&gt; for getting ImHex onto Flathub&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iTrooz&#34;&gt;iTrooz&lt;/a&gt; for many improvements related to release packaging and the GitHub Action runners.&lt;/li&gt; &#xA; &lt;li&gt;Everybody else who has reported issues on Discord or GitHub that I had great conversations with :)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks a lot to ocornut for their amazing &lt;a href=&#34;https://github.com/ocornut/imgui&#34;&gt;Dear ImGui&lt;/a&gt; which is used for building the entire interface &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Thanks to ocornut as well for their hex editor view used as base for this project.&lt;/li&gt; &#xA;   &lt;li&gt;Thanks to BalazsJako for their incredible &lt;a href=&#34;https://github.com/BalazsJako/ImGuiColorTextEdit&#34;&gt;ImGuiColorTextEdit&lt;/a&gt; used for the pattern language syntax highlighting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Thanks to nlohmann for their &lt;a href=&#34;https://github.com/nlohmann/json&#34;&gt;json&lt;/a&gt; library used for project files&lt;/li&gt; &#xA; &lt;li&gt;Thanks to aquynh for &lt;a href=&#34;https://github.com/aquynh/capstone&#34;&gt;capstone&lt;/a&gt; which is the base of the disassembly window&lt;/li&gt; &#xA; &lt;li&gt;Thanks to vitaut for their &lt;a href=&#34;https://github.com/fmtlib/fmt&#34;&gt;libfmt&lt;/a&gt; library which makes formatting and logging so much better&lt;/li&gt; &#xA; &lt;li&gt;Thanks to rxi for &lt;a href=&#34;https://github.com/rxi/microtar&#34;&gt;microtar&lt;/a&gt; used for extracting downloaded store assets&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>UZ-SLAMLab/ORB_SLAM3</title>
    <updated>2022-07-09T01:30:01Z</updated>
    <id>tag:github.com,2022-07-09:/UZ-SLAMLab/ORB_SLAM3</id>
    <link href="https://github.com/UZ-SLAMLab/ORB_SLAM3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ORB-SLAM3&lt;/h1&gt; &#xA;&lt;h3&gt;V1.0, December 22th, 2021&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlos Campos, Richard Elvira, Juan J. G√≥mez Rodr√≠guez, &lt;a href=&#34;http://webdiis.unizar.es/~josemari/&#34;&gt;Jos√© M. M. Montiel&lt;/a&gt;, &lt;a href=&#34;http://webdiis.unizar.es/~jdtardos/&#34;&gt;Juan D. Tardos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/UZ-SLAMLab/ORB_SLAM3/raw/master/Changelog.md&#34;&gt;Changelog&lt;/a&gt; describes the features of each version.&lt;/p&gt; &#xA;&lt;p&gt;ORB-SLAM3 is the first real-time SLAM library able to perform &lt;strong&gt;Visual, Visual-Inertial and Multi-Map SLAM&lt;/strong&gt; with &lt;strong&gt;monocular, stereo and RGB-D&lt;/strong&gt; cameras, using &lt;strong&gt;pin-hole and fisheye&lt;/strong&gt; lens models. In all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate.&lt;/p&gt; &#xA;&lt;p&gt;We provide examples to run ORB-SLAM3 in the &lt;a href=&#34;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&#34;&gt;EuRoC dataset&lt;/a&gt; using stereo or monocular, with or without IMU, and in the &lt;a href=&#34;https://vision.in.tum.de/data/datasets/visual-inertial-dataset&#34;&gt;TUM-VI dataset&lt;/a&gt; using fisheye stereo or monocular, with or without IMU. Videos of some example executions can be found at &lt;a href=&#34;https://www.youtube.com/channel/UCXVt-kXG6T95Z4tVaYlU80Q&#34;&gt;ORB-SLAM3 channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This software is based on &lt;a href=&#34;https://github.com/raulmur/ORB_SLAM2&#34;&gt;ORB-SLAM2&lt;/a&gt; developed by &lt;a href=&#34;http://webdiis.unizar.es/~raulmur/&#34;&gt;Raul Mur-Artal&lt;/a&gt;, &lt;a href=&#34;http://webdiis.unizar.es/~jdtardos/&#34;&gt;Juan D. Tardos&lt;/a&gt;, &lt;a href=&#34;http://webdiis.unizar.es/~josemari/&#34;&gt;J. M. M. Montiel&lt;/a&gt; and &lt;a href=&#34;http://doriangalvez.com/&#34;&gt;Dorian Galvez-Lopez&lt;/a&gt; (&lt;a href=&#34;https://github.com/dorian3d/DBoW2&#34;&gt;DBoW2&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/HyLNq-98LRo&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/HyLNq-98LRo/0.jpg&#34; alt=&#34;ORB-SLAM3&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Related Publications:&lt;/h3&gt; &#xA;&lt;p&gt;[ORB-SLAM3] Carlos Campos, Richard Elvira, Juan J. G√≥mez Rodr√≠guez, Jos√© M. M. Montiel and Juan D. Tard√≥s, &lt;strong&gt;ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM&lt;/strong&gt;, &lt;em&gt;IEEE Transactions on Robotics 37(6):1874-1890, Dec. 2021&lt;/em&gt;. &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.11898&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[IMU-Initialization] Carlos Campos, J. M. M. Montiel and Juan D. Tard√≥s, &lt;strong&gt;Inertial-Only Optimization for Visual-Inertial Initialization&lt;/strong&gt;, &lt;em&gt;ICRA 2020&lt;/em&gt;. &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.05766.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[ORBSLAM-Atlas] Richard Elvira, J. M. M. Montiel and Juan D. Tard√≥s, &lt;strong&gt;ORBSLAM-Atlas: a robust and accurate multi-map system&lt;/strong&gt;, &lt;em&gt;IROS 2019&lt;/em&gt;. &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1908.11585.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[ORBSLAM-VI] Ra√∫l Mur-Artal, and Juan D. Tard√≥s, &lt;strong&gt;Visual-inertial monocular SLAM with map reuse&lt;/strong&gt;, IEEE Robotics and Automation Letters, vol. 2 no. 2, pp. 796-803, 2017. &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1610.05949.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[Stereo and RGB-D] Ra√∫l Mur-Artal and Juan D. Tard√≥s. &lt;strong&gt;ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras&lt;/strong&gt;. &lt;em&gt;IEEE Transactions on Robotics,&lt;/em&gt; vol. 33, no. 5, pp. 1255-1262, 2017. &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1610.06475.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[Monocular] Ra√∫l Mur-Artal, Jos√© M. M. Montiel and Juan D. Tard√≥s. &lt;strong&gt;ORB-SLAM: A Versatile and Accurate Monocular SLAM System&lt;/strong&gt;. &lt;em&gt;IEEE Transactions on Robotics,&lt;/em&gt; vol. 31, no. 5, pp. 1147-1163, 2015. (&lt;strong&gt;2015 IEEE Transactions on Robotics Best Paper Award&lt;/strong&gt;). &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.00956.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[DBoW2 Place Recognition] Dorian G√°lvez-L√≥pez and Juan D. Tard√≥s. &lt;strong&gt;Bags of Binary Words for Fast Place Recognition in Image Sequences&lt;/strong&gt;. &lt;em&gt;IEEE Transactions on Robotics,&lt;/em&gt; vol. 28, no. 5, pp. 1188-1197, 2012. &lt;strong&gt;&lt;a href=&#34;http://doriangalvez.com/php/dl.php?dlp=GalvezTRO12.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;1. License&lt;/h1&gt; &#xA;&lt;p&gt;ORB-SLAM3 is released under &lt;a href=&#34;https://github.com/UZ-SLAMLab/ORB_SLAM3/LICENSE&#34;&gt;GPLv3 license&lt;/a&gt;. For a list of all code/library dependencies (and associated licenses), please see &lt;a href=&#34;https://github.com/UZ-SLAMLab/ORB_SLAM3/raw/master/Dependencies.md&#34;&gt;Dependencies.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a closed-source version of ORB-SLAM3 for commercial purposes, please contact the authors: orbslam (at) unizar (dot) es.&lt;/p&gt; &#xA;&lt;p&gt;If you use ORB-SLAM3 in an academic work, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ORBSLAM3_TRO,&#xA;  title={{ORB-SLAM3}: An Accurate Open-Source Library for Visual, Visual-Inertial &#xA;           and Multi-Map {SLAM}},&#xA;  author={Campos, Carlos AND Elvira, Richard AND G\¬¥omez, Juan J. AND Montiel, &#xA;          Jos\&#39;e M. M. AND Tard\&#39;os, Juan D.},&#xA;  journal={IEEE Transactions on Robotics}, &#xA;  volume={37},&#xA;  number={6},&#xA;  pages={1874-1890},&#xA;  year={2021}&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;2. Prerequisites&lt;/h1&gt; &#xA;&lt;p&gt;We have tested the library in &lt;strong&gt;Ubuntu 16.04&lt;/strong&gt; and &lt;strong&gt;18.04&lt;/strong&gt;, but it should be easy to compile in other platforms. A powerful computer (e.g. i7) will ensure real-time performance and provide more stable and accurate results.&lt;/p&gt; &#xA;&lt;h2&gt;C++11 or C++0x Compiler&lt;/h2&gt; &#xA;&lt;p&gt;We use the new thread and chrono functionalities of C++11.&lt;/p&gt; &#xA;&lt;h2&gt;Pangolin&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/stevenlovegrove/Pangolin&#34;&gt;Pangolin&lt;/a&gt; for visualization and user interface. Dowload and install instructions can be found at: &lt;a href=&#34;https://github.com/stevenlovegrove/Pangolin&#34;&gt;https://github.com/stevenlovegrove/Pangolin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenCV&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;http://opencv.org&#34;&gt;OpenCV&lt;/a&gt; to manipulate images and features. Dowload and install instructions can be found at: &lt;a href=&#34;http://opencv.org&#34;&gt;http://opencv.org&lt;/a&gt;. &lt;strong&gt;Required at leat 3.0. Tested with OpenCV 3.2.0 and 4.4.0&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Eigen3&lt;/h2&gt; &#xA;&lt;p&gt;Required by g2o (see below). Download and install instructions can be found at: &lt;a href=&#34;http://eigen.tuxfamily.org&#34;&gt;http://eigen.tuxfamily.org&lt;/a&gt;. &lt;strong&gt;Required at least 3.1.0&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;DBoW2 and g2o (Included in Thirdparty folder)&lt;/h2&gt; &#xA;&lt;p&gt;We use modified versions of the &lt;a href=&#34;https://github.com/dorian3d/DBoW2&#34;&gt;DBoW2&lt;/a&gt; library to perform place recognition and &lt;a href=&#34;https://github.com/RainerKuemmerle/g2o&#34;&gt;g2o&lt;/a&gt; library to perform non-linear optimizations. Both modified libraries (which are BSD) are included in the &lt;em&gt;Thirdparty&lt;/em&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Required to calculate the alignment of the trajectory with the ground truth. &lt;strong&gt;Required Numpy module&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(win) &lt;a href=&#34;http://www.python.org/downloads/windows&#34;&gt;http://www.python.org/downloads/windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(deb) &lt;code&gt;sudo apt install libpython2.7-dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;(mac) preinstalled with osx&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ROS (optional)&lt;/h2&gt; &#xA;&lt;p&gt;We provide some examples to process input of a monocular, monocular-inertial, stereo, stereo-inertial or RGB-D camera using ROS. Building these examples is optional. These have been tested with ROS Melodic under Ubuntu 18.04.&lt;/p&gt; &#xA;&lt;h1&gt;3. Building ORB-SLAM3 library and examples&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git ORB_SLAM3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a script &lt;code&gt;build.sh&lt;/code&gt; to build the &lt;em&gt;Thirdparty&lt;/em&gt; libraries and &lt;em&gt;ORB-SLAM3&lt;/em&gt;. Please make sure you have installed all required dependencies (see section 2). Execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ORB_SLAM3&#xA;chmod +x build.sh&#xA;./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create &lt;strong&gt;libORB_SLAM3.so&lt;/strong&gt; at &lt;em&gt;lib&lt;/em&gt; folder and the executables in &lt;em&gt;Examples&lt;/em&gt; folder.&lt;/p&gt; &#xA;&lt;h1&gt;4. Running ORB-SLAM3 with your camera&lt;/h1&gt; &#xA;&lt;p&gt;Directory &lt;code&gt;Examples&lt;/code&gt; contains several demo programs and calibration files to run ORB-SLAM3 in all sensor configurations with Intel Realsense cameras T265 and D435i. The steps needed to use your own camera are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Calibrate your camera following &lt;code&gt;Calibration_Tutorial.pdf&lt;/code&gt; and write your calibration file &lt;code&gt;your_camera.yaml&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Modify one of the provided demos to suit your specific camera model, and build it&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Connect the camera to your computer using USB3 or the appropriate interface&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run ORB-SLAM3. For example, for our D435i camera, we would execute:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./Examples/Stereo-Inertial/stereo_inertial_realsense_D435i Vocabulary/ORBvoc.txt ./Examples/Stereo-Inertial/RealSense_D435i.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;5. EuRoC Examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&#34;&gt;EuRoC dataset&lt;/a&gt; was recorded with two pinhole cameras and an inertial sensor. We provide an example script to launch EuRoC sequences in all the sensor configurations.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download a sequence (ASL format) from &lt;a href=&#34;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&#34;&gt;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the script &#34;euroc_examples.sh&#34; in the root of the project. Change &lt;strong&gt;pathDatasetEuroc&lt;/strong&gt; variable to point to the directory where the dataset has been uncompressed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute the following script to process all the sequences with all sensor configurations:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./euroc_examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;EuRoC provides ground truth for each sequence in the IMU body reference. As pure visual executions report trajectories centered in the left camera, we provide in the &#34;evaluation&#34; folder the transformation of the ground truth to the left camera reference. Visual-inertial trajectories use the ground truth from the dataset.&lt;/p&gt; &#xA;&lt;p&gt;Execute the following script to process sequences and compute the RMS ATE:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./euroc_eval_examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;6. TUM-VI Examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vision.in.tum.de/data/datasets/visual-inertial-dataset&#34;&gt;TUM-VI dataset&lt;/a&gt; was recorded with two fisheye cameras and an inertial sensor.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download a sequence from &lt;a href=&#34;https://vision.in.tum.de/data/datasets/visual-inertial-dataset&#34;&gt;https://vision.in.tum.de/data/datasets/visual-inertial-dataset&lt;/a&gt; and uncompress it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the script &#34;tum_vi_examples.sh&#34; in the root of the project. Change &lt;strong&gt;pathDatasetTUM_VI&lt;/strong&gt; variable to point to the directory where the dataset has been uncompressed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute the following script to process all the sequences with all sensor configurations:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./tum_vi_examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;In TUM-VI ground truth is only available in the room where all sequences start and end. As a result the error measures the drift at the end of the sequence.&lt;/p&gt; &#xA;&lt;p&gt;Execute the following script to process sequences and compute the RMS ATE:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./tum_vi_eval_examples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;7. ROS Examples&lt;/h1&gt; &#xA;&lt;h3&gt;Building the nodes for mono, mono-inertial, stereo, stereo-inertial and RGB-D&lt;/h3&gt; &#xA;&lt;p&gt;Tested with ROS Melodic and ubuntu 18.04.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add the path including &lt;em&gt;Examples/ROS/ORB_SLAM3&lt;/em&gt; to the ROS_PACKAGE_PATH environment variable. Open .bashrc file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;gedit ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and add at the end the following line. Replace PATH by the folder where you cloned ORB_SLAM3:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:PATH/ORB_SLAM3/Examples/ROS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;build_ros.sh&lt;/code&gt; script:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x build_ros.sh&#xA;./build_ros.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Monocular Node&lt;/h3&gt; &#xA;&lt;p&gt;For a monocular input from topic &lt;code&gt;/camera/image_raw&lt;/code&gt; run node ORB_SLAM3/Mono. You will need to provide the vocabulary file and a settings file. See the monocular examples above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun ORB_SLAM3 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Monocular-Inertial Node&lt;/h3&gt; &#xA;&lt;p&gt;For a monocular input from topic &lt;code&gt;/camera/image_raw&lt;/code&gt; and an inertial input from topic &lt;code&gt;/imu&lt;/code&gt;, run node ORB_SLAM3/Mono_Inertial. Setting the optional third argument to true will apply CLAHE equalization to images (Mainly for TUM-VI dataset).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun ORB_SLAM3 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE [EQUALIZATION]&#x9;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Stereo Node&lt;/h3&gt; &#xA;&lt;p&gt;For a stereo input from topic &lt;code&gt;/camera/left/image_raw&lt;/code&gt; and &lt;code&gt;/camera/right/image_raw&lt;/code&gt; run node ORB_SLAM3/Stereo. You will need to provide the vocabulary file and a settings file. For Pinhole camera model, if you &lt;strong&gt;provide rectification matrices&lt;/strong&gt; (see Examples/Stereo/EuRoC.yaml example), the node will recitify the images online, &lt;strong&gt;otherwise images must be pre-rectified&lt;/strong&gt;. For FishEye camera model, rectification is not required since system works with original images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun ORB_SLAM3 Stereo PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Stereo-Inertial Node&lt;/h3&gt; &#xA;&lt;p&gt;For a stereo input from topics &lt;code&gt;/camera/left/image_raw&lt;/code&gt; and &lt;code&gt;/camera/right/image_raw&lt;/code&gt;, and an inertial input from topic &lt;code&gt;/imu&lt;/code&gt;, run node ORB_SLAM3/Stereo_Inertial. You will need to provide the vocabulary file and a settings file, including rectification matrices if required in a similar way to Stereo case:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun ORB_SLAM3 Stereo_Inertial PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION [EQUALIZATION]&#x9;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running RGB_D Node&lt;/h3&gt; &#xA;&lt;p&gt;For an RGB-D input from topics &lt;code&gt;/camera/rgb/image_raw&lt;/code&gt; and &lt;code&gt;/camera/depth_registered/image_raw&lt;/code&gt;, run node ORB_SLAM3/RGBD. You will need to provide the vocabulary file and a settings file. See the RGB-D example above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun ORB_SLAM3 RGBD PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running ROS example:&lt;/strong&gt; Download a rosbag (e.g. V1_02_medium.bag) from the EuRoC dataset (&lt;a href=&#34;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&#34;&gt;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&lt;/a&gt;). Open 3 tabs on the terminal and run the following command at each tab for a Stereo-Inertial configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roscore&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun ORB_SLAM3 Stereo_Inertial Vocabulary/ORBvoc.txt Examples/Stereo-Inertial/EuRoC.yaml true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosbag play --pause V1_02_medium.bag /cam0/image_raw:=/camera/left/image_raw /cam1/image_raw:=/camera/right/image_raw /imu0:=/imu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once ORB-SLAM3 has loaded the vocabulary, press space in the rosbag tab.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; For rosbags from TUM-VI dataset, some play issue may appear due to chunk size. One possible solution is to rebag them with the default chunk size, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosrun rosbag fastrebag.py dataset-room1_512_16.bag dataset-room1_512_16_small_chunks.bag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;8. Running time analysis&lt;/h1&gt; &#xA;&lt;p&gt;A flag in &lt;code&gt;include\Config.h&lt;/code&gt; activates time measurements. It is necessary to uncomment the line &lt;code&gt;#define REGISTER_TIMES&lt;/code&gt; to obtain the time stats of one execution which is shown at the terminal and stored in a text file(&lt;code&gt;ExecTimeMean.txt&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h1&gt;9. Calibration&lt;/h1&gt; &#xA;&lt;p&gt;You can find a tutorial for visual-inertial calibration and a detailed description of the contents of valid configuration files at &lt;code&gt;Calibration_Tutorial.pdf&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IntelRealSense/librealsense</title>
    <updated>2022-07-09T01:30:01Z</updated>
    <id>tag:github.com,2022-07-09:/IntelRealSense/librealsense</id>
    <link href="https://github.com/IntelRealSense/librealsense" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Intel¬Æ RealSense‚Ñ¢ SDK&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/img/realsense.png&#34; width=&#34;70%&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/actions/workflows/buildsCI.yaml&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IntelRealSense/actions/workflows/buildsCI.yaml/badge.svg?branch=development&#34; alt=&#34;GitHub CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intel¬Æ RealSense‚Ñ¢ SDK 2.0&lt;/strong&gt; is a cross-platform library for Intel¬Æ RealSense‚Ñ¢ depth cameras (D400 &amp;amp; L500 series and the SR300) and the &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/t265.md&#34;&gt;T265 tracking camera&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; For other Intel¬Æ RealSense‚Ñ¢ devices (F200, R200, LR200 and ZR300), please refer to the &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/tree/v1.12.1&#34;&gt;latest legacy release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The SDK allows depth and color streaming, and provides intrinsic and extrinsic calibration information. The library also offers synthetic streams (pointcloud, depth aligned to color and vise-versa), and a built-in support for &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/record-and-playback.md&#34;&gt;record and playback&lt;/a&gt; of streaming sessions.&lt;/p&gt; &#xA;&lt;p&gt;Developer kits containing the necessary hardware to use this library are available for purchase at &lt;a href=&#34;https://store.intelrealsense.com/products.html&#34;&gt;store.intelrealsense.com&lt;/a&gt;. Information about the Intel¬Æ RealSense‚Ñ¢ technology at &lt;a href=&#34;https://www.intelrealsense.com/&#34;&gt;www.intelrealsense.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üìÇ&lt;/span&gt; Don&#39;t have access to a RealSense camera? Check-out &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/sample-data.md&#34;&gt;sample data&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Special notice from Intel¬Æ RealSense‚Ñ¢ regarding the recent press announcement&lt;/h2&gt; &#xA;&lt;p&gt;Intel has decided to wind down the RealSense business and is announcing the EOL of LiDAR, Facial Authentication, and Tracking product lines this month. Intel will continue to provide Stereo products to its current distribution customers. Intel will focus our new development on advancing innovative technologies that better support our core businesses and IDM 2.0 strategy. The products identified in this notification will be discontinued and unavailable for additional orders after Feb 28, 2022.&lt;/p&gt; &#xA;&lt;p&gt;The following Stereo Product Lines WILL continue to be supported: D410, D415, D430, D450 modules and D415, D435, D435i, D455 depth cameras.&lt;/p&gt; &#xA;&lt;p&gt;We will also continue the work to support and develop our LibRealSense open source SDK.&lt;/p&gt; &#xA;&lt;p&gt;In the coming future Intel and the RealSense team will focus our new development on advancing innovative technologies that better support our core businesses and IDM 2.0 strategy.&lt;/p&gt; &#xA;&lt;h2&gt;Building librealsense - Using vcpkg&lt;/h2&gt; &#xA;&lt;p&gt;You can download and install librealsense using the &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Microsoft/vcpkg.git&#xA;cd vcpkg&#xA;./bootstrap-vcpkg.sh&#xA;./vcpkg integrate install&#xA;./vcpkg install realsense2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The librealsense port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h2&gt;Download and Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; - The latest releases including the Intel RealSense SDK, Viewer and Depth Quality tools are available at: &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/releases&#34;&gt;&lt;strong&gt;latest releases&lt;/strong&gt;&lt;/a&gt;. Please check the &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/wiki/Release-Notes&#34;&gt;&lt;strong&gt;release notes&lt;/strong&gt;&lt;/a&gt; for the supported platforms, new features and capabilities, known issues, how to upgrade the Firmware and more.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; - You can also install or build from source the SDK (on &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/distribution_linux.md&#34;&gt;Linux&lt;/a&gt; \ &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/distribution_windows.md&#34;&gt;Windows&lt;/a&gt; \ &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/installation_osx.md&#34;&gt;Mac OS&lt;/a&gt; \ &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/android.md&#34;&gt;Android&lt;/a&gt; \ &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/scripts/Docker/readme.md&#34;&gt;Docker&lt;/a&gt;), connect your D400 depth camera and you are ready to start writing your first application.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Support &amp;amp; Issues&lt;/strong&gt;: If you need product support (e.g. ask a question about / are having problems with the device), please check the &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/wiki/Troubleshooting-Q%26A&#34;&gt;FAQ &amp;amp; Troubleshooting&lt;/a&gt; section. If not covered there, please search our &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aclosed&#34;&gt;Closed GitHub Issues&lt;/a&gt; page, &lt;a href=&#34;https://communities.intel.com/community/tech/realsense&#34;&gt;Community&lt;/a&gt; and &lt;a href=&#34;https://www.intel.com/content/www/us/en/support/emerging-technologies/intel-realsense-technology.html&#34;&gt;Support&lt;/a&gt; sites. If you still cannot find an answer to your question, please &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/issues/new&#34;&gt;open a new issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What‚Äôs included in the SDK:&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;What&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Download link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/tools/realsense-viewer&#34;&gt;Intel¬Æ RealSense‚Ñ¢ Viewer&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;With this application, you can quickly access your Intel¬Æ RealSense‚Ñ¢ Depth Camera to view the depth stream, visualize point clouds, record and playback streams, configure your camera settings, modify advanced controls, enable depth visualization and post processing and much more.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IntelRealSense/librealsense/releases&#34;&gt;&lt;strong&gt;Intel.RealSense.Viewer.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/tools/depth-quality&#34;&gt;Depth Quality Tool&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This application allows you to test the camera‚Äôs depth quality, including: standard deviation from plane fit, normalized RMS ‚Äì the subpixel accuracy, distance accuracy and fill rate. You should be able to easily get and interpret several of the depth quality metrics and record and save the data for offline analysis.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IntelRealSense/librealsense/releases&#34;&gt;&lt;strong&gt;Depth.Quality.Tool.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/tools/&#34;&gt;Debug Tools&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Device enumeration, FW logger, etc as can be seen at the tools directory&lt;/td&gt; &#xA;   &lt;td&gt;Included in &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/releases&#34;&gt;&lt;strong&gt;Intel.RealSense.SDK.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples&#34;&gt;Code Samples&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;These simple examples demonstrate how to easily use the SDK to include code snippets that access the camera into your applications. Check some of the &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples&#34;&gt;&lt;strong&gt;C++ examples&lt;/strong&gt;&lt;/a&gt; including capture, pointcloud and more and basic &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples/C&#34;&gt;&lt;strong&gt;C examples&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Included in &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/releases&#34;&gt;&lt;strong&gt;Intel.RealSense.SDK.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/IntelRealSense/librealsense/tree/development/wrappers&#34;&gt;Wrappers&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/python&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/csharp&#34;&gt;C#/.NET&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/nodejs&#34;&gt;Node.js&lt;/a&gt; API, as well as integration with the following 3rd-party technologies: &lt;a href=&#34;https://github.com/intel-ros/realsense/releases&#34;&gt;ROS&lt;/a&gt;, &lt;a href=&#34;https://github.com/intel/ros2_intel_realsense&#34;&gt;ROS2&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/labview&#34;&gt;LabVIEW&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/opencv&#34;&gt;OpenCV&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/pcl&#34;&gt;PCL&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/unity&#34;&gt;Unity&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/matlab&#34;&gt;Matlab&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/openni2&#34;&gt;OpenNI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/unrealengine4&#34;&gt;UnrealEngine4&lt;/a&gt; and more to come.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Ready to Hack!&lt;/h2&gt; &#xA;&lt;p&gt;Our library offers a high level API for using Intel RealSense depth cameras (in addition to lower level ones). The following snippet shows how to start streaming frames and extracting the depth value of a pixel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// Create a Pipeline - this serves as a top-level API for streaming and processing frames&#xA;rs2::pipeline p;&#xA;&#xA;// Configure and start the pipeline&#xA;p.start();&#xA;&#xA;while (true)&#xA;{&#xA;    // Block program until frames arrive&#xA;    rs2::frameset frames = p.wait_for_frames();&#xA;&#xA;    // Try to get a frame of a depth image&#xA;    rs2::depth_frame depth = frames.get_depth_frame();&#xA;&#xA;    // Get the depth frame&#39;s dimensions&#xA;    float width = depth.get_width();&#xA;    float height = depth.get_height();&#xA;&#xA;    // Query the distance from the camera to the object in the center of the image&#xA;    float dist_to_center = depth.get_distance(width / 2, height / 2);&#xA;&#xA;    // Print the distance&#xA;    std::cout &amp;lt;&amp;lt; &#34;The camera is facing an object &#34; &amp;lt;&amp;lt; dist_to_center &amp;lt;&amp;lt; &#34; meters away \r&#34;;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on the library, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples&#34;&gt;examples&lt;/a&gt;, and read the &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc&#34;&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;In order to contribute to Intel RealSense SDK, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/LICENSE&#34;&gt;Apache License, Version 2.0&lt;/a&gt;. Copyright 2018 Intel Corporation&lt;/p&gt;</summary>
  </entry>
</feed>