<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-07T01:30:08Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>YuriSizuku/OnscripterYuri</title>
    <updated>2025-06-07T01:30:08Z</updated>
    <id>tag:github.com,2025-06-07:/YuriSizuku/OnscripterYuri</id>
    <link href="https://github.com/YuriSizuku/OnscripterYuri" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An enhancement ONScripter project porting to many platforms, especially web.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Onscripter-Yuri&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/YuriSizuku/OnscripterYuri?color=green&amp;amp;label=onsyuri&amp;amp;logo=4chan&amp;amp;style=flat-square&#34; alt=&#34;GitHub release&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/YuriSizuku/OnscripterYuri/build_web.yml?label=web(wasm)&amp;amp;logo=firefox&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/YuriSizuku/OnscripterYuri/build_android.yml?label=android(arm%7Carm64)&amp;amp;&amp;amp;logo=android&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/YuriSizuku/OnscripterYuri/build_win.yml?label=win_mingw(x86%7Cx64)&amp;amp;logo=mingww64&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/YuriSizuku/OnscripterYuri/build_win.yml?label=win_msvc(x86%7Cx64%7Carm64)&amp;amp;logo=codeblocks&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/YuriSizuku/OnscripterYuri/build_linux.yml?label=linux(x86%7Cx64%7Carm%7Carm64)&amp;amp;logo=linux&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/YuriSizuku/OnscripterYuri/build_darwin.yml?label=mac(x64%7Carm64)&amp;amp;logo=apple&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚òòÔ∏è An enhancement ONScripter project porting to many platforms, especially &lt;strong&gt;web&lt;/strong&gt; ÔºÅ&lt;br&gt; We also support for &lt;code&gt;windows&lt;/code&gt;, &lt;code&gt;linux&lt;/code&gt;, &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;android&lt;/code&gt;, &lt;code&gt;retroarch&lt;/code&gt; and &lt;code&gt;psv&lt;/code&gt;. This project is base on &lt;a href=&#34;https://github.com/jh10001/ONScripter-Jh&#34;&gt;ONScripter-Jh&lt;/a&gt; by &lt;code&gt;SDL2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Online Demo: &lt;a href=&#34;https://onsgame.netlify.app/lifegame/&#34;&gt;lifegame&lt;/a&gt;, &lt;a href=&#34;https://onsgame.netlify.app/luasnow/&#34;&gt;luasnow&lt;/a&gt;, &lt;a href=&#34;https://onsgame.netlify.app/noesis1/&#34;&gt;noesis1 (lazyload)&lt;/a&gt;&lt;br&gt; PSV: &lt;a href=&#34;https://github.com/YuriSizuku/psv-OnscripterJH/releases&#34;&gt;psv-OnscripterJH&lt;/a&gt;&lt;br&gt; Android : &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.yuri.onscripter&#34;&gt;Google PlayStore&lt;/a&gt;&lt;br&gt; Multi Platform: &lt;a href=&#34;https://github.com/YuriSizuku/OnscripterYuri/releases&#34;&gt;github action release&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YuriSizuku/OnscripterYuri/master/screenshot/onsyuri_mo2_webtest.png&#34; alt=&#34;onsyuri_webtest_mo2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;New features :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;script&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; lua script and animation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; support English half-width text, see &lt;a href=&#34;https://github.com/YuriSizuku/OnscripterYuri/issues/2&#34;&gt;Word wrapping&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;nt2&lt;/code&gt;, &lt;code&gt;nt3&lt;/code&gt; script encryption (Mine exclusive format)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; long click or touch to invoke menu&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;render&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fullscreen by &lt;code&gt;--fullscreen&lt;/code&gt; or &lt;code&gt;alt+enter&lt;/code&gt;, scretch to fullscreen by &lt;code&gt;--fullscreen2&lt;/code&gt; or &lt;code&gt;f10&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; arbitary resolution &lt;code&gt;--width&lt;/code&gt;, &lt;code&gt;--height&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; gles2 sharpness rendering by &lt;code&gt;--sharpness 1.0&lt;/code&gt; parameter, fix bug on windows&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;platform&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; windows &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; x86, x64 (local or cross compile by mingw, static link)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; amd64, arm64 (local msvc, vcpkg, contributed by &lt;a href=&#34;https://github.com/YuriSizuku/OnscripterYuri/pull/3&#34;&gt;ryank231231&lt;/a&gt;)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; video by system player&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; linux &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; x86, x64 (local compile, static or dynamic link)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; arm, aarch64 (cross compile, SDL2 build from raspberrypi, static link)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; retroarch (contributed by &lt;a href=&#34;https://github.com/iyzsong&#34;&gt;iyzsong&lt;/a&gt;)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; mac &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; x64, arm64 (local compile, contributed by &lt;a href=&#34;https://github.com/yujincheng08&#34;&gt;yujincheng08&lt;/a&gt;)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; web (by emscripten) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fs to save in indexdb&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; lazy load by &lt;del&gt;BrowserFS or worker&lt;/del&gt; async fetch to avoid block the audio&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; mobile web with touch, with webui menu&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; android &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SDK level above 21 (android 5.1, Lolipop)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; extern SD card by &lt;a href=&#34;https://github.com/YuriSizuku/android-SafFile&#34;&gt;SAF&lt;/a&gt;, and scoped storage&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; non-english charactor in path&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; video by system player&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; psv, see &lt;a href=&#34;https://github.com/YuriSizuku/psv-OnscripterJH&#34;&gt;psv-Onscripter&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;bugfix&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fix some bugs in origin version (can not read &lt;code&gt;00.txt&lt;/code&gt; problem)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fix lua animation problem&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fix android onresume gles null pointer&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fix android file stat problem (save not found)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;develop&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; clear camke project structure&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; well documention for develop and usage&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; scripts to compile or cross compile without pain&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; vscode and android studio project for multi enviroment&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ci in github action to automaticly build&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; use docker to build for all platform&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. Usage&lt;/h2&gt; &#xA;&lt;h3&gt;(1) general command&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./onsyuri --help&#xA;./onsyuri --root /path/to/game --save-dir /path/to/save --font /path/default.ttf --enc:sjis&#xA;./onsyuri --width 1280 --height 720 --sharpness=3.1&#xA;./onsyuri --fullscreen2 # fullscreen1 alt+f, fullscreen2 f10 (toggle stretch)&#xA;&#xA;Usage: onsyuri [option ...]&#xA;  -h, --help            show this help and exit&#xA;  -v, --version         show the version information and exit&#xA;&#xA; load options:&#xA;  -f, --font file       set a TTF font file&#xA;  -r, --root path       set the root path to the archives&#xA;      --save-dir        set save dir&#xA;      --debug:1         print debug info&#xA;      --enc:sjis        use sjis coding script&#xA;&#xA; render options:&#xA;      --window          start in windowed mode&#xA;      --width 1280      force window width&#xA;      --height 720      force window height&#xA;      --fullscreen      start in fullscreen mode (alt+f or alt+enter)&#xA;      --fullscreen2     start in fullscreen mode with stretch (f10 to toggle stretch)&#xA;      --sharpness 3.1    use gles to make image sharp&#xA;      --no-video        do not decode video&#xA;      --no-vsync        turn off vsync&#xA;&#xA; other options:&#xA;      --cdaudio         use CD audio if available&#xA;      --cdnumber no     choose the CD-ROM drive number&#xA;      --registry file   set a registry file&#xA;      --dll file        set a dll file&#xA;      --enable-wheeldown-advance        advance the text on mouse wheel down&#xA;      --disable-rescale do not rescale the images in the archives&#xA;      --force-button-shortcut   ignore useescspc and getenter command&#xA;      --render-font-outline     render the outline of a text instead of casting a shadow&#xA;      --edit            enable online modification of the volume and variables when &#39;z&#39; is pressed&#xA;      --key-exe file    set a file (*.EXE) that includes a key table&#xA;      --fontcache       cache default font&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‚ùó If you force exit the game, the save might be damaged, try to remvoe envdata to play again.&lt;/p&gt; &#xA;&lt;h3&gt;(2) linux&lt;/h3&gt; &#xA;&lt;p&gt;You can either download the prebuild static elf from the &lt;a href=&#34;https://github.com/YuriSizuku/OnscripterYuri/releases&#34;&gt;release&lt;/a&gt; or build from source (see next part).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YuriSizuku/OnscripterYuri/master/screenshot/onsyuri_mo2_linuxtest.jpg&#34; alt=&#34;onsyuri_mo2_linuxtest&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Arch User Repository&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://aur.archlinux.org/packages/onscripter-yuri&#34;&gt;aur onscripter-yuri&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# for Arch based distributions, install directly from AUR.&#xA;yay -S onscripter-yuri&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(3) web&lt;/h3&gt; &#xA;&lt;p&gt;This project can run in a browser through hosted web server.&lt;br&gt; Press &lt;code&gt;F10&lt;/code&gt; to strech full in a webpage, &lt;code&gt;F11&lt;/code&gt; in fullscreen, &lt;code&gt;F9 | Right Click| Long Click&lt;/code&gt; to invoke menu.&lt;/p&gt; &#xA;&lt;p&gt;The structure is as bellow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;onsyuri.html&#xA;onsyuri.js&#xA;onsyuri.wasm&#xA;onsyuri_index.json&#xA;[your game files]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YuriSizuku/OnscripterYuri/master/screenshot/onsyuri_mo2_webtest3.jpg&#34; alt=&#34;onsyuri_mo2_webtest3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It will load the game according to &lt;code&gt;onsyuri_index.json&lt;/code&gt;, whitch is defined by &lt;code&gt;&amp;lt;meta onsyuri_index=&#34;onsyuri_index.json&#34;&amp;gt;&lt;/code&gt; in &lt;code&gt;onsyuri.html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;title&#34;: &#34;game1&#34;,&#xA;  &#34;gamedir&#34;: &#34;/onsyuri/game1&#34;,&#xA;  &#34;savedir&#34;: &#34;/onsyuri_save/game1&#34;,&#xA;  &#34;args&#34;: [],&#xA;  &#34;lazyload&#34;: &#34;true&#34;,&#xA;  &#34;files&#34;:[&#xA;    {&#34;path&#34;: &#34;0.txt&#34; , &#34;url&#34;:&#34;http://localhost:5500/asset/test/0.txt&#34;},&#xA;    {&#34;path&#34;: &#34;bgm/bgm.ogg&#34; , &#34;url&#34;:&#34;http://localhost:5500/asset/test/bgm/bgm.ogg&#34;},&#xA;    {&#34;path&#34;: &#34;default.ttf&#34; , &#34;url&#34;:&#34;http://localhost:5500/asset/test/default.ttf&#34;}&#xA;  ]&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This can be generated by &lt;code&gt;onsyuri_index.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;del&gt;Futhremoe, use &lt;code&gt;make_xhrfs_index &amp;gt; onsyuri_lazyload.json&lt;/code&gt; after &lt;code&gt;npm i -g browserfs&lt;/code&gt;&lt;/del&gt; deprecated.&lt;/p&gt; &#xA;&lt;h3&gt;(4) android&lt;/h3&gt; &#xA;&lt;p&gt;Install the apk, use &lt;code&gt;3 figures|long touch&lt;/code&gt; for invoke menu, &lt;code&gt;4 figures&lt;/code&gt; to invoke skip.&lt;br&gt; You can either put game into &lt;code&gt;/storage/emulated/0/Android/data/com.yuri.onscripter/files&lt;/code&gt;, or any directory by &lt;code&gt;saf&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YuriSizuku/OnscripterYuri/master/screenshot/onsyuri_ui_androidtest2.jpg&#34; alt=&#34;onsyuri_ui_androidtest1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2. Build&lt;/h2&gt; &#xA;&lt;h3&gt;(1) local windows&lt;/h3&gt; &#xA;&lt;p&gt;Install the dependency in msys2,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pacman -Syu --noconfirm&#xA;pacman -S --noconfirm make tar vim curl # util tools&#xA;pacman -S --noconfirm mingw-w64-x86_64-binutils mingw-w64-x86_64-gcc mingw-w64-x86_64-gdb # mingw64 compile tool&#xA;pacman -S --noconfirm mingw-w64-i686-binutils mingw-w64-i686-gcc mingw-w64-i686-gdb # mingw32 compile tool&#xA;&#xA;pacman -S --noconfirm mingw-w64-i686-SDL2 mingw-w64-x86_64-SDL2&#xA;pacman -S --noconfirm mingw-w64-i686-SDL2_image mingw-w64-x86_64-SDL2_image&#xA;pacman -S --noconfirm mingw-w64-i686-SDL2_ttf mingw-w64-x86_64-SDL2_ttf&#xA;pacman -S --noconfirm mingw-w64-i686-SDL2_mixer mingw-w64-x86_64-SDL2_mixer&#xA;&#xA;pacman -S --noconfirm mingw-w64-i686-brotli mingw-w64-x86_64-brotli&#xA;pacman -S --noconfirm mingw-w64-i686-mesa mingw-w64-x86_64-mesa&#xA;pacman -S --noconfirm mingw-w64-i686-lua mingw-w64-x86_64-lua&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then use these &lt;code&gt;local_msys2mingw32.sh&lt;/code&gt; or &lt;code&gt;local_msys2mingw64.sh&lt;/code&gt; to build.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd script&#xA;chmod +x *.sh&#xA;sh -c &#34;export BUILD_TYPE=Debug &amp;amp;&amp;amp; export MSYS2_HOME=/path/to/msys2 &amp;amp;&amp;amp; ./local_msys2mingw32.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(2) local linux&lt;/h3&gt; &#xA;&lt;p&gt;Install the dependency&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# linux64&#xA;sudo apt-get update&#xA;sudo apt-get -y install gcc gdb make cmake git curl&#xA;sudo apt-get -y install libsdl2-dev libsdl2-ttf-dev libsdl2-image-dev libsdl2-mixer-dev&#xA;sudo apt-get -y install libbz2-dev libjpeg-dev libpng-dev&#xA;sudo apt-get -y install liblua5.3-dev libgl1-mesa-dev&#xA;&#xA;# linux32&#xA;sudo dpkg --add-architecture i386 &#xA;sudo apt-get update&#xA;sudo apt-get -y install gcc-multilib g++-multilib &#xA;sudo apt-get -y install libsdl2-dev:i386 libsdl2-ttf-dev:i386 libsdl2-image-dev:i386 libsdl2-mixer-dev:i386&#xA;sudo apt-get -y install libbz2-dev:i386 libjpeg-dev:i386 libpng-dev:i386&#xA;sudo apt-get -y install liblua5.3-dev:i386 libgl1-mesa-dev:i386&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd script&#xA;chmod +x *.sh&#xA;sh -c &#34;export BUILD_TYPE=Debug &amp;amp;&amp;amp; ./local_linux32.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then use &lt;code&gt;local_linux32.sh&lt;/code&gt; or &lt;code&gt;local_linux64.sh&lt;/code&gt; to build.&lt;/p&gt; &#xA;&lt;h3&gt;(3) cross web&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/emscripten-core/emsdk&#34;&gt;emsdk&lt;/a&gt; and use &lt;code&gt;cross_web.sh&lt;/code&gt; to build.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd script&#xA;chmod +x *.sh&#xA;sh -c &#34;export BUILD_TYPE=Debug &amp;amp;&amp;amp; export EMCSDK=/path/to/emsdk &amp;amp;&amp;amp; ./cross_web.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(4) cross linux arm&lt;/h3&gt; &#xA;&lt;p&gt;This is aimed for raspberrypi or the other arm64 devices cross compiling. As there are many system bindings in SDL2,&lt;br&gt; just build libraries in the target machine, and use these build cache to link.&lt;br&gt; Or you can use &lt;code&gt;docker_linuxarm64.sh&lt;/code&gt; for cross compile.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YuriSizuku/OnscripterYuri/master/screenshot/onsyuri_mo2_linuxtest2.png&#34; alt=&#34;onsyuri_mo2_linuxtest2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install the dependency for aarch64 cross compiler,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install in the target machine, aarch64&#xA;sudo apt-get -y install libx11-dev libxext-dev libasound2-dev &#xA;sudo apt-get -y install libgl1-mesa-dev mesa-utils&#xA;&#xA;# install in the target machine, armhf (if you want to run armhf in aarch64)&#xA;sudo dpkg --add-architecture armhf &amp;amp;&amp;amp; sudo apt-get update&#xA;sudo apt-get -y install libc6:armhf &#xA;sudo apt-get -y install libx11-dev:armhf libxext-dev:armhf libasound2-dev:armhf &#xA;sudo apt-get -y install libgl1-mesa-dev:armhf mesa-utils:armhf&#xA;&#xA;# install in the local machine&#xA;sudo apt-get -y install tar make cmake curl git&#xA;sudo apt-get -y install crossbuild-essential-armhf&#xA;sudo apt-get -y install crossbuild-essential-arm64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then use &lt;code&gt;cross_linuxa64.sh&lt;/code&gt; or &lt;code&gt;cross_linuxa32.sh&lt;/code&gt; to compile.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# at first build sdl2 in raspberry pi&#xA;sh -c &#34;export SYSROOT=/ &amp;amp;&amp;amp; ./local_linux64.sh&#34;&#xA;&#xA;# copy prebuild of dependency to local&#xA;cp -rf /path/to/rpi/OnscripterYuri/thirdparty/build/arch_aarch64 thirdparty/build/arch_aarch64&#xA;&#xA;# use SKIP_PORTS to skip thirdpart builds&#xA;cd script&#xA;chmod +x *.sh&#xA;sh -c &#34;export BUILD_TYPE=Debug &amp;amp;&amp;amp; export SKIP_PORTS=yes &amp;amp;&amp;amp; ./local_linux64.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(5) cross mingw&lt;/h3&gt; &#xA;&lt;p&gt;Install mingw cross compiler and tools,&lt;/p&gt; &#xA;&lt;p&gt;in &lt;code&gt;debian&lt;/code&gt; or &lt;code&gt;wsl2&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get -y install tar make cmake curl git&#xA;# use pkg-config is to find sdl2 by compiling sdl2_image, do not install mingw-w64-tools, this pkg-config is broken&#xA;sudo apt-get -y install mingw-w64 zstd pkg-config &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in &lt;code&gt;msys2&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pacman -Syu --noconfirm&#xA;pacman -S --noconfirm make tar vim curl # util tools&#xA;pacman -S --noconfirm mingw-w64-x86_64-gcc mingw-w64-x86_64-gdb&#xA;pacman -S --noconfirm mingw-w64-x86_64-binutils mingw-w64-x86_64-pkg-config&#xA;pacman -S --noconfirm mingw-w64-i686-gcc mingw-w64-i686-gdb&#xA;pacman -S --noconfirm mingw-w64-i686-binutils mingw-w64-i686-pkg-config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then use &lt;code&gt;cross_mingw32.sh&lt;/code&gt; or &lt;code&gt;cross_mingw64.sh&lt;/code&gt; to compile.&lt;/p&gt; &#xA;&lt;p&gt;If you want to make compatible for &lt;code&gt;winxp&lt;/code&gt;, should use gcc below 12.&lt;/p&gt; &#xA;&lt;h3&gt;(6) cross llvmmingw&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/mstorsjo/llvm-mingw/releases/tag/20240619&#34;&gt;llvm-mingw&lt;/a&gt; and add &lt;code&gt;${MINGWSDK_HOME}/bin&lt;/code&gt; to path,&lt;/p&gt; &#xA;&lt;p&gt;then use &lt;code&gt;cross_llvmmingw32.sh&lt;/code&gt; or &lt;code&gt;cross_llvmmingw64.sh&lt;/code&gt; to compile (either &lt;code&gt;bash (git bash)&lt;/code&gt; or &lt;code&gt;msys2 shell&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If you want to build compatible for &lt;code&gt;winxp&lt;/code&gt;, you can use &lt;a href=&#34;https://github.com/niXman/mingw-builds-binaries/releases/download/11.2.0-rt_v9-rev1/i686-11.2.0-release-win32-dwarf-rt_v9-rev1.7z&#34;&gt;i686-11.2.0-release-win32-dwarf-rt_v9-rev1&lt;/a&gt;. Higher version like &lt;code&gt;gcc 14&lt;/code&gt; is not available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export PATH=/path/to/mingw32/bin:$PATH&#xA;export CC=i686-w64-mingw32-gcc&#xA;export CXX=i686-w64-mingw32-g++&#xA;export RC=windres&#xA;&#xA;echo CC=$(which $CC)&#xA;echo CXX=$(which $CXX)&#xA;echo RC=$(which $RC)&#xA;&#xA;bash -c &#34;./cross_llvmmingw32.sh&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(7) cross android&lt;/h3&gt; &#xA;&lt;p&gt;Install android sdk and ndk, then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# prepare port dependencies&#xA;cd script&#xA;sh ./cross_android.sh &#xA;cd -&#xA;&#xA;# use ANDROID_HOME or local.properties for sdk&#xA;cd src/onsyuri_android/&#xA;chmod +x ./gradlew &amp;amp;&amp;amp; ./gradlew assembleDebug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(8) cross by docker&lt;/h3&gt; &#xA;&lt;p&gt;You can easily build all supported platforms by docker, see &lt;code&gt;docker/docker_xxx.sh&lt;/code&gt; in detail. If you want to build for linux arm in x86 platform, install qemu at first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install qemu-user-static binfmt-support&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Compatibility&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;game&lt;/th&gt; &#xA;   &lt;th&gt;version&lt;/th&gt; &#xA;   &lt;th&gt;status&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;4. Issues (including already solved)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;general&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;del&gt;release file too big&lt;/del&gt;; This is because of static link all libraries for better compatibility. Partly solved by recomiple SDL libraries.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;windows&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;del&gt;windows fullscreen aliasing&lt;/del&gt;; This is because window high dpi scale problem, use &lt;code&gt;change high dpi setting&lt;/code&gt; or edit reg as below:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;set app_name=onsyuri.exe&#xA;reg add &#34;HKCU\SOFTWARE\Microsoft\Windows NT\CurrentVersion\AppCompatFlags\Layers&#34; /t reg_sz /v &#34;%~dp0%app_name%&#34; /d &#34;~ HIGHDPIAWARE&#34; /f&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;linux&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;android&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;web&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;del&gt;lazy load in workerfs not work, see this &lt;a href=&#34;https://github.com/emscripten-core/emscripten/issues/18698&#34;&gt;issue&lt;/a&gt;&lt;/del&gt; ; solved by using &lt;a href=&#34;https://github.com/jvilk/BrowserFS&#34;&gt;BrowserFS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;del&gt;Audio glitch problem in slow network&lt;/del&gt; ; solved by async fetch without blocking audio&lt;/li&gt; &#xA;   &lt;li&gt;SDL2 Mix_LoadMUS can not decode mp3&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;del&gt;video support (future plan)&lt;/del&gt; partly finish by invoke system player&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;web preloading (future plan)&lt;/del&gt; might not need ? lazyload partly solved&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ai-dynamo/nixl</title>
    <updated>2025-06-07T01:30:08Z</updated>
    <id>tag:github.com,2025-06-07:/ai-dynamo/nixl</id>
    <link href="https://github.com/ai-dynamo/nixl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA Inference Xfer Library (NIXL)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA Inference Xfer Library (NIXL)&lt;/h1&gt; &#xA;&lt;p&gt;NVIDIA Inference Xfer Library (NIXL) is targeted for accelerating point to point communications in AI inference frameworks such as NVIDIA Dynamo, while providing an abstraction over various types of memory (e.g., CPU and GPU) and storage (e.g., file, block and object store) through a modular plug-in architecture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ai-dynamo/nixl/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/ai-dynamo/nixl&#34; alt=&#34;GitHub Release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pre-build Distributions&lt;/h2&gt; &#xA;&lt;h3&gt;PyPI Wheel&lt;/h3&gt; &#xA;&lt;p&gt;The nixl python API and libraries, including UCX, are available directly through PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nixl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prerequisites for source build&lt;/h2&gt; &#xA;&lt;h3&gt;Ubuntu:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;$ sudo apt install build-essential cmake pkg-config&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fedora:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;$ sudo dnf install gcc-c++ cmake pkg-config&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;$ pip3 install meson ninja pybind11&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;UCX&lt;/h3&gt; &#xA;&lt;p&gt;NIXL was tested with UCX version 1.18.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/gdrcopy&#34;&gt;GDRCopy&lt;/a&gt; is available on Github and is necessary for maximum performance, but UCX and NIXL will work without it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/openucx/ucx/releases/download/v1.18.0/ucx-1.18.0.tar.gz&#xA;$ tar xzf ucx-1.18.0.tar.gz&#xA;$ cd ucx-1.18.0&#xA;$ ./configure                          \&#xA;    --enable-shared                    \&#xA;    --disable-static                   \&#xA;    --disable-doxygen-doc              \&#xA;    --enable-optimizations             \&#xA;    --enable-cma                       \&#xA;    --enable-devel-headers             \&#xA;    --with-cuda=&amp;lt;cuda install&amp;gt;         \&#xA;    --with-verbs                       \&#xA;    --with-dm                          \&#xA;    --with-gdrcopy=&amp;lt;gdrcopy install&amp;gt;   \&#xA;    --enable-mt&#xA;$ make -j&#xA;$ make -j install-strip&#xA;$ ldconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ETCD (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;NIXL can use ETCD for metadata distribution and coordination between nodes in distributed environments. To use ETCD with NIXL:&lt;/p&gt; &#xA;&lt;h4&gt;ETCD Server and Client&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt install etcd etcd-server etcd-client&#xA;&#xA;# Or use Docker&#xA;$ docker run -d -p 2379:2379 quay.io/coreos/etcd:v3.5.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;ETCD CPP API&lt;/h4&gt; &#xA;&lt;p&gt;Installed from &lt;a href=&#34;https://github.com/etcd-cpp-apiv3/etcd-cpp-apiv3&#34;&gt;https://github.com/etcd-cpp-apiv3/etcd-cpp-apiv3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt install libgrpc-dev libgrpc++-dev libprotobuf-dev protobuf-compiler-grpc&#xA;$ sudo apt install libcpprest-dev&#xA;$ git clone https://github.com/etcd-cpp-apiv3/etcd-cpp-apiv3.git&#xA;$ cd etcd-cpp-apiv3&#xA;$ mkdir build &amp;amp;&amp;amp; cd build&#xA;$ cmake ..&#xA;$ make -j$(nproc) &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional plugins&lt;/h3&gt; &#xA;&lt;p&gt;Some plugins may have additional build requirements, see them here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ai-dynamo/nixl/main/src/plugins/mooncake/README.md&#34;&gt;Mooncake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ai-dynamo/nixl/main/src/plugins/posix/README.md&#34;&gt;POSIX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ai-dynamo/nixl/main/src/plugins/cuda_gds/README.md&#34;&gt;GDS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Build &amp;amp; install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ meson setup &amp;lt;name_of_build_dir&amp;gt;&#xA;$ cd &amp;lt;name_of_build_dir&amp;gt;&#xA;$ ninja&#xA;$ ninja install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build Options&lt;/h3&gt; &#xA;&lt;p&gt;NIXL supports several build options that can be specified during the meson setup phase:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic build setup with default options&#xA;$ meson setup &amp;lt;name_of_build_dir&amp;gt;&#xA;&#xA;# Setup with custom options (example)&#xA;$ meson setup &amp;lt;name_of_build_dir&amp;gt; \&#xA;    -Dbuild_docs=true \           # Build Doxygen documentation&#xA;    -Ducx_path=/path/to/ucx \     # Custom UCX installation path&#xA;    -Dinstall_headers=true \      # Install development headers&#xA;    -Ddisable_gds_backend=false   # Enable GDS backend&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Common build options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;build_docs&lt;/code&gt;: Build Doxygen documentation (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ucx_path&lt;/code&gt;: Path to UCX installation (default: system path)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;install_headers&lt;/code&gt;: Install development headers (default: true)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;disable_gds_backend&lt;/code&gt;: Disable GDS backend (default: false)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cudapath_inc&lt;/code&gt;, &lt;code&gt;cudapath_lib&lt;/code&gt;: Custom CUDA paths&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;static_plugins&lt;/code&gt;: Comma-separated list of plugins to build statically&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Building Documentation&lt;/h3&gt; &#xA;&lt;p&gt;If you have Doxygen installed, you can build the documentation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Configure with documentation enabled&#xA;$ meson setup &amp;lt;name_of_build_dir&amp;gt; -Dbuild_docs=true&#xA;$ cd &amp;lt;name_of_build_dir&amp;gt;&#xA;$ ninja&#xA;&#xA;# Documentation will be generated in &amp;lt;name_of_build_dir&amp;gt;/html&#xA;# After installation (ninja install), documentation will be available in &amp;lt;prefix&amp;gt;/share/doc/nixl/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;pybind11 Python Interface&lt;/h3&gt; &#xA;&lt;p&gt;The pybind11 bindings for the public facing NIXL API are available in src/bindings/python. These bindings implement the headers in the src/api/cpp directory.&lt;/p&gt; &#xA;&lt;p&gt;The preferred way is to build it through meson-python, which will just let it be installed with pip. This can be done from the root nixl directory:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt; $ pip install .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Rust Bindings&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Build with default NIXL installation (/opt/nvidia/nvda_nixl)&#xA;$ cd src/bindings/rust&#xA;$ cargo build --release&#xA;&#xA;# Or specify custom NIXL location&#xA;$ NIXL_PREFIX=/path/to/nixl cargo build --release&#xA;&#xA;# Run tests&#xA;$ cargo test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use in your project by adding to &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[dependencies]&#xA;nixl-sys = { path = &#34;path/to/nixl/bindings/rust&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other build options&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ai-dynamo/nixl/main/contrib/README.md&#34;&gt;contrib/README.md&lt;/a&gt; for more build options.&lt;/p&gt; &#xA;&lt;h3&gt;Building Docker container&lt;/h3&gt; &#xA;&lt;p&gt;To build the docker container, first clone the current repository. Also make sure you are able to pull docker images to your machine before attempting to build the container.&lt;/p&gt; &#xA;&lt;p&gt;Run the following from the root folder of the cloned NIXL repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./contrib/build-container.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the container is built with Ubuntu 24.04. To build a container for Ubuntu 22.04 use the --os option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./contrib/build-container.sh --os ubuntu22&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see all the options supported by the container use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./contrib/build-container.sh -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The container also includes a prebuilt python wheel in /workspace/dist if required for installing/distributing. Also, the wheel can be built with a separate script (see below).&lt;/p&gt; &#xA;&lt;h3&gt;Building the python wheel&lt;/h3&gt; &#xA;&lt;p&gt;The contrib folder also includes a script to build the python wheel with the UCX dependencies. Note, that UCX and other NIXL dependencies are required to be installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./contrib/build-wheel.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running with ETCD&lt;/h2&gt; &#xA;&lt;p&gt;NIXL can use ETCD for metadata exchange between distributed nodes. This is especially useful in containerized or cloud-native environments.&lt;/p&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;p&gt;To use ETCD with NIXL, set the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set ETCD endpoints (required) - replace localhost with the hostname of the etcd server&#xA;export NIXL_ETCD_ENDPOINTS=&#34;http://localhost:2379&#34;&#xA;&#xA;# Set ETCD namespace (optional, defaults to /nixl/agents)&#xA;export NIXL_ETCD_NAMESPACE=&#34;/nixl/agents&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running the ETCD Example&lt;/h3&gt; &#xA;&lt;p&gt;NIXL includes an example demonstrating metadata exchange and data transfer using ETCD:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start an ETCD server if not already running&#xA;# For example:&#xA;# docker run -d -p 2379:2379 quay.io/coreos/etcd:v3.5.1&#xA;&#xA;# Set the ETCD env variables as above&#xA;&#xA;# Run the example. The two agents in the example will exchange metadata through ETCD&#xA;# and perform data transfers&#xA;./&amp;lt;nixl_build_path&amp;gt;/examples/nixl_etcd_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;nixlbench Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;For more comprehensive testing, the nixlbench benchmarking tool supports ETCD for worker coordination:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Build nixlbench (see benchmark/nixlbench/README.md for details)&#xA;cd benchmark/nixlbench&#xA;meson setup build &amp;amp;&amp;amp; cd build &amp;amp;&amp;amp; ninja&#xA;&#xA;# Run benchmark with ETCD&#xA;./nixlbench --etcd-endpoints http://localhost:2379 --backend UCX --initiator_seg_type VRAM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ai-dynamo/nixl/tree/main/examples/cpp&#34;&gt;C++ examples&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ai-dynamo/nixl/tree/main/examples/python&#34;&gt;Python examples&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kvcache-ai/Mooncake</title>
    <updated>2025-06-07T01:30:08Z</updated>
    <id>tag:github.com,2025-06-07:/kvcache-ai/Mooncake</id>
    <link href="https://github.com/kvcache-ai/Mooncake" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/image/mooncake-icon.png&#34; width=&#34;44%&#34;&gt; &#xA; &lt;h2 align=&#34;center&#34;&gt; A KVCache-centric Disaggregated Architecture for LLM Serving &lt;/h2&gt; &#xA; &lt;a href=&#34;https://www.usenix.org/system/files/fast25-qin.pdf&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://www.usenix.org/system/files/fast25_slides-qin.pdf&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/FAST25-release/traces&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Traces&lt;/strong&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://arxiv.org/abs/2407.00079&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Technical Report&lt;/strong&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://kvcache-ai.github.io/Mooncake/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Mooncake is the serving platform for &lt;a href=&#34;https://kimi.ai/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/image/kimi.png&#34; alt=&#34;icon&#34; style=&#34;height: 16px; vertical-align: middle;&#34;&gt; Kimi&lt;/a&gt;, a leading LLM service provided by &lt;a href=&#34;https://www.moonshot.cn/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/image/moonshot.jpg&#34; alt=&#34;icon&#34; style=&#34;height: 16px; vertical-align: middle;&#34;&gt; Moonshot AI&lt;/a&gt;. Now both the Transfer Engine and Mooncake Store are open-sourced! This repository also hosts its technical report and the open sourced traces.&lt;/p&gt; &#xA;&lt;h2 id=&#34;updates&#34;&gt;üîÑ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;May 9, 2025&lt;/strong&gt;: NIXL officially supports Mooncake Transfer Engine as &lt;a href=&#34;https://github.com/ai-dynamo/nixl/raw/main/src/plugins/mooncake/README.md&#34;&gt;a backend plugin&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;May 8, 2025&lt;/strong&gt;: Mooncake x LMCache &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/lmcache-integration.md&#34; target=&#34;_blank&#34;&gt;unite&lt;/a&gt; to pioneer KVCache-centric LLM serving system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;May 5, 2025&lt;/strong&gt;: Supported by Mooncake Team, SGLang release &lt;a href=&#34;https://lmsys.org/blog/2025-05-05-large-scale-ep/&#34; target=&#34;_blank&#34;&gt;guidance&lt;/a&gt; to deploy DeepSeek with PD Disaggregation on 96 H100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr 22, 2025&lt;/strong&gt;: LMCache officially supports Mooncake Store as a &lt;a href=&#34;https://blog.lmcache.ai/2025-04-22-tencent/&#34; target=&#34;_blank&#34;&gt;remote connector&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr 10, 2025&lt;/strong&gt;: SGLang officially supports Mooncake Transfer Engine for disaggregated prefilling and KV cache transfer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mar 7, 2025&lt;/strong&gt;: We open sourced the Mooncake Store, a distributed KVCache based on Transfer Engine. vLLM&#39;s xPyD disaggregated prefilling &amp;amp; decoding based on Mooncake Store will be released soon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feb 25, 2025&lt;/strong&gt;: Mooncake receives the &lt;strong&gt;Best Paper Award&lt;/strong&gt; at &lt;strong&gt;FAST 2025&lt;/strong&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feb 21, 2025&lt;/strong&gt;: The updated &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/FAST25-release/traces&#34; target=&#34;_blank&#34;&gt;traces&lt;/a&gt; used in our FAST&#39;25 paper have been released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dec 16, 2024&lt;/strong&gt;: vLLM officially supports Mooncake Transfer Engine for disaggregated prefilling and KV cache transfer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nov 28, 2024&lt;/strong&gt;: We open sourced the Transfer Engine, the central component of Mooncake. We also provide two demonstrations of Transfer Engine: a P2P Store and vLLM integration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;July 9, 2024&lt;/strong&gt;: We open sourced the trace as a &lt;a href=&#34;https://github.com/kvcache-ai/Mooncake/raw/main/mooncake_trace.jsonl&#34; target=&#34;_blank&#34;&gt;jsonl file&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;June 27, 2024&lt;/strong&gt;: We present a series of Chinese blogs with more discussions on &lt;a href=&#34;https://zhuanlan.zhihu.com/p/705754254&#34;&gt;zhihu 1&lt;/a&gt;, &lt;a href=&#34;https://zhuanlan.zhihu.com/p/705910725&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://zhuanlan.zhihu.com/p/706204757&#34;&gt;3&lt;/a&gt;, &lt;a href=&#34;https://zhuanlan.zhihu.com/p/707997501&#34;&gt;4&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;June 26, 2024&lt;/strong&gt;: Initial technical report release.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 id=&#34;overview&#34;&gt;üéâ Overview&lt;/h2&gt; &#xA;&lt;p&gt;Mooncake features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/image/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs) requirements. Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake‚Äôs innovative architecture enables &lt;a href=&#34;https://kimi.ai/&#34;&gt;Kimi&lt;/a&gt; to handle 75% more requests.&lt;/p&gt; &#xA;&lt;h2 id=&#34;components&#34;&gt;üß© Components&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/image/components.png&#34; alt=&#34;components&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mooncake Core Component: Transfer Engine (TE)&lt;/strong&gt;&lt;br&gt; The core of Mooncake is the Transfer Engine (TE), which provides a unified interface for batched data transfer across various storage devices and network links. Supporting multiple protocols including TCP, RDMA, CXL/shared-memory, and NVMe over Fabric (NVMe-of), TE is designed to enable fast and reliable data transfer for AI workloads. Compared to Gloo (used by Distributed PyTorch) and traditional TCP, TE achieves significantly lower I/O latency, making it a superior solution for efficient data transmission.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;P2P Store and Mooncake Store&lt;/strong&gt;&lt;br&gt; Both P2P Store and Mooncake Store are built on the Transfer Engine and provide key/value caching for different scenarios. P2P Store focuses on sharing temporary objects (e.g., checkpoint files) across nodes in a cluster, preventing bandwidth saturation on a single machine. Mooncake Store, on the other hand, supports distributed pooled KVCache, specifically designed for XpYd disaggregation to enhance resource utilization and system performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mooncake Integration with Leading LLM Inference Systems&lt;/strong&gt;&lt;br&gt; Mooncake has been seamlessly integrated with several popular large language model (LLM) inference systems. Through collaboration with the vLLM and SGLang teams, Mooncake now officially supports prefill-decode disaggregation. By leveraging the high-efficiency communication capabilities of RDMA devices, Mooncake significantly improves inference efficiency in prefill-decode disaggregation scenarios, providing robust technical support for large-scale distributed inference tasks.&lt;/p&gt; &#xA;&lt;h2 id=&#34;show-cases&#34;&gt;üî• Show Cases&lt;/h2&gt; &#xA;&lt;h3&gt;Use Transfer Engine Standalone (&lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/transfer-engine.md&#34;&gt;Guide&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Transfer Engine is a high-performance data transfer framework. Transfer Engine provides a unified interface to transfer data from DRAM, VRAM or NVMe, while the technical details related to hardware are hidden. Transfer Engine supports TCP, RDMA (InfiniBand/RoCEv2/eRDMA/NVIDIA GPUDirect) and NVMe over Fabric (NVMe-of) protocols.&lt;/p&gt; &#xA;&lt;h4&gt;Highlights&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient use of multiple RDMA NIC devices.&lt;/strong&gt; Transfer Engine supports the use of multiple RDMA NIC devices to achieve the &lt;em&gt;aggregation of transfer bandwidth&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Topology aware path selection.&lt;/strong&gt; Transfer Engine can &lt;em&gt;select optimal devices&lt;/em&gt; based on the location (NUMA affinity, etc.) of both source and destination.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;More robust on temporary network error.&lt;/strong&gt; Once transmission fails, Transfer Engine will try to use alternative paths for data delivery automatically.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Performance&lt;/h4&gt; &#xA;&lt;p&gt;With 40 GB of data (equivalent to the size of the KVCache generated by 128k tokens in the LLaMA3-70B model), Mooncake Transfer Engine delivers up to &lt;strong&gt;87 GB/s&lt;/strong&gt; and &lt;strong&gt;190 GB/s&lt;/strong&gt; of bandwidth in 4√ó200 Gbps and 8√ó400 Gbps RoCE networks respectively, which are about &lt;strong&gt;2.4x and 4.6x faster&lt;/strong&gt; than the TCP protocol.&lt;/p&gt; &#xA;&lt;!-- ![transfer-engine-performance.png](image/transfer-engine-performance.png) --&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/image/transfer-engine-performance.png&#34; width=&#34;75%&#34;&gt; &#xA;&lt;h3&gt;P2P Store (&lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/p2p-store.md&#34;&gt;Guide&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;P2P Store is built on the Transfer Engine and supports sharing temporary objects between peer nodes in a cluster. P2P Store is ideal for scenarios like checkpoint transfer, where data needs to be rapidly and efficiently shared across a cluster. &lt;strong&gt;P2P Store has been used in the checkpoint transfer service of Moonshot AI.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Highlights&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Decentralized architecture.&lt;/strong&gt; P2P Store leverages a pure client-side architecture with global metadata managed by the etcd service.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient data distribution.&lt;/strong&gt; Designed to enhance the efficiency of large-scale data distribution, P2P Store &lt;em&gt;avoids bandwidth saturation&lt;/em&gt; issues by allowing replicated nodes to share data directly. This reduces the CPU/RDMA NIC pressures of data providers (e.g., trainers).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- #### Performance&#xA;Thanks to the high performance of Transfer Engine, P2P Stores can also distribute objects with full utilization of *hardware incoming bandwidth* (e.g., A 25Gbps NIC was used in the following figure, and the throughput of get replica is about 3.1 GB/s). --&gt; &#xA;&lt;!-- ![p2p-store.gif](image/p2p-store.gif) --&gt; &#xA;&lt;h3&gt;Mooncake Store (&lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/mooncake-store-preview.md&#34;&gt;Guide&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Mooncake Store is a distributed KVCache storage engine specialized for LLM inference based on Transfer Engine. It is the central component of the KVCache-centric disaggregated architecture. The goal of Mooncake Store is to store the reusable KV caches across various locations in an inference cluster. Mooncake Store has been supported in &lt;a href=&#34;https://docs.vllm.ai/en/latest/features/disagg_prefill.html&#34;&gt;vLLM&#39;s prefill serving&lt;/a&gt; and is now integrated with &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/lmcache-integration.md&#34;&gt;LMCache&lt;/a&gt; to provide enhanced KVCache management capabilities.&lt;/p&gt; &#xA;&lt;h4&gt;Highlights&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-replica support&lt;/strong&gt;: Mooncake Store supports storing multiple data replicas for the same object, effectively alleviating hotspots in access pressure.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;High bandwidth utilization&lt;/strong&gt;: Mooncake Store supports striping and parallel I/O transfer of large objects, fully utilizing multi-NIC aggregated bandwidth for high-speed data reads and writes.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;vLLM Integration (&lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/vllm-integration-v0.2.md&#34;&gt;Guide v0.2&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;To optimize LLM inference, the vLLM community is working on supporting &lt;a href=&#34;https://github.com/vllm-project/vllm/pull/10502&#34;&gt;disaggregated prefilling (PR 10502)&lt;/a&gt;. This feature allows separating the &lt;strong&gt;prefill&lt;/strong&gt; phase from the &lt;strong&gt;decode&lt;/strong&gt; phase in different processes. The vLLM uses &lt;code&gt;nccl&lt;/code&gt; and &lt;code&gt;gloo&lt;/code&gt; as the transport layer by default, but currently it cannot efficiently decouple both phases in different machines.&lt;/p&gt; &#xA;&lt;p&gt;We have implemented vLLM integration, which uses Transfer Engine as the network layer instead of &lt;code&gt;nccl&lt;/code&gt; and &lt;code&gt;gloo&lt;/code&gt;, to support &lt;strong&gt;inter-node KVCache transfer&lt;/strong&gt; &lt;a href=&#34;https://github.com/vllm-project/vllm/pull/10884&#34;&gt;(PR 10884)&lt;/a&gt;. Transfer Engine provides simpler interfaces and more efficient use of RDMA devices.&lt;/p&gt; &#xA;&lt;p&gt;We will soon release the new vLLM integration based on Mooncake Store, which supports xPyD prefill/decode disaggregation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update[Dec 16, 2024]: Here is the latest vLLM Integration (&lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/vllm-integration-v0.2.md&#34;&gt;Guide v0.2&lt;/a&gt;) that is based on vLLM&#39;s main branch.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Performance&lt;/h4&gt; &#xA;&lt;p&gt;By supporting Topology Aware Path Selection and multi-card bandwidth aggregation, Mean TTFT of vLLM with Transfer Engine is up to 25% lower than traditional TCP-based transports. In the future, we will further improve TTFT through GPUDirect RDMA and zero-copy.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Backend/Setting&lt;/th&gt; &#xA;   &lt;th&gt;Output Token Throughput (tok/s)&lt;/th&gt; &#xA;   &lt;th&gt;Total Token Throughput (tok/s)&lt;/th&gt; &#xA;   &lt;th&gt;Mean TTFT (ms)&lt;/th&gt; &#xA;   &lt;th&gt;Median TTFT (ms)&lt;/th&gt; &#xA;   &lt;th&gt;P99 TTFT (ms)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transfer Engine (RDMA)&lt;/td&gt; &#xA;   &lt;td&gt;12.06&lt;/td&gt; &#xA;   &lt;td&gt;2042.74&lt;/td&gt; &#xA;   &lt;td&gt;1056.76&lt;/td&gt; &#xA;   &lt;td&gt;635.00&lt;/td&gt; &#xA;   &lt;td&gt;4006.59&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TCP&lt;/td&gt; &#xA;   &lt;td&gt;12.05&lt;/td&gt; &#xA;   &lt;td&gt;2041.13&lt;/td&gt; &#xA;   &lt;td&gt;1414.05&lt;/td&gt; &#xA;   &lt;td&gt;766.23&lt;/td&gt; &#xA;   &lt;td&gt;6035.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Click &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/vllm-benchmark-results-v0.2.md&#34;&gt;here&lt;/a&gt; to access detailed benchmark results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;More advanced features will coming soon, so stay tuned!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2 id=&#34;quick-start&#34;&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Before using Mooncake&lt;/h3&gt; &#xA;&lt;p&gt;Mooncake is designed and optimized for high-speed RDMA networks. Though Mooncake supports TCP-only data transfer, we &lt;strong&gt;strongly&lt;/strong&gt; recommend users to evaluate the functionality and performance of Mooncake with RDMA network support.&lt;/p&gt; &#xA;&lt;p&gt;The following needs to be installed before running any component of Mooncake:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RDMA Driver &amp;amp; SDK, such as Mellanox OFED.&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10, virtual environment is recommended.&lt;/li&gt; &#xA; &lt;li&gt;CUDA 12.1 and above, including NVIDIA GPUDirect Storage Support, if the package is build with &lt;code&gt;-DUSE_CUDA&lt;/code&gt; (disabled by default). &lt;em&gt;You may install them from &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Use Python package&lt;/h3&gt; &#xA;&lt;p&gt;The most simple way to use Mooncake Transfer Engine is using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install mooncake-transfer-engine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] If users encounter problems such as missing &lt;code&gt;lib*.so&lt;/code&gt;, they should uninstall this package by &lt;code&gt;pip uninstall mooncake-transfer-engine&lt;/code&gt;, and build the binaries manually.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Use Docker image&lt;/h3&gt; &#xA;&lt;p&gt;Mooncake supports Docker-based deployment, see &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/doc/en/build.md&#34;&gt;Build Guide&lt;/a&gt; in detail.&lt;/p&gt; &#xA;&lt;h3&gt;Build and use binaries&lt;/h3&gt; &#xA;&lt;p&gt;The following are additional dependencies of building Mooncake:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build essentials, including gcc, g++ (9.4+) and cmake (3.16+).&lt;/li&gt; &#xA; &lt;li&gt;Go 1.20+, if you want to build with &lt;code&gt;-DWITH_P2P_STORE&lt;/code&gt; or &lt;code&gt;-DUSE_ETCD&lt;/code&gt; (enabled by default).&lt;/li&gt; &#xA; &lt;li&gt;CUDA 12.1 and above, including NVIDIA GPUDirect Storage Support, if the package is build with &lt;code&gt;-DUSE_CUDA&lt;/code&gt; . &lt;em&gt;This is NOT included in the &lt;code&gt;dependencies.sh&lt;/code&gt; script. You may install them from &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[Optional] Rust Toolclain, if you want to build with &lt;code&gt;-DWITH_RUST_EXAMPLE&lt;/code&gt;. &lt;em&gt;This is NOT included in the &lt;code&gt;dependencies.sh&lt;/code&gt; script.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Optional] &lt;code&gt;hiredis&lt;/code&gt;, if you want to build with &lt;code&gt;-DUSE_REDIS&lt;/code&gt;, so that you use Redis instead of etcd as metadata servers.&lt;/li&gt; &#xA; &lt;li&gt;[Optional] &lt;code&gt;curl&lt;/code&gt;, if you want to build with &lt;code&gt;-DUSE_HTTP&lt;/code&gt;, so that you use HTTP instead of etcd as metadata servers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The building and installation steps are the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Retrieve source code from GitHub repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/kvcache-ai/Mooncake.git&#xA;cd Mooncake&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash dependencies.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Compile Mooncake and examples&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j&#xA;sudo make install # optional, make it ready to be used by vLLM/SGLang&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2 id=&#34;milestones&#34;&gt; üõ£Ô∏è Incoming Milestones&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; First release of Mooncake and integrate with latest vLLM&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Share KV caches across multiple serving engines&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; User and developer documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 id=&#34;trace&#34;&gt;üì¶ Open Source Trace&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;timestamp&#34;: 27482,&#xA;    &#34;input_length&#34;: 6955,&#xA;    &#34;output_length&#34;: 52,&#xA;    &#34;hash_ids&#34;: [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 2353, 2354]&#xA;}&#xA;{&#xA;    &#34;timestamp&#34;: 30535,&#xA;    &#34;input_length&#34;: 6472,&#xA;    &#34;output_length&#34;: 26,&#xA;    &#34;hash_ids&#34;: [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 2366]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above presents two samples from our trace dataset. The trace includes the timing of request arrivals, the number of input tokens, the number of output tokens, and the remapped block hash. To protect our customers&#39; privacy, we applied several mechanisms to remove user-related information while preserving the dataset&#39;s utility for simulated evaluation. More descriptions of the trace (e.g., up to 50% cache hit ratio) can be found in Section 4 of the technical report.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Update[Feb 21, 2025]: The updated &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/FAST25-release/traces&#34;&gt;traces&lt;/a&gt; used in our FAST&#39;25 paper have been released! Please refer to the paper&#39;s appendix (found &lt;a href=&#34;https://raw.githubusercontent.com/kvcache-ai/Mooncake/main/FAST25-release/Mooncake-FAST25.pdf&#34;&gt;here&lt;/a&gt;) for more details.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2 id=&#34;citation&#34;&gt;üìë Citation&lt;/h2&gt; Please kindly cite our paper if you find the paper or the traces are useful: &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{qin2024mooncake,&#xA;  title        = {Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving},&#xA;  author       = {Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu},&#xA;  year         = {2024},&#xA;  url          = {https://arxiv.org/abs/2407.00079}&#xA;}&#xA;&#xA;@inproceedings {qin2025mooncake,&#xA;  author       = {Ruoyu Qin and Zheming Li and Weiran He and Jialei Cui and Feng Ren and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu},&#xA;  title        = {Mooncake: Trading More Storage for Less Computation {\textemdash} A {KVCache-centric} Architecture for Serving {LLM} Chatbot},&#xA;  booktitle    = {23rd USENIX Conference on File and Storage Technologies (FAST 25)},&#xA;  year         = {2025},&#xA;  isbn         = {978-1-939133-45-8},&#xA;  address      = {Santa Clara, CA},&#xA;  pages        = {155--170},&#xA;  url          = {https://www.usenix.org/conference/fast25/presentation/qin},&#xA;  publisher    = {USENIX Association},&#xA;  month        = feb&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>