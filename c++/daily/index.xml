<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-21T01:29:46Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ggerganov/ggml</title>
    <updated>2024-07-21T01:29:46Z</updated>
    <id>tag:github.com,2024-07-21:/ggerganov/ggml</id>
    <link href="https://github.com/ggerganov/ggml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensor library for machine learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ggml&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/users/ggerganov/projects/7&#34;&gt;Roadmap&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/discussions/205&#34;&gt;Manifesto&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tensor library for machine learning&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note that this project is under active development. &lt;br&gt; Some of the development is currently happening in the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt; repos&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Written in C&lt;/li&gt; &#xA; &lt;li&gt;16-bit float support&lt;/li&gt; &#xA; &lt;li&gt;Integer quantization support (4-bit, 5-bit, 8-bit, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Automatic differentiation&lt;/li&gt; &#xA; &lt;li&gt;ADAM and L-BFGS optimizers&lt;/li&gt; &#xA; &lt;li&gt;Optimized for Apple Silicon&lt;/li&gt; &#xA; &lt;li&gt;On x86 architectures utilizes AVX / AVX2 intrinsics&lt;/li&gt; &#xA; &lt;li&gt;On ppc64 architectures utilizes VSX intrinsics&lt;/li&gt; &#xA; &lt;li&gt;No third-party dependencies&lt;/li&gt; &#xA; &lt;li&gt;Zero memory allocations during runtime&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of GPT-2 inference &lt;a href=&#34;https://github.com/ggerganov/ggml/tree/master/examples/gpt-2&#34;&gt;examples/gpt-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of GPT-J inference &lt;a href=&#34;https://github.com/ggerganov/ggml/tree/master/examples/gpt-j&#34;&gt;examples/gpt-j&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of Whisper inference &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;ggerganov/whisper.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of LLaMA inference &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;ggerganov/llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of LLaMA training &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/tree/master/examples/baby-llama&#34;&gt;ggerganov/llama.cpp/examples/baby-llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of Falcon inference &lt;a href=&#34;https://github.com/cmp-nct/ggllm.cpp&#34;&gt;cmp-nct/ggllm.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of BLOOM inference &lt;a href=&#34;https://github.com/NouamaneTazi/bloomz.cpp&#34;&gt;NouamaneTazi/bloomz.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of RWKV inference &lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp&#34;&gt;saharNooby/rwkv.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of SAM inference &lt;a href=&#34;https://github.com/ggerganov/ggml/tree/master/examples/sam&#34;&gt;examples/sam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of BERT inference &lt;a href=&#34;https://github.com/skeskinen/bert.cpp&#34;&gt;skeskinen/bert.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of BioGPT inference &lt;a href=&#34;https://github.com/PABannier/biogpt.cpp&#34;&gt;PABannier/biogpt.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of Encodec inference &lt;a href=&#34;https://github.com/PABannier/encodec.cpp&#34;&gt;PABannier/encodec.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of CLIP inference &lt;a href=&#34;https://github.com/monatis/clip.cpp&#34;&gt;monatis/clip.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of MiniGPT4 inference &lt;a href=&#34;https://github.com/Maknee/minigpt4.cpp&#34;&gt;Maknee/minigpt4.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of ChatGLM inference &lt;a href=&#34;https://github.com/li-plus/chatglm.cpp&#34;&gt;li-plus/chatglm.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of Stable Diffusion inference &lt;a href=&#34;https://github.com/leejet/stable-diffusion.cpp&#34;&gt;leejet/stable-diffusion.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of Qwen inference &lt;a href=&#34;https://github.com/QwenLM/qwen.cpp&#34;&gt;QwenLM/qwen.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of YOLO inference &lt;a href=&#34;https://github.com/ggerganov/ggml/tree/master/examples/yolo&#34;&gt;examples/yolo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of ViT inference &lt;a href=&#34;https://github.com/staghado/vit.cpp&#34;&gt;staghado/vit.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Example of multiple LLMs inference &lt;a href=&#34;https://github.com/foldl/chatllm.cpp&#34;&gt;foldl/chatllm.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SeamlessM4T inference &lt;em&gt;(in development)&lt;/em&gt; &lt;a href=&#34;https://github.com/facebookresearch/seamless_communication/tree/main/ggml&#34;&gt;https://github.com/facebookresearch/seamless_communication/tree/main/ggml&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GPT inference (example)&lt;/h2&gt; &#xA;&lt;p&gt;With ggml you can efficiently run &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/ggml/master/examples/gpt-2&#34;&gt;GPT-2&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/ggml/master/examples/gpt-j&#34;&gt;GPT-J&lt;/a&gt; inference on the CPU.&lt;/p&gt; &#xA;&lt;p&gt;Here is how to run the example programs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Build ggml + examples&#xA;git clone https://github.com/ggerganov/ggml&#xA;cd ggml&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;make -j4 gpt-2-backend gpt-j&#xA;&#xA;# Run the GPT-2 small 117M model&#xA;../examples/gpt-2/download-ggml-model.sh 117M&#xA;./bin/gpt-2-backend -m models/gpt-2-117M/ggml-model.bin -p &#34;This is an example&#34;&#xA;&#xA;# Run the GPT-J 6B model (requires 12GB disk space and 16GB CPU RAM)&#xA;../examples/gpt-j/download-ggml-model.sh 6B&#xA;./bin/gpt-j -m models/gpt-j-6B/ggml-model.bin -p &#34;This is an example&#34;&#xA;&#xA;# Install Python dependencies&#xA;python3 -m pip install -r ../requirements.txt&#xA;&#xA;# Run the Cerebras-GPT 111M model&#xA;# Download from: https://huggingface.co/cerebras&#xA;python3 ../examples/gpt-2/convert-cerebras-to-ggml.py /path/to/Cerebras-GPT-111M/&#xA;./bin/gpt-2 -m /path/to/Cerebras-GPT-111M/ggml-model-f16.bin -p &#34;This is an example&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The inference speeds that I get for the different models on my 32GB MacBook M1 Pro are as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Time / Token&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;117M&lt;/td&gt; &#xA;   &lt;td&gt;5 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;345M&lt;/td&gt; &#xA;   &lt;td&gt;12 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;774M&lt;/td&gt; &#xA;   &lt;td&gt;23 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;1558M&lt;/td&gt; &#xA;   &lt;td&gt;42 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;   &lt;td&gt;---&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;125 ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For more information, checkout the corresponding programs in the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/ggml/master/examples&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Using Metal (only with GPT-2)&lt;/h2&gt; &#xA;&lt;p&gt;For GPT-2 models, offloading to GPU is possible. Note that it will not improve inference performances but will reduce power consumption and free up the CPU for other tasks.&lt;/p&gt; &#xA;&lt;p&gt;To enable GPU offloading on MacOS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -DGGML_METAL=ON -DBUILD_SHARED_LIBS=Off ..&#xA;&#xA;# add -ngl 1&#xA;./bin/gpt-2 -t 4 -ngl 100 -m models/gpt-2-117M/ggml-model.bin -p &#34;This is an example&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using cuBLAS&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# fix the path to point to your CUDA compiler&#xA;cmake -DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12.1/bin/nvcc ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using hipBLAS&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -DCMAKE_C_COMPILER=&#34;$(hipconfig -l)/clang&#34; -DCMAKE_CXX_COMPILER=&#34;$(hipconfig -l)/clang++&#34; -DGGML_HIPBLAS=ON&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using SYCL&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# linux&#xA;source /opt/intel/oneapi/setvars.sh&#xA;cmake -G &#34;Ninja&#34; -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL=ON ..&#xA;&#xA;# windows&#xA;&#34;C:\Program Files (x86)\Intel\oneAPI\setvars.bat&#34;&#xA;cmake -G &#34;Ninja&#34; -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx -DGGML_SYCL=ON ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiling for Android&lt;/h2&gt; &#xA;&lt;p&gt;Download and unzip the NDK from this download &lt;a href=&#34;https://developer.android.com/ndk/downloads&#34;&gt;page&lt;/a&gt;. Set the NDK_ROOT_PATH environment variable or provide the absolute path to the CMAKE_ANDROID_NDK in the command below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake .. \&#xA;   -DCMAKE_SYSTEM_NAME=Android \&#xA;   -DCMAKE_SYSTEM_VERSION=33 \&#xA;   -DCMAKE_ANDROID_ARCH_ABI=arm64-v8a \&#xA;   -DCMAKE_ANDROID_NDK=$NDK_ROOT_PATH&#xA;   -DCMAKE_ANDROID_STL_TYPE=c++_shared&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create directories&#xA;adb shell &#39;mkdir /data/local/tmp/bin&#39;&#xA;adb shell &#39;mkdir /data/local/tmp/models&#39;&#xA;&#xA;# Push the compiled binaries to the folder&#xA;adb push bin/* /data/local/tmp/bin/&#xA;&#xA;# Push the ggml library&#xA;adb push src/libggml.so /data/local/tmp/&#xA;&#xA;# Push model files&#xA;adb push models/gpt-2-117M/ggml-model.bin /data/local/tmp/models/&#xA;&#xA;&#xA;# Now lets do some inference ...&#xA;adb shell&#xA;&#xA;# Now we are in shell&#xA;cd /data/local/tmp&#xA;export LD_LIBRARY_PATH=/data/local/tmp&#xA;./bin/gpt-2-backend -m models/ggml-model.bin -p &#34;this is an example&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rustformers/llm/raw/main/crates/ggml/README.md&#34;&gt;GGML - Large Language Models for Everyone&lt;/a&gt;: a description of the GGML format provided by the maintainers of the &lt;code&gt;llm&lt;/code&gt; Rust crate, which provides Rust bindings for GGML&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marella/ctransformers&#34;&gt;marella/ctransformers&lt;/a&gt;: Python bindings for GGML models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-skynet/go-ggml-transformers.cpp&#34;&gt;go-skynet/go-ggml-transformers.cpp&lt;/a&gt;: Golang bindings for GGML models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/smspillaz/ggml-gobject&#34;&gt;smspillaz/ggml-gobject&lt;/a&gt;: GObject-introspectable wrapper for use of GGML on the GNOME platform.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>