<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-16T01:32:13Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hku-mars/r3live</title>
    <updated>2022-10-16T01:32:13Z</updated>
    <id>tag:github.com,2022-10-16:/hku-mars/r3live</id>
    <link href="https://github.com/hku-mars/r3live" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;R3LIVE&lt;/h1&gt; &#xA;&lt;h2&gt;A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package&lt;/h2&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;R3LIVE&lt;/strong&gt; is a novel LiDAR-Inertial-Visual sensor fusion framework, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R3LIVE is built upon our previous work &lt;a href=&#34;https://github.com/hku-mars/r2live&#34;&gt;R2LIVE&lt;/a&gt;, is contained of two subsystems: the LiDAR-inertial odometry (LIO) and the visual-inertial odometry (VIO). The LIO subsystem (&lt;a href=&#34;https://github.com/hku-mars/FAST_LIO&#34;&gt;FAST-LIO&lt;/a&gt;) takes advantage of the measurement from LiDAR and inertial sensors and builds the geometric structure of (i.e. the position of 3D points) global maps. The VIO subsystem utilizes the data of visual-inertial sensors and renders the map&#39;s texture (i.e. the color of 3D points). &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;The source code of this package is released under &lt;a href=&#34;http://www.gnu.org/licenses/&#34;&gt;&lt;strong&gt;GPLv2&lt;/strong&gt;&lt;/a&gt; license. We only allow it free for personal and academic usage. For commercial use, please contact me &amp;lt;ziv.lin.ljrATgmail.com&amp;gt; and Dr. Fu Zhang &amp;lt;fuzhangAThku.hk&amp;gt; to negotiate a different license.&lt;/p&gt; &#xA;&lt;h3&gt;1.1 Our paper&lt;/h3&gt; &#xA;&lt;p&gt;Our paper has been accepted to &lt;strong&gt;ICRA2022&lt;/strong&gt;, which is available online on this &lt;a href=&#34;https://ieeexplore.ieee.org/document/9811935&#34;&gt;page&lt;/a&gt; or be downloaded &lt;a href=&#34;https://github.com/hku-mars/r3live/raw/master/papers/R3LIVE%20--%20A%20Robust%2C%20Real-time%2C%20RGB-colored%2C%20LiDAR-Inertial-Visual%20tightly-coupled%20stateEstimation%20and%20mapping%20package.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;1.2 Our accompanying videos&lt;/h3&gt; &#xA;&lt;p&gt;Our &lt;strong&gt;accompanying videos&lt;/strong&gt; are now available on YouTube (click below images to open) and Bilibili&lt;sup&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1d341117d6?share_source=copy_web&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://www.bilibili.com/video/BV1e3411q7Di?share_source=copy_web&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://youtu.be/j5fT8NE5fdg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/R3LIVE_paper.jpg&#34; alt=&#34;video&#34; width=&#34;48%&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://youtu.be/4rjrrLgL3nk&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/R3LIVE_demos.jpg&#34; alt=&#34;video&#34; width=&#34;48%&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;1.3 Our associate dataset: R3LIVE-dataset&lt;/h3&gt; &#xA;&lt;p&gt;Our associate dataset &lt;a href=&#34;https://github.com/ziv-lin/r3live_dataset&#34;&gt;&lt;strong&gt;R3LIVE-dataset&lt;/strong&gt;&lt;/a&gt; that use for evaluation is also available online. You can access and download our datasets via this &lt;a href=&#34;https://github.com/ziv-lin/r3live_dataset&#34;&gt;&lt;strong&gt;Github repository&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;1.4 Our open-source hardware design&lt;/h3&gt; &#xA;&lt;p&gt;All of the mechanical modules of our handheld device that use for data collection are designed as FDM printable, with the schematics of the design are also open-sourced in this &lt;a href=&#34;https://github.com/ziv-lin/rxlive_handheld&#34;&gt;&lt;strong&gt;Github repository&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;2. What can R3LIVE do?&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 Strong robustness in various challenging scenarios&lt;/h3&gt; &#xA;&lt;p&gt;R3LIVE is robust enough to work well in various of LiDAR-degenerated scenarios (see following figures):&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/degenerate_01_pic.png&#34; width=&#34;98%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/degenerate_02_pic.png&#34; width=&#34;98%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/degenerate_01.gif&#34; width=&#34;48%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/degenerate_02.gif&#34; width=&#34;48%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;And even in simultaneously LiDAR degenerated and visual texture-less environments (see Experiment-1 of our &lt;a href=&#34;https://github.com/hku-mars/r3live/raw/master/papers/R3LIVE:%20A%20Robust%2C%20Real-time%2C%20RGB-colored%2C%20LiDAR-Inertial-Visual%20tightly-coupled%20stateEstimation%20and%20mapping%20package.pdf&#34;&gt;paper&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/exp_00.png&#34; alt=&#34;video&#34; width=&#34;48%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/degenerate_00.gif&#34; alt=&#34;video&#34; width=&#34;48%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;2.2 Real-time RGB maps reconstruction&lt;/h3&gt; &#xA;&lt;p&gt;R3LIVE is able to reconstruct the precise, dense, 3D, RGB-colored maps of surrounding environment in real-time (watch this &lt;a href=&#34;https://youtu.be/j5fT8NE5fdg&#34;&gt;video&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/cover_half.jpg&#34; width=&#34;98%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/hku_campus_seq_01.png&#34; width=&#34;98%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/hku_park_01.png&#34; width=&#34;98%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/hku_demo.gif&#34; width=&#34;48%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/hkust_demo.gif&#34; alt=&#34;video&#34; width=&#34;48%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;2.3 Ready for 3D applications&lt;/h3&gt; &#xA;&lt;p&gt;To make R3LIVE more extensible, we also provide a series of offline utilities for reconstructing and texturing meshes, which further reduce the gap between R3LIVE and various 3D applications (watch this &lt;a href=&#34;https://youtu.be/4rjrrLgL3nk&#34;&gt;video&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/pics/mesh.png&#34; alt=&#34;video&#34; width=&#34;98%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/ue_game_0.gif&#34; alt=&#34;video&#34; width=&#34;48%&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/r3live_dataset/raw/main/gifs/ue_game_1.gif&#34; width=&#34;48%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;3. Prerequisites&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 &lt;strong&gt;ROS&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Following this &lt;a href=&#34;http://wiki.ros.org/ROS/Installation&#34;&gt;ROS Installation&lt;/a&gt; to install ROS and its additional pacakge:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install ros-XXX-cv-bridge ros-XXX-tf ros-XXX-message-filters ros-XXX-image-transport ros-XXX-image-transport*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTICE:&lt;/strong&gt; remember to replace &#34;XXX&#34; on above command as your ROS distributions, for example, if your use ROS-kinetic, the command should be:&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install ros-kinetic-cv-bridge ros-kinetic-tf ros-kinetic-message-filters ros-kinetic-image-transport*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3.2. &lt;strong&gt;livox_ros_driver&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Follow this &lt;a href=&#34;https://github.com/Livox-SDK/livox_ros_driver&#34;&gt;livox_ros_driver Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3.3 &lt;strong&gt;CGAL&lt;/strong&gt; and &lt;strong&gt;pcl_viewer&lt;/strong&gt; (optional)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install libcgal-dev pcl-tools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3.4 &lt;strong&gt;OpenCV &amp;gt;= 3.3&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can use the following command to check your OpenCV version, &lt;strong&gt;if your openCV version lower than OpenCV-3.3&lt;/strong&gt;, we recommend you to update your you openCV version if you meet errors in complying our codes. Otherwise, skip this step ^_^&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pkg-config --modversion opencv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have successfully test our algorithm with version &lt;strong&gt;3.3.1&lt;/strong&gt;, &lt;strong&gt;3.4.16&lt;/strong&gt;, &lt;strong&gt;4.2.1&lt;/strong&gt; and &lt;strong&gt;4.5.3&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notice:&lt;/strong&gt; We have noticed that a large number of users meet a crash of problem after launching our package due to the mismatch of openCV version (see &lt;a href=&#34;https://github.com/hku-mars/r3live/issues/11&#34;&gt;issue #11&lt;/a&gt;, &lt;a href=&#34;https://github.com/hku-mars/r3live/issues/20&#34;&gt;issue #20&lt;/a&gt; and &lt;a href=&#34;https://github.com/hku-mars/r3live/issues/23&#34;&gt;issue #23&lt;/a&gt;, and etc.). If you meet with similar problems, please make sure that the OpenCV you complied are same as the OpenCV you run with. This is &lt;strong&gt;very very&lt;/strong&gt; important to launch R3LIVE correctly.&lt;/p&gt; &#xA;&lt;h2&gt;4. Build R3LIVE on ROS:&lt;/h2&gt; &#xA;&lt;p&gt;Clone this repository and catkin_make:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ~/catkin_ws/src&#xA;git clone https://github.com/hku-mars/r3live.git&#xA;cd ../&#xA;catkin_make&#xA;source ~/catkin_ws/devel/setup.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. Run our examples&lt;/h2&gt; &#xA;&lt;h3&gt;5.1 Download our rosbag files (&lt;a href=&#34;https://github.com/ziv-lin/r3live_dataset&#34;&gt;r3live_dataset&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Our datasets for evaluation can be download from our &lt;a href=&#34;https://drive.google.com/drive/folders/15i-TRa0EA8BCbNdARVqPMDsU9JOlagVF?usp=sharing&#34;&gt;Google drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1zmVxkcwOSul8oTBwaHfuFg&#34;&gt;Baidu-NetDisk [百度网盘]&lt;/a&gt; (code提取码: wwxw). We have released totally &lt;strong&gt;9&lt;/strong&gt; rosbag files for evaluating r3live, with the introduction of these datasets can be found on this &lt;a href=&#34;https://github.com/ziv-lin/r3live_dataset&#34;&gt;page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;5.2 Run our examples&lt;/h3&gt; &#xA;&lt;p&gt;After you have downloaded our bag files, you can now run our example ^_^&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch r3live r3live_bag.launch&#xA;rosbag play YOUR_DOWNLOADED.bag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything is correct, you will get the result that matches our &lt;a href=&#34;https://github.com/hku-mars/r3live/raw/master/papers/R3LIVE:%20A%20Robust%2C%20Real-time%2C%20RGB-colored%2C%20LiDAR-Inertial-Visual%20tightly-coupled%20stateEstimation%20and%20mapping%20package.pdf&#34;&gt;paper&lt;/a&gt; and the results posted on this &lt;a href=&#34;https://github.com/ziv-lin/r3live_dataset&#34;&gt;page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;5.3 Save the maps to your disk&lt;/h3&gt; &#xA;&lt;p&gt;R3LIVE allow you to save the maps you build at anytime you wanted. You just need to click on the &#34;Control panel&#34; and press &#39;S&#39; or &#39;s&#39; key.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hku-mars/r3live/master/control_panel.png&#34; alt=&#34;video&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;5.3 Reconstruct and texture your mesh&lt;/h3&gt; &#xA;&lt;p&gt;After you have save your offline map on your disk (default save in directory: ${HOME}/r3live_output), you can launch our utility to reconstruct and texture your mesh.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch r3live r3live_reconstruct_mesh.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5.4 Visualize your saved maps.&lt;/h3&gt; &#xA;&lt;p&gt;As default, your offline map (and reconstructed mesh) will be saved in the directory &lt;strong&gt;${HOME}/r3live_output&lt;/strong&gt;, you can open it with pcl_viewer (and &lt;a href=&#34;https://www.meshlab.net/&#34;&gt;meshlab&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Install pcl_viewer and meshlab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install pcl-tools meshlab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visualizing your offline point cloud maps (with suffix *.pcd):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ${HOME}/r3live_output&#xA;pcl_viewer rgb_pt.pcd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visualizing your reconstructed mesh (with suffix *.ply):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ${HOME}/r3live_output&#xA;meshlab textured_mesh.ply&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. Sample and run your own data&lt;/h2&gt; &#xA;&lt;h3&gt;6.1 Livox-ros-driver for R2/R3LIVE&lt;/h3&gt; &#xA;&lt;p&gt;Since the LiDAR data and IMU data published by the official Livox-ros-driver is with the timestamp of LiDAR (started from 0 in each recording), and the timestamp of the image is usually recorded with the timestamp of the operation system. To make them working under the same time-based, we modified the source code of Livox-ros-driver, which is available at &lt;a href=&#34;https://github.com/ziv-lin/livox_ros_driver_for_R2LIVE&#34;&gt;here&lt;/a&gt;. We suggest you replace the official driver with it when sampling your own data for R3LIVE.&lt;/p&gt; &#xA;&lt;h3&gt;6.2 Sensor calibration&lt;/h3&gt; &#xA;&lt;p&gt;In order to launch R3LIVE on your own hardware setup, you need to have a carefully calibration of the extrinsic among LiDAR, Camera and IMU. We recommend you using the following repo to kindly calibrate your sensors:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hku-mars/livox_camera_calib&#34;&gt;&lt;strong&gt;livox_camera_calib&lt;/strong&gt;&lt;/a&gt;: A robust, high accuracy extrinsic calibration tool between high resolution LiDAR (e.g. Livox) and camera in targetless environment.&lt;/p&gt; &#xA;&lt;h2&gt;7. Support of the spinning LiDAR&lt;/h2&gt; &#xA;&lt;p&gt;Even though our proposed method is unrelated to what kind of LiDAR you used, it is impossible for us making R3LIVE compatible with all kinds of existing LiDARs. To launch R3LIVE with spinning LIDAR, it requires you to take some effort in modifying the source code of our LiDAR front-end (see &lt;a href=&#34;https://github.com/hku-mars/r3live/raw/master/r3live/src/loam/LiDAR_front_end.cpp&#34;&gt;LiDAR_front_end.cpp&lt;/a&gt;). Here we give an example to test our LIO-subsystem with an &lt;a href=&#34;https://ouster.com/zh-cn/products/scanning-lidar/os2-sensor/&#34;&gt;Ouster-2 64 Line Spinning LIDAR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;7.1 Example-1: Ouster OS2-64&lt;/h3&gt; &#xA;&lt;p&gt;Download our recorded rosbag file from &lt;a href=&#34;https://drive.google.com/file/d/1mZoDWDOZOcZ0H6MBMEpGMRc1nueeqToc/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch r3live r3live_bag_ouster.launch &#xA;rosbag play ouster_example_for_LIO_test.bag &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice: We manually disable our VIO-subsystem due the missed of calibration files in this example.&lt;/p&gt; &#xA;&lt;p&gt;Finally, we are still working on making R3LIVE compatible with more spinning LiDARs. More and more examples will be released in the future.&lt;/p&gt; &#xA;&lt;h2&gt;8. Access our open source hardware design&lt;/h2&gt; &#xA;&lt;p&gt;In order to facilitate our users in reproducing our work, we also make our hardware design public available, where you can download all of our CAD source files in &lt;a href=&#34;https://github.com/ziv-lin/rxlive_handheld&#34;&gt;rxlive_handheld&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/ziv-lin/rxlive_handheld/raw/master/pics/introduction_alpha.png&#34; width=&#34;98%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;9. Report our problems and bugs&lt;/h2&gt; &#xA;&lt;p&gt;We know our packages might not totally stable in this stage, and we are keep working on improving the performance and reliability of our codes. So, if you have met any bug or problem, please feel free to draw an issue and I will respond ASAP.&lt;/p&gt; &#xA;&lt;p&gt;  For reporting our problems and bugs, please attach both your hardware and software environment if possible (printed by R3LIVE, see the following figure), which will be a great help for me in locating your problems.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hku-mars/r3live/master/envs.png&#34; alt=&#34;video&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;10. Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;In the development of R3LIVE, we stand on the shoulders of the following repositories:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hku-mars/r2live&#34;&gt;R2LIVE&lt;/a&gt;: A robust, real-time tightly-coupled multi-sensor fusion package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hku-mars/FAST_LIO&#34;&gt;FAST-LIO&lt;/a&gt;: A computationally efficient and robust LiDAR-inertial odometry package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hku-mars/ikd-Tree&#34;&gt;ikd-Tree&lt;/a&gt;: A state-of-art dynamic KD-Tree for 3D kNN search.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hku-mars/livox_camera_calib&#34;&gt;livox_camera_calib&lt;/a&gt;: A robust, high accuracy extrinsic calibration tool between high resolution LiDAR (e.g. Livox) and camera in targetless environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hku-mars/loam_livox&#34;&gt;LOAM-Livox&lt;/a&gt;: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cdcseacave/openMVS&#34;&gt;openMVS&lt;/a&gt;: A library for computer-vision scientists and especially targeted to the Multi-View Stereo reconstruction community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cnr-isti-vclab/vcglib&#34;&gt;VCGlib&lt;/a&gt;: An open source, portable, header-only Visualization and Computer Graphics Library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;CGAL&lt;/a&gt;: A C++ Computational Geometry Algorithms Library.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The source code of this package is released under &lt;a href=&#34;http://www.gnu.org/licenses/&#34;&gt;&lt;strong&gt;GPLv2&lt;/strong&gt;&lt;/a&gt; license. We only allow it free for personal and academic usage. For commercial use, please contact me &amp;lt;ziv.lin.ljrATgmail.com&amp;gt; and Dr. Fu Zhang &amp;lt;fuzhangAThku.hk&amp;gt; to negotiate a different license.&lt;/p&gt; &#xA;&lt;p&gt;We are still working on improving the performance and reliability of our codes. For any technical issues, please contact me via email Jiarong Lin &amp;lt; ziv.lin.ljrATgmail.com &amp;gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you use any code of this repo in your academic research, please cite &lt;strong&gt;at least one&lt;/strong&gt; of our papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[1] Lin, Jiarong, and Fu Zhang. &#34;R3LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package.&#34; &#xA;[2] Xu, Wei, et al. &#34;Fast-lio2: Fast direct lidar-inertial odometry.&#34;&#xA;[3] Lin, Jiarong, et al. &#34;R2LIVE: A Robust, Real-time, LiDAR-Inertial-Visual tightly-coupled state Estimator and mapping.&#34; &#xA;[4] Xu, Wei, and Fu Zhang. &#34;Fast-lio: A fast, robust lidar-inertial odometry package by tightly-coupled iterated kalman filter.&#34;&#xA;[5] Cai, Yixi, Wei Xu, and Fu Zhang. &#34;ikd-Tree: An Incremental KD Tree for Robotic Applications.&#34;&#xA;[6] Lin, Jiarong, and Fu Zhang. &#34;Loam-livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>juce-framework/JUCE</title>
    <updated>2022-10-16T01:32:13Z</updated>
    <id>tag:github.com,2022-10-16:/juce-framework/JUCE</id>
    <link href="https://github.com/juce-framework/JUCE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;JUCE is an open-source cross-platform C++ application framework for desktop and mobile applications, including VST, VST3, AU, AUv3, RTAS and AAX audio plug-ins.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://assets.juce.com/juce/JUCE_banner_github.png&#34; alt=&#34;alt text&#34; title=&#34;JUCE&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;JUCE is an open-source cross-platform C++ application framework for creating high quality desktop and mobile applications, including VST, VST3, AU, AUv3, AAX and LV2 audio plug-ins and plug-in hosts. JUCE can be easily integrated with existing projects via CMake, or can be used as a project generation tool via the &lt;a href=&#34;https://juce.com/discover/projucer&#34;&gt;Projucer&lt;/a&gt;, which supports exporting projects for Xcode (macOS and iOS), Visual Studio, Android Studio, Code::Blocks and Linux Makefiles as well as containing a source code editor.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The JUCE repository contains a &lt;a href=&#34;https://github.com/juce-framework/JUCE/tree/master&#34;&gt;master&lt;/a&gt; and &lt;a href=&#34;https://github.com/juce-framework/JUCE/tree/develop&#34;&gt;develop&lt;/a&gt; branch. The develop branch contains the latest bugfixes and features and is periodically merged into the master branch in stable &lt;a href=&#34;https://github.com/juce-framework/JUCE/releases&#34;&gt;tagged releases&lt;/a&gt; (the latest release containing pre-built binaries can be also downloaded from the &lt;a href=&#34;https://juce.com/get-juce&#34;&gt;JUCE website&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;JUCE projects can be managed with either the Projucer (JUCE&#39;s own project-configuration tool) or with CMake.&lt;/p&gt; &#xA;&lt;h3&gt;The Projucer&lt;/h3&gt; &#xA;&lt;p&gt;The repository doesn&#39;t contain a pre-built Projucer so you will need to build it for your platform - Xcode, Visual Studio and Linux Makefile projects are located in &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/extras/Projucer/Builds&#34;&gt;extras/Projucer/Builds&lt;/a&gt; (the minimum system requirements are listed in the &lt;strong&gt;System Requirements&lt;/strong&gt; section below). The Projucer can then be used to create new JUCE projects, view tutorials and run examples. It is also possible to include the JUCE modules source code in an existing project directly, or build them into a static or dynamic library which can be linked into a project.&lt;/p&gt; &#xA;&lt;p&gt;For further help getting started, please refer to the JUCE &lt;a href=&#34;https://juce.com/learn/documentation&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://juce.com/learn/tutorials&#34;&gt;tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;CMake&lt;/h3&gt; &#xA;&lt;p&gt;Version 3.15 or higher is required. To use CMake, you will need to install it, either from your system package manager or from the &lt;a href=&#34;https://cmake.org/download/&#34;&gt;official download page&lt;/a&gt;. For comprehensive documentation on JUCE&#39;s CMake API, see the &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/docs/CMake%20API.md&#34;&gt;JUCE CMake documentation&lt;/a&gt;. For examples which may be useful as starting points for new CMake projects, see the &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/examples/CMake&#34;&gt;CMake examples directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building Examples&lt;/h4&gt; &#xA;&lt;p&gt;To use CMake to build the examples and extras bundled with JUCE, simply clone JUCE and then run the following commands, replacing &#34;DemoRunner&#34; with the name of the target you wish to build.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /path/to/JUCE&#xA;cmake . -B cmake-build -DJUCE_BUILD_EXAMPLES=ON -DJUCE_BUILD_EXTRAS=ON&#xA;cmake --build cmake-build --target DemoRunner&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Minimum System Requirements&lt;/h2&gt; &#xA;&lt;h4&gt;Building JUCE Projects&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;macOS/iOS&lt;/strong&gt;: Xcode 10.1 (macOS 10.13.6)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Windows 8.1 and Visual Studio 2015 Update 3 64-bit&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: g++ 5.0 or Clang 3.4 (for a full list of dependencies, see &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/docs/Linux%20Dependencies.md&#34;&gt;here&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: Android Studio on Windows, macOS or Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Deployment Targets&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: macOS 10.7&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Windows Vista&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Mainstream Linux distributions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;iOS&lt;/strong&gt;: iOS 9.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: Jelly Bean (API 16)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/.github/contributing.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;The core JUCE modules (juce_audio_basics, juce_audio_devices, juce_core and juce_events) are permissively licensed under the terms of the &lt;a href=&#34;http://www.isc.org/downloads/software-support-policy/isc-license/&#34;&gt;ISC license&lt;/a&gt;. Other modules are covered by a &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.en.html&#34;&gt;GPL&lt;/a&gt;/Commercial license.&lt;/p&gt; &#xA;&lt;p&gt;There are multiple commercial licensing tiers for JUCE, with different terms for each:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;JUCE Personal (developers or startup businesses with revenue under 50K USD) - free&lt;/li&gt; &#xA; &lt;li&gt;JUCE Indie (small businesses with revenue under 500K USD) - $40/month or $800 perpetual&lt;/li&gt; &#xA; &lt;li&gt;JUCE Pro (no revenue limit) - $130/month or $2600 perpetual&lt;/li&gt; &#xA; &lt;li&gt;JUCE Educational (no revenue limit) - free for bona fide educational institutes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For full terms see &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The JUCE framework contains the following dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_devices/native/oboe/&#34;&gt;Oboe&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_devices/native/oboe/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_formats/codecs/flac/&#34;&gt;FLAC&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_formats/codecs/flac/Flac%20Licence.txt&#34;&gt;BSD&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_formats/codecs/oggvorbis/&#34;&gt;Ogg Vorbis&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_formats/codecs/oggvorbis/Ogg%20Vorbis%20Licence.txt&#34;&gt;BSD&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_plugin_client/AU/CoreAudioUtilityClasses/&#34;&gt;CoreAudioUtilityClasses&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_plugin_client/AU/CoreAudioUtilityClasses/AUBase.cpp&#34;&gt;Apple&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_plugin_client/AUResources.r&#34;&gt;AUResources.r&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_plugin_client/AUResources.r&#34;&gt;Apple&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_processors/format_types/LV2_SDK/&#34;&gt;LV2&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_processors/format_types/LV2_SDK/lv2/COPYING&#34;&gt;ISC&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_processors/format_types/pslextensions/ipslcontextinfo.h&#34;&gt;pslextensions&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_processors/format_types/pslextensions/ipslcontextinfo.h&#34;&gt;Public domain&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_processors/format_types/VST3_SDK/&#34;&gt;VST3&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_audio_processors/format_types/VST3_SDK/LICENSE.txt&#34;&gt;Proprietary Steinberg VST3/GPLv3&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_core/zip/zlib/&#34;&gt;zlib&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_core/zip/zlib/README&#34;&gt;zlib&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_box2d/box2d/&#34;&gt;Box2D&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_box2d/box2d/Box2D.h&#34;&gt;zlib&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_graphics/image_formats/jpglib/&#34;&gt;jpeglib&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_graphics/image_formats/jpglib/README&#34;&gt;Independent JPEG Group License&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_graphics/image_formats/pnglib/&#34;&gt;pnglib&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_graphics/image_formats/pnglib/LICENSE&#34;&gt;zlib&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_opengl/opengl/juce_gl.h&#34;&gt;GLEW&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_opengl/opengl/juce_gl.h&#34;&gt;BSD&lt;/a&gt;), including &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_opengl/opengl/juce_gl.h&#34;&gt;Mesa&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_opengl/opengl/juce_gl.h&#34;&gt;MIT&lt;/a&gt;) and &lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_opengl/opengl/juce_gl.h&#34;&gt;Khronos&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/modules/juce_opengl/opengl/juce_gl.h&#34;&gt;MIT&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The JUCE examples are licensed under the terms of the &lt;a href=&#34;http://www.isc.org/downloads/software-support-policy/isc-license/&#34;&gt;ISC license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Dependencies in the examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/examples/Plugins/extern/&#34;&gt;reaper-sdk&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/examples/Plugins/extern/LICENSE.md&#34;&gt;zlib&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Dependencies in the bundled applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/extras/Projucer/Source/Utility/UI/jucer_Icons.cpp&#34;&gt;Projucer icons&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/extras/Projucer/Source/Utility/UI/jucer_Icons.cpp&#34;&gt;MIT&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Dependencies in the build system:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/examples/DemoRunner/Builds/Android/gradle/wrapper/LICENSE-for-gradlewrapper.txt&#34;&gt;Android Gradle&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/juce-framework/JUCE/master/examples/DemoRunner/Builds/Android/gradle/wrapper/LICENSE-for-gradlewrapper.txt&#34;&gt;Apache 2.0&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>deepmind/open_spiel</title>
    <updated>2022-10-16T01:32:13Z</updated>
    <id>tag:github.com,2022-10-16:/deepmind/open_spiel</id>
    <link href="https://github.com/deepmind/open_spiel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenSpiel: A Framework for Reinforcement Learning in Games&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openspiel.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/openspiel/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/deepmind/open_spiel/workflows/build_and_test/badge.svg?sanitize=true&#34; alt=&#34;build_and_test&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games. OpenSpiel supports n-player (single- and multi- agent) zero-sum, cooperative and general-sum, one-shot and sequential, strictly turn-taking and simultaneous-move, perfect and imperfect information games, as well as traditional multiagent environments such as (partially- and fully- observable) grid worlds and social dilemmas. OpenSpiel also includes tools to analyze learning dynamics and other common evaluation metrics. Games are represented as procedural extensive-form games, with some natural extensions. The core API and games are implemented in C++ and exposed to Python. Algorithms and tools are written both in C++ and Python.&lt;/p&gt; &#xA;&lt;p&gt;To try OpenSpiel in Google Colaboratory, please refer to &lt;code&gt;open_spiel/colabs&lt;/code&gt; subdirectory or start &lt;a href=&#34;https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/install_open_spiel.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/_static/OpenSpielB.png&#34; alt=&#34;OpenSpiel visual asset&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Index&lt;/h1&gt; &#xA;&lt;p&gt;Please choose among the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/install.md&#34;&gt;Installing OpenSpiel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/intro.md&#34;&gt;Introduction to OpenSpiel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/concepts.md&#34;&gt;API Overview and First Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/api_reference.md&#34;&gt;API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/games.md&#34;&gt;Overview of Implemented Games&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/algorithms.md&#34;&gt;Overview of Implemented Algorithms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/developer_guide.md&#34;&gt;Developer Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/library.md&#34;&gt;Using OpenSpiel as a C++ Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/contributing.md&#34;&gt;Guidelines and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepmind/open_spiel/master/docs/authors.md&#34;&gt;Authors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a longer introduction to the core concepts, formalisms, and terminology, including an overview of the algorithms and some results, please see &lt;a href=&#34;https://arxiv.org/abs/1908.09453&#34;&gt;OpenSpiel: A Framework for Reinforcement Learning in Games&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For an overview of OpenSpiel and example uses of the core API, please check out our tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8NCPqtPwlFQ&#34;&gt;Motivation, Core API, Brief Intro to Replictor Dynamics and Imperfect Information Games&lt;/a&gt; by Marc Lanctot. &lt;a href=&#34;http://mlanctot.info/files/OpenSpiel_Tutorial_KU_Leuven_2022.pdf&#34;&gt;(slides)&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/OpenSpielTutorial.ipynb&#34;&gt;(colab)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=o6JNHoGUXCo&#34;&gt;Motivation, Core API, Implementing CFR and REINFORCE on Kuhn poker, Leduc poker, and Goofspiel&lt;/a&gt; by Edward Lockhart. &lt;a href=&#34;http://mlanctot.info/files/open_spiel_tutorial-mar2021-comarl.pdf&#34;&gt;(slides)&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/deepmind/open_spiel/blob/master/open_spiel/colabs/CFR_and_REINFORCE.ipynb&#34;&gt;(colab)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you use OpenSpiel in your research, please cite the paper using the following BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{LanctotEtAl2019OpenSpiel,&#xA;  title     = {{OpenSpiel}: A Framework for Reinforcement Learning in Games},&#xA;  author    = {Marc Lanctot and Edward Lockhart and Jean-Baptiste Lespiau and&#xA;               Vinicius Zambaldi and Satyaki Upadhyay and Julien P\&#39;{e}rolat and&#xA;               Sriram Srinivasan and Finbarr Timbers and Karl Tuyls and&#xA;               Shayegan Omidshafiei and Daniel Hennes and Dustin Morrill and&#xA;               Paul Muller and Timo Ewalds and Ryan Faulkner and J\&#39;{a}nos Kram\&#39;{a}r&#xA;               and Bart De Vylder and Brennan Saeta and James Bradbury and David Ding&#xA;               and Sebastian Borgeaud and Matthew Lai and Julian Schrittwieser and&#xA;               Thomas Anthony and Edward Hughes and Ivo Danihelka and Jonah Ryan-Davis},&#xA;  year      = {2019},&#xA;  eprint    = {1908.09453},&#xA;  archivePrefix = {arXiv},&#xA;  primaryClass = {cs.LG},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/1908.09453},&#xA;  url       = {http://arxiv.org/abs/1908.09453},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Versioning&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://semver.org/&#34;&gt;Semantic Versioning&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>