<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-01T01:31:47Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hoffstadt/DearPyGui</title>
    <updated>2022-07-01T01:31:47Z</updated>
    <id>tag:github.com,2022-07-01:/hoffstadt/DearPyGui</id>
    <link href="https://github.com/hoffstadt/DearPyGui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dear PyGui: A fast and powerful Graphical User Interface Toolkit for Python with minimal dependencies&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/assets/readme/dpg_logo_button.png&#34; alt=&#34;Dear PyGui logo&#34;&gt;&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;A modern, fast and powerful GUI framework for Python&lt;/h4&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/dearpygui&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/dearpygui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/dearpygui&#34; alt=&#34;PYPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/dearpygui&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/dearpygui&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#license&#34;&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/mit_badge.svg?sanitize=true&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/actions?workflow=Embedded%20Build&#34;&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/workflows/Embedded%20Build/badge.svg?branch=master&#34; alt=&#34;static-analysis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/actions?workflow=Static%20Analysis&#34;&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/workflows/Static%20Analysis/badge.svg?branch=master&#34; alt=&#34;static-analysis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/actions/workflows/Deployment.yml&#34;&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/actions/workflows/Deployment.yml/badge.svg?branch=master&#34; alt=&#34;Deployment&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dearpygui.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/dearpygui/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#features&#34;&gt;Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#installation&#34;&gt;Installation&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#how-to-use&#34;&gt;How To Use&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#demo&#34;&gt;Demo&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#resources&#34;&gt;Resources&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#support&#34;&gt;Support&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#tech-stack&#34;&gt;Tech stack&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#credits&#34;&gt;Credits&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#license&#34;&gt;License&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/master/#gallery&#34;&gt;Gallery&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/assets/linuxthemes.PNG&#34; alt=&#34;Themes&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modern look&lt;/strong&gt; ‚Äî Complete theme and style control&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Great performance&lt;/strong&gt; ‚Äî GPU-based rendering and efficient C/C++ code&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stable operation&lt;/strong&gt; ‚Äî Asynchronous function support&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast graphs&lt;/strong&gt; ‚Äî Display over 1 million datapoints at 60 fps, zoom and pan&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Node editor&lt;/strong&gt; ‚Äî Intuitive user interaction&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Built-in demo&lt;/strong&gt; ‚Äî Quickly learn all features&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Developer tools&lt;/strong&gt; ‚Äî Theme and resource inspection, runtime metrics, debugger&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt; ‚Äî Windows, Linux, MacOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MIT license&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/stem.gif&#34; width=&#34;380&#34;&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/tables.gif&#34; width=&#34;380&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/pie.gif&#34; width=&#34;380&#34;&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/candle.gif&#34; width=&#34;380&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have at least Python 3.7 64bit.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install dearpygui&#xA;or&#xA;pip3 install dearpygui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use?&lt;/h2&gt; &#xA;&lt;p&gt;Using Dear PyGui is as simple as the following Python script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;import dearpygui.dearpygui as dpg&#xA;&#xA;def save_callback():&#xA;    print(&#34;Save Clicked&#34;)&#xA;&#xA;dpg.create_context()&#xA;dpg.create_viewport()&#xA;dpg.setup_dearpygui()&#xA;&#xA;with dpg.window(label=&#34;Example Window&#34;):&#xA;    dpg.add_text(&#34;Hello world&#34;)&#xA;    dpg.add_button(label=&#34;Save&#34;, callback=save_callback)&#xA;    dpg.add_input_text(label=&#34;string&#34;)&#xA;    dpg.add_slider_float(label=&#34;float&#34;)&#xA;&#xA;dpg.show_viewport()&#xA;dpg.start_dearpygui()&#xA;dpg.destroy_context()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://dearpygui.readthedocs.io/en/latest/tutorials/first-steps.html#first-run&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/assets/readme/first_app.gif&#34; alt=&#34;Dear PyGui example window&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;The built-in demo shows all of Dear PyGui&#39;s functionality. Use &lt;a href=&#34;https://dearpygui.readthedocs.io/en/latest/tutorials/first-steps.html#demo&#34;&gt;this code&lt;/a&gt; to run the demo. The following impression shows a few, but not nearly all, of the available widgets and features. Since the Python code of the demo can be &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/raw/master/dearpygui/demo.py&#34; alt=&#34;demo code repository&#34;&gt;inspected&lt;/a&gt;, you can leverage the demo code to build your own apps. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://dearpygui.readthedocs.io/en/latest/tutorials/first-steps.html#demo&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hoffstadt/DearPyGui/assets/readme/demo.gif&#34; alt=&#34;Dear PyGui demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dearpygui.readthedocs.io/en/latest/index.html&#34;&gt;API documentation&lt;/a&gt; &lt;span&gt;üìö&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/projects/4&#34;&gt;Development Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/discussions/categories/frequently-asked-questions-faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/issues?q=is%3Aissue+is%3Aopen+label%3A%22type%3A+feature%22&#34;&gt;Feature Tracker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/issues?q=is%3Aissue+is%3Aopen+label%3A%22type%3A+bug%22&#34;&gt;Bug Tracker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/wiki/Dear-PyGui-Showcase&#34;&gt;Showcase apps including source code&lt;/a&gt; &lt;span&gt;‚≠ê&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you are having issues or want to help, here are some places you can go.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/tyE7Gu4&#34;&gt;Discord Forum&lt;/a&gt; üí¨&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/DearPyGui/&#34;&gt;Reddit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/tyE7Gu4&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/736279277242417272?logo=discord&#34; alt=&#34;Chat on Discord&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://www.reddit.com/r/DearPyGui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/reddit/subreddit-subscribers/dearpygui?label=r%2Fdearpygui&#34; alt=&#34;Reddit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tech stack&lt;/h2&gt; &#xA;&lt;p&gt;Dear PyGui is built on top of &lt;a href=&#34;https://github.com/ocornut/imgui&#34; target=&#34;_blank&#34;&gt;Dear ImGui&lt;/a&gt;, including the &lt;a href=&#34;https://github.com/epezent/implot&#34;&gt;ImPlot&lt;/a&gt; and &lt;a href=&#34;https://github.com/Nelarius/imnodes&#34;&gt;imnodes&lt;/a&gt; extensions, and is fundamentally different than other Python GUI frameworks. Under the hood, it uses the immediate mode paradigm and your computer&#39;s GPU to facilitate extremely dynamic interfaces. In the same manner Dear ImGui provides a simple way to create tools for game developers, Dear PyGui provides a simple way for python developers to create quick and powerful GUIs for scripts. Dear PyGui is written in C/C++ resulting in highly performant Python applications. Dear PyGui is currently supported on the following platforms. &lt;br&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Platform&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Graphics API&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Newest Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Windows 10&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;DirectX 11&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pypi.org/project/dearpygui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/dearpygui&#34; alt=&#34;PYPI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;Metal&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pypi.org/project/dearpygui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/dearpygui&#34; alt=&#34;PYPI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;OpenGL 3&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pypi.org/project/dearpygui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/dearpygui&#34; alt=&#34;PYPI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Raspberry Pi 4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;OpenGL ES&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://img.shields.io/badge/pypi-v1.6-blue&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pypi-v1.6-blue&#34; alt=&#34;PYPI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Developed by &lt;a href=&#34;https://github.com/hoffstadt&#34;&gt;Jonathan Hoffstadt&lt;/a&gt;, &lt;a href=&#34;https://github.com/Pcothren&#34;&gt;Preston Cothren&lt;/a&gt; and every direct or indirect contributor.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.miracleworld.net/&#34;&gt;Omar Cornut&lt;/a&gt; for all his incredible work on &lt;a href=&#34;https://github.com/ocornut/imgui&#34;&gt;Dear ImGui&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://evanpezent.com/&#34;&gt;Evan Pezent&lt;/a&gt; for all his work on &lt;a href=&#34;https://github.com/epezent/implot&#34;&gt;ImPlot&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Nelarius&#34;&gt;Johann Muszynski&lt;/a&gt; for all of his work on &lt;a href=&#34;https://github.com/Nelarius/imnodes&#34;&gt;imnodes&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Dear PyGui is licensed under the &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsor&lt;/h2&gt; &#xA;&lt;p&gt;Continued maintenance and development are a full-time endeavor which we would like to sustain and grow. Ongoing development is financially supported by users and private sponsors. If you enjoy Dear PyGui please consider becoming a &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui/wiki/Sponsors&#34;&gt;sponsor&lt;/a&gt; or buy us a &lt;a href=&#34;https://www.buymeacoffee.com/DearPyGui&#34;&gt;cup of coffee&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/sponsors/hoffstadt?label=Github%20Sponsors&#34;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;img src=&#34;https://img.shields.io/opencollective/sponsors/dearpygui?label=Open%20Collective%20Sponsors&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Gallery&lt;/h2&gt; &#xA;&lt;h4&gt;Plotting/Graphing&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;Dear PyGui&lt;/em&gt; includes a plotting API built with &lt;a href=&#34;https://github.com/epezent/implot&#34;&gt;ImPlot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/controls.gif&#34; width=&#34;380&#34;&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/dnd.gif&#34; width=&#34;380&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/query.gif&#34; width=&#34;380&#34;&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/bars.gif&#34; width=&#34;380&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/rt.gif&#34; width=&#34;380&#34;&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/markers.gif&#34; width=&#34;380&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/shaded.gif&#34; width=&#34;380&#34;&gt;&amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/epezent/implot/screenshots3/heat.gif&#34; width=&#34;380&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Node Editor&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;Dear PyGui&lt;/em&gt; includes a node editor built with &lt;a href=&#34;https://github.com/Nelarius/imnodes&#34;&gt;imnodes&lt;/a&gt; &lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/nodes2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Canvas&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;Dear PyGui&lt;/em&gt; includes a drawing API to create custom drawings, plot, and even 2D games. &lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/tetris.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/3d.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/nodes1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/space.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/snake.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/drawing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/canvas.png?raw=true&#34; alt=&#34;BasicUsageExample&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/nodes3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/3d1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/game1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/mandlebrot.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/hoffstadt/DearPyGui/raw/assets/readme/nodes4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chriskohlhoff/asio</title>
    <updated>2022-07-01T01:31:47Z</updated>
    <id>tag:github.com,2022-07-01:/chriskohlhoff/asio</id>
    <link href="https://github.com/chriskohlhoff/asio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Asio C++ Library&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>wang-xinyu/tensorrtx</title>
    <updated>2022-07-01T01:31:47Z</updated>
    <id>tag:github.com,2022-07-01:/wang-xinyu/tensorrtx</id>
    <link href="https://github.com/wang-xinyu/tensorrtx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of popular deep learning networks with TensorRT network definition API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorRTx&lt;/h1&gt; &#xA;&lt;p&gt;TensorRTx aims to implement popular deep learning networks with tensorrt network definition APIs. As we know, tensorrt has builtin parsers, including caffeparser, uffparser, onnxparser, etc. But when we use these parsers, we often run into some &#34;unsupported operations or layers&#34; problems, especially some state-of-the-art models are using new type of layers.&lt;/p&gt; &#xA;&lt;p&gt;So why don&#39;t we just skip all parsers? We just use TensorRT network definition APIs to build the whole network, it&#39;s not so complicated.&lt;/p&gt; &#xA;&lt;p&gt;I wrote this project to get familiar with tensorrt API, and also to share and learn from the community.&lt;/p&gt; &#xA;&lt;p&gt;All the models are implemented in pytorch/mxnet/tensorflown first, and export a weights file xxx.wts, and then use tensorrt to load weights, define network and do inference. Some pytorch implementations can be found in my repo &lt;a href=&#34;https://github.com/wang-xinyu/pytorchx&#34;&gt;Pytorchx&lt;/a&gt;, the remaining are from polular open-source implementations.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;26 May 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/triple-Mu&#34;&gt;triple-Mu&lt;/a&gt;: YOLOv5 python script with CUDA Python API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;23 May 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/yester31&#34;&gt;yhpark&lt;/a&gt;: Real-ESRGAN, Practical Algorithms for General Image/Video Restoration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;19 May 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/vjsrinivas&#34;&gt;vjsrinivas&lt;/a&gt;: YOLOv3 TRT8 support and Python script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;15 Mar 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/wdhao&#34;&gt;sky_hole&lt;/a&gt;: Swin Transformer - Semantic Segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;19 Oct 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/liuqi123123&#34;&gt;liuqi123123&lt;/a&gt; added cuda preprossing for yolov5, preprocessing + inference is 3x faster when batchsize=8.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;18 Oct 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/xupengao&#34;&gt;xupengao&lt;/a&gt;: YOLOv5 updated to v6.0, supporting n/s/m/l/x/n6/s6/m6/l6/x6.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;31 Aug 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/FamousDirector&#34;&gt;FamousDirector&lt;/a&gt;: update retinaface to support TensorRT 8.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;27 Aug 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/HaiyangPeng&#34;&gt;HaiyangPeng&lt;/a&gt;: add a python wrapper for hrnet segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1 Jul 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/freedenS&#34;&gt;freedenS&lt;/a&gt;: DE‚´∂TR: End-to-End Object Detection with Transformers. First Transformer model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;10 Jun 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/upczww&#34;&gt;upczww&lt;/a&gt;: EfficientNet b0-b8 and l2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;23 May 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/SsisyphusTao&#34;&gt;SsisyphusTao&lt;/a&gt;: CenterNet DLA-34 with DCNv2 plugin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;17 May 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/ybw108&#34;&gt;ybw108&lt;/a&gt;: arcface LResNet100E-IR and MobileFaceNet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6 May 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/makaveli10&#34;&gt;makaveli10&lt;/a&gt;: scaled-yolov4 yolov4-csp.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;29 Apr 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/upczww&#34;&gt;upczww&lt;/a&gt;: hrnet segmentation w18/w32/w48, ocr branch also.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;28 Apr 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/aditya-dl&#34;&gt;aditya-dl&lt;/a&gt;: mobilenetv2, alexnet, densenet121, mobilenetv3 with python API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/install.md&#34;&gt;Install the dependencies.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/getting_started.md&#34;&gt;A guide for quickly getting started, taking lenet5 as a demo.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/getting_started.md#the-wts-content-format&#34;&gt;The .wts file content format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/faq.md&#34;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/migrating_from_tensorrt_4_to_7.md&#34;&gt;Migrating from TensorRT 4 to 7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/multi_GPU_processing.md&#34;&gt;How to implement multi-GPU processing, taking YOLOv4 as example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/check_fp16_int8_support.md&#34;&gt;Check if Your GPU support FP16/INT8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/run_on_windows.md&#34;&gt;How to Compile and Run on Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/isarsoft/yolov4-triton-tensorrt&#34;&gt;Deploy YOLOv4 with Triton Inference Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/from_pytorch_to_trt_stepbystep_hrnet.md&#34;&gt;From pytorch to trt step by step, hrnet as example(Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Test Environment&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;TensorRT 7.x&lt;/li&gt; &#xA; &lt;li&gt;TensorRT 8.x(Some of the models support 8.x)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to run&lt;/h2&gt; &#xA;&lt;p&gt;Each folder has a readme inside, which explains how to run the models inside.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Following models are implemented.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/mlp&#34;&gt;mlp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the very basic model for starters, properly documented&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/lenet&#34;&gt;lenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the simplest, as a &#34;hello world&#34; of this project&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/alexnet&#34;&gt;alexnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;easy to implement, all layers are supported in tensorrt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/googlenet&#34;&gt;googlenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GoogLeNet (Inception v1)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/inception&#34;&gt;inception&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Inception v3, v4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/mnasnet&#34;&gt;mnasnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MNASNet with depth multiplier of 0.5 from the paper&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/mobilenet&#34;&gt;mobilenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MobileNet v2, v3-small, v3-large&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/resnet&#34;&gt;resnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;resnet-18, resnet-50 and resnext50-32x4d are implemented&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/senet&#34;&gt;senet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;se-resnet50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/shufflenetv2&#34;&gt;shufflenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ShuffleNet v2 with 0.5x output channels&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/squeezenet&#34;&gt;squeezenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SqueezeNet 1.1 model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/vgg&#34;&gt;vgg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;VGG 11-layer model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov3-tiny&#34;&gt;yolov3-tiny&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;weights and pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov3&#34;&gt;yolov3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;darknet-53, weights and pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov3-spp&#34;&gt;yolov3-spp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;darknet-53, weights and pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov4&#34;&gt;yolov4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CSPDarknet53, weights from &lt;a href=&#34;https://github.com/AlexeyAB/darknet#pre-trained-models&#34;&gt;AlexeyAB/darknet&lt;/a&gt;, pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov5&#34;&gt;yolov5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yolov5 v1.0-v6.0, pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;ultralytics/yolov5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/retinaface&#34;&gt;retinaface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;resnet50 and mobilnet0.25, weights from &lt;a href=&#34;https://github.com/biubug6/Pytorch_Retinaface&#34;&gt;biubug6/Pytorch_Retinaface&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/arcface&#34;&gt;arcface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LResNet50E-IR, LResNet100E-IR and MobileFaceNet, weights from &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;deepinsight/insightface&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/retinafaceAntiCov&#34;&gt;retinafaceAntiCov&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;mobilenet0.25, weights from &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;deepinsight/insightface&lt;/a&gt;, retinaface anti-COVID-19, detect face and mask attribute&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/dbnet&#34;&gt;dbnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Scene Text Detection, weights from &lt;a href=&#34;https://github.com/BaofengZan/DBNet.pytorch&#34;&gt;BaofengZan/DBNet.pytorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/crnn&#34;&gt;crnn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pytorch implementation from &lt;a href=&#34;https://github.com/meijieru/crnn.pytorch&#34;&gt;meijieru/crnn.pytorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/ufld&#34;&gt;ufld&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pytorch implementation from &lt;a href=&#34;https://github.com/cfzd/Ultra-Fast-Lane-Detection&#34;&gt;Ultra-Fast-Lane-Detection&lt;/a&gt;, ECCV2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/hrnet&#34;&gt;hrnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;hrnet-image-classification and hrnet-semantic-segmentation, pytorch implementation from &lt;a href=&#34;https://github.com/HRNet/HRNet-Image-Classification&#34;&gt;HRNet-Image-Classification&lt;/a&gt; and &lt;a href=&#34;https://github.com/HRNet/HRNet-Semantic-Segmentation&#34;&gt;HRNet-Semantic-Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/psenet&#34;&gt;psenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PSENet Text Detection, tensorflow implementation from &lt;a href=&#34;https://github.com/liuheng92/tensorflow_PSENet&#34;&gt;liuheng92/tensorflow_PSENet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/ibnnet&#34;&gt;ibnnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IBN-Net, pytorch implementation from &lt;a href=&#34;https://github.com/XingangPan/IBN-Net&#34;&gt;XingangPan/IBN-Net&lt;/a&gt;, ECCV2018&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/unet&#34;&gt;unet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;U-Net, pytorch implementation from &lt;a href=&#34;https://github.com/milesial/Pytorch-UNet&#34;&gt;milesial/Pytorch-UNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/repvgg&#34;&gt;repvgg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RepVGG, pytorch implementation from &lt;a href=&#34;https://github.com/DingXiaoH/RepVGG&#34;&gt;DingXiaoH/RepVGG&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/lprnet&#34;&gt;lprnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LPRNet, pytorch implementation from &lt;a href=&#34;https://github.com/xuexingyu24/License_Plate_Detection_Pytorch&#34;&gt;xuexingyu24/License_Plate_Detection_Pytorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/refinedet&#34;&gt;refinedet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RefineDet, pytorch implementation from &lt;a href=&#34;https://github.com/luuuyi/RefineDet.PyTorch&#34;&gt;luuuyi/RefineDet.PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/densenet&#34;&gt;densenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DenseNet-121, from torchvision.models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/rcnn&#34;&gt;rcnn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FasterRCNN and MaskRCNN, model from &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;detectron2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tsm&#34;&gt;tsm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TSM: Temporal Shift Module for Efficient Video Understanding, ICCV2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/scaled-yolov4&#34;&gt;scaled-yolov4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yolov4-csp, pytorch from &lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;WongKinYiu/ScaledYOLOv4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/centernet&#34;&gt;centernet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CenterNet DLA-34, pytorch from &lt;a href=&#34;https://github.com/xingyizhou/CenterNet&#34;&gt;xingyizhou/CenterNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/efficientnet&#34;&gt;efficientnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;EfficientNet b0-b8 and l2, pytorch from &lt;a href=&#34;https://github.com/lukemelas/EfficientNet-PyTorch&#34;&gt;lukemelas/EfficientNet-PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/detr&#34;&gt;detr&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DE‚´∂TR, pytorch from &lt;a href=&#34;https://github.com/facebookresearch/detr&#34;&gt;facebookresearch/detr&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/swin-transformer&#34;&gt;swin-transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Swin Transformer - Semantic Segmentation, only support Swin-T. The Pytorch implementation is &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer.git&#34;&gt;microsoft/Swin-Transformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/real-esrgan&#34;&gt;real-esrgan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Real-ESRGAN. The Pytorch implementation is &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;real-esrgan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;The .wts files can be downloaded from model zoo for quick evaluation. But it is recommended to convert .wts from pytorch/mxnet/tensorflow model, so that you can retrain your own model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1Ri0IDa5OChtcA3zjqRTW57uG6TnfN4Do?usp=sharing&#34;&gt;GoogleDrive&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/19s6hO8esU7-TtZEXN7G3OA&#34;&gt;BaiduPan&lt;/a&gt; pwd: uvv2&lt;/p&gt; &#xA;&lt;h2&gt;Tricky Operations&lt;/h2&gt; &#xA;&lt;p&gt;Some tricky operations encountered in these models, already solved, but might have better solutions.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BatchNorm&lt;/td&gt; &#xA;   &lt;td&gt;Implement by a scale layer, used in resnet, googlenet, mobilenet, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxPool2d(ceil_mode=True)&lt;/td&gt; &#xA;   &lt;td&gt;use a padding layer before maxpool to solve ceil_mode=True, see googlenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;average pool with padding&lt;/td&gt; &#xA;   &lt;td&gt;use setAverageCountExcludesPadding() when necessary, see inception.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;relu6&lt;/td&gt; &#xA;   &lt;td&gt;use &lt;code&gt;Relu6(x) = Relu(x) - Relu(x-6)&lt;/code&gt;, see mobilenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;torch.chunk()&lt;/td&gt; &#xA;   &lt;td&gt;implement the &#39;chunk(2, dim=C)&#39; by tensorrt plugin, see shufflenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;channel shuffle&lt;/td&gt; &#xA;   &lt;td&gt;use two shuffle layers to implement &lt;code&gt;channel_shuffle&lt;/code&gt;, see shufflenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;adaptive pool&lt;/td&gt; &#xA;   &lt;td&gt;use fixed input dimension, and use regular average pooling, see shufflenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;leaky relu&lt;/td&gt; &#xA;   &lt;td&gt;I wrote a leaky relu plugin, but PRelu in &lt;code&gt;NvInferPlugin.h&lt;/code&gt; can be used, see yolov3 in branch &lt;code&gt;trt4&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yolo layer v1&lt;/td&gt; &#xA;   &lt;td&gt;yolo layer is implemented as a plugin, see yolov3 in branch &lt;code&gt;trt4&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yolo layer v2&lt;/td&gt; &#xA;   &lt;td&gt;three yolo layers implemented in one plugin, see yolov3-spp.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;upsample&lt;/td&gt; &#xA;   &lt;td&gt;replaced by a deconvolution layer, see yolov3.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hsigmoid&lt;/td&gt; &#xA;   &lt;td&gt;hard sigmoid is implemented as a plugin, hsigmoid and hswish are used in mobilenetv3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;retinaface output decode&lt;/td&gt; &#xA;   &lt;td&gt;implement a plugin to decode bbox, confidence and landmarks, see retinaface.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mish&lt;/td&gt; &#xA;   &lt;td&gt;mish activation is implemented as a plugin, mish is used in yolov4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;prelu&lt;/td&gt; &#xA;   &lt;td&gt;mxnet&#39;s prelu activation with trainable gamma is implemented as a plugin, used in arcface&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HardSwish&lt;/td&gt; &#xA;   &lt;td&gt;hard_swish = x * hard_sigmoid, used in yolov5 v3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSTM&lt;/td&gt; &#xA;   &lt;td&gt;Implemented pytorch nn.LSTM() with tensorrt api&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Speed Benchmark&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BatchSize&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mode&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Shape(HxW)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3-tiny&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;333&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3(darknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3(darknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;INT8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3-spp(darknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv4(CSPDarknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv4(CSPDarknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv4(CSPDarknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;142&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;173&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;190&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-m v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-l v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-x v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;142&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-m v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-l v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-x v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace(resnet50)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace(resnet50)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;INT8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;204&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace(mobilenet0.25)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;417&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ArcFace(LResNet50E-IR)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;112x112&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;333&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CRNN&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32x100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Help wanted, if you got speed results, please add an issue or PR.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments &amp;amp; Contact&lt;/h2&gt; &#xA;&lt;p&gt;Any contributions, questions and discussions are welcomed, contact me by following info.&lt;/p&gt; &#xA;&lt;p&gt;E-mail: &lt;a href=&#34;mailto:wangxinyu_es@163.com&#34;&gt;wangxinyu_es@163.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;WeChat ID: wangxinyu0375 (ÂèØÂä†ÊàëÂæÆ‰ø°Ëøõtensorrtx‰∫§ÊµÅÁæ§Ôºå&lt;strong&gt;Â§áÊ≥®Ôºötensorrtx&lt;/strong&gt;)&lt;/p&gt;</summary>
  </entry>
</feed>