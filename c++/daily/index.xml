<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-17T01:27:03Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>0x36/Pixel_GPU_Exploit</title>
    <updated>2024-03-17T01:27:03Z</updated>
    <id>tag:github.com,2024-03-17:/0x36/Pixel_GPU_Exploit</id>
    <link href="https://github.com/0x36/Pixel_GPU_Exploit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Android 14 kernel exploit for Pixel7/8 Pro&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mali GPU Kernel LPE&lt;/h1&gt; &#xA;&lt;p&gt;This article provides an in-depth analysis of two kernel vulnerabilities within the Mali GPU, reachable from the default application sandbox, which I independently identified and reported to Google. It includes a kernel exploit that achieves arbitrary kernel r/w capabilities. Consequently, it disables SELinux and elevates privileges to root on Google Pixel 7 and 8 Pro models running the following Android 14 versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pixel 8 Pro: &lt;code&gt;google/husky/husky:14/UD1A.231105.004/11010374:user/release-keys&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pixel 7 Pro: &lt;code&gt;google/cheetah/cheetah:14/UP1A.231105.003/11010452:user/release-keys&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pixel 7 Pro: &lt;code&gt;google/cheetah/cheetah:14/UP1A.231005.007/10754064:user/release-keys&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pixel 7: &lt;code&gt;google/panther/panther:14/UP1A.231105.003/11010452:user/release-keys&lt;/code&gt; (by &lt;a href=&#34;https://github.com/m4b4&#34;&gt;m4b4 (Marcel)&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Vulnerabilities&lt;/h2&gt; &#xA;&lt;p&gt;This exploit leverages two vulnerabilities: an integer overflow resulting from an incomplete patch in the &lt;code&gt;gpu_pixel_handle_buffer_liveness_update_ioctl&lt;/code&gt; ioctl command, and an information leak within the timeline stream message buffers.&lt;/p&gt; &#xA;&lt;h3&gt;Buffer Underflow in gpu_pixel_handle_buffer_liveness_update_ioctl() Due to Incorrect Integer Overflow Fix&lt;/h3&gt; &#xA;&lt;p&gt;Google addressed an integer overflow in the &lt;code&gt;gpu_pixel_handle_buffer_liveness_update_ioctl&lt;/code&gt; ioctl command &lt;a href=&#34;https://android.googlesource.com/kernel/google-modules/gpu/+/68073dce197709c025a520359b66ed12c5430914%5E%21/#F0&#34;&gt;in this commit&lt;/a&gt;. At first, when I reported this issue, I thought the bug was caused by an issue in the patch described earlier. After reviewing the report, I came to the realization that my analysis of the a vulnerability was inaccurate. Despite my first assumption of the patch being incomplete, it effectively resolves and prevents an underflow in the calculation. This lead me to suspect that the change wasn&#39;t applied in the production builds. However, although I can cause an underflow in the calculation, it is not possible to cause an overflow. This suggests that the ioctl command has been partially fixed, although not with the above patch shown above. Looking at IDA revealed that another incomplete patch was shipped in the production releases, and this patch is not present in any git branch of the mali gpu kernel module.&lt;/p&gt; &#xA;&lt;p&gt;This vulnerability was first discovered in the latest Android version and reported on November 19, 2023. Google later informed me that they had already internally identified it and had assigned it &lt;a href=&#34;https://source.android.com/docs/security/bulletin/pixel/2023-12-01&#34;&gt;CVE-2023-48409&lt;/a&gt; in the December Android Security Bulletin, labeling it as a duplicate issue. Although I was able to verify that the bug had been internally identified months prior to my report, (based on the commit date around August 30) there remains confusion. Specifically, it&#39;s strange that the Security Patch Levels (SPL) for October and November of the most recent devices were still affected by this vulnerability â€”I haven&#39;t investigated versions prior to these. Therefore, I am unable to conclusively determine whether this was truly a duplicate issue and if the appropriate patch was indeed scheduled for December prior to my submission or if there was an oversight in addressing this vulnerability.&lt;/p&gt; &#xA;&lt;p&gt;Anyway, what makes this bug powerful is the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The buffer &lt;code&gt;info.live_ranges&lt;/code&gt; is fully user-controlled.&lt;/li&gt; &#xA; &lt;li&gt;The overflowing values are user-controlled input, thereby, we can overflow the calculation so the &lt;code&gt;info.live_ranges&lt;/code&gt; pointer can be at an arbitrary offset prior to the start of the &lt;code&gt;buff&lt;/code&gt; kernel address.&lt;/li&gt; &#xA; &lt;li&gt;The allocation size is also user controlled input, which gives the ability to request a memory allocation from any general-purpose slab allocator.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This vulnerability shares similarities with the &lt;a href=&#34;https://github.com/0x36/weightBufs&#34;&gt;DeCxt::RasterizeScaleBiasData() Buffer underflow vulnerability&lt;/a&gt; I found and exploited in the iOS 15 kernel back in 2022.&lt;/p&gt; &#xA;&lt;h3&gt;Leakage of Kernel Pointers in Timeline Stream Message Buffers&lt;/h3&gt; &#xA;&lt;p&gt;The GPU Mali implements a custom &lt;code&gt;timeline stream&lt;/code&gt; designed to gather information, serialize it, and subsequently write it to a ring buffer following a specific format. Users can invoke the ioctl command &lt;code&gt;kbase_api_tlstream_acquire&lt;/code&gt; to obtain a file descriptor, enabling them to read from this ring buffer. The format of the messages is as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-pantah-5.10-android14-qpr2-beta/mali_kbase/mali_kbase_mipe_proto.h#68&#34;&gt;packet header&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-pantah-5.10-android14-qpr2-beta/mali_kbase/tl/mali_kbase_tracepoints.c#34&#34;&gt;message id&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A serialized message buffer, where the specific content is contingent upon the message ID. For example, the &lt;code&gt;__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait&lt;/code&gt; function serializes the &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; and &lt;code&gt;dma_fence&lt;/code&gt; kernel pointers into the message buffer, resulting in leaking kernel pointers to user space process.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait(&#xA;&#x9;struct kbase_tlstream *stream,&#xA;&#x9;const void *kcpu_queue,&#xA;&#x9;const void *fence&#xA;)&#xA;{&#xA;&#x9;const u32 msg_id = KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT;&#xA;&#x9;const size_t msg_size = sizeof(msg_id) + sizeof(u64)&#xA;&#x9;&#x9;+ sizeof(kcpu_queue)&#xA;&#x9;&#x9;+ sizeof(fence)&#xA;&#x9;&#x9;;&#xA;&#x9;char *buffer;&#xA;&#x9;unsigned long acq_flags;&#xA;&#x9;size_t pos = 0;&#xA;&#xA;&#x9;buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &amp;amp;acq_flags);&#xA;&#xA;&#x9;pos = kbasep_serialize_bytes(buffer, pos, &amp;amp;msg_id, sizeof(msg_id));&#xA;&#x9;pos = kbasep_serialize_timestamp(buffer, pos);&#xA;&#x9;pos = kbasep_serialize_bytes(buffer,&#xA;&#x9;&#x9;pos, &amp;amp;kcpu_queue, sizeof(kcpu_queue));&#xA;&#x9;pos = kbasep_serialize_bytes(buffer,&#xA;&#x9;&#x9;pos, &amp;amp;fence, sizeof(fence));&#xA;&#xA;&#x9;kbase_tlstream_msgbuf_release(stream, acq_flags);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The proof of concept exploit leaks the &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; object address by monitoring to the message id &lt;code&gt;KBASE_TL_KBASE_NEW_KCPUQUEUE&lt;/code&gt; which is dispatched by the &lt;code&gt;kbasep_kcpu_queue_new&lt;/code&gt; function whenever a new kcpu queue object is allocated.&lt;/p&gt; &#xA;&lt;p&gt;Google informed me that the vulnerability was reported in March 2023 and was assigned &lt;a href=&#34;https://source.android.com/docs/security/bulletin/2023-07-01&#34;&gt;CVE-2023-26083&lt;/a&gt; in their security bulletin. Nonetheless, I was able to replicate the issue on the latest Pixel devices shipped with the Security Patch Levels (SPL) for October and November, indicating that the fix had not been applied correctly or at all. Subsequently, Google quickly addressed the issue in the December Security Update Bulletin without offering credit, and later informed me that the issue was considered a duplicate. The rationale behind labeling this issue as a duplicate, however, remains questionable.&lt;/p&gt; &#xA;&lt;h2&gt;Exploitation&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;So I have two interesting vulnerabilities. The first one offers a powerful capability to modify the content of any 16-byte aligned kernel address that comes before the allocated &lt;del&gt;buff&lt;/del&gt; address. The second vulnerability provides hints into the potential locations of objects within the kernel memory.&lt;/p&gt; &#xA;&lt;h3&gt;Notes on buffer_count and live_ranges_count Values&lt;/h3&gt; &#xA;&lt;p&gt;With total control over the &lt;code&gt;buffer_count&lt;/code&gt; and &lt;code&gt;live_ranges_count&lt;/code&gt; fields, I have the flexibility to select the target slab and the precise offset I intend to write to. However, selecting values for &lt;code&gt;buffer_count&lt;/code&gt; and &lt;code&gt;live_ranges_count&lt;/code&gt; requires careful consideration due to several constraints and factors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Both values are related, and the overflow will occur only if all the newly introduced checks are bypassed.&lt;/li&gt; &#xA; &lt;li&gt;The requirement for the negative offset to be 16-bytes aligned restricts the ability to write to any chosen location. However, this is generally not a significant hindrance.&lt;/li&gt; &#xA; &lt;li&gt;Opting for a larger offset leads to a large amount of data being written to areas of memory that may not be intended targets. For instance, if the allocation size overflows to &lt;code&gt;0x3004&lt;/code&gt;, the &lt;code&gt;live_ranges&lt;/code&gt; pointer would be set to &lt;code&gt;-0x4000&lt;/code&gt; bytes from the &lt;code&gt;buff&lt;/code&gt; object&#39;s allocated space. The &lt;code&gt;copy_from_user&lt;/code&gt; function would then write &lt;code&gt;0x7004&lt;/code&gt; bytes, based on the calculation of &lt;code&gt;update-&amp;gt;live_ranges_count&lt;/code&gt; times 4. Consequently, this operation would result in user-controlled data overwriting the memory area between the &lt;code&gt;live_ranges&lt;/code&gt; pointer and the &lt;code&gt;buff&lt;/code&gt; allocation. It is essential, therefore, to carefully ensure that no critical system objects within that range are accidentally overwritten. Given that the operation involves a &lt;code&gt;copy_from_user&lt;/code&gt; call, one might consider triggering an &lt;code&gt;EFAULT&lt;/code&gt; by deliberately un-mapping the undesired memory region following the user source buffer to prevent data from being written to sensitive locations. However, this approach is ineffective, that&#39;s because if the &lt;code&gt;raw_copy_from_user&lt;/code&gt; function fails, it will zero out the remaining bytes in the destination kernel buffer. This behavior is implemented to ensure that in case of a partial copy due to an error, the rest of the kernel buffer does not contain uninitialized data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;static inline __must_check unsigned long&#xA;_copy_from_user(void *to, const void __user *from, unsigned long n)&#xA;{&#xA;&#x9;unsigned long res = n;&#xA;&#x9;might_fault();&#xA;&#x9;if (!should_fail_usercopy() &amp;amp;&amp;amp; likely(access_ok(from, n))) {&#xA;&#x9;&#x9;instrument_copy_from_user(to, from, n);&#xA;&#x9;&#x9;res = raw_copy_from_user(to, from, n);&#xA;&#x9;}&#xA;&#x9;if (unlikely(res))&#xA;&#x9;&#x9;memset(to + (n - res), 0, res);&#xA;&#x9;return res;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Considering this, we need to carefully select the object to overwrite and the data to write.&lt;/p&gt; &#xA;&lt;h3&gt;Choosing the Right Object to Overwrite&lt;/h3&gt; &#xA;&lt;p&gt;Because Iâ€™m stuck with this unfortunate check, my strategy is to identify an object that, if nulled out, will not produce any undesired outcome. But, before I get to that, there&#39;s another issue to deal with. Remember when I said in the last part that I can choose any allocation size and thus any general purpose slab cache allocator to service my allocation buffer? Thatâ€™s not correct, because it is because of &lt;code&gt;copy_from_user&lt;/code&gt; again! It is due to the &lt;a href=&#34;https://lwn.net/Articles/693745/&#34;&gt;CONFIG_HARDENED_USERCOPY&lt;/a&gt; mitigation. It forbids specifying a size that does not meet the corresponding slab cache size where the kernel destination buffer corresponds (in this case) of a heap object. It determines whether the buffer&#39;s page is a slab page, and if so, it retrieves the matching &lt;code&gt;kmem_cache-&amp;gt;size&lt;/code&gt; and determines whether the user supplied size will not exceed it; otherwise, the kernel just crashes due to the size mismatch. So, in other words, I cannot target objects that belong to the general purpose allocator, BUT I can still target objects that have large sizes (i.e. those served directly by the page allocator).&lt;/p&gt; &#xA;&lt;p&gt;The first thought that came to mind was to use the &lt;code&gt;pipe_buffer&lt;/code&gt; technique, which is a very elegant technique to obtain arbitrary read/write primitives. I won&#39;t go into detail about the technique, but readers are encouraged to read this fantastic blog from &lt;a href=&#34;https://www.interruptlabs.co.uk/articles/pipe-buffer&#34;&gt;Interrupt Labs&lt;/a&gt;. When constructing a pipe object, the &lt;code&gt;pipe_buffer&lt;/code&gt; object is initially created in an array of 16 elements; however, the array size can be adjusted using &lt;code&gt;fcntl(F_SETPIPE_SZ)&lt;/code&gt;. Therefore, the &lt;code&gt;pipe_buffer&lt;/code&gt; array allocation can be adjusted such that it can be served from the page allocator, making it a perfect target object to attack. After selecting the pipe_buffer object as a target candidate, the next step toward achieving kernel r/w is to overwrite its content with the underflow vulnerability, which will allow me to read/write from/to any memory location whose page is overwriting the &lt;code&gt;pipe_buffer-&amp;gt;page&lt;/code&gt; field. Because the vulnerability allows me to write arbitrary data, I can control the whole content of &#39;&lt;code&gt;pipe_buffer&lt;/code&gt;,&#39; including its page field, and to do so, I need to allocate the &lt;code&gt;pipe_buffer&lt;/code&gt; array before the vulnerable &lt;code&gt;kbuff&lt;/code&gt; object and they have to be next to each other.&lt;/p&gt; &#xA;&lt;h3&gt;Positioning pipe_buffer and buff Objects Adjacently&lt;/h3&gt; &#xA;&lt;p&gt;I sprayed the kernel memory with a lot of &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; objects then followed by a bunch of &lt;code&gt;pipe_buffer&lt;/code&gt; arrays. I canâ€™t just use the &lt;code&gt;pipe_buffer&lt;/code&gt; arrays alone as a primary source for spraying due to the limitation imposed by &lt;code&gt;pipe_max_size&lt;/code&gt;. Therefore, I decided to start spraying with the &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; object. Choosing the &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; object was for two reasons: its allocation size is &lt;code&gt;0x38C8&lt;/code&gt; thus handled by the page allocator, and I can deterministically obtain its kernel address using the information kernel leak bug, making it a good object to spray with as well as a good object to target (as weâ€™ll see in the next section).&lt;/p&gt; &#xA;&lt;p&gt;As mentioned before, I used &lt;code&gt;fcntl(F_SETPIPE_SZ)&lt;/code&gt; to increase the size of the &lt;code&gt;pipe_buffer&lt;/code&gt; array allocation so that it can be served by the page allocator. To be more specific, I chose the allocation size to be a ==0x4000 bytes (4 * PAGE_SIZE)== in order to be consistent with the &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; allocations.&lt;/p&gt; &#xA;&lt;h3&gt;Obtaining a struct page Address&lt;/h3&gt; &#xA;&lt;p&gt;In order to properly use the &lt;code&gt;pipe_buffer&lt;/code&gt;, a page address is required. Being able to identify the kernel address of a &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; object that I can deliberately create and destroy makes it a good candidate to use and finding its matching &lt;code&gt;struct page&lt;/code&gt; can be achieved by using the &lt;code&gt;virt_to_page&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;h3&gt;Contents to Write in the pipe_buffer&lt;/h3&gt; &#xA;&lt;p&gt;So the &lt;code&gt;pipe_buffer&lt;/code&gt; object is as follow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;struct pipe_buffer {&#xA;&#x9;struct page *page;&#xA;&#x9;unsigned int offset, len;&#xA;&#x9;const struct pipe_buf_operations *ops;&#xA;&#x9;unsigned int flags;&#xA;&#x9;unsigned long private;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As previously mentioned, the &lt;code&gt;page&lt;/code&gt; field must include a valid page address. The &lt;code&gt;offset&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt; fields must not exceed &lt;code&gt;PAGE_SIZE&lt;/code&gt;, otherwise the pipe will increase the head/tail counters, resulting in the use of a new &lt;code&gt;pipe_buffer&lt;/code&gt; object and loss of control over the fake&amp;nbsp;pipe buffer. Also, the &lt;code&gt;flags&lt;/code&gt; must be &lt;code&gt;PIPE_BUF_FLAG_CAN_MERGE&lt;/code&gt; so the following &lt;code&gt;pipe_write&lt;/code&gt; calls instead of blindly incrementing the head counter and using the next pipe buffer, it first checks whether thereâ€™s a space in the current &lt;code&gt;pipe_buffer&lt;/code&gt; that will fit the write request or not, and if there is, it will simply append data to the same pipe buffer starting from the value stored at the &lt;code&gt;len&lt;/code&gt; field. In order to avoid crashing the device at &lt;code&gt;pipe_buf_confirm&lt;/code&gt;, which is called by &lt;code&gt;pipe_write&lt;/code&gt; and &lt;code&gt;pipe_read&lt;/code&gt;â€™, the &lt;code&gt;ops&lt;/code&gt; pointer must also be a valid kernel address with a &lt;code&gt;ops-&amp;gt;confirm&lt;/code&gt; field set to &lt;em&gt;NULL&lt;/em&gt;. I can simply use an offset within the leaked &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; object that is NULL and will not change under any circumstances.&lt;/p&gt; &#xA;&lt;h3&gt;Choosing the Optimal Offset Value for Underflow&lt;/h3&gt; &#xA;&lt;p&gt;While the allocation sizes of the &lt;code&gt;buff&lt;/code&gt; ,&lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; and &lt;code&gt;pipe_buffer&lt;/code&gt; are &lt;del&gt;0x4000&lt;/del&gt; bytes, I chose to underflow the buffer with &lt;strong&gt;0x8000&lt;/strong&gt; bytes. why ?&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s take a brief look at how &lt;code&gt;pipe_buffers&lt;/code&gt; are updated during read and write operations. Assume we can shape the &lt;code&gt;pipe_buffer&lt;/code&gt; to look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;struct pipe_buffer {&#xA;&#x9;.page = virt_to_page(addr),&#xA;&#x9;.offset =  0,&#xA;&#x9;.len = 0x40,&#xA;&#x9;.ops = kcpu_addr + 0x50,&#xA;&#x9;.flags = PIPE_BUF_FLAG_CAN_MERGE,&#xA;&#x9;unsigned long private = 0&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;While the bug gives the ability to arbitrary control this content of this object, it only does so &lt;strong&gt;once&lt;/strong&gt; because the underflowed object is freed immediately after the &lt;code&gt;ioctl&lt;/code&gt; call finishes. This actually poses a problem because I need to manually update the &lt;code&gt;pipe_buffer&lt;/code&gt; object to make it useable again since each pipe read/write operation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;.page&lt;/code&gt; field is not updated; it remains the same, and when the buffer is empty, it is released, which I do not want to happen because the &lt;code&gt;.ops&lt;/code&gt; field is not correctly set.&lt;/li&gt; &#xA; &lt;li&gt;Because the &lt;code&gt;pipe_buffer&lt;/code&gt; updates the &lt;code&gt;.offset&lt;/code&gt; field on a read operation, therefore, I cannot read the same memory region again.&lt;/li&gt; &#xA; &lt;li&gt;The data written to the &lt;code&gt;pipe_buffer&lt;/code&gt; will be appended to the buffer starting from the &lt;code&gt;.len&lt;/code&gt; value (assuming that &lt;code&gt;PIPE_BUF_FLAG_CAN_MERGE&lt;/code&gt; flag is set) and the &lt;code&gt;.len&lt;/code&gt; is updated accordingly. That is, we can&#39;t write data into the exact address twice.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As a result, unless I properly update the &lt;code&gt;pipe_buffer&lt;/code&gt; after each read or write operation, I cannot read and write from/to the same pipe at the same time. That&#39;s why underflowing with &lt;code&gt;0x8000&lt;/code&gt; bytes is much more practical, because instead of overwriting a single &lt;code&gt;pipe_buffer&lt;/code&gt;, &lt;strong&gt;I&#39;ll overwrite two distinct pipe_buffer instances of two distinct pipes objects: one for will be considered for read and the other for write operations&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#define PIPE_BUF_FLAG_CAN_MERGE&#x9;0x10&#x9;/* can merge buffers */&#xA;&#xA;pipe_read = (struct pipe_buffer *)( ptr);&#xA;pipe_read-&amp;gt;page = virt_to_page(ta-&amp;gt;kcpu_kaddr);&#xA;pipe_read-&amp;gt;offset = 0;&#xA;pipe_read-&amp;gt;len = 0xfff;&#xA;pipe_read-&amp;gt;ops = (const void *)(ta-&amp;gt;kcpu_kaddr + 0x50);&#xA;pipe_read-&amp;gt;flags = PIPE_BUF_FLAG_CAN_MERGE;&#xA;pipe_read-&amp;gt;private = 0;&#xA;&#xA;pipe_write = (struct pipe_buffer *)( ptr + 0x4000);&#xA;pipe_write-&amp;gt;page = virt_to_page(ta-&amp;gt;kcpu_kaddr);&#xA;pipe_write-&amp;gt;offset = 0;&#xA;pipe_write-&amp;gt;len = 0;             /* This is the starting position of the pipe_write */&#xA;pipe_write-&amp;gt;ops = (const void *)(ta-&amp;gt;kcpu_kaddr + 0x50);&#xA;pipe_write-&amp;gt;flags = PIPE_BUF_FLAG_CAN_MERGE;&#xA;pipe_write-&amp;gt;private = 0;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;pipe_read&lt;/code&gt; is a fake pipe buffer that will be used for reading data from the target page starting at &lt;code&gt;.offset = 0&lt;/code&gt; up to &lt;code&gt;0xfff&lt;/code&gt; bytes, whereas &lt;code&gt;pipe_write&lt;/code&gt; is a fake &lt;code&gt;pipe_buffer&lt;/code&gt; that will be used for writing data starting from &lt;code&gt;.len = 0&lt;/code&gt; up to &lt;code&gt;0xfff&lt;/code&gt; bytes. It&#39;s also very important to mention again that writing more than &lt;code&gt;PAGE_SIZE&lt;/code&gt; bytes will push the pipe to increment the head counter, therefore using a fresh newly allocated &lt;code&gt;pipe_buffer&lt;/code&gt; and losing control over our fake &lt;code&gt;pipe_write&lt;/code&gt;. In the other hand, emptying (reading 0xfff data from) &lt;code&gt;fake_read&lt;/code&gt; buffer tells the kernel to release the actual page by calling &lt;code&gt;opsâ†’release&lt;/code&gt; causing the kernel to crash because I still donâ€™t have a kernel text address. Although I managed to segregate the pipe read and write operations so that performing a write in one pipe end will not interfere with the other pipe buffer and vice versa, I still havenâ€™t solved the core issue: How to reliably update the pipe buffer? The obvious answer came to mind was just to repeat the spray process again and again after each pipe read or write call. And this makes no sense because it would have had a significant impact on exploit reliability. In the following section, I will divide the goal into two sub-goals: to begin, I&#39;ll focus on the &lt;code&gt;.page&lt;/code&gt; field only, followed by the &lt;code&gt;.len/.offset&lt;/code&gt; fields afterward.&lt;/p&gt; &#xA;&lt;h3&gt;Modifying the pipe_bufferâ†’page Field&lt;/h3&gt; &#xA;&lt;p&gt;To my surprise, I don&#39;t have or need to update the &lt;code&gt;.page&lt;/code&gt; at all, that&#39;s because I can overwrite the &lt;code&gt;pipe_bufferâ†’page&lt;/code&gt; to point to the page address of the leaked &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt;. Therefore, **All I need to do is release the &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; object and overlap it with a new &lt;code&gt;pipe_buffer&lt;/code&gt; object. Yup! Now I have a &lt;code&gt;pipe_bufferâ†’page&lt;/code&gt; that points to a legitimate &lt;code&gt;pipe_buffer&lt;/code&gt; object! Replacing &lt;code&gt;kbase_kcpu_command_queue&lt;/code&gt; with &lt;code&gt;pipe_buffer&lt;/code&gt; gives us the ability to manipulate a legitimate pipe buffer without regularly having to update the &lt;code&gt;.page&lt;/code&gt; field. However, I still have to deal with the &lt;code&gt;.len&lt;/code&gt; and &lt;code&gt;.offset&lt;/code&gt; fields.&lt;/p&gt; &#xA;&lt;h3&gt;Modifying the pipe_bufferâ†’len/offset Fields&lt;/h3&gt; &#xA;&lt;p&gt;As I&#39;ve mentioned earlier, doing pipe read/write updates the &lt;code&gt;.len&lt;/code&gt; and &lt;code&gt;.offset&lt;/code&gt; fields, rendering subsequent read/write operations on the same page unusable, even if performed over the two distinct pipes. Here&#39;s another trick: &lt;strong&gt;there&#39;s a technique to read/write data without even touching the &lt;code&gt;.len/.offset&lt;/code&gt; fields!&lt;/strong&gt;. And it is possible to achieve this by faulting &lt;code&gt;copy_page_from_iter&lt;/code&gt; and &lt;code&gt;copy_page_to_iter&lt;/code&gt; calls on &lt;code&gt;pipe_read/write&lt;/code&gt;! Yes, just like &lt;code&gt;copy_to/from_user&lt;/code&gt;, &lt;code&gt;copy_page_to/from_iter&lt;/code&gt; copies data from/to user-space that is passed through the &lt;code&gt;iov_iter&lt;/code&gt; structure, and it can be faulted.&lt;/p&gt; &#xA;&lt;p&gt;To continue with the previous example, if we wish to write 8 bytes of data to an address, the provided user space buffer size must be 8, followed by an unmapped or non-readable area of memory, and then pass &lt;code&gt;9&lt;/code&gt; as a size argument to the &lt;code&gt;write&lt;/code&gt; system call, indicating the amount of data that we want to write.This operation will write 8 bytes and fail on the &lt;em&gt;ninth&lt;/em&gt; because it encounters an unmapped/unread memory location. As a result, the data has been effectively written to the destination kernel buffer and the&lt;code&gt;.len&lt;/code&gt; field has not been modified. The &lt;code&gt;pipe_write&lt;/code&gt; kernel function will just return without updating the &lt;code&gt;buf-&amp;gt;len&lt;/code&gt; field.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;&#x9;&#x9;if ((buf-&amp;gt;flags &amp;amp; PIPE_BUF_FLAG_CAN_MERGE) &amp;amp;&amp;amp;&#xA;&#x9;&#x9;    offset + chars &amp;lt;= PAGE_SIZE) {&#xA;&#x9;&#x9;&#x9;ret = pipe_buf_confirm(pipe, buf);&#xA;&#x9;&#x9;&#x9;if (ret)&#xA;&#x9;&#x9;&#x9;&#x9;goto out;&#xA;&#xA;&#x9;&#x9;&#x9;ret = copy_page_from_iter(buf-&amp;gt;page, offset, chars, from);&#xA;&#x9;&#x9;&#x9;if (unlikely(ret &amp;lt; chars)) {&#xA;&#x9;&#x9;&#x9;&#x9;ret = -EFAULT;&#xA;&#x9;&#x9;&#x9;&#x9;goto out;&#xA;&#x9;&#x9;&#x9;}&#xA;&#xA;&#x9;&#x9;&#x9;buf-&amp;gt;len += ret;&#xA;&#x9;&#x9;&#x9;if (!iov_iter_count(from))&#xA;&#x9;&#x9;&#x9;&#x9;goto out;&#xA;&#x9;&#x9;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same is true for read operations; if we wish to read 8 bytes, make the ninth byte of the buffer unreadable then just claim that we want to read 9 bytes, the data will be copied to the user buffer without changing the &lt;code&gt;.offset&lt;/code&gt; field. As a result, we are able to perform unlimited read/write operations on any kernel memory address without having to recurrently go through the spray process.&lt;/p&gt; &#xA;&lt;h3&gt;Getting root&lt;/h3&gt; &#xA;&lt;p&gt;Now that I have a strong arbitrary read/write primitive, I just looked through all the &lt;code&gt;struct page&lt;/code&gt; in the &lt;code&gt;VMEMMAP_START&lt;/code&gt; array to determine the kernel text starting address using the technique outlined in the &lt;a href=&#34;https://www.interruptlabs.co.uk/articles/pipe-buffer&#34;&gt;Interrupt Labs&lt;/a&gt; blog post. Then I realized that &lt;code&gt;init_task&lt;/code&gt; is nulled out in &lt;em&gt;Android November Security Updates&lt;/em&gt;, so I just used &lt;code&gt;kthreadd_task&lt;/code&gt; instead. Having &lt;code&gt;kthreadd_task&lt;/code&gt; kernel address allowed me to walk the &lt;code&gt;task-&amp;gt;tasks&lt;/code&gt; list and obtain my own &lt;code&gt;current&lt;/code&gt; task kernel address, then zero out the &lt;code&gt;cred&lt;/code&gt; structure to achieve root privileges.&lt;/p&gt; &#xA;&lt;p&gt;Later, I realized scanning all the page addresses was unnecessary because I already had the anon_pipe_buf_ops kernel text address from a pipe_buffer object. With this information, I could deduce the kernel text base address, effectively bypassing KASLR.&lt;/p&gt; &#xA;&lt;h3&gt;Disable SELinux&lt;/h3&gt; &#xA;&lt;p&gt;The exploit disables SELinux also, with the kernel text base address, I just need to find the &lt;code&gt;selinux_state&lt;/code&gt; global structure location and then zero out the &lt;code&gt;.enforcing&lt;/code&gt; value.&lt;/p&gt; &#xA;&lt;h2&gt;Proof of Concept&lt;/h2&gt; &#xA;&lt;p&gt;The proof of concept accompanying the report was tested on Pixel 7 and 8 Pro devices running Android 14 with the October and November ASBs, achieving a success rate of nearly 100%. It&#39;s also important to mention that the exploit will not work out of the box in other devices due to the use of some hardcoded offsets. In order to add support for a new device, one must have to provide the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;kthreadd_task&lt;/code&gt; offset from the kernel base address.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;selinux_state&lt;/code&gt; offset from the kernel base address.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;task_struct-&amp;gt;cred&lt;/code&gt; , &lt;code&gt;task_struct-&amp;gt;pid&lt;/code&gt; and &lt;code&gt;task_struct-&amp;gt;tasks&lt;/code&gt; structure offsets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;anon_pipe_buf_ops&lt;/code&gt; offset from the kernel base address.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compilation&lt;/h3&gt; &#xA;&lt;p&gt;To compile the exploit as a standalone binary use the following command, then use &lt;code&gt;adb shell&lt;/code&gt; to run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ aarch64-linux-androidXX-clang++ -static-libstdc++ -w -Wno-c++11-narrowing -DUSE_STANDALONE -o poc poc.cpp -llog&#xA;$ adb push poc /data/local/tmp/&#xA;$ adb shell /data/local/tmp/poc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also run the exploit via an Android Studio App by embeding this directory with it and make sure to disable the useless C++ warnings by adding &lt;code&gt;-w -Wno-c++11-narrowing&lt;/code&gt; to the cmake file.&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ adb logcat  |grep -i EXPLOIT&#xA;11-28 16:04:12.500  7989  7989 E EXPLOIT : [+] Target device: &#39;google/husky/husky:14/UD1A.231105.004/11010374:user/release-keys&#39; 0xa9027bfdd10203ff 0xa90467faa9036ffc&#xA;11-28 16:04:15.563  7989  7989 E EXPLOIT : [+] Got the kcpu_id (0) kernel address = 0xffffff8901390000  from context (0x0)&#xA;11-28 16:04:18.441  7989  7989 E EXPLOIT : [+] Got the kcpu_id (255) kernel address = 0xffffff89b0bf8000  from context (0xff)&#xA;11-28 16:04:18.442  7989  7989 E EXPLOIT : [+] Found corrupted pipe with size 0xfff&#xA;11-28 16:04:18.442  7989  7989 E EXPLOIT : [+] SUCCESS! we have a fake pipe_buffer (0)!&#xA;11-28 16:04:18.444  7989  7989 E EXPLOIT : 10 00 39 01 89 FF FF FF  10 00 39 01 89 FF FF FF  | ..9.......9.....&#xA;11-28 16:04:18.444  7989  7989 E EXPLOIT : 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  | ................&#xA;11-28 16:04:18.444  7989  7989 E EXPLOIT : 00 B0 CD 12 C0 FF FF FF  00 00 00 00 00 00 00 00  | ................&#xA;11-28 16:04:18.444  7989  7989 E EXPLOIT : 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  | ................&#xA;11-28 16:04:18.445  7989  7989 E EXPLOIT : [+] Freeing kcpu_id = 0 (0xffffff8901390000)&#xA;11-28 16:04:18.446  7989  7989 E EXPLOIT : [+] Allocating 61 pipes with 256 slots&#xA;11-28 16:04:18.462  7989  7989 E EXPLOIT : [+] Successfully overlapped the kcpuqueue object with a pipe buffer&#xA;11-28 16:04:18.463  7989  7989 E EXPLOIT : 40 AB BA 26 FE FF FF FF  00 00 00 00 30 00 00 00  | @..&amp;amp;........0...&#xA;11-28 16:04:18.463  7989  7989 E EXPLOIT : 70 37 8D F1 DA FF FF FF  10 00 00 00 00 00 00 00  | p7..............&#xA;11-28 16:04:18.463  7989  7989 E EXPLOIT : 00 00 00 00 00 00 00 00                           | ........&#xA;11-28 16:04:18.463  7989  7989 E EXPLOIT : [+] pipe_buffer {.page = 0xfffffffe26baab40, .offset = 0x0, .len = 0x30, ops = 0xffffffdaf18d3770}&#xA;11-28 16:04:18.463  7989  7989 E EXPLOIT : [+] kernel base = 0xffffffdaf0010000, kthreadd_task = 0xffffff8002da3780 selinux_state = 0xffffffdaf28a3168&#xA;11-28 16:04:20.097  7989  7989 E EXPLOIT : [+] Found our own task struct 0xffffff88416c5c80&#xA;11-28 16:04:20.097  7989  7989 E EXPLOIT : [+] Successfully got root: getuid() = 0 getgid() = 0&#xA;11-28 16:04:20.097  7989  7989 E EXPLOIT : [+] Successfully disabled SELinux&#xA;11-28 16:04:20.102  7989  7989 E EXPLOIT : [+] Cleanup  ... OK&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ollama/ollama</title>
    <updated>2024-03-17T01:27:03Z</updated>
    <id>tag:github.com,2024-03-17:/ollama/ollama</id>
    <link href="https://github.com/ollama/ollama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Get up and running with Llama 2, Mistral, Gemma, and other large language models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;ollama&#34; height=&#34;200px&#34; src=&#34;https://github.com/jmorganca/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Ollama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ollama&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/ollama?style=flat&amp;amp;compact=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Get up and running with large language models locally.&lt;/p&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ollama.com/download/Ollama-darwin.zip&#34;&gt;Download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Windows preview&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ollama.com/download/OllamaSetup.exe&#34;&gt;Download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -fsSL https://ollama.com/install.sh | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jmorganca/ollama/raw/main/docs/linux.md&#34;&gt;Manual install instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;The official &lt;a href=&#34;https://hub.docker.com/r/ollama/ollama&#34;&gt;Ollama Docker image&lt;/a&gt; &lt;code&gt;ollama/ollama&lt;/code&gt; is available on Docker Hub.&lt;/p&gt; &#xA;&lt;h3&gt;Libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama-python&#34;&gt;ollama-python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama-js&#34;&gt;ollama-js&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To run and chat with &lt;a href=&#34;https://ollama.com/library/llama2&#34;&gt;Llama 2&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model library&lt;/h2&gt; &#xA;&lt;p&gt;Ollama supports a list of models available on &lt;a href=&#34;https://ollama.com/library&#34; title=&#34;ollama model library&#34;&gt;ollama.com/library&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here are some example models that can be downloaded:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dolphin Phi&lt;/td&gt; &#xA;   &lt;td&gt;2.7B&lt;/td&gt; &#xA;   &lt;td&gt;1.6GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run dolphin-phi&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-2&lt;/td&gt; &#xA;   &lt;td&gt;2.7B&lt;/td&gt; &#xA;   &lt;td&gt;1.7GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run phi&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Chat&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run neural-chat&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Starling&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.1GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run starling-lm&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 Uncensored&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama2-uncensored&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 13B&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama2:13b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 70B&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;39GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llama2:70b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orca Mini&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;1.9GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run orca-mini&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;3.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.5GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run llava&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;2B&lt;/td&gt; &#xA;   &lt;td&gt;1.4GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run gemma:2b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;4.8GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ollama run gemma:7b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Customize a model&lt;/h2&gt; &#xA;&lt;h3&gt;Import from GGUF&lt;/h3&gt; &#xA;&lt;p&gt;Ollama supports importing GGUF models in the Modelfile:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile&lt;/code&gt;, with a &lt;code&gt;FROM&lt;/code&gt; instruction with the local filepath to the model you want to import.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./vicuna-33b.Q4_0.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create the model in Ollama&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama create example -f Modelfile&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the model&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run example&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Import from PyTorch or Safetensors&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md&#34;&gt;guide&lt;/a&gt; on importing models for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Customize a prompt&lt;/h3&gt; &#xA;&lt;p&gt;Models from the Ollama library can be customized with a prompt. For example, to customize the &lt;code&gt;llama2&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM llama2&#xA;&#xA;# set the temperature to 1 [higher is more creative, lower is more coherent]&#xA;PARAMETER temperature 1&#xA;&#xA;# set the system message&#xA;SYSTEM &#34;&#34;&#34;&#xA;You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, create and run the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile&#xA;ollama run mario&#xA;&amp;gt;&amp;gt;&amp;gt; hi&#xA;Hello! It&#39;s your friend Mario.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/examples&#34;&gt;examples&lt;/a&gt; directory. For more information on working with a Modelfile, see the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md&#34;&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h2&gt;CLI Reference&lt;/h2&gt; &#xA;&lt;h3&gt;Create a model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ollama create&lt;/code&gt; is used to create a model from a Modelfile.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama create mymodel -f ./Modelfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama pull llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This command can also be used to update a local model. Only the diff will be pulled.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Remove a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama rm llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Copy a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama cp llama2 my-llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multiline input&lt;/h3&gt; &#xA;&lt;p&gt;For multiline input, you can wrap text with &lt;code&gt;&#34;&#34;&#34;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &#34;&#34;&#34;Hello,&#xA;... world!&#xA;... &#34;&#34;&#34;&#xA;I&#39;m a basic program that prints the famous &#34;Hello, world!&#34; message to the console.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multimodal models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; What&#39;s in this image? /Users/jmorgan/Desktop/smile.png&#xA;The image features a yellow smiley face, which is likely the central focus of the picture.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pass in prompt as arguments&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ollama run llama2 &#34;Summarize this file: $(cat README.md)&#34;&#xA; Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List models on your computer&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start Ollama&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;ollama serve&lt;/code&gt; is used when you want to start ollama without running the desktop application.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;code&gt;cmake&lt;/code&gt; and &lt;code&gt;go&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install cmake go&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then generate dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go generate ./...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then build the binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;go build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More detailed instructions can be found in the &lt;a href=&#34;https://github.com/jmorganca/ollama/raw/main/docs/development.md&#34;&gt;developer guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running local builds&lt;/h3&gt; &#xA;&lt;p&gt;Next, start the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, in a separate shell, run a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ollama run llama2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;REST API&lt;/h2&gt; &#xA;&lt;p&gt;Ollama has a REST API for running and managing models.&lt;/p&gt; &#xA;&lt;h3&gt;Generate a response&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:11434/api/generate -d &#39;{&#xA;  &#34;model&#34;: &#34;llama2&#34;,&#xA;  &#34;prompt&#34;:&#34;Why is the sky blue?&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat with a model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:11434/api/chat -d &#39;{&#xA;  &#34;model&#34;: &#34;mistral&#34;,&#xA;  &#34;messages&#34;: [&#xA;    { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;why is the sky blue?&#34; }&#xA;  ]&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md&#34;&gt;API documentation&lt;/a&gt; for all endpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Community Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;Web &amp;amp; Desktop&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bionic-gpt/bionic-gpt&#34;&gt;Bionic GPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AugustDev/enchanted&#34;&gt;Enchanted (macOS native)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rtcfirefly/ollama-ui&#34;&gt;HTML UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ivanfioravanti/chatbot-ollama&#34;&gt;Chatbot UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file&#34;&gt;Typescript UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/richawo/minimal-llm-ui&#34;&gt;Minimalistic React UI for Ollama Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;Open WebUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kevinhermawan/Ollamac&#34;&gt;Ollamac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enricoros/big-AGI/raw/main/docs/config-local-ollama.md&#34;&gt;big-AGI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cheshire-cat-ai/core&#34;&gt;Cheshire Cat assistant framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/semperai/amica&#34;&gt;Amica&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BruceMacD/chatd&#34;&gt;chatd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kghandour/Ollama-SwiftUI&#34;&gt;Ollama-SwiftUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mindmac.app&#34;&gt;MindMac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jakobhoeg/nextjs-ollama-llm-ui&#34;&gt;NextJS Web Interface for Ollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://msty.app&#34;&gt;Msty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Bin-Huang/Chatbox&#34;&gt;Chatbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tgraupmann/WinForm_Ollama_Copilot&#34;&gt;WinForm Ollama Copilot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&#34;&gt;NextChat&lt;/a&gt; with &lt;a href=&#34;https://docs.nextchat.dev/models/ollama&#34;&gt;Get Started Doc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leonid20000/OdinRunes&#34;&gt;Odin Runes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdjohnson/llm-x&#34;&gt;LLM-X: Progressive Web App&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Terminal&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggozad/oterm&#34;&gt;oterm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/s-kostyaev/ellama&#34;&gt;Ellama Emacs client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zweifisch/ollama&#34;&gt;Emacs client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/David-Kunz/gen.nvim&#34;&gt;gen.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nomnivore/ollama.nvim&#34;&gt;ollama.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gerazov/ollama-chat.nvim&#34;&gt;ollama-chat.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huynle/ogpt.nvim&#34;&gt;ogpt.nvim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karthink/gptel&#34;&gt;gptel Emacs client&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dustinblackman/oatmeal&#34;&gt;Oatmeal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pgibler/cmdh&#34;&gt;cmdh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pythops/tenere&#34;&gt;tenere&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/taketwo/llm-ollama&#34;&gt;llm-ollama&lt;/a&gt; for &lt;a href=&#34;https://llm.datasette.io/en/stable/&#34;&gt;Datasette&#39;s LLM CLI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/djcopley/ShellOracle&#34;&gt;ShellOracle&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Database&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mindsdb/mindsdb/raw/staging/mindsdb/integrations/handlers/ollama_handler/README.md&#34;&gt;MindsDB&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Package managers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://archlinux.org/packages/extra/x86_64/ollama/&#34;&gt;Pacman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://artifacthub.io/packages/helm/ollama-helm/ollama&#34;&gt;Helm Chart&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/integrations/llms/ollama&#34;&gt;LangChain&lt;/a&gt; and &lt;a href=&#34;https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ollama&#34;&gt;LangChain.js&lt;/a&gt; with &lt;a href=&#34;https://js.langchain.com/docs/use_cases/question_answering/local_retrieval_qa&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tmc/langchaingo/&#34;&gt;LangChainGo&lt;/a&gt; with &lt;a href=&#34;https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain4j/langchain4j&#34;&gt;LangChain4j&lt;/a&gt; with &lt;a href=&#34;https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gpt-index.readthedocs.io/en/stable/examples/llm/ollama.html&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain4j/langchain4j/tree/main/langchain4j-ollama&#34;&gt;LangChain4j&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awaescher/OllamaSharp&#34;&gt;OllamaSharp for .NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gbaptista/ollama-ai&#34;&gt;Ollama for Ruby&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pepperoni21/ollama-rs&#34;&gt;Ollama-rs for Rust&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/amithkoujalgi/ollama4j&#34;&gt;Ollama4j for Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelfusion.dev/integration/model-provider/ollama&#34;&gt;ModelFusion Typescript Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kevinhermawan/OllamaKit&#34;&gt;OllamaKit for Swift&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/breitburg/dart-ollama&#34;&gt;Ollama for Dart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cloudstudio/ollama-laravel&#34;&gt;Ollama for Laravel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/davidmigloz/langchain_dart&#34;&gt;LangChainDart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama&#34;&gt;Semantic Kernel - Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepset-ai/haystack-integrations/raw/main/integrations/ollama.md&#34;&gt;Haystack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brainlid/langchain&#34;&gt;Elixir LangChain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JBGruber/rollama&#34;&gt;Ollama for R - rollama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lebrunel/ollama-ex&#34;&gt;Ollama-ex for Elixir&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/b-tocs/abap_btocs_ollama&#34;&gt;Ollama Connector for SAP ABAP&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Mobile&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AugustDev/enchanted&#34;&gt;Enchanted&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mobile-Artificial-Intelligence/maid&#34;&gt;Maid&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extensions &amp;amp; Plugins&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MassimilianoPasquini97/raycast_ollama&#34;&gt;Raycast extension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mxyng/discollama&#34;&gt;Discollama&lt;/a&gt; (Discord bot inside the Ollama discord channel)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/continuedev/continue&#34;&gt;Continue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hinterdupfinger/obsidian-ollama&#34;&gt;Obsidian Ollama plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omagdy7/ollama-logseq&#34;&gt;Logseq Ollama plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andersrex/notesollama&#34;&gt;NotesOllama&lt;/a&gt; (Apple Notes Ollama plugin)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/samalba/dagger-chatbot&#34;&gt;Dagger Chatbot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mekb-turtle/discord-ai-bot&#34;&gt;Discord AI Bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ruecat/ollama-telegram&#34;&gt;Ollama Telegram Bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ej52/hass-ollama-conversation&#34;&gt;Hass Ollama Conversation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abrenneke/rivet-plugin-ollama&#34;&gt;Rivet plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ex3ndr/llama-coder&#34;&gt;Llama Coder&lt;/a&gt; (Copilot alternative using Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/longy2k/obsidian-bmo-chatbot&#34;&gt;Obsidian BMO Chatbot plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logancyang/obsidian-copilot&#34;&gt;Copilot for Obsidian plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pfrankov/obsidian-local-gpt&#34;&gt;Obsidian Local GPT plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openinterpreter.com/language-model-setup/local-models/ollama&#34;&gt;Open Interpreter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rjmacarthy/twinny&#34;&gt;twinny&lt;/a&gt; (Copilot and Copilot chat alternative using Ollama)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RussellCanfield/wingman-ai&#34;&gt;Wingman-AI&lt;/a&gt; (Copilot code and chat alternative using Ollama and HuggingFace)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/n4ze3m/page-assist&#34;&gt;Page Assist&lt;/a&gt; (Chrome Extension)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>