<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-14T01:32:51Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>notepad-plus-plus/notepad-plus-plus</title>
    <updated>2022-10-14T01:32:51Z</updated>
    <id>tag:github.com,2022-10-14:/notepad-plus-plus/notepad-plus-plus</id>
    <link href="https://github.com/notepad-plus-plus/notepad-plus-plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notepad++ official repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;What is Notepad++ ?&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/notepad-plus-plus/notepad-plus-plus.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://ci.appveyor.com/project/donho/notepad-plus-plus&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/github/notepad-plus-plus/notepad-plus-plus?branch=master&amp;amp;svg=true&#34; alt=&#34;Appveyor build status&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://community.notepad-plus-plus.org/&#34;&gt;&lt;img src=&#34;https://notepad-plus-plus.org/assets/images/NppCommunityBadge.svg?sanitize=true&#34; alt=&#34;Join the disscussions at https://community.notepad-plus-plus.org/&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notepad++ is a free (free as in both &#34;free speech&#34; and &#34;free beer&#34;) source code editor and Notepad replacement that supports several programming languages and natural languages. Running in the MS Windows environment, its use is governed by &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/LICENSE&#34;&gt;GPL License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://notepad-plus-plus.org/&#34;&gt;Notepad++ official site&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Notepad++ Release Key&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Since the release of version 7.6.5 Notepad++ is signed using GPG with the following key:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Signer:&lt;/strong&gt; Notepad++&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;E-mail:&lt;/strong&gt; &lt;a href=&#34;mailto:don.h@free.fr&#34;&gt;don.h@free.fr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key ID:&lt;/strong&gt; 0x8D84F46E&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key fingerprint:&lt;/strong&gt; 14BC E436 2749 B2B5 1F8C 7122 6C42 9F1D 8D84 F46E&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key type:&lt;/strong&gt; RSA 4096/4096&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Created:&lt;/strong&gt; 2019-03-11&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Expires:&lt;/strong&gt; 2024-03-11&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/notepad-plus-plus/notepad-plus-plus/raw/master/nppGpgPub.asc&#34;&gt;https://github.com/notepad-plus-plus/notepad-plus-plus/blob/master/nppGpgPub.asc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported OS&lt;/h2&gt; &#xA;&lt;p&gt;All the Windows systems still supported by Microsoft are supported by Notepad++. However, not all Notepad++ users can or want to use the newest system. Here is the &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/SUPPORTED_SYSTEM.md&#34;&gt;Supported systems information&lt;/a&gt; you may need in case you are one of them.&lt;/p&gt; &#xA;&lt;h2&gt;Build Notepad++&lt;/h2&gt; &#xA;&lt;p&gt;Please follow &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/BUILD.md&#34;&gt;build guide&lt;/a&gt; to build Notepad++ from source.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome. Be mindful of our &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/CONTRIBUTING.md&#34;&gt;Contribution Rules&lt;/a&gt; to increase the likelihood of your contribution getting accepted.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/notepad-plus-plus/notepad-plus-plus/graphs/contributors&#34;&gt;Notepad++ Contributors&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/tiny-cuda-nn</title>
    <updated>2022-10-14T01:32:51Z</updated>
    <id>tag:github.com,2022-10-14:/NVlabs/tiny-cuda-nn</id>
    <link href="https://github.com/NVlabs/tiny-cuda-nn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lightning fast C++/CUDA neural network framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tiny CUDA Neural Networks &lt;img src=&#34;https://github.com/NVlabs/tiny-cuda-nn/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is a small, self-contained framework for training and querying neural networks. Most notably, it contains a lightning fast &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/fully-fused-mlp-diagram.png&#34;&gt;&#34;fully fused&#34; multi-layer perceptron&lt;/a&gt; (&lt;a href=&#34;https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf&#34;&gt;technical paper&lt;/a&gt;), a versatile &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/multiresolution-hash-encoding-diagram.png&#34;&gt;multiresolution hash encoding&lt;/a&gt; (&lt;a href=&#34;https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf&#34;&gt;technical paper&lt;/a&gt;), as well as support for various other input encodings, losses, and optimizers.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/fully-fused-vs-tensorflow.png&#34; alt=&#34;Image&#34;&gt; &lt;em&gt;Fully fused networks vs. TensorFlow v2.5.0 w/ XLA. Measured on 64 (solid line) and 128 (dashed line) neurons wide multi-layer perceptrons on an RTX 3090. Generated by &lt;code&gt;benchmarks/bench_ours.cu&lt;/code&gt; and &lt;code&gt;benchmarks/bench_tensorflow.py&lt;/code&gt; using &lt;code&gt;data/config_oneblob.json&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Tiny CUDA neural networks have a simple C++/CUDA API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;tiny-cuda-nn/common.h&amp;gt;&#xA;&#xA;// Configure the model&#xA;nlohmann::json config = {&#xA;&#x9;{&#34;loss&#34;, {&#xA;&#x9;&#x9;{&#34;otype&#34;, &#34;L2&#34;}&#xA;&#x9;}},&#xA;&#x9;{&#34;optimizer&#34;, {&#xA;&#x9;&#x9;{&#34;otype&#34;, &#34;Adam&#34;},&#xA;&#x9;&#x9;{&#34;learning_rate&#34;, 1e-3},&#xA;&#x9;}},&#xA;&#x9;{&#34;encoding&#34;, {&#xA;&#x9;&#x9;{&#34;otype&#34;, &#34;HashGrid&#34;},&#xA;&#x9;&#x9;{&#34;n_levels&#34;, 16},&#xA;&#x9;&#x9;{&#34;n_features_per_level&#34;, 2},&#xA;&#x9;&#x9;{&#34;log2_hashmap_size&#34;, 19},&#xA;&#x9;&#x9;{&#34;base_resolution&#34;, 16},&#xA;&#x9;&#x9;{&#34;per_level_scale&#34;, 2.0},&#xA;&#x9;}},&#xA;&#x9;{&#34;network&#34;, {&#xA;&#x9;&#x9;{&#34;otype&#34;, &#34;FullyFusedMLP&#34;},&#xA;&#x9;&#x9;{&#34;activation&#34;, &#34;ReLU&#34;},&#xA;&#x9;&#x9;{&#34;output_activation&#34;, &#34;None&#34;},&#xA;&#x9;&#x9;{&#34;n_neurons&#34;, 64},&#xA;&#x9;&#x9;{&#34;n_hidden_layers&#34;, 2},&#xA;&#x9;}},&#xA;};&#xA;&#xA;using namespace tcnn;&#xA;&#xA;auto model = create_from_config(n_input_dims, n_output_dims, config);&#xA;&#xA;// Train the model&#xA;GPUMatrix&amp;lt;float&amp;gt; training_batch_inputs(n_input_dims, batch_size);&#xA;GPUMatrix&amp;lt;float&amp;gt; training_batch_targets(n_output_dims, batch_size);&#xA;&#xA;for (int i = 0; i &amp;lt; n_training_steps; ++i) {&#xA;&#x9;generate_training_batch(&amp;amp;training_batch_inputs, &amp;amp;training_batch_targets); // &amp;lt;-- your code&#xA;&#xA;&#x9;float loss;&#xA;&#x9;model.trainer-&amp;gt;training_step(training_batch_inputs, training_batch_targets, &amp;amp;loss);&#xA;&#x9;std::cout &amp;lt;&amp;lt; &#34;iteration=&#34; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &#34; loss=&#34; &amp;lt;&amp;lt; loss &amp;lt;&amp;lt; std::endl;&#xA;}&#xA;&#xA;// Use the model&#xA;GPUMatrix&amp;lt;float&amp;gt; inference_inputs(n_input_dims, batch_size);&#xA;generate_inputs(&amp;amp;inference_inputs); // &amp;lt;-- your code&#xA;&#xA;GPUMatrix&amp;lt;float&amp;gt; inference_outputs(n_output_dims, batch_size);&#xA;model.network-&amp;gt;inference(inference_inputs, inference_outputs);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example: learning a 2D image&lt;/h2&gt; &#xA;&lt;p&gt;We provide a sample application where an image function &lt;em&gt;(x,y) -&amp;gt; (R,G,B)&lt;/em&gt; is learned. It can be run via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tiny-cuda-nn/build$ ./mlp_learning_an_image ../data/images/albert.jpg ../data/config_hash.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;producing an image every 1000 training steps. Each 1000 steps should take roughly 0.42 seconds with the default configuration on an RTX 3090.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;10 steps (4.2 ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;100 steps (42 ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1000 steps (420 ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reference image&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/10.jpg&#34; alt=&#34;10steps&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/100.jpg&#34; alt=&#34;100steps&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/1000.jpg&#34; alt=&#34;1000steps&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/images/albert.jpg&#34; alt=&#34;reference&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An &lt;strong&gt;NVIDIA GPU&lt;/strong&gt;; tensor cores increase performance when available. All shown results come from an RTX 3090.&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;C++14&lt;/strong&gt; capable compiler. The following choices are recommended and have been tested: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Visual Studio 2019&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; GCC/G++ 7.5 or higher&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt; v10.2 or higher&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; v3.21 or higher&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The fully fused MLP component of this framework requires a &lt;strong&gt;very large&lt;/strong&gt; amount of shared memory in its default configuration. It will likely only work on an RTX 3090, an RTX 2080 Ti, or high-end enterprise GPUs. Lower end cards must reduce the &lt;code&gt;n_neurons&lt;/code&gt; parameter or use the &lt;code&gt;CutlassMLP&lt;/code&gt; (better compatibility but slower) instead.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are using Linux, install the following packages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get install build-essential git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also recommend installing &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt; in &lt;code&gt;/usr/local/&lt;/code&gt; and adding the CUDA installation to your PATH. For example, if you have CUDA 11.4, add the following to your &lt;code&gt;~/.bashrc&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export PATH=&#34;/usr/local/cuda-11.4/bin:$PATH&#34;&#xA;export LD_LIBRARY_PATH=&#34;/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compilation (Windows &amp;amp; Linux)&lt;/h2&gt; &#xA;&lt;p&gt;Begin by cloning this repository and all its submodules using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone --recursive https://github.com/nvlabs/tiny-cuda-nn&#xA;$ cd tiny-cuda-nn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, use CMake to build the project: (on Windows, this must be in a &lt;a href=&#34;https://docs.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-160#developer_command_prompt&#34;&gt;developer command prompt&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tiny-cuda-nn$ cmake . -B build&#xA;tiny-cuda-nn$ cmake --build build --config RelWithDebInfo -j&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;PyTorch extension&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;tiny-cuda-nn&lt;/strong&gt; comes with a &lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch&lt;/a&gt; extension that allows using the fast MLPs and input encodings from within a &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; context. These bindings can be significantly faster than full Python implementations; in particular for the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/data/readme/multiresolution-hash-encoding-diagram.png&#34;&gt;multiresolution hash encoding&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The overheads of Python/PyTorch can nonetheless be extensive. For example, the bundled &lt;code&gt;mlp_learning_an_image&lt;/code&gt; example is &lt;strong&gt;~2x slower&lt;/strong&gt; through PyTorch than native CUDA.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Begin by setting up a Python 3.X environment with a recent, CUDA-enabled version of PyTorch. Then, invoke&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, if you would like to install from a local clone of &lt;strong&gt;tiny-cuda-nn&lt;/strong&gt;, invoke&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;tiny-cuda-nn$ cd bindings/torch&#xA;tiny-cuda-nn/bindings/torch$ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upon success, you can use &lt;strong&gt;tiny-cuda-nn&lt;/strong&gt; models as in the following example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import commentjson as json&#xA;import tinycudann as tcnn&#xA;import torch&#xA;&#xA;with open(&#34;data/config_hash.json&#34;) as f:&#xA;&#x9;config = json.load(f)&#xA;&#xA;# Option 1: efficient Encoding+Network combo.&#xA;model = tcnn.NetworkWithInputEncoding(&#xA;&#x9;n_input_dims, n_output_dims,&#xA;&#x9;config[&#34;encoding&#34;], config[&#34;network&#34;]&#xA;)&#xA;&#xA;# Option 2: separate modules. Slower but more flexible.&#xA;encoding = tcnn.Encoding(n_input_dims, config[&#34;encoding&#34;])&#xA;network = tcnn.Network(encoding.n_output_dims, n_output_dims, config[&#34;network&#34;])&#xA;model = torch.nn.Sequential(encoding, network)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;samples/mlp_learning_an_image_pytorch.py&lt;/code&gt; for an example.&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;Following is a summary of the components of this framework. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/tiny-cuda-nn/master/DOCUMENTATION.md&#34;&gt;The JSON documentation&lt;/a&gt; lists configuration options.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Networks&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fully fused MLP&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;src/fully_fused_mlp.cu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Lightning fast implementation of small multi-layer perceptrons (MLPs).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CUTLASS MLP&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;src/cutlass_mlp.cu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MLP based on &lt;a href=&#34;https://github.com/NVIDIA/cutlass&#34;&gt;CUTLASS&lt;/a&gt;&#39; GEMM routines. Slower than fully-fused, but handles larger networks and still is reasonably fast.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Input encodings&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Composite&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/composite.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Allows composing multiple encodings. Can be, for example, used to assemble the Neural Radiance Caching encoding &lt;a href=&#34;https://tom94.net/&#34;&gt;[Müller et al. 2021]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Frequency&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/frequency.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NeRF&#39;s &lt;a href=&#34;https://www.matthewtancik.com/nerf&#34;&gt;[Mildenhall et al. 2020]&lt;/a&gt; positional encoding applied equally to all dimensions.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Grid&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/grid.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Encoding based on trainable multiresolution grids. Used for &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;Instant Neural Graphics Primitives [Müller et al. 2022]&lt;/a&gt;. The grids can be backed by hashtables, dense storage, or tiled storage.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Identity&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/identity.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Leaves values untouched.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Oneblob&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/oneblob.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;From Neural Importance Sampling &lt;a href=&#34;https://tom94.net/data/publications/mueller18neural/mueller18neural-v4.pdf&#34;&gt;[Müller et al. 2019]&lt;/a&gt; and Neural Control Variates &lt;a href=&#34;https://tom94.net/data/publications/mueller20neural/mueller20neural.pdf&#34;&gt;[Müller et al. 2020]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SphericalHarmonics&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/spherical_harmonics.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;A frequency-space encoding that is more suitable to direction vectors than component-wise ones.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TriangleWave&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/encodings/triangle_wave.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Low-cost alternative to the NeRF&#39;s encoding. Used in Neural Radiance Caching &lt;a href=&#34;https://tom94.net/&#34;&gt;[Müller et al. 2021]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Losses&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;L1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/l1.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Standard L1 loss.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Relative L1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/l1.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Relative L1 loss normalized by the network prediction.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MAPE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/mape.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mean absolute percentage error (MAPE). The same as Relative L1, but normalized by the target.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SMAPE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/smape.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Symmetric mean absolute percentage error (SMAPE). The same as Relative L1, but normalized by the mean of the prediction and the target.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;L2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/l2.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Standard L2 loss.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Relative L2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/relative_l2.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Relative L2 loss normalized by the network prediction &lt;a href=&#34;https://github.com/NVlabs/noise2noise&#34;&gt;[Lehtinen et al. 2018]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Relative L2 Luminance&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/relative_l2_luminance.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Same as above, but normalized by the luminance of the network prediction. Only applicable when network prediction is RGB. Used in Neural Radiance Caching &lt;a href=&#34;https://tom94.net/&#34;&gt;[Müller et al. 2021]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Cross Entropy&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/cross_entropy.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Standard cross entropy loss. Only applicable when the network prediction is a PDF.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Variance&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/losses/variance_is.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Standard variance loss. Only applicable when the network prediction is a PDF.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Optimizers&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&amp;nbsp;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Adam&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/adam.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implementation of Adam &lt;a href=&#34;https://arxiv.org/abs/1412.6980&#34;&gt;[Kingma and Ba 2014]&lt;/a&gt;, generalized to AdaBound &lt;a href=&#34;https://github.com/Luolc/AdaBound&#34;&gt;[Luo et al. 2019]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Novograd&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/lookahead.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implementation of Novograd &lt;a href=&#34;https://arxiv.org/abs/1905.11286&#34;&gt;[Ginsburg et al. 2019]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SGD&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/sgd.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Standard stochastic gradient descent (SGD).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Shampoo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/shampoo.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implementation of the 2nd order Shampoo optimizer &lt;a href=&#34;https://arxiv.org/abs/1802.09568&#34;&gt;[Gupta et al. 2018]&lt;/a&gt; with home-grown optimizations as well as those by &lt;a href=&#34;https://arxiv.org/abs/2002.09018&#34;&gt;Anil et al. [2020]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Average&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/average.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Wraps another optimizer and computes a linear average of the weights over the last N iterations. The average is used for inference only (does not feed back into training).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Batched&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/batched.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Wraps another optimizer, invoking the nested optimizer once every N steps on the averaged gradient. Has the same effect as increasing the batch size but requires only a constant amount of memory.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EMA&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/average.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Wraps another optimizer and computes an exponential moving average of the weights. The average is used for inference only (does not feed back into training).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Exponential Decay&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/exponential_decay.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Wraps another optimizer and performs piecewise-constant exponential learning-rate decay.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Lookahead&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;include/tiny-cuda-nn/optimizers/lookahead.h&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Wraps another optimizer, implementing the lookahead algorithm &lt;a href=&#34;https://arxiv.org/abs/1907.08610&#34;&gt;[Zhang et al. 2019]&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License and Citation&lt;/h2&gt; &#xA;&lt;p&gt;This framework is licensed under the BSD 3-clause license. Please see &lt;code&gt;LICENSE.txt&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;If you use it in your research, we would appreciate a citation via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{tiny-cuda-nn,&#xA;&#x9;author = {M\&#34;uller, Thomas},&#xA;&#x9;license = {BSD-3-Clause},&#xA;&#x9;month = {4},&#xA;&#x9;title = {{tiny-cuda-nn}},&#xA;&#x9;url = {https://github.com/NVlabs/tiny-cuda-nn},&#xA;&#x9;version = {1.6},&#xA;&#x9;year = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;p&gt;This framework powers the following publications:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Instant Neural Graphics Primitives with a Multiresolution Hash Encoding&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://tom94.net&#34;&gt;Thomas Müller&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/alex-evans&#34;&gt;Alex Evans&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/christoph-schied&#34;&gt;Christoph Schied&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/alex-keller&#34;&gt;Alexander Keller&lt;/a&gt;&lt;br&gt; &lt;em&gt;ACM Transactions on Graphics (&lt;strong&gt;SIGGRAPH&lt;/strong&gt;), July 2022&lt;/em&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;Website&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf&#34;&gt;Paper&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;Code&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4&#34;&gt;Video&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.bib&#34;&gt;BibTeX&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Extracting Triangular 3D Models, Materials, and Lighting From Images&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://research.nvidia.com/person/jacob-munkberg&#34;&gt;Jacob Munkberg&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/jon-hasselgren&#34;&gt;Jon Hasselgren&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~shenti11/&#34;&gt;Tianchang Shen&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~wenzheng/&#34;&gt;Wenzheng Chen&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/alex-evans&#34;&gt;Alex Evans&lt;/a&gt;, &lt;a href=&#34;https://tom94.net&#34;&gt;Thomas Müller&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt;&lt;br&gt; &lt;strong&gt;CVPR (Oral)&lt;/strong&gt;, June 2022&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://nvlabs.github.io/nvdiffrec/&#34;&gt;Website&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://nvlabs.github.io/nvdiffrec/assets/paper.pdf&#34;&gt;Paper&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://nvlabs.github.io/nvdiffrec/assets/video.mp4&#34;&gt;Video&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://nvlabs.github.io/nvdiffrec/assets/bib.txt&#34;&gt;BibTeX&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Real-time Neural Radiance Caching for Path Tracing&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://tom94.net&#34;&gt;Thomas Müller&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/fabrice-rousselle&#34;&gt;Fabrice Rousselle&lt;/a&gt;, &lt;a href=&#34;http://jannovak.info&#34;&gt;Jan Novák&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/person/alex-keller&#34;&gt;Alexander Keller&lt;/a&gt;&lt;br&gt; &lt;em&gt;ACM Transactions on Graphics (&lt;strong&gt;SIGGRAPH&lt;/strong&gt;), August 2021&lt;/em&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://tom94.net/data/publications/mueller21realtime/mueller21realtime.pdf&#34;&gt;Paper&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://gtc21.event.nvidia.com/media/Fully%20Fused%20Neural%20Network%20for%20Radiance%20Caching%20in%20Real%20Time%20Rendering%20%5BE31307%5D/1_liqy6k1c&#34;&gt;GTC talk&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://tom94.net/data/publications/mueller21realtime/mueller21realtime.mp4&#34;&gt;Video&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://tom94.net/data/publications/mueller21realtime/interactive-viewer/&#34;&gt;Interactive results viewer&lt;/a&gt;&amp;nbsp;/ &lt;a href=&#34;https://tom94.net/data/publications/mueller21realtime/mueller21realtime.bib&#34;&gt;BibTeX&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Special thanks go to the NRC authors for helpful discussions and to &lt;a href=&#34;https://research.nvidia.com/person/nikolaus-binder&#34;&gt;Nikolaus Binder&lt;/a&gt; for providing part of the infrastructure of this framework, as well as for help with utilizing TensorCores from within CUDA.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>iovisor/bpftrace</title>
    <updated>2022-10-14T01:32:51Z</updated>
    <id>tag:github.com,2022-10-14:/iovisor/bpftrace</id>
    <link href="https://github.com/iovisor/bpftrace" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-level tracing language for Linux eBPF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bpftrace&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/iovisor/bpftrace/actions?query=workflow%3ACI+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/iovisor/bpftrace/workflows/CI/badge.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://webchat.oftc.net/?channels=bpftrace&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IRC-bpftrace-blue.svg?sanitize=true&#34; alt=&#34;IRC#bpftrace&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/iovisor/bpftrace/alerts/&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/alerts/g/iovisor/bpftrace.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Total alerts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;bpftrace is a high-level tracing language for Linux enhanced Berkeley Packet Filter (eBPF) available in recent Linux kernels (4.x). bpftrace uses LLVM as a backend to compile scripts to BPF-bytecode and makes use of &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;BCC&lt;/a&gt; for interacting with the Linux BPF system, as well as existing Linux tracing capabilities: kernel dynamic tracing (kprobes), user-level dynamic tracing (uprobes), and tracepoints. The bpftrace language is inspired by awk and C, and predecessor tracers such as DTrace and SystemTap. bpftrace was created by &lt;a href=&#34;https://github.com/ajor&#34;&gt;Alastair Robertson&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about bpftrace, see the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/man/adoc/bpftrace.adoc&#34;&gt;Manual&lt;/a&gt; the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/reference_guide.md&#34;&gt;Reference Guide&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/tutorial_one_liners.md&#34;&gt;One-Liner Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;One-Liners&lt;/h2&gt; &#xA;&lt;p&gt;The following one-liners demonstrate different capabilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Files opened by process&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_enter_open { printf(&#34;%s %s\n&#34;, comm, str(args-&amp;gt;filename)); }&#39;&#xA;&#xA;# Syscall count by program&#xA;bpftrace -e &#39;tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }&#39;&#xA;&#xA;# Read bytes by process:&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_exit_read /args-&amp;gt;ret/ { @[comm] = sum(args-&amp;gt;ret); }&#39;&#xA;&#xA;# Read size distribution by process:&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_exit_read { @[comm] = hist(args-&amp;gt;ret); }&#39;&#xA;&#xA;# Show per-second syscall rates:&#xA;bpftrace -e &#39;tracepoint:raw_syscalls:sys_enter { @ = count(); } interval:s:1 { print(@); clear(@); }&#39;&#xA;&#xA;# Trace disk size by process&#xA;bpftrace -e &#39;tracepoint:block:block_rq_issue { printf(&#34;%d %s %d\n&#34;, pid, comm, args-&amp;gt;bytes); }&#39;&#xA;&#xA;# Count page faults by process&#xA;bpftrace -e &#39;software:faults:1 { @[comm] = count(); }&#39;&#xA;&#xA;# Count LLC cache misses by process name and PID (uses PMCs):&#xA;bpftrace -e &#39;hardware:cache-misses:1000000 { @[comm, pid] = count(); }&#39;&#xA;&#xA;# Profile user-level stacks at 99 Hertz, for PID 189:&#xA;bpftrace -e &#39;profile:hz:99 /pid == 189/ { @[ustack] = count(); }&#39;&#xA;&#xA;# Files opened, for processes in the root cgroup-v2&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_enter_openat /cgroup == cgroupid(&#34;/sys/fs/cgroup/unified/mycg&#34;)/ { printf(&#34;%s\n&#34;, str(args-&amp;gt;filename)); }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More powerful scripts can easily be constructed. See &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools&#34;&gt;Tools&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;For build and install instructions, see &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;bpftrace contains various tools, which also serve as examples of programming in the bpftrace language.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bashreadline.bt&#34;&gt;bashreadline.bt&lt;/a&gt;: Print entered bash commands system wide. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bashreadline_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biolatency.bt&#34;&gt;biolatency.bt&lt;/a&gt;: Block I/O latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biolatency_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biosnoop.bt&#34;&gt;biosnoop.bt&lt;/a&gt;: Block I/O tracing tool, showing per I/O latency. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biosnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biostacks.bt&#34;&gt;biostacks.bt&lt;/a&gt;: Show disk I/O latency with initialization stacks. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biostacks_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bitesize.bt&#34;&gt;bitesize.bt&lt;/a&gt;: Show disk I/O size as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bitesize_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/capable.bt&#34;&gt;capable.bt&lt;/a&gt;: Trace security capability checks. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/capable_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/cpuwalk.bt&#34;&gt;cpuwalk.bt&lt;/a&gt;: Sample which CPUs are executing processes. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/cpuwalk_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/dcsnoop.bt&#34;&gt;dcsnoop.bt&lt;/a&gt;: Trace directory entry cache (dcache) lookups. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/dcsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/execsnoop.bt&#34;&gt;execsnoop.bt&lt;/a&gt;: Trace new processes via exec() syscalls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/execsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/gethostlatency.bt&#34;&gt;gethostlatency.bt&lt;/a&gt;: Show latency for getaddrinfo/gethostbyname[2] calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/gethostlatency_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/killsnoop.bt&#34;&gt;killsnoop.bt&lt;/a&gt;: Trace signals issued by the kill() syscall. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/killsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/loads.bt&#34;&gt;loads.bt&lt;/a&gt;: Print load averages. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/loads_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/mdflush.bt&#34;&gt;mdflush.bt&lt;/a&gt;: Trace md flush events. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/mdflush_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/naptime.bt&#34;&gt;naptime.bt&lt;/a&gt;: Show voluntary sleep calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/naptime_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/opensnoop.bt&#34;&gt;opensnoop.bt&lt;/a&gt;: Trace open() syscalls showing filenames. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/opensnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/oomkill.bt&#34;&gt;oomkill.bt&lt;/a&gt;: Trace OOM killer. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/oomkill_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/pidpersec.bt&#34;&gt;pidpersec.bt&lt;/a&gt;: Count new processes (via fork). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/pidpersec_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlat.bt&#34;&gt;runqlat.bt&lt;/a&gt;: CPU scheduler run queue latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlat_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlen.bt&#34;&gt;runqlen.bt&lt;/a&gt;: CPU scheduler run queue length as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlen_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/setuids.bt&#34;&gt;setuids.bt&lt;/a&gt;: Trace the setuid syscalls: privilege escalation. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/setuids_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/ssllatency.bt&#34;&gt;ssllatency.bt&lt;/a&gt;: Summarize SSL/TLS handshake latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/ssllatency_example.txt&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/sslsnoop.bt&#34;&gt;sslsnoop.bt&lt;/a&gt;: Trace SSL/TLS handshake, showing latency and return value. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/sslsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/statsnoop.bt&#34;&gt;statsnoop.bt&lt;/a&gt;: Trace stat() syscalls for general debugging. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/statsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/swapin.bt&#34;&gt;swapin.bt&lt;/a&gt;: Show swapins by process. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/swapin_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syncsnoop.bt&#34;&gt;syncsnoop.bt&lt;/a&gt;: Trace sync() variety of syscalls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syncsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syscount.bt&#34;&gt;syscount.bt&lt;/a&gt;: Count system calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syscount_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpaccept.bt&#34;&gt;tcpaccept.bt&lt;/a&gt;: Trace TCP passive connections (accept()). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpaccept_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpconnect.bt&#34;&gt;tcpconnect.bt&lt;/a&gt;: Trace TCP active connections (connect()). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpconnect_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpdrop.bt&#34;&gt;tcpdrop.bt&lt;/a&gt;: Trace kernel-based TCP packet drops with details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpdrop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcplife.bt&#34;&gt;tcplife.bt&lt;/a&gt;: Trace TCP session lifespans with connection details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcplife_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpretrans.bt&#34;&gt;tcpretrans.bt&lt;/a&gt;: Trace TCP retransmits. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpretrans_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpsynbl.bt&#34;&gt;tcpsynbl.bt&lt;/a&gt;: Show TCP SYN backlog as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpsynbl_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/threadsnoop.bt&#34;&gt;threadsnoop.bt&lt;/a&gt;: List new thread creation. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/threadsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/undump.bt&#34;&gt;undump.bt&lt;/a&gt;: Capture UNIX domain socket packages. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/undump_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfscount.bt&#34;&gt;vfscount.bt&lt;/a&gt;: Count VFS calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfscount_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfsstat.bt&#34;&gt;vfsstat.bt&lt;/a&gt;: Count some VFS calls, with per-second summaries. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfsstat_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/writeback.bt&#34;&gt;writeback.bt&lt;/a&gt;: Trace file system writeback events with details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/writeback_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/xfsdist.bt&#34;&gt;xfsdist.bt&lt;/a&gt;: Summarize XFS operation latency distribution as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/xfsdist_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more eBPF observability tools, see &lt;a href=&#34;https://github.com/iovisor/bcc#tools&#34;&gt;bcc tools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Probe types&lt;/h2&gt; &#xA;&lt;center&gt;&#xA; &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/images/bpftrace_probes_2018.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/images/bpftrace_probes_2018.png&#34; border=&#34;0&#34; width=&#34;700&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/reference_guide.md&#34;&gt;Reference Guide&lt;/a&gt; for more detail.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;For additional help / discussion, please use our &lt;a href=&#34;https://github.com/iovisor/bpftrace/discussions&#34;&gt;discussions&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Have ideas for new bpftrace tools? &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/CONTRIBUTING-TOOLS.md&#34;&gt;CONTRIBUTING-TOOLS.md&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bugs reports and feature requests: &lt;a href=&#34;https://github.com/iovisor/bpftrace/issues&#34;&gt;Issue Tracker&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;bpftrace development IRC: #bpftrace at irc.oftc.net&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;For build &amp;amp; test directly in docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For build in docker then test directly on host&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build-static.sh&#xA;$ ./build-static/src/bpftrace&#xA;$ ./build-static/tests/bpftrace_test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vagrant&lt;/h3&gt; &#xA;&lt;p&gt;For development and testing a &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/Vagrantfile&#34;&gt;Vagrantfile&lt;/a&gt; is available.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have the &lt;code&gt;vbguest&lt;/code&gt; plugin installed, it is required to correctly install the shared file system driver on the ubuntu boxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant plugin install vagrant-vbguest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start VM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant status&#xA;$ vagrant up $YOUR_CHOICE&#xA;$ vagrant ssh $YOUR_CHOICE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2019 Alastair Robertson&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
</feed>