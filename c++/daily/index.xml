<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-04T01:29:58Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ikawrakow/ik_llama.cpp</title>
    <updated>2025-05-04T01:29:58Z</updated>
    <id>tag:github.com,2025-05-04:/ikawrakow/ik_llama.cpp</id>
    <link href="https://github.com/ikawrakow/ik_llama.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;llama.cpp fork with additional SOTA quants and improved performance&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ik_llama.cpp: llama.cpp fork with better CPU performance&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a fork of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; with better CPU and hybrid GPU/CPU performance, new SOTA quantization types, first-class Bitnet support, better DeepSeek performance via MLA, FlashMLA, fused MoE operations and tensor overrides for hybrid GPU/CPU inference, row-interleaved quant packing, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;April 29 2025: Qwen3 support added&lt;/li&gt; &#xA; &lt;li&gt;April 26 2025: GLM-4 support added&lt;/li&gt; &#xA; &lt;li&gt;April 26 2025: Command-A support added&lt;/li&gt; &#xA; &lt;li&gt;April 22 2025: Support for the latest Microsoft Bitnet model added&lt;/li&gt; &#xA; &lt;li&gt;April 21 2025: ik_llama.cpp builds and runs successfully on Android (using termux)&lt;/li&gt; &#xA; &lt;li&gt;April 17 2025: Better CPU Flash Attention token generation performance&lt;/li&gt; &#xA; &lt;li&gt;April 13 2025: &lt;code&gt;IQ1_M&lt;/code&gt; quantization improvements&lt;/li&gt; &#xA; &lt;li&gt;April 10 2025: LLaMA-4 support added&lt;/li&gt; &#xA; &lt;li&gt;April 7 2025: &lt;code&gt;IQ2_XS&lt;/code&gt; quantization improvements&lt;/li&gt; &#xA; &lt;li&gt;April 3 2025: Much faster MoE implementation on Metal&lt;/li&gt; &#xA; &lt;li&gt;April 1 2025: Quantization improvements for &lt;code&gt;Q2_K, Q4_K, Q5_K, Q4_1, Q5_1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;March 28 2025: Quantization imrovements for &lt;code&gt;Q4_0, Q5_0, Q6_0, Q3_K, Q6_K, IQ4_XS, IQ4_NL&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;March 25 2025: Better MoE performance on CUDA&lt;/li&gt; &#xA; &lt;li&gt;March 23 2025: Better batched processing speed for DeepSeek models&lt;/li&gt; &#xA; &lt;li&gt;March 22 2025: Gemma3 support added&lt;/li&gt; &#xA; &lt;li&gt;March 21 2025: FlashMLA-3: fastest CPU-only inference for DeepSeek models&lt;/li&gt; &#xA; &lt;li&gt;March 18 2025: reduce compute buffer size&lt;/li&gt; &#xA; &lt;li&gt;March 17 2025: FlashMLA-2 performance improvements&lt;/li&gt; &#xA; &lt;li&gt;March 12 2025: Allow &lt;code&gt;Q8_0&lt;/code&gt; KV cache with FlashMLA-2 on CUDA&lt;/li&gt; &#xA; &lt;li&gt;March 10 2025: Better TG performance for MoE models on CUDA&lt;/li&gt; &#xA; &lt;li&gt;March 9 2025: FlashMLA on CUDA&lt;/li&gt; &#xA; &lt;li&gt;March 8 2025: Faster FlashMLA CPU implementation&lt;/li&gt; &#xA; &lt;li&gt;March 7 2025: Custom quantization mixes using regular expressions&lt;/li&gt; &#xA; &lt;li&gt;March 5 2025: FlashMLA on CUDA&lt;/li&gt; &#xA; &lt;li&gt;March 3 2025: Introducing FlashMLA - MLA with Flash Attention&lt;/li&gt; &#xA; &lt;li&gt;March 1 2025: Smart Expert Reduction for faster DeepSeek inference&lt;/li&gt; &#xA; &lt;li&gt;Feb 27 2025: MLA without transposed cache&lt;/li&gt; &#xA; &lt;li&gt;Feb 25 2025: tensor overrides for better control where model weights are stored (GPU or CPU)&lt;/li&gt; &#xA; &lt;li&gt;Feb 23 2025: fused FFN ops for faster MoE inference&lt;/li&gt; &#xA; &lt;li&gt;Feb 23 2025: &lt;code&gt;sweep-bench&lt;/code&gt; - better performance benchmarking&lt;/li&gt; &#xA; &lt;li&gt;Feb 20 2025: fast GEMM/GEMV for &lt;code&gt;IQ1_S&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Feb 19 2025: &lt;code&gt;Q8_KV&lt;/code&gt; - new type for 8-bit KV-cache quantization&lt;/li&gt; &#xA; &lt;li&gt;Feb 13 2025: allow &lt;code&gt;Q8_0&lt;/code&gt; quantized cache with MLA&lt;/li&gt; &#xA; &lt;li&gt;Feb 11 2025: Flash Attention support for DeepSeek models&lt;/li&gt; &#xA; &lt;li&gt;Feb 9 2025: MLA for DeepSeek models&lt;/li&gt; &#xA; &lt;li&gt;Jan 23 2025: DeepSeek-V3 support added&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;There is no single point of reference describing all new &lt;code&gt;ik_llama.cpp&lt;/code&gt; features. Pull requests often contain detailed information, so browsing the PRs is often the best way to learn about new features and how to use them. In addition&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ikawrakow/ik_llama.cpp/wiki&#34;&gt;The Wiki page&lt;/a&gt; has performance comparisons to mainline &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ikawrakow/ik_llama.cpp/discussions/258&#34;&gt;This guide&lt;/a&gt; is a good place to start if you came here because of DeepSeek models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ikawrakow/ik_llama.cpp/discussions/266&#34;&gt;This discussion&lt;/a&gt; is about running DeepSeek-V3/R1 on a 16 x 3090 setup&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ikawrakow/ik_llama.cpp/discussions/8&#34;&gt;This discussion&lt;/a&gt; describes the new quantization types available in &lt;code&gt;ik_llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions in form of pull requests, issue submissions (bug reports, feature requests), or general discussions, are welcome.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>