<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-18T01:35:35Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>momo5502/boiii</title>
    <updated>2022-09-18T01:35:35Z</updated>
    <id>tag:github.com,2022-09-18:/momo5502/boiii</id>
    <link href="https://github.com/momo5502/boiii" rel="alternate"></link>
    <summary type="html">&lt;p&gt;☄️ Reverse engineering and analysis of Call of Duty: Black Ops 3&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/momo5502/boiii.svg?sanitize=true&#34; alt=&#34;license&#34;&gt; &lt;a href=&#34;https://github.com/momo5502/boiii/actions&#34;&gt;&lt;img src=&#34;https://github.com/momo5502/boiii/workflows/Build/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paypal.me/momo5502&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PayPal-support-blue.svg?logo=paypal&#34; alt=&#34;paypal&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;BOIII ☄️&lt;/h1&gt; &#xA;&lt;p&gt;Reverse engineering and analysis of Call of Duty: Black Ops 3. Very experimental.&lt;/p&gt; &#xA;&lt;img src=&#34;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQeSXYzQITJrcjiifN1nqX1fsVE7VwLZ3vl2g&amp;amp;usqp=CAU&#34;&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Steam API Emulation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Steam Integrity Bypass&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Offline Multiplayer/Zombies/Campaign Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; RE Tool Detection Bypass (IDA Pro, HxD, ...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Disable Hardware Breakpoint Detection&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Disable Integrity Checks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Demonware Emulation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Disable Anti-Debugging Mechanisms&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;Unzip &lt;a href=&#34;https://nightly.link/momo5502/boiii/workflows/build/main/Release%20Binary.zip&#34;&gt;this&lt;/a&gt; into your BOIII folder and run the BlackOps3.exe&lt;/p&gt; &#xA;&lt;h2&gt;Compile from source&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the Git repo. Do NOT download it as ZIP, that won&#39;t work.&lt;/li&gt; &#xA; &lt;li&gt;Update the submodules and run &lt;code&gt;premake5 vs2022&lt;/code&gt; or simply use the delivered &lt;code&gt;generate.bat&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Build via solution file in &lt;code&gt;build\boiii.sln&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software has been created purely for the purposes of academic research. It is not intended to be used to harm others. Project maintainers are not responsible or liable for misuse of the software. Use responsibly.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zeromq/libzmq</title>
    <updated>2022-09-18T01:35:35Z</updated>
    <id>tag:github.com,2022-09-18:/zeromq/libzmq</id>
    <link href="https://github.com/zeromq/libzmq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ZeroMQ core engine in C++, implements ZMTP/3.1&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ZeroMQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zeromq/libzmq/actions/workflows/CI.yaml&#34;&gt;&lt;img src=&#34;https://github.com/zeromq/libzmq/actions/workflows/CI.yaml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/zeromq/libzmq&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/e2ks424yrs1un3wt?svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/zeromq/libzmq?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/github/zeromq/libzmq/badge.svg?branch=master&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Welcome&lt;/h2&gt; &#xA;&lt;p&gt;The ZeroMQ lightweight messaging kernel is a library which extends the standard socket interfaces with features traditionally provided by specialised messaging middleware products. ZeroMQ sockets provide an abstraction of asynchronous message queues, multiple messaging patterns, message filtering (subscriptions), seamless access to multiple transport protocols and more.&lt;/p&gt; &#xA;&lt;h2&gt;Supported platforms &lt;a name=&#34;#platforms&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Libzmq is mainly written in C++98 with some optional C++11-fragments. For configuration either autotools or CMake is employed. See below for some lists of platforms, where libzmq has been successfully compiled on.&lt;/p&gt; &#xA;&lt;h3&gt;Supported platforms with primary CI&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS and version&lt;/th&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Compiler and version&lt;/th&gt; &#xA;   &lt;th&gt;Build system&lt;/th&gt; &#xA;   &lt;th&gt;Remarks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Android NDK r24&lt;/td&gt; &#xA;   &lt;td&gt;arm, arm64, x86, x86_64&lt;/td&gt; &#xA;   &lt;td&gt;llvm (see NDK)&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ubuntu 14.04.5 LTS (trusty)&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;clang 5.0.0&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;STABLE, extras: GSSAPI, PGM, NORM, C++98 mode only&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ubuntu 14.04.5 LTS (trusty)&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 4.8.4&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;STABLE, DRAFT, extras: GSSAPI, PGM, NORM, TIPC, IPV6, also POLLER=poll, POLLER=select, also valgrind and address sanitizer executions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ubuntu 14.04.5 LTS (trusty)&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 4.8.4&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.12.2&lt;/td&gt; &#xA;   &lt;td&gt;STABLE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2008&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.12.2&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2010 SP1&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.12.2&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2012 Update 5&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.12.2&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2013 Update 5&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.12.2&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT, STABLE (x86 Release only), also POLLER=epoll&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2015 Update 3&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.12.2&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows Server 2016&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2017 15.9.6&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.13.3&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cygwin 3.0.0 on Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 7.4.0&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.6.2&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MSYS2 ? on Windows Server 2012 R2&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 6.4.0&lt;/td&gt; &#xA;   &lt;td&gt;CMake ?&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mac OS X 10.13&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;Xcode 9.4.1, Apple LLVM 9.1.0&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;STABLE, DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mac OS X 10.13&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;Xcode 9.4.1, Apple LLVM 9.1.0&lt;/td&gt; &#xA;   &lt;td&gt;CMake 3.11.4&lt;/td&gt; &#xA;   &lt;td&gt;DRAFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: the platforms are regularly updated by the service providers, so this information might get out of date without any changes on the side of libzmq. For Appveyor, refer to &lt;a href=&#34;https://www.appveyor.com/updates/&#34;&gt;https://www.appveyor.com/updates/&lt;/a&gt; regarding platform updates. For travis-ci, refer to &lt;a href=&#34;https://changelog.travis-ci.com/&#34;&gt;https://changelog.travis-ci.com/&lt;/a&gt; regarding platform updates.&lt;/p&gt; &#xA;&lt;h3&gt;Supported platforms with secondary CI&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS and version&lt;/th&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Compiler and version&lt;/th&gt; &#xA;   &lt;th&gt;Build system&lt;/th&gt; &#xA;   &lt;th&gt;Remarks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CentOS 6&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CentOS 7&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Debian 8.0&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Debian 9.0&lt;/td&gt; &#xA;   &lt;td&gt;ARM64, x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fedora 28&lt;/td&gt; &#xA;   &lt;td&gt;ARM64, ARM32, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fedora 29&lt;/td&gt; &#xA;   &lt;td&gt;ARM64, ARM32, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fedora Rawhide&lt;/td&gt; &#xA;   &lt;td&gt;ARM64, ARM32, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RedHat Enterprise Linux 7&lt;/td&gt; &#xA;   &lt;td&gt;amd64, ppc64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SuSE Linux Enterprise 12 SP4&lt;/td&gt; &#xA;   &lt;td&gt;ARM64, amd64, ppc64, s390x&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SuSE Linux Enterprise 15&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xUbuntu 12.04&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xUbuntu 14.04&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xUbuntu 16.04&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xUbuntu 18.04&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xUbuntu 18.10&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Supported platforms with known active users&lt;/h3&gt; &#xA;&lt;p&gt;At the time of writing, no explicit reports have been available. Please report your experiences by opening a PR adding an entry or moving an entry from the section below.&lt;/p&gt; &#xA;&lt;p&gt;Under &#34;last report&#34;, please name either the SHA1 in case of an unreleased version, or the version number in case of a released version.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS and version&lt;/th&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Compiler and version&lt;/th&gt; &#xA;   &lt;th&gt;Build system&lt;/th&gt; &#xA;   &lt;th&gt;Last report&lt;/th&gt; &#xA;   &lt;th&gt;Remarks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Solaris 10&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64, sparc&lt;/td&gt; &#xA;   &lt;td&gt;GCC 8.1.0&lt;/td&gt; &#xA;   &lt;td&gt;CMake&lt;/td&gt; &#xA;   &lt;td&gt;2019/03/18&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DragonFly BSD&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 8.3&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;2018/08/07 git-72854e63&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IBM i&lt;/td&gt; &#xA;   &lt;td&gt;ppc64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 6.3&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;2019/10/02 git-25320a3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QNX 7.0&lt;/td&gt; &#xA;   &lt;td&gt;x86_64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 5.4.0&lt;/td&gt; &#xA;   &lt;td&gt;CMake&lt;/td&gt; &#xA;   &lt;td&gt;4.3.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows 10&lt;/td&gt; &#xA;   &lt;td&gt;ARM64&lt;/td&gt; &#xA;   &lt;td&gt;Visual Studio 2019&lt;/td&gt; &#xA;   &lt;td&gt;CMake&lt;/td&gt; &#xA;   &lt;td&gt;2021/11/15 git-2375ca8b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows 10&lt;/td&gt; &#xA;   &lt;td&gt;ARM64&lt;/td&gt; &#xA;   &lt;td&gt;clang&lt;/td&gt; &#xA;   &lt;td&gt;CMake&lt;/td&gt; &#xA;   &lt;td&gt;2021/11/15 git-2375ca8b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Supported platforms without known active users&lt;/h3&gt; &#xA;&lt;p&gt;Note: this list is incomplete and inaccurate and still needs some work.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS and version&lt;/th&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Compiler and version&lt;/th&gt; &#xA;   &lt;th&gt;Build system&lt;/th&gt; &#xA;   &lt;th&gt;Remarks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Any Linux distribution&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc ?+, clang ?+, icc ?+&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SunOS, Solaris&lt;/td&gt; &#xA;   &lt;td&gt;x86, amd64&lt;/td&gt; &#xA;   &lt;td&gt;SunPro&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNU/kFreeBSD&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FreeBSD&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NetBSD&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenBSD&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DragonFly BSD&lt;/td&gt; &#xA;   &lt;td&gt;amd64&lt;/td&gt; &#xA;   &lt;td&gt;gcc 8.3&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HP-UX&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools, CMake&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNU/Hurd&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;autotools&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VxWorks 6.8&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows CE&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows UWP&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenVMS&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Unsupported platforms&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS and version&lt;/th&gt; &#xA;   &lt;th&gt;Architecture&lt;/th&gt; &#xA;   &lt;th&gt;Compiler and version&lt;/th&gt; &#xA;   &lt;th&gt;Remarks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QNX 6.3&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;gcc 3.3.5&lt;/td&gt; &#xA;   &lt;td&gt;see #3371, support was added by a user, but not contributed to upstream&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/zeromq/libzmq/master/SupportedPlatforms.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For some platforms (Linux, Mac OS X), &lt;a href=&#34;https://raw.githubusercontent.com/zeromq/libzmq/master/#installation&#34;&gt;prebuilt binary packages are supplied by the ZeroMQ organization&lt;/a&gt;. For other platforms, you need to &lt;a href=&#34;https://raw.githubusercontent.com/zeromq/libzmq/master/#build&#34;&gt;build your own binaries&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation of binary packages &lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;For Linux users, pre-built binary packages are available for most distributions. Note that DRAFT APIs can change at any time without warning, pick a STABLE build to avoid having them enabled.&lt;/p&gt; &#xA;&lt;h4&gt;Latest releases&lt;/h4&gt; &#xA;&lt;h5&gt;DEB&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Arelease-stable&amp;amp;package=libzmq3-dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-stable-yellow.svg?sanitize=true&#34; alt=&#34;OBS release stable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Arelease-draft&amp;amp;package=libzmq3-dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-draft-yellow.svg?sanitize=true&#34; alt=&#34;OBS release draft&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;RPM&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Arelease-stable&amp;amp;package=zeromq-devel&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-stable-yellow.svg?sanitize=true&#34; alt=&#34;OBS release stable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Arelease-draft&amp;amp;package=zeromq-devel&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-draft-yellow.svg?sanitize=true&#34; alt=&#34;OBS release draft&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Bleeding edge packages&lt;/h4&gt; &#xA;&lt;h5&gt;DEB&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Agit-stable&amp;amp;package=libzmq3-dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-stable-yellow.svg?sanitize=true&#34; alt=&#34;OBS release stable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Agit-draft&amp;amp;package=libzmq3-dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-draft-yellow.svg?sanitize=true&#34; alt=&#34;OBS release draft&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;RPM&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Agit-stable&amp;amp;package=zeromq-devel&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-stable-yellow.svg?sanitize=true&#34; alt=&#34;OBS release stable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://software.opensuse.org/download.html?project=network%3Amessaging%3Azeromq%3Agit-draft&amp;amp;package=zeromq-devel&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OBS%20master-draft-yellow.svg?sanitize=true&#34; alt=&#34;OBS release draft&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Example: Debian 9 latest release, no DRAFT apis&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#34;deb http://download.opensuse.org/repositories/network:/messaging:/zeromq:/release-stable/Debian_9.0/ ./&#34; &amp;gt;&amp;gt; /etc/apt/sources.list&#xA;wget https://download.opensuse.org/repositories/network:/messaging:/zeromq:/release-stable/Debian_9.0/Release.key -O- | sudo apt-key add&#xA;apt-get install libzmq3-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OSX&lt;/h3&gt; &#xA;&lt;p&gt;For OSX users, packages are available via brew.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install zeromq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation of package manager &lt;a name=&#34;package manager&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;vcpkg&lt;/h3&gt; &#xA;&lt;p&gt;vcpkg is a full platform package manager, you can easily install libzmq via vcpkg.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/microsoft/vcpkg.git&#xA;./bootstrap-vcpkg.bat # For powershell&#xA;./bootstrap-vcpkg.sh # For bash&#xA;./vcpkg install zeromq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build from sources &lt;a name=&#34;build&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To build from sources, see the INSTALL file included with the distribution.&lt;/p&gt; &#xA;&lt;h3&gt;Android&lt;/h3&gt; &#xA;&lt;p&gt;To build from source, see &lt;a href=&#34;https://raw.githubusercontent.com/zeromq/libzmq/master/builds/android/README.md&#34;&gt;README&lt;/a&gt; file in the android build directory.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;p&gt;Extensive documentation is provided with the distribution. Refer to doc/zmq.html, or &#34;man zmq&#34; after you have installed libzmq on your system.&lt;/p&gt; &#xA;&lt;p&gt;Website: &lt;a href=&#34;http://www.zeromq.org/&#34;&gt;http://www.zeromq.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Development mailing list: &lt;a href=&#34;mailto:zeromq-dev@lists.zeromq.org&#34;&gt;zeromq-dev@lists.zeromq.org&lt;/a&gt; Announcements mailing list: &lt;a href=&#34;mailto:zeromq-announce@lists.zeromq.org&#34;&gt;zeromq-announce@lists.zeromq.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Git repository: &lt;a href=&#34;http://github.com/zeromq/libzmq&#34;&gt;http://github.com/zeromq/libzmq&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ZeroMQ developers can also be found on the IRC channel #zeromq, on the Libera Chat network (irc.libera.chat).&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The project license is specified in COPYING and COPYING.LESSER.&lt;/p&gt; &#xA;&lt;p&gt;libzmq is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License (LGPL) as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.&lt;/p&gt; &#xA;&lt;p&gt;As a special exception, the Contributors give you permission to link this library with independent modules to produce an executable, regardless of the license terms of these independent modules, and to copy and distribute the resulting executable under terms of your choice, provided that you also meet, for each linked independent module, the terms and conditions of the license of that module. An independent module is a module which is not derived from or based on this library. If you modify this library, you must extend this exception to your version of the library.&lt;/p&gt; &#xA;&lt;p&gt;libzmq is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://rfc.zeromq.org/spec:42/C4/&#34;&gt;C4(Collective Code Construction Contract)&lt;/a&gt; process for contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/FasterTransformer</title>
    <updated>2022-09-18T01:35:35Z</updated>
    <id>tag:github.com,2022-09-18:/NVIDIA/FasterTransformer</id>
    <link href="https://github.com/NVIDIA/FasterTransformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Transformer related optimization, including BERT, GPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FasterTransformer&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.&lt;/p&gt; &#xA;&lt;h2&gt;Table Of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#fastertransformer&#34;&gt;FasterTransformer&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#table-of-contents&#34;&gt;Table Of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#model-overview&#34;&gt;Model overview&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#support-matrix&#34;&gt;Support matrix&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#advanced&#34;&gt;Advanced&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#performance&#34;&gt;Performance&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performance&#34;&gt;BERT base performance&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performances-of-fastertransformer-new-features&#34;&gt;BERT base performances of FasterTransformer new features&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performance-on-tensorflow&#34;&gt;BERT base performance on TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performance-on-pytorch&#34;&gt;BERT base performance on PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#decoding-and-decoder-performance&#34;&gt;Decoding and Decoder performance&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#decoder-and-decoding-end-to-end-translation-performance-on-tensorflow&#34;&gt;Decoder and Decoding end-to-end translation performance on TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#decoder-and-decoding-end-to-end-translation-performance-on-pytorch&#34;&gt;Decoder and Decoding end-to-end translation performance on PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#gpt-performance&#34;&gt;GPT performance&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#release-notes&#34;&gt;Release notes&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#known-issues&#34;&gt;Known issues&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model overview&lt;/h2&gt; &#xA;&lt;p&gt;In NLP, encoder and decoder are two important components, with the transformer layer becoming a popular architecture for both components. FasterTransformer implements a highly optimized transformer layer for both the encoder and decoder for inference. On Volta, Turing and Ampere GPUs, the computing power of Tensor Cores are used automatically when the precision of the data and weights are FP16.&lt;/p&gt; &#xA;&lt;p&gt;FasterTransformer is built on top of CUDA, cuBLAS, cuBLASLt and C++. We provide at least one API of the following frameworks: TensorFlow, PyTorch and Triton backend. Users can integrate FasterTransformer into these frameworks directly. For supporting frameworks, we also provide example codes to demonstrate how to use, and show the performance on these frameworks.&lt;/p&gt; &#xA;&lt;h3&gt;Support matrix&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;FP16&lt;/th&gt; &#xA;   &lt;th&gt;INT8 (after Turing)&lt;/th&gt; &#xA;   &lt;th&gt;Sparsity (after Ampere)&lt;/th&gt; &#xA;   &lt;th&gt;Tensor parallel&lt;/th&gt; &#xA;   &lt;th&gt;Pipeline parallel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XLNet&lt;/td&gt; &#xA;   &lt;td&gt;C++&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Encoder&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Encoder&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoder&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoder&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Longformer&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swin Transformer&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swin Transformer&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-NeoX&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note that the FasterTransformer supports the models above on C++ because all source codes are built on C++.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details of specific models are put in &lt;code&gt;xxx_guide.md&lt;/code&gt; of &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs&#34;&gt;&lt;code&gt;docs/&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;xxx&lt;/code&gt; means the model name. Some common questions and the respective answers are put in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/QAList.md&#34;&gt;&lt;code&gt;docs/QAList.md&lt;/code&gt;&lt;/a&gt;. Note that the model of Encoder and BERT are similar and we put the explanation into &lt;code&gt;bert_guide.md&lt;/code&gt; together.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced&lt;/h2&gt; &#xA;&lt;p&gt;The following code lists the directory structure of FasterTransformer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/src/fastertransformer: source code of FasterTransformer&#xA;    |--/models: Implementation of different models, like BERT, GPT.&#xA;    |--/layers: Implementation of layer modules, like attention layer, ffn layer.&#xA;    |--/kernels: CUDA kernels for different models/layers and operations, like addBiasResiual.&#xA;    |--/tensorrt_plugin: encapluate FasterTransformer into TensorRT plugin.&#xA;    |--/tf_op: custom Tensorflow OP implementation&#xA;    |--/th_op: custom PyTorch OP implementation&#xA;    |--/triton_backend: custom triton backend implementation&#xA;    |--/utils: Contains common cuda utils, like cublasMMWrapper, memory_utils&#xA;/examples: C++, tensorflow and pytorch interface examples&#xA;    |--/cpp: C++ interface examples&#xA;    |--/pytorch: PyTorch OP examples&#xA;    |--/tensorflow: TensorFlow OP examples&#xA;    |--tensorrt: TensorRT examples&#xA;/docs: Documents to explain the details of implementation of different models, and show the benchmark&#xA;/benchmark: Contains the scripts to run the benchmarks of different models&#xA;/tests: Unit tests&#xA;/templates: Documents to explain how to add a new model/example into FasterTransformer repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that many folders contains many sub-folders to split different models. Quantization tools are move to &lt;code&gt;examples&lt;/code&gt;, like &lt;code&gt;examples/tensorflow/bert/bert-quantization/&lt;/code&gt; and &lt;code&gt;examples/pytorch/bert/bert-quantization-sparsity/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Hardware settings:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8xA100-80GBs (with mclk 1593MHz, pclk 1410MHz) with AMD EPYC 7742 64-Core Processor&lt;/li&gt; &#xA; &lt;li&gt;T4 (with mclk 5000MHz, pclk 1590MHz) with Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In order to run the following benchmark, we need to install the unix computing tool &#34;bc&#34; by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get install bc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;BERT base performance&lt;/h3&gt; &#xA;&lt;p&gt;The FP16 results of TensorFlow were obtained by running the &lt;code&gt;benchmarks/bert/tf_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The INT8 results of TensorFlow were obtained by running the &lt;code&gt;benchmarks/bert/tf_int8_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The FP16 results of PyTorch were obtained by running the &lt;code&gt;benchmarks/bert/pyt_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The INT8 results of PyTorch were obtained by running the &lt;code&gt;benchmarks/bert/pyt_int8_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More benchmarks are put in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/bert_guide.md#bert-performance&#34;&gt;&lt;code&gt;docs/bert_guide.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;BERT base performances of FasterTransformer new features&lt;/h4&gt; &#xA;&lt;p&gt;The following figure compares the performances of different features of FasterTransformer and FasterTransformer under FP16 on T4.&lt;/p&gt; &#xA;&lt;p&gt;For large batch size and sequence length, both EFF-FT and FT-INT8-v2 bring about 2x speedup. Using Effective FasterTransformer and int8v2 at the same time can bring about 3.5x speedup compared to FasterTransformer FP16 for large case.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/FT_Encoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h4&gt;BERT base performance on TensorFlow&lt;/h4&gt; &#xA;&lt;p&gt;The following figure compares the performances of different features of FasterTransformer and TensorFlow XLA under FP16 on T4.&lt;/p&gt; &#xA;&lt;p&gt;For small batch size and sequence length, using FasterTransformer can bring about 3x speedup.&lt;/p&gt; &#xA;&lt;p&gt;For large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/TF_Encoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h4&gt;BERT base performance on PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;The following figure compares the performances of different features of FasterTransformer and PyTorch TorchScript under FP16 on T4.&lt;/p&gt; &#xA;&lt;p&gt;For small batch size and sequence length, using FasterTransformer CustomExt can bring about 4x ~ 6x speedup.&lt;/p&gt; &#xA;&lt;p&gt;For large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/Py_Encoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Decoding and Decoder performance&lt;/h3&gt; &#xA;&lt;p&gt;The results of TensorFlow were obtained by running the &lt;code&gt;benchmarks/decoding/tf_decoding_beamsearch_benchmark.sh&lt;/code&gt; and &lt;code&gt;benchmarks/decoding/tf_decoding_sampling_benchmark.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The results of PyTorch were obtained by running the &lt;code&gt;benchmarks/decoding/pyt_decoding_beamsearch_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the experiments of decoding, we updated the following parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;head_num = 8&lt;/li&gt; &#xA; &lt;li&gt;size_per_head = 64&lt;/li&gt; &#xA; &lt;li&gt;num_layers = 6 for both encoder and decoder&lt;/li&gt; &#xA; &lt;li&gt;vocabulary_size = 32001 for TensorFlow sample codes, 31538 for PyTorch sample codes&lt;/li&gt; &#xA; &lt;li&gt;memory_hidden_dim = 512&lt;/li&gt; &#xA; &lt;li&gt;max sequenc elength = 128&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More benchmarks are put in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/decoder_guide.md#decoding-performance&#34;&gt;&lt;code&gt;docs/decoder_guide.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Decoder and Decoding end-to-end translation performance on TensorFlow&lt;/h4&gt; &#xA;&lt;p&gt;The following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to TensorFlow under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to TensorFlow, FT-Decoder provides 1.5x ~ 3x speedup; while FT-Decoding provides 4x ~ 18x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/TF_Decoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Decoder and Decoding end-to-end translation performance on PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;The following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to PyTorch under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to PyTorch, FT-Decoder provides 1.2x ~ 3x speedup; while FT-Decoding provides 3.8x ~ 13x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/Py_Decoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;GPT performance&lt;/h3&gt; &#xA;&lt;p&gt;The following figure compares the performances of Megatron and FasterTransformer under FP16 on A100.&lt;/p&gt; &#xA;&lt;p&gt;In the experiments of decoding, we updated the following parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;head_num = 96&lt;/li&gt; &#xA; &lt;li&gt;size_per_head = 128&lt;/li&gt; &#xA; &lt;li&gt;num_layers = 48 for GPT-89B model, 96 for GPT-175B model&lt;/li&gt; &#xA; &lt;li&gt;data_type = FP16&lt;/li&gt; &#xA; &lt;li&gt;vocab_size = 51200&lt;/li&gt; &#xA; &lt;li&gt;top_p = 0.9&lt;/li&gt; &#xA; &lt;li&gt;tensor parallel size = 8&lt;/li&gt; &#xA; &lt;li&gt;input sequence length = 512&lt;/li&gt; &#xA; &lt;li&gt;output sequence length = 32&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/FT_GPT_A100.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Release notes&lt;/h2&gt; &#xA;&lt;h3&gt;Changelog&lt;/h3&gt; &#xA;&lt;p&gt;Aug 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 5.1&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support for interactive generation&lt;/li&gt; &#xA; &lt;li&gt;Support for attention time-limited memory&lt;/li&gt; &#xA; &lt;li&gt;Support mt5 and t5-v1.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;July 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support UL2 huggingface ckpt. (&lt;a href=&#34;https://huggingface.co/google/ul2&#34;&gt;link&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fix bug of T5 under bfloat16.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Add ViT INT8 TensorRT Plugin&lt;/li&gt; &#xA; &lt;li&gt;Support batch sampling&lt;/li&gt; &#xA; &lt;li&gt;Support shared context optimization in GPT model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;June 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support streaming generation for triton backend.&lt;/li&gt; &#xA; &lt;li&gt;Support OPT.&lt;/li&gt; &#xA; &lt;li&gt;Support multi-node multi-GPU BERT under FP32, FP16 and BF16.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;May 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support bfloat16 on most models.&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://arxiv.org/pdf/2101.00190.pdf&#34;&gt;prefix-prompt&lt;/a&gt; for GPT-J.&lt;/li&gt; &#xA; &lt;li&gt;Support GPT-NeoX. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;epsilon value used in layernorm is now a parameter&lt;/li&gt; &#xA;   &lt;li&gt;rotary embedding GPT-NeoX style (only GPT-J was implemented)&lt;/li&gt; &#xA;   &lt;li&gt;load per-GPU layernorm and bias parameters&lt;/li&gt; &#xA;   &lt;li&gt;weight conversion from EleutherAI checkpoint&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;April 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 5.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Change the default accumulation type of all gemm to FP32.&lt;/li&gt; &#xA;   &lt;li&gt;Support bfloat16 inference in GPT model.&lt;/li&gt; &#xA;   &lt;li&gt;Support Nemo Megatron T5 and Megatron-LM T5 model.&lt;/li&gt; &#xA;   &lt;li&gt;Support ViT.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;March 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support &lt;code&gt;stop_ids&lt;/code&gt; and &lt;code&gt;ban_bad_ids&lt;/code&gt; in GPT-J.&lt;/li&gt; &#xA; &lt;li&gt;Support dynamice &lt;code&gt;start_id&lt;/code&gt; and &lt;code&gt;end_id&lt;/code&gt; in GPT-J, GPT, T5 and Decoding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;February 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support Swin Transformer.&lt;/li&gt; &#xA; &lt;li&gt;Optimize the k/v cache update of beam search by in-direction buffer.&lt;/li&gt; &#xA; &lt;li&gt;Support runtime input for GPT-J, T5 and GPT.&lt;/li&gt; &#xA; &lt;li&gt;Support soft prompt in GPT and GPT-J.&lt;/li&gt; &#xA; &lt;li&gt;Support custom all reduce kernel. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Limitation: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Only support tensor parallel size = 8 on DGX-A100.&lt;/li&gt; &#xA;     &lt;li&gt;Only support CUDA with cudaMallocAsync.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;December 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add TensorRT plugin of T5 model.&lt;/li&gt; &#xA; &lt;li&gt;Change some hyper-parameters of GPT model to runtime query.&lt;/li&gt; &#xA; &lt;li&gt;Optimize the memory allocator under C++ code.&lt;/li&gt; &#xA; &lt;li&gt;Fix bug of CUB including when using CUDA 11.5 or newer version.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;November 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the FasterTransformer 5.0 beta&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add GPT-3 INT8 weight only qauntization for batch size &amp;lt;= 2.&lt;/li&gt; &#xA; &lt;li&gt;Support multi-node multi-gpu support on T5.&lt;/li&gt; &#xA; &lt;li&gt;Enhance the multi-node multi-gpu supporting in GPT-3.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;August 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 5.0 beta&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Refactor the repo and codes&lt;/li&gt; &#xA;   &lt;li&gt;And special thanks to NAVER Corp. for contributing a lot to this version, as listed below. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Bugs fix &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Fix error that occurs when batch_size is less than max_batch_size for gpt pytorch wrapper.&lt;/li&gt; &#xA;       &lt;li&gt;Fix memory leak that occurs every forward because of reused allocator.&lt;/li&gt; &#xA;       &lt;li&gt;Fix race condition that occurs in repetition penalty kernel.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Enhancement &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Add random seed setting.&lt;/li&gt; &#xA;       &lt;li&gt;Fix GEMM buffer overflow on FP16 of GPT.&lt;/li&gt; &#xA;       &lt;li&gt;Change to invalidate finished buffer for every completion.&lt;/li&gt; &#xA;       &lt;li&gt;Introduce stop_before for early stop.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Support Longformer.&lt;/li&gt; &#xA;   &lt;li&gt;Rename &lt;code&gt;layer_para&lt;/code&gt; to &lt;code&gt;pipeline_para&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Optimize the sorting of top p sampling.&lt;/li&gt; &#xA;   &lt;li&gt;Support sparsity for Ampere GPUs on BERT.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;code&gt;size_per_head&lt;/code&gt; 96, 160, 192, 224, 256 for GPT model.&lt;/li&gt; &#xA;   &lt;li&gt;Support multi-node inference for GPT Triton backend.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;June 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support XLNet&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;April 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 4.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support multi-gpus and multi-nodes inference for GPT model on C++ and PyTorch.&lt;/li&gt; &#xA;   &lt;li&gt;Support single node, multi-gpus inference for GPT model on triton.&lt;/li&gt; &#xA;   &lt;li&gt;Add the int8 fused multi-head attention kernel for bert.&lt;/li&gt; &#xA;   &lt;li&gt;Add the FP16 fused multi-head attention kernel of V100 for bert.&lt;/li&gt; &#xA;   &lt;li&gt;Optimize the kernel of decoder.&lt;/li&gt; &#xA;   &lt;li&gt;Move to independent repo.&lt;/li&gt; &#xA;   &lt;li&gt;Eager mode PyTorch extension is deprecated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Dec 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 3.1&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optimize the decoding by adding the finisehd mask to prevent useless computing.&lt;/li&gt; &#xA;   &lt;li&gt;Support opennmt encoder.&lt;/li&gt; &#xA;   &lt;li&gt;Remove the TensorRT plugin supporting.&lt;/li&gt; &#xA;   &lt;li&gt;TorchScript custom op is deprecated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Nov 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimize the INT8 inference.&lt;/li&gt; &#xA; &lt;li&gt;Support PyTorch INT8 inference.&lt;/li&gt; &#xA; &lt;li&gt;Provide PyTorch INT8 quantiztion tools.&lt;/li&gt; &#xA; &lt;li&gt;Integrate the fused multi-head attention kernel of TensorRT into FasterTransformer.&lt;/li&gt; &#xA; &lt;li&gt;Add unit test of SQuAD.&lt;/li&gt; &#xA; &lt;li&gt;Update the missed NGC checkpoints.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sep 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support GPT2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 3.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support INT8 quantization of encoder of cpp and TensorFlow op.&lt;/li&gt; &#xA;   &lt;li&gt;Add bert-tf-quantization tool.&lt;/li&gt; &#xA;   &lt;li&gt;Fix the issue that Cmake 15 or Cmake 16 fail to build this project.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Aug 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix the bug of trt plugin.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;June 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 2.1&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add Effective FasterTransformer based on the idea of &lt;a href=&#34;https://github.com/bytedance/effective_transformer&#34;&gt;Effective Transformer&lt;/a&gt; idea.&lt;/li&gt; &#xA;   &lt;li&gt;Optimize the beam search kernels.&lt;/li&gt; &#xA;   &lt;li&gt;Add PyTorch op supporting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;May 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix the bug that seq_len of encoder must be larger than 3.&lt;/li&gt; &#xA; &lt;li&gt;Add the position_encoding of decoding as the input of FasterTransformer decoding. This is convenient to use different types of position encoding. FasterTransformer does not compute the position encoding value, but only lookup the table.&lt;/li&gt; &#xA; &lt;li&gt;Modifying the method of loading model in &lt;code&gt;translate_sample.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;April 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rename &lt;code&gt;decoding_opennmt.h&lt;/code&gt; to &lt;code&gt;decoding_beamsearch.h&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add DiverseSiblingsSearch for decoding.&lt;/li&gt; &#xA; &lt;li&gt;Add sampling into Decoding &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The implementation is in the &lt;code&gt;decoding_sampling.h&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Add top_k sampling, top_p sampling for decoding.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Refactor the tensorflow custom op codes. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Merge &lt;code&gt;bert_transformer_op.h&lt;/code&gt;, &lt;code&gt;bert_transformer_op.cu.cc&lt;/code&gt; into &lt;code&gt;bert_transformer_op.cc&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Merge &lt;code&gt;decoder.h&lt;/code&gt;, &lt;code&gt;decoder.cu.cc&lt;/code&gt; into &lt;code&gt;decoder.cc&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Merge &lt;code&gt;decoding_beamsearch.h&lt;/code&gt;, &lt;code&gt;decoding_beamsearch.cu.cc&lt;/code&gt; into &lt;code&gt;decoding_beamsearch.cc&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix the bugs of finalize function decoding.py.&lt;/li&gt; &#xA; &lt;li&gt;Fix the bug of tf DiverseSiblingSearch.&lt;/li&gt; &#xA; &lt;li&gt;Add BLEU scorer &lt;code&gt;bleu_score.py&lt;/code&gt; into &lt;code&gt;utils&lt;/code&gt;. Note that the BLEU score requires python3.&lt;/li&gt; &#xA; &lt;li&gt;Fuse QKV Gemm of encoder and masked_multi_head_attention of decoder.&lt;/li&gt; &#xA; &lt;li&gt;Add dynamic batch size and dynamic sequence length features into all ops.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;March 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add feature in FasterTransformer 2.0 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;translate_sample.py&lt;/code&gt; to demonstrate how to translate a sentence by restoring the pretrained model of OpenNMT-tf.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix bugs of Fastertransformer 2.0 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fix the bug of maximum sequence length of decoder cannot be larger than 128.&lt;/li&gt; &#xA;   &lt;li&gt;Fix the bug that decoding does not check finish or not after each step.&lt;/li&gt; &#xA;   &lt;li&gt;Fix the bug of decoder about max_seq_len.&lt;/li&gt; &#xA;   &lt;li&gt;Modify the decoding model structure to fit the OpenNMT-tf decoding model. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Add a layer normalization layer after decoder.&lt;/li&gt; &#xA;     &lt;li&gt;Add a normalization for inputs of decoder&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;February 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 2.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Provide a highly optimized OpenNMT-tf based decoder and decoding, including C++ API and TensorFlow op.&lt;/li&gt; &#xA;   &lt;li&gt;Refine the sample codes of encoder.&lt;/li&gt; &#xA;   &lt;li&gt;Add dynamic batch size feature into encoder op.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;July 2019&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 1.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Provide a highly optimized bert equivalent transformer layer, including C++ API, TensorFlow op and TensorRT plugin.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Undefined symbol errors when import the extension &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Please &lt;code&gt;import torch&lt;/code&gt; first. If this has been done, it is due to the incompatible C++ ABI. You may need to check the PyTorch used during compilation and execution are the same, or you need to check how your PyTorch is compiled, or the version of your GCC, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Results of TensorFlow and OP would be different in decoding. This problem is caused by the accumulated log probability, and we do not avoid this problem.&lt;/li&gt; &#xA; &lt;li&gt;If encounter some problem in the custom environment, try to use the gcc/g++ 4.8 to build the project of TensorFlow op, especially for TensorFlow 1.14.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>