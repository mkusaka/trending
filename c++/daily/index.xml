<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-22T01:28:25Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SaadAhla/IP-Hunter</title>
    <updated>2024-05-22T01:28:25Z</updated>
    <id>tag:github.com,2024-05-22:/SaadAhla/IP-Hunter</id>
    <link href="https://github.com/SaadAhla/IP-Hunter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hunt for C2 servers and phishing web sites using VirusTotal API , you can modify code to kill the malicious process&lt;/p&gt;&lt;hr&gt;&lt;p&gt;You better use VirusTotal Premium&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SaadAhla/IP-Hunter/assets/123980007/f20e04a1-809a-46f8-8338-917b424ce580&#34;&gt;https://github.com/SaadAhla/IP-Hunter/assets/123980007/f20e04a1-809a-46f8-8338-917b424ce580&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>triton-lang/triton</title>
    <updated>2024-05-22T01:28:25Z</updated>
    <id>tag:github.com,2024-05-22:/triton-lang/triton</id>
    <link href="https://github.com/triton-lang/triton" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Development repository for the Triton language and compiler&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://cdn.openai.com/triton/assets/triton-logo.png&#34; alt=&#34;Triton logo&#34; width=&#34;88&#34; height=&#34;100&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We&#39;re hiring! If you are interested in working on Triton at OpenAI, we have roles open for &lt;a href=&#34;https://openai.com/careers/software-engineer-triton-compiler&#34;&gt;Compiler Engineers&lt;/a&gt; and &lt;a href=&#34;https://openai.com/careers/kernel-engineer&#34;&gt;Kernel Engineers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;&lt;code&gt;Nightly Wheels&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://triton-lang.org/&#34;&gt;&lt;img src=&#34;https://github.com/triton-lang/triton/actions/workflows/documentation.yml/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/triton-lang/triton/actions/workflows/wheels.yml&#34;&gt;&lt;img src=&#34;https://github.com/triton-lang/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x&#34; alt=&#34;Wheels&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Triton&lt;/h1&gt; &#xA;&lt;p&gt;This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.&lt;/p&gt; &#xA;&lt;p&gt;The foundations of this project are described in the following MAPL2019 publication: &lt;a href=&#34;http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf&#34;&gt;Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations&lt;/a&gt;. Please consider citing this work if you use Triton!&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://triton-lang.org&#34;&gt;official documentation&lt;/a&gt; contains installation instructions and tutorials. See also these third-party &lt;a href=&#34;https://github.com/srush/Triton-Puzzles&#34;&gt;Triton puzzles&lt;/a&gt;, which can all be run using the Triton interpreter -- no GPU required.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Installation&lt;/h1&gt; &#xA;&lt;p&gt;You can install the latest stable release of Triton from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install triton&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Binary wheels are available for CPython 3.8-3.12 and PyPy 3.8-3.9.&lt;/p&gt; &#xA;&lt;p&gt;And the latest nightly release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Install from source&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/triton-lang/triton.git;&#xA;cd triton;&#xA;&#xA;pip install ninja cmake wheel; # build-time dependencies&#xA;pip install -e python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or with a virtualenv:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/triton-lang/triton.git;&#xA;cd triton;&#xA;&#xA;python -m venv .venv --prompt triton;&#xA;source .venv/bin/activate;&#xA;&#xA;pip install ninja cmake wheel; # build-time dependencies&#xA;pip install -e python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building with a custom LLVM&lt;/h1&gt; &#xA;&lt;p&gt;Triton uses LLVM to generate code for GPUs and CPUs. Normally, the Triton build downloads a prebuilt LLVM, but you can also build LLVM from source and use that.&lt;/p&gt; &#xA;&lt;p&gt;LLVM does not have a stable API, so the Triton build will not work at an arbitrary LLVM version.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Find the version of LLVM that Triton builds against. Check &lt;code&gt;cmake/llvm-hash.txt&lt;/code&gt; to see the current version. For example, if it says: 49af6502c6dcb4a7f7520178bd14df396f78240c&lt;/p&gt; &lt;p&gt;This means that the version of Triton you have builds against &lt;a href=&#34;https://github.com/llvm/llvm-project&#34;&gt;LLVM&lt;/a&gt; 49af6502.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;git checkout&lt;/code&gt; LLVM at this revision. Optionally, make additional modifications to LLVM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://llvm.org/docs/CMake.html&#34;&gt;Build LLVM&lt;/a&gt;. For example, you might run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ cd $HOME/llvm-project  # your clone of LLVM.&#xA;$ mkdir build&#xA;$ cd build&#xA;$ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON ../llvm -DLLVM_ENABLE_PROJECTS=&#34;mlir;llvm&#34; -DLLVM_TARGETS_TO_BUILD=&#34;host;NVPTX;AMDGPU&#34;&#xA;$ ninja&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Grab a snack, this will take a while.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build Triton as above, but set the following environment variables.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Modify as appropriate to point to your LLVM build.&#xA;$ export LLVM_BUILD_DIR=$HOME/llvm-project/build&#xA;&#xA;$ cd &amp;lt;triton install&amp;gt;&#xA;$ LLVM_INCLUDE_DIRS=$LLVM_BUILD_DIR/include \&#xA;  LLVM_LIBRARY_DIR=$LLVM_BUILD_DIR/lib \&#xA;  LLVM_SYSPATH=$LLVM_BUILD_DIR \&#xA;  pip install -e python&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Tips for building&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;code&gt;TRITON_BUILD_WITH_CLANG_LLD=true&lt;/code&gt; as an environment variable to use clang and lld. lld in particular results in faster builds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set &lt;code&gt;TRITON_BUILD_WITH_CCACHE=true&lt;/code&gt; to build with ccache.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pass &lt;code&gt;--no-build-isolation&lt;/code&gt; to &lt;code&gt;pip install&lt;/code&gt; to make nop builds faster. Without this, every invocation of &lt;code&gt;pip install&lt;/code&gt; uses a different symlink to cmake, and this forces ninja to rebuild most of the &lt;code&gt;.a&lt;/code&gt; files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;vscode intellisense has some difficulty figuring out how to build Triton&#39;s C++ (probably because, in our build, users don&#39;t invoke cmake directly, but instead use setup.py). Teach vscode how to compile Triton as follows.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Do a local build.&lt;/li&gt; &#xA;   &lt;li&gt;Get the full path to the &lt;code&gt;compile_commands.json&lt;/code&gt; file produced by the build: &lt;code&gt;find python/build -name &#39;compile_commands.json | xargs readlink -f&#39;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;In vscode, install the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools&#34;&gt;C/C++ extension&lt;/a&gt;, then open the command palette (&lt;code&gt;Shift + Command + P&lt;/code&gt; on Mac, or &lt;code&gt;Shift + Ctrl + P&lt;/code&gt; on Windows/Linux) and open &lt;code&gt;C/C++: Edit Configurations (UI)&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Open &#34;Advanced Settings&#34; and paste the full path to &lt;code&gt;compile_commands.json&lt;/code&gt; into the &#34;Compile Commands&#34; textbox.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Running tests&lt;/h1&gt; &#xA;&lt;p&gt;There currently isn&#39;t a turnkey way to run all the Triton tests, but you can follow the following recipe.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# One-time setup.  Note we have to reinstall local Triton because torch&#xA;# overwrites it with the public version.&#xA;$ pip install scipy numpy torch pytest lit pandas matplotlib &amp;amp;&amp;amp; pip install -e python&#xA;&#xA;# Run Python tests using your local GPU.&#xA;$ python3 -m pytest python/test/unit&#xA;&#xA;# Move to builddir.  Fill in &amp;lt;...&amp;gt; with the full path, e.g.&#xA;# `cmake.linux-x86_64-cpython-3.11`.&#xA;$ cd python/build/cmake&amp;lt;...&amp;gt;&#xA;&#xA;# Run C++ unit tests.&#xA;$ ninja test&#xA;&#xA;# Run lit tests.&#xA;$ lit test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may find it helpful to make a symlink to the builddir and tell your local git to ignore it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ln -s python/build/cmake&amp;lt;...&amp;gt; build&#xA;$ echo build &amp;gt;&amp;gt; .git/info/exclude&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can e.g. rebuild and run lit with the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ninja -C build &amp;amp;&amp;amp; ( cd build ; lit test )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Tips for hacking&lt;/h1&gt; &#xA;&lt;p&gt;For detailed instructions on how to debug Triton&#39;s frontend, please refer to this &lt;a href=&#34;https://triton-lang.org/main/programming-guide/chapter-3/debugging.html&#34;&gt;tutorial&lt;/a&gt;. The following includes additional tips for hacking on Triton&#39;s backend.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Helpful environment variables&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;MLIR_ENABLE_DUMP=1&lt;/code&gt; dumps the IR before every MLIR pass Triton runs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LLVM_IR_ENABLE_DUMP=1&lt;/code&gt; dumps the IR before every pass run over the LLVM IR.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;TRITON_INTERPRET=1&lt;/code&gt; uses the Triton interpreter instead of running on the GPU. You can insert Python breakpoints in your kernel code!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;TRITON_ENABLE_LLVM_DEBUG=1&lt;/code&gt; passes &lt;code&gt;-debug&lt;/code&gt; to LLVM, printing a lot of debugging information to stdout. If this is too noisy, run with just &lt;code&gt;TRITON_LLVM_DEBUG_ONLY&lt;/code&gt; instead to limit the output.&lt;/p&gt; &lt;p&gt;An alternative way to reduce output noisiness is running with &lt;code&gt;LLVM_IR_ENABLE_DUMP=1&lt;/code&gt;, extract the IR before the LLVM pass of interest, and then run LLVM&#39;s &lt;code&gt;opt&lt;/code&gt; standalone, perhaps passing &lt;code&gt;-debug-only=foo&lt;/code&gt; on the command line.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;TRITON_LLVM_DEBUG_ONLY=&amp;lt;comma-separated&amp;gt;&lt;/code&gt; is the equivalent of LLVM&#39;s &lt;code&gt;-debug-only&lt;/code&gt; command-line option. This limits the LLVM debug output to specific pass or component names (which are specified using &lt;code&gt;#define DEBUG_TYPE&lt;/code&gt; throughout LLVM and Triton) in order to allow the debug output to be less noisy. &lt;code&gt;TRITON_LLVM_DEBUG_ONLY&lt;/code&gt; allows for one or more comma separated values to be specified (eg &lt;code&gt;TRITON_LLVM_DEBUG_ONLY=&#34;tritongpu-remove-layout-conversions&lt;/code&gt; or &lt;code&gt;TRITON_LLVM_DEBUG_ONLY=&#34;tritongpu-remove-layout-conversions,regalloc&#34;&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;USE_TTGIR_LOC=1&lt;/code&gt; reparses the ttgir such that the location information will be the line number of the ttgir instead of line number of the python file. This can provide a direct mapping from ttgir to llir/ptx. When used with performance tools, it can provide a breakdown on ttgir instructions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;TRITON_PRINT_AUTOTUNING=1&lt;/code&gt; prints out the best autotuning config and total time spent for each kernel after autotuning is complete.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;DISABLE_LLVM_OPT&lt;/code&gt; will disable llvm optimizations for make_llir and make_ptx if its value is true when parsing as Bool. Otherwise, it will be parsed as a list of flags to disable llvm optimizations. One usage case is &lt;code&gt;DISABLE_LLVM_OPT=&#34;disable-lsr&#34;&lt;/code&gt; Loop strength reduction is known to cause up to 10% performance changes for certain kernels with register pressure.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;TRITON_ALWAYS_COMPILE=1&lt;/code&gt; forces to compile kernels regardless of cache hit.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;MLIR_ENABLE_TIMING&lt;/code&gt; dumps the timing information for each MLIR pass.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LLVM_ENABLE_TIMING&lt;/code&gt; dumps the timing information for each LLVM pass.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Changelog&lt;/h1&gt; &#xA;&lt;p&gt;Version 2.0 is out! New features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many, many bug fixes&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA; &lt;li&gt;Backend rewritten to use MLIR&lt;/li&gt; &#xA; &lt;li&gt;Support for kernels that contain back-to-back matmuls (e.g., flash attention)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Community contributions are more than welcome, whether it be to fix bugs or to add new features at &lt;a href=&#34;https://github.com/triton-lang/triton/&#34;&gt;github&lt;/a&gt;. For more detailed instructions, please visit our &lt;a href=&#34;https://raw.githubusercontent.com/triton-lang/triton/main/CONTRIBUTING.md&#34;&gt;contributor&#39;s guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Compatibility&lt;/h1&gt; &#xA;&lt;p&gt;Supported Platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported Hardware:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NVIDIA GPUs (Compute Capability 7.0+)&lt;/li&gt; &#xA; &lt;li&gt;Under development: AMD GPUs, CPUs&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>