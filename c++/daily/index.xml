<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-21T01:25:14Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>leejet/stable-diffusion.cpp</title>
    <updated>2023-08-21T01:25:14Z</updated>
    <id>tag:github.com,2023-08-21:/leejet/stable-diffusion.cpp</id>
    <link href="https://github.com/leejet/stable-diffusion.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion in pure C/C++&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/a%20lovely%20cat.png&#34; width=&#34;256x&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;stable-diffusion.cpp&lt;/h1&gt; &#xA;&lt;p&gt;Inference of &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; in pure C/C++&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation based on &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;, working in the same way as &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16-bit, 32-bit float support&lt;/li&gt; &#xA; &lt;li&gt;4-bit, 5-bit and 8-bit integer quantization support&lt;/li&gt; &#xA; &lt;li&gt;Accelerated memory-efficient CPU inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Only requires ~2.3GB when using txt2img with fp16 precision to generate a 512x512 image&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;AVX, AVX2 and AVX512 support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;Original &lt;code&gt;txt2img&lt;/code&gt; and &lt;code&gt;img2img&lt;/code&gt; mode&lt;/li&gt; &#xA; &lt;li&gt;Negative prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt; style tokenizer (not all the features, only token weighting for now)&lt;/li&gt; &#xA; &lt;li&gt;Sampling method &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Euler A&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Supported platforms &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux&lt;/li&gt; &#xA;   &lt;li&gt;Mac OS&lt;/li&gt; &#xA;   &lt;li&gt;Windows&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TODO&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More sampling methods&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GPU support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make inference faster &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The current implementation of ggml_conv_2d is slow and has high memory usage&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Continuing to reduce memory usage (quantizing the weights of ggml_conv_2d)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LoRA support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; k-quants support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cross-platform reproducibility (perhaps ensuring consistency with the original SD)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/leejet/stable-diffusion.cpp&#xA;cd stable-diffusion.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have already cloned the repository, you can use the following command to update the repository to the latest code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd stable-diffusion.cpp&#xA;git pull origin master&#xA;git submodule update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Convert weights&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;download original weights(.ckpt or .safetensors). For example&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Stable Diffusion v1.4 from &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&#34;&gt;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Stable Diffusion v1.5 from &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;https://huggingface.co/runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -L -O https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt&#xA;# curl -L -O https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;convert weights to ggml model format&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd models&#xA;pip install -r requirements.txt&#xA;python convert.py [path to weights] --out_type [output precision]&#xA;# For example, python convert.py sd-v1-4.ckpt --out_type f16&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can specify the output model format using the --out_type parameter&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;f16&lt;/code&gt; for 16-bit floating-point&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;f32&lt;/code&gt; for 32-bit floating-point&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q8_0&lt;/code&gt; for 8-bit integer quantization&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q5_0&lt;/code&gt; or &lt;code&gt;q5_1&lt;/code&gt; for 5-bit integer quantization&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q4_0&lt;/code&gt; or &lt;code&gt;q4_1&lt;/code&gt; for 4-bit integer quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using OpenBLAS&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake .. -DGGML_OPENBLAS=ON&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: ./bin/sd [arguments]&#xA;&#xA;arguments:&#xA;  -h, --help                         show this help message and exit&#xA;  -M, --mode [txt2img or img2img]    generation mode (default: txt2img)&#xA;  -t, --threads N                    number of threads to use during computation (default: -1).&#xA;                                     If threads &amp;lt;= 0, then threads will be set to the number of CPU physical cores&#xA;  -m, --model [MODEL]                path to model&#xA;  -i, --init-img [IMAGE]             path to the input image, required by img2img&#xA;  -o, --output OUTPUT                path to write result image to (default: .\output.png)&#xA;  -p, --prompt [PROMPT]              the prompt to render&#xA;  -n, --negative-prompt PROMPT       the negative prompt (default: &#34;&#34;)&#xA;  --cfg-scale SCALE                  unconditional guidance scale: (default: 7.0)&#xA;  --strength STRENGTH                strength for noising/unnoising (default: 0.75)&#xA;                                     1.0 corresponds to full destruction of information in init image&#xA;  -H, --height H                     image height, in pixel space (default: 512)&#xA;  -W, --width W                      image width, in pixel space (default: 512)&#xA;  --sample-method SAMPLE_METHOD      sample method (default: &#34;eular a&#34;)&#xA;  --steps  STEPS                     number of sample steps (default: 20)&#xA;  -s SEED, --seed SEED               RNG seed (default: 42, use random seed for &amp;lt; 0)&#xA;  -v, --verbose                      print extra info&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;txt2img example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;./bin/sd -m ../models/sd-v1-4-ggml-model-f16.bin -p &#34;a lovely cat&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using formats of different precisions will yield results of varying quality.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;f32&lt;/th&gt; &#xA;   &lt;th&gt;f16&lt;/th&gt; &#xA;   &lt;th&gt;q8_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_1&lt;/th&gt; &#xA;   &lt;th&gt;q4_0&lt;/th&gt; &#xA;   &lt;th&gt;q4_1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/f32.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/f16.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q8_0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q5_0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q5_1.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q4_0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q4_1.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;img2img example&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;./output.png&lt;/code&gt; is the image generated from the above txt2img pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;./bin/sd --mode img2img -m ../models/sd-v1-4-ggml-model-f16.bin -p &#34;cat with blue eyes&#34; -i ./output.png -o ./img2img_output.png --strength 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/img2img_output.png&#34; width=&#34;256x&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Memory/Disk Requirements&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;f32&lt;/th&gt; &#xA;   &lt;th&gt;f16&lt;/th&gt; &#xA;   &lt;th&gt;q8_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_1&lt;/th&gt; &#xA;   &lt;th&gt;q4_0&lt;/th&gt; &#xA;   &lt;th&gt;q4_1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Disk&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.7G&lt;/td&gt; &#xA;   &lt;td&gt;2.0G&lt;/td&gt; &#xA;   &lt;td&gt;1.7G&lt;/td&gt; &#xA;   &lt;td&gt;1.6G&lt;/td&gt; &#xA;   &lt;td&gt;1.6G&lt;/td&gt; &#xA;   &lt;td&gt;1.5G&lt;/td&gt; &#xA;   &lt;td&gt;1.5G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Memory&lt;/strong&gt;(txt2img - 512 x 512)&lt;/td&gt; &#xA;   &lt;td&gt;~2.8G&lt;/td&gt; &#xA;   &lt;td&gt;~2.3G&lt;/td&gt; &#xA;   &lt;td&gt;~2.1G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;k-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>RuanJY/SLAMesh</title>
    <updated>2023-08-21T01:25:14Z</updated>
    <id>tag:github.com,2023-08-21:/RuanJY/SLAMesh</id>
    <link href="https://github.com/RuanJY/SLAMesh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code of SLAMesh, a real-time LiDAR simultaneous localization and meshing method.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SLAMesh&lt;/h1&gt; &#xA;&lt;h2&gt;Real-time LiDAR Simultaneous Localization and Meshing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personal repository&lt;/strong&gt; of our work SLAMesh, please raise issues &lt;strong&gt;here&lt;/strong&gt; so that I can get reminders immediately. The code may also more friendly to use and read, but I don&#39;t always guarantee it can replicate the result in out paper (now it can).&lt;/p&gt; &#xA;&lt;p&gt;We are confident that this work introduced a novel approach to LiDAR SLAM, so we publish the code to benefit robotics society and welcome everyone to explore opportunities in this approach. &lt;span&gt;👬&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;You are also welcome to visit the &lt;a href=&#34;https://github.com/lab-sun/SLAMesh&#34;&gt;repo in our lab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Update&lt;/h3&gt; &#xA;&lt;p&gt;17/Aug/2023, Code released. Spent some time to make my code more user-friendly &lt;span&gt;😤&lt;/span&gt;, feel free to contact me with any questions.&lt;/p&gt; &#xA;&lt;p&gt;10/Mar/2023, Preprint of our paper can be found on: &lt;a href=&#34;https://arxiv.org/pdf/2303.05252.pdf&#34; title=&#34;title text&#34;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;16/Jan/2023, The paper has been accepted for presentation on &lt;strong&gt;ICRA 2023&lt;/strong&gt;,&lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This work designs a &lt;strong&gt;S&lt;/strong&gt;imultaneously &lt;strong&gt;L&lt;/strong&gt;ocalization &lt;strong&gt;A&lt;/strong&gt;nd &lt;strong&gt;Mesh&lt;/strong&gt;ing system (SLAMesh). Mesh is a lightweight 3-D dense model. It can model complex structures and has the feasibility for rendering. We bridge localization with meshing at the same time to benefit each other.&lt;/p&gt; &#xA;&lt;h3&gt;1.1 Main features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build, register, and update the mesh maps in real time with CPU resources. The experiments show that our SLAMesh can run at around 40 Hz.&lt;/li&gt; &#xA; &lt;li&gt;Provide accurate odometry. The localization and meshing accuracy also outperforms the state-of-the-art methods.&lt;/li&gt; &#xA; &lt;li&gt;Different from point-cloud (LOAM), NDT, and Surfel map SLAM, this work has established a new approach to LiDAR SLAM.&lt;/li&gt; &#xA; &lt;li&gt;The key idea is that we conduct a reconstruction of the raw point cloud before registration. This strategy enables fast meshing, data-association without the kd-tree.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://github.com/RuanJY/SLAMesh/raw/master/fig/fig1_mesh_by_slamesh.png&#34; alt=&#34;cover&#34; width=&#34;60%&#34;&gt; &#xA;&lt;p&gt;Author: Jianyuan Ruan, Bo Li, Yibo Wang, Yuxiang Sun.&lt;/p&gt; &#xA;&lt;h3&gt;1.2 Demo video&lt;/h3&gt; &#xA;&lt;p&gt;On public dataset:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bm9u0-d4giw&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/RuanJY/SLAMesh/raw/master/fig/cover.png&#34; alt=&#34;video1&#34; width=&#34;85%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(or watch it on &lt;a href=&#34;https://www.bilibili.com/video/BV1HB4y1J7k3/?vd_source=a7075e8cce0b5d3273610c2b2539377d&#34;&gt;bilibili&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;On self-collected dataset:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-zMNndGmUho&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/RuanJY/SLAMesh/raw/master/fig/real_word_cover2.png&#34; alt=&#34;video2&#34; width=&#34;85%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(or watch it on &lt;a href=&#34;https://www.bilibili.com/video/BV1u84y1h7g3/?vd_source=a7075e8cce0b5d3273610c2b2539377d&#34;&gt;bilibili&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h3&gt;1.3 Find more detail&lt;/h3&gt; &#xA;&lt;p&gt;If you find our research helpful, please cite our &lt;a href=&#34;https://arxiv.org/pdf/2303.05252.pdf&#34; title=&#34;title text&#34;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt;. :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[1] Jianyuan Ruan, Bo Li, Yibo Wang, and Yuxiang Sun, &#34;SLAMesh: Real-time&#xA;LiDAR Simultaneous Localization and Meshing&#34; ICRA 2023. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other related papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[2] Jianyuan Ruan, Bo Li, Yinqiang Wang and Zhou Fang, &#34;GP-SLAM+: real-time&#xA;3D lidar SLAM based on improved regionalized Gaussian process map&#xA;reconstruction,&#34; IROS 2020.&#xA;&#xA;[3] Bo Li, Yinqiang Wang, Yu Zhang. Wenjie Zhao, Jianyuan Ruan, and Pin Li,&#xA;&#34;GP-SLAM: laser-based SLAM approach based on regionalized Gaussian process&#xA;map reconstruction&#34;. Auton Robot 2020.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you understand Chinese, you can also refer to my &lt;a href=&#34;https://www.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;amp;dbname=CMFD202101&amp;amp;filename=1020413366.nh&amp;amp;uniplatform=OVERSEA&amp;amp;v=bau761Q363a22-250-r1zodKdsRM2f8qd7_GsHgUJXZ6Fyy2BkPgtk4nClfy_MyQ&#34;&gt;Master&#39;s thesis&lt;/a&gt; and an article on the WeChat platform: &lt;a href=&#34;https://mp.weixin.qq.com/s/zYORZ1sOVkh-UnPkzzfh_g&#34;&gt;SLAMesh: 实时LiDAR定位与网格化模型构建 &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2. Install&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 Prerequisite&lt;/h3&gt; &#xA;&lt;p&gt;We tested our code in &lt;em&gt;Ubuntu18.04&lt;/em&gt; with &lt;em&gt;ROS melodic&lt;/em&gt; and &lt;em&gt;Ubuntu20.04&lt;/em&gt; with &lt;em&gt;ROS neotic&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ROS&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install ros following &lt;a href=&#34;http://wiki.ros.org/melodic/Installation/Ubuntu&#34;&gt;ROS Installation&lt;/a&gt;. We use the PCL and Eigen library in ROS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ceres&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Ceres Solver, version &amp;gt; 2.0. follow &lt;a href=&#34;http://ceres-solver.org/installation.html&#34;&gt;Ceres Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;mesh_tools&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We use mesh_tools to visualize the mesh map with the &lt;code&gt;mesh_msgs::MeshGeometryStamped&lt;/code&gt; ROS message. mesh_tools also incorporates navigation functions upon mesh map. &lt;a href=&#34;https://github.com/naturerobots/mesh_tools&#34;&gt;Mesh tool introduction&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install mesh_tools by:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://github.com/uos/lvr2&#34;&gt;lvr2&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install build-essential \&#xA;     cmake cmake-curses-gui libflann-dev \&#xA;     libgsl-dev libeigen3-dev libopenmpi-dev \&#xA;     openmpi-bin opencl-c-headers ocl-icd-opencl-dev \&#xA;     libboost-all-dev \&#xA;     freeglut3-dev libhdf5-dev qtbase5-dev \&#xA;     qt5-default libqt5opengl5-dev liblz4-dev \&#xA;     libopencv-dev libyaml-cpp-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Ubuntu18.04, use &lt;code&gt;libvtk6&lt;/code&gt; because &lt;code&gt;libvtk7&lt;/code&gt; will conflict with &lt;code&gt;pcl-ros&lt;/code&gt; in melodic.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install  libvtk6-dev libvtk6-qt-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Ubuntu 20.04,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install  libvtk7-dev libvtk7-qt-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd a_non_ros_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/uos/lvr2.git&#xA;cd lvr2 &#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake .. &amp;amp;&amp;amp; make&#xA;sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It may take you some time.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install mesh_tools, (I can not install it from official ROS repos now, so I build it from source)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd slamesh_ws/src&#xA;git clone https://github.com/naturerobots/mesh_tools.git&#xA;cd ..&#xA;rosdep update&#xA;rosdep install --from-paths src --ignore-src -r -y&#xA;catkin_make&#xA;source devel/setup.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.2 SLAMesh&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd slamesh_ws/src&#xA;git clone https://github.com/RuanJY/SLAMesh.git&#xA;cd ..&#xA;catkin_make&#xA;source ~/slamesh_ws/src/devel/setup.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.3 Docker support&lt;/h3&gt; &#xA;&lt;p&gt;If you encounter some trouble with prerequisites, the problem may lay down on the prerequisite; we advise you to use our docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull pleaserun/rjy_slam_work:slamesh_18.04&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After cloning the image, remember to run it with correct path of dataset via option &lt;code&gt;-v&lt;/code&gt;, like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -it -p 5900:5900 -p 2222:22 -e RESOLUTION=1920x1080  \&#xA;-v path_in_your_PC:/root/dataset \&#xA;--name test_slamesh \&#xA;pleaserun/rjy_slam_work:slamesh_18.04&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use VNC to enter a graphical interface via port 5900, or use ssh to connect container via port 2222.&lt;/p&gt; &#xA;&lt;p&gt;Move to the dictionary &lt;code&gt;slamesh_ws/src&lt;/code&gt;, and complete step after 2.1.&lt;/p&gt; &#xA;&lt;h2&gt;3. Usage&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 Run kitti dataset:&lt;/h3&gt; &#xA;&lt;p&gt;The dataset is available at &lt;a href=&#34;https://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;KITTI dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Set the parameter &lt;code&gt;data_path&lt;/code&gt; in &lt;code&gt;slamesh_kitti.launch&lt;/code&gt; to your folder of kitti dataset path&lt;/p&gt; &#xA;&lt;p&gt;The file tree should be like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;file_loc_dataset&#xA;    ├── 00&#xA;    |   └──velodyne&#xA;    |       ├── 000000.bin&#xA;    |       └── ...&#xA;    └──01   &#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, if you want to run the 07 sequence:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch slamesh slamesh_kitti_meshing.launch seq:=/07&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should get:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/RuanJY/SLAMesh/raw/master/fig/kitti07_mesh.png&#34; alt=&#34;kitti07_mesh&#34; width=&#34;60%&#34;&gt; &#xA;&lt;p&gt;If you can not see the mesh, check that the Rviz plugin is sourced correctly. When &lt;code&gt;mesh_visualization&lt;/code&gt; is disabled, only vertices are published as a point cloud.&lt;/p&gt; &#xA;&lt;h3&gt;3.2 Run Mai City dataset:&lt;/h3&gt; &#xA;&lt;p&gt;The dataset is available at &lt;a href=&#34;https://www.ipb.uni-bonn.de/data/mai-city-dataset/&#34;&gt;Mai City Dataset&lt;/a&gt;. Sequence 01 can be fed into SLAM and sequence 02 can be accumulated into a dense ground truth point cloud map.&lt;/p&gt; &#xA;&lt;p&gt;Similarly, set the parameter &lt;code&gt;data_path&lt;/code&gt; in &lt;code&gt;slamesh_maicity.launch&lt;/code&gt; to your folder of the kitti dataset path.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch slamesh slamesh_maicity.launch seq:=/01&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should get:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/RuanJY/SLAMesh/raw/master/fig/maicity_mesh.png&#34; alt=&#34;maicity_mesh&#34; width=&#34;60%&#34;&gt; &#xA;&lt;h3&gt;3.3 Run online or ros bag&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch slamesh slamesh_online.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And play your bag, in launch file, remap the topic &#34;/velodyne_points&#34; to your LiDAR topic like &#34;/os_cloud_node/points&#34;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosbag play your_bag.bag &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The number of LiDAR channels does not matter because our algorithm does not extract features.&lt;/p&gt; &#xA;&lt;p&gt;You can use our sample data recorded with an Ouster OS1-32 LiDAR: &lt;a href=&#34;https://connectpolyu-my.sharepoint.com/:f:/g/personal/21041552r_connect_polyu_hk/EjhEKl8-1GBLseA_2F7TOvEB3w7OyAJ_kS7DAaWoLay9ng?e=GK1fsd&#34;&gt;SLAMesh dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;SLAMesh saves all its report to the path &lt;code&gt;result_path&lt;/code&gt;. If you find ros warning: &lt;code&gt; Can not open Report file&lt;/code&gt;, create the folder of &lt;code&gt;result_path&lt;/code&gt; first, or just follow the steps below.&lt;/p&gt; &#xA;&lt;h3&gt;4.1 Kitti odometry accuracy&lt;/h3&gt; &#xA;&lt;p&gt;The file &lt;code&gt;0x_pred.txt&lt;/code&gt; is the KITTI format path. I use &lt;a href=&#34;https://github.com/LeoQLi/KITTI_odometry_evaluation_tool&#34;&gt;KITTI odometry evaluation tool &lt;/a&gt; for evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd slamesh_ws&#xA;mkdir result &amp;amp;&amp;amp; cd result&#xA;git clone https://github.com/LeoQLi/KITTI_odometry_evaluation_tool&#xA;cd KITTI_odometry_evaluation_tool/&#xA;python evaluation.py --result_dir=./data/ --eva_seqs=07.pred&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run SLAMesh by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch slamesh slamesh_kitti_odometry.launch seq:=/07&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, the result on the KITTI odometry benchmark is:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Sequence&lt;/th&gt; &#xA;   &lt;th&gt;00&lt;/th&gt; &#xA;   &lt;th&gt;01&lt;/th&gt; &#xA;   &lt;th&gt;02&lt;/th&gt; &#xA;   &lt;th&gt;03&lt;/th&gt; &#xA;   &lt;th&gt;04&lt;/th&gt; &#xA;   &lt;th&gt;05&lt;/th&gt; &#xA;   &lt;th&gt;06&lt;/th&gt; &#xA;   &lt;th&gt;07&lt;/th&gt; &#xA;   &lt;th&gt;08&lt;/th&gt; &#xA;   &lt;th&gt;09&lt;/th&gt; &#xA;   &lt;th&gt;10&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Translation (%)&lt;/td&gt; &#xA;   &lt;td&gt;0.771&lt;/td&gt; &#xA;   &lt;td&gt;1.2519&lt;/td&gt; &#xA;   &lt;td&gt;0.7742&lt;/td&gt; &#xA;   &lt;td&gt;0.6366&lt;/td&gt; &#xA;   &lt;td&gt;0.5044&lt;/td&gt; &#xA;   &lt;td&gt;0.5182&lt;/td&gt; &#xA;   &lt;td&gt;0.5294&lt;/td&gt; &#xA;   &lt;td&gt;0.3607&lt;/td&gt; &#xA;   &lt;td&gt;0.8745&lt;/td&gt; &#xA;   &lt;td&gt;0.573&lt;/td&gt; &#xA;   &lt;td&gt;0.6455&lt;/td&gt; &#xA;   &lt;td&gt;0.6763&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rotation (deg/m)&lt;/td&gt; &#xA;   &lt;td&gt;0.0035&lt;/td&gt; &#xA;   &lt;td&gt;0.003&lt;/td&gt; &#xA;   &lt;td&gt;0.003&lt;/td&gt; &#xA;   &lt;td&gt;0.0043&lt;/td&gt; &#xA;   &lt;td&gt;0.0013&lt;/td&gt; &#xA;   &lt;td&gt;0.003&lt;/td&gt; &#xA;   &lt;td&gt;0.0022&lt;/td&gt; &#xA;   &lt;td&gt;0.0023&lt;/td&gt; &#xA;   &lt;td&gt;0.0027&lt;/td&gt; &#xA;   &lt;td&gt;0.0025&lt;/td&gt; &#xA;   &lt;td&gt;0.0042&lt;/td&gt; &#xA;   &lt;td&gt;0.0029&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Notice that to achieve better KITTI odometry performance, the parameter in &lt;code&gt;slamesh_kitti_meshing.launch&lt;/code&gt; are set as followed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;full_cover: false # due to discontinuity phenomenon between cells, shirnk the test locations can improve accuracy.&#xA;num_margin_old_cell: 500 # margin old cells, because KITTI evaluate odometry accuracy rather than consistency.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, if you want to have better meshing result, they should be: (in launch &lt;code&gt;slamesh_kitti_meshing.launch&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;full_cover: true # so that there is no gap between cells.&#xA;num_margin_old_cell: -1  # do not margin old cells, the cell-based map will have implicit loop closure effect.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4.2 Mesh quality&lt;/h3&gt; &#xA;&lt;p&gt;I use the &lt;code&gt;TanksAndTemples/evaluation&lt;/code&gt; tool to evaluate the mesh. I slightly modify it (remove trajectory). You can find it here: &lt;a href=&#34;https://github.com/RuanJY/TanksAndTemples&#34;&gt;TanksAndTemples/evaluation_rjy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then compare the mesh with the ground-truth point cloud map:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd TanksAndTemples_direct_rjy/python_toolbox/evaluation/&#xA;python run.py \&#xA;--dataset-dir ./data/ground_truth_point_cloud.ply \&#xA;--traj-path ./ \&#xA;--ply-path ./data/your_mesh.ply&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4.3 Time cost&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;***_report.txt&lt;/code&gt; record time cost and other logs.&lt;/p&gt; &#xA;&lt;h2&gt;5. Help you to read the code&lt;/h2&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Author: Jianyuan Ruan, Bo Li, Yibo Wang, Yuxiang Sun.&lt;/p&gt; &#xA;&lt;p&gt;Email: &lt;a href=&#34;mailto:jianyuan.ruan@connect.polyu.hk&#34;&gt;jianyuan.ruan@connect.polyu.hk&lt;/a&gt;, &lt;a href=&#34;mailto:120052@zust.edu.cn&#34;&gt;120052@zust.edu.cn&lt;/a&gt;, &lt;a href=&#34;mailto:me-yibo.wang@connect.polyu.hk&#34;&gt;me-yibo.wang@connect.polyu.hk&lt;/a&gt;, &lt;a href=&#34;mailto:yx.sun@polyu.edu.hk&#34;&gt;yx.sun@polyu.edu.hk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Most of the code are build from scratch , but we also want to acknowledge the following open-source projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/isl-org/TanksAndTemples/tree/master/python_toolbox/evaluation&#34;&gt;TanksAndTemples/evaluation&lt;/a&gt;: evaluation&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/A-LOAM&#34;&gt;A-LOAM&lt;/a&gt;: kitti dataset loader and tic-toc&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wh200720041/floam&#34;&gt;F-LOAM&lt;/a&gt;: help me to write the ceres residuals&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>flexflow/FlexFlow</title>
    <updated>2023-08-21T01:25:14Z</updated>
    <id>tag:github.com,2023-08-21:/flexflow/FlexFlow</id>
    <link href="https://github.com/flexflow/FlexFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A distributed deep learning framework.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlexFlow Serve: Low-Latency, High-Performance LLM Serving&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/build/badge.svg?branch=inference&#34; alt=&#34;build&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/gpu-ci/badge.svg?branch=inference&#34; alt=&#34;gpu tests&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/multinode-test/badge.svg?branch=master&#34; alt=&#34;multinode gpu tests&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/docker-build/badge.svg?branch=inference&#34; alt=&#34;docker&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/pip-install/badge.svg?branch=inference&#34; alt=&#34;pip&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/Shell%20Check/badge.svg?branch=inference&#34; alt=&#34;shell-check&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/flexflow/workflows/clang-format%20Check/badge.svg?branch=inference&#34; alt=&#34;clang-format&#34;&gt; &lt;a href=&#34;https://flexflow.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/flexflow/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;News🔥:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[08/16/2023] Adding Starcoder model support&lt;/li&gt; &#xA; &lt;li&gt;[08/14/2023] Released Dockerfile for different CUDA versions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is FlexFlow Serve&lt;/h2&gt; &#xA;&lt;p&gt;The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. FlexFlow Serve is an open-source compiler and distributed system for &lt;strong&gt;low latency&lt;/strong&gt;, &lt;strong&gt;high performance&lt;/strong&gt; LLM serving. FlexFlow Serve outperforms existing systems by 1.3-2.0x for single-node, multi-GPU inference and by 1.4-2.4x for multi-node, multi-GPU inference.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/FlexFlow/raw/inference/img/performance.png?raw=true&#34; alt=&#34;Performance comparison&#34; height=&#34;320&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Install FlexFlow Serve&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OS: Linux&lt;/li&gt; &#xA; &lt;li&gt;GPU backend: Hip-ROCm or CUDA &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CUDA version: 10.2 – 12.0&lt;/li&gt; &#xA;   &lt;li&gt;NVIDIA compute capability: 6.0 or higher&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Python: 3.6 or higher&lt;/li&gt; &#xA; &lt;li&gt;Package dependencies: &lt;a href=&#34;https://github.com/flexflow/FlexFlow/raw/inference/requirements.txt&#34;&gt;see here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install with pip&lt;/h3&gt; &#xA;&lt;p&gt;You can install FlexFlow Serve using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install flexflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Try it in Docker&lt;/h3&gt; &#xA;&lt;p&gt;If you run into any issue during the install, or if you would like to use the C++ API without needing to install from source, you can also use our pre-built Docker package for different CUDA versions and the &lt;code&gt;hip_rocm&lt;/code&gt; backend. To download and run our pre-built Docker container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all -it --rm --shm-size=8g ghcr.io/flexflow/flexflow-cuda-11.8:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download a Docker container for a backend other than CUDA v11.8, you can replace the &lt;code&gt;cuda-11.8&lt;/code&gt; suffix with any of the following backends: &lt;code&gt;cuda-11.1&lt;/code&gt;, &lt;code&gt;cuda-11.2&lt;/code&gt;, &lt;code&gt;cuda-11.3&lt;/code&gt;, &lt;code&gt;cuda-11.5&lt;/code&gt;, &lt;code&gt;cuda-11.6&lt;/code&gt;, &lt;code&gt;cuda-11.7&lt;/code&gt;, &lt;code&gt;cuda-11.8&lt;/code&gt;, and &lt;code&gt;hip_rocm&lt;/code&gt;). More info on the Docker images, with instructions to build a new image from source, or run with additional configurations, can be found &lt;a href=&#34;https://raw.githubusercontent.com/flexflow/FlexFlow/docker/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;p&gt;You can install FlexFlow Serve from source code by building the inference branch of FlexFlow. Please follow these &lt;a href=&#34;https://flexflow.readthedocs.io/en/latest/installation.html&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The following example shows how to deploy an LLM using FlexFlow Serve and accelerate its serving using &lt;a href=&#34;https://raw.githubusercontent.com/flexflow/FlexFlow/inference/#speculative-inference&#34;&gt;speculative inference&lt;/a&gt;. First, we import &lt;code&gt;flexflow.serve&lt;/code&gt; and initialize the FlexFlow Serve runtime. Note that &lt;code&gt;memory_per_gpu&lt;/code&gt; and &lt;code&gt;zero_copy_memory_per_node&lt;/code&gt; specify the size of device memory on each GPU (in MB) and zero-copy memory on each node (in MB), respectively. We need to make sure the aggregated GPU memory and zero-copy memory are &lt;strong&gt;both&lt;/strong&gt; sufficient to store LLM parameters in non-offloading serving. FlexFlow Serve combines tensor and pipeline model parallelism for LLM serving.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import flexflow.serve as ff&#xA;&#xA;ff.init(&#xA;        num_gpus=4,&#xA;        memory_per_gpu=14000,&#xA;        zero_copy_memory_per_node=30000,&#xA;        tensor_parallelism_degree=4,&#xA;        pipeline_parallelism_degree=1&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second, we specify the LLM to serve and the SSM(s) used to accelerate LLM serving. The list of supported LLMs and SSMs is available at &lt;a href=&#34;https://raw.githubusercontent.com/flexflow/FlexFlow/inference/#supported-llms-and-ssms&#34;&gt;supported models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specify the LLM&#xA;llm = ff.LLM(&#34;decapoda-research/llama-7b-hf&#34;)&#xA;&#xA;# Specify a list of SSMs (just one in this case)&#xA;ssms=[]&#xA;ssm = ff.SSM(&#34;JackFram/llama-68m&#34;)&#xA;ssms.append(ssm)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we declare the generation configuration and compile both the LLM and SSMs. Note that all SSMs should run in the &lt;strong&gt;beam search&lt;/strong&gt; mode, and the LLM should run in the &lt;strong&gt;tree verification&lt;/strong&gt; mode to verify the speculated tokens from SSMs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create the sampling configs&#xA;generation_config = ff.GenerationConfig(&#xA;    do_sample=False, temperature=0.9, topp=0.8, topk=1&#xA;)&#xA;&#xA;# Compile the SSMs for inference and load the weights into memory&#xA;for ssm in ssms:&#xA;    ssm.compile(generation_config)&#xA;&#xA;# Compile the LLM for inference and load the weights into memory&#xA;llm.compile(generation_config, ssms=ssms)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we call &lt;code&gt;llm.generate&lt;/code&gt; to generate the output, which is organized as a list of &lt;code&gt;GenerationResult&lt;/code&gt;, which include the output tokens and text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = llm.generate(&#34;Here are some travel tips for Tokyo:\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Incremental decoding&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Expand here&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import flexflow.serve as ff&#xA;&#xA;# Initialize the FlexFlow runtime. ff.init() takes a dictionary or the path to a JSON file with the configs&#xA;ff.init(&#xA;        num_gpus=4,&#xA;        memory_per_gpu=14000,&#xA;        zero_copy_memory_per_node=30000,&#xA;        tensor_parallelism_degree=4,&#xA;        pipeline_parallelism_degree=1&#xA;    )&#xA;&#xA;# Create the FlexFlow LLM&#xA;llm = ff.LLM(&#34;decapoda-research/llama-7b-hf&#34;)&#xA;&#xA;# Create the sampling configs&#xA;generation_config = ff.GenerationConfig(&#xA;    do_sample=True, temperature=0.9, topp=0.8, topk=1&#xA;)&#xA;&#xA;# Compile the LLM for inference and load the weights into memory&#xA;llm.compile(generation_config)&#xA;&#xA;# Generation begins!&#xA;result = llm.generate(&#34;Here are some travel tips for Tokyo:\n&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;C++ interface&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to use the C++ interface (mostly used for development and benchmarking purposes), you should install from source, and follow the instructions below.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Expand here&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;h4&gt;Downloading models&lt;/h4&gt; &#xA; &lt;p&gt;Before running FlexFlow Serve, you should manually download the LLM and SSM(s) model of interest using the &lt;a href=&#34;https://github.com/flexflow/FlexFlow/raw/inference/inference/utils/download_hf_model.py&#34;&gt;inference/utils/download_hf_model.py&lt;/a&gt; script (see example below). By default, the script will download all of a model&#39;s assets (weights, configs, tokenizer files, etc...) into the cache folder &lt;code&gt;~/.cache/flexflow&lt;/code&gt;. If you would like to use a different folder, you can request that via the parameter &lt;code&gt;--cache-folder&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 ./inference/utils/download_hf_model.py &amp;lt;HF model 1&amp;gt; &amp;lt;HF model 2&amp;gt; ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Running the C++ examples&lt;/h4&gt; &#xA; &lt;p&gt;A C++ example is available at &lt;a href=&#34;https://raw.githubusercontent.com/flexflow/FlexFlow/inference/spec_infer/&#34;&gt;this folder&lt;/a&gt;. After building FlexFlow Serve, the executable will be available at &lt;code&gt;/build_dir/inference/spec_infer/spec_infer&lt;/code&gt;. You can use the following command-line arguments to run FlexFlow Serve:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;-ll:gpu&lt;/code&gt;: number of GPU processors to use on each node for serving an LLM (default: 0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-ll:fsize&lt;/code&gt;: size of device memory on each GPU in MB&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-ll:zsize&lt;/code&gt;: size of zero-copy memory (pinned DRAM with direct GPU access) in MB. FlexFlow Serve keeps a replica of the LLM parameters on zero-copy memory, and therefore requires that the zero-copy memory is sufficient for storing the LLM parameters.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-llm-model&lt;/code&gt;: the LLM model ID from HuggingFace (e.g. &#34;decapoda-research/llama-7b-hf&#34;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-ssm-model&lt;/code&gt;: the SSM model ID from HuggingFace (e.g. &#34;JackFram/llama-160m&#34;). You can use multiple &lt;code&gt;-ssm-model&lt;/code&gt;s in the command line to launch multiple SSMs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-cache-folder&lt;/code&gt;: the folder&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-data-parallelism-degree&lt;/code&gt;, &lt;code&gt;-tensor-parallelism-degree&lt;/code&gt; and &lt;code&gt;-pipeline-parallelism-degree&lt;/code&gt;: parallelization degrees in the data, tensor, and pipeline dimensions. Their product must equal the number of GPUs available on the machine. When any of the three parallelism degree arguments is omitted, a default value of 1 will be used.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-prompt&lt;/code&gt;: (optional) path to the prompt file. FlexFlow Serve expects a json format file for prompts. In addition, users can also use the following API for registering requests:&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;-output-file&lt;/code&gt;: (optional) filepath to use to save the output of the model, together with the generation latency&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;For example, you can use the following command line to serve a LLaMA-7B or LLaMA-13B model on 4 GPUs and use two collectively boost-tuned LLaMA-68M models for speculative inference.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./inference/spec_infer/spec_infer -ll:gpu 4 -ll:fsize 14000 -ll:zsize 30000 -llm-model decapoda-research/llama-7b-hf -ssm-model JackFram/llama-68m -prompt /path/to/prompt.json -tensor-parallelism-degree 4 --fusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Speculative Inference&lt;/h2&gt; &#xA;&lt;p&gt;A key technique that enables FlexFlow Serve to accelerate LLM serving is speculative inference, which combines various collectively boost-tuned small speculative models (SSMs) to jointly predict the LLM’s outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM’s output in parallel using a novel tree-based parallel decoding mechanism. FlexFlow Serve uses an LLM as a token tree verifier instead of an incremental decoder, which largely reduces the end-to-end inference latency and computational requirement for serving generative LLMs while provably preserving model quality.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/flexflow/FlexFlow/raw/inference/img/spec_infer_demo.gif?raw=true&#34; alt=&#34;A Speculative Inference Demo&#34; width=&#34;630&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Supported LLMs and SSMs&lt;/h3&gt; &#xA;&lt;p&gt;FlexFlow Serve currently supports all HuggingFace models with the following architectures:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;LlamaForCausalLM&lt;/code&gt; / &lt;code&gt;LLaMAForCausalLM&lt;/code&gt; (e.g. LLaMA/LLaMA-2, Guanaco, Vicuna, Alpaca, ...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OPTForCausalLM&lt;/code&gt; (models from the OPT family)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RWForCausalLM&lt;/code&gt; (models from the Falcon family)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GPTBigCodeForCausalLM&lt;/code&gt; (models from the Starcoder family)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Below is a list of models that we have explicitly tested and for which a SSM may be available:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model id on HuggingFace&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Boost-tuned SSMs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;decapoda-research/llama-7b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;decapoda-research/llama-13b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-30B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;decapoda-research/llama-30b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-65B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;decapoda-research/llama-65b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;meta-llama/Llama-2-7b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;meta-llama/Llama-2-13b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-2-70B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;meta-llama/Llama-2-70b-hf&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/JackFram/llama-68m&#34;&gt;LLaMA-68M&lt;/a&gt; , &lt;a href=&#34;https://huggingface.co/JackFram/llama-160m&#34;&gt;LLaMA-160M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OPT-6.7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;facebook/opt-6.7b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-125m&#34;&gt;OPT-125M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OPT-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;facebook/opt-13b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-125m&#34;&gt;OPT-125M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OPT-30B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;facebook/opt-30b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-125m&#34;&gt;OPT-125M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OPT-66B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;facebook/opt-66b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-125m&#34;&gt;OPT-125M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Falcon-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;tiiuae/falcon-7b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Falcon-40B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;tiiuae/falcon-40b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StarCoder-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bigcode/starcoderbase-7b&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StarCoder-15.5B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bigcode/starcoder&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;CPU Offloading&lt;/h3&gt; &#xA;&lt;p&gt;FlexFlow Serve also offers offloading-based inference for running large models (e.g., llama-7B) on a single GPU. CPU offloading is a choice to save tensors in CPU memory, and only copy the tensor to GPU when doing calculation. Notice that now we selectively offload the largest weight tensors (weights tensor in Linear, Attention). Besides, since the small model occupies considerably less space, it it does not pose a bottleneck for GPU memory, the offloading will bring more runtime space and computational cost, so we only do the offloading for the large model. You can run the offloading example by enabling the &lt;code&gt;-offload&lt;/code&gt; and &lt;code&gt;-offload-reserve-space-size&lt;/code&gt; flags.&lt;/p&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;FlexFlow Serve supports int4 and int8 quantization. The compressed tensors are stored on the CPU side. Once copied to the GPU, these tensors undergo decompression and conversion back to their original precision. Please find the compressed weight files in our s3 bucket, or use &lt;a href=&#34;https://raw.githubusercontent.com/flexflow/FlexFlow/inference/utils/compress_llama_weights.py&#34;&gt;this script&lt;/a&gt; from &lt;a href=&#34;https://github.com/FMInference/FlexGen&#34;&gt;FlexGen&lt;/a&gt; project to do the compression manually.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We provide five prompt datasets for evaluating FlexFlow Serve: &lt;a href=&#34;https://specinfer.s3.us-east-2.amazonaws.com/prompts/chatbot.json&#34;&gt;Chatbot instruction prompts&lt;/a&gt;, &lt;a href=&#34;https://specinfer.s3.us-east-2.amazonaws.com/prompts/chatgpt.json&#34;&gt;ChatGPT Prompts&lt;/a&gt;, &lt;a href=&#34;https://specinfer.s3.us-east-2.amazonaws.com/prompts/webqa.json&#34;&gt;WebQA&lt;/a&gt;, &lt;a href=&#34;https://specinfer.s3.us-east-2.amazonaws.com/prompts/alpaca.json&#34;&gt;Alpaca&lt;/a&gt;, and &lt;a href=&#34;https://specinfer.s3.us-east-2.amazonaws.com/prompts/piqa.json&#34;&gt;PIQA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;p&gt;FlexFlow Serve is under active development. We currently focus on the following tasks and strongly welcome all contributions from bug fixes to new features and extensions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMD support. We are actively working on supporting FlexFlow Serve on AMD GPUs and welcome any contributions to this effort.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is initiated by members from CMU, Stanford, and UCSD. We will be continuing developing and supporting FlexFlow Serve.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;FlexFlow uses Apache License 2.0.&lt;/p&gt;</summary>
  </entry>
</feed>