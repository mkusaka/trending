<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-06T01:24:13Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>b4rtaz/distributed-llama</title>
    <updated>2024-03-06T01:24:13Z</updated>
    <id>tag:github.com,2024-03-06:/b4rtaz/distributed-llama</id>
    <link href="https://github.com/b4rtaz/distributed-llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run LLMs on weak devices or make powerful devices even more powerful by distributing the workload and dividing the RAM usage.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/.github/cover.png&#34; alt=&#34;Distributed Llama&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Distributed Llama&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/b4rtaz/distributed-llama/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/b4rtaz/distributed-llama/.github%2Fworkflows%2Fmain.yml?style=flat-square&#34; alt=&#34;GitHub Actions Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/mashape/apistatus.svg?style=flat-square&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/b4rtaz&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/b4rtaz.svg?style=social&#34; alt=&#34;X: b4rtaz&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run LLMs on weak devices or make powerful devices even more powerful by distributing the workload and dividing the RAM usage. This project proves that it&#39;s possible split the workload of LLMs across multiple devices and achieve a significant speedup. Distributed Llama allows you to run huge LLMs in-house. The project uses TCP sockets to synchronize the state. You can easily configure your AI cluster by using a home router.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/.github/8raspi.jpg&#34; width=&#34;50%&#34; alt=&#34;Distributed Llama running on 8 Raspberry Pi 4B devices&#34;&gt;&lt;br&gt; &lt;sub&gt;&lt;sup&gt;Distributed Llama running on 8 Raspberry Pi 4B devices&lt;/sup&gt;&lt;/sub&gt; &lt;/p&gt; &#xA;&lt;p&gt;This project was initiated based on the &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; repository. Big thanks to &lt;a href=&#34;https://github.com/karpathy&#34;&gt;@karpathy&lt;/a&gt; and other contributors. Most ARM optimizations come from the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;üìÉ &lt;a href=&#34;https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/report/report.pdf&#34;&gt;Read the report&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Known limitations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project is a proof of concept, it&#39;s not optimized for production usage.&lt;/li&gt; &#xA; &lt;li&gt;You can run Distributed Llama only on 1, 2, 4... 2^n devices.&lt;/li&gt; &#xA; &lt;li&gt;The project supports only the inference mode, the chat mode is not supported.&lt;/li&gt; &#xA; &lt;li&gt;Optimized for (weights format √ó buffer format): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ARM CPUs &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;‚úÖ F32 √ó F32&lt;/li&gt; &#xA;     &lt;li&gt;‚ùå F16 √ó F32&lt;/li&gt; &#xA;     &lt;li&gt;‚ùå Q40 √ó F32&lt;/li&gt; &#xA;     &lt;li&gt;‚úÖ Q40 √ó Q80&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;x86_64 AVX2 CPUs &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;‚ùå F32 √ó F32&lt;/li&gt; &#xA;     &lt;li&gt;‚ùå F16 √ó F32&lt;/li&gt; &#xA;     &lt;li&gt;‚ùå Q40 √ó F32&lt;/li&gt; &#xA;     &lt;li&gt;‚ö†Ô∏è Q40 √ó Q80 (partial optimization)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama 2 7B&lt;/li&gt; &#xA; &lt;li&gt;Llama 2 13B&lt;/li&gt; &#xA; &lt;li&gt;Llama 2 70B&lt;/li&gt; &#xA; &lt;li&gt;Llama 2 compatible models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;br&gt; The project is split up into two parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Root node&lt;/strong&gt; - it&#39;s responsible for loading the model and weights and forward them to workers. Also, it synchronizes the state of the neural network. The root node is also a worker, it processes own slice of the neural network.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Worker node&lt;/strong&gt; - it processes own slice of the neural network. It doesn&#39;t require any configuration related to the model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You always need the root node and you can add 2^n - 1 worker nodes to speed up the inference. The RAM usage of the neural network is split up across all nodes. The root node requires a bit more RAM than worker nodes.&lt;/p&gt; &#xA;&lt;h2&gt;üìä Measurements&lt;/h2&gt; &#xA;&lt;h3&gt;Average Single Token Generation Time&lt;/h3&gt; &#xA;&lt;p&gt;All tests below utilized Q40 weights and a Q80 buffer. The generation time encompasses the inference time, network transfer time, sampling time, and multi-thread synchronization time. Number of samples: 16.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raspberry Pi 4B 8 GB&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/b4rtaz/distributed-llama/main/.github/8raspi2.jpg&#34; width=&#34;35%&#34; alt=&#34;8 x Raspberry Pi 4B 8GB&#34;&gt;&lt;br&gt; &lt;sub&gt;&lt;sup&gt;8 x Raspberry Pi 4B 8GB&lt;/sup&gt;&lt;/sub&gt; &lt;/p&gt; &#xA;&lt;p&gt;All Raspberry Pi units were connected via Gigabit Ethernet to the TP-Link LS1008G Switch.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;1 x RasPi 4B 8 GB&lt;/th&gt; &#xA;   &lt;th&gt;2 x RasPi 4B 8 GB&lt;/th&gt; &#xA;   &lt;th&gt;4 x RasPi 4B 8 GB&lt;/th&gt; &#xA;   &lt;th&gt;8 x RasPi 4B 8 GB&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1312.50 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 1307.94 ms, T: 1.81 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;793.69 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 739.00 ms, T: 52.50 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;494.00 ms&lt;/strong&gt; üî• &lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 458.81 ms, T: 34.06 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;588.19 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 296.69 ms, T: 289.75 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sub&gt;&lt;sup&gt;Not enough RAM&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1497.19 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 1465.06 ms, T: 30.88 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;848.19 ms&lt;/strong&gt; üî•&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 746.88 ms, T: 99.50 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1114.88 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 460.8 ms, T: 652.88 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sub&gt;&lt;sup&gt;Not enough RAM&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sub&gt;&lt;sup&gt;Not enough RAM&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;sub&gt;&lt;sup&gt;Not enough RAM&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4842.81 ms&lt;/strong&gt; üî•&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 2121.94 ms, T: 2719.62 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;I - inference time of the root node, T - network transfer time&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raspberry Pi 5 8GB&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;1 x RasPi 5 8 GB&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;436.25 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 433.31 ms, T: 2.19 ms) by &lt;a href=&#34;https://github.com/b4rtaz/distributed-llama/issues/8#issuecomment-1913588926&#34;&gt;@segabor&lt;/a&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;I - inference time of the root node, T - network transfer time&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;x86_64 CPU Cloud Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;All tests below were conducted on c3d-highcpu-30 (30 vCPU, 15 core, 59 GB memory) VMs in Google Cloud. &lt;a href=&#34;https://github.com/b4rtaz/distributed-llama/discussions/9&#34;&gt;More details&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;1 x VM&lt;/th&gt; &#xA;   &lt;th&gt;2 x VM&lt;/th&gt; &#xA;   &lt;th&gt;4 x VM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;101.81 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 101.06 ms, T: 0.19 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;69.69 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 61.50 ms, T: 7.62 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;53.69 ms&lt;/strong&gt; üî•&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 40.25 ms, T: 12.81 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;184.19 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 182.88 ms, T: 0.69 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;115.38 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 107.12 ms, T: 7.81 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;86.81 ms&lt;/strong&gt; üî•&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 66.25 ms, T: 19.94 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;909.69 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 907.25 ms, T: 1.75 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;501.38 ms&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 475.50 ms, T: 25.00 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;293.06 ms&lt;/strong&gt; üî•&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(I: 264.00 ms, T: 28.50 ms)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;I - inference time of the root node, T - network transfer time&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Network Transfer for Generating Single Token&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;F32 Buffer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;2 devices&lt;/th&gt; &#xA;   &lt;th&gt;4 devices&lt;/th&gt; &#xA;   &lt;th&gt;8 devices&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4192 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 2224 kB, R: 1968 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;10656 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 7704 kB, R: 2952 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;22624 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 19180 kB, R: 3444 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6560 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 3480 kB, R: 3080 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;16680 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 12060 kB, R: 4620 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;35420 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 30030 kB, R: 5390 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;S - sent data from the root node to workers, R - received data by the root node from workers&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q80 Buffer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;2 devices&lt;/th&gt; &#xA;   &lt;th&gt;4 devices&lt;/th&gt; &#xA;   &lt;th&gt;8 devices&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1112 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 590 kB, R: 522 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2830 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 2046 kB, R: 784 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6008 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 5094 kB, R: 914 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1742 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 924 kB, R: 818 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4430 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 3203 kB, R: 1227 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;9407 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 7976 kB, R: 1431 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5525 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 3230 kB, R: 2295 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;14917 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 11475 kB, R: 3442 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;32873 kB&lt;/strong&gt;&lt;br&gt;&lt;sub&gt;&lt;sup&gt;(S: 28857 kB, R: 4016 kB)&lt;/sup&gt;&lt;/sub&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;S - sent data from the root node to workers, R - received data by the root node from workers&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üî® How to Convert Llama 2 Weights&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2&lt;/a&gt; weights from Meta. This project supports 7B, 13B and 70B models. This project doesn&#39;t support chat models.&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;llama-2-7b/params.json&lt;/code&gt; file and replace &lt;code&gt;&#34;vocab_size&#34;: -1&lt;/code&gt; to &lt;code&gt;&#34;vocab_size&#34;: 32000&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies of the converter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd converter &amp;amp;&amp;amp; pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Convert weights to Distributed Llama format. This will take a bit of time. The script requires Python 3.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python convert-llama2.py /path/to/meta/llama-2-7b q40&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the table below, you can find the expected size of the converted weights with different floating-point types.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Original size&lt;/th&gt; &#xA;   &lt;th&gt;Float32&lt;/th&gt; &#xA;   &lt;th&gt;Float16&lt;/th&gt; &#xA;   &lt;th&gt;Q40&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 7B&lt;/td&gt; &#xA;   &lt;td&gt;13.48 GB&lt;/td&gt; &#xA;   &lt;td&gt;25.10GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.95 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 13B&lt;/td&gt; &#xA;   &lt;td&gt;26.03 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7.35 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2 70B&lt;/td&gt; &#xA;   &lt;td&gt;137.97 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;36.98 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üî® How to Convert .bin Weights&lt;/h2&gt; &#xA;&lt;p&gt;You can convert weights compatible with &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; to the Distributed Llama format. The legacy converter converts weights only to Float32 format.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download weights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin&#xA;wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies of the converter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd converter &amp;amp;&amp;amp; pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Convert weights to Distributed Llama format.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python convert-legacy.py stories42M.bin true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìü How to Run on Raspberry Pi Devices&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;code&gt;Raspberry Pi OS Lite (64 bit)&lt;/code&gt; on your Raspberry Pi devices. This OS doesn&#39;t have desktop environment.&lt;/li&gt; &#xA; &lt;li&gt;Connect all devices to the Gigabit switch.&lt;/li&gt; &#xA; &lt;li&gt;Connect to all devices via SSH.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;ssh user@raspberrypi1.local&#xA;ssh user@raspberrypi2.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install Git:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt install git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/b4rtaz/distributed-llama.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Compile Distributed Llama:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Download the &lt;code&gt;tokenizer.bin&lt;/code&gt; file from the &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; repository to the root device.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Transfer converted weights to the root device.&lt;/li&gt; &#xA; &lt;li&gt;Optional: assign static IP addresses.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo ip addr add 10.0.0.1/24 dev eth0 # 1th device&#xA;sudo ip addr add 10.0.0.2/24 dev eth0 # 2th device&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;Run worker nodes on worker devices:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo nice -n -20 ./main worker --port 9998 --nthreads 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;Run root node on the root device:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo nice -n -20 ./main inference --model ../dllama_llama-2-7b_q40.bin --tokenizer ../tokenizer.bin --weights-float-type q40 --buffer-float-type q80 --prompt &#34;Hello world&#34; --steps 16 --nthreads 4 --workers 10.0.0.2:9998&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To add more worker nodes, just add more addresses to the &lt;code&gt;--workers&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./main inference ... --workers 10.0.0.2:9998 10.0.0.3:9998 10.0.0.4:9998&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/b4rtaz/distributed-llama/discussions&#34;&gt;Share your results&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;üíª How to Run on MacOS or Linux&lt;/h2&gt; &#xA;&lt;p&gt;You need to have x86_64 AVX2 CPU or ARM CPU. Different devices may have different CPUs. The below instructions are for Debian-based distributions but you can easily adapt them to your distribution or macOS.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Git and G++:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt install git build-essential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/b4rtaz/distributed-llama.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Compile Distributed Llama:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download the &lt;code&gt;tokenizer.bin&lt;/code&gt; file from the &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; repository.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Download converted weights from your Google Drive. To get the file ID you need to share the file (&#34;Anyone with the link&#34;) and copy the ID from the URL.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt install python pip&#xA;pip install gdown&#xA;gdown https://drive.google.com/uc?id=&amp;lt;FILE_ID&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run worker nodes on worker devices:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo nice -n -20 ./main worker --port 9998 --nthreads 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Run worker nodes on worker devices:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo nice -n -20 ./main inference --model ../dllama_llama-2-7b_q40.bin --tokenizer ../tokenizer.bin --weights-float-type q40 --buffer-float-type q80 --prompt &#34;Hello world&#34; --steps 16 --nthreads 4 --workers 192.168.0.1:9998&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/b4rtaz/distributed-llama/discussions&#34;&gt;Share your results&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;üí° License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the MIT license.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{dllama,&#xA;  author = {Bart≈Çomiej Tadych},&#xA;  title = {Distributed Llama},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/b4rtaz/distributed-llama}},&#xA;  commit = {7eb77ca93ec0d502e28d36b6fb20039b449cbea4}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>