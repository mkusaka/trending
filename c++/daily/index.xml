<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-08T01:30:21Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>flashinfer-ai/flashinfer</title>
    <updated>2025-08-08T01:30:21Z</updated>
    <id>tag:github.com,2025-08-08:/flashinfer-ai/flashinfer</id>
    <link href="https://github.com/flashinfer-ai/flashinfer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FlashInfer: Kernel Library for LLM Serving&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/flashinfer-ai/web-data/blob/main/logo/FlashInfer-black-background.png?raw=true&#34;&gt; &#xA;  &lt;img alt=&#34;FlashInfer&#34; src=&#34;https://github.com/flashinfer-ai/web-data/raw/main/logo/FlashInfer-white-background.png?raw=true&#34; width=&#34;55%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Kernel Library for LLM Serving &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://flashinfer.ai&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://docs.flashinfer.ai&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://join.slack.com/t/flashinfer/shared_invite/zt-379wct3hc-D5jR~1ZKQcU00WHsXhgvtA&#34;&gt;&lt;b&gt;Slack&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/orgs/flashinfer-ai/discussions&#34;&gt;&lt;b&gt;Discussion Forum&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ci.tlcpack.ai/job/flashinfer-ci/job/main/&#34;&gt;&lt;img src=&#34;https://ci.tlcpack.ai/job/flashinfer-ci/job/main/badge/icon&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/flashinfer-ai/flashinfer/actions/workflows/release_wheel.yml&#34;&gt;&lt;img src=&#34;https://github.com/flashinfer-ai/flashinfer/actions/workflows/release_wheel.yml/badge.svg?sanitize=true&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/flashinfer-ai/flashinfer/actions/workflows/build-doc.yml&#34;&gt;&lt;img src=&#34;https://github.com/flashinfer-ai/flashinfer/actions/workflows/build-doc.yml/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;FlashInfer is a library and kernel generator for Large Language Models that provides high-performance implementation of LLM GPU kernels such as FlashAttention, SparseAttention, PageAttention, Sampling, and more. FlashInfer focuses on LLM serving and inference, and delivers state-of-the-art performance across diverse scenarios.&lt;/p&gt; &#xA;&lt;p&gt;Check our &lt;a href=&#34;https://flashinfer.ai/2024/12/16/flashinfer-v02-release.html&#34;&gt;v0.2 release blog&lt;/a&gt; for new features!&lt;/p&gt; &#xA;&lt;p&gt;The core features of FlashInfer include:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient Sparse/Dense Attention Kernels&lt;/strong&gt;: Efficient single/batch attention for sparse(paged)/dense KV-storage on CUDA Cores and Tensor Cores (both FA2 &amp;amp; FA3) templates. The vector-sparse attention can achieve 90% of the bandwidth of dense kernels with same problem size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Load-Balanced Scheduling&lt;/strong&gt;: FlashInfer decouples &lt;code&gt;plan&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; stage of attention computation where we schedule the computation of variable-length inputs in &lt;code&gt;plan&lt;/code&gt; stage to alleviate load-imbalance issue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory Efficiency&lt;/strong&gt;: FlashInfer offers &lt;a href=&#34;https://docs.flashinfer.ai/api/cascade.html#flashinfer.cascade.MultiLevelCascadeAttentionWrapper&#34;&gt;Cascade Attention&lt;/a&gt; for hierarchical KV-Cache, and implements Head-Query fusion for accelerating Grouped-Query Attention, and efficient kernels for low-precision attention and fused-RoPE attention for compressed KV-Cache.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable Attention&lt;/strong&gt;: Bring your own attention variants through JIT-compilation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CUDAGraph and torch.compile Compatibility&lt;/strong&gt;: FlashInfer kernels can be captured by CUDAGraphs and torch.compile for low-latency inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient LLM-specific Operators&lt;/strong&gt;: High-Performance &lt;a href=&#34;https://docs.flashinfer.ai/api/sampling.html&#34;&gt;fused kernel for Top-P, Top-K/Min-P sampling&lt;/a&gt; without the need to sorting.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;FlashInfer supports PyTorch, TVM and C++ (header-only) APIs, and can be easily integrated into existing projects.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Mar 10, 2025] &lt;a href=&#34;https://flashinfer.ai/2025/03/10/sampling.html&#34;&gt;Blog Post&lt;/a&gt; Sorting-Free GPU Kernels for LLM Sampling, which explains the design of sampling kernels in FlashInfer.&lt;/li&gt; &#xA; &lt;li&gt;[Mar 1, 2025] Checkout flashinfer&#39;s &lt;a href=&#34;https://github.com/flashinfer-ai/flashinfer/tree/main/profiler&#34;&gt;intra-kernel profiler&lt;/a&gt; for visualizing the timeline of each threadblock in GPU kernels.&lt;/li&gt; &#xA; &lt;li&gt;[Dec 16, 2024] &lt;a href=&#34;https://flashinfer.ai/2024/12/16/flashinfer-v02-release.html&#34;&gt;Blog Post&lt;/a&gt; FlashInfer 0.2 - Efficient and Customizable Kernels for LLM Inference Serving&lt;/li&gt; &#xA; &lt;li&gt;[Sept 2024] We&#39;ve launched a &lt;a href=&#34;https://join.slack.com/t/flashinfer/shared_invite/zt-2r93kj2aq-wZnC2n_Z2~mf73N5qnVGGA&#34;&gt;Slack&lt;/a&gt; workspace for Flashinfer users and developers. Join us for timely support, discussions, updates and knowledge sharing!&lt;/li&gt; &#xA; &lt;li&gt;[Jan 31, 2024] &lt;a href=&#34;https://flashinfer.ai/2024/01/08/cascade-inference.html&#34;&gt;Blog Post&lt;/a&gt; Cascade Inference: Memory-Efficient Shared Prefix Batch Decoding&lt;/li&gt; &#xA; &lt;li&gt;[Jan 31, 2024] &lt;a href=&#34;https://flashinfer.ai/2024/01/03/introduce-flashinfer.html&#34;&gt;Blog Post&lt;/a&gt; Accelerating Self-Attentions for LLM Serving with FlashInfer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Using our PyTorch API is the easiest way to get started:&lt;/p&gt; &#xA;&lt;h3&gt;Install from PIP&lt;/h3&gt; &#xA;&lt;p&gt;We provide prebuilt python wheels for Linux. Install FlashInfer with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For CUDA 12.6 &amp;amp; torch 2.6&#xA;pip install flashinfer-python -i https://flashinfer.ai/whl/cu126/torch2.6&#xA;# For other CUDA &amp;amp; torch versions, check https://docs.flashinfer.ai/installation.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To try the latest features from the main branch, use our nightly-built wheels:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install flashinfer-python -i https://flashinfer.ai/whl/nightly/cu126/torch2.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a JIT version (compiling every kernel from scratch, &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVCC&lt;/a&gt; is required), install from &lt;a href=&#34;https://pypi.org/project/flashinfer-python/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install flashinfer-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install from Source&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, build FlashInfer from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/flashinfer-ai/flashinfer.git --recursive&#xA;cd flashinfer&#xA;python -m pip install -v .&#xA;&#xA;# for development &amp;amp; contribution, install in editable mode&#xA;python -m pip install --no-build-isolation -e . -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pre-compile essential kernels ahead-of-time (AOT), run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Set target CUDA architectures&#xA;export TORCH_CUDA_ARCH_LIST=&#34;7.5 8.0 8.9 9.0a 10.0a&#34;&#xA;# Build AOT kernels. Will produce AOT kernels in aot-ops/&#xA;python -m flashinfer.aot&#xA;# Build AOT wheel&#xA;python -m build --no-isolation --wheel&#xA;# Install AOT wheel&#xA;python -m pip install dist/flashinfer_*.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details, refer to the &lt;a href=&#34;https://docs.flashinfer.ai/installation.html#install-from-source&#34;&gt;Install from Source documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Trying it out&lt;/h3&gt; &#xA;&lt;p&gt;Below is a minimal example of using FlashInfer&#39;s single-request decode/append/prefill attention kernels:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import flashinfer&#xA;&#xA;kv_len = 2048&#xA;num_kv_heads = 32&#xA;head_dim = 128&#xA;&#xA;k = torch.randn(kv_len, num_kv_heads, head_dim).half().to(0)&#xA;v = torch.randn(kv_len, num_kv_heads, head_dim).half().to(0)&#xA;&#xA;# decode attention&#xA;&#xA;num_qo_heads = 32&#xA;q = torch.randn(num_qo_heads, head_dim).half().to(0)&#xA;&#xA;o = flashinfer.single_decode_with_kv_cache(q, k, v) # decode attention without RoPE on-the-fly&#xA;o_rope_on_the_fly = flashinfer.single_decode_with_kv_cache(q, k, v, pos_encoding_mode=&#34;ROPE_LLAMA&#34;) # decode with LLaMA style RoPE on-the-fly&#xA;&#xA;# append attention&#xA;append_qo_len = 128&#xA;q = torch.randn(append_qo_len, num_qo_heads, head_dim).half().to(0) # append attention, the last 128 tokens in the KV-Cache are the new tokens&#xA;o = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=True) # append attention without RoPE on-the-fly, apply causal mask&#xA;o_rope_on_the_fly = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=True, pos_encoding_mode=&#34;ROPE_LLAMA&#34;) # append attention with LLaMA style RoPE on-the-fly, apply causal mask&#xA;&#xA;# prefill attention&#xA;qo_len = 2048&#xA;q = torch.randn(qo_len, num_qo_heads, head_dim).half().to(0) # prefill attention&#xA;o = flashinfer.single_prefill_with_kv_cache(q, k, v, causal=False) # prefill attention without RoPE on-the-fly, do not apply causal mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://docs.flashinfer.ai/&#34;&gt;documentation&lt;/a&gt; for usage of batch decode/append/prefill kernels and shared-prefix cascading kernels.&lt;/p&gt; &#xA;&lt;h2&gt;Custom Attention Variants&lt;/h2&gt; &#xA;&lt;p&gt;Starting from FlashInfer v0.2, users can customize their own attention variants with additional parameters. For more details, refer to our &lt;a href=&#34;https://github.com/flashinfer-ai/flashinfer/raw/main/tests/test_jit_example.py&#34;&gt;JIT examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;C++ API and TVM Bindings&lt;/h2&gt; &#xA;&lt;p&gt;FlashInfer also provides C++ API and TVM bindings, please refer to &lt;a href=&#34;https://docs.flashinfer.ai/&#34;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Adoption&lt;/h2&gt; &#xA;&lt;p&gt;We are thrilled to share that FlashInfer is being adopted by many cutting-edge projects, including but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlc-ai/mlc-llm&#34;&gt;MLC-LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/punica-ai/punica&#34;&gt;Punica&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vectorch-ai/ScaleLLM&#34;&gt;ScaleLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;TGI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;lorax&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ModelTC/lightllm&#34;&gt;LightLLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;FlashInfer is inspired by &lt;a href=&#34;https://github.com/dao-AILab/flash-attention/&#34;&gt;FlashAttention 1&amp;amp;2&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.03598&#34;&gt;stream-K&lt;/a&gt;, &lt;a href=&#34;https://github.com/nvidia/cutlass&#34;&gt;cutlass&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookincubator/AITemplate&#34;&gt;AITemplate&lt;/a&gt; projects.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find FlashInfer helpful in your project or research, please consider citing our &lt;a href=&#34;https://arxiv.org/abs/2501.01005&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{ye2025flashinfer,&#xA;    title = {FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving},&#xA;    author = {&#xA;      Ye, Zihao and&#xA;      Chen, Lequn and&#xA;      Lai, Ruihang and&#xA;      Lin, Wuwei and&#xA;      Zhang, Yineng and&#xA;      Wang, Stephanie and&#xA;      Chen, Tianqi and&#xA;      Kasikci, Baris and&#xA;      Grover, Vinod and&#xA;      Krishnamurthy, Arvind and&#xA;      Ceze, Luis&#xA;    },&#xA;    journal = {arXiv preprint arXiv:2501.01005},&#xA;    year = {2025},&#xA;    url = {https://arxiv.org/abs/2501.01005}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>abdularis/LAN-Share</title>
    <updated>2025-08-08T01:30:21Z</updated>
    <id>tag:github.com,2025-08-08:/abdularis/LAN-Share</id>
    <link href="https://github.com/abdularis/LAN-Share" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cross platform LAN File transfer application built with Qt C++ framework&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/abdularis/LAN-Share/master/src/img/icon.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/abdularis/LAN-Share&#34;&gt;&lt;img src=&#34;https://travis-ci.org/abdularis/LAN-Share.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;LAN Share&lt;/h1&gt; &#xA;&lt;p&gt;LAN Share is a cross platform local area network file transfer application, built using Qt GUI framework. It can be used to transfer a whole folder, one or more files, large or small immediatelly without any additional configuration.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;You can download the Ubuntu package or linux AppImage or Windows executable from the release page. &lt;a href=&#34;https://github.com/abdularis/LAN-Share/releases&#34;&gt;https://github.com/abdularis/LAN-Share/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using .deb (Ubuntu/Debian)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;download the latest version, for example &lt;code&gt;lanshare_1.2.1-1_amd64.deb&lt;/code&gt; then open a terminal, navigate to directory where the downloaded package is located and type&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sudo dpkg -i ./lanshare_1.2.1-1_amd64.deb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using AppImage (All Linux)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;simply download the AppImage and run it on all linux distribution&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the github release page or go to &lt;a href=&#34;https://www.softpedia.com/get/Internet/File-Sharing/LAN-Share.shtml&#34;&gt;Softpedia&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compiling instructions&lt;/h2&gt; &#xA;&lt;p&gt;To compile LANShare you need to install Qt tools, on Debian you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt install qt5-qmake qt5-default&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about qmake you can visit this &lt;a href=&#34;https://doc.qt.io/qt-5/qmake-tutorial.html&#34;&gt;page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then you can download the sourcecode with git:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/abdularis/LAN-Share&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and compile with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd LAN-Share-1.2.1/src&#xA;$ qmake -o Makefile LANShare.pro&#xA;$ make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Send one or more files&lt;/li&gt; &#xA; &lt;li&gt;Send folder&lt;/li&gt; &#xA; &lt;li&gt;Send to multiple receiver at the same time&lt;/li&gt; &#xA; &lt;li&gt;Cancel, pause and resume operations while transfering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Computer has to be connected to a local area network (wired or wireless)&lt;/li&gt; &#xA; &lt;li&gt;Run the application in both sender and receiver&lt;/li&gt; &#xA; &lt;li&gt;Send the file/folder by, select &lt;em&gt;Send&lt;/em&gt; (files or folder) then -&amp;gt; select receiver in the &lt;em&gt;&#39;Select Receiver&#39;&lt;/em&gt; dialog, finally click &lt;em&gt;&#39;Send&#39;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;File/folder would automatically received by the receiver&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/abdularis/LAN-Share/master/screenshot.png&#34; alt=&#34;Screenshot 1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/abdularis/LAN-Share/master/screenshot2.png&#34; alt=&#34;Screenshot 2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/abdularis/LAN-Share/master/screenshot3.png&#34; alt=&#34;Screenshot 3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;GPLv3&lt;/p&gt;</summary>
  </entry>
</feed>