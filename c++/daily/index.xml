<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-16T01:30:35Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DamRsn/NeuralNote</title>
    <updated>2023-05-16T01:30:35Z</updated>
    <id>tag:github.com,2023-05-16:/DamRsn/NeuralNote</id>
    <link href="https://github.com/DamRsn/NeuralNote" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Audio Plugin for Audio to MIDI transcription using deep learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeuralNote &lt;img style=&#34;float: right;&#34; src=&#34;https://raw.githubusercontent.com/DamRsn/NeuralNote/master/NeuralNote/Assets/logo.png&#34; width=&#34;100&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;NeuralNote is the audio plugin that brings &lt;strong&gt;state-of-the-art Audio to MIDI conversion&lt;/strong&gt; into your favorite Digital Audio Workstation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Works with any tonal instrument (voice included)&lt;/li&gt; &#xA; &lt;li&gt;Supports polyphonic transcription&lt;/li&gt; &#xA; &lt;li&gt;Supports pitch bends&lt;/li&gt; &#xA; &lt;li&gt;Lightweight and very fast transcription&lt;/li&gt; &#xA; &lt;li&gt;Can scale and time quantize transcribed MIDI directly in the plugin&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install NeuralNote&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest release for your platform &lt;a href=&#34;https://github.com/DamRsn/NeuralNote/releases&#34;&gt;here&lt;/a&gt; (Windows and macOS ( Universal) supported)!&lt;/p&gt; &#xA;&lt;p&gt;Currently, only the raw &lt;code&gt;.vst3&lt;/code&gt;, &lt;code&gt;.component&lt;/code&gt; (Audio Unit), &lt;code&gt;.app&lt;/code&gt; and &lt;code&gt;.exe&lt;/code&gt; (Standalone) files are provided. Installers will be created soon. In the meantime, you can manually copy the plugin/app file in the appropriate directory. The code is signed on macOS, but not on Windows, so you might have to perform few extra steps in order to be able to use NeuralNote on Windows (to be documented soon).&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DamRsn/NeuralNote/master/NeuralNote_UI.png&#34; alt=&#34;UI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;NeuralNote comes as a simple AudioFX plugin (VST3/AU/Standalone app) to be applied on the track to transcribe.&lt;/p&gt; &#xA;&lt;p&gt;The workflow is very simple:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gather some audio &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Click record. Works when recording for real or when playing the track in a DAW.&lt;/li&gt; &#xA;   &lt;li&gt;Or drop an audio file on the plugin. (.wav, .aiff and .flac supported)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;The midi transcription instantly appears in the piano roll section. Play with the different settings to adjust it.&lt;/li&gt; &#xA; &lt;li&gt;Export the MIDI transcription with a simple drag and drop from the plugin to a MIDI track.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Watch our presentation video for the Neural Audio Plugin competition &lt;a href=&#34;https://www.youtube.com/watch?v=6_MC0_aG_DQ&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;NeuralNote uses internally the model from Spotify&#39;s &lt;a href=&#34;https://github.com/spotify/basic-pitch&#34;&gt;basic-pitch&lt;/a&gt;. See their &lt;a href=&#34;https://engineering.atspotify.com/2022/06/meet-basic-pitch/&#34;&gt;blogpost&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2203.09893&#34;&gt;paper&lt;/a&gt; for more information. In NeuralNote, basic-pitch is run using &lt;a href=&#34;https://github.com/jatinchowdhury18/RTNeural&#34;&gt;RTNeural&lt;/a&gt; for the CNN part and &lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNXRuntime&lt;/a&gt; for the feature part (Constant-Q transform calculation + Harmonic Stacking). As part of this project, &lt;a href=&#34;https://github.com/jatinchowdhury18/RTNeural/pull/89&#34;&gt;we contributed to RTNeural&lt;/a&gt; to add 2D convolution support.&lt;/p&gt; &#xA;&lt;h2&gt;Build from source&lt;/h2&gt; &#xA;&lt;p&gt;Requirements are: &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;cmake&lt;/code&gt;, and your OS&#39;s preferred compiler suite.&lt;/p&gt; &#xA;&lt;p&gt;Use this when cloning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules --shallow-submodules https://github.com/DamRsn/NeuralNote&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following OS-specific build scripts have to be executed at least once before being able to use the project as a normal CMake project. The script downloads onnxruntime static library (that we created with &lt;a href=&#34;https://github.com/olilarkin/ort-builder&#34;&gt;ort-builder&lt;/a&gt;) before calling CMake.&lt;/p&gt; &#xA;&lt;h4&gt;macOS&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;Due to &lt;a href=&#34;https://github.com/DamRsn/NeuralNote/issues/21&#34;&gt;a known issue&lt;/a&gt;, if you&#39;re not using Visual Studio 2022 (MSVC version: 19.35.x, check &lt;code&gt;cl&lt;/code&gt; output), then you&#39;ll need to manually build onnxruntime.lib like so:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have Python installed; if not, download at &lt;a href=&#34;https://www.python.org/downloads/windows/&#34;&gt;https://www.python.org/downloads/windows/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute each of the following lines in a command prompt:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --depth 1 --recurse-submodules --shallow-submodules https://github.com/tiborvass/libonnxruntime-neuralnote ThirdParty\onnxruntime&#xA;cd ThirdParty\onnxruntime&#xA;python3 -m venv venv&#xA;.\venv\Scripts\activate.bat&#xA;pip install -r requirements.txt&#xA;.\convert-model-to-ort.bat model.onnx&#xA;.\build-win.bat model.required_operators_and_types.with_runtime_opt.config&#xA;copy model.with_runtime_opt.ort ..\..\Lib\ModelData\features_model.ort&#xA;cd ..\..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can get back to building NeuralNote as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; .\build.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;IDEs&lt;/h4&gt; &#xA;&lt;p&gt;Once the build script has been executed at least once, you can load this project in your favorite IDE (CLion/Visual Studio/VSCode/etc) and click &#39;build&#39; for one of the targets.&lt;/p&gt; &#xA;&lt;h2&gt;Reuse code from NeuralNote‚Äôs transcription engine&lt;/h2&gt; &#xA;&lt;p&gt;All the code to perform the transcription is in &lt;code&gt;Lib/Model&lt;/code&gt; and all the model weights are in &lt;code&gt;Lib/ModelData/&lt;/code&gt;. Feel free to use only this part of the code in your own project! We&#39;ll try to isolate it more from the rest of the repo in the future and make it a library.&lt;/p&gt; &#xA;&lt;p&gt;The code to generate the files in &lt;code&gt;Lib/ModelData/&lt;/code&gt; is not currently available as it required a lot of manual operations. But here&#39;s a description of the process we followed to create those files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;features_model.onnx&lt;/code&gt; was generated by converting a keras model containing only the CQT + Harmonic Stacking part of the full basic-pitch graph using &lt;code&gt;tf2onnx&lt;/code&gt; (with manually added weights for batch normalization).&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;.json&lt;/code&gt; files containing the weights of the basic-pitch cnn were generated from the tensorflow-js model available in the &lt;a href=&#34;https://github.com/spotify/basic-pitch-ts&#34;&gt;basic-pitch-ts repository&lt;/a&gt;, then converted to onnx with &lt;code&gt;tf2onnx&lt;/code&gt;. Finally, the weights were gathered manually to &lt;code&gt;.npy&lt;/code&gt; thanks to &lt;a href=&#34;https://netron.app/&#34;&gt;Netron&lt;/a&gt; and finally applied to a split keras model created with &lt;a href=&#34;https://github.com/spotify/basic-pitch&#34;&gt;basic-pitch&lt;/a&gt; code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original basic-pitch CNN was split in 4 sequential models wired together, so they can be run with RTNeural.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve stability&lt;/li&gt; &#xA; &lt;li&gt;Save plugin internal state properly, so it can be loaded back when reentering a session&lt;/li&gt; &#xA; &lt;li&gt;Add tooltips&lt;/li&gt; &#xA; &lt;li&gt;Build a simple synth in the plugin so that one can listen to the transcription while playing with the settings, before export&lt;/li&gt; &#xA; &lt;li&gt;Allow pitch bends on non-overlapping parts of overlapping notes&lt;/li&gt; &#xA; &lt;li&gt;Support transcription of mp3 files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bug reports and feature requests&lt;/h2&gt; &#xA;&lt;p&gt;If you have any request/suggestion concerning the plugin or encounter a bug, please file a GitHub issue.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are most welcome! If you want to add some features to the plugin or simply improve the documentation, please open a PR!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;NeuralNote software and code is published under the Apache-2.0 license. See the &lt;a href=&#34;https://raw.githubusercontent.com/DamRsn/NeuralNote/master/LICENSE&#34;&gt;license file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Third Party libraries used and license&lt;/h4&gt; &#xA;&lt;p&gt;Here&#39;s a list of all the third party libraries used in NeuralNote and the license under which they are used.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://juce.com/&#34;&gt;JUCE&lt;/a&gt; (JUCE Personal)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jatinchowdhury18/RTNeural&#34;&gt;RTNeural&lt;/a&gt; (BSD-3-Clause license)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNXRuntime&lt;/a&gt; (MIT License)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/olilarkin/ort-builder&#34;&gt;ort-builder&lt;/a&gt; (MIT License)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/basic-pitch&#34;&gt;basic-pitch&lt;/a&gt; (Apache-2.0 license)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/basic-pitch-ts&#34;&gt;basic-pitch-ts&lt;/a&gt; (Apache-2.0 license)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Could NeuralNote transcribe audio in real-time?&lt;/h2&gt; &#xA;&lt;p&gt;Unfortunately no and this for a few reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic Pitch uses the Constant-Q transform (CQT) as input feature. The CQT requires really long audio chunks (&amp;gt; 1s) to get amplitudes for the lowest frequency bins. This makes the latency too high to have real-time transcription.&lt;/li&gt; &#xA; &lt;li&gt;The basic pitch CNN has an additional latency of approximately 120ms.&lt;/li&gt; &#xA; &lt;li&gt;Very few DAWs support audio input/MIDI output plugins as far as I know. This is partially why NeuralNote is an Audio FX plugin (audio-to-audio) and that MIDI is exported via drag and drop.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;But if you have ideas please share!&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;NeuralNote was developed by &lt;a href=&#34;https://github.com/DamRsn&#34;&gt;Damien Ronssin&lt;/a&gt; and &lt;a href=&#34;https://github.com/tiborvass&#34;&gt;Tibor Vass&lt;/a&gt;. The plugin user interface was designed by Perrine Morel.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Korepi/Korepi</title>
    <updated>2023-05-16T01:30:35Z</updated>
    <id>tag:github.com,2023-05-16:/Korepi/Korepi</id>
    <link href="https://github.com/Korepi/Korepi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Korek Api&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Korepi/Korepi/main/#&#34;&gt;&lt;img width=&#34;500&#34; height=&#34;500&#34; src=&#34;https://media.discordapp.net/attachments/1033549666769449002/1107009612210765955/matches.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Korepi/Korepi/main/#&#34;&gt;&lt;img width=&#34;690&#34; height=&#34;133&#34; src=&#34;https://share.creavite.co/FBkHy3zbN4CgWCr0.gif&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Korepi/Korepi/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Korepi/Korepi?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Korepi/Korepi/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/Korepi/Korepi/total.svg?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Korepi/Korepi/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/Korepi/Korepi?style=for-the-badge&amp;amp;color=red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/8UZbDtEvrW&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/440536354544156683?label=Discord&amp;amp;logo=discord&amp;amp;style=for-the-badge&amp;amp;color=blueviolet&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Korepi/Korepi/main/README.md&#34;&gt;EN&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Korepi/Korepi/main/README_vn-vn.md&#34;&gt;VI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Head over to the &lt;a href=&#34;https://github.com/Korepi/Korepi/releases&#34;&gt;releases page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download the latest binaries&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure that &lt;code&gt;HoYoKProtect.dll&lt;/code&gt; is in the same folder that &lt;code&gt;injector.exe&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;injector.exe&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;code&gt;GenshinImpact.exe&lt;/code&gt; or &lt;code&gt;YuanShen.exe&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Game will be launched automatically, wait for interface to appear.&lt;/li&gt; &#xA; &lt;li&gt;Press F1 to open Korepi GUI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Features&lt;/h1&gt; &#xA;&lt;h4&gt;General&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Protection Bypass&lt;/li&gt; &#xA; &lt;li&gt;In-Game GUI&lt;/li&gt; &#xA; &lt;li&gt;Hotkeys&lt;/li&gt; &#xA; &lt;li&gt;Notifications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Player&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;God Mode(Invincible)&lt;/li&gt; &#xA; &lt;li&gt;Attack Modifier: Multi-Hit/Target/Animation&lt;/li&gt; &#xA; &lt;li&gt;No Cooldown: Skill/Ultimate/Sprint/Bow&lt;/li&gt; &#xA; &lt;li&gt;Unlimited Stamina&lt;/li&gt; &#xA; &lt;li&gt;No Clip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;World&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto Seelie&lt;/li&gt; &#xA; &lt;li&gt;Open Team Immediately&lt;/li&gt; &#xA; &lt;li&gt;GameSpeed&lt;/li&gt; &#xA; &lt;li&gt;Dumb Enemies&lt;/li&gt; &#xA; &lt;li&gt;Freeze Enemies&lt;/li&gt; &#xA; &lt;li&gt;Auto Destroy: Ores/Shields/Doodads/Plants&lt;/li&gt; &#xA; &lt;li&gt;Auto Loot/Open Chests&lt;/li&gt; &#xA; &lt;li&gt;Pickup Range&lt;/li&gt; &#xA; &lt;li&gt;Auto Talk&lt;/li&gt; &#xA; &lt;li&gt;Auto Tree Farm&lt;/li&gt; &#xA; &lt;li&gt;Auto Cook&lt;/li&gt; &#xA; &lt;li&gt;Fake Time&lt;/li&gt; &#xA; &lt;li&gt;Auto Fish&lt;/li&gt; &#xA; &lt;li&gt;Kill Aura&lt;/li&gt; &#xA; &lt;li&gt;Mob Vacuum&lt;/li&gt; &#xA; &lt;li&gt;Vacuum Loot&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Teleport&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chest/Oculi Teleport (Teleports to nearest)&lt;/li&gt; &#xA; &lt;li&gt;Map Teleport (Teleport to mark on map)&lt;/li&gt; &#xA; &lt;li&gt;Custom Teleport (Teleport through list)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Visuals&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ESP&lt;/li&gt; &#xA; &lt;li&gt;Interactive Map&lt;/li&gt; &#xA; &lt;li&gt;Elemental Sight&lt;/li&gt; &#xA; &lt;li&gt;No Fog&lt;/li&gt; &#xA; &lt;li&gt;FPS Unlock&lt;/li&gt; &#xA; &lt;li&gt;Camera Zoom&lt;/li&gt; &#xA; &lt;li&gt;Chest Indicator&lt;/li&gt; &#xA; &lt;li&gt;Hide UI&lt;/li&gt; &#xA; &lt;li&gt;In-game Embedded Browser&lt;/li&gt; &#xA; &lt;li&gt;Enable Peeking&lt;/li&gt; &#xA; &lt;li&gt;Profile Changer: UID/Nickname/AR/WorldLevel/Avatar/Namecard&lt;/li&gt; &#xA; &lt;li&gt;Custom Weather&lt;/li&gt; &#xA; &lt;li&gt;Free Camera&lt;/li&gt; &#xA; &lt;li&gt;Paimon Follow&lt;/li&gt; &#xA; &lt;li&gt;Texture Changer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Debugging&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Entities Manager&lt;/li&gt; &#xA; &lt;li&gt;Position Info&lt;/li&gt; &#xA; &lt;li&gt;FPS Graph&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Akebi-Group/Akebi-PacketSniffer&#34;&gt;Packet Sniffer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Demo&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Map Teleportation&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/map-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Noclip&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/noclip-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TP to Oculi&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/oculi-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TP to Chests&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/chest-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Rapid Fire&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/rapid-fire-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Auto Talk&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/auto-talk-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Bugs&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the short explanation for bug reporting&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You found a bug.&lt;/li&gt; &#xA; &lt;li&gt;Write down what happened, as well as your first thoughts on what you think caused it.&lt;/li&gt; &#xA; &lt;li&gt;Can it be reproduced? Yes or no. If yes: Explain in as much clear as possible. i.e what happens when the bug occurs and why it occurs.&lt;/li&gt; &#xA; &lt;li&gt;Tell us which version you are using. copy the &lt;code&gt;SHA&lt;/code&gt;/ Version Number of the latest commit when you built the mod. For example: &lt;code&gt;bd17a00ec388f3b93624280cde9e1c66e740edf9&lt;/code&gt; / Release 0.7&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Korepi/Korepi/issues&#34;&gt;Open an Issue(there&#39;s a template!)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Adding a feature&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the Project&lt;/li&gt; &#xA; &lt;li&gt;Create your Feature Branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Commit your Changes (&lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the Branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Korepi/Korepi/pulls&#34;&gt;Open a Pull Request&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Suggestions&lt;/h2&gt; &#xA;&lt;p&gt;Open an &lt;a href=&#34;https://github.com/Korepi/Korepi/issues&#34;&gt;issue&lt;/a&gt; with the title of the suggestion you want to make. In the description, make sure it is &lt;strong&gt;descriptive enough&lt;/strong&gt; so our devs can understand what you want and how you want it.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nomic-ai/gpt4all</title>
    <updated>2023-05-16T01:30:35Z</updated>
    <id>tag:github.com,2023-05-16:/nomic-ai/gpt4all</id>
    <link href="https://github.com/nomic-ai/gpt4all" rel="alternate"></link>
    <summary type="html">&lt;p&gt;gpt4all: an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;GPT4All&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Open-source assistant-style large language models that run locally on your CPU&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://gpt4all.io&#34;&gt;GPT4All Website&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/mGZE39AS3e&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://gpt4all.io/reports/GPT4All_Technical_Report_3.pdf&#34;&gt;&lt;span&gt;üìó&lt;/span&gt; Technical Report 3: GPT4All Snoozy and Groovy &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf&#34;&gt;&lt;span&gt;üìó&lt;/span&gt; Technical Report 2: GPT4All-J &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf&#34;&gt;&lt;span&gt;üìó&lt;/span&gt; Technical Report 1: GPT4All&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/python/README.md&#34;&gt;&lt;span&gt;üêç&lt;/span&gt; Official Python Bindings&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/typescript&#34;&gt;&lt;span&gt;üíª&lt;/span&gt; Official Typescript Bindings&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/nomic-ai/gpt4all/raw/main/gpt4all-chat/README.md&#34;&gt;&lt;span&gt;üí¨&lt;/span&gt; Official Chat Interface&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/nomic-ai/gpt4all-ui&#34;&gt;&lt;span&gt;üí¨&lt;/span&gt; Official Web Chat Interface&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://python.langchain.com/en/latest/modules/models/llms/integrations/gpt4all.html&#34;&gt;ü¶úÔ∏èüîó Official Langchain Backend&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; GPT4All is made possible by our compute partner &lt;a href=&#34;https://www.paperspace.com/&#34;&gt;Paperspace&lt;/a&gt;. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;600&#34; height=&#34;365&#34; src=&#34;https://user-images.githubusercontent.com/13879686/231876409-e3de1934-93bb-4b4b-9013-b491a969ebbc.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Run on an M1 Mac (not sped up!) &lt;/p&gt; &#xA;&lt;h2&gt;GPT4All: An ecosystem of open-source on-edge large language models.&lt;/h2&gt; &#xA;&lt;p&gt;GTP4All is an ecosystem to train and deploy &lt;strong&gt;powerful&lt;/strong&gt; and &lt;strong&gt;customized&lt;/strong&gt; large language models that run locally on consumer grade CPUs.&lt;/p&gt; &#xA;&lt;p&gt;The goal is simple - be the best instruction tuned assistant-style language model that any person or enterprise can freely use, distribute and build on.&lt;/p&gt; &#xA;&lt;p&gt;A GPT4All model is a 3GB - 8GB file that you can download and plug into the GPT4All open-source ecosystem software. &lt;strong&gt;Nomic AI&lt;/strong&gt; supports and maintains this software ecosystem to enforce quality and security alongside spearheading the effort to allow any person or enterprise to easily train and deploy their own on-edge large language models.&lt;/p&gt; &#xA;&lt;h3&gt;Chat Client&lt;/h3&gt; &#xA;&lt;p&gt;Run any GPT4All model natively on your home desktop with the auto-updating desktop chat client. See &lt;a href=&#34;https://gpt4all.io&#34;&gt;GPT4All Website&lt;/a&gt; for a full list of open-source models you can run with this powerful desktop application.&lt;/p&gt; &#xA;&lt;p&gt;Direct Installer Links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gpt4all.io/installers/gpt4all-installer-darwin.dmg&#34;&gt;Mac/OSX&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gpt4all.io/installers/gpt4all-installer-win64.exe&#34;&gt;Windows&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gpt4all.io/installers/gpt4all-installer-linux.run&#34;&gt;Ubuntu&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have older hardware that only supports avx and not avx2 you can use these.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gpt4all.io/installers/gpt4all-installer-darwin-avx-only.dmg&#34;&gt;Mac/OSX - avx-only&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gpt4all.io/installers/gpt4all-installer-win64-avx-only.exe&#34;&gt;Windows - avx-only&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://gpt4all.io/installers/gpt4all-installer-linux-avx-only.run&#34;&gt;Ubuntu - avx-only&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find the most up-to-date information on the &lt;a href=&#34;https://gpt4all.io/&#34;&gt;GPT4All Website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chat Client building and running&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the visual instructions on the chat client &lt;a href=&#34;https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-chat/build_and_run.md&#34;&gt;build_and_run&lt;/a&gt; page&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Bindings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/python/README.md&#34;&gt;&lt;span&gt;üêç&lt;/span&gt; Official Python Bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/typescript&#34;&gt;&lt;span&gt;üíª&lt;/span&gt; Official Typescript Bindings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/golang&#34;&gt;&lt;span&gt;üíª&lt;/span&gt; Official GoLang Bindings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;GPT4All welcomes contributions, involvement, and discussion from the open source community! Please see CONTRIBUTING.md and follow the issues, bug reports, and PR markdown templates.&lt;/p&gt; &#xA;&lt;p&gt;Check project discord, with project owners, or through existing issues/PRs to avoid duplicate work. Please make sure to tag all of the above with relevant project identifiers or your contribution could potentially get lost. Example tags: &lt;code&gt;backend&lt;/code&gt;, &lt;code&gt;bindings&lt;/code&gt;, &lt;code&gt;python-bindings&lt;/code&gt;, &lt;code&gt;documentation&lt;/code&gt;, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you utilize this repository, models or data in a downstream project, please consider citing it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gpt4all,&#xA;  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},&#xA;  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/nomic-ai/gpt4all}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>