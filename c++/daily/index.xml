<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-23T01:32:52Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GPUOpen-Tools/radeon_raytracing_analyzer</title>
    <updated>2022-11-23T01:32:52Z</updated>
    <id>tag:github.com,2022-11-23:/GPUOpen-Tools/radeon_raytracing_analyzer</id>
    <link href="https://github.com/GPUOpen-Tools/radeon_raytracing_analyzer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Radeon Raytracing Analyzer (RRA) is a tool to visualize and inspect Bounding Volume Hierarchies (BVH) for ray tracing applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Radeon™ Raytracing Analyzer&lt;/h1&gt; &#xA;&lt;p&gt;The Radeon Raytracing Analyzer (RRA) is a tool designed to help improve the raytracing performance of AMD GPU&#39;s that support raytracing. The tool thus far focuses on the visualization of the Acceleration Structures, which consist of Bounding Volume Hierarchies.&lt;/p&gt; &#xA;&lt;p&gt;Game developers are responsible for creating the acceleration structures and so need a method of visualizing these acceleration structures and how they can affect performance.&lt;/p&gt; &#xA;&lt;p&gt;RRA allows the developer to visualize the bounding box hierarchies, and related scene geometries, via a standard rasterizer renderer or using a traveral counter view which will quickly hightlight areas of concern. Once identified, the developer can revisit their BVH generation strategy to reduce performance bottlenecks.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the latest AMD Video/display driver. Completely remove previously installed drivers. On Windows, the driver installation factory reset option should be used.&lt;/li&gt; &#xA; &lt;li&gt;Unzip/Untar the download file. The directory includes the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Radeon Developer Service (RDS)&lt;/li&gt; &#xA;   &lt;li&gt;Radeon Developer Service CLI (RDS headless)&lt;/li&gt; &#xA;   &lt;li&gt;Radeon Developer Panel (RDP)&lt;/li&gt; &#xA;   &lt;li&gt;Radeon Raytracing Analyzer (RRA)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;To capture a trace from a game, run the Radeon Developer Panel and follow the instructions in the Help. Help can be found in the following locations: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Help web pages exist in the &#34;docs&#34; sub directory&lt;/li&gt; &#xA;   &lt;li&gt;Help web pages can be accessed from the &lt;strong&gt;Help&lt;/strong&gt; button in the Developer Panel&lt;/li&gt; &#xA;   &lt;li&gt;Help web pages can be accessed from the Welcome screen in the Radeon Raytracing Analyzer, or from the &lt;strong&gt;Help&lt;/strong&gt; menu&lt;/li&gt; &#xA;   &lt;li&gt;The documentation is hosted publicly at: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://radeon-developer-panel.readthedocs.io/en/latest/&#34;&gt;https://radeon-developer-panel.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://radeon-raytracing-analyzer.readthedocs.io/en/latest/&#34;&gt;https://radeon-raytracing-analyzer.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Supported APIs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DirectX12&lt;/li&gt; &#xA; &lt;li&gt;Vulkan&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported ASICs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMD Radeon RX 6000 series&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Operating Systems&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows® 10&lt;/li&gt; &#xA; &lt;li&gt;Windows® 11&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 20.04 LTS (Vulkan only)&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 22.04 LTS (Vulkan only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build instructions&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/GPUOpen-Tools/radeon_raytracing_analyzer/main/BUILD.md&#34;&gt;BUILD.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Radeon Raytracing Analyzer is licensed under the MIT license. See the &lt;a href=&#34;https://raw.githubusercontent.com/GPUOpen-Tools/radeon_raytracing_analyzer/main/License.txt&#34;&gt;License.txt&lt;/a&gt; file for complete license information.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright information&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/GPUOpen-Tools/radeon_raytracing_analyzer/main/NOTICES.txt&#34;&gt;NOTICES.txt&lt;/a&gt; for third party license information.&lt;/p&gt; &#xA;&lt;h2&gt;DISCLAIMER&lt;/h2&gt; &#xA;&lt;p&gt;The information contained herein is for informational purposes only, and is subject to change without notice. While every precaution has been taken in the preparation of this document, it may contain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to update or otherwise correct this information. Advanced Micro Devices, Inc. makes no representations or warranties with respect to the accuracy or completeness of the contents of this document, and assumes no liability of any kind, including the implied warranties of noninfringement, merchantability or fitness for particular purposes, with respect to the operation or use of AMD hardware, software or other products described herein. No license, including implied or arising by estoppel, to any intellectual property rights is granted by this document. Terms and limitations applicable to the purchase or use of AMD’s products are as set forth in a signed agreement between the parties or in AMD&#39;s Standard Terms and Conditions of Sale.&lt;/p&gt; &#xA;&lt;p&gt;AMD, the AMD Arrow logo, Radeon, Ryzen, RDNA and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective companies.&lt;/p&gt; &#xA;&lt;p&gt;Visual Studio, DirectX and Windows are registered trademarks of Microsoft Corporation in the US and other jurisdictions.&lt;/p&gt; &#xA;&lt;p&gt;Vulkan and the Vulkan logo are registered trademarks of the Khronos Group Inc.&lt;/p&gt; &#xA;&lt;p&gt;Python is a registered trademark of the PSF. The Python logos (in several variants) are use trademarks of the PSF as well.&lt;/p&gt; &#xA;&lt;p&gt;CMake is a registered trademark of Kitware, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Qt and the Qt logo are registered trademarks of the Qt Company Ltd and/or its subsidiaries worldwide.&lt;/p&gt; &#xA;&lt;p&gt;© 2022 Advanced Micro Devices, Inc. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>assimp/assimp</title>
    <updated>2022-11-23T01:32:52Z</updated>
    <id>tag:github.com,2022-11-23:/assimp/assimp</id>
    <link href="https://github.com/assimp/assimp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official Open-Asset-Importer-Library Repository. Loads 40+ 3D-file-formats into one unified and clean data structure.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Asset Import Library (assimp)&lt;/h1&gt; &#xA;&lt;p&gt;A library to import and export various 3d-model-formats including scene-post-processing to generate missing render data.&lt;/p&gt; &#xA;&lt;h3&gt;Current project status&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/assimp&#34;&gt;&lt;img src=&#34;https://opencollective.com/assimp/all/badge.svg?label=financial+contributors&#34; alt=&#34;Financial Contributors on Open Collective&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/assimp/assimp/workflows/C/C++%20CI/badge.svg?sanitize=true&#34; alt=&#34;C/C++ CI&#34;&gt; &lt;a href=&#34;https://scan.coverity.com/projects/5607&#34;&gt; &lt;img alt=&#34;Coverity Scan Build Status&#34; src=&#34;https://scan.coverity.com/projects/5607/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.codacy.com/gh/assimp/assimp/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=assimp/assimp&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/9973693b7bdd4543b07084d5d9cf4745&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://coveralls.io/github/assimp/assimp?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/github/assimp/assimp/badge.svg?branch=master&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/assimp/assimp?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/assimp/assimp.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/assimp/assimp&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/assimp/assimp&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/assimp/assimp.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/assimp/assimp&#34; title=&#34;Percentage of issues still open&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/open/assimp/assimp.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/assimp/assimp/alerts/&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/alerts/g/assimp/assimp.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Total alerts&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;APIs are provided for C and C++. There are various bindings to other languages (C#, Java, Python, Delphi, D). Assimp also runs on Android and iOS. Additionally, assimp features various &lt;strong&gt;mesh post processing tools&lt;/strong&gt;: normals and tangent space generation, triangulation, vertex cache locality optimization, removal of degenerate primitives and duplicate vertices, sorting by primitive type, merging of redundant materials and many more.&lt;/p&gt; &#xA;&lt;h3&gt;Latest Doc&#39;s&lt;/h3&gt; &#xA;&lt;p&gt;Please check the latest documents at &lt;a href=&#34;https://assimp-docs.readthedocs.io/en/latest/&#34;&gt;Asset-Importer-Lib-Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Get involved&lt;/h3&gt; &#xA;&lt;p&gt;This is the development repo containing the latest features and bugfixes. For productive use though, we recommend one of the stable releases available from &lt;a href=&#34;https://github.com/assimp/assimp/releases&#34;&gt;Github Assimp Releases&lt;/a&gt;. &lt;br&gt; You find a bug in the docs? Use &lt;a href=&#34;https://github.com/assimp/assimp-docs&#34;&gt;Doc-Repo&lt;/a&gt;. &lt;br&gt; Please check our Wiki as well: &lt;a href=&#34;https://github.com/assimp/assimp/wiki&#34;&gt;https://github.com/assimp/assimp/wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to check our Model-Database, use the following repo: &lt;a href=&#34;https://github.com/assimp/assimp-mdb&#34;&gt;https://github.com/assimp/assimp-mdb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Supported file formats&lt;/h4&gt; &#xA;&lt;p&gt;You can find the complete list of supported file-formats &lt;a href=&#34;https://github.com/assimp/assimp/raw/master/doc/Fileformats.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;Take a look into the &lt;a href=&#34;https://github.com/assimp/assimp/raw/master/Build.md&#34;&gt;https://github.com/assimp/assimp/blob/master/Build.md&lt;/a&gt; file. We are available in vcpkg, and our build system is CMake; if you used CMake before there is a good chance you know what to do.&lt;/p&gt; &#xA;&lt;h3&gt;Ports&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/assimp/assimp/master/port/AndroidJNI/README.md&#34;&gt;Android&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/assimp/assimp/master/port/PyAssimp/README.md&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bitbucket.org/Starnick/assimpnet/src/master/&#34;&gt;.NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/assimp/assimp/master/port/AssimpPascal/Readme.md&#34;&gt;Pascal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/makc/assimp2json&#34;&gt;Javascript (Alpha)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kovacsv/assimpjs&#34;&gt;Javascript/Node.js Interface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ricardoreis.net/trilib-2/&#34;&gt;Unity 3d Plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/irajsb/UE4_Assimp/&#34;&gt;Unreal Engine Plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kotlin-graphics/assimp&#34;&gt;JVM&lt;/a&gt; Full jvm port (current &lt;a href=&#34;https://github.com/kotlin-graphics/assimp/wiki/Status&#34;&gt;status&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/longde123/assimp-haxe&#34;&gt;HAXE-Port&lt;/a&gt; The Assimp-HAXE-port.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jkvargas/russimp&#34;&gt;Rust&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other tools&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/acgessler/open3mod&#34;&gt;open3mod&lt;/a&gt; is a powerful 3D model viewer based on Assimp&#39;s import and export abilities.&lt;/p&gt; &#xA;&lt;h4&gt;Repository structure&lt;/h4&gt; &#xA;&lt;p&gt;Open Asset Import Library is implemented in C++. The directory structure looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/code&#x9;&#x9;Source code&#xA;/contrib&#x9;Third-party libraries&#xA;/doc&#x9;&#x9;Documentation (doxysource and pre-compiled docs)&#xA;/fuzz           Contains the test-code for the Google-Fuzzer project&#xA;/include&#x9;Public header C and C++ header files&#xA;/scripts &#x9;Scripts used to generate the loading code for some formats&#xA;/port&#x9;&#x9;Ports to other languages and scripts to maintain those.&#xA;/test&#x9;&#x9;Unit- and regression tests, test suite of models&#xA;/tools&#x9;&#x9;Tools (old assimp viewer, command line `assimp`)&#xA;/samples&#x9;A small number of samples to illustrate possible&#xA;                    use cases for Assimp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The source code is organized in the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;code/Common&#x9;&#x9;&#x9;The base implementation for importers and the infrastructure&#xA;code/PostProcessing&#x9;&#x9;The post-processing steps&#xA;code/AssetLib/&amp;lt;FormatName&amp;gt;&#x9;Implementation for import and export for the format&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Where to get help&lt;/h3&gt; &#xA;&lt;p&gt;For more information, visit &lt;a href=&#34;http://assimp.org/&#34;&gt;our website&lt;/a&gt;. Or check out the &lt;code&gt;./doc&lt;/code&gt;- folder, which contains the official documentation in HTML format. (CHMs for Windows are included in some release packages and should be located right here in the root folder).&lt;/p&gt; &#xA;&lt;p&gt;If the docs don&#39;t solve your problem, ask on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/assimp?sort=newest&#34;&gt;StackOverflow with the assimp-tag&lt;/a&gt;. If you think you found a bug, please open an issue on Github.&lt;/p&gt; &#xA;&lt;p&gt;Open Asset Import Library is a library to load various 3d file formats into a shared, in-memory format. It supports more than &lt;strong&gt;40 file formats&lt;/strong&gt; for import and a growing selection of file formats for export.&lt;/p&gt; &#xA;&lt;p&gt;And we also have a Gitter-channel:Gitter &lt;a href=&#34;https://gitter.im/assimp/assimp?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/assimp/assimp.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/assimp/assimp&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;Contributions to assimp are highly appreciated. The easiest way to get involved is to submit a pull request with your changes against the main repository&#39;s &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;h3&gt;Code Contributors&lt;/h3&gt; &#xA;&lt;p&gt;This project exists thanks to all the people who contribute. [&lt;a href=&#34;https://raw.githubusercontent.com/assimp/assimp/master/CONTRIBUTING.md&#34;&gt;Contribute&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/assimp/assimp/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/assimp/contributors.svg?width=890&amp;amp;button=false&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Financial Contributors&lt;/h3&gt; &#xA;&lt;p&gt;Become a financial contributor and help us sustain our community. [&lt;a href=&#34;https://opencollective.com/assimp/contribute&#34;&gt;Contribute&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;Individuals&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/assimp&#34;&gt;&lt;img src=&#34;https://opencollective.com/assimp/individuals.svg?width=890&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Organizations&lt;/h4&gt; &#xA;&lt;p&gt;Support this project with your organization. Your logo will show up here with a link to your website. [&lt;a href=&#34;https://opencollective.com/assimp/contribute&#34;&gt;Contribute&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/assimp/organization/0/website&#34;&gt;&lt;img src=&#34;https://opencollective.com/assimp/organization/0/avatar.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;Our license is based on the modified, &lt;strong&gt;3-clause BSD&lt;/strong&gt;-License.&lt;/p&gt; &#xA;&lt;p&gt;An &lt;em&gt;informal&lt;/em&gt; summary is: do whatever you want, but include Assimp&#39;s license text with your product - and don&#39;t sue us if our code doesn&#39;t work. Note that, unlike LGPLed code, you may link statically to Assimp. For the legal details, see the &lt;code&gt;LICENSE&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h3&gt;Why this name&lt;/h3&gt; &#xA;&lt;p&gt;Sorry, we&#39;re germans :-), no english native speakers ...&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HKUST-Aerial-Robotics/VINS-Mono</title>
    <updated>2022-11-23T01:32:52Z</updated>
    <id>tag:github.com,2022-11-23:/HKUST-Aerial-Robotics/VINS-Mono</id>
    <link href="https://github.com/HKUST-Aerial-Robotics/VINS-Mono" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Robust and Versatile Monocular Visual-Inertial State Estimator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VINS-Mono&lt;/h1&gt; &#xA;&lt;h2&gt;A Robust and Versatile Monocular Visual-Inertial State Estimator&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;11 Jan 2019&lt;/strong&gt;: An extension of &lt;strong&gt;VINS&lt;/strong&gt;, which supports stereo cameras / stereo cameras + IMU / mono camera + IMU, is published at &lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Fusion&#34;&gt;VINS-Fusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;29 Dec 2017&lt;/strong&gt;: New features: Add map merge, pose graph reuse, online temporal calibration function, and support rolling shutter camera. Map reuse videos:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/embed/WDpH80nfZes&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/WDpH80nfZes/0.jpg&#34; alt=&#34;cla&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/embed/eINyJHB34uU&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/eINyJHB34uU/0.jpg&#34; alt=&#34;icra&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VINS-Mono is a real-time SLAM framework for &lt;strong&gt;Monocular Visual-Inertial Systems&lt;/strong&gt;. It uses an optimization-based sliding window formulation for providing high-accuracy visual-inertial odometry. It features efficient IMU pre-integration with bias correction, automatic estimator initialization, online extrinsic calibration, failure detection and recovery, loop detection, and global pose graph optimization, map merge, pose graph reuse, online temporal calibration, rolling shutter support. VINS-Mono is primarily designed for state estimation and feedback control of autonomous drones, but it is also capable of providing accurate localization for AR applications. This code runs on &lt;strong&gt;Linux&lt;/strong&gt;, and is fully integrated with &lt;strong&gt;ROS&lt;/strong&gt;. For &lt;strong&gt;iOS&lt;/strong&gt; mobile implementation, please go to &lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Mobile&#34;&gt;VINS-Mobile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;http://www.qintonguav.com&#34;&gt;Tong Qin&lt;/a&gt;, &lt;a href=&#34;https://github.com/PeiliangLi&#34;&gt;Peiliang Li&lt;/a&gt;, &lt;a href=&#34;https://github.com/dvorak0&#34;&gt;Zhenfei Yang&lt;/a&gt;, and &lt;a href=&#34;http://www.ece.ust.hk/ece.php/profile/facultydetail/eeshaojie&#34;&gt;Shaojie Shen&lt;/a&gt; from the &lt;a href=&#34;http://uav.ust.hk/&#34;&gt;HKUST Aerial Robotics Group&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Videos:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/embed/mv_9snb_bKs&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/mv_9snb_bKs/0.jpg&#34; alt=&#34;euroc&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/embed/g_wN0Nt0VAU&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/g_wN0Nt0VAU/0.jpg&#34; alt=&#34;indoor_outdoor&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/embed/I4txdvGhT6I&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/I4txdvGhT6I/0.jpg&#34; alt=&#34;AR_demo&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;EuRoC dataset; Indoor and outdoor performance; AR application;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/embed/2zE84HqT0es&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/2zE84HqT0es/0.jpg&#34; alt=&#34;MAV platform&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/embed/CI01qbPWlYY&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/CI01qbPWlYY/0.jpg&#34; alt=&#34;Mobile platform&#34; width=&#34;240&#34; height=&#34;180&#34; border=&#34;10&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MAV application; Mobile implementation (Video link for mainland China friends: &lt;a href=&#34;http://www.bilibili.com/video/av10813254/&#34;&gt;Video1&lt;/a&gt; &lt;a href=&#34;http://www.bilibili.com/video/av10813205/&#34;&gt;Video2&lt;/a&gt; &lt;a href=&#34;http://www.bilibili.com/video/av10813089/&#34;&gt;Video3&lt;/a&gt; &lt;a href=&#34;http://www.bilibili.com/video/av10813325/&#34;&gt;Video4&lt;/a&gt; &lt;a href=&#34;http://www.bilibili.com/video/av10813030/&#34;&gt;Video5&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Related Papers&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Online Temporal Calibration for Monocular Visual-Inertial Systems&lt;/strong&gt;, Tong Qin, Shaojie Shen, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS, 2018), &lt;strong&gt;best student paper award&lt;/strong&gt; &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/8593603&#34;&gt;pdf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator&lt;/strong&gt;, Tong Qin, Peiliang Li, Zhenfei Yang, Shaojie Shen, IEEE Transactions on Robotics&lt;a href=&#34;https://ieeexplore.ieee.org/document/8421746/?arnumber=8421746&amp;amp;source=authoralert&#34;&gt;pdf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;If you use VINS-Mono for your academic research, please cite at least one of our related papers.&lt;/em&gt;&lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Mono/raw/master/support_files/paper_bib.txt&#34;&gt;bib&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;1. Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;1.1 &lt;strong&gt;Ubuntu&lt;/strong&gt; and &lt;strong&gt;ROS&lt;/strong&gt; Ubuntu 16.04. ROS Kinetic. &lt;a href=&#34;http://wiki.ros.org/ROS/Installation&#34;&gt;ROS Installation&lt;/a&gt; additional ROS pacakge&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    sudo apt-get install ros-YOUR_DISTRO-cv-bridge ros-YOUR_DISTRO-tf ros-YOUR_DISTRO-message-filters ros-YOUR_DISTRO-image-transport&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;1.2. &lt;strong&gt;Ceres Solver&lt;/strong&gt; Follow &lt;a href=&#34;http://ceres-solver.org/installation.html&#34;&gt;Ceres Installation&lt;/a&gt;, remember to &lt;strong&gt;make install&lt;/strong&gt;. (Our testing environment: Ubuntu 16.04, ROS Kinetic, OpenCV 3.3.1, Eigen 3.3.3)&lt;/p&gt; &#xA;&lt;h2&gt;2. Build VINS-Mono on ROS&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository and catkin_make:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    cd ~/catkin_ws/src&#xA;    git clone https://github.com/HKUST-Aerial-Robotics/VINS-Mono.git&#xA;    cd ../&#xA;    catkin_make&#xA;    source ~/catkin_ws/devel/setup.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Visual-Inertial Odometry and Pose Graph Reuse on Public datasets&lt;/h2&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&#34;&gt;EuRoC MAV Dataset&lt;/a&gt;. Although it contains stereo cameras, we only use one camera. The system also works with &lt;a href=&#34;http://robotics.ethz.ch/~asl-datasets/maplab/multi_session_mapping_CLA/bags/&#34;&gt;ETH-asl cla dataset&lt;/a&gt;. We take EuRoC as the example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3.1 visual-inertial odometry and loop closure&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;3.1.1 Open three terminals, launch the vins_estimator , rviz and play the bag file respectively. Take MH_01 for example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    roslaunch vins_estimator euroc.launch &#xA;    roslaunch vins_estimator vins_rviz.launch&#xA;    rosbag play YOUR_PATH_TO_DATASET/MH_01_easy.bag &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(If you fail to open vins_rviz.launch, just open an empty rviz, then load the config file: file -&amp;gt; Open Config-&amp;gt; YOUR_VINS_FOLDER/config/vins_rviz_config.rviz)&lt;/p&gt; &#xA;&lt;p&gt;3.1.2 (Optional) Visualize ground truth. We write a naive benchmark publisher to help you visualize the ground truth. It uses a naive strategy to align VINS with ground truth. Just for visualization. not for quantitative comparison on academic publications.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    roslaunch benchmark_publisher publish.launch  sequence_name:=MH_05_difficult&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Green line is VINS result, red line is ground truth).&lt;/p&gt; &#xA;&lt;p&gt;3.1.3 (Optional) You can even run EuRoC &lt;strong&gt;without extrinsic parameters&lt;/strong&gt; between camera and IMU. We will calibrate them online. Replace the first command with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    roslaunch vins_estimator euroc_no_extrinsic_param.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;No extrinsic parameters&lt;/strong&gt; in that config file. Waiting a few seconds for initial calibration. Sometimes you cannot feel any difference as the calibration is done quickly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3.2 map merge&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;After playing MH_01 bag, you can continue playing MH_02 bag, MH_03 bag ... The system will merge them according to the loop closure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3.3 map reuse&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;3.3.1 map save&lt;/p&gt; &#xA;&lt;p&gt;Set the &lt;strong&gt;pose_graph_save_path&lt;/strong&gt; in the config file (YOUR_VINS_FOLEDER/config/euroc/euroc_config.yaml). After playing MH_01 bag, input &lt;strong&gt;s&lt;/strong&gt; in vins_estimator terminal, then &lt;strong&gt;enter&lt;/strong&gt;. The current pose graph will be saved.&lt;/p&gt; &#xA;&lt;p&gt;3.3.2 map load&lt;/p&gt; &#xA;&lt;p&gt;Set the &lt;strong&gt;load_previous_pose_graph&lt;/strong&gt; to 1 before doing 3.1.1. The system will load previous pose graph from &lt;strong&gt;pose_graph_save_path&lt;/strong&gt;. Then you can play MH_02 bag. New sequence will be aligned to the previous pose graph.&lt;/p&gt; &#xA;&lt;h2&gt;4. AR Demo&lt;/h2&gt; &#xA;&lt;p&gt;4.1 Download the &lt;a href=&#34;https://www.dropbox.com/s/s29oygyhwmllw9k/ar_box.bag?dl=0&#34;&gt;bag file&lt;/a&gt;, which is collected from HKUST Robotic Institute. For friends in mainland China, download from &lt;a href=&#34;https://pan.baidu.com/s/1geEyHNl&#34;&gt;bag file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;4.2 Open three terminals, launch the ar_demo, rviz and play the bag file respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    roslaunch ar_demo 3dm_bag.launch&#xA;    roslaunch ar_demo ar_rviz.launch&#xA;    rosbag play YOUR_PATH_TO_DATASET/ar_box.bag &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We put one 0.8m x 0.8m x 0.8m virtual box in front of your view.&lt;/p&gt; &#xA;&lt;h2&gt;5. Run with your device&lt;/h2&gt; &#xA;&lt;p&gt;Suppose you are familiar with ROS and you can get a camera and an IMU with raw metric measurements in ROS topic, you can follow these steps to set up your device. For beginners, we highly recommend you to first try out &lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Mobile&#34;&gt;VINS-Mobile&lt;/a&gt; if you have iOS devices since you don&#39;t need to set up anything.&lt;/p&gt; &#xA;&lt;p&gt;5.1 Change to your topic name in the config file. The image should exceed 20Hz and IMU should exceed 100Hz. Both image and IMU should have the accurate time stamp. IMU should contain absolute acceleration values including gravity.&lt;/p&gt; &#xA;&lt;p&gt;5.2 Camera calibration:&lt;/p&gt; &#xA;&lt;p&gt;We support the &lt;a href=&#34;http://docs.opencv.org/2.4.8/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html&#34;&gt;pinhole model&lt;/a&gt; and the &lt;a href=&#34;http://www.robots.ox.ac.uk/~cmei/articles/single_viewpoint_calib_mei_07.pdf&#34;&gt;MEI model&lt;/a&gt;. You can calibrate your camera with any tools you like. Just write the parameters in the config file in the right format. If you use rolling shutter camera, please carefully calibrate your camera, making sure the reprojection error is less than 0.5 pixel.&lt;/p&gt; &#xA;&lt;p&gt;5.3 &lt;strong&gt;Camera-Imu extrinsic parameters&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;If you have seen the config files for EuRoC and AR demos, you can find that we can estimate and refine them online. If you familiar with transformation, you can figure out the rotation and position by your eyes or via hand measurements. Then write these values into config as the initial guess. Our estimator will refine extrinsic parameters online. If you don&#39;t know anything about the camera-IMU transformation, just ignore the extrinsic parameters and set the &lt;strong&gt;estimate_extrinsic&lt;/strong&gt; to &lt;strong&gt;2&lt;/strong&gt;, and rotate your device set at the beginning for a few seconds. When the system works successfully, we will save the calibration result. you can use these result as initial values for next time. An example of how to set the extrinsic parameters is in&lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Mono/raw/master/config/extrinsic_parameter_example.pdf&#34;&gt;extrinsic_parameter_example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;5.4 &lt;strong&gt;Temporal calibration&lt;/strong&gt;: Most self-made visual-inertial sensor sets are unsynchronized. You can set &lt;strong&gt;estimate_td&lt;/strong&gt; to 1 to online estimate the time offset between your camera and IMU.&lt;/p&gt; &#xA;&lt;p&gt;5.5 &lt;strong&gt;Rolling shutter&lt;/strong&gt;: For rolling shutter camera (carefully calibrated, reprojection error under 0.5 pixel), set &lt;strong&gt;rolling_shutter&lt;/strong&gt; to 1. Also, you should set rolling shutter readout time &lt;strong&gt;rolling_shutter_tr&lt;/strong&gt;, which is from sensor datasheet(usually 0-0.05s, not exposure time). Don&#39;t try web camera, the web camera is so awful.&lt;/p&gt; &#xA;&lt;p&gt;5.6 Other parameter settings: Details are included in the config file.&lt;/p&gt; &#xA;&lt;p&gt;5.7 Performance on different devices:&lt;/p&gt; &#xA;&lt;p&gt;(global shutter camera + synchronized high-end IMU, e.g. VI-Sensor) &amp;gt; (global shutter camera + synchronized low-end IMU) &amp;gt; (global camera + unsync high frequency IMU) &amp;gt; (global camera + unsync low frequency IMU) &amp;gt; (rolling camera + unsync low frequency IMU).&lt;/p&gt; &#xA;&lt;h2&gt;6. Docker Support&lt;/h2&gt; &#xA;&lt;p&gt;To further facilitate the building process, we add docker in our code. Docker environment is like a sandbox, thus makes our code environment-independent. To run with docker, first make sure &lt;a href=&#34;http://wiki.ros.org/ROS/Installation&#34;&gt;ros&lt;/a&gt; and &lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/ubuntu/&#34;&gt;docker&lt;/a&gt; are installed on your machine. Then add your account to &lt;code&gt;docker&lt;/code&gt; group by &lt;code&gt;sudo usermod -aG docker $YOUR_USER_NAME&lt;/code&gt;. &lt;strong&gt;Relaunch the terminal or logout and re-login if you get &lt;code&gt;Permission denied&lt;/code&gt; error&lt;/strong&gt;, type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ~/catkin_ws/src/VINS-Mono/docker&#xA;make build&#xA;./run.sh LAUNCH_FILE_NAME   # ./run.sh euroc.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the docker building process may take a while depends on your network and machine. After VINS-Mono successfully started, open another terminal and play your bag file, then you should be able to see the result. If you need modify the code, simply run &lt;code&gt;./run.sh LAUNCH_FILE_NAME&lt;/code&gt; after your changes.&lt;/p&gt; &#xA;&lt;h2&gt;7. Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;http://ceres-solver.org/&#34;&gt;ceres solver&lt;/a&gt; for non-linear optimization and &lt;a href=&#34;https://github.com/dorian3d/DBoW2&#34;&gt;DBoW2&lt;/a&gt; for loop detection, and a generic &lt;a href=&#34;https://github.com/hengli/camodocal&#34;&gt;camera model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;8. Licence&lt;/h2&gt; &#xA;&lt;p&gt;The source code is released under &lt;a href=&#34;http://www.gnu.org/licenses/&#34;&gt;GPLv3&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;p&gt;We are still working on improving the code reliability. For any technical issues, please contact Tong QIN &amp;lt;tong.qinATconnect.ust.hk&amp;gt; or Peiliang LI &amp;lt;pliapATconnect.ust.hk&amp;gt;.&lt;/p&gt; &#xA;&lt;p&gt;For commercial inquiries, please contact Shaojie SHEN &amp;lt;eeshaojieATust.hk&amp;gt;&lt;/p&gt;</summary>
  </entry>
</feed>