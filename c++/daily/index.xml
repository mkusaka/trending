<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-12T01:28:47Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>turing-machines/mentals-ai</title>
    <updated>2024-09-12T01:28:47Z</updated>
    <id>tag:github.com,2024-09-12:/turing-machines/mentals-ai</id>
    <link href="https://github.com/turing-machines/mentals-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🍓🍓🍓 Agents in Markdown syntax (loops, memory and tools included).&lt;/p&gt;&lt;hr&gt;&lt;p&gt;🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓🍓&lt;/p&gt; &#xA;&lt;p&gt;Mentals AI is a tool designed for creating and operating agents that feature &lt;strong&gt;&lt;code&gt;loops&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;memory&lt;/code&gt;&lt;/strong&gt;, and various &lt;strong&gt;&lt;code&gt;tools&lt;/code&gt;&lt;/strong&gt;, all through straightforward &lt;strong&gt;&lt;code&gt;markdown&lt;/code&gt;&lt;/strong&gt; file with a .gen extension. Think of an agent file as an executable file. You focus entirely on the logic of the agent, eliminating the necessity to write scaffolding code in Python or any other language. Essentially, it redefines the foundational frameworks for future AI applications.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[work in progress] A local vector database to store your chats with the agents as well as your private information. See &lt;a href=&#34;https://github.com/turing-machines/mentals-ai/tree/memory&#34;&gt;memory&lt;/a&gt; branch.&lt;/li&gt; &#xA;  &lt;li&gt;[work in progress] Web UI with agents, tools and vector storage&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#-getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#-differences-from-other-frameworks&#34;&gt;Differences from Other Frameworks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#%EF%B8%8F-key-concepts&#34;&gt;Key Concepts&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#-instruction-prompt&#34;&gt;Instruction (prompt)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#-working-memory-context&#34;&gt;Working Memory (context)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#-short-term-memory-experimental&#34;&gt;Short-Term Memory (experimental)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#%EF%B8%8F-control-flow-from-strings-to-algorithms&#34;&gt;Control flow: From strings to algorithms&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#%EF%B8%8F-roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/#-the-idea&#34;&gt;The Idea&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📌 Examples&lt;/h2&gt; &#xA;&lt;p&gt;Word chain game in a self-loop controlled by LLM: &lt;img src=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/assets/word_chain.gen.gif&#34; alt=&#34;Word Chain game in a loop&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NLOP — Natural Language Operation&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Or more complex use cases:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;🔄 Any multi-agent interactions&lt;/th&gt; &#xA;   &lt;th&gt;👾 Space Invaders generator agent&lt;/th&gt; &#xA;   &lt;th&gt;🍄 2D platformer generator agent&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/assets/react.png&#34; alt=&#34;react&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/assets/space_invaders.gif&#34; alt=&#34;space_invaders.gen&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/assets/mario.gif&#34; alt=&#34;mario.gen&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Or help with the content:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Collect YouTube videos on a given topic and save to a .csv file with the videos, views, channel name, and link;&lt;/li&gt; &#xA; &lt;li&gt;Get the transcription from the video and create a table of contents;&lt;/li&gt; &#xA; &lt;li&gt;Take top news from Hacker News, choose a topic and write an article on the topic with the participation of the critic and save to a file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of the above examples are located in the &lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/agents&#34;&gt;agents&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Llama3 support is available for providers using a compatible OpenAI API.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Begin by securing an OpenAI API key through the creation of an &lt;a href=&#34;https://platform.openai.com/docs/quickstart?context=node&#34;&gt;OpenAI account&lt;/a&gt;. If you already have an API key, skip this step.&lt;/p&gt; &#xA;&lt;h3&gt;🏗️ Build and Run&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before building the project, ensure the following dependencies are installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;libcurl: Used for making HTTP requests&lt;/li&gt; &#xA; &lt;li&gt;libfmt: Provides an API for formatting&lt;/li&gt; &#xA; &lt;li&gt;pgvector: Vector operations with PostgreSQL&lt;/li&gt; &#xA; &lt;li&gt;poppler: Required for PDF processing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Depending on your operating system, you can install these using the following commands:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get update&#xA;sudo apt-get install libcurl4-openssl-dev libfmt-dev libpoppler-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew update&#xA;brew install curl fmt poppler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For Windows, it&#39;s recommended to use vcpkg or a similar package manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vcpkg install curl fmt poppler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;pgvector installation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] In the &lt;code&gt;main&lt;/code&gt; branch you can skip this step&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pgvector/pgvector?tab=readme-ov-file#installation&#34;&gt;Build from sources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pgvector/pgvector?tab=readme-ov-file#additional-installation-methods&#34;&gt;Docker, Homebrew, PGXN, APT, etc.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/turing-machines/mentals-ai&#xA;cd mentals-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Place your API key in the &lt;code&gt;config.toml&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[llm]&#xA;# OpenAI&#xA;api_key = &#34;&#34;&#xA;endpoint = &#34;https://api.openai.com/v1&#34;&#xA;model = &#34;gpt-4o&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Build the project&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./build/mentals agents/loop.gen -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🆚 Differences from Other Frameworks&lt;/h2&gt; &#xA;&lt;p&gt;Mentals AI distinguishes itself from other frameworks in three significant ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;Agent Executor&lt;/code&gt; 🧠 operates through a recursive loop. The LLM determines the next steps: selecting instructions (prompts) and managing data based on previous loops. This recursive decision-making process is integral to our system, outlined in &lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/mentals_system.prompt&#34;&gt;mentals_system.prompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Agents of any complexity can be created using &lt;code&gt;Markdown&lt;/code&gt;, eliminating the need for traditional programming languages. However, Python can be integrated directly into the agent&#39;s &lt;code&gt;Markdown&lt;/code&gt; script if necessary.&lt;/li&gt; &#xA; &lt;li&gt;Unlike platforms that include preset reasoning frameworks, Mentals AI serves as a blank canvas. It enables the creation and integration of your own reasoning frameworks, including existing ones: &lt;code&gt;Tree of Thoughts&lt;/code&gt;, &lt;code&gt;ReAct&lt;/code&gt;, &lt;code&gt;Self-Discovery&lt;/code&gt;, &lt;code&gt;Auto-CoT&lt;/code&gt;, and others. One can also link these frameworks together into more complex sequences, even creating a network of various reasoning frameworks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🗝️ Key Concepts&lt;/h2&gt; &#xA;&lt;p&gt;The agent file is a textual description of the agent instructions with a &lt;code&gt;.gen&lt;/code&gt; extension.&lt;/p&gt; &#xA;&lt;h3&gt;📖 Instruction (prompt)&lt;/h3&gt; &#xA;&lt;p&gt;Instruction is the basic component of an agent in Mentals. An agent can consist of one or more instructions, which can refer to each other.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/assets/instructions.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;Instructions can be written in free form, but they always have a name that starts with the &lt;code&gt;#&lt;/code&gt; symbol. The &lt;code&gt;## use:&lt;/code&gt; directive is used to specify a reference to other instructions. Multiple references are listed separated by commas.&lt;/p&gt; &#xA;&lt;p&gt;Below is an example with two instructions &lt;code&gt;root&lt;/code&gt; and &lt;code&gt;meme_explain&lt;/code&gt; with a reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# root&#xA;## use: meme_explain&#xA;&#xA;1. Create 3 memes about AGI;&#xA;2. Then do meme explain with meme per call;&#xA;3. Output memes and their explanations in a list.&#xA;&#xA;# meme_explain&#xA;&#xA;Explain the gist of the meme in 20 words in medieval style.&#xA;Return explanation.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, the &lt;code&gt;root&lt;/code&gt; instruction calls the &lt;code&gt;meme_explain&lt;/code&gt; instruction. The response from meme_explain is then returned to the instruction from which it was called, namely the root.&lt;/p&gt; &#xA;&lt;p&gt;An instruction can take an &lt;code&gt;input&lt;/code&gt; parameter, which is automatically generated based on the context when the instruction is called. To specify the input data more precisely, you can use a &lt;em&gt;free-form prompt&lt;/em&gt; in the &lt;code&gt;## input:&lt;/code&gt; directive, such as a JSON object or &lt;code&gt;null&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Using a document for input:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# some_instruction&#xA;## input: design document only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using a JSON object as input:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# duckduckgo&#xA;## input: { search_query: search query, search_limit: search limit }&#xA;&#xA;Write a Python script to search in DuckDuckGo.&#xA;Simulate request headers correctly e.g. user-agent as Mozilla and Linux.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Instruction calls are implemented independently from function or tool calls at OpenAI, enabling the operation of agents with models like Llama3. The implementation of instruction calls is transparent and included in the mentals_system.prompt file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;🛠️ Tool&lt;/h4&gt; &#xA;&lt;p&gt;Tool is a kind of instruction. Mentals has a set of native tools to handle message output, user input, file handling, Python interpreter, Bash commands, and Short-term memory.&lt;/p&gt; &#xA;&lt;p&gt;Ask user example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# root&#xA;## use: user_input&#xA;&#xA;Ask user name.&#xA;Then output: `Welcome, user_name!`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;File handling example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# root&#xA;## use: write_file, read_file&#xA;&#xA;Write &#39;Hello world&#39; to a file.&#xA;Then read and output file content.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The full list of native tools is listed in the file &lt;code&gt;native_tools.toml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;🧠 Working Memory (context)&lt;/h3&gt; &#xA;&lt;p&gt;Each instruction has its own working memory — context. When exiting an instruction and re-entering it, the context is kept by default. To clear the context when exiting an instruction, you can use the &lt;code&gt;## keep_context: false&lt;/code&gt; directive:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# meme_explain&#xA;## keep_context: false&#xA;&#xA;Explain the gist of the meme in 20 words in medieval style.&#xA;Return explanation.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the size of the instruction context is not limited. To limit the context, there is a directive &lt;code&gt;## max_context: number&lt;/code&gt; which specifies that only the &lt;code&gt;number&lt;/code&gt; of the most recent messages should be stored. Older messages will be pushed out of the context. This feature is useful when you want to keep the most recent data in context so that older data does not affect the chain of reasoning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# python_code_generation&#xA;## input: development tasks in a list&#xA;## use: write_file&#xA;## max_context: 5&#xA;&#xA;Do all development tasks in a loop: task by task.&#xA;Save the Python code you implement in the main.py file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;⏳ Short-Term Memory (experimental)&lt;/h3&gt; &#xA;&lt;p&gt;Short-term memory allows for the storage of intermediate results from an agent&#39;s activities, which can then be used for further reasoning. The contents of this memory are accessible across all instruction contexts.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;memory&lt;/code&gt; tool is used to store data. When data is stored, a keyword and a description of the content are generated. In the example below, the &lt;code&gt;meme_recall&lt;/code&gt; instruction is aware of the meme because it was previously stored in memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# root&#xA;## use: memory, meme_recall&#xA;&#xA;Come up with and memorize a meme.&#xA;Call meme recall.&#xA;&#xA;# meme_recall&#xA;## input: nothing&#xA;&#xA;What the meme was about?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;⚙️ Control flow: From strings to algorithms&lt;/h3&gt; &#xA;&lt;p&gt;The control flow, which includes conditions, instruction calls, and loops (such as &lt;code&gt;ReAct&lt;/code&gt;, &lt;code&gt;Auto-CoT&lt;/code&gt;, etc.), is fully expressed in natural language. This method enables the creation of &lt;code&gt;semantic conditions&lt;/code&gt; that direct data stream branching. For instance, you can request an agent to autonomously play a word chain game in a loop or establish an ambiguous exit condition: &lt;code&gt;exit the loop if you are satisfied with the result&lt;/code&gt;. Here, the language model and its context determine whether to continue or stop. All this is achieved without needing to define flow logic in Python or any other programming languages.&lt;/p&gt; &#xA;&lt;h4&gt;⚖️ Reason Action (ReAct) example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;## use: execute_bash_command, software_development, quality_assurance&#xA;&#xA;...&#xA;You run in a loop of &#34;Thought&#34;, &#34;Action&#34;, &#34;Observation&#34;.&#xA;At the end of the loop return with the final answer.&#xA;Use &#34;Thought&#34; to describe your thoughts about the task &#xA;you have been asked. Use &#34;Action&#34; to run one of the actions &#xA;available to you. Output action as: &#34;Action: action name to call&#34;.&#xA;&#34;Observation&#34; will be the result of running those actions.&#xA;&#xA;Your available actions:&#xA;- `execute_bash_command` for util purposes e.g. make directory, install packages, etc.;&#xA;- `software_development` for software development and bug fixing purposes;&#xA;- `quality_assurance` for QA testing purposes.&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;🌳 Tree of Thoughts (ToT) example&lt;/h4&gt; &#xA;&lt;p&gt;The idea behind ToT is to generate multiple ideas to solve a problem and then evaluate their value. Valuable ideas are kept and developed, other ideas are discarded.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s take the example of the 24 game. The 24 puzzle is an arithmetical puzzle in which the objective is to find a way to manipulate four integers so that the end result is 24. First, we define the instruction that creates and manipulates the tree data structure. The model knows what a tree is and can represent it in any format, from plain text to XML/JSON or any custom format.&lt;/p&gt; &#xA;&lt;p&gt;In this example, we will use the plain text format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# tree&#xA;## input: e.g. &#34;add to node `A` child nodes `B` and `C`&#34;, &#34;remove node `D` with all branches&#34;, etc. &#xA;## use: memory&#xA;## keep_context: false&#xA;&#xA;Build/update tree structure in formatted text.&#xA;&#xA;Update the tree structure within the specified action;&#xA;Memorize final tree structure.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next we need to initialize the tree with initial data, let&#39;s start with the root instruction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# root&#xA;## use: tree&#xA;&#xA;Input: 4 5 8 2&#xA;Generate 8 possible next steps.&#xA;Store all steps in the tree as nodes e.g. &#xA;Node value 1: &#34;2 + 8 = 10 (left: 8 10 14)&#34;&#xA;Node value 2: &#34;8 / 2 = 4 (left: 4 8 14)&#34;&#xA;etc.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Calling the root instruction will suggest 8 possible next steps to calculate with the first 2 numbers and store these steps as tree nodes. Further work by the agent results in the construction of a tree that is convenient for the model to understand and infer the final answer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;4 5 8 2&#xA;├── 4 + 5 = 9 (left: 9, 8, 2)&#xA;│   └── discard&#xA;├── 4 + 8 = 12 (left: 12, 5, 2)&#xA;│   └── discard&#xA;├── 4 + 2 = 6 (left: 6, 5, 8)&#xA;│   └── discard&#xA;├── 5 + 8 = 13 (left: 13, 4, 2)&#xA;│   └── discard&#xA;├── 5 + 2 = 7 (left: 7, 4, 8)&#xA;│   └── (7 - 4) * 8 = 24&#xA;├── 8 + 2 = 10 (left: 10, 4, 5)&#xA;│   └── discard&#xA;├── 4 * 5 = 20 (left: 20, 8, 2)&#xA;│   └── (20 - 8) * 2 = 24&#xA;└── 4 * 8 = 32 (left: 32, 5, 2)&#xA;    └── discard&#xA;&#xA;Based on the evaluations, we have found two successful paths to reach 24:&#xA;&#xA;1. From the node &#34;5 + 2 = 7 (left: 7, 4, 8)&#34;, we have the equation: (7 - 4) * 8 = 24.&#xA;2. From the node &#34;4 * 5 = 20 (left: 20, 8, 2)&#34;, we have the equation: (20 - 8) * 2 = 24.&#xA;&#xA;Thus, the final equations using all given numbers from the input are:&#xA;1. (5 + 2 - 4) * 8 = 24&#xA;2. (4 * 5 - 8) * 2 = 24&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A complete example is contained in the &lt;a href=&#34;https://raw.githubusercontent.com/turing-machines/mentals-ai/main/agents/tree_structure.gen&#34;&gt;agents/tree_structure.gen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🗺️ Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Web UI -- WIP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Vector database tools -- WIP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Agent&#39;s experience (experimental)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tools: Image generation, Browser&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✨ The Idea&lt;/h2&gt; &#xA;&lt;p&gt;The concept originated from studies on psychoanalysis &lt;a href=&#34;https://en.wikipedia.org/wiki/Executive_functions&#34;&gt;Executive functions&lt;/a&gt;, &lt;a href=&#34;https://www.krigolsonteaching.com/uploads/4/3/8/4/43848243/baddeley_1996.pdf&#34;&gt;Exploring Central Executive, Alan Baddeley, 1996&lt;/a&gt;. He described a system that orchestrates cognitive processes and working memory, facilitating retrievals from long-term memory. The LLM functions as &lt;code&gt;System 1&lt;/code&gt;, processing queries and executing instructions without inherent motivation or goal-setting. So, what then is &lt;code&gt;System 2&lt;/code&gt;? Drawing from historical insights, now reconsidered through a scientific lens:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The central executive, or executive functions, is crucial for controlled processing in working memory. It manages tasks including directing attention, maintaining task objectives, decision-making, and memory retrieval.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This sparks an intriguing possibility: constructing more sophisticated agents by integrating &lt;code&gt;System 1&lt;/code&gt; and &lt;code&gt;System 2&lt;/code&gt;. The LLM, as the cognitive executor &lt;code&gt;System 1&lt;/code&gt;, works in tandem with the Central Executive &lt;code&gt;System 2&lt;/code&gt;, which governs and controls the LLM. This partnership forms the dual relationship foundational to Mentals AI.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Shopify/react-native-skia</title>
    <updated>2024-09-12T01:28:47Z</updated>
    <id>tag:github.com,2024-09-12:/Shopify/react-native-skia</id>
    <link href="https://github.com/Shopify/react-native-skia" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-performance React Native Graphics using Skia&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;React Native Skia&lt;/h1&gt; &#xA;&lt;p&gt;High-performance 2d Graphics for React Native using Skia&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Shopify/react-native-skia/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/Shopify/react-native-skia/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@shopify/react-native-skia&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@shopify/react-native-skia.svg?style=flat&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shopify/react-native-skia/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/shopify/react-native-skia.svg?style=flat&#34; alt=&#34;issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;400&#34; alt=&#34;skia&#34; src=&#34;https://user-images.githubusercontent.com/306134/146549218-b7959ad9-0107-4c1c-b439-b96c780f5230.png&#34;&gt; &#xA;&lt;p&gt;Checkout the full documentation &lt;a href=&#34;https://shopify.github.io/react-native-skia&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;React Native Skia brings the Skia Graphics Library to React Native. Skia serves as the graphics engine for Google Chrome and Chrome OS, Android, Flutter, Mozilla Firefox and Firefox OS, and many other products.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://shopify.github.io/react-native-skia/docs/getting-started/installation/&#34;&gt;Installation instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Library Development&lt;/h2&gt; &#xA;&lt;p&gt;To develop react-native-skia, you need to build the skia libraries on your computer.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to check out the sub modules:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You also need to install some tools for the build scripts to work. Run &lt;code&gt;yarn&lt;/code&gt; in the root of the project to install them.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have all the tools required for building the skia libraries (XCode, Ninja, CMake, Android NDK / build tools).&lt;/p&gt; &#xA;&lt;p&gt;On MacOS you can install Ninja via homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;brew install ninja&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have Android Studio installed, make sure &lt;code&gt;$ANDROID_NDK&lt;/code&gt; is available. &lt;code&gt;ANDROID_NDK=/Users/username/Library/Android/sdk/ndk/&amp;lt;version&amp;gt;&lt;/code&gt; for instance.&lt;/p&gt; &#xA;&lt;p&gt;If the NDK is not installed, you can install it via Android Studio by going to the menu &lt;em&gt;File &amp;gt; Project Structure&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And then the &lt;em&gt;SDK Location&lt;/em&gt; section. It will show you the NDK path, or the option to Download it if you don&#39;t have it installed.&lt;/p&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;yarn&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go to the package folder &lt;code&gt;cd packages/skia&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build the Skia libraries with &lt;code&gt;yarn build-skia&lt;/code&gt; (this can take a while)&lt;/li&gt; &#xA; &lt;li&gt;Copy Skia headers &lt;code&gt;yarn copy-skia-headers&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;yarn pod:install&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Upgrading&lt;/h3&gt; &#xA;&lt;p&gt;If a new version of Skia is included in an upgrade of this library, you need to perform a few extra steps before continuing:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Update submodules: &lt;code&gt;git submodule update --recursive --remote&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Clean Skia: &lt;code&gt;yarn clean-skia&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build Skia: &lt;code&gt;yarn build-skia&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy Skia Headers: &lt;code&gt;yarn copy-skia-headers&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run pod install in the example project&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Publishing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the commands in the &lt;a href=&#34;https://raw.githubusercontent.com/Shopify/react-native-skia/main/#building&#34;&gt;Building&lt;/a&gt; section&lt;/li&gt; &#xA; &lt;li&gt;Build the Android binaries with &lt;code&gt;yarn build-skia-android&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build the NPM package with &lt;code&gt;yarn build-npm&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Publish the NPM package manually. The output is found in the &lt;code&gt;dist&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Cocoapods in the example/ios folder &lt;code&gt;cd example/ios &amp;amp;&amp;amp; pod install &amp;amp;&amp;amp; cd ..&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;When making contributions to the project, an important part is testing. In the &lt;code&gt;package&lt;/code&gt; folder, we have several scripts set up to help you maintain the quality of the codebase and test your changes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn lint&lt;/code&gt; — Lints the code for potential errors and to ensure consistency with our coding standards.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn tsc&lt;/code&gt; — Runs the TypeScript compiler to check for typing issues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn test&lt;/code&gt; — Executes the unit tests to ensure existing features work as expected after changes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn e2e&lt;/code&gt; — Runs end-to-end tests. For these tests to run properly, you need to have the example app running. Use &lt;code&gt;yarn ios&lt;/code&gt; or &lt;code&gt;yarn android&lt;/code&gt; in the &lt;code&gt;example&lt;/code&gt; folder and navigate to the Tests screen within the app.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running End-to-End Tests&lt;/h2&gt; &#xA;&lt;p&gt;To ensure the best reliability, we encourage running end-to-end tests before submitting your changes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the example app:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd example&#xA;yarn ios # or yarn android for Android testing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the app is open in your simulator or device, press the &#34;Tests&#34; item at the bottom of the list.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;With the example app running and the Tests screen open, run the following command in the &lt;code&gt;package&lt;/code&gt; folder:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;yarn e2e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run through the automated tests and verify that your changes have not introduced any regressions. You can also run a particular using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;E2E=true yarn test -i e2e/Colors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Writing End-to-End Tests&lt;/h3&gt; &#xA;&lt;p&gt;Contributing end-to-end tests to React Native Skia is extremely useful. Below you&#39;ll find guidelines for writing tests using the &lt;code&gt;eval&lt;/code&gt;, &lt;code&gt;draw&lt;/code&gt;, and &lt;code&gt;drawOffscreen&lt;/code&gt; commands.&lt;/p&gt; &#xA;&lt;p&gt;e2e tests are located in the &lt;code&gt;package/__tests__/e2e/&lt;/code&gt; directory. You can create a file there or add a new test to an existing file depending on what is most sensible. When looking to contribute a new test, you can refer to existing tests to see how these can be built. The &lt;code&gt;eval&lt;/code&gt; command is used to test Skia&#39;s imperative API. It requires a pure function that invokes Skia operations and returns a serialized result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tsx&#34;&gt;it(&#34;should generate commands properly&#34;, async () =&amp;gt; {&#xA;  const result = await surface.eval((Skia) =&amp;gt; {&#xA;    const path = Skia.Path.Make();&#xA;    path.lineTo(30, 30);&#xA;    return path.toCmds();&#xA;  });&#xA;  expect(result).toEqual([[0, 0, 0], [1, 30, 30]]);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Both the &lt;code&gt;eval&lt;/code&gt; and &lt;code&gt;draw&lt;/code&gt; commands require a function that will be executed in an isolated context, so the functions must be pure (without external dependencies) and serializable. You can use the second parameter to provide extra data to that function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tsx&#34;&gt;it(&#34;should generate commands properly&#34;, async () =&amp;gt; {&#xA;  // Referencing the SVG variable directly in the tests would fail&#xA;  // as the function wouldn&#39;t be able to run in an isolated context&#xA;  const svg = &#34;M 0 0, L 30 30&#34;;&#xA;  const result = await surface.eval((Skia, ctx) =&amp;gt; {&#xA;    const path = Skia.Path.MakeFromSVGString(ctx.svg);&#xA;    return path.toCmds();&#xA;  }, { svg });&#xA;  expect(result).toEqual([[0, 0, 0], [1, 30, 30]]);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A second option is to use the &lt;code&gt;draw&lt;/code&gt; command where you can test the Skia components and get the resulting image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tsx&#34;&gt;it(&#34;Path with default fillType&#34;, async () =&amp;gt; {&#xA;  const { Skia } = importSkia();&#xA;  const path = star(Skia);&#xA;  const img = await surface.draw(&#xA;    &amp;lt;&amp;gt;&#xA;      &amp;lt;Fill color=&#34;white&#34; /&amp;gt;&#xA;      &amp;lt;Path path={path} style=&#34;stroke&#34; strokeWidth={4} color=&#34;#3EB489&#34; /&amp;gt;&#xA;      &amp;lt;Path path={path} color=&#34;lightblue&#34; /&amp;gt;&#xA;    &amp;lt;/&amp;gt;&#xA;  );&#xA;  checkImage(image, &#34;snapshots/drawings/path.png&#34;);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can use &lt;code&gt;drawOffscreen&lt;/code&gt; to receive a canvas object as parameter. You will also get the resulting image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tsx&#34;&gt;  it(&#34;Should draw cyan&#34;, async () =&amp;gt; {&#xA;    const image = await surface.drawOffscreen(&#xA;      (Skia, canvas, { size }) =&amp;gt; {&#xA;        canvas.drawColor(Skia.Color(&#34;cyan&#34;));&#xA;      }&#xA;    );&#xA;    checkImage(image, &#34;snapshots/cyan.png&#34;);&#xA;  });&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, since &lt;code&gt;eval&lt;/code&gt;, &lt;code&gt;draw&lt;/code&gt;, and &lt;code&gt;drawOffscreen&lt;/code&gt; serialize the function&#39;s content, avoid any external dependencies that can&#39;t be serialized.&lt;/p&gt;</summary>
  </entry>
</feed>