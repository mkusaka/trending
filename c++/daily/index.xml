<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T01:31:19Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>BigCorvus/LORA-QWERTY-Communicator</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/BigCorvus/LORA-QWERTY-Communicator</id>
    <link href="https://github.com/BigCorvus/LORA-QWERTY-Communicator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A tidy and feature-packed LORA QWERTY communication device based on a Blackberry Q10 keyboard, a nRF52840 and a 2.7&#39;&#39; Sharp Memory LCD&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LORA-QWERTY-Communicator&lt;/h1&gt; &#xA;&lt;p&gt;A tidy, versatile and feature-packed LORA QWERTY communication device mainly based on a Blackberry Q10 keyboard, nRF52840, SX1262 and a 2.7&#39;&#39; Sharp Memory LCD (LS027B7DH01). I created this because I wanted to practice a little and put some components that I had laying around to good use. The system had to have a low power consumption and outdoor usability, hence the component choices. The SX1262 draws 4,6mA in RX mode and the nRF52 can poll the keyboard while also drawing little power. Without optimizing anything I got 12mA. This can be further reduced by turning off some sensors. The unit can monitor its own power consumption and battery charge status via a BQ27441 Lithium Fuel Gauge.&lt;br&gt; &lt;img src=&#34;https://github.com/BigCorvus/LORA-QWERTY-Communicator/raw/main/Q10%20Lora%20Communicator/Images/20220512_214537.jpg&#34; alt=&#34;finished devices&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some words about the memory LCDs: I got fake ones from Ali and only one out of three works properly! One consumes 20mA instead of ~400¬µA and one had some dead pixels after some days of usage (seen on the pictures). They are not that much cheaper (~25 vs ca 32$), so avoid them and only get genuine SHARP displays from digikey or so.&lt;br&gt; There is a passive piezo buzzer (9032 type) and a vibration motor (&#34;10x4mm&#34; look on Aliexpress) for notification purposes. A BME280 can be soldered for altimeter functionality. A DS3231M RTC is supposed to keep time (the wires on the images are for optional backup battery connection). A MPU9250 9-DOF IMU was included to act as a tilt-compensated compass or for air mouse functionality. A GD25Q16CE QSPI flash (apparently optional, not required by the bootloader, works with mass storage TinyUSB test sketch) and an ¬µSD card slot for storage of for example chat history or sensor values. An optional GPS module can be connected to the exposed connector at the top of the device. I used a BN220 which fits into the enclosure if you use an appropriate LiPo cell. All I2C sensor are on one I2C bus and are optional. They do not need to be populated. Time and altitude can just as well be fetched via GPS. The connector for the Q10 keyboard is a Hirose BM14B(0.8)-24DS-0.4V(53)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/BigCorvus/LORA-QWERTY-Communicator/raw/main/Q10%20Lora%20Communicator/Images/20220512_214741.jpg&#34; alt=&#34;open devices&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/BigCorvus/LORA-QWERTY-Communicator/raw/main/Q10%20Lora%20Communicator/Images/20220512_214833.jpg&#34; alt=&#34;open devices 2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The repository contains the EAGLE PCB design files, gerber, pick and place and BOM files, Solidworks design files and .STL for a perfectly fitting enclosure and an Arduino test sketch to test most of the system components. The enclosure lacks inserts for the pushbutton and slide swith. This I will add in the future. You require 4 2mmx10mm plastic screws to hold it together. The resulting device has a thickness of only about 13,5mm.&lt;br&gt; While the hardware is confirmed working (only one minor bug had to be fixed in the prototype - conflicting I2C addresses), the firmware part is very raw and has yet to be developed. Unfortunately I lack time desperately for such hobby things at the moment. What I had in mind for this device is mainly something like an encrypted &#34;apocalypse&#34; LORA communicator, maybe even based on Meshtastic. It can also communicate with other BLE devices and acquire, log and share sensor data or act as a USB or BLE HID keyboard. The test firmware is based on the Adafruit nRF52 core v1.0.0. In order for the device to be programmable via the Arduino IDE, the Feather nRF52840 Express bootloader has to be flashed, which I usually do via a J-Link and the Arduino IDE (&#34;Burn Bootloader&#34;). The bootloader can be activated at any time double-pulling the reset pin low. In a future revision there should also be reset button for that purpose, now there is at least a test point. The variant files for the Feather nRF52840 express need to be modified as well before programming the board. They are provided with the test sketch together with a readme.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/BigCorvus/LORA-QWERTY-Communicator/raw/main/Q10%20Lora%20Communicator/Images/q10-lora-bottom.png&#34; alt=&#34;bottom&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/BigCorvus/LORA-QWERTY-Communicator/raw/main/Q10%20Lora%20Communicator/Images/q10-lora-top.png&#34; alt=&#34;top&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/BigCorvus/LORA-QWERTY-Communicator/raw/main/Q10%20Lora%20Communicator/Images/q10-lora-v1.1-schematic.png&#34; alt=&#34;schematics&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scottbez1/smartknob</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/scottbez1/smartknob</id>
    <link href="https://github.com/scottbez1/smartknob" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Haptic input knob with software-defined endstops and virtual detents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SmartKnob&lt;/h1&gt; &#xA;&lt;p&gt;SmartKnob is an open-source input device with software-configurable endstops and virtual detents.&lt;/p&gt; &#xA;&lt;p&gt;A brushless gimbal motor is paired with a magnetic encoder to provide closed-loop torque feedback control, making it possible to dynamically create and adjust the feel of detents and endstops.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/scottbez1/smartknob/actions/workflows/electronics.yml&#34;&gt;&lt;img src=&#34;https://github.com/scottbez1/smartknob/actions/workflows/electronics.yml/badge.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/scottbez1/smartknob/actions/workflows/pio.yml&#34;&gt;&lt;img src=&#34;https://github.com/scottbez1/smartknob/actions/workflows/pio.yml/badge.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Designs&lt;/h1&gt; &#xA;&lt;h2&gt;SmartKnob View&lt;/h2&gt; &#xA;&lt;p&gt;Premium SmartKnob experience. Under active development.&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Update (2022-03-24): As a result of the popularity of this project, it seems like the recommended motors are unfortunately no longer available.&lt;/strong&gt; I expect that these motors are simply no longer in production and therefore limited stock was available (for future reference in case you find them being sold elsewhere at a higher price, they were originally selling for US$2.56 each before this project was published). However... üëá&lt;/p&gt; &#xA;&lt;p&gt;üí° &lt;strong&gt;There is an ongoing search for new motors in &lt;a href=&#34;https://github.com/scottbez1/smartknob/issues/16&#34;&gt;issue #16&lt;/a&gt;&lt;/strong&gt; - follow along there for the latest info (or join in and help us find a good replacement). Any change in motor will likely require substantial redesigns, so &lt;em&gt;don&#39;t order PCBs/printed parts until there is more clarity on the motor.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;240x240 round LCD (&#34;GC9A01&#34;), protected by 39.5mm watch glass on rotor&lt;/li&gt; &#xA; &lt;li&gt;BLDC gimbal motor, with a hollow shaft for mechanically &amp;amp; electrically connecting the LCD&lt;/li&gt; &#xA; &lt;li&gt;Powered by ESP32-PICO-V3-02 (Lilygo TMicro32 Plus module)&lt;/li&gt; &#xA; &lt;li&gt;PCB flexure and strain gauges used for press detection (haptic feedback provided via the motor)&lt;/li&gt; &#xA; &lt;li&gt;8 side-firing RGB LEDs (SK6812-SIDE-A) illuminate ring around the knob&lt;/li&gt; &#xA; &lt;li&gt;USB-C (2.0) connector for 5V power and serial data/programming (CH340)&lt;/li&gt; &#xA; &lt;li&gt;VEML7700 ambient light sensor for automatic backlight &amp;amp; LED intensity adjustment&lt;/li&gt; &#xA; &lt;li&gt;Versatile back plate for mounting - use either 4x screws, or 2x 3M medium Command strips (with cutouts for accessing removal tabs after installation)&lt;/li&gt; &#xA; &lt;li&gt;Front cover snaps on for easy access to the PCB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; Not recommended for general use (mechanical and electrical revisions are planned)&lt;/p&gt; &#xA;&lt;h3&gt;Demo video&lt;/h3&gt; &#xA;&lt;a href=&#34;https://www.youtube.com/watch?v=ip641WmY4pA&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/ip641WmY4pA/maxresdefault.jpg&#34; width=&#34;480&#34;&gt; &lt;/a&gt; &#xA;&lt;h3&gt;3D CAD&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/scottbez1/smartknob/master/doc/img/explodedv145.gif&#34; alt=&#34;Exploded view&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Latest Fusion 360 Model: &lt;a href=&#34;https://a360.co/3BzkU0n&#34;&gt;https://a360.co/3BzkU0n&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build your own?&lt;/h3&gt; &#xA;&lt;p&gt;While this is a &#34;DIY&#34; open-source project, it is not yet a mature plug-and-play project. If you intend to build your own, note that it requires advanced soldering experience to build - very small-pitch surface-mount soldering is required (reflow or hot air recommended), and assembly is quite time-consuming and delicate. Please go into it with the expectation that you will almost certainly need to be able to troubleshoot some hardware and firmware issues yourself - I recommend reviewing/understanding the schematics and basic firmware before jumping in!&lt;/p&gt; &#xA;&lt;p&gt;More documentation on the BOM and what parts you need to order is coming in the future - thanks so much for your interest! Follow me on &lt;a href=&#34;https://twitter.com/scottbez1&#34;&gt;Twitter&lt;/a&gt; for the latest updates on this and other projects.&lt;/p&gt; &#xA;&lt;p&gt;View the latest auto-generated (untested) &lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-ibom.html&#34;&gt;Base PCB Interactive BOM&lt;/a&gt; and &lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-ibom.html&#34;&gt;Screen PCB Interactive BOM&lt;/a&gt; (or, the combined &lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-bom.csv&#34;&gt;BOM csv&lt;/a&gt;) for electronics/hardware parts list. ‚ö†Ô∏è These are auto-generated from the latest untested revision on GitHub. For tested/stable/recommended artifacts, use a &lt;a href=&#34;https://github.com/scottbez1/smartknob/releases&#34;&gt;release&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;A few miscellaneous notes in the meantime:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This can &lt;em&gt;probably&lt;/em&gt; be FDM 3D printed with a well-tuned printer, but the parts shown in videos/photos were MJF printed in nylon for tight tolerances and better surface finish&lt;/li&gt; &#xA; &lt;li&gt;If you wanted a simpler build, you could omit the LCD and just merge the knob + glass from the model into a single STL to get a closed-top knob&lt;/li&gt; &#xA; &lt;li&gt;There&#39;s limited space inside the LCD mount for wiring, and 8 wires need to fit through the hole in the center. I used 30 AWG wire-wrapping wire. Enamel-coated wire would probably work too.&lt;/li&gt; &#xA; &lt;li&gt;Strain gauges are BF350-3AA, and glued in place with CA glue (I&#39;ll include video of this process in the future, but essentially I used kapton tape to pick up the strain gauge and hold it in place during curing). This has to be done after reflow soldering, and would be hard to remove/fix in case of a mistake, so MAKE SURE TO PRACTICE GLUING strain gauges to other items before attempting on the PCB!&lt;/li&gt; &#xA; &lt;li&gt;The TMC6300 is &lt;em&gt;tiny&lt;/em&gt; and has a bottom pad, so I would seriously consider getting a stencil along with the PCB order. Even with the stencil I needed to manually clean up some bridging afterward; I &lt;em&gt;highly&lt;/em&gt; recommend Chip Quik NC191 gel flux, available on &lt;a href=&#34;https://amzn.to/3MGDSr5&#34;&gt;Amazon&lt;/a&gt; (or use this &lt;a href=&#34;https://www.amazon.com/Smooth-Flow-No-Clean-syringe-plunger/dp/B08KJPG3NZ&#34;&gt;non-affiliate link&lt;/a&gt; instead) or from your electronics distributor of choice. Flux is also very helpful when soldering the LCD ribbon cable to to screen PCB.&lt;/li&gt; &#xA; &lt;li&gt;For breadboard prototyping, the &lt;a href=&#34;https://www.trinamic.com/support/eval-kits/details/tmc6300-bob/&#34;&gt;TMC6300-BOB&lt;/a&gt; is awesome and way easier to work with than the bare chip if you just want to play around with low current BLDC motors&lt;/li&gt; &#xA; &lt;li&gt;For AliExpress purchases: I highly recommend &lt;strong&gt;only&lt;/strong&gt; using AliExpress Standard Shipping (purchasing in the US). I have had multiple purchases take months or never get delivered when purchased with Cainiao or other low cost shipping options, whereas AliExpress Standard is very reliable and generally faster in my experience.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to check the &lt;a href=&#34;https://github.com/scottbez1/smartknob/issues&#34;&gt;open issues&lt;/a&gt; - this design is not yet &#34;stable&#34;, so beware that everything may not go smoothly. I would not recommend ordering these parts yourself until the &lt;a href=&#34;https://github.com/scottbez1/smartknob/milestone/1&#34;&gt;stable release v1.0 milestone&lt;/a&gt; is complete, as there are some mechanical interference issues in the current revision.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Future plans:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;consider switch to using an ESP32-S3-MINI-1 module (once Arduino core support is complete), as that would allow for direct USB HID support (for joystick/macro-pad type input to a computer)&lt;/li&gt; &#xA; &lt;li&gt;Bluetooth HID support?&lt;/li&gt; &#xA; &lt;li&gt;get wifi configured and working (probably MQTT?). Currently memory is an issue with the full display framebuffer sprite. PSRAM might fix this (requires newer ESP-IDF &amp;amp; unreleased Arduino core, and from a brief test I got horrible performance with PSRAM enabled), or the next item might help reduce memory:&lt;/li&gt; &#xA; &lt;li&gt;migrate to LVGL, for better display rendering and easy support for menus, etc. Shouldn&#39;t require a full 240x240x24b framebuffer in memory, freeing some for wifi, etc.&lt;/li&gt; &#xA; &lt;li&gt;integrate nanopb for structured serial data (see &lt;a href=&#34;https://github.com/scottbez1/splitflap/raw/1440aba54d5b0d822ec5da68762431879988d7ef/arduino/splitflap/esp32/splitflap/serial_proto_protocol.cpp&#34;&gt;splitflap protobuf protocol&lt;/a&gt; for example)&lt;/li&gt; &#xA; &lt;li&gt;Home Assistant integration, or other real-world applications&lt;/li&gt; &#xA; &lt;li&gt;???&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sponsors/scottbez1/&#34;&gt;Profit&lt;/a&gt; üòâ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Base PCB&lt;/h4&gt; &#xA;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-front-3d.png&#34;&gt; &lt;img src=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-front-3d.png&#34; width=&#34;300&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-back-3d.png&#34;&gt; &lt;img src=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-back-3d.png&#34; width=&#34;300&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Ordering notes: use white soldermask, for reflecting light from RGB LED ring around the knob. Should be 1.2mm thick (not &#34;standard&#34; 1.6mm).&lt;/p&gt; &#xA;&lt;p&gt;Latest auto-generated (untested and likely broken!) artifacts‚ö†Ô∏è:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-schematic.pdf&#34;&gt;Schematic&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-ibom.html&#34;&gt;Interactive BOM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-pcb-packet.pdf&#34;&gt;PCB Packet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_base-jlc/gerbers.zip&#34;&gt;Gerbers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è For tested/stable/recommended artifacts, use a &lt;a href=&#34;https://github.com/scottbez1/smartknob/releases&#34;&gt;release&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;h4&gt;Screen PCB&lt;/h4&gt; &#xA;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-front-3d.png&#34;&gt; &lt;img src=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-front-3d.png&#34; width=&#34;300&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-back-3d.png&#34;&gt; &lt;img src=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-back-3d.png&#34; width=&#34;300&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Ordering notes: Must be 1.2mm thick (not &#34;standard&#34; 1.6mm) per mechanical design.&lt;/p&gt; &#xA;&lt;p&gt;Latest auto-generated (untested and likely broken!) artifacts‚ö†Ô∏è:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-schematic.pdf&#34;&gt;Schematic&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-ibom.html&#34;&gt;Interactive BOM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-pcb-packet.pdf&#34;&gt;PCB Packet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://smartknob-artifacts.s3.us-west-1.amazonaws.com/master/electronics/view_screen-jlc/gerbers.zip&#34;&gt;Gerbers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è For tested/stable/recommended artifacts, use a &lt;a href=&#34;https://github.com/scottbez1/smartknob/releases&#34;&gt;release&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;h2&gt;SmartKnob Mini&lt;/h2&gt; &#xA;&lt;p&gt;Planned for the future.&lt;/p&gt; &#xA;&lt;h1&gt;Frequently Asked Questions (FAQ)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;How much does it cost?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I wish I could tell you now, but I don&#39;t actually know off the top of my head. Check back soon - I&#39;ve only built 1 so far, which was the result of a bunch of tinkering and prototyping over an extended period, so I don&#39;t have all the expenses tallied up yet. Certainly less than $200 in parts, and maybe closer to $100?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Does it work with XYZ?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Not yet. So far I&#39;ve only implemented enough firmware for the demo shown in the video, so you can&#39;t actually use it for anything productive yet. The basic detent configuration API is there, but not much else. Lots of firmware work remains to be done. If you build one, I&#39;d love your help adding support for XYZ though!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can I buy one as a kit or already assembled?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Probably not? Or at least, I don&#39;t have any immediate plans to sell them myself. It&#39;s not that I don&#39;t want you to be happy, but hardware is a hard business and I just work on this stuff in my free time.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s open source with a fairly permissive license though, so in theory anyone could start offering kits/assemblies. If someone does go down that route of selling them, note that attribution is &lt;em&gt;required&lt;/em&gt; (and I wouldn&#39;t say no to &lt;a href=&#34;https://github.com/sponsors/scottbez1/&#34;&gt;royalties/tips/thanks&lt;/a&gt; if you&#39;re in a giving mood üôÇ).&lt;/p&gt; &#xA;&lt;h2&gt;General Component Info&lt;/h2&gt; &#xA;&lt;h3&gt;Magnetic encoders&lt;/h3&gt; &#xA;&lt;h4&gt;MT6701 (MagnTek)&lt;/h4&gt; &#xA;&lt;p&gt;Excellent sensor at a reasonable price - highly recommended. Less noisy than TLV493D, and more responsive (control loop is more stable) using SSI.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lots of IO options - SSI, I2C, and ABZ - should offer good response latency&lt;/li&gt; &#xA; &lt;li&gt;SSI includes CRC to validate data&lt;/li&gt; &#xA; &lt;li&gt;No power-down or low-power options - may not be ideal for battery-powered devices&lt;/li&gt; &#xA; &lt;li&gt;Not available from US distributors (Mouser, Digi-Key)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.magntek.com.cn/upload/MT6701_Rev.1.5.pdf&#34;&gt;Datasheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lcsc.com/product-detail/Angle-Linear-Position-Sensors_Magn-Tek-MT6701CT-STD_C2856764.html&#34;&gt;Ordering (LCSC)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;TLV493D (Infineon)&lt;/h4&gt; &#xA;&lt;p&gt;A mediocre choice. Easy to prototype with using &lt;a href=&#34;https://www.adafruit.com/product/4366&#34;&gt;Adafruit&#39;s QWIIC breakout board&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In my testing, it is a little noisy, requiring filtering/smoothing that can slow responsiveness, hurting control loop stability. Or, with less filtering, the noise can easily be &#34;amplified&#34; by the derivative component in the PID motor torque controller, causing audible (and tactile) humming/buzzing.&lt;/p&gt; &#xA;&lt;p&gt;There is also apparently a known silicon issue that causes the internal ADC to sometimes completely lock up, requiring a full reset and re-configuration. See section 5.6 in the &lt;a href=&#34;https://www.infineon.com/dgdl/Infineon-TLV493D-A1B6_3DMagnetic-UM-v01_03-EN.pdf?fileId=5546d46261d5e6820161e75721903ddd&#34;&gt;User Manual&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;In the Master Controlled Mode (MCM) or the Fast Mode (FM) the ADC conversion may hang up. A hang up can&#xA;be detected by:&#xA; - Frame Counter (FRM) counter stucks and does not increment anymore.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In my experience testing 4 different Adafruit breakout boards, 2 of them (50%) regularly exhibit this lockup behavior within a minute or two of use. It is possible to detect and auto-reset (and there is code in the project to do so), but it is slow and may cause undesirable jumps/delays if the sensor locks up often.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.mouser.com/datasheet/2/196/Infineon_TLV493D_A1B6_DataSheet_v01_10_EN-1227967.pdf&#34;&gt;Datasheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;AS5600 (AMS)&lt;/h4&gt; &#xA;&lt;p&gt;A mediocre choice. Cheap breakout boards are readily available.&lt;/p&gt; &#xA;&lt;p&gt;In my testing, it&#39;s fairly noisy (anecdotally, noisier than the TLV493d), requiring filtering/smoothing that can slow responsiveness, hurting control loop stability. Additionally, it saturates at a lower magnetic field strength than other sensors I tested, requiring a significant air gap (8-10mm) when used with a strong neodymium diametric magnet like &lt;a href=&#34;https://www.digikey.com/en/products/detail/radial-magnets-inc/8995/5126077&#34;&gt;Radial Magnets 8995&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ams.com/documents/20143/36005/AS5600_DS000365_5-00.pdf&#34;&gt;Datasheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Motor drivers&lt;/h3&gt; &#xA;&lt;h4&gt;TMC6300-LA&lt;/h4&gt; &#xA;&lt;p&gt;This is a relatively new IC and it&#39;s a perfect match! There generally aren&#39;t any other drivers (with integrated fets) that meet the requirements for the low-voltage and low-current motors used in this project (DRV8316 might work, but has not been tested).&lt;/p&gt; &#xA;&lt;p&gt;Highlights:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2-11V DC motor supply input&lt;/li&gt; &#xA; &lt;li&gt;Up to 1.2A RMS&lt;/li&gt; &#xA; &lt;li&gt;Tiny (3x3mm QFN)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.trinamic.com/fileadmin/assets/Products/ICs_Documents/TMC6300_datasheet_rev1.07.pdf&#34;&gt;Datasheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.trinamic.com/products/integrated-circuits/details/tmc6300-la/&#34;&gt;Product page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Motors&lt;/h3&gt; &#xA;&lt;h4&gt;32mm Rotor, Hollow Shaft, Diametric magnet&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/scottbez1/smartknob/master/doc/img/motors/PXL_20220121_221746595.jpg&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/scottbez1/smartknob/master/doc/img/motors/PXL_20220121_221746595.jpg&#34; width=&#34;200&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/scottbez1/smartknob/master/doc/img/motors/PXL_20220121_221738745.jpg&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/scottbez1/smartknob/master/doc/img/motors/PXL_20220121_221738745.jpg&#34; width=&#34;200&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;32mm rotor&lt;/li&gt; &#xA; &lt;li&gt;15mm overall height (including magnet), 12.75mm height w/o magnet, 9mm rotor height&lt;/li&gt; &#xA; &lt;li&gt;low/zero cogging - excellent for completely smooth input&lt;/li&gt; &#xA; &lt;li&gt;5.9mm hollow shaft&lt;/li&gt; &#xA; &lt;li&gt;built-in diametric magnet for encoder&lt;/li&gt; &#xA; &lt;li&gt;Proven option&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is overall the easiest motor to get started with. Low cogging and a built-in diametric magnet are great!&lt;/p&gt; &#xA;&lt;p&gt;Sadly, does not seem to be available any longer.&lt;/p&gt; &#xA;&lt;h1&gt;Firmware&lt;/h1&gt; &#xA;&lt;p&gt;TODO: document this&lt;/p&gt; &#xA;&lt;p&gt;Also TODO: implement a lot more of the firmware&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;This project was greatly inspired by Jesse Schoch&#39;s video &#34;&lt;a href=&#34;https://www.youtube.com/watch?v=1gPQfDkX3BU&#34;&gt;haptic textures and virtual detents&lt;/a&gt;&#34; and the corresponding &lt;a href=&#34;https://community.simplefoc.com/t/haptic-textures/301&#34;&gt;discussion in the SimpleFOC community&lt;/a&gt;. Seriously, this project wouldn&#39;t exist if not for that video - thank you Jesse!&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under Apache v2 (software, electronics, documentation) and Creative Commons Attribution 4.0 (hardware/mechanical) (see &lt;a href=&#34;https://raw.githubusercontent.com/scottbez1/smartknob/master/LICENSE.txt&#34;&gt;LICENSE.txt&lt;/a&gt; and &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2022 Scott Bezek&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;    http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>wang-xinyu/tensorrtx</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/wang-xinyu/tensorrtx</id>
    <link href="https://github.com/wang-xinyu/tensorrtx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of popular deep learning networks with TensorRT network definition API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorRTx&lt;/h1&gt; &#xA;&lt;p&gt;TensorRTx aims to implement popular deep learning networks with tensorrt network definition APIs. As we know, tensorrt has builtin parsers, including caffeparser, uffparser, onnxparser, etc. But when we use these parsers, we often run into some &#34;unsupported operations or layers&#34; problems, especially some state-of-the-art models are using new type of layers.&lt;/p&gt; &#xA;&lt;p&gt;So why don&#39;t we just skip all parsers? We just use TensorRT network definition APIs to build the whole network, it&#39;s not so complicated.&lt;/p&gt; &#xA;&lt;p&gt;I wrote this project to get familiar with tensorrt API, and also to share and learn from the community.&lt;/p&gt; &#xA;&lt;p&gt;All the models are implemented in pytorch/mxnet/tensorflown first, and export a weights file xxx.wts, and then use tensorrt to load weights, define network and do inference. Some pytorch implementations can be found in my repo &lt;a href=&#34;https://github.com/wang-xinyu/pytorchx&#34;&gt;Pytorchx&lt;/a&gt;, the remaining are from polular open-source implementations.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;26 May 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/triple-Mu&#34;&gt;triple-Mu&lt;/a&gt;: YOLOv5 python script with CUDA Python API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;23 May 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/yester31&#34;&gt;yhpark&lt;/a&gt;: Real-ESRGAN, Practical Algorithms for General Image/Video Restoration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;19 May 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/vjsrinivas&#34;&gt;vjsrinivas&lt;/a&gt;: YOLOv3 TRT8 support and Python script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;15 Mar 2022&lt;/code&gt;. &lt;a href=&#34;https://github.com/wdhao&#34;&gt;sky_hole&lt;/a&gt;: Swin Transformer - Semantic Segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;19 Oct 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/liuqi123123&#34;&gt;liuqi123123&lt;/a&gt; added cuda preprossing for yolov5, preprocessing + inference is 3x faster when batchsize=8.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;18 Oct 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/xupengao&#34;&gt;xupengao&lt;/a&gt;: YOLOv5 updated to v6.0, supporting n/s/m/l/x/n6/s6/m6/l6/x6.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;31 Aug 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/FamousDirector&#34;&gt;FamousDirector&lt;/a&gt;: update retinaface to support TensorRT 8.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;27 Aug 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/HaiyangPeng&#34;&gt;HaiyangPeng&lt;/a&gt;: add a python wrapper for hrnet segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;1 Jul 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/freedenS&#34;&gt;freedenS&lt;/a&gt;: DE‚´∂TR: End-to-End Object Detection with Transformers. First Transformer model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;10 Jun 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/upczww&#34;&gt;upczww&lt;/a&gt;: EfficientNet b0-b8 and l2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;23 May 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/SsisyphusTao&#34;&gt;SsisyphusTao&lt;/a&gt;: CenterNet DLA-34 with DCNv2 plugin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;17 May 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/ybw108&#34;&gt;ybw108&lt;/a&gt;: arcface LResNet100E-IR and MobileFaceNet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6 May 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/makaveli10&#34;&gt;makaveli10&lt;/a&gt;: scaled-yolov4 yolov4-csp.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;29 Apr 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/upczww&#34;&gt;upczww&lt;/a&gt;: hrnet segmentation w18/w32/w48, ocr branch also.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;28 Apr 2021&lt;/code&gt;. &lt;a href=&#34;https://github.com/aditya-dl&#34;&gt;aditya-dl&lt;/a&gt;: mobilenetv2, alexnet, densenet121, mobilenetv3 with python API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/install.md&#34;&gt;Install the dependencies.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/getting_started.md&#34;&gt;A guide for quickly getting started, taking lenet5 as a demo.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/getting_started.md#the-wts-content-format&#34;&gt;The .wts file content format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/faq.md&#34;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/migrating_from_tensorrt_4_to_7.md&#34;&gt;Migrating from TensorRT 4 to 7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/multi_GPU_processing.md&#34;&gt;How to implement multi-GPU processing, taking YOLOv4 as example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/check_fp16_int8_support.md&#34;&gt;Check if Your GPU support FP16/INT8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/run_on_windows.md&#34;&gt;How to Compile and Run on Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/isarsoft/yolov4-triton-tensorrt&#34;&gt;Deploy YOLOv4 with Triton Inference Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tutorials/from_pytorch_to_trt_stepbystep_hrnet.md&#34;&gt;From pytorch to trt step by step, hrnet as example(Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Test Environment&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;TensorRT 7.x&lt;/li&gt; &#xA; &lt;li&gt;TensorRT 8.x(Some of the models support 8.x)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to run&lt;/h2&gt; &#xA;&lt;p&gt;Each folder has a readme inside, which explains how to run the models inside.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Following models are implemented.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/mlp&#34;&gt;mlp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the very basic model for starters, properly documented&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/lenet&#34;&gt;lenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the simplest, as a &#34;hello world&#34; of this project&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/alexnet&#34;&gt;alexnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;easy to implement, all layers are supported in tensorrt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/googlenet&#34;&gt;googlenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GoogLeNet (Inception v1)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/inception&#34;&gt;inception&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Inception v3, v4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/mnasnet&#34;&gt;mnasnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MNASNet with depth multiplier of 0.5 from the paper&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/mobilenet&#34;&gt;mobilenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MobileNet v2, v3-small, v3-large&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/resnet&#34;&gt;resnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;resnet-18, resnet-50 and resnext50-32x4d are implemented&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/senet&#34;&gt;senet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;se-resnet50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/shufflenetv2&#34;&gt;shufflenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ShuffleNet v2 with 0.5x output channels&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/squeezenet&#34;&gt;squeezenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SqueezeNet 1.1 model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/vgg&#34;&gt;vgg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;VGG 11-layer model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov3-tiny&#34;&gt;yolov3-tiny&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;weights and pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov3&#34;&gt;yolov3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;darknet-53, weights and pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov3-spp&#34;&gt;yolov3-spp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;darknet-53, weights and pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov4&#34;&gt;yolov4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CSPDarknet53, weights from &lt;a href=&#34;https://github.com/AlexeyAB/darknet#pre-trained-models&#34;&gt;AlexeyAB/darknet&lt;/a&gt;, pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;ultralytics/yolov3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/yolov5&#34;&gt;yolov5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yolov5 v1.0-v6.0, pytorch implementation from &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;ultralytics/yolov5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/retinaface&#34;&gt;retinaface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;resnet50 and mobilnet0.25, weights from &lt;a href=&#34;https://github.com/biubug6/Pytorch_Retinaface&#34;&gt;biubug6/Pytorch_Retinaface&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/arcface&#34;&gt;arcface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LResNet50E-IR, LResNet100E-IR and MobileFaceNet, weights from &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;deepinsight/insightface&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/retinafaceAntiCov&#34;&gt;retinafaceAntiCov&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;mobilenet0.25, weights from &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;deepinsight/insightface&lt;/a&gt;, retinaface anti-COVID-19, detect face and mask attribute&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/dbnet&#34;&gt;dbnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Scene Text Detection, weights from &lt;a href=&#34;https://github.com/BaofengZan/DBNet.pytorch&#34;&gt;BaofengZan/DBNet.pytorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/crnn&#34;&gt;crnn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pytorch implementation from &lt;a href=&#34;https://github.com/meijieru/crnn.pytorch&#34;&gt;meijieru/crnn.pytorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/ufld&#34;&gt;ufld&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pytorch implementation from &lt;a href=&#34;https://github.com/cfzd/Ultra-Fast-Lane-Detection&#34;&gt;Ultra-Fast-Lane-Detection&lt;/a&gt;, ECCV2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/hrnet&#34;&gt;hrnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;hrnet-image-classification and hrnet-semantic-segmentation, pytorch implementation from &lt;a href=&#34;https://github.com/HRNet/HRNet-Image-Classification&#34;&gt;HRNet-Image-Classification&lt;/a&gt; and &lt;a href=&#34;https://github.com/HRNet/HRNet-Semantic-Segmentation&#34;&gt;HRNet-Semantic-Segmentation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/psenet&#34;&gt;psenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PSENet Text Detection, tensorflow implementation from &lt;a href=&#34;https://github.com/liuheng92/tensorflow_PSENet&#34;&gt;liuheng92/tensorflow_PSENet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/ibnnet&#34;&gt;ibnnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IBN-Net, pytorch implementation from &lt;a href=&#34;https://github.com/XingangPan/IBN-Net&#34;&gt;XingangPan/IBN-Net&lt;/a&gt;, ECCV2018&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/unet&#34;&gt;unet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;U-Net, pytorch implementation from &lt;a href=&#34;https://github.com/milesial/Pytorch-UNet&#34;&gt;milesial/Pytorch-UNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/repvgg&#34;&gt;repvgg&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RepVGG, pytorch implementation from &lt;a href=&#34;https://github.com/DingXiaoH/RepVGG&#34;&gt;DingXiaoH/RepVGG&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/lprnet&#34;&gt;lprnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LPRNet, pytorch implementation from &lt;a href=&#34;https://github.com/xuexingyu24/License_Plate_Detection_Pytorch&#34;&gt;xuexingyu24/License_Plate_Detection_Pytorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/refinedet&#34;&gt;refinedet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RefineDet, pytorch implementation from &lt;a href=&#34;https://github.com/luuuyi/RefineDet.PyTorch&#34;&gt;luuuyi/RefineDet.PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/densenet&#34;&gt;densenet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DenseNet-121, from torchvision.models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/rcnn&#34;&gt;rcnn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FasterRCNN and MaskRCNN, model from &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;detectron2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/tsm&#34;&gt;tsm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TSM: Temporal Shift Module for Efficient Video Understanding, ICCV2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/scaled-yolov4&#34;&gt;scaled-yolov4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yolov4-csp, pytorch from &lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;WongKinYiu/ScaledYOLOv4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/centernet&#34;&gt;centernet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CenterNet DLA-34, pytorch from &lt;a href=&#34;https://github.com/xingyizhou/CenterNet&#34;&gt;xingyizhou/CenterNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/efficientnet&#34;&gt;efficientnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;EfficientNet b0-b8 and l2, pytorch from &lt;a href=&#34;https://github.com/lukemelas/EfficientNet-PyTorch&#34;&gt;lukemelas/EfficientNet-PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/detr&#34;&gt;detr&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DE‚´∂TR, pytorch from &lt;a href=&#34;https://github.com/facebookresearch/detr&#34;&gt;facebookresearch/detr&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/swin-transformer&#34;&gt;swin-transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Swin Transformer - Semantic Segmentation, only support Swin-T. The Pytorch implementation is &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer.git&#34;&gt;microsoft/Swin-Transformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wang-xinyu/tensorrtx/master/real-esrgan&#34;&gt;real-esrgan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Real-ESRGAN. The Pytorch implementation is &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;real-esrgan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;The .wts files can be downloaded from model zoo for quick evaluation. But it is recommended to convert .wts from pytorch/mxnet/tensorflow model, so that you can retrain your own model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1Ri0IDa5OChtcA3zjqRTW57uG6TnfN4Do?usp=sharing&#34;&gt;GoogleDrive&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/19s6hO8esU7-TtZEXN7G3OA&#34;&gt;BaiduPan&lt;/a&gt; pwd: uvv2&lt;/p&gt; &#xA;&lt;h2&gt;Tricky Operations&lt;/h2&gt; &#xA;&lt;p&gt;Some tricky operations encountered in these models, already solved, but might have better solutions.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BatchNorm&lt;/td&gt; &#xA;   &lt;td&gt;Implement by a scale layer, used in resnet, googlenet, mobilenet, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MaxPool2d(ceil_mode=True)&lt;/td&gt; &#xA;   &lt;td&gt;use a padding layer before maxpool to solve ceil_mode=True, see googlenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;average pool with padding&lt;/td&gt; &#xA;   &lt;td&gt;use setAverageCountExcludesPadding() when necessary, see inception.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;relu6&lt;/td&gt; &#xA;   &lt;td&gt;use &lt;code&gt;Relu6(x) = Relu(x) - Relu(x-6)&lt;/code&gt;, see mobilenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;torch.chunk()&lt;/td&gt; &#xA;   &lt;td&gt;implement the &#39;chunk(2, dim=C)&#39; by tensorrt plugin, see shufflenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;channel shuffle&lt;/td&gt; &#xA;   &lt;td&gt;use two shuffle layers to implement &lt;code&gt;channel_shuffle&lt;/code&gt;, see shufflenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;adaptive pool&lt;/td&gt; &#xA;   &lt;td&gt;use fixed input dimension, and use regular average pooling, see shufflenet.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;leaky relu&lt;/td&gt; &#xA;   &lt;td&gt;I wrote a leaky relu plugin, but PRelu in &lt;code&gt;NvInferPlugin.h&lt;/code&gt; can be used, see yolov3 in branch &lt;code&gt;trt4&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yolo layer v1&lt;/td&gt; &#xA;   &lt;td&gt;yolo layer is implemented as a plugin, see yolov3 in branch &lt;code&gt;trt4&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yolo layer v2&lt;/td&gt; &#xA;   &lt;td&gt;three yolo layers implemented in one plugin, see yolov3-spp.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;upsample&lt;/td&gt; &#xA;   &lt;td&gt;replaced by a deconvolution layer, see yolov3.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hsigmoid&lt;/td&gt; &#xA;   &lt;td&gt;hard sigmoid is implemented as a plugin, hsigmoid and hswish are used in mobilenetv3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;retinaface output decode&lt;/td&gt; &#xA;   &lt;td&gt;implement a plugin to decode bbox, confidence and landmarks, see retinaface.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mish&lt;/td&gt; &#xA;   &lt;td&gt;mish activation is implemented as a plugin, mish is used in yolov4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;prelu&lt;/td&gt; &#xA;   &lt;td&gt;mxnet&#39;s prelu activation with trainable gamma is implemented as a plugin, used in arcface&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HardSwish&lt;/td&gt; &#xA;   &lt;td&gt;hard_swish = x * hard_sigmoid, used in yolov5 v3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LSTM&lt;/td&gt; &#xA;   &lt;td&gt;Implemented pytorch nn.LSTM() with tensorrt api&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Speed Benchmark&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BatchSize&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mode&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Shape(HxW)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3-tiny&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;333&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3(darknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3(darknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;INT8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv3-spp(darknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv4(CSPDarknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv4(CSPDarknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv4(CSPDarknet53)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;142&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;173&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;190&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-m v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-l v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-x v3.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-s v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;142&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-m v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-l v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOv5-x v4.0&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608x608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace(resnet50)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace(resnet50)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;INT8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;204&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RetinaFace(mobilenet0.25)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;417&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ArcFace(LResNet50E-IR)&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;112x112&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;333&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CRNN&lt;/td&gt; &#xA;   &lt;td&gt;Xeon E5-2620/GTX1080&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32x100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Help wanted, if you got speed results, please add an issue or PR.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments &amp;amp; Contact&lt;/h2&gt; &#xA;&lt;p&gt;Any contributions, questions and discussions are welcomed, contact me by following info.&lt;/p&gt; &#xA;&lt;p&gt;E-mail: &lt;a href=&#34;mailto:wangxinyu_es@163.com&#34;&gt;wangxinyu_es@163.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;WeChat ID: wangxinyu0375 (ÂèØÂä†ÊàëÂæÆ‰ø°Ëøõtensorrtx‰∫§ÊµÅÁæ§Ôºå&lt;strong&gt;Â§áÊ≥®Ôºötensorrtx&lt;/strong&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InoriRus/Kyty</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/InoriRus/Kyty</id>
    <link href="https://github.com/InoriRus/Kyty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PS4 &amp; PS5 emulator&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://ci.appveyor.com/project/InoriRus/kyty&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/0du32fg9flol63to?svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Kyty&lt;/h1&gt; &#xA;&lt;h2&gt;PS4 &amp;amp; PS5 emulator&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The project is in its early stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:inorirus@gmail.com&#34;&gt;Vladimir M&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the MIT license.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Graphics for PS5 is not yet implemented&lt;/p&gt; &#xA;&lt;p&gt;It is possible to run some simple games for PS4&lt;/p&gt; &#xA;&lt;p&gt;There maybe graphics glitches, crashes, freezes and low FPS. It&#39;s OK for now.&lt;/p&gt; &#xA;&lt;p&gt;Features that are not implemented:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Audio input/output&lt;/li&gt; &#xA; &lt;li&gt;MP4 video&lt;/li&gt; &#xA; &lt;li&gt;Network&lt;/li&gt; &#xA; &lt;li&gt;Multi-user&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Path to Savedata folder is hardcoded and can&#39;t be configured. System parameters (language, date format, etc.) are also hardcoded.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Screenshots&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674296-4185e2da-99f9-4073-8ca9-19dc124c7459.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674298-df817d95-7288-46fe-a040-3c0a40c29a6b.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674301-37a3f947-76cd-4a9b-8c81-adec3d5d9c59.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674303-13edae7d-24d3-4ec6-ba94-586e13c69df5.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;Supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows 10 x64&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Toolchains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visual Studio + clang-cl + ninja&lt;/li&gt; &#xA; &lt;li&gt;Eclipse CDT + mingw-w64 + gcc/clang + ninja/mingw32-make&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported versions:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Tool&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cmake&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Visual Studio 2019&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.10.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;clang&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;clang-cl&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gcc (MinGW-W64 x86_64-posix-seh)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ninja&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.10.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MinGW-w64&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Eclipse CDT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qt&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.15.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Define environment variable named Qt5_DIR pointing to the proper version of Qt&lt;/p&gt; &#xA;&lt;p&gt;MSVC compiler (cl.exe) is not supported!&lt;/p&gt; &#xA;&lt;p&gt;External dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vulkan SDK 1.2.198.1&lt;/li&gt; &#xA; &lt;li&gt;Qt 5.15.0&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebook/rocksdb</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/facebook/rocksdb</id>
    <link href="https://github.com/facebook/rocksdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library that provides an embeddable, persistent key-value store for fast storage.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;RocksDB: A Persistent Key-Value Store for Flash and RAM Storage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebook/rocksdb.svg?style=svg&#34; alt=&#34;CircleCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/github/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/facebook/rocksdb.svg?branch=main&#34; alt=&#34;TravisCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/Facebook/rocksdb/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/fbgfu0so3afcno78/branch/main?svg=true&#34; alt=&#34;Appveyor Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://140-211-168-68-openstack.osuosl.org:8080/job/rocksdb&#34;&gt;&lt;img src=&#34;http://140-211-168-68-openstack.osuosl.org:8080/buildStatus/icon?job=rocksdb&amp;amp;style=plastic&#34; alt=&#34;PPC64le Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RocksDB is developed and maintained by Facebook Database Engineering Team. It is built on earlier work on &lt;a href=&#34;https://github.com/google/leveldb&#34;&gt;LevelDB&lt;/a&gt; by Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;This code is a library that forms the core building block for a fast key-value server, especially suited for storing data on flash drives. It has a Log-Structured-Merge-Database (LSM) design with flexible tradeoffs between Write-Amplification-Factor (WAF), Read-Amplification-Factor (RAF) and Space-Amplification-Factor (SAF). It has multi-threaded compactions, making it especially suitable for storing multiple terabytes of data in a single database.&lt;/p&gt; &#xA;&lt;p&gt;Start with example usage here: &lt;a href=&#34;https://github.com/facebook/rocksdb/tree/main/examples&#34;&gt;https://github.com/facebook/rocksdb/tree/main/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki&#34;&gt;github wiki&lt;/a&gt; for more explanation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in &lt;code&gt;include/&lt;/code&gt;. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Questions and discussions are welcome on the &lt;a href=&#34;https://www.facebook.com/groups/rocksdb.dev/&#34;&gt;RocksDB Developers Public&lt;/a&gt; Facebook group and &lt;a href=&#34;https://groups.google.com/g/rocksdb&#34;&gt;email list&lt;/a&gt; on Google Groups.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;RocksDB is dual-licensed under both the GPLv2 (found in the COPYING file in the root directory) and Apache 2.0 License (found in the LICENSE.Apache file in the root directory). You may select, at your option, one of the above-listed licenses.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JonathanSalwan/Triton</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/JonathanSalwan/Triton</id>
    <link href="https://github.com/JonathanSalwan/Triton" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Triton is a dynamic binary analysis library. Build your own program analysis tools, automate your reverse engineering, perform software verification or just emulate code.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;http://triton.quarkslab.com/files/triton2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Triton&lt;/strong&gt; is a dynamic binary analysis library. It provides internal components like a &lt;strong&gt;dynamic symbolic execution&lt;/strong&gt; engine, a &lt;strong&gt;dynamic taint analysis&lt;/strong&gt; engine, &lt;strong&gt;AST representation&lt;/strong&gt; of the &lt;strong&gt;x86&lt;/strong&gt;, &lt;strong&gt;x86-64&lt;/strong&gt;, &lt;strong&gt;ARM32&lt;/strong&gt; and &lt;strong&gt;AArch64&lt;/strong&gt; ISA semantic, an &lt;strong&gt;expressions synthesis&lt;/strong&gt; engine, some &lt;strong&gt;SMT simplification&lt;/strong&gt; passes, &lt;strong&gt;SMT solver&lt;/strong&gt; interface to &lt;strong&gt;Z3&lt;/strong&gt; and &lt;strong&gt;Bitwuzla&lt;/strong&gt; and, the last but not least, &lt;strong&gt;Python bindings&lt;/strong&gt;. Based on these components, you are able to build your program analysis tools, automate reverse engineering, perform software verification or just emulate code.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://triton.quarkslab.com/files/triton_v09_architecture.svg?sanitize=true&#34; width=&#34;80%&#34;&gt;&lt;br&gt; &lt;img src=&#34;http://triton.quarkslab.com/files/triton_multi_os.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;As &lt;strong&gt;Triton&lt;/strong&gt; is a kind of a part-time project, please, &lt;strong&gt;don&#39;t blame us&lt;/strong&gt; if it is not fully reliable. &lt;a href=&#34;https://github.com/JonathanSalwan/Triton/issues&#34;&gt;Open issues&lt;/a&gt; or &lt;a href=&#34;https://github.com/JonathanSalwan/Triton/pulls&#34;&gt;pull requests&lt;/a&gt; are always better than trolling =). However, you can follow the development on twitter &lt;a href=&#34;https://twitter.com/qb_triton&#34;&gt;@qb_triton&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/JonathanSalwan/Triton/actions/workflows/linux.yml/&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/JonathanSalwan/Triton/Tests%20on%20Linux/master?label=Linux&amp;amp;logo=linux&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://github.com/JonathanSalwan/Triton/actions/workflows/osx.yml/&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/JonathanSalwan/Triton/Tests%20on%20OSX/master?label=OSX&amp;amp;logo=apple&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://ci.appveyor.com/project/JonathanSalwan/triton&#34;&gt; &lt;img src=&#34;https://img.shields.io/appveyor/ci/JonathanSalwan/triton/master.svg?label=Windows&amp;amp;logo=windows&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://codecov.io/gh/JonathanSalwan/Triton&#34;&gt; &lt;img src=&#34;https://codecov.io/gh/JonathanSalwan/Triton/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://github.com/JonathanSalwan/Triton/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/JonathanSalwan/Triton?logo=github&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://github.com/jonathansalwan/Triton/tree/dev-v1.0&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=dev&amp;amp;message=v1.0&amp;amp;logo=github&amp;amp;color=blue&#34;&gt; &lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://twitter.com/qb_triton&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/qb_triton?color=1da1f2&amp;amp;label=Follow&amp;amp;logo=twitter&amp;amp;logoColor=white&amp;amp;style=square&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/#install&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://triton.quarkslab.com/documentation/doxygen/py_triton_page.html&#34;&gt;Python API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://triton.quarkslab.com/documentation/doxygen/annotated.html&#34;&gt;C++ API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JonathanSalwan/Triton/tree/master/src/examples/python&#34;&gt;Python Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/#they-already-used-triton&#34;&gt;They already used Triton&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from triton import *&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Create the Triton context with a defined architecture&#xA;&amp;gt;&amp;gt;&amp;gt; ctx = TritonContext(ARCH.X86_64)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Define concrete values (optional)&#xA;&amp;gt;&amp;gt;&amp;gt; ctx.setConcreteRegisterValue(ctx.registers.rip, 0x40000)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Symbolize data (optional)&#xA;&amp;gt;&amp;gt;&amp;gt; ctx.symbolizeRegister(ctx.registers.rax, &#39;my_rax&#39;)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Execute instructions&#xA;&amp;gt;&amp;gt;&amp;gt; ctx.processing(Instruction(b&#34;\x48\x35\x34\x12\x00\x00&#34;)) # xor rax, 0x1234&#xA;&amp;gt;&amp;gt;&amp;gt; ctx.processing(Instruction(b&#34;\x48\x89\xc1&#34;)) # mov rcx, rax&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Get the symbolic expression&#xA;&amp;gt;&amp;gt;&amp;gt; rcx_expr = ctx.getSymbolicRegister(ctx.registers.rcx)&#xA;&amp;gt;&amp;gt;&amp;gt; print(rcx_expr)&#xA;(define-fun ref!8 () (_ BitVec 64) ref!1) ; MOV operation - 0x40006: mov rcx, rax&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Solve constraint&#xA;&amp;gt;&amp;gt;&amp;gt; ctx.getModel(rcx_expr.getAst() == 0xdead)&#xA;{0: my_rax:64 = 0xcc99}&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # 0xcc99 XOR 0x1234 is indeed equal to 0xdead&#xA;&amp;gt;&amp;gt;&amp;gt; hex(0xcc99 ^ 0x1234)&#xA;&#39;0xdead&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Triton relies on the following dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* libcapstone                &amp;gt;= 4.0.x   https://github.com/capstone-engine/capstone&#xA;* libboost      (optional)   &amp;gt;= 1.68&#xA;* libpython     (optional)   &amp;gt;= 3.6&#xA;* libz3         (optional)   &amp;gt;= 4.6.0   https://github.com/Z3Prover/z3&#xA;* libbitwuzla   (optional)   n/a        https://github.com/bitwuzla/bitwuzla&#xA;* llvm          (optional)   &amp;gt;= 12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linux and OS X&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ git clone https://github.com/JonathanSalwan/Triton&#xA;$ cd Triton&#xA;$ mkdir build ; cd build&#xA;$ cmake ..&#xA;$ make -j3&#xA;$ sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, LLVM and Bitwuzla are not compiled. If you want to enjoy the full power of Triton, the cmake compile is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ cmake -DLLVM_INTERFACE=ON -DCMAKE_PREFIX_PATH=$(llvm-config --prefix) -DBITWUZLA_INTERFACE=ON ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;You can use cmake to generate the .sln file of libTriton.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; git clone https://github.com/JonathanSalwan/Triton.git&#xA;&amp;gt; cd Triton&#xA;&amp;gt; mkdir build&#xA;&amp;gt; cd build&#xA;&amp;gt; cmake -G &#34;Visual Studio 14 2015 Win64&#34; \&#xA;  -DBOOST_ROOT=&#34;C:/Users/jonathan/Works/Tools/boost_1_61_0&#34; \&#xA;  -DPYTHON_INCLUDE_DIRS=&#34;C:/Python36/include&#34; \&#xA;  -DPYTHON_LIBRARIES=&#34;C:/Python36/libs/python36.lib&#34; \&#xA;  -DZ3_INCLUDE_DIRS=&#34;C:/Users/jonathan/Works/Tools/z3-4.6.0-x64-win/include&#34; \&#xA;  -DZ3_LIBRARIES=&#34;C:/Users/jonathan/Works/Tools/z3-4.6.0-x64-win/bin/libz3.lib&#34; \&#xA;  -DCAPSTONE_INCLUDE_DIRS=&#34;C:/Users/jonathan/Works/Tools/capstone-4.0.2-win64/include&#34; \&#xA;  -DCAPSTONE_LIBRARIES=&#34;C:/Users/jonathan/Works/Tools/capstone-4.0.2-win64/capstone.lib&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, if you prefer to directly download the precompiled library, check out our AppVeyor&#39;s &lt;a href=&#34;https://ci.appveyor.com/project/JonathanSalwan/triton/history&#34;&gt;artefacts&lt;/a&gt;. Note that if you use AppVeyor&#39;s artefacts, you probably have to install the &lt;a href=&#34;https://www.microsoft.com/en-US/download/details.aspx?id=30679&#34;&gt;Visual C++ Redistributable&lt;/a&gt; packages for Visual Studio 2012.&lt;/p&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Triton is strongly powered by &lt;a href=&#34;https://quarkslab.com&#34;&gt;Quarkslab&lt;/a&gt; for years but also by several strong contributors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/algillera&#34;&gt;&lt;strong&gt;Alberto Garcia Illera&lt;/strong&gt;&lt;/a&gt; - Cruise Automation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vishnya.xyz/&#34;&gt;&lt;strong&gt;Alexey Vishnyakov&lt;/strong&gt;&lt;/a&gt; - ISP RAS&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/black-binary&#34;&gt;&lt;strong&gt;Black Binary&lt;/strong&gt;&lt;/a&gt; - n/a&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cnheitman&#34;&gt;&lt;strong&gt;Christian Heitman&lt;/strong&gt;&lt;/a&gt; - Quarkslab&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apach301&#34;&gt;&lt;strong&gt;Daniil Kuts&lt;/strong&gt;&lt;/a&gt; - ISP RAS&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ek0&#34;&gt;&lt;strong&gt;Jessy Campos&lt;/strong&gt;&lt;/a&gt; - n/a&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/fvrmatteo&#34;&gt;&lt;strong&gt;Matteo F.&lt;/strong&gt;&lt;/a&gt; - n/a&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pbrunet&#34;&gt;&lt;strong&gt;Pierrick Brunet&lt;/strong&gt;&lt;/a&gt; - Quarkslab&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixelRick&#34;&gt;&lt;strong&gt;PixelRick&lt;/strong&gt;&lt;/a&gt; - n/a&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/rh0main&#34;&gt;&lt;strong&gt;Romain Thomas&lt;/strong&gt;&lt;/a&gt; - Quarkslab&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;They already used Triton&lt;/h2&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/illera88/Ponce&#34;&gt;Ponce&lt;/a&gt;: IDA 2016 plugin contest winner! Symbolic Execution just one-click away!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/quarkslab/qsynthesis&#34;&gt;QSynthesis&lt;/a&gt;: Greybox Synthesizer geared for deobfuscation of assembly instructions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kamou/pimp&#34;&gt;Pimp&lt;/a&gt;: Triton based R2 plugin for concolic execution and total control.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/d4em0n/exrop&#34;&gt;Exrop&lt;/a&gt;: Automatic ROPChain Generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Papers and conference&lt;/h3&gt; &#xA;&lt;ul dir=&#34;auto&#34;&gt; &#xA; &lt;li&gt; &lt;b&gt;Greybox Program Synthesis: A New Approach to Attack Dataflow Obfuscation&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: Blackhat USA, Las Vegas, Nevada, 2021. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/BHUSA2021-David-Greybox-Program-Synthesis.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Robin David&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;This talk presents the latest advances in program synthesis applied for deobfuscation. It aims at demystifying this analysis technique by showing how it can be put into action on obfuscation. Especially the implementation Qsynthesis released for this talk shows a complete end-to-end workflow to deobfuscate assembly instructions back in optimized (deobfuscated) instructions reassembled back in the binary.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;From source code to crash test-case through software testing automation&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: C&amp;amp;ESAR, Rennes, France, 2021. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/CESAR2021_robin-david-paper.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/CESAR2021_robin-david-slide.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Robin David, Jonathan Salwan, Justin Bourroux&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;This paper present an approach automating the software testing process from a source code to the dynamic testing of the compiled program. More specifically, from a static analysis report indicating alerts on source lines it enables testing to cover these lines dynamically and opportunistically checking whether whether or not they can trigger a crash. The result is a test corpus allowing to cover alerts and to trigger them if they happen to be true positives. This paper discuss the methodology employed to track alerts down in the compiled binary, the testing engines selection process and the results obtained on a TCP/IP stack implementation for embedded and IoT systems.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Symbolic Security Predicates: Hunt Program Weaknesses&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: Ivannikov ISP RAS Open Conference, Moscow, Russia, 2021. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/ISPOPEN2021-security-predicates-vishnyakov.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/ISPOPEN2021-slide-security-predicates-vishnyakov.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: A.Vishnyakov, V.Logunova, E.Kobrin, D.Kuts, D.Parygina, A.Fedotov&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;Dynamic symbolic execution (DSE) is a powerful method for path exploration during hybrid fuzzing and automatic bug detection. We propose security predicates to effectively detect undefined behavior and memory access violation errors. Initially, we symbolically execute program on paths that don‚Äôt trigger any errors (hybrid fuzzing may explore these paths). Then we construct a symbolic security predicate to verify some error condition. Thus, we may change the program data flow to entail null pointer dereference, division by zero, out-of-bounds access, or integer overflow weaknesses. Unlike static analysis, dynamic symbolic execution does not only report errors but also generates new input data to reproduce them. Furthermore, we introduce function semantics modeling for common C/C++ standard library functions. We aim to model the control flow inside a function with a single symbolic formula. This assists bug detection, speeds up path exploration, and overcomes overconstraints in path predicate. We implement the proposed techniques in our dynamic symbolic execution tool Sydr. Thus, we utilize powerful methods from Sydr such as path predicate slicing that eliminates irrelevant constraints. &lt;br&gt; &lt;/em&gt;&lt;p&gt;&lt;em&gt;We present Juliet Dynamic to measure dynamic bug detection tools accuracy. The testing system also verifies that generated inputs trigger sanitizers. We evaluate Sydr accuracy for 11 CWEs from Juliet test suite. Sydr shows 95.59% overall accuracy. We make Sydr evaluation artifacts publicly available to facilitate results reproducibility.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Towards Symbolic Pointers Reasoning in Dynamic Symbolic Execution&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: Ivannikov Memorial Workshop, Nizhny Novgorod, Russia, 2021. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/IVMEM2021-symbolic-pointers-kuts.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/IVMEM2021-slide-symbolic-pointers-kuts.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Daniil Kuts&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;Dynamic symbolic execution is a widely used technique for automated software testing, designed for execution paths exploration and program errors detection. A hybrid approach has recently become widespread, when the main goal of symbolic execution is helping fuzzer increase program coverage. The more branches symbolic executor can invert, the more useful it is for fuzzer. A program control flow often depends on memory values, which are obtained by computing address indexes from user input. However, most DSE tools don&#39;t support such dependencies, so they miss some desired program branches. We implement symbolic addresses reasoning on memory reads in our dynamic symbolic execution tool Sydr. Possible memory access regions are determined by either analyzing memory address symbolic expressions, or binary searching with SMT-solver. We propose an enhanced linearization technique to model memory accesses. Different memory modeling methods are compared on the set of programs. Our evaluation shows that symbolic addresses handling allows to discover new symbolic branches and increase the program coverage.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;QSynth: A Program Synthesis based Approach for Binary Code Deobfuscation&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: BAR, San Diego, California, 2020. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/BAR2020-qsynth-robin-david.pdf&#34;&gt;paper&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Robin David, Luigi Coniglio, Mariano Ceccato&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;We present a generic approach leveraging both DSE and program synthesis to successfully synthesize programs obfuscated with Mixed-Boolean-Arithmetic, Data-Encoding or Virtualization. The synthesis algorithm proposed is an offline enumerate synthesis primitive guided by top-down breath-first search. We shows its effectiveness against a state-of-the-art obfuscator and its scalability as it supersedes other similar approaches based on synthesis. We also show its effectiveness in presence of composite obfuscation (combination of various techniques). This ongoing work enlightens the effectiveness of synthesis to target certain kinds of obfuscations and opens the way to more robust algorithms and simplification strategies.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Sydr: Cutting Edge Dynamic Symbolic Execution&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: Ivannikov ISP RAS Open Conference, Moscow, Russia, 2020. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/ISPRAS2020-sydr.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/ISPOPEN2020-slide-sydr-vishnyakov.pdf&#34;&gt;slide&lt;/a&gt;] [&lt;a href=&#34;https://www.ispras.ru/conf/2020/video/compiler-technology-11-december.mp4#t=6021&#34;&gt;video&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: A.Vishnyakov, A.Fedotov, D.Kuts, A.Novikov, D.Parygina, E.Kobrin, V.Logunova, P.Belecky, S.Kurmangaleev&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;Dynamic symbolic execution (DSE) has enormous amount of applications in computer security (fuzzing, vulnerability discovery, reverse-engineering, etc.). We propose several performance and accuracy improvements for dynamic symbolic execution. Skipping non-symbolic instructions allows to build a path predicate 1.2--3.5 times faster. Symbolic engine simplifies formulas during symbolic execution. Path predicate slicing eliminates irrelevant conjuncts from solver queries. We handle each jump table (switch statement) as multiple branches and describe the method for symbolic execution of multi-threaded programs. The proposed solutions were implemented in Sydr tool. Sydr performs inversion of branches in path predicate. Sydr combines DynamoRIO dynamic binary instrumentation tool with Triton symbolic engine.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Symbolic Deobfuscation: From Virtualized Code Back to the Original&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: DIMVA, Paris-Saclay, France, 2018. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/DIMVA2018-deobfuscation-salwan-bardin-potet.pdf&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/DIMVA2018-slide-deobfuscation-salwan-bardin-potet.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan, S√©bastien Bardin, Marie-Laure Potet&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;Software protection has taken an important place during the last decade in order to protect legit software against reverse engineering or tampering. Virtualization is considered as one of the very best defenses against such attacks. We present a generic approach based on symbolic path exploration, taint and recompilation allowing to recover, from a virtualized code, a devirtualized code semantically identical to the original one and close in size. We define criteria and metrics to evaluate the relevance of the deobfuscated results in terms of correctness and precision. Finally we propose an open-source setup allowing to evaluate the proposed approach against several forms of virtualization.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Deobfuscation of VM based software protection &lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: SSTIC, Rennes, France, 2017. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/SSTIC2017-French-Article-desobfuscation_binaire_reconstruction_de_fonctions_virtualisees-salwan_potet_bardin.pdf&#34;&gt;french paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/SSTIC2017_Deobfuscation_of_VM_based_software_protection.pdf&#34;&gt;english slide&lt;/a&gt;] [&lt;a href=&#34;https://static.sstic.org/videos2017/SSTIC_2017-06-07_P08.mp4&#34;&gt;french video&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan, S√©bastien Bardin, Marie-Laure Potet&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;In this presentation we describe an approach which consists to automatically analyze virtual machine based software protections and which recompiles a new version of the binary without such protections. This automated approach relies on a symbolic execution guide by a taint analysis and some concretization policies, then on a binary rewriting using LLVM transition.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;How Triton can help to reverse virtual machine based software protections&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: CSAW SOS, NYC, New York, 2016. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/CSAW2016-SOS-Virtual-Machine-Deobfuscation-RThomas_JSalwan.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan, Romain Thomas&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;The first part of the talk is going to be an introduction to the Triton framework to expose its components and to explain how they work together. Then, the second part will include demonstrations on how it&#39;s possible to reverse virtual machine based protections using taint analysis, symbolic execution, SMT simplifications and LLVM-IR optimizations.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Dynamic Binary Analysis and Obfuscated Codes&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: St&#39;Hack, Bordeaux, France, 2016. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/StHack2016_Dynamic_Binary_Analysis_and_Obfuscated_Codes_RThomas_JSalwan.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan, Romain Thomas&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;At this presentation we will talk about how a DBA (Dynamic Binary Analysis) may help a reverse engineer to reverse obfuscated code. We will first introduce some basic obfuscation techniques and then expose how it&#39;s possible to break some stuffs (using our open-source DBA framework - Triton) like detect opaque predicates, reconstruct CFG, find the original algorithm, isolate sensible data and many more... Then, we will conclude with a demo and few words about our future work.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;How Triton may help to analyse obfuscated binaries&lt;/b&gt;&lt;br&gt; &lt;b&gt;Publication at&lt;/b&gt;: MISC magazine 82, 2015. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/MISC-82_French_Paper_How_Triton_may_help_to_analyse_obfuscated_binaries_RThomas_JSalwan.pdf&#34;&gt;french article&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan, Romain Thomas&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;Binary obfuscation is used to protect software&#39;s intellectual property. There exist different kinds of obfucation but roughly, it transforms a binary structure into another binary structure by preserving the same semantic. The aim of obfuscation is to ensure that the original information is &#34;drown&#34; in useless information that will make reverse engineering harder. In this article we will show how we can analyse an ofbuscated program and break some obfuscations using the Triton framework.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Triton: A Concolic Execution Framework&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: SSTIC, Rennes, France, 2015. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/SSTIC2015_French_Paper_Triton_Framework_dexecution_Concolique_FSaudel_JSalwan.pdf&#34;&gt;french paper&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/SSTIC2015_English_slide_detailed_version_Triton_Concolic_Execution_FrameWork_FSaudel_JSalwan.pdf&#34;&gt;detailed english slide&lt;/a&gt;] &lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan, Florent Saudel&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;This talk is about the release of Triton, a concolic execution framework based on Pin. It provides components like a taint engine, a dynamic symbolic execution engine, a snapshot engine, translation of x64 instruction to SMT2, a Z3 interface to solve constraints and Python bindings. Based on these components, Triton offers the possibility to build tools for vulnerabilities research or reverse-engineering assistance.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Dynamic Behavior Analysis Using Binary Instrumentation&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: St&#39;Hack, Bordeaux, France, 2015. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/StHack2015_Dynamic_Behavior_Analysis_using_Binary_Instrumentation_Jonathan_Salwan.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;This talk can be considered like the part 2 of our talk at SecurityDay. In the previous part, we talked about how it was possible to cover a targeted function in memory using the DSE (Dynamic Symbolic Execution) approach. Cover a function (or its states) doesn&#39;t mean find all vulnerabilities, some vulnerability doesn&#39;t crashes the program. That&#39;s why we must implement specific analysis to find specific bugs. These analysis are based on the binary instrumentation and the runtime behavior analysis of the program. In this talk, we will see how it&#39;s possible to find these following kind of bugs : off-by-one, stack / heap overflow, use-after-free, format string and {write, read}-what-where.&lt;/em&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;b&gt;Covering a function using a Dynamic Symbolic Execution approach&lt;/b&gt;&lt;br&gt; &lt;b&gt;Talk at&lt;/b&gt;: Security Day, Lille, France, 2015. [&lt;a href=&#34;https://raw.githubusercontent.com/JonathanSalwan/Triton/master/publications/SecurityDay2015_dynamic_symbolic_execution_Jonathan_Salwan.pdf&#34;&gt;slide&lt;/a&gt;]&lt;br&gt; &lt;b&gt;Authors&lt;/b&gt;: Jonathan Salwan&lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt;: &lt;em&gt;This talk is about binary analysis and instrumentation. We will see how it&#39;s possible to target a specific function, snapshot the context memory/registers before the function, translate the instrumentation into an intermediate representation,apply a taint analysis based on this IR, build/keep formulas for a Dynamic Symbolic Execution (DSE), generate a concrete value to go through a specific path, restore the context memory/register and generate another concrete value to go through another path then repeat this operation until the target function is covered.&lt;/em&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite Triton&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@inproceedings{SSTIC2015-Saudel-Salwan,&#xA;  author    = {Saudel, Florent and Salwan, Jonathan},&#xA;  title     = {Triton: A Dynamic Symbolic Execution Framework},&#xA;  booktitle = {Symposium sur la s{\&#39;{e}}curit{\&#39;{e}} des technologies de l&#39;information&#xA;               et des communications},&#xA;  series    = {SSTIC},&#xA;  pages     = {31--54},&#xA;  address   = {Rennes, France},&#xA;  month     = jun,&#xA;  year      = {2015},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>uglide/RedisDesktopManager</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/uglide/RedisDesktopManager</id>
    <link href="https://github.com/uglide/RedisDesktopManager" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üîß Cross-platform Developer GUI for Redis&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://vshymanskyy.github.io/StandWithUkraine&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg?sanitize=true&#34; alt=&#34;SWUbanner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://resp.app&#34; title=&#34;RESP.app Official Site&#34;&gt;RESP.app - GUI for Redis ¬Æ (Formerly RedisDesktopManager)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://docs.resp.app/en/latest/install/&#34;&gt;Install &amp;amp; Run&lt;/a&gt; | &lt;a href=&#34;http://docs.resp.app/en/latest/quick-start/&#34;&gt;Quick Start&lt;/a&gt; | &lt;a href=&#34;http://docs.resp.app/en/latest/known-issues/&#34;&gt;Known issues&lt;/a&gt; | &lt;a href=&#34;https://t.me/RedisDesktopManager&#34;&gt;&lt;strong&gt;Telegram Chat&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;http://docs.resp.app/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/redisdesktopmanager/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Open source cross-platform Desktop Manager for Redis ¬Æ based on Qt 5&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://raw.githubusercontent.com/www.microsoft.com/store/apps/9NDK76ZVZ3TM?cid=storebadge&amp;amp;ocid=badge&#34;&gt; &lt;img height=&#34;50&#34; src=&#34;https://developer.microsoft.com/en-us/store/badges/images/English_get-it-from-MS.png&#34; alt=&#34;Get it from Microsoft&#34;&gt; &lt;/a&gt; &#xA;&lt;a class=&#34;btn btn-lg btn-block&#34; href=&#34;https://apps.apple.com/app/redisdesktopmanager/id1475905948&#34; target=&#34;_blank&#34;&gt; &lt;img height=&#34;50&#34; src=&#34;https://resp.app/static/img/v2/app-store-badge.svg?sanitize=true&#34; alt=&#34;Download from Apple App Store&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://snapcraft.io/redis-desktop-manager&#34;&gt; &lt;img height=&#34;50&#34; alt=&#34;Get it from the Snap Store&#34; src=&#34;https://snapcraft.io/static/images/badges/en/snap-store-black.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://flathub.org/apps/details/app.resp.RESP&#34;&gt;&lt;img height=&#34;50&#34; alt=&#34;Download on Flathub&#34; src=&#34;https://flathub.org/assets/badges/flathub-badge-en.png&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://resp.app/static/img/features/all.png?v2021&#34; alt=&#34;RESP.app screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Redis versions&lt;/strong&gt;: 2.8+ (for old redis-servers use &lt;a href=&#34;https://github.com/uglide/RedisDesktopManager/releases/tag/0.8.8&#34;&gt;RedisDesktopManager 0.8.8&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>skyline-emu/skyline</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/skyline-emu/skyline</id>
    <link href="https://github.com/skyline-emu/skyline" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run Nintendo Switch homebrew &amp; games on your Android device!&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/skyline-emu/skyline&#34; target=&#34;_blank&#34;&gt; &lt;img height=&#34;60%&#34; width=&#34;60%&#34; src=&#34;https://raw.github.com/skyline-emu/branding/master/banner/skyline-banner-rounded.png&#34;&gt;&lt;br&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/XnbXNQM&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/545842171459272705.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=5865F2&amp;amp;labelColor=404EED&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/skyline-emu/skyline/actions/workflows/ci.yml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/skyline-emu/skyline/actions/workflows/ci.yml/badge.svg?sanitize=true&#34;&gt;&lt;br&gt; &lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skyline-emu/skyline/master/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/skyline-emu/skyline/master/BUILDING.md&#34;&gt;Building Guide&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Skyline&lt;/b&gt; is an experimental emulator that runs on &lt;b&gt;ARMv8 Android‚Ñ¢&lt;/b&gt; devices and emulates the functionality of a &lt;b&gt;Nintendo Switch‚Ñ¢&lt;/b&gt; system, licensed under &lt;a href=&#34;https://github.com/skyline-emu/skyline/raw/master/LICENSE.md&#34;&gt;&lt;b&gt;Mozilla Public License 2.0&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;p&gt;You can contact the core developers of Skyline at our &lt;strong&gt;&lt;a href=&#34;https://discord.gg/XnbXNQM&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt;. If you have any questions, feel free to ask. It&#39;s also a good place to just keep up with the emulator, as most talk regarding development goes on over there.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Special Thanks&lt;/h3&gt; &#xA;&lt;p&gt;A few noteworthy teams/projects who&#39;ve helped us along the way are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ryujinx.org/&#34;&gt;Ryujinx&lt;/a&gt;:&lt;/strong&gt; We&#39;ve used Ryujinx for reference throughout the project, the accuracy of their HLE implementations of Switch subsystems make it an amazing reference. The team behind the project has been extremely helpful with any queries we&#39;ve had and have constantly helped us with any issues we&#39;ve come across. &lt;strong&gt;It should be noted that Skyline is not based on Ryujinx&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://yuzu-emu.org/&#34;&gt;yuzu&lt;/a&gt;:&lt;/strong&gt; Skyline&#39;s shader compiler is a &lt;strong&gt;fork&lt;/strong&gt; of &lt;em&gt;yuzu&lt;/em&gt;&#39;s shader compiler with Skyline-specific changes, using it allowed us to focus on the parts of GPU emulation that we could specifically optimize for mobile while having a high-quality shader compiler implementation as a base. The team behind &lt;em&gt;yuzu&lt;/em&gt; has also often helped us and have graciously provided us with a license exemption.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/switchbrew/&#34;&gt;Switchbrew&lt;/a&gt;:&lt;/strong&gt; We&#39;ve extensively used Switchbrew whether that be their &lt;strong&gt;&lt;a href=&#34;https://switchbrew.org/&#34;&gt;wiki&lt;/a&gt;&lt;/strong&gt; with its colossal amount of information on the Switch that has saved us countless hours of time or &lt;strong&gt;&lt;a href=&#34;https://github.com/switchbrew/libnx&#34;&gt;libnx&lt;/a&gt;&lt;/strong&gt; which was crucial to initial development of the emulator to ensure that our HLE kernel and sysmodule implementations were accurate.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nintendo Switch&lt;/strong&gt; is a trademark of &lt;strong&gt;Nintendo Co., Ltd&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt; is a trademark of &lt;strong&gt;Google LLC&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>IntelRealSense/realsense-ros</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/IntelRealSense/realsense-ros</id>
    <link href="https://github.com/IntelRealSense/realsense-ros" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Intel(R) RealSense(TM) ROS Wrapper for D400 series, SR300 Camera and T265 Tracking Module&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ROS Wrapper for Intel¬Æ RealSense‚Ñ¢ Devices&lt;/h1&gt; &#xA;&lt;p&gt;These are packages for using Intel RealSense cameras (D400 series SR300 camera and T265 Tracking Module) with ROS.&lt;/p&gt; &#xA;&lt;p&gt;This version supports Kinetic, Melodic and Noetic distributions.&lt;/p&gt; &#xA;&lt;p&gt;For running in ROS2 environment please switch to the &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros/tree/ros2-beta&#34;&gt;ros2 branch&lt;/a&gt;. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;LibRealSense2 supported version: v2.50.0 (see &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros/releases&#34;&gt;realsense2_camera release notes&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Ubuntu&lt;/h3&gt; &#xA;&lt;h4&gt;Step 1: Install the ROS distribution&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h4&gt;Install &lt;a href=&#34;http://wiki.ros.org/kinetic/Installation/Ubuntu&#34;&gt;ROS Kinetic&lt;/a&gt;, on Ubuntu 16.04, &lt;a href=&#34;http://wiki.ros.org/melodic/Installation/Ubuntu&#34;&gt;ROS Melodic&lt;/a&gt; on Ubuntu 18.04 or &lt;a href=&#34;http://wiki.ros.org/noetic/Installation/Ubuntu&#34;&gt;ROS Noetic&lt;/a&gt; on Ubuntu 20.04.&lt;/h4&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;h4&gt;Step 1: Install the ROS distribution&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h4&gt;Install &lt;a href=&#34;https://wiki.ros.org/Installation/Windows&#34;&gt;ROS Melodic or later on Windows 10&lt;/a&gt;&lt;/h4&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;There are 2 sources to install realsense2_camera from:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h3&gt;Method 1: The ROS distribution:&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Ubuntu&lt;/em&gt;&lt;/p&gt; &lt;p&gt;realsense2_camera is available as a debian package of ROS distribution. It can be installed by typing:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo apt-get install ros-$ROS_DISTRO-realsense2-camera&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This will install both realsense2_camera and its dependents, including librealsense2 library and matching udev-rules.&lt;/p&gt; &lt;p&gt;Notice:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The version of librealsense2 is almost always behind the one availeable in RealSense‚Ñ¢ official repository.&lt;/li&gt; &#xA;   &lt;li&gt;librealsense2 is not built to use native v4l2 driver but the less stable RS-USB protocol. That is because the last is more general and operational on a larger variety of platforms.&lt;/li&gt; &#xA;   &lt;li&gt;realsense2_description is available as a separate debian package of ROS distribution. It includes the 3D-models of the devices and is necessary for running launch files that include these models (i.e. rs_d435_camera_with_model.launch). It can be installed by typing: &lt;code&gt;sudo apt-get install ros-$ROS_DISTRO-realsense2-description&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Windows&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chocolatey distribution Coming soon&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Method 2: The RealSense‚Ñ¢ distribution:&lt;/h3&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;This option is demonstrated in the &lt;a href=&#34;https://github.com/intel-ros/realsense/raw/development/.travis.yml&#34;&gt;.travis.yml&lt;/a&gt; file. It basically summerize the elaborate instructions in the following 2 steps:&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;h3&gt;Step 1: Install the latest Intel¬Æ RealSense‚Ñ¢ SDK 2.0&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Ubuntu&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Install librealsense2 debian package:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Jetson users - use the &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/installation_jetson.md&#34;&gt;Jetson Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Otherwise, install from &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/distribution_linux.md#installing-the-packages&#34;&gt;Linux Debian Installation Guide&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;In that case treat yourself as a developer. Make sure you follow the instructions to also install librealsense2-dev and librealsense2-dkms packages.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Windows&lt;/em&gt; Install using vcpkg&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  `vcpkg install realsense2:x64-windows` &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;OR&lt;/h4&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;h4&gt;Build from sources by downloading the latest &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/releases/tag/v2.50.0&#34;&gt;Intel¬Æ RealSense‚Ñ¢ SDK 2.0&lt;/a&gt; and follow the instructions under &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/installation.md&#34;&gt;Linux Installation&lt;/a&gt;&lt;/h4&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;h3&gt;Step 2: Install Intel¬Æ RealSense‚Ñ¢ ROS from Sources&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Create a &lt;a href=&#34;http://wiki.ros.org/catkin#Installing_catkin&#34;&gt;catkin&lt;/a&gt; workspace &lt;em&gt;Ubuntu&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/catkin_ws/src&#xA;cd ~/catkin_ws/src/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Windows&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;mkdir c:\catkin_ws\src&#xA;cd c:\catkin_ws\src&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Clone the latest Intel¬Æ RealSense‚Ñ¢ ROS from &lt;a href=&#34;https://github.com/intel-ros/realsense/releases&#34;&gt;here&lt;/a&gt; into &#39;catkin_ws/src/&#39;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bashrc&#34;&gt;git clone https://github.com/IntelRealSense/realsense-ros.git&#xA;cd realsense-ros/&#xA;git checkout `git tag | sort -V | grep -P &#34;^2.\d+\.\d+&#34; | tail -1`&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure all dependent packages are installed. You can check .travis.yml file for reference.&lt;/li&gt; &#xA;   &lt;li&gt;Specifically, make sure that the ros package &lt;em&gt;ddynamic_reconfigure&lt;/em&gt; is installed. If &lt;em&gt;ddynamic_reconfigure&lt;/em&gt; cannot be installed using APT or if you are using &lt;em&gt;Windows&lt;/em&gt; you may clone it into your workspace &#39;catkin_ws/src/&#39; from &lt;a href=&#34;https://github.com/pal-robotics/ddynamic_reconfigure/tree/kinetic-devel&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;catkin_init_workspace&#xA;cd ..&#xA;catkin_make clean&#xA;catkin_make -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release&#xA;catkin_make install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Ubuntu&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#34;source ~/catkin_ws/devel/setup.bash&#34; &amp;gt;&amp;gt; ~/.bashrc&#xA;source ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Windows&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;devel\setup.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Start the camera node&lt;/h3&gt; &#xA;&lt;p&gt;To start the camera node in ROS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_camera.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will stream all camera sensors and publish on the appropriate ROS topics.&lt;/p&gt; &#xA;&lt;p&gt;Other stream resolutions and frame rates can optionally be provided as parameters to the &#39;rs_camera.launch&#39; file.&lt;/p&gt; &#xA;&lt;h3&gt;Published Topics&lt;/h3&gt; &#xA;&lt;p&gt;The published topics differ according to the device and parameters. After running the above command with D435i attached, the following list of topics will be available (This is a partial list. For full one type &lt;code&gt;rostopic list&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/camera/color/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/color/image_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/color/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/depth/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/depth/image_rect_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/depth/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/extrinsics/depth_to_color&lt;/li&gt; &#xA; &lt;li&gt;/camera/extrinsics/depth_to_infra1&lt;/li&gt; &#xA; &lt;li&gt;/camera/extrinsics/depth_to_infra2&lt;/li&gt; &#xA; &lt;li&gt;/camera/infra1/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/infra1/image_rect_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/infra2/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/infra2/image_rect_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/gyro/imu_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/gyro/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/gyro/sample&lt;/li&gt; &#xA; &lt;li&gt;/camera/accel/imu_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/accel/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/accel/sample&lt;/li&gt; &#xA; &lt;li&gt;/diagnostics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Using an L515 device the list differs a little by adding a 4-bit confidence grade (pulished as a mono8 image):&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;/camera/confidence/camera_info&lt;/li&gt; &#xA;  &lt;li&gt;/camera/confidence/image_rect_raw&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;It also replaces the 2 infrared topics with the single available one:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;/camera/infra/camera_info&lt;/li&gt; &#xA;  &lt;li&gt;/camera/infra/image_raw&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The &#34;/camera&#34; prefix is the default and can be changed. Check the rs_multiple_devices.launch file for an example. If using D435 or D415, the gyro and accel topics wont be available. Likewise, other topics will be available when using T265 (see below).&lt;/p&gt; &#xA;&lt;h3&gt;Launch parameters&lt;/h3&gt; &#xA;&lt;p&gt;The following parameters are available by the wrapper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;serial_no&lt;/strong&gt;: will attach to the device with the given serial number (&lt;em&gt;serial_no&lt;/em&gt;) number. Default, attach to available RealSense device in random.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;usb_port_id&lt;/strong&gt;: will attach to the device with the given USB port (&lt;em&gt;usb_port_id&lt;/em&gt;). i.e 4-1, 4-2 etc. Default, ignore USB port when choosing a device.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;device_type&lt;/strong&gt;: will attach to a device whose name includes the given &lt;em&gt;device_type&lt;/em&gt; regular expression pattern. Default, ignore device type. For example, device_type:=d435 will match d435 and d435i. device_type=d435(?!i) will match d435 but not d435i.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;rosbag_filename&lt;/strong&gt;: Will publish topics from rosbag file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;initial_reset&lt;/strong&gt;: On occasions the device was not closed properly and due to firmware issues needs to reset. If set to true, the device will reset prior to usage.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;reconnect_timeout&lt;/strong&gt;: When the driver cannot connect to the device try to reconnect after this timeout (in seconds).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;align_depth&lt;/strong&gt;: If set to true, will publish additional topics for the &#34;aligned depth to color&#34; image.: &lt;code&gt;/camera/aligned_depth_to_color/image_raw&lt;/code&gt;, &lt;code&gt;/camera/aligned_depth_to_color/camera_info&lt;/code&gt;.&lt;br&gt; The pointcloud, if enabled, will be built based on the aligned_depth_to_color image.&lt;br&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;filters&lt;/strong&gt;: any of the following options, separated by commas:&lt;br&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;colorizer&lt;/code&gt;: will color the depth image. On the depth topic an RGB image will be published, instead of the 16bit depth values .&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;pointcloud&lt;/code&gt;: will add a pointcloud topic &lt;code&gt;/camera/depth/color/points&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The texture of the pointcloud can be modified in rqt_reconfigure (see below) or using the parameters: &lt;code&gt;pointcloud_texture_stream&lt;/code&gt; and &lt;code&gt;pointcloud_texture_index&lt;/code&gt;. Run rqt_reconfigure to see available values for these parameters.&lt;br&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The depth FOV and the texture FOV are not similar. By default, pointcloud is limited to the section of depth containing the texture. You can have a full depth to pointcloud, coloring the regions beyond the texture with zeros, by setting &lt;code&gt;allow_no_texture_points&lt;/code&gt; to true.&lt;/li&gt; &#xA;   &lt;li&gt;pointcloud is of an unordered format by default. This can be changed by setting &lt;code&gt;ordered_pc&lt;/code&gt; to true.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;hdr_merge&lt;/code&gt;: Allows depth image to be created by merging the information from 2 consecutive frames, taken with different exposure and gain values. The way to set exposure and gain values for each sequence in runtime is by first selecting the sequence id, using rqt_reconfigure &lt;code&gt;stereo_module/sequence_id&lt;/code&gt; parameter and then modifying the &lt;code&gt;stereo_module/gain&lt;/code&gt;, and &lt;code&gt;stereo_module/exposure&lt;/code&gt;.&lt;br&gt; To view the effect on the infrared image for each sequence id use the &lt;code&gt;sequence_id_filter/sequence_id&lt;/code&gt; parameter.&lt;br&gt; To initialize these parameters in start time use the following parameters:&lt;br&gt; &lt;code&gt;stereo_module/exposure/1&lt;/code&gt;, &lt;code&gt;stereo_module/gain/1&lt;/code&gt;, &lt;code&gt;stereo_module/exposure/2&lt;/code&gt;, &lt;code&gt;stereo_module/gain/2&lt;/code&gt;&lt;br&gt; * For in-depth review of the subject please read the accompanying &lt;a href=&#34;https://dev.intelrealsense.com/docs/high-dynamic-range-with-stereoscopic-depth-cameras&#34;&gt;white paper&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following filters have detailed descriptions in : &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/post-processing-filters.md&#34;&gt;https://github.com/IntelRealSense/librealsense/blob/master/doc/post-processing-filters.md&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;disparity&lt;/code&gt; - convert depth to disparity before applying other filters and back.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;spatial&lt;/code&gt; - filter the depth image spatially.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temporal&lt;/code&gt; - filter the depth image temporally.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;hole_filling&lt;/code&gt; - apply hole-filling filter.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;decimation&lt;/code&gt; - reduces depth scene complexity.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;enable_sync&lt;/strong&gt;: gathers closest frames of different sensors, infra red, color and depth, to be sent with the same timetag. This happens automatically when such filters as pointcloud are enabled.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;&amp;lt;stream_type&amp;gt;&lt;/em&gt;_width&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;&amp;lt;stream_type&amp;gt;&lt;/em&gt;_height&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;&amp;lt;stream_type&amp;gt;&lt;/em&gt;_fps&lt;/strong&gt;: &amp;lt;stream_type&amp;gt; can be any of &lt;em&gt;infra, color, fisheye, depth, gyro, accel, pose, confidence&lt;/em&gt;. Sets the required format of the device. If the specified combination of parameters is not available by the device, the stream will be replaced with the default for that stream. Setting a value to 0, will choose the first format in the inner list. (i.e. consistent between runs but not defined).&lt;br&gt;*Note: for gyro accel and pose, only _fps option is meaningful.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;enable_&lt;em&gt;&amp;lt;stream_name&amp;gt;&lt;/em&gt;&lt;/strong&gt;: Choose whether to enable a specified stream or not. Default is true for images and false for orientation streams. &amp;lt;stream_name&amp;gt; can be any of &lt;em&gt;infra1, infra2, color, depth, fisheye, fisheye1, fisheye2, gyro, accel, pose, confidence&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;tf_prefix&lt;/strong&gt;: By default all frame&#39;s ids have the same prefix - &lt;code&gt;camera_&lt;/code&gt;. This allows changing it per camera.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;&amp;lt;stream_name&amp;gt;&lt;/em&gt;_frame_id&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;&amp;lt;stream_name&amp;gt;&lt;/em&gt;_optical_frame_id&lt;/strong&gt;, &lt;strong&gt;aligned_depth_to_&lt;em&gt;&amp;lt;stream_name&amp;gt;&lt;/em&gt;_frame_id&lt;/strong&gt;: Specify the different frame_id for the different frames. Especially important when using multiple cameras.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;base_frame_id&lt;/strong&gt;: defines the frame_id all static transformations refers to.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;odom_frame_id&lt;/strong&gt;: defines the origin coordinate system in ROS convention (X-Forward, Y-Left, Z-Up). pose topic defines the pose relative to that system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;All the rest of the frame_ids can be found in the template launch file: &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/development/realsense2_camera/launch/includes/nodelet.launch.xml&#34;&gt;nodelet.launch.xml&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;unite_imu_method&lt;/strong&gt;: The D435i and T265 cameras have built in IMU components which produce 2 unrelated streams: &lt;em&gt;gyro&lt;/em&gt; - which shows angular velocity and &lt;em&gt;accel&lt;/em&gt; which shows linear acceleration. Each with it&#39;s own frequency. By default, 2 corresponding topics are available, each with only the relevant fields of the message sensor_msgs::Imu are filled out. Setting &lt;em&gt;unite_imu_method&lt;/em&gt; creates a new topic, &lt;em&gt;imu&lt;/em&gt;, that replaces the default &lt;em&gt;gyro&lt;/em&gt; and &lt;em&gt;accel&lt;/em&gt; topics. The &lt;em&gt;imu&lt;/em&gt; topic is published at the rate of the gyro. All the fields of the Imu message under the &lt;em&gt;imu&lt;/em&gt; topic are filled out.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;linear_interpolation&lt;/strong&gt;: Every gyro message is attached by the an accel message interpolated to the gyro&#39;s timestamp.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;copy&lt;/strong&gt;: Every gyro message is attached by the last accel message.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;clip_distance&lt;/strong&gt;: remove from the depth image all values above a given value (meters). Disable by giving negative value (default)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;linear_accel_cov&lt;/strong&gt;, &lt;strong&gt;angular_velocity_cov&lt;/strong&gt;: sets the variance given to the Imu readings. For the T265, these values are being modified by the inner confidence value.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;hold_back_imu_for_frames&lt;/strong&gt;: Images processing takes time. Therefor there is a time gap between the moment the image arrives at the wrapper and the moment the image is published to the ROS environment. During this time, Imu messages keep on arriving and a situation is created where an image with earlier timestamp is published after Imu message with later timestamp. If that is a problem, setting &lt;em&gt;hold_back_imu_for_frames&lt;/em&gt; to &lt;em&gt;true&lt;/em&gt; will hold the Imu messages back while processing the images and then publish them all in a burst, thus keeping the order of publication as the order of arrival. Note that in either case, the timestamp in each message&#39;s header reflects the time of it&#39;s origin.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;topic_odom_in&lt;/strong&gt;: For T265, add wheel odometry information through this topic. The code refers only to the &lt;em&gt;twist.linear&lt;/em&gt; field in the message.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;calib_odom_file&lt;/strong&gt;: For the T265 to include odometry input, it must be given a &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/unit-tests/resources/calibration_odometry.json&#34;&gt;configuration file&lt;/a&gt;. Explanations can be found &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/pull/3462&#34;&gt;here&lt;/a&gt;. The calibration is done in ROS coordinates system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;publish_tf&lt;/strong&gt;: boolean, publish or not TF at all. Defaults to True.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;tf_publish_rate&lt;/strong&gt;: double, positive values mean dynamic transform publication with specified rate, all other values mean static transform publication. Defaults to 0&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;publish_odom_tf&lt;/strong&gt;: If True (default) publish TF from odom_frame to pose_frame.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;infra_rgb&lt;/strong&gt;: When set to True (default: False), it configures the infrared camera to stream in RGB (color) mode, thus enabling the use of a RGB image in the same frame as the depth image, potentially avoiding frame transformation related errors. When this feature is required, you are additionally required to also enable &lt;code&gt;enable_infra:=true&lt;/code&gt; for the infrared stream to be enabled.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt; The configuration required for &lt;code&gt;enable_infra&lt;/code&gt; is independent of &lt;code&gt;enable_depth&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt; To enable the Infrared stream, you should enable &lt;code&gt;enable_infra:=true&lt;/code&gt; NOT &lt;code&gt;enable_infra1:=true&lt;/code&gt; nor &lt;code&gt;enable_infra2:=true&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt; This feature is only supported by Realsense sensors with RGB streams available from the &lt;code&gt;infra&lt;/code&gt; cameras, which can be checked by observing the output of &lt;code&gt;rs-enumerate-devices&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Available services:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;reset : Cause a hardware reset of the device. Usage: &lt;code&gt;rosservice call /camera/realsense2_camera/reset&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;enable : Start/Stop all streaming sensors. Usage example: &lt;code&gt;rosservice call /camera/enable False&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;device_info : retrieve information about the device - serial_number, firmware_version etc. Type &lt;code&gt;osservice type /camera/realsense2_camera/device_info | rossrv show&lt;/code&gt; for the full list. Call example: &lt;code&gt;rosservice call /camera/realsense2_camera/device_info&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Point Cloud&lt;/h3&gt; &#xA;&lt;p&gt;Here is an example of how to start the camera node and make it publish the point cloud using the pointcloud option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_camera.launch filters:=pointcloud&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open rviz to watch the pointcloud:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17433152/35396613-ddcb1d6c-01f5-11e8-8887-4debf178d0cc.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Aligned Depth Frames&lt;/h3&gt; &#xA;&lt;p&gt;Here is an example of how to start the camera node and make it publish the aligned depth stream to other available streams such as color or infra-red.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_camera.launch align_depth:=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;https://user-images.githubusercontent.com/17433152/35343104-6eede0f0-0132-11e8-8866-e6c7524dd079.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Set Camera Controls Using Dynamic Reconfigure Params&lt;/h3&gt; &#xA;&lt;p&gt;The following command allow to change camera control values using [http://wiki.ros.org/rqt_reconfigure].&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rosrun rqt_reconfigure rqt_reconfigure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/40540281/55330573-065d8600-549a-11e9-996a-5d193cbd9a93.PNG&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Work with multiple cameras&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Notice:&lt;/strong&gt; Launching multiple T265 cameras is currently not supported. This will be addressed in a later version.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of how to start the camera node and streaming with two cameras using the &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/development/realsense2_camera/launch/rs_multiple_devices.launch&#34;&gt;rs_multiple_devices.launch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_multiple_devices.launch serial_no_camera1:=&amp;lt;serial number of the first camera&amp;gt; serial_no_camera2:=&amp;lt;serial number of the second camera&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The camera serial number should be provided to &lt;code&gt;serial_no_camera1&lt;/code&gt; and &lt;code&gt;serial_no_camera2&lt;/code&gt; parameters. One way to get the serial number is from the &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/58d99783cc2781b1026eeed959aa3f7b562b20ca/tools/enumerate-devices/readme.md&#34;&gt;rs-enumerate-devices&lt;/a&gt; tool.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rs-enumerate-devices | grep Serial&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another way of obtaining the serial number is connecting the camera alone, running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_camera.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and looking for the serial number in the log printed to screen under &#34;[INFO][...]Device Serial No:&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Another way to use multiple cameras is running each from a different terminal. Make sure you set a different namespace for each camera using the &#34;camera&#34; argument:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_camera.launch camera:=cam_1 serial_no:=&amp;lt;serial number of the first camera&amp;gt;&#xA;roslaunch realsense2_camera rs_camera.launch camera:=cam_2 serial_no:=&amp;lt;serial number of the second camera&amp;gt;&#xA;...&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using T265&lt;/h2&gt; &#xA;&lt;h3&gt;Start the camera node&lt;/h3&gt; &#xA;&lt;p&gt;To start the camera node in ROS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera rs_t265.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will stream all camera sensors and publish on the appropriate ROS topics.&lt;/p&gt; &#xA;&lt;p&gt;The T265 sets its usb unique ID during initialization and without this parameter it wont be found. Once running it will publish, among others, the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/camera/odom/sample&lt;/li&gt; &#xA; &lt;li&gt;/camera/accel/sample&lt;/li&gt; &#xA; &lt;li&gt;/camera/gyro/sample&lt;/li&gt; &#xA; &lt;li&gt;/camera/fisheye1/image_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/fisheye2/image_raw&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To visualize the pose output and frames in RViz, start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_camera demo_t265.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;About Frame ID&lt;/h3&gt; &#xA;&lt;p&gt;The wrapper publishes static transformations(TFs). The Frame Ids are divided into 3 groups:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ROS convention frames: follow the format of &amp;lt;tf_prefix&amp;gt;_&amp;lt;_stream&amp;gt;&#34;_frame&#34; for example: camera_depth_frame, camera_infra1_frame, etc.&lt;/li&gt; &#xA; &lt;li&gt;Original frame coordinate system: with the suffix of &amp;lt;_optical_frame&amp;gt;. For example: camera_infra1_optical_frame. Check the device documentation for specific coordinate system for each stream.&lt;/li&gt; &#xA; &lt;li&gt;base_link: For example: camera_link. A reference frame for the device. In D400 series and SR300 it is the depth frame. In T265, the pose frame.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;realsense2_description package:&lt;/h3&gt; &#xA;&lt;p&gt;For viewing included models, a separate package is included. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;roslaunch realsense2_description view_d415_model.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unit tests:&lt;/h3&gt; &#xA;&lt;p&gt;Unit-tests are based on bag files saved on S3 server. These can be downloaded using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd catkin_ws&#xA;wget &#34;https://librealsense.intel.com/rs-tests/TestData/outdoors.bag&#34; -P &#34;records/&#34;&#xA;wget &#34;https://librealsense.intel.com/rs-tests/D435i_Depth_and_IMU_Stands_still.bag&#34; -P &#34;records/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, unit-tests can be run using the following command (use either python or python3):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/realsense/realsense2_camera/scripts/rs2_test.py --all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Packages using RealSense ROS Camera&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ROS Object Analytics&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intel/ros_object_analytics&#34;&gt;github&lt;/a&gt; / &lt;a href=&#34;http://wiki.ros.org/IntelROSProject&#34;&gt;ROS Wiki&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This ROS node does not currently support &lt;a href=&#34;http://wiki.ros.org/lunar&#34;&gt;ROS Lunar Loggerhead&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This ROS node currently does not support running multiple T265 cameras at once. This will be addressed in a future update.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2018 Intel Corporation&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this project except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://www.apache.org/licenses/LICENSE-2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;p&gt;*&lt;em&gt;Other names and brands may be claimed as the property of others&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleSpeech</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/PaddlePaddle/PaddleSpeech</id>
    <link href="https://github.com/PaddlePaddle/PaddleSpeech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use Speech Toolkit including SOTA/Streaming ASR with punctuation, influential TTS with text frontend, Speaker Verification System and End-to-End Speech Simultaneous Translation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;|English)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/PaddleSpeech_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;support os&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleSpeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/paddlespeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt; Quick Start &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt; Quick Start Server &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt; Quick Start Streaming Server&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#documents&#34;&gt; Documents &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt; Models List &lt;/a&gt; | &lt;a href=&#34;https://aistudio.baidu.com/aistudio/education/group/info/25130&#34;&gt; AIStudio Courses &lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2205.12007&#34;&gt; Paper &lt;/a&gt; | &lt;a href=&#34;https://gitee.com/paddlepaddle/PaddleSpeech&#34;&gt; Gitee &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleSpeech&lt;/strong&gt; is an open-source toolkit on &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt; platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models.&lt;/p&gt; &#xA;&lt;h5&gt;Speech Recognition&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Recognition Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;I knocked at the door on the ancient side of the building.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;ÊàëËÆ§‰∏∫Ë∑ëÊ≠•ÊúÄÈáçË¶ÅÁöÑÂ∞±ÊòØÁªôÊàëÂ∏¶Êù•‰∫ÜË∫´‰ΩìÂÅ•Â∫∑„ÄÇ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Speech Translation (English to Chinese)&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Translations Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;Êàë Âú® ËøôÊ†ã Âª∫Á≠ë ÁöÑ Âè§ËÄÅ Èó®‰∏ä Êï≤Èó®„ÄÇ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Text-to-Speech&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Input Text&lt;/th&gt; &#xA;    &lt;th&gt;Synthetic Audio&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Life was like a box of chocolates, you never know what you&#39;re gonna get.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Êó©‰∏äÂ•ΩÔºå‰ªäÂ§©ÊòØ2020/10/29ÔºåÊúÄ‰ΩéÊ∏©Â∫¶ÊòØ-3¬∞C„ÄÇ&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Â≠£Âß¨ÂØÇÔºåÈõÜÈ∏°ÔºåÈ∏°Âç≥Ê£òÈ∏°„ÄÇÊ£òÈ∏°È••ÂèΩÔºåÂ≠£Âß¨ÂèäÁÆïÁ®∑ÊµéÈ∏°„ÄÇÈ∏°Êó¢ÊµéÔºåË∑ªÂß¨Á¨àÔºåÂ≠£Âß¨ÂøåÔºåÊÄ•Âí≠È∏°ÔºåÈ∏°ÊÄ•ÔºåÁªßÂúæÂá†ÔºåÂ≠£Âß¨ÊÄ•ÔºåÂç≥Á±çÁÆïÂáªÈ∏°ÔºåÁÆïÁñæÂáªÂá†‰ºéÔºå‰ºéÂç≥ÈΩëÔºåÈ∏°ÂèΩÈõÜÂá†Âü∫ÔºåÂ≠£Âß¨ÊÄ•ÊûÅÂ±êÂáªÈ∏°ÔºåÈ∏°Êó¢ÊÆõÔºåÂ≠£Âß¨ÊøÄÔºåÂç≥ËÆ∞„ÄäÂ≠£Âß¨ÂáªÈ∏°ËÆ∞„Äã„ÄÇ&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For more synthesized audios, please refer to &lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;PaddleSpeech Text-to-Speech samples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Punctuation Restoration&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Input Text &lt;/th&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Output Text &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‰ªäÂ§©ÁöÑÂ§©Ê∞îÁúü‰∏çÈîôÂïä‰Ω†‰∏ãÂçàÊúâÁ©∫ÂêóÊàëÊÉ≥Á∫¶‰Ω†‰∏ÄËµ∑ÂéªÂêÉÈ•≠&lt;/td&gt; &#xA;    &lt;td&gt;‰ªäÂ§©ÁöÑÂ§©Ê∞îÁúü‰∏çÈîôÂïäÔºÅ‰Ω†‰∏ãÂçàÊúâÁ©∫ÂêóÔºüÊàëÊÉ≥Á∫¶‰Ω†‰∏ÄËµ∑ÂéªÂêÉÈ•≠„ÄÇ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Via the easy-to-use, efficient, flexible and scalable implementation, our vision is to empower both industrial application and academic research, including training, inference &amp;amp; testing modules, and deployment process. To be more specific, this toolkit features at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Ease of Use&lt;/strong&gt;: low barriers to install, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt;CLI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt;Server&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt;Streaming Server&lt;/a&gt; is available to quick-start your journey.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Align to the State-of-the-Art&lt;/strong&gt;: we provide high-speed and ultra-lightweight models, and also cutting-edge technology.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Streaming ASR and TTS System&lt;/strong&gt;: we provide production ready streaming asr and streaming tts system.&lt;/li&gt; &#xA; &lt;li&gt;üíØ &lt;strong&gt;Rule-based Chinese frontend&lt;/strong&gt;: our frontend contains Text Normalization and Grapheme-to-Phoneme (G2P, including Polyphone and Tone Sandhi). Moreover, we use self-defined linguistic rules to adapt Chinese context.&lt;/li&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Varieties of Functions that Vitalize both Industrial and Academia&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üõéÔ∏è &lt;em&gt;Implementation of critical audio tasks&lt;/em&gt;: this toolkit contains audio functions like Automatic Speech Recognition, Text-to-Speech Synthesis, Speaker Verfication, KeyWord Spotting, Audio Classification, and Speech Translation, etc.&lt;/li&gt; &#xA;   &lt;li&gt;üî¨ &lt;em&gt;Integration of mainstream models and datasets&lt;/em&gt;: the toolkit implements modules that participate in the whole pipeline of the speech tasks, and uses mainstream datasets like LibriSpeech, LJSpeech, AIShell, CSMSC, etc. See also &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt;model list&lt;/a&gt; for more details.&lt;/li&gt; &#xA;   &lt;li&gt;üß© &lt;em&gt;Cascaded models application&lt;/em&gt;: as an extension of the typical traditional audio tasks, we combine the workflows of the aforementioned tasks with other fields like Natural language processing (NLP) and Computer Vision (CV).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recent Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üëë 2022.05.13: Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/PPASR.md&#34;&gt;PP-ASR&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/PPTTS.md&#34;&gt;PP-TTS&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/vpr/PPVPR.md&#34;&gt;PP-VPR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.05.06: &lt;code&gt;Streaming ASR&lt;/code&gt; with &lt;code&gt;Punctuation Restoration&lt;/code&gt; and &lt;code&gt;Token Timestamp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.05.06: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;, and &lt;code&gt;Punctuation Restoration&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.04.28: &lt;code&gt;Streaming Server&lt;/code&gt; is available for &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.03.28: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.03.28: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ü§ó 2021.12.14: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS&lt;/a&gt; Demos on Hugging Face Spaces are available!&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2021.12.10: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt;, &lt;code&gt;Speech Translation (English to Chinese)&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scan the QR code below with your Wechat, you can access to official technical exchange group and get the bonus ( more than 20GB learning materials, such as papers, codes and videos ) and the live link of the lessons. Look forward to your participation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/23690325/169763015-cbd8e28d-602c-4723-810d-dbc6da49441e.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We strongly recommend our users to install PaddleSpeech in &lt;strong&gt;Linux&lt;/strong&gt; with &lt;em&gt;python&amp;gt;=3.7&lt;/em&gt;. Up to now, &lt;strong&gt;Linux&lt;/strong&gt; supports CLI for the all our tasks, &lt;strong&gt;Mac OSX&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; only supports PaddleSpeech CLI for Audio Classification, Speech-to-Text and Text-to-Speech. To install &lt;code&gt;PaddleSpeech&lt;/code&gt;, please see &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our models with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/cli/README.md&#34;&gt;PaddleSpeech Command Line&lt;/a&gt;. Change &lt;code&gt;--input&lt;/code&gt; to test your own audio/text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech cls --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech vector --task spk --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Speech Recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech asr --lang zh --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Automatic Speech Recognition is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Translation&lt;/strong&gt; (English to Chinese) (not support for Mac and Windows now)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech st --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech tts --input &#34;‰Ω†Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®È£ûÊ°®Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºÅ&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Text to Speech is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Postprocessing&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Punctuation Restoration &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;paddlespeech text --task punc --input ‰ªäÂ§©ÁöÑÂ§©Ê∞îÁúü‰∏çÈîôÂïä‰Ω†‰∏ãÂçàÊúâÁ©∫ÂêóÊàëÊÉ≥Á∫¶‰Ω†‰∏ÄËµ∑ÂéªÂêÉÈ•≠&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Batch Process&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo -e &#34;1 Ê¨¢ËøéÂÖâ‰∏¥„ÄÇ\n2 Ë∞¢Ë∞¢ÊÉ†È°æ„ÄÇ&#34; | paddlespeech tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shell Pipeline&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ASR + Punctuation Restoration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech asr --input ./zh.wav | paddlespeech text --task punc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos&#34;&gt;demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try more functions like training and tuning, please have a look at &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Speech-to-Text Quick Start&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our speech server with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/server/README.md&#34;&gt;PaddleSpeech Server Command Line&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_server start --config_file ./paddlespeech/server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input &#34;ÊÇ®Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®ÁôæÂ∫¶È£ûÊ°®ËØ≠Èü≥ÂêàÊàêÊúçÂä°„ÄÇ&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Audio Classification Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about server command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server&#34;&gt;speech server demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartstreamingserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Streaming Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt; server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Speech Recognition Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Text to Speech Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input &#34;ÊÇ®Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®ÁôæÂ∫¶È£ûÊ°®ËØ≠Èü≥ÂêàÊàêÊúçÂä°„ÄÇ&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information please see: &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;ModelList&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech supports a series of most popular models. They are summarized in &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;released models&lt;/a&gt; and attached with available pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeechToText&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt; contains &lt;em&gt;Acoustic Model&lt;/em&gt;, &lt;em&gt;Language Model&lt;/em&gt;, and &lt;em&gt;Speech Translation&lt;/em&gt;, with the following details:&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Speech-to-Text Module Type&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Speech Recogination&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Aishell&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeech2 RNN + Conv based Models&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr0&#34;&gt;deepspeech2-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr1&#34;&gt;u2.transformer.conformer-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Librispeech&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr0&#34;&gt;deepspeech2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr1&#34;&gt;transformer.conformer.u2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr2&#34;&gt;transformer.conformer.u2-kaldi-librispeech&lt;/a&gt; &lt;/td&gt;  &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TIMIT&lt;/td&gt; &#xA;   &lt;td&gt;Unified Streaming &amp;amp; Non-streaming Two-pass&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/timit/asr1&#34;&gt; u2-timit&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment&lt;/td&gt; &#xA;   &lt;td&gt;THCHS30&lt;/td&gt; &#xA;   &lt;td&gt;MFA&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/.examples/thchs30/align0&#34;&gt;mfa-thchs30&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Language Model&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Ngram Language Model&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ngram_lm&#34;&gt;kenlm&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Speech Translation (English to Chinese)&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;TED En-Zh&lt;/td&gt; &#xA;   &lt;td&gt;Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st0&#34;&gt;transformer-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FAT + Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st1&#34;&gt;fat-st-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;TextToSpeech&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; in PaddleSpeech mainly contains three modules: &lt;em&gt;Text Frontend&lt;/em&gt;, &lt;em&gt;Acoustic Model&lt;/em&gt; and &lt;em&gt;Vocoder&lt;/em&gt;. Acoustic Model and Vocoder models are listed as follow:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Text-to-Speech Module Type &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Text Frontend &lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt; ‚ÄÉ &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/tn&#34;&gt;tn&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/g2p&#34;&gt;g2p&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Acoustic Model&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts0&#34;&gt;tacotron2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts0&#34;&gt;tacotron2-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer TTS&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts1&#34;&gt;transformer-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeedySpeech&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts2&#34;&gt;speedyspeech-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts3&#34;&gt;fastspeech2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/tts3&#34;&gt;fastspeech2-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts3&#34;&gt;fastspeech2-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/tts3&#34;&gt;fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;6&#34;&gt;Vocoder&lt;/td&gt; &#xA;   &lt;td&gt;WaveFlow&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc0&#34;&gt;waveflow-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parallel WaveGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc1&#34;&gt;PWGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc1&#34;&gt;PWGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc1&#34;&gt;PWGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc1&#34;&gt;PWGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi Band MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc3&#34;&gt;Multi Band MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Style MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc4&#34;&gt;Style MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiFiGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc5&#34;&gt;HiFiGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc5&#34;&gt;HiFiGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc5&#34;&gt;HiFiGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc5&#34;&gt;HiFiGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WaveRNN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc6&#34;&gt;WaveRNN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Voice Cloning&lt;/td&gt; &#xA;   &lt;td&gt;GE2E&lt;/td&gt; &#xA;   &lt;td&gt;Librispeech, etc.&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ge2e&#34;&gt;ge2e&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc0&#34;&gt;ge2e-tacotron2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc1&#34;&gt;ge2e-fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;AudioClassification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio Classification&lt;/td&gt; &#xA;   &lt;td&gt;ESC-50&lt;/td&gt; &#xA;   &lt;td&gt;PANN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/esc50/cls0&#34;&gt;pann-esc50&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeakerVerification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;VoxCeleb12&lt;/td&gt; &#xA;   &lt;td&gt;ECAPA-TDNN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/voxceleb/sv0&#34;&gt;ecapa-tdnn-voxceleb12&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;PunctuationRestoration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Punctuation Restoration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Punctuation Restoration&lt;/td&gt; &#xA;   &lt;td&gt;IWLST2012_zh&lt;/td&gt; &#xA;   &lt;td&gt;Ernie Linear&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/iwslt2012/punc0&#34;&gt;iwslt2012-punc0&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;p&gt;Normally, &lt;a href=&#34;https://paperswithcode.com/area/speech&#34;&gt;Speech SoTA&lt;/a&gt;, &lt;a href=&#34;https://paperswithcode.com/area/audio&#34;&gt;Audio SoTA&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/area/music&#34;&gt;Music SoTA&lt;/a&gt; give you an overview of the hot academic topics in the related area. To focus on the tasks in PaddleSpeech, you will find the following guidelines are helpful to grasp the core ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/README.md&#34;&gt;Some Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Automatic Speech Recognition&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/data_preparation.md&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/ngram_lm.md&#34;&gt;Ngram LM&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/advanced_usage.md&#34;&gt;Advanced Usage&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/zh_text_frontend.md&#34;&gt;Chinese Rule Based Text Frontend&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;Test Audio Samples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Speaker Verification &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_searching/README.md&#34;&gt;Audio Searching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speaker_verification/README.md&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_tagging/README.md&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_translation/README.md&#34;&gt;Speech Translation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_server/README.md&#34;&gt;Speech Server&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;Released Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeechToText&#34;&gt;Speech-to-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#TextToSpeech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#AudioClassification&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeakerVerification&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#PunctuationRestoration&#34;&gt;Punctuation Restoration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#contribution&#34;&gt;Welcome to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Text-to-Speech module is originally called &lt;a href=&#34;https://github.com/PaddlePaddle/Parakeet&#34;&gt;Parakeet&lt;/a&gt;, and now merged with this repository. If you are interested in academic research about this task, please see &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview&#34;&gt;TTS research overview&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/raw/develop/docs/source/tts/models_introduction.md&#34;&gt;this document&lt;/a&gt; is a good guideline for the pipeline components.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt;: Use PaddleSpeech TTS to generate virtual human voice.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web&#34;&gt;&lt;img src=&#34;https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/demo_video.html&#34;&gt;PaddleSpeech Demo Video&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt;: Use PaddleSpeech TTS and ASR to clone voice from videos.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jerryuhoo/VTuberTalk/main/gui/gui.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;To cite PaddleSpeech for research, please use the following format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{zhang2022paddlespeech,&#xA;    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},&#xA;    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},&#xA;    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},&#xA;    year = {2022},&#xA;    publisher = {Association for Computational Linguistics},&#xA;}&#xA;&#xA;@inproceedings{zheng2021fused,&#xA;  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},&#xA;  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},&#xA;  booktitle={International Conference on Machine Learning},&#xA;  pages={12736--12746},&#xA;  year={2021},&#xA;  organization={PMLR}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;contribution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to PaddleSpeech&lt;/h2&gt; &#xA;&lt;p&gt;You are warmly welcome to submit questions in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/discussions&#34;&gt;discussions&lt;/a&gt; and bug reports in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;issues&lt;/a&gt;! Also, we highly appreciate if you are willing to contribute to this project!&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zh794390558&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3038472?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackwaterveg&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/87408988?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yt605155624&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24568452?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kuke&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3064195?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinghai-sun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7038341?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pkuyym&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5782283?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KPatr1ck&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22954146?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LittleChenCc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10339970?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/745165806&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/20623194?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Mingxue-Xu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/92848346?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chrisxu2016&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18379485?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfchener&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6771821?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luotao1&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6836917?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wanghaoshuang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7534971?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gongel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24390500?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mmglove&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/38800877?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/iclementine&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/16222986?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ZeyuChen&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1371212?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81195143?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qingqing01&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7845005?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ericxk&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/4719594?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kvinwang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6442159?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jiqiren11&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/82639260?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AshishKarel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/58069375?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chesterkuo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6285069?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tensor-tang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/21351065?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hysunflower&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/52739577?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wwhu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6081200?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lispc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/2833376?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24245709?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harisankarh&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1307053?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackiexiao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18050469?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/limpidezza&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/71760778?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/yeyupiaoling&#34;&gt;yeyupiaoling&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PPASR&#34;&gt;PPASR&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech&#34;&gt;PaddlePaddle-DeepSpeech&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle&#34;&gt;VoiceprintRecognition-PaddlePaddle&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle&#34;&gt;AudioClassification-PaddlePaddle&lt;/a&gt; for years of attention, constructive advice and great help.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/mymagicpower&#34;&gt;mymagicpower&lt;/a&gt; for the Java implementation of ASR upon &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk&#34;&gt;short&lt;/a&gt; and &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk&#34;&gt;long&lt;/a&gt; audio files.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/JiehangXie&#34;&gt;JiehangXie&lt;/a&gt;/&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt; for developing Virtual Uploader(VUP)/Virtual YouTuber(VTuber) with PaddleSpeech TTS function.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;745165806&lt;/a&gt;/&lt;a href=&#34;https://github.com/745165806/PaddleSpeechTask&#34;&gt;PaddleSpeechTask&lt;/a&gt; for contributing Punctuation Restoration model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;kslz&lt;/a&gt; for supplementary Chinese documents.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/awmmmm&#34;&gt;awmmmm&lt;/a&gt; for contributing fastspeech2 aishell3 conformer pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/phecda-xu&#34;&gt;phecda-xu&lt;/a&gt;/&lt;a href=&#34;https://github.com/phecda-xu/PaddleDubbing&#34;&gt;PaddleDubbing&lt;/a&gt; for developing a dubbing tool with GUI based on PaddleSpeech TTS model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;jerryuhoo&lt;/a&gt;/&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt; for developing a GUI tool based on PaddleSpeech TTS and code for making datasets from videos based on PaddleSpeech ASR.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Besides, PaddleSpeech depends on a lot of open source repositories. See &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/reference.md&#34;&gt;references&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ApolloAuto/apollo</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/ApolloAuto/apollo</id>
    <link href="https://github.com/ApolloAuto/apollo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open autonomous driving platform&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://180.76.142.62:8111/viewType.html?buildTypeId=Apollo_Build&amp;amp;guest=1&#34;&gt;&lt;img src=&#34;http://180.76.142.62:8111/app/rest/builds/buildType:Apollo_Build/statusIcon&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://azure.apollo.auto/daily-build/public&#34;&gt;&lt;img src=&#34;https://azure.apollo.auto/dailybuildstatus.svg?sanitize=true&#34; alt=&#34;Simulation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;We choose to go to the moon in this decade and do the other things,&#xA;&#xA;not because they are easy, but because they are hard.&#xA;&#xA;-- John F. Kennedy, 1962&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Welcome to Apollo&#39;s GitHub page!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://apollo.auto&#34;&gt;Apollo&lt;/a&gt; is a high performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles.&lt;/p&gt; &#xA;&lt;p&gt;For business and partnership, please visit &lt;a href=&#34;http://apollo.auto&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#individual-versions&#34;&gt;Individual Versions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#quick-starts&#34;&gt;Quick Starts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/#documents&#34;&gt;Documents&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Apollo is loaded with new modules and features but needs to be calibrated and configured perfectly before you take it for a spin. Please review the prerequisites and installation steps in detail to ensure that you are well equipped to build and launch Apollo. You could also check out Apollo&#39;s architecture overview for a greater understanding of Apollo&#39;s core technology and platforms.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[New 2021-01]&lt;/strong&gt; The Apollo platform (stable version) is now upgraded with software packages and library dependencies of newer versions including:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;CUDA upgraded to version 11.1 to support Nvidia Ampere (30x0 series) GPUs, with NVIDIA driver &amp;gt;= 455.32&lt;/li&gt; &#xA; &lt;li&gt;LibTorch (both CPU and GPU version) bumped to version 1.7.0 accordingly.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We do not expect a disruption to your current work, but to ease your life of migratation, you would need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Update NVIDIA driver on your host to version &amp;gt;= 455.32. (&lt;a href=&#34;https://www.nvidia.com/Download/index.aspx?lang=en-us&#34;&gt;Web link&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Pull latest code and run the following commands after restarting and logging into Apollo Development container:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Remove Bazel output of previous builds&#xA;rm -rf /apollo/.cache/{bazel,build,repos}&#xA;# Re-configure bazelrc.&#xA;./apollo.sh config --noninteractive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The vehicle equipped with the by-wire system, including but not limited to brake-by-wire, steering-by-wire, throttle-by-wire and shift-by-wire (Apollo is currently tested on Lincoln MKZ)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A machine with a 8-core processor and 16GB memory minimum&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;NVIDIA Turing GPU is strongly recommended&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ubuntu 18.04&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;NVIDIA driver version 455.32.00 and above (&lt;a href=&#34;https://www.nvidia.com/Download/index.aspx?lang=en-us&#34;&gt;Web link&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker-CE version 19.03 and above (&lt;a href=&#34;https://docs.docker.com/engine/install/ubuntu/&#34;&gt;Official doc&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;NVIDIA Container Toolkit (&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;Official doc&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;, it is recommended that you install the versions of Apollo in the following order: &lt;strong&gt;1.0 -&amp;gt; whichever version you would like to test out&lt;/strong&gt;. The reason behind this recommendation is that you need to confirm whether individual hardware components and modules are functioning correctly, and clear various version test cases before progressing to a higher and more capable version for your safety and the safety of those around you.&lt;/p&gt; &#xA;&lt;h2&gt;Individual Versions:&lt;/h2&gt; &#xA;&lt;p&gt;The following diagram highlights the scope and features of each Apollo release:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_Roadmap_6_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_1_0_hardware_system_installation_guide.md&#34;&gt;&lt;strong&gt;Apollo 1.0:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 1.0, also referred to as the Automatic GPS Waypoint Following, works in an enclosed venue such as a test track or parking lot. This installation is necessary to ensure that Apollo works perfectly with your vehicle. The diagram below lists the various modules in Apollo 1.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_1_5_hardware_system_installation_guide.md&#34;&gt;&lt;strong&gt;Apollo 1.5:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 1.5 is meant for fixed lane cruising. With the addition of LiDAR, vehicles with this version now have better perception of its surroundings and can better map its current position and plan its trajectory for safer maneuvering on its lane. Please note, the modules highlighted in Yellow are additions or upgrades for version 1.5.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_1_5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_2_0_hardware_system_installation_guide_v1.md#key-hardware-components&#34;&gt;&lt;strong&gt;Apollo 2.0:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 2.0 supports vehicles autonomously driving on simple urban roads. Vehicles are able to cruise on roads safely, avoid collisions with obstacles, stop at traffic lights, and change lanes if needed to reach their destination. Please note, the modules highlighted in Red are additions or upgrades for version 2.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_2_5_hardware_system_installation_guide_v1.md&#34;&gt;&lt;strong&gt;Apollo 2.5:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 2.5 allows the vehicle to autonomously run on geo-fenced highways with a camera for obstacle detection. Vehicles are able to maintain lane control, cruise and avoid collisions with vehicles ahead of them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Please note, if you need to test Apollo 2.5; for safety purposes, please seek the help of the&#xA;Apollo Engineering team. Your safety is our #1 priority,&#xA;and we want to ensure Apollo 2.5 was integrated correctly with your vehicle before you hit the road.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_2_5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_3_0_quick_start.md&#34;&gt;&lt;strong&gt;Apollo 3.0:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 3.0&#39;s primary focus is to provide a platform for developers to build upon in a closed venue low-speed environment. Vehicles are able to maintain lane control, cruise and avoid collisions with vehicles ahead of them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_3.0_diagram.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_3_5_quick_start.md&#34;&gt;&lt;strong&gt;Apollo 3.5:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 3.5 is capable of navigating through complex driving scenarios such as residential and downtown areas. The car now has 360-degree visibility, along with upgraded perception algorithms to handle the changing conditions of urban roads, making the car more secure and aware. Scenario-based planning can navigate through complex scenarios, including unprotected turns and narrow streets often found in residential areas and roads with stop signs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_3_5_Architecture.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_3_5_quick_start.md&#34;&gt;&lt;strong&gt;Apollo 5.0:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 5.0 is an effort to support volume production for Geo-Fenced Autonomous Driving. The car now has 360-degree visibility, along with upgraded perception deep learning model to handle the changing conditions of complex road scenarios, making the car more secure and aware. Scenario-based planning has been enhanced to support additional scenarios like pull over and crossing bare intersections.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_5_0_diagram1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_5_5_quick_start.md&#34;&gt;&lt;strong&gt;Apollo 5.5:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 5.5 enhances the complex urban road autonomous driving capabilities of previous Apollo releases, by introducing curb-to-curb driving support. With this new addition, Apollo is now a leap closer to fully autonomous urban road driving. The car has complete 360-degree visibility, along with upgraded perception deep learning model and a brand new prediction model to handle the changing conditions of complex road and junction scenarios, making the car more secure and aware.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_5_5_Architecture.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_6_0_quick_start.md&#34;&gt;&lt;strong&gt;Apollo 6.0:&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 6.0 incorporates new deep learning models to enhance the capabilities for certain Apollo modules. This version works seamlessly with new additions of data pipeline services to better serve Apollo developers. Apollo 6.0 is also the first version to integrate certain features as a demonstration of our continuous exploration and experimentation efforts towards driverless technology.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_6_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Apollo 7.0:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo 7.0 incorporates 3 brand new deep learning models to enhance the capabilities for Apollo Perception and Prediction modules. Apollo Studio is introduced in this version, combining with Data Pipeline, to provide a one-stop online development platform to better serve Apollo developers. Apollo 7.0 also publishes the PnC reinforcement learning model training and simulation evaluation service based on previous simulation service.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_7_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware/ Vehicle Overview&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Hardware_overview_3_5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware Connection Overview&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Hardware_connection_3_5_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Software Overview&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/demo_guide/images/Apollo_3_5_software_architecture.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_3_5_hardware_system_installation_guide.md&#34;&gt;Hardware installation guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_software_installation_guide.md&#34;&gt;Software installation guide&lt;/a&gt; - &lt;strong&gt;This step is required&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/howto/how_to_launch_and_run_apollo.md&#34;&gt;Launch and run Apollo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Congratulations! You have successfully built out Apollo without Hardware. If you do have a vehicle and hardware setup for a particular version, please pick the Quickstart guide most relevant to your setup:&lt;/p&gt; &#xA;&lt;h2&gt;Quick Starts:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_6_0_quick_start.md&#34;&gt;Apollo 6.0 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_5_5_quick_start.md&#34;&gt;Apollo 5.5 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_5_0_quick_start.md&#34;&gt;Apollo 5.0 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_3_5_quick_start.md&#34;&gt;Apollo 3.5 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_3_0_quick_start.md&#34;&gt;Apollo 3.0 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_2_5_quick_start.md&#34;&gt;Apollo 2.5 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_2_0_quick_start.md&#34;&gt;Apollo 2.0 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_1_5_quick_start.md&#34;&gt;Apollo 1.5 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/quickstart/apollo_1_0_quick_start.md&#34;&gt;Apollo 1.0 QuickStart Guide&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/technical_tutorial/README.md&#34;&gt;Technical Tutorials&lt;/a&gt;: Everything you need to know about Apollo. Written as individual versions with links to every document related to that version.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/howto/README.md&#34;&gt;How-To Guides&lt;/a&gt;: Brief technical solutions to common problems that developers face during the installation and use of the Apollo platform&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/specs/README.md&#34;&gt;Specs&lt;/a&gt;: A Deep dive into Apollo&#39;s Hardware and Software specifications (only recommended for expert level developers that have successfully installed and launched Apollo)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ApolloAuto/apollo/master/docs/FAQs/README.md&#34;&gt;FAQs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions&lt;/h2&gt; &#xA;&lt;p&gt;You are welcome to submit questions and bug reports as &lt;a href=&#34;https://github.com/ApolloAuto/apollo/issues&#34;&gt;GitHub Issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and License&lt;/h2&gt; &#xA;&lt;p&gt;Apollo is provided under the &lt;a href=&#34;https://github.com/ApolloAuto/apollo/raw/master/LICENSE&#34;&gt;Apache-2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Apollo open source platform only has the source code for models, algorithms and processes, which will be integrated with cybersecurity defense strategy in the deployment for commercialization and productization.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the Disclaimer of Apollo in &lt;a href=&#34;http://apollo.auto/docs/disclaimer.html&#34;&gt;Apollo&#39;s official website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Connect with us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ApolloAuto/apollo/issues&#34;&gt;Have suggestions for our GitHub page?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/apolloplatform&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC8wR_NX_NShUTSSqIaEUY9Q&#34;&gt;YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.medium.com/apollo-auto&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://eepurl.com/c-mLSz&#34;&gt;Newsletter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Interested in our turnKey solutions or partnering with us Mail us at: &lt;a href=&#34;mailto:apollopartner@baidu.com&#34;&gt;apollopartner@baidu.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>google/leveldb</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/google/leveldb</id>
    <link href="https://github.com/google/leveldb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;ci&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Authors: Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keys and values are arbitrary byte arrays.&lt;/li&gt; &#xA; &lt;li&gt;Data is stored sorted by key.&lt;/li&gt; &#xA; &lt;li&gt;Callers can provide a custom comparison function to override the sort order.&lt;/li&gt; &#xA; &lt;li&gt;The basic operations are &lt;code&gt;Put(key,value)&lt;/code&gt;, &lt;code&gt;Get(key)&lt;/code&gt;, &lt;code&gt;Delete(key)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiple changes can be made in one atomic batch.&lt;/li&gt; &#xA; &lt;li&gt;Users can create a transient snapshot to get a consistent view of data.&lt;/li&gt; &#xA; &lt;li&gt;Forward and backward iteration is supported over the data.&lt;/li&gt; &#xA; &lt;li&gt;Data is automatically compressed using the &lt;a href=&#34;https://google.github.io/snappy/&#34;&gt;Snappy compression library&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/raw/main/doc/index.md&#34;&gt;LevelDB library documentation&lt;/a&gt; is online and bundled with the source code.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is not a SQL database. It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.&lt;/li&gt; &#xA; &lt;li&gt;Only a single process (possibly multi-threaded) can access a particular database at a time.&lt;/li&gt; &#xA; &lt;li&gt;There is no client-server support builtin to the library. An application that needs such support will have to wrap their own server around the library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting the Source&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/google/leveldb.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;This project supports &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; out of the box.&lt;/p&gt; &#xA;&lt;h3&gt;Build for POSIX&lt;/h3&gt; &#xA;&lt;p&gt;Quick start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake -DCMAKE_BUILD_TYPE=Release .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building for Windows&lt;/h3&gt; &#xA;&lt;p&gt;First generate the Visual Studio 2017 project/solution files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -G &#34;Visual Studio 15&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default default will build for x86. For 64-bit run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmake -G &#34;Visual Studio 15 Win64&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile the Windows solution from the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;devenv /build Debug leveldb.sln&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or open leveldb.sln in Visual Studio and build from within.&lt;/p&gt; &#xA;&lt;p&gt;Please see the CMake documentation and &lt;code&gt;CMakeLists.txt&lt;/code&gt; for more advanced usage.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing to the leveldb Project&lt;/h1&gt; &#xA;&lt;p&gt;The leveldb project welcomes contributions. leveldb&#39;s primary goal is to be a reliable and fast key/value store. Changes that are in line with the features/limitations outlined above, and meet the requirements below, will be considered.&lt;/p&gt; &#xA;&lt;p&gt;Contribution requirements:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tested platforms only&lt;/strong&gt;. We &lt;em&gt;generally&lt;/em&gt; will only accept changes for platforms that are compiled and tested. This means POSIX (for Linux and macOS) or Windows. Very small changes will sometimes be accepted, but consider that more of an exception than the rule.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stable API&lt;/strong&gt;. We strive very hard to maintain a stable API. Changes that require changes for projects using leveldb &lt;em&gt;might&lt;/em&gt; be rejected without sufficient benefit to the project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tests&lt;/strong&gt;: All changes must be accompanied by a new (or changed) test, or a sufficient explanation as to why a new (or changed) test is not required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Style&lt;/strong&gt;: This project conforms to the &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google C++ Style Guide&lt;/a&gt;. To ensure your changes are properly formatted please run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clang-format -i --style=file &amp;lt;file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We are unlikely to accept contributions to the build configuration files, such as &lt;code&gt;CMakeLists.txt&lt;/code&gt;. We are focused on maintaining a build configuration that allows us to test that the project works in a few supported configurations inside Google. We are not currently interested in supporting other requirements, such as different operating systems, compilers, or build systems.&lt;/p&gt; &#xA;&lt;h2&gt;Submitting a Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;Before any pull request will be accepted the author must first sign a Contributor License Agreement (CLA) at &lt;a href=&#34;https://cla.developers.google.com/&#34;&gt;https://cla.developers.google.com/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to keep the commit timeline linear &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits&#34;&gt;squash&lt;/a&gt; your changes down to a single commit and &lt;a href=&#34;https://git-scm.com/docs/git-rebase&#34;&gt;rebase&lt;/a&gt; on google/leveldb/main. This keeps the commit timeline linear and more easily sync&#39;ed with the internal repository at Google. More information at GitHub&#39;s &lt;a href=&#34;https://help.github.com/articles/about-git-rebase/&#34;&gt;About Git rebase&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Here is a performance report (with explanations) from the run of the included db_bench program. The results are somewhat noisy, but should be enough to get a ballpark performance estimate.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use a database with a million entries. Each entry has a 16 byte key, and a 100 byte value. Values used by the benchmark compress to about half their original size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LevelDB:    version 1.1&#xA;Date:       Sun May  1 12:11:26 2011&#xA;CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz&#xA;CPUCache:   4096 KB&#xA;Keys:       16 bytes each&#xA;Values:     100 bytes each (50 bytes after compression)&#xA;Entries:    1000000&#xA;Raw Size:   110.6 MB (estimated)&#xA;File Size:  62.9 MB (estimated)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Write performance&lt;/h2&gt; &#xA;&lt;p&gt;The &#34;fill&#34; benchmarks create a brand new database, in either sequential, or random order. The &#34;fillsync&#34; benchmark flushes data from the operating system to the disk after every operation; the other write operations leave the data sitting in the operating system buffer cache for a while. The &#34;overwrite&#34; benchmark does random writes that update existing keys in the database.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fillseq      :       1.765 micros/op;   62.7 MB/s&#xA;fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)&#xA;fillrandom   :       2.460 micros/op;   45.0 MB/s&#xA;overwrite    :       2.380 micros/op;   46.5 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each &#34;op&#34; above corresponds to a write of a single key/value pair. I.e., a random write benchmark goes at approximately 400,000 writes per second.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;fillsync&#34; operation costs much less (0.3 millisecond) than a disk seek (typically 10 milliseconds). We suspect that this is because the hard disk itself is buffering the update in its memory and responding before the data has been written to the platter. This may or may not be safe based on whether or not the hard disk has enough power to save its memory in the event of a power failure.&lt;/p&gt; &#xA;&lt;h2&gt;Read performance&lt;/h2&gt; &#xA;&lt;p&gt;We list the performance of reading sequentially in both the forward and reverse direction, and also the performance of a random lookup. Note that the database created by the benchmark is quite small. Therefore the report characterizes the performance of leveldb when the working set fits in memory. The cost of reading a piece of data that is not present in the operating system buffer cache will be dominated by the one or two disk seeks needed to fetch the data from disk. Write performance will be mostly unaffected by whether or not the working set fits in memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)&#xA;readseq     :  0.476 micros/op;  232.3 MB/s&#xA;readreverse :  0.724 micros/op;  152.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LevelDB compacts its underlying storage data in the background to improve read performance. The results listed above were done immediately after a lot of random writes. The results after compactions (which are usually triggered automatically) are better.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)&#xA;readseq     :  0.423 micros/op;  261.8 MB/s&#xA;readreverse :  0.663 micros/op;  166.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the high cost of reads comes from repeated decompression of blocks read from disk. If we supply enough cache to the leveldb so it can hold the uncompressed blocks in memory, the read performance improves again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)&#xA;readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository contents&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/index.md&#34;&gt;doc/index.md&lt;/a&gt; for more explanation. See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/impl.md&#34;&gt;doc/impl.md&lt;/a&gt; for a brief overview of the implementation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in include/leveldb/*.h. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Guide to header files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/db.h&lt;/strong&gt;: Main interface to the DB: Start here.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/options.h&lt;/strong&gt;: Control over the behavior of an entire database, and also control over the behavior of individual reads and writes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/comparator.h&lt;/strong&gt;: Abstraction for user-specified comparison function. If you want just bytewise comparison of keys, you can use the default comparator, but clients can write their own comparator implementations if they want custom ordering (e.g. to handle different character encodings, etc.).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/iterator.h&lt;/strong&gt;: Interface for iterating over data. You can get an iterator from a DB object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/write_batch.h&lt;/strong&gt;: Interface for atomically applying multiple updates to a database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/slice.h&lt;/strong&gt;: A simple module for maintaining a pointer and a length into some other byte array.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/status.h&lt;/strong&gt;: Status is returned from many of the public interfaces and is used to report success and various kinds of errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/env.h&lt;/strong&gt;: Abstraction of the OS environment. A posix implementation of this interface is in util/env_posix.cc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/table.h, include/leveldb/table_builder.h&lt;/strong&gt;: Lower-level modules that most clients probably won&#39;t use directly.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>alibaba/MNN</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/alibaba/MNN</id>
    <link href="https://github.com/alibaba/MNN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/banner.png&#34; alt=&#34;MNN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/README_CN.md&#34;&gt;‰∏≠ÊñáÁâàÊú¨&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.mnn.zone&#34;&gt;MNN Homepage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;MNN is a highly efficient and lightweight deep learning framework. It supports inference and training of deep learning models, and has industry leading performance for inference and training on-device. At present, MNN has been integrated in more than 20 apps of Alibaba Inc, such as Taobao, Tmall, Youku, Dingtalk, Xianyu and etc., covering more than 70 usage scenarios such as live broadcast, short video capture, search recommendation, product searching by image, interactive marketing, equity distribution, security risk control. In addition, MNN is also used on embedded devices, such as IoT.&lt;/p&gt; &#xA;&lt;p&gt;The design principles and performance data of MNN has been published in an MLSys 2020 paper &lt;a href=&#34;https://arxiv.org/pdf/2002.12418.pdf&#34;&gt;here&lt;/a&gt;. Please cite MNN in your publications if it helps your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{alibaba2020mnn,&#xA;  author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},&#xA;  title = {MNN: A Universal and Efficient Inference Engine},&#xA;  booktitle = {MLSys},&#xA;  year = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation and Tools&lt;/h2&gt; &#xA;&lt;p&gt;MNN&#39;s docs are in placed in &lt;a href=&#34;https://www.yuque.com/mnn/en&#34;&gt;Yuque docs here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MNN Workbench could be downloaded from &lt;a href=&#34;http://www.mnn.zone&#34;&gt;MNN&#39;s homepage&lt;/a&gt;, which provides pretrained models, visualized training tools, and one-click deployment of models to devices.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;High performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implements core computing with lots of optimized assembly code to make full use of the ARM CPU.&lt;/li&gt; &#xA; &lt;li&gt;For iOS, GPU acceleration (Metal) can be turned on, which is faster than Apple&#39;s native CoreML.&lt;/li&gt; &#xA; &lt;li&gt;For Android, &lt;code&gt;OpenCL&lt;/code&gt;, &lt;code&gt;Vulkan&lt;/code&gt;, and &lt;code&gt;OpenGL&lt;/code&gt; are available and deep tuned for mainstream GPUs (&lt;code&gt;Adreno&lt;/code&gt; and &lt;code&gt;Mali&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Convolution and transposition convolution algorithms are efficient and stable. The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3 -&amp;gt; 7x7.&lt;/li&gt; &#xA; &lt;li&gt;Twice speed increase for the new architecture ARM v8.2 with FP16 half-precision calculation support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Lightweight&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimized for devices, no dependencies, can be easily deployed to mobile devices and a variety of embedded devices.&lt;/li&gt; &#xA; &lt;li&gt;iOS platform: static library size for armv7+arm64 platforms is about 5MB, size increase of linked executables is about 620KB, and metallib file is about 600KB.&lt;/li&gt; &#xA; &lt;li&gt;Android platform: core so size is about 400KB, OpenCL so is about 400KB, Vulkan so is about 400KB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Versatility&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports &lt;code&gt;Tensorflow&lt;/code&gt;, &lt;code&gt;Caffe&lt;/code&gt;, &lt;code&gt;ONNX&lt;/code&gt;, and supports common neural networks such as &lt;code&gt;CNN&lt;/code&gt;, &lt;code&gt;RNN&lt;/code&gt;, &lt;code&gt;GAN&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MNN model converter supports 149 &lt;code&gt;Tensorflow&lt;/code&gt; OPs, 58 &lt;code&gt;TFLite&lt;/code&gt; OPs, 47 &lt;code&gt;Caffe&lt;/code&gt; OPs and 74 &lt;code&gt;ONNX&lt;/code&gt; OPs; Number of OPs by different MNN hardware backends: 111 for CPU, 6 for ARM V8.2, 55 for Metal, 43 for OpenCL, and 32 for Vulkan.&lt;/li&gt; &#xA; &lt;li&gt;Supports iOS 8.0+, Android 4.3+ and embedded devices with POSIX interface.&lt;/li&gt; &#xA; &lt;li&gt;Supports hybrid computing on multiple devices. Currently supports CPU and GPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ease of use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Efficient image processing module, speeding up affine transform and color space transform without libyuv or opencv.&lt;/li&gt; &#xA; &lt;li&gt;Provides callbacks throughout the workflow to extract data or control the execution precisely.&lt;/li&gt; &#xA; &lt;li&gt;Provides options for selecting inference branch and paralleling branches on CPU and GPU.&lt;/li&gt; &#xA; &lt;li&gt;(BETA) MNN Python API helps ML engineers to easily use MNN to build a model, train it and quantize it, without dipping their toes in C++ code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;MNN can be divided into two parts: Converter and Interpreter.&lt;/p&gt; &#xA;&lt;p&gt;Converter consists of Frontends and Graph Optimize. The former is responsible for supporting different training frameworks. MNN currently supports Tensorflow, Tensorflow Lite, Caffe and ONNX (PyTorch/MXNet); the latter optimizes graphs by operator fusion, operator substitution, and layout adjustment.&lt;/p&gt; &#xA;&lt;p&gt;Interpreter consists of Engine and Backends. The former is responsible for the loading of the model and the scheduling of the calculation graph; the latter includes the memory allocation and the Op implementation under each computing device. In Engine and Backends, MNN applies a variety of optimization schemes, including applying Winograd algorithm in convolution and deconvolution, applying Strassen algorithm in matrix multiplication, low-precision calculation, Neon optimization, hand-written assembly, multi-thread optimization, memory reuse, heterogeneous computing, etc.&lt;/p&gt; &#xA;&lt;h2&gt;How to Discuss and Get Help From MNN Community&lt;/h2&gt; &#xA;&lt;p&gt;The group discussions are predominantly Chinese. But we welcome and will help English speakers.&lt;/p&gt; &#xA;&lt;p&gt;Dingtalk discussion groups:&lt;/p&gt; &#xA;&lt;p&gt;Group #1 (Full): 23329087&lt;/p&gt; &#xA;&lt;p&gt;Group #2 (Full): 23350225&lt;/p&gt; &#xA;&lt;p&gt;Group #3: &lt;a href=&#34;https://h5.dingtalk.com/circle/healthCheckin.html?dtaction=os&amp;amp;corpId=ding2c1d5c85a81030b9a483726330e8af54&amp;amp;574b2bb2-c53a-4=497bad6b-25a5-4&amp;amp;cbdbhh=qwertyuiop&#34;&gt;https://h5.dingtalk.com/circle/healthCheckin.html?dtaction=os&amp;amp;corpId=ding2c1d5c85a81030b9a483726330e8af54&amp;amp;574b2bb2-c53a-4=497bad6b-25a5-4&amp;amp;cbdbhh=qwertyuiop&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache 2.0&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MNN participants: Taobao Technology Department, Search Engineering Team, DAMO Team, Youku and other Alibaba Group employees.&lt;/p&gt; &#xA;&lt;p&gt;MNN refers to the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BVLC/caffe&#34;&gt;Caffe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/flatbuffers&#34;&gt;flatbuffer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/gemmlowp&#34;&gt;gemmlowp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.github.com/googlesamples/android-vulkan-tutorials&#34;&gt;Google Vulkan demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halide/Halide&#34;&gt;Halide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XiaoMi/mace&#34;&gt;Mace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/onnx&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf&#34;&gt;protobuffer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/skia&#34;&gt;skia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;ncnn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/paddle-mobile&#34;&gt;paddle-mobile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nothings/stb&#34;&gt;stb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/rapidjson&#34;&gt;rapidjson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pybind/pybind11&#34;&gt;pybind11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huawei-noah/bolt&#34;&gt;bolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chromium.googlesource.com/libyuv/libyuv&#34;&gt;libyuv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/libjpeg-turbo/libjpeg-turbo&#34;&gt;libjpeg&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Neargye/magic_enum</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/Neargye/magic_enum</id>
    <link href="https://github.com/Neargye/magic_enum" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Static reflection for enums (to string, from string, iteration) for modern C++, work with any enum type without any macro or boilerplate code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://bit.ly/3OMysM8&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg?sanitize=true&#34; alt=&#34;Stand With Ukraine&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt; __  __             _        ______                          _____&#xA;|  \/  |           (_)      |  ____|                        / ____|_     _&#xA;| \  / | __ _  __ _ _  ___  | |__   _ __  _   _ _ __ ___   | |   _| |_ _| |_&#xA;| |\/| |/ _` |/ _` | |/ __| |  __| | &#39;_ \| | | | &#39;_ ` _ \  | |  |_   _|_   _|&#xA;| |  | | (_| | (_| | | (__  | |____| | | | |_| | | | | | | | |____|_|   |_|&#xA;|_|  |_|\__,_|\__, |_|\___| |______|_| |_|\__,_|_| |_| |_|  \_____|&#xA;               __/ |&#xA;              |___/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Neargye/magic_enum/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/Neargye/magic_enum.svg?sanitize=true&#34; alt=&#34;Github releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://conan.io/center/magic_enum&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Conan-package-blueviolet&#34; alt=&#34;Conan package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/vcpkg/tree/master/ports/magic-enum&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Vcpkg-package-blueviolet&#34; alt=&#34;Vcpkg package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.cppget.org/magic_enum?q=magic_enum&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Build2-package-blueviolet&#34; alt=&#34;Build2 package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/Neargye/magic_enum.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://wandbox.org/permlink/JPMZqT9mgaUdooyC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try-online-blue.svg?sanitize=true&#34; alt=&#34;Try online&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://godbolt.org/z/BxfmsH&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/compiler_explorer-online-blue.svg?sanitize=true&#34; alt=&#34;Compiler explorer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Magic Enum C++&lt;/h1&gt; &#xA;&lt;p&gt;Header-only C++17 library provides static reflection for enums, work with any enum type without any macro or boilerplate code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_cast&lt;/code&gt; obtains enum value from string or integer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_value&lt;/code&gt; returns enum value at specified index.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_values&lt;/code&gt; obtains enum value sequence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_count&lt;/code&gt; returns number of enum values.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_integer&lt;/code&gt; obtains integer value from enum value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_name&lt;/code&gt; returns name from enum value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_names&lt;/code&gt; obtains string enum name sequence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_entries&lt;/code&gt; obtains pair (value enum, string enum name) sequence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_index&lt;/code&gt; obtains index in enum value sequence from enum value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_contains&lt;/code&gt; checks whether enum contains enumerator with such value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_type_name&lt;/code&gt; returns name of enum type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_fuse&lt;/code&gt; allows multidimensional switch/cases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_switch&lt;/code&gt; allows runtime enum value transformation to constexpr context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enum_for_each&lt;/code&gt; calls a function with all enum constexpr value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;is_unscoped_enum&lt;/code&gt; checks whether type is an &lt;a href=&#34;https://en.cppreference.com/w/cpp/language/enum#Unscoped_enumeration&#34;&gt;Unscoped enumeration&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;is_scoped_enum&lt;/code&gt; checks whether type is an &lt;a href=&#34;https://en.cppreference.com/w/cpp/language/enum#Scoped_enumerations&#34;&gt;Scoped enumeration&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;underlying_type&lt;/code&gt; improved UB-free &#34;SFINAE-friendly&#34; &lt;a href=&#34;https://en.cppreference.com/w/cpp/types/underlying_type&#34;&gt;underlying_type&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ostream_operators&lt;/code&gt; ostream operators for enums.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bitwise_operators&lt;/code&gt; bitwise operators for enums.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/reference.md&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/limitations.md&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/#Integration&#34;&gt;Integration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;C++17&lt;/li&gt; &#xA; &lt;li&gt;Header-only&lt;/li&gt; &#xA; &lt;li&gt;Dependency-free&lt;/li&gt; &#xA; &lt;li&gt;Compile-time&lt;/li&gt; &#xA; &lt;li&gt;Enum to string&lt;/li&gt; &#xA; &lt;li&gt;String to enum&lt;/li&gt; &#xA; &lt;li&gt;Iterating over enum&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/example/example.cpp&#34;&gt;Examples&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// For example color enum.&#xA;enum class Color { RED = 2, BLUE = 4, GREEN = 8 };&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum value to string&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Color color = Color::RED;&#xA;auto color_name = magic_enum::enum_name(color);&#xA;// color_name -&amp;gt; &#34;RED&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;String to enum value&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;std::string color_name{&#34;GREEN&#34;};&#xA;auto color = magic_enum::enum_cast&amp;lt;Color&amp;gt;(color_name);&#xA;if (color.has_value()) {&#xA;  // color.value() -&amp;gt; Color::GREEN&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Integer to enum value&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;int color_integer = 2;&#xA;auto color = magic_enum::enum_cast&amp;lt;Color&amp;gt;(color_integer);&#xA;if (color.has_value()) {&#xA;  // color.value() -&amp;gt; Color::RED&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Indexed access to enum value&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;std::size_t i = 1;&#xA;Color color = magic_enum::enum_value&amp;lt;Color&amp;gt;(i);&#xA;// color -&amp;gt; Color::BLUE&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum value sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;constexpr auto colors = magic_enum::enum_values&amp;lt;Color&amp;gt;();&#xA;// colors -&amp;gt; {Color::RED, Color::BLUE, Color::GREEN}&#xA;// colors[0] -&amp;gt; Color::RED&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Number of enum elements&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;constexpr std::size_t color_count = magic_enum::enum_count&amp;lt;Color&amp;gt;();&#xA;// color_count -&amp;gt; 3&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum value to integer&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Color color = Color::RED;&#xA;auto color_integer = magic_enum::enum_integer(color);&#xA;// color -&amp;gt; 2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum names sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;constexpr auto color_names = magic_enum::enum_names&amp;lt;Color&amp;gt;();&#xA;// color_names -&amp;gt; {&#34;RED&#34;, &#34;BLUE&#34;, &#34;GREEN&#34;}&#xA;// color_names[0] -&amp;gt; &#34;RED&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum entries sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;constexpr auto color_entries = magic_enum::enum_entries&amp;lt;Color&amp;gt;();&#xA;// color_entries -&amp;gt; {{Color::RED, &#34;RED&#34;}, {Color::BLUE, &#34;BLUE&#34;}, {Color::GREEN, &#34;GREEN&#34;}}&#xA;// color_entries[0].first -&amp;gt; Color::RED&#xA;// color_entries[0].second -&amp;gt; &#34;RED&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum fusion for multi-level switch/case statements&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;switch (magic_enum::enum_fuse(color, direction).value()) {&#xA;  case magic_enum::enum_fuse(Color::RED, Directions::Up).value(): // ...&#xA;  case magic_enum::enum_fuse(Color::BLUE, Directions::Down).value(): // ...&#xA;// ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum switch runtime value as constexpr constant&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;Color color = Color::RED;&#xA;&#xA;magic_enum::enum_switch([] (auto val) {&#xA;  constexpr Color c_color = val;&#xA;  // ...&#xA;}, color);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enum iterate for each enum as constexpr constant&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;magic_enum::enum_for_each&amp;lt;Color&amp;gt;([] (auto val) {&#xA;  constexpr Color c_color = val;&#xA;  // ...&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ostream operator for enum&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;using namespace magic_enum::ostream_operators; // out-of-the-box ostream operators for enums.&#xA;Color color = Color::BLUE;&#xA;std::cout &amp;lt;&amp;lt; color &amp;lt;&amp;lt; std::endl; // &#34;BLUE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bitwise operator for enum&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;enum class Flags { A = 1 &amp;lt;&amp;lt; 0, B = 1 &amp;lt;&amp;lt; 1, C = 1 &amp;lt;&amp;lt; 2, D = 1 &amp;lt;&amp;lt; 3 };&#xA;using namespace magic_enum::bitwise_operators; // out-of-the-box bitwise operators for enums.&#xA;// Support operators: ~, |, &amp;amp;, ^, |=, &amp;amp;=, ^=.&#xA;Flags flags = Flags::A | Flags::B &amp;amp; ~Flags::C;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Checks whether type is an &lt;a href=&#34;https://en.cppreference.com/w/cpp/language/enum#Unscoped_enumeration&#34;&gt;Unscoped enumeration&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;enum color { red, green, blue };&#xA;enum class direction { left, right };&#xA;&#xA;magic_enum::is_unscoped_enum&amp;lt;color&amp;gt;::value -&amp;gt; true&#xA;magic_enum::is_unscoped_enum&amp;lt;direction&amp;gt;::value -&amp;gt; false&#xA;magic_enum::is_unscoped_enum&amp;lt;int&amp;gt;::value -&amp;gt; false&#xA;&#xA;// Helper variable template.&#xA;magic_enum::is_unscoped_enum_v&amp;lt;color&amp;gt; -&amp;gt; true&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Checks whether type is an &lt;a href=&#34;https://en.cppreference.com/w/cpp/language/enum#Scoped_enumerations&#34;&gt;Scoped enumeration&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;enum color { red, green, blue };&#xA;enum class direction { left, right };&#xA;&#xA;magic_enum::is_scoped_enum&amp;lt;color&amp;gt;::value -&amp;gt; false&#xA;magic_enum::is_scoped_enum&amp;lt;direction&amp;gt;::value -&amp;gt; true&#xA;magic_enum::is_scoped_enum&amp;lt;int&amp;gt;::value -&amp;gt; false&#xA;&#xA;// Helper variable template.&#xA;magic_enum::is_scoped_enum_v&amp;lt;direction&amp;gt; -&amp;gt; true&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Static storage enum variable to string This version is much lighter on the compile times and is not restricted to the enum_range &lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/limitations.md&#34;&gt;limitation&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;constexpr Color color = Color::BLUE;&#xA;constexpr auto color_name = magic_enum::enum_name&amp;lt;color&amp;gt;();&#xA;// color_name -&amp;gt; &#34;BLUE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Remarks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;magic_enum&lt;/code&gt; does not pretend to be a silver bullet for reflection for enums, it was originally designed for small enum.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Before use, read the &lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/limitations.md&#34;&gt;limitations&lt;/a&gt; of functionality.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Integration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You should add the required file &lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/include/magic_enum.hpp&#34;&gt;magic_enum.hpp&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&#34;https://github.com/Microsoft/vcpkg/&#34;&gt;vcpkg&lt;/a&gt; on your project for external dependencies, then you can use the &lt;a href=&#34;https://github.com/microsoft/vcpkg/tree/master/ports/magic-enum&#34;&gt;magic-enum package&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&#34;https://www.conan.io/&#34;&gt;Conan&lt;/a&gt; to manage your dependencies, merely add &lt;code&gt;magic_enum/x.y.z&lt;/code&gt; to your conan&#39;s requires, where &lt;code&gt;x.y.z&lt;/code&gt; is the release version you want to use.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&#34;https://build2.org/&#34;&gt;Build2&lt;/a&gt; to build and manage your dependencies, add &lt;code&gt;depends: magic_enum ^x.y.z&lt;/code&gt; to the manifest file where &lt;code&gt;x.y.z&lt;/code&gt; is the release version you want to use. You can then import the target using &lt;code&gt;magic_enum%lib{magic_enum}&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Alternatively, you can use something like &lt;a href=&#34;https://github.com/TheLartians/CPM&#34;&gt;CPM&lt;/a&gt; which is based on CMake&#39;s &lt;code&gt;Fetch_Content&lt;/code&gt; module.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;CPMAddPackage(&#xA;    NAME magic_enum&#xA;    GITHUB_REPOSITORY Neargye/magic_enum&#xA;    GIT_TAG x.y.z # Where `x.y.z` is the release version you want to use.&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bazel is also supported, simply add to your WORKSPACE file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http_archive(&#xA;    name = &#34;magic_enum&#34;,&#xA;    strip_prefix = &#34;magic_enum-&amp;lt;commit&amp;gt;&#34;,&#xA;    urls = [&#34;https://github.com/Neargye/magic_enum/archive/&amp;lt;commit&amp;gt;.zip&#34;],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To use bazel inside the repository it&#39;s possible to do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bazel build //...&#xA;bazel test //...&#xA;bazel run //:example&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Note that you must use a supported compiler or specify it with &lt;code&gt;export CC= &amp;lt;compiler&amp;gt;&lt;/code&gt;.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&#34;https://www.ros.org/&#34;&gt;Ros&lt;/a&gt;, you can include this package by adding &lt;code&gt;&amp;lt;depend&amp;gt;magic_enum&amp;lt;/depend&amp;gt;&lt;/code&gt; to your package.xml and include this package in your workspace. In your CMakeLists.txt add the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;find_package(magic_enum CONFIG REQUIRED)&#xA;...&#xA;target_link_libraries(your_executable magic_enum::magic_enum)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compiler compatibility&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clang/LLVM &amp;gt;= 6&lt;/li&gt; &#xA; &lt;li&gt;MSVC++ &amp;gt;= 14.11 / Visual Studio &amp;gt;= 2017&lt;/li&gt; &#xA; &lt;li&gt;Xcode &amp;gt;= 10&lt;/li&gt; &#xA; &lt;li&gt;GCC &amp;gt;= 9&lt;/li&gt; &#xA; &lt;li&gt;MinGW &amp;gt;= 9&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/Neargye/magic_enum/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>ZLMediaKit/ZLMediaKit</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/ZLMediaKit/ZLMediaKit</id>
    <link href="https://github.com/ZLMediaKit/ZLMediaKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WebRTC/RTSP/RTMP/HTTP/HLS/HTTP-FLV/WebSocket-FLV/HTTP-TS/HTTP-fMP4/WebSocket-TS/WebSocket-fMP4/GB28181 server and client framework based on C++11&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xia-chu/ZLMediaKit/master/www/logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;‰∏Ä‰∏™Âü∫‰∫éC++11ÁöÑÈ´òÊÄßËÉΩËøêËê•Á∫ßÊµÅÂ™í‰ΩìÊúçÂä°Ê°ÜÊû∂&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/license-MIT-green.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://en.cppreference.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-c++-red.svg?sanitize=true&#34; alt=&#34;C++&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-blue.svg?sanitize=true&#34; alt=&#34;platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-yellow.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/xia-chu/ZLMediaKit&#34;&gt;&lt;img src=&#34;https://travis-ci.org/xia-chu/ZLMediaKit.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;È°πÁõÆÁâπÁÇπ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Âü∫‰∫éC++11ÂºÄÂèëÔºåÈÅøÂÖç‰ΩøÁî®Ë£∏ÊåáÈíàÔºå‰ª£Á†ÅÁ®≥ÂÆöÂèØÈù†ÔºåÊÄßËÉΩ‰ºòË∂ä„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊîØÊåÅÂ§öÁßçÂçèËÆÆ(RTSP/RTMP/HLS/HTTP-FLV/WebSocket-FLV/GB28181/HTTP-TS/WebSocket-TS/HTTP-fMP4/WebSocket-fMP4/MP4/WebRTC),ÊîØÊåÅÂçèËÆÆ‰∫íËΩ¨„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;‰ΩøÁî®Â§öË∑ØÂ§çÁî®/Â§öÁ∫øÁ®ã/ÂºÇÊ≠•ÁΩëÁªúIOÊ®°ÂºèÂºÄÂèëÔºåÂπ∂ÂèëÊÄßËÉΩ‰ºòË∂äÔºåÊîØÊåÅÊµ∑ÈáèÂÆ¢Êà∑Á´ØËøûÊé•„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;‰ª£Á†ÅÁªèËøáÈïøÊúüÂ§ßÈáèÁöÑÁ®≥ÂÆöÊÄß„ÄÅÊÄßËÉΩÊµãËØïÔºåÂ∑≤ÁªèÂú®Á∫ø‰∏äÂïÜÁî®È™åËØÅÂ∑≤‰πÖ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊîØÊåÅlinux„ÄÅmacos„ÄÅios„ÄÅandroid„ÄÅwindowsÂÖ®Âπ≥Âè∞„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊîØÊåÅÁîªÈù¢ÁßíÂºÄ„ÄÅÊûÅ‰ΩéÂª∂Êó∂(&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/%E5%BB%B6%E6%97%B6%E6%B5%8B%E8%AF%95&#34;&gt;500ÊØ´ÁßíÂÜÖÔºåÊúÄ‰ΩéÂèØËææ100ÊØ´Áßí&lt;/a&gt;)„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Êèê‰æõÂÆåÂñÑÁöÑÊ†áÂáÜ&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/api/include&#34;&gt;C API&lt;/a&gt;,ÂèØ‰ª•‰ΩúSDKÁî®ÔºåÊàñ‰æõÂÖ∂‰ªñËØ≠Ë®ÄË∞ÉÁî®„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Êèê‰æõÂÆåÊï¥ÁöÑ&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/server&#34;&gt;MediaServer&lt;/a&gt;ÊúçÂä°Âô®ÔºåÂèØ‰ª•ÂÖçÂºÄÂèëÁõ¥Êé•ÈÉ®ÁΩ≤‰∏∫ÂïÜÁî®ÊúçÂä°Âô®„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Êèê‰æõÂÆåÂñÑÁöÑ&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-API&#34;&gt;restful api&lt;/a&gt;‰ª•Âèä&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-HOOK-API&#34;&gt;web hook&lt;/a&gt;ÔºåÊîØÊåÅ‰∏∞ÂØåÁöÑ‰∏öÂä°ÈÄªËæë„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊâìÈÄö‰∫ÜËßÜÈ¢ëÁõëÊéßÂçèËÆÆÊ†à‰∏éÁõ¥Êí≠ÂçèËÆÆÊ†àÔºåÂØπRTSP/RTMPÊîØÊåÅÈÉΩÂæàÂÆåÂñÑ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÂÖ®Èù¢ÊîØÊåÅH265/H264/AAC/G711/OPUS„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÂäüËÉΩÂÆåÂñÑÔºåÊîØÊåÅÈõÜÁæ§„ÄÅÊåâÈúÄËΩ¨ÂçèËÆÆ„ÄÅÊåâÈúÄÊé®ÊãâÊµÅ„ÄÅÂÖàÊí≠ÂêéÊé®„ÄÅÊñ≠ËøûÁª≠Êé®Á≠âÂäüËÉΩ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊûÅËá¥ÊÄßËÉΩÔºåÂçïÊú∫10WÁ∫ßÂà´Êí≠ÊîæÂô®Ôºå100Gb/sÁ∫ßÂà´ioÂ∏¶ÂÆΩËÉΩÂäõ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊûÅËá¥‰ΩìÈ™åÔºå&lt;a href=&#34;https://github.com/ZLMediaKit/ZLMediaKit/wiki/ZLMediakit%E7%8B%AC%E5%AE%B6%E7%89%B9%E6%80%A7%E4%BB%8B%E7%BB%8D&#34;&gt;Áã¨ÂÆ∂ÁâπÊÄß&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZLMediaKit/ZLMediaKit/issues/511&#34;&gt;Ë∞ÅÂú®‰ΩøÁî®zlmediakit?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÂÖ®Èù¢ÊîØÊåÅipv6ÁΩëÁªú&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;È°πÁõÆÂÆö‰Ωç&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ÁßªÂä®ÂµåÂÖ•ÂºèË∑®Âπ≥Âè∞ÊµÅÂ™í‰ΩìËß£ÂÜ≥ÊñπÊ°à„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÂïÜÁî®Á∫ßÊµÅÂ™í‰ΩìÊúçÂä°Âô®„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÁΩëÁªúÁºñÁ®ã‰∫åÊ¨°ÂºÄÂèëSDK„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÂäüËÉΩÊ∏ÖÂçï&lt;/h2&gt; &#xA;&lt;h3&gt;ÂäüËÉΩ‰∏ÄËßà&lt;/h3&gt; &#xA;&lt;img width=&#34;800&#34; alt=&#34;ÂäüËÉΩ‰∏ÄËßà&#34; src=&#34;https://user-images.githubusercontent.com/11495632/114176523-d50fce80-996d-11eb-81f8-0a2e2715ba7b.png&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;RTSP[S]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RTSP[S] ÊúçÂä°Âô®ÔºåÊîØÊåÅRTMP/MP4/HLSËΩ¨RTSP[S],ÊîØÊåÅ‰∫öÈ©¨ÈÄäecho showËøôÊ†∑ÁöÑËÆæÂ§á&lt;/li&gt; &#xA;   &lt;li&gt;RTSP[S] Êí≠ÊîæÂô®ÔºåÊîØÊåÅRTSP‰ª£ÁêÜÔºåÊîØÊåÅÁîüÊàêÈùôÈü≥Èü≥È¢ë&lt;/li&gt; &#xA;   &lt;li&gt;RTSP[S] Êé®ÊµÅÂÆ¢Êà∑Á´Ø‰∏éÊúçÂä°Âô®&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅ &lt;code&gt;rtp over udp&lt;/code&gt; &lt;code&gt;rtp over tcp&lt;/code&gt; &lt;code&gt;rtp over http&lt;/code&gt; &lt;code&gt;rtpÁªÑÊí≠&lt;/code&gt; ÂõõÁßçRTP‰º†ËæìÊñπÂºè&lt;/li&gt; &#xA;   &lt;li&gt;ÊúçÂä°Âô®/ÂÆ¢Êà∑Á´ØÂÆåÊï¥ÊîØÊåÅBasic/DigestÊñπÂºèÁöÑÁôªÂΩïÈâ¥ÊùÉÔºåÂÖ®ÂºÇÊ≠•ÂèØÈÖçÁΩÆÂåñÁöÑÈâ¥ÊùÉÊé•Âè£&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH265ÁºñÁ†Å&lt;/li&gt; &#xA;   &lt;li&gt;ÊúçÂä°Âô®ÊîØÊåÅRTSPÊé®ÊµÅ(ÂåÖÊã¨&lt;code&gt;rtp over udp&lt;/code&gt; &lt;code&gt;rtp over tcp&lt;/code&gt;ÊñπÂºè)&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†ÅÔºåÂÖ∂‰ªñÁºñÁ†ÅËÉΩËΩ¨Âèë‰ΩÜ‰∏çËÉΩËΩ¨ÂçèËÆÆ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RTMP[S]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RTMP[S] Êí≠ÊîæÊúçÂä°Âô®ÔºåÊîØÊåÅRTSP/MP4/HLSËΩ¨RTMP&lt;/li&gt; &#xA;   &lt;li&gt;RTMP[S] ÂèëÂ∏ÉÊúçÂä°Âô®ÔºåÊîØÊåÅÂΩïÂà∂ÂèëÂ∏ÉÊµÅ&lt;/li&gt; &#xA;   &lt;li&gt;RTMP[S] Êí≠ÊîæÂô®ÔºåÊîØÊåÅRTMP‰ª£ÁêÜÔºåÊîØÊåÅÁîüÊàêÈùôÈü≥Èü≥È¢ë&lt;/li&gt; &#xA;   &lt;li&gt;RTMP[S] Êé®ÊµÅÂÆ¢Êà∑Á´Ø&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅhttp[s]-flvÁõ¥Êí≠&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅwebsocket-flvÁõ¥Êí≠&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†ÅÔºåÂÖ∂‰ªñÁºñÁ†ÅËÉΩËΩ¨Âèë‰ΩÜ‰∏çËÉΩËΩ¨ÂçèËÆÆ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅ&lt;a href=&#34;https://github.com/ksvc/FFmpeg/wiki&#34;&gt;RTMP-H265&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅ&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/RTMP%E5%AF%B9H265%E5%92%8COPUS%E7%9A%84%E6%94%AF%E6%8C%81&#34;&gt;RTMP-OPUS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HLS&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅHLSÊñá‰ª∂ÁîüÊàêÔºåËá™Â∏¶HTTPÊñá‰ª∂ÊúçÂä°Âô®&lt;/li&gt; &#xA;   &lt;li&gt;ÈÄöËøácookieËøΩË∏™ÊäÄÊúØÔºåÂèØ‰ª•Ê®°ÊãüHLSÊí≠Êîæ‰∏∫ÈïøËøûÊé•ÔºåÂèØ‰ª•ÂÆûÁé∞HLSÊåâÈúÄÊãâÊµÅ„ÄÅÊí≠ÊîæÁªüËÆ°Á≠â‰∏öÂä°&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅHLSÊí≠ÂèëÂô®ÔºåÊîØÊåÅÊãâÊµÅHLSËΩ¨rtsp/rtmp/mp4&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†Å&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TS&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅhttp[s]-tsÁõ¥Êí≠&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅws[s]-tsÁõ¥Êí≠&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†Å&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;fMP4&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅhttp[s]-fmp4Áõ¥Êí≠&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅws[s]-fmp4Áõ¥Êí≠&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†Å&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HTTP[S]‰∏éWebSocket&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊúçÂä°Âô®ÊîØÊåÅ&lt;code&gt;ÁõÆÂΩïÁ¥¢ÂºïÁîüÊàê&lt;/code&gt;,&lt;code&gt;Êñá‰ª∂‰∏ãËΩΩ&lt;/code&gt;,&lt;code&gt;Ë°®ÂçïÊèê‰∫§ËØ∑Ê±Ç&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ÂÆ¢Êà∑Á´ØÊèê‰æõ&lt;code&gt;Êñá‰ª∂‰∏ãËΩΩÂô®(ÊîØÊåÅÊñ≠ÁÇπÁª≠‰º†)&lt;/code&gt;,&lt;code&gt;Êé•Âè£ËØ∑Ê±ÇÂô®&lt;/code&gt;,&lt;code&gt;Êñá‰ª∂‰∏ä‰º†Âô®&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ÂÆåÊï¥HTTP APIÊúçÂä°Âô®ÔºåÂèØ‰ª•‰Ωú‰∏∫webÂêéÂè∞ÂºÄÂèëÊ°ÜÊû∂&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅË∑®ÂüüËÆøÈóÆ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅhttpÂÆ¢Êà∑Á´Ø„ÄÅÊúçÂä°Âô®cookie&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅWebSocketÊúçÂä°Âô®ÂíåÂÆ¢Êà∑Á´Ø&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅhttpÊñá‰ª∂ËÆøÈóÆÈâ¥ÊùÉ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GB28181‰∏éRTPÊé®ÊµÅ&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅUDP/TCPÂõΩÊ†áRTP(PSÊàñTS)Êé®ÊµÅÊúçÂä°Âô®ÔºåÂèØ‰ª•ËΩ¨Êç¢ÊàêRTSP/RTMP/HLSÁ≠âÂçèËÆÆ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅRTSP/RTMP/HLSËΩ¨ÂõΩÊ†áÊé®ÊµÅÂÆ¢Êà∑Á´ØÔºåÊîØÊåÅTCP/UDPÊ®°ÂºèÔºåÊèê‰æõÁõ∏Â∫îrestful api&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†Å&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÊµ∑Â∫∑ehomeÊé®ÊµÅ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MP4ÁÇπÊí≠‰∏éÂΩïÂà∂&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÂΩïÂà∂‰∏∫FLV/HLS/MP4&lt;/li&gt; &#xA;   &lt;li&gt;RTSP/RTMP/HTTP-FLV/WS-FLVÊîØÊåÅMP4Êñá‰ª∂ÁÇπÊí≠ÔºåÊîØÊåÅseek&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅH264/H265/AAC/G711/OPUSÁºñÁ†Å&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;WebRTC&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅWebRTCÊé®ÊµÅÔºåÊîØÊåÅËΩ¨ÂÖ∂‰ªñÂçèËÆÆ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅWebRTCÊí≠ÊîæÔºåÊîØÊåÅÂÖ∂‰ªñÂçèËÆÆËΩ¨WebRTC&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÂèåÂêëecho test&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅsimulcastÊé®ÊµÅ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅ‰∏ä‰∏ãË°årtx/nack‰∏¢ÂåÖÈáç‰º†&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ÊîØÊåÅÂçïÁ´ØÂè£„ÄÅÂ§öÁ∫øÁ®ã„ÄÅÂÆ¢Êà∑Á´ØÁΩëÁªúËøûÊé•ËøÅÁßª(ÂºÄÊ∫êÁïåÂîØ‰∏Ä)&lt;/strong&gt;„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅTWCC rtcpÂä®ÊÄÅË∞ÉÊï¥Á†ÅÁéá&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅremb/pli/sr/rr rtcp&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅrtpÊâ©Â±ïËß£Êûê&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅGOPÁºìÂÜ≤ÔºåwebrtcÊí≠ÊîæÁßíÂºÄ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅdatachannel&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ÂÖ∂‰ªñ&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÊîØÊåÅ‰∏∞ÂØåÁöÑrestful api‰ª•Âèäweb hook‰∫ã‰ª∂&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÁÆÄÂçïÁöÑtelnetË∞ÉËØï&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÈÖçÁΩÆÊñá‰ª∂ÁÉ≠Âä†ËΩΩ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÊµÅÈáèÁªüËÆ°„ÄÅÊé®ÊãâÊµÅÈâ¥ÊùÉÁ≠â‰∫ã‰ª∂&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅËôöÊãü‰∏ªÊú∫,ÂèØ‰ª•ÈöîÁ¶ª‰∏çÂêåÂüüÂêç&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÊåâÈúÄÊãâÊµÅÔºåÊó†‰∫∫ËßÇÁúãËá™Âä®ÂÖ≥Êñ≠ÊãâÊµÅ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÂÖàÊí≠ÊîæÂêéÊé®ÊµÅÔºåÊèêÈ´òÂèäÊó∂Êé®ÊµÅÁîªÈù¢ÊâìÂºÄÁéá&lt;/li&gt; &#xA;   &lt;li&gt;Êèê‰æõc api sdk&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅFFmpegÊãâÊµÅ‰ª£ÁêÜ‰ªªÊÑèÊ†ºÂºèÁöÑÊµÅ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅhttp apiÁîüÊàêÂπ∂ËøîÂõûÂÆûÊó∂Êà™Âõæ&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÊåâÈúÄËß£Â§çÁî®„ÄÅËΩ¨ÂçèËÆÆÔºåÂΩìÊúâ‰∫∫ËßÇÁúãÊó∂ÊâçÂºÄÂêØËΩ¨ÂçèËÆÆÔºåÈôç‰ΩécpuÂç†Áî®Áéá&lt;/li&gt; &#xA;   &lt;li&gt;ÊîØÊåÅÊ∫ØÊ∫êÊ®°ÂºèÁöÑÈõÜÁæ§ÈÉ®ÁΩ≤ÔºåÊ∫ØÊ∫êÊñπÂºèÊîØÊåÅrtsp/rtmp/hls/http-ts, ËæπÊ≤øÁ´ôÊîØÊåÅhls, Ê∫êÁ´ôÊîØÊåÅÂ§ö‰∏™(ÈááÁî®round robinÊñπÂºèÊ∫ØÊ∫ê)&lt;/li&gt; &#xA;   &lt;li&gt;rtsp/rtmp/webrtcÊé®ÊµÅÂºÇÂ∏∏Êñ≠ÂºÄÂêéÔºåÂèØ‰ª•Âú®Ë∂ÖÊó∂Êó∂Èó¥ÂÜÖÈáçËøûÊé®ÊµÅÔºåÊí≠ÊîæÂô®Êó†ÊÑüÁü•&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÁºñËØë‰ª•ÂèäÊµãËØï&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;ÁºñËØëÂâçÂä°ÂøÖ‰ªîÁªÜÂèÇËÄÉwiki:&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B&#34;&gt;Âø´ÈÄüÂºÄÂßã&lt;/a&gt;Êìç‰Ωú!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ÊÄé‰πà‰ΩøÁî®&lt;/h2&gt; &#xA;&lt;p&gt;‰Ω†Êúâ‰∏âÁßçÊñπÊ≥ï‰ΩøÁî®ZLMediaKitÔºåÂàÜÂà´ÊòØÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1„ÄÅ‰ΩøÁî®c apiÔºå‰Ωú‰∏∫sdk‰ΩøÁî®ÔºåËØ∑ÂèÇËÄÉ&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/api/include&#34;&gt;ËøôÈáå&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;2„ÄÅ‰Ωú‰∏∫Áã¨Á´ãÁöÑÊµÅÂ™í‰ΩìÊúçÂä°Âô®‰ΩøÁî®Ôºå‰∏çÊÉ≥ÂÅöc/c++ÂºÄÂèëÁöÑÔºåÂèØ‰ª•ÂèÇËÄÉ &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-API&#34;&gt;restful api&lt;/a&gt; Âíå &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-HOOK-API&#34;&gt;web hook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;3„ÄÅÂ¶ÇÊûúÊÉ≥ÂÅöc/c++ÂºÄÂèëÔºåÊ∑ªÂä†‰∏öÂä°ÈÄªËæëÂ¢ûÂä†ÂäüËÉΩÔºåÂèØ‰ª•ÂèÇËÄÉËøôÈáåÁöÑ&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/tests&#34;&gt;ÊµãËØïÁ®ãÂ∫è&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker ÈïúÂÉè&lt;/h2&gt; &#xA;&lt;p&gt;‰Ω†ÂèØ‰ª•‰ªéDocker Hub‰∏ãËΩΩÂ∑≤ÁªèÁºñËØëÂ•ΩÁöÑÈïúÂÉèÂπ∂ÂêØÂä®ÂÆÉÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#Ê≠§ÈïúÂÉè‰∏∫zlmediakitÂºÄÂèëÂõ¢ÈòüÊèê‰æõÔºåÊé®Ëçê&#xA;docker run -id -p 1935:1935 -p 8080:80 -p 8554:554 -p 10000:10000 -p 10000:10000/udp -p 8000:8000/udp zlmediakit/zlmediakit:Release.last&#xA;&#xA;#Ê≠§ÈïúÂÉèÂßîÊâòÁ¨¨‰∏âÊñπÊèê‰æõ&#xA;docker run -id -p 1935:1935 -p 8080:80 -p 8554:554 -p 10000:10000 -p 10000:10000/udp panjjo/zlmediakit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‰Ω†‰πüÂèØ‰ª•Ê†πÊçÆDockerfileÁºñËØëÈïúÂÉèÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash build_docker_images.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Âêà‰ΩúÈ°πÁõÆ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ÂèØËßÜÂåñÁÆ°ÁêÜÁΩëÁ´ô&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/langmansh/AKStreamNVR&#34;&gt;ÊúÄÊñ∞ÁöÑÂâçÂêéÁ´ØÂàÜÁ¶ªwebÈ°πÁõÆ,ÊîØÊåÅwebrtcÊí≠Êîæ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://gitee.com/kkkkk5G/MediaServerUI&#34;&gt;Âü∫‰∫éZLMediaKit‰∏ªÁ∫øÁöÑÁÆ°ÁêÜWEBÁΩëÁ´ô&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/chenxiaolei/ZLMediaKit_NVR_UI&#34;&gt;Âü∫‰∫éZLMediaKitÂàÜÊîØÁöÑÁÆ°ÁêÜWEBÁΩëÁ´ô&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/MingZhuLiu/ZLMediaServerManagent&#34;&gt;‰∏Ä‰∏™ÈùûÂ∏∏ÊºÇ‰∫ÆÁöÑÂèØËßÜÂåñÂêéÂè∞ÁÆ°ÁêÜÁ≥ªÁªü&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ÊµÅÂ™í‰ΩìÁÆ°ÁêÜÂπ≥Âè∞&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/648540858/wvp-GB28181-pro&#34;&gt;GB28181ÂÆåÊï¥Ëß£ÂÜ≥ÊñπÊ°à,Ëá™Â∏¶webÁÆ°ÁêÜÁΩëÁ´ô,ÊîØÊåÅwebrtc„ÄÅh265Êí≠Êîæ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/chatop2020/AKStream&#34;&gt;ÂäüËÉΩÂº∫Â§ßÁöÑÊµÅÂ™í‰ΩìÊéßÂà∂ÁÆ°ÁêÜÊé•Âè£Âπ≥Âè∞,ÊîØÊåÅGB28181&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/panjjo/gosip&#34;&gt;GoÂÆûÁé∞ÁöÑGB28181ÊúçÂä°Âô®&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://gitee.com/hfwudao/GB28181_Node_Http&#34;&gt;node-jsÁâàÊú¨ÁöÑGB28181Âπ≥Âè∞&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tsingeye/FreeEhome&#34;&gt;GoÂÆûÁé∞ÁöÑÊµ∑Â∫∑ehomeÊúçÂä°Âô®&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ÂÆ¢Êà∑Á´Ø&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/hctym1995/ZLM_ApiDemo&#34;&gt;Âü∫‰∫éC SDKÂÆûÁé∞ÁöÑÊé®ÊµÅÂÆ¢Êà∑Á´Ø&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/chengxiaosheng/ZLMediaKit.HttpApi&#34;&gt;C#ÁâàÊú¨ÁöÑHttp API‰∏éHook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/MingZhuLiu/ZLMediaKit.DotNetCore.Sdk&#34;&gt;DotNetCoreÁöÑRESTfulÂÆ¢Êà∑Á´Ø&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Êí≠ÊîæÂô®&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/numberwolf/h265web.js&#34;&gt;Âü∫‰∫éwasmÊîØÊåÅH265ÁöÑÊí≠ÊîæÂô®&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/v354412101/wsPlayer&#34;&gt;Âü∫‰∫éMSEÁöÑwebsocket-fmp4Êí≠ÊîæÂô®&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/metartc/metaRTC&#34;&gt;ÂÖ®ÂõΩ‰∫ßwebrtc sdk(metaRTC)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÊéàÊùÉÂçèËÆÆ&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆËá™Êúâ‰ª£Á†Å‰ΩøÁî®ÂÆΩÊùæÁöÑMITÂçèËÆÆÔºåÂú®‰øùÁïôÁâàÊùÉ‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ãÂèØ‰ª•Ëá™Áî±Â∫îÁî®‰∫éÂêÑËá™ÂïÜÁî®„ÄÅÈùûÂïÜ‰∏öÁöÑÈ°πÁõÆ„ÄÇ ‰ΩÜÊòØÊú¨È°πÁõÆ‰πüÈõ∂Á¢éÁöÑ‰ΩøÁî®‰∫Ü‰∏Ä‰∫õÂÖ∂‰ªñÁöÑÂºÄÊ∫ê‰ª£Á†ÅÔºåÂú®ÂïÜÁî®ÁöÑÊÉÖÂÜµ‰∏ãËØ∑Ëá™Ë°åÊõø‰ª£ÊàñÂâîÈô§Ôºõ Áî±‰∫é‰ΩøÁî®Êú¨È°πÁõÆËÄå‰∫ßÁîüÁöÑÂïÜ‰∏öÁ∫†Á∫∑Êàñ‰æµÊùÉË°å‰∏∫‰∏ÄÊ¶Ç‰∏éÊú¨È°πÁõÆÂèäÂºÄÂèëËÄÖÊó†ÂÖ≥ÔºåËØ∑Ëá™Ë°åÊâøÊãÖÊ≥ïÂæãÈ£éÈô©„ÄÇ Âú®‰ΩøÁî®Êú¨È°πÁõÆ‰ª£Á†ÅÊó∂Ôºå‰πüÂ∫îËØ•Âú®ÊéàÊùÉÂçèËÆÆ‰∏≠ÂêåÊó∂Ë°®ÊòéÊú¨È°πÁõÆ‰æùËµñÁöÑÁ¨¨‰∏âÊñπÂ∫ìÁöÑÂçèËÆÆ„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;ËÅîÁ≥ªÊñπÂºè&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ÈÇÆÁÆ±Ôºö&lt;a href=&#34;mailto:1213642868@qq.com&#34;&gt;1213642868@qq.com&lt;/a&gt;(Êú¨È°πÁõÆÁõ∏ÂÖ≥ÊàñÊµÅÂ™í‰ΩìÁõ∏ÂÖ≥ÈóÆÈ¢òËØ∑Ëµ∞issueÊµÅÁ®ãÔºåÂê¶ÂàôÊÅï‰∏çÈÇÆ‰ª∂Á≠îÂ§ç)&lt;/li&gt; &#xA; &lt;li&gt;QQÁæ§ÔºöqqÁæ§Âè∑Âú®wiki‰∏≠ÔºåËØ∑ÈòÖËØªwikiÂêéÂÜçÂä†Áæ§&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÊÄé‰πàÊèêÈóÆÔºü&lt;/h2&gt; &#xA;&lt;p&gt;Â¶ÇÊûúË¶ÅÂØπÈ°πÁõÆÊúâÁõ∏ÂÖ≥ÁñëÈóÆÔºåÂª∫ËÆÆÊÇ®Ëøô‰πàÂÅöÔºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1„ÄÅ‰ªîÁªÜÁúã‰∏ãreadme„ÄÅwikiÔºåÂ¶ÇÊûúÊúâÂøÖË¶ÅÂèØ‰ª•Êü•Áúã‰∏ãissue.&lt;/li&gt; &#xA; &lt;li&gt;2„ÄÅÂ¶ÇÊûúÊÇ®ÁöÑÈóÆÈ¢òËøòÊ≤°Ëß£ÂÜ≥ÔºåÂèØ‰ª•Êèêissue.&lt;/li&gt; &#xA; &lt;li&gt;3„ÄÅÊúâ‰∫õÈóÆÈ¢òÔºåÂ¶ÇÊûú‰∏çÂÖ∑Â§áÂèÇËÄÉÊÄßÁöÑÔºåÊó†ÈúÄÂú®issueÊèêÁöÑÔºåÂèØ‰ª•Âú®qqÁæ§Êèê.&lt;/li&gt; &#xA; &lt;li&gt;4„ÄÅQQÁßÅËÅä‰∏ÄËà¨‰∏çÊé•ÂèóÊó†ÂÅøÊäÄÊúØÂí®ËØ¢ÂíåÊîØÊåÅ(&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%BB%BA%E8%AE%AEQQ%E7%A7%81%E8%81%8A%E5%92%A8%E8%AF%A2%E9%97%AE%E9%A2%98%EF%BC%9F&#34;&gt;‰∏∫‰ªÄ‰πà‰∏çÊèêÂÄ°QQÁßÅËÅä&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÁâπÂà´ÊÑüË∞¢&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆÈááÁî®‰∫Ü&lt;a href=&#34;https://github.com/ireader&#34;&gt;ËÄÅÈôà&lt;/a&gt; ÁöÑ &lt;a href=&#34;https://github.com/ireader/media-server&#34;&gt;media-server&lt;/a&gt; Â∫ìÔºå Êú¨È°πÁõÆÁöÑ ts/fmp4/mp4/ps ÂÆπÂô®Ê†ºÂºèÁöÑÂ§çÁî®Ëß£Â§çÁî®ÈÉΩ‰æùËµñmedia-serverÂ∫ì„ÄÇÂú®ÂÆûÁé∞Êú¨È°πÁõÆËØ∏Â§öÂäüËÉΩÊó∂ÔºåËÄÅÈôàÂ§öÊ¨°Áªô‰∫à‰∫ÜÊó†ÁßÅÁÉ≠ÊÉÖÂÖ≥ÈîÆÁöÑÂ∏ÆÂä©Ôºå ÁâπÊ≠§ÂØπ‰ªñË°®Á§∫ËØöÊåöÁöÑÊÑüË∞¢ÔºÅ&lt;/p&gt; &#xA;&lt;h2&gt;Ëá¥Ë∞¢&lt;/h2&gt; &#xA;&lt;p&gt;ÊÑüË∞¢‰ª•‰∏ãÂêÑ‰ΩçÂØπÊú¨È°πÁõÆÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é‰ª£Á†ÅË¥°ÁåÆ„ÄÅÈóÆÈ¢òÂèçÈ¶à„ÄÅËµÑÈáëÊçêËµ†Á≠âÂêÑÁßçÊñπÂºèÁöÑÊîØÊåÅÔºÅ‰ª•‰∏ãÊéíÂêç‰∏çÂàÜÂÖàÂêéÔºö&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ireader&#34;&gt;ËÄÅÈôà&lt;/a&gt; &lt;a href=&#34;https://github.com/gemfield&#34;&gt;Gemfield&lt;/a&gt; &lt;a href=&#34;https://github.com/nanguantong2&#34;&gt;ÂçóÂÜ†ÂΩ§&lt;/a&gt; &lt;a href=&#34;https://github.com/tsingeye&#34;&gt;ÂáπÂá∏ÊÖ¢&lt;/a&gt; &lt;a href=&#34;https://github.com/chenxiaolei&#34;&gt;chenxiaolei&lt;/a&gt; &lt;a href=&#34;https://github.com/zqsong&#34;&gt;Âè≤ÂâçÂ∞èËô´&lt;/a&gt; &lt;a href=&#34;https://github.com/baiyfcu&#34;&gt;Ê∏ÖÊ∂©ÁªøËå∂&lt;/a&gt; &lt;a href=&#34;https://github.com/3503207480&#34;&gt;3503207480&lt;/a&gt; &lt;a href=&#34;https://github.com/DroidChow&#34;&gt;DroidChow&lt;/a&gt; &lt;a href=&#34;https://github.com/HuoQiShuai&#34;&gt;ÈòøÂ°û&lt;/a&gt; &lt;a href=&#34;https://github.com/ChinaCCF&#34;&gt;ÁÅ´ÂÆ£&lt;/a&gt; &lt;a href=&#34;https://github.com/JerryLinGd&#34;&gt;Œ≥ÁëûŒ≥„Éü&lt;/a&gt; &lt;a href=&#34;https://www.linkingvision.com/&#34;&gt;linkingvision&lt;/a&gt; &lt;a href=&#34;https://github.com/taotaobujue2008&#34;&gt;ËåÑÂ≠ê&lt;/a&gt; &lt;a href=&#34;mailto:409257224@qq.com&#34;&gt;Â•ΩÂøÉÊÉÖ&lt;/a&gt; &lt;a href=&#34;https://github.com/MingZhuLiu&#34;&gt;ÊµÆÊ≤â&lt;/a&gt; &lt;a href=&#34;https://github.com/wasphin&#34;&gt;Xiaofeng Wang&lt;/a&gt; &lt;a href=&#34;https://github.com/doodoocoder&#34;&gt;doodoocoder&lt;/a&gt; &lt;a href=&#34;https://github.com/Colibrow&#34;&gt;qingci&lt;/a&gt; &lt;a href=&#34;https://github.com/swwheihei&#34;&gt;swwheihei&lt;/a&gt; &lt;a href=&#34;https://gitee.com/kkkkk5G&#34;&gt;KKKKK5G&lt;/a&gt; &lt;a href=&#34;mailto:zhouweimin@supremind.com&#34;&gt;Zhou Weimin&lt;/a&gt; &lt;a href=&#34;https://github.com/jim-king-2000&#34;&gt;Jim Jin&lt;/a&gt; &lt;a href=&#34;mailto:392293307@qq.com&#34;&gt;Ë•øÁìú‰∏∂&lt;/a&gt; &lt;a href=&#34;https://github.com/MingZhuLiu&#34;&gt;MingZhuLiu&lt;/a&gt; &lt;a href=&#34;https://github.com/chengxiaosheng&#34;&gt;chengxiaosheng&lt;/a&gt; &lt;a href=&#34;mailto:2381267071@qq.com&#34;&gt;big panda&lt;/a&gt; &lt;a href=&#34;https://github.com/tanningzhong&#34;&gt;tanningzhong&lt;/a&gt; &lt;a href=&#34;https://github.com/hctym1995&#34;&gt;hctym1995&lt;/a&gt; &lt;a href=&#34;https://gitee.com/kingyuanyuan&#34;&gt;hewenyuan&lt;/a&gt; &lt;a href=&#34;mailto:sunhui200475@163.com&#34;&gt;sunhui&lt;/a&gt; &lt;a href=&#34;mailto:fangpengcheng@bilibili.com&#34;&gt;mirs&lt;/a&gt; &lt;a href=&#34;mailto:kevin__cheng@outlook.com&#34;&gt;Kevin Cheng&lt;/a&gt; &lt;a href=&#34;mailto:root@oopy.org&#34;&gt;Liu Jiang&lt;/a&gt; &lt;a href=&#34;https://github.com/alongl&#34;&gt;along&lt;/a&gt; &lt;a href=&#34;mailto:xpy66swsry@gmail.com&#34;&gt;qingci&lt;/a&gt; &lt;a href=&#34;mailto:zh.ghlong@qq.com&#34;&gt;lyg1949&lt;/a&gt; &lt;a href=&#34;mailto:zh.ghlong@qq.com&#34;&gt;zhlong&lt;/a&gt; &lt;a href=&#34;mailto:3503207480@qq.com&#34;&gt;Â§ßË£§Ë°©&lt;/a&gt; &lt;a href=&#34;mailto:droid.chow@gmail.com&#34;&gt;droid.chow&lt;/a&gt; &lt;a href=&#34;https://github.com/musicwood&#34;&gt;ÈôàÊôìÊûó&lt;/a&gt; &lt;a href=&#34;https://github.com/CharleyWangHZ&#34;&gt;CharleyWangHZ&lt;/a&gt; &lt;a href=&#34;https://github.com/johzzy&#34;&gt;Johnny&lt;/a&gt; &lt;a href=&#34;https://github.com/DoubleX69&#34;&gt;DoubleX69&lt;/a&gt; &lt;a href=&#34;https://github.com/lawrencehj&#34;&gt;lawrencehj&lt;/a&gt; &lt;a href=&#34;mailto:xyyangkun@163.com&#34;&gt;yangkun&lt;/a&gt; &lt;a href=&#34;mailto:holychaossword@hotmail.com&#34;&gt;Xinghua Zhao&lt;/a&gt; &lt;a href=&#34;https://github.com/brokensword2018&#34;&gt;hejilin&lt;/a&gt; &lt;a href=&#34;https://github.com/rqb500&#34;&gt;rqb500&lt;/a&gt; &lt;a href=&#34;https://github.com/alexliyu7352&#34;&gt;Alex&lt;/a&gt; &lt;a href=&#34;https://github.com/Dw9&#34;&gt;Dw9&lt;/a&gt; &lt;a href=&#34;mailto:mingyuejingque@gmail.com&#34;&gt;ÊòéÊúàÊÉäÈπä&lt;/a&gt; &lt;a href=&#34;mailto:2958580318@qq.com&#34;&gt;cgm&lt;/a&gt; &lt;a href=&#34;mailto:1724010622@qq.com&#34;&gt;hejilin&lt;/a&gt; &lt;a href=&#34;mailto:liyu7352@gmail.com&#34;&gt;alexliyu7352&lt;/a&gt; &lt;a href=&#34;mailto:2958580318@qq.com&#34;&gt;cgm&lt;/a&gt; &lt;a href=&#34;https://github.com/HaoruiWang&#34;&gt;haorui wang&lt;/a&gt; &lt;a href=&#34;mailto:joshuafc@foxmail.com&#34;&gt;joshuafc&lt;/a&gt; &lt;a href=&#34;https://github.com/JayChen0519&#34;&gt;JayChen0519&lt;/a&gt; &lt;a href=&#34;mailto:zuoxue@qq.com&#34;&gt;zx&lt;/a&gt; &lt;a href=&#34;mailto:wangcker@163.com&#34;&gt;wangcker&lt;/a&gt; &lt;a href=&#34;mailto:wp@zafu.edu.cn&#34;&gt;WuPeng&lt;/a&gt; &lt;a href=&#34;https://github.com/starry&#34;&gt;starry&lt;/a&gt; &lt;a href=&#34;https://github.com/mtdxc&#34;&gt;mtdxc&lt;/a&gt; &lt;a href=&#34;https://github.com/hugangfeng333&#34;&gt;ËÉ°ÂàöÈ£é&lt;/a&gt; &lt;a href=&#34;https://github.com/zhao85&#34;&gt;zhao85&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‰ΩøÁî®Ê°à‰æã&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨È°πÁõÆÂ∑≤ÁªèÂæóÂà∞‰∏çÂ∞ëÂÖ¨Âè∏Âíå‰∏™‰∫∫ÂºÄÂèëËÄÖÁöÑËÆ§ÂèØÔºåÊçÆ‰ΩúËÄÖ‰∏çÂÆåÂÖ®ÁªüËÆ°Ôºå ‰ΩøÁî®Êú¨È°πÁõÆÁöÑÂÖ¨Âè∏ÂåÖÊã¨Áü•ÂêçÁöÑ‰∫íËÅîÁΩëÂ∑®Â§¥„ÄÅÂõΩÂÜÖÊéíÂêçÂâçÂàóÁöÑ‰∫ëÊúçÂä°ÂÖ¨Âè∏„ÄÅÂ§öÂÆ∂Áü•ÂêçÁöÑAIÁã¨ËßíÂÖΩÂÖ¨Âè∏Ôºå ‰ª•Âèä‰∏ÄÁ≥ªÂàó‰∏≠Â∞èÂûãÂÖ¨Âè∏„ÄÇ‰ΩøÁî®ËÄÖÂèØ‰ª•ÈÄöËøáÂú® &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/issues/511&#34;&gt;issue&lt;/a&gt; ‰∏äÁ≤òË¥¥ÂÖ¨Âè∏ÁöÑÂ§ßÂêçÂíåÁõ∏ÂÖ≥È°πÁõÆ‰ªãÁªç‰∏∫Êú¨È°πÁõÆËÉå‰π¶ÔºåÊÑüË∞¢ÊîØÊåÅÔºÅ&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>protocolbuffers/protobuf</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/protocolbuffers/protobuf</id>
    <link href="https://github.com/protocolbuffers/protobuf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2008 Google Inc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf&#39;s documentation on the Google Developers site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Compiler Installation&lt;/h2&gt; &#xA;&lt;p&gt;The protocol compiler is written in C++. If you are using C++, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; &#xA;&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf/releases&#34;&gt;https://github.com/protocolbuffers/protobuf/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the maven repo here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&#34;&gt;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; &#xA;&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&#34;&gt;src&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&#34;&gt;java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&#34;&gt;objectivec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C#&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&#34;&gt;csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&#34;&gt;ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf-go&#34;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&#34;&gt;php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dart-lang/protobuf&#34;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The best way to learn how to use protobuf is to follow the tutorials in our developer guide:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;https://developers.google.com/protocol-buffers/docs/tutorials&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The complete documentation for Protocol Buffers is available via the web at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/react-native-windows</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/microsoft/react-native-windows</id>
    <link href="https://github.com/microsoft/react-native-windows" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A framework for building native Windows apps with React.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; React Native for Windows &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Build native Windows apps with React. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/react-native-windows/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;React Native for Windows is released under the MIT license.&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.npmjs.org/package/react-native-windows&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/v/react-native-windows?color=e80441&amp;amp;label=react-native-windows&#34; alt=&#34;Current npm package version.&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/react-native-windows#contributing&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs welcome!&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/microsoft/react-native-windows/raw/main/.github/hero2.png&#34; alt=&#34;Hero Image with Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;See the official &lt;a href=&#34;https://reactnative.dev/&#34;&gt;React Native website&lt;/a&gt; for an introduction to React Native.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://reactnative.dev&#34;&gt;React Native&lt;/a&gt; is a framework developed by Facebook that enables you to build world-class application experiences on native platforms using a consistent developer experience based on JavaScript and &lt;a href=&#34;https://reactjs.org/&#34;&gt;React&lt;/a&gt;. The focus of React Native is on developer efficiency across all the platforms you care about - learn once, write anywhere.&lt;/p&gt; &#xA;&lt;p&gt;This repository adds support for the &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/downloads&#34;&gt;Windows 10 SDK&lt;/a&gt;, which allows you to build apps for &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/get-started-windows-10&#34;&gt;all devices supported by Windows 10&lt;/a&gt; including PCs, tablets, 2-in-1s, Xbox, Mixed reality devices etc.&lt;/p&gt; &#xA;&lt;p&gt;Visit the official &lt;a href=&#34;https://microsoft.github.io/react-native-windows&#34;&gt;React Native for Windows + macOS website&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/#code-of-conduct&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Status and roadmap&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://microsoft.github.io/react-native-windows/blog/&#34;&gt;Check out our blog&lt;/a&gt; if you&#39;d like to stay up to date on the status of React Native for Windows and check out current and past roadmaps. We will post all new releases, updates and general news about the project there.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You can run React Native Windows apps only on devices supported by the &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/downloads&#34;&gt;Windows 10 SDK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a full and detailed list of the system requirements and how to set up your development platform, see our &lt;a href=&#34;https://microsoft.github.io/react-native-windows/docs/rnw-dependencies&#34;&gt;System Requirements&lt;/a&gt; documentation on our website.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://microsoft.github.io/react-native-windows/docs/getting-started&#34;&gt;Getting Started Guide&lt;/a&gt; on our React Native for Windows + macOS website to build your first React Native for Windows app.&lt;/p&gt; &#xA;&lt;h3&gt;Logging Issues&lt;/h3&gt; &#xA;&lt;p&gt;Search the &lt;a href=&#34;https://github.com/microsoft/react-native-windows/issues&#34;&gt;existing issues&lt;/a&gt; and try to make sure your problem doesn‚Äôt already exist before opening a new issue. If your issue doesn&#39;t exist yet, try to make sure you provide as much information as possible to us so we can help you sooner. It‚Äôs helpful if you include information like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The version of Windows, React Native, React Native Windows extension, and device family (i.e., mobile, desktop, Xbox, etc.) where you ran into the issue.&lt;/li&gt; &#xA; &lt;li&gt;A stack trace and reduced repro case when possible.&lt;/li&gt; &#xA; &lt;li&gt;Ensure the &lt;a href=&#34;https://github.com/microsoft/react-native-windows/issues/new/choose&#34;&gt;appropriate template&lt;/a&gt; is used when filing your issue(s).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/microsoft/react-native-windows/raw/main/CONTRIBUTING.md&#34;&gt;Contributing guidelines&lt;/a&gt; for how to setup your fork of the repo and start a PR to contribute to React Native for Windows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/react-native-windows/labels/good%20first%20issue&#34;&gt;good first issue&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/react-native-windows/labels/help%20wanted&#34;&gt;help wanted&lt;/a&gt; are great starting points for PRs.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://reactnative.dev/docs/getting-started&#34;&gt;React Native already has great documentation&lt;/a&gt; and we&#39;re working to ensure the React Native Windows is part of that documentation story.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://microsoft.github.io/react-native-windows/&#34;&gt;React Native for Windows&lt;/a&gt; has it&#39;s own separate documentation site where Windows and macOS specific information, like API docs and blog updates live.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using the CLI in the &lt;a href=&#34;https://microsoft.github.io/react-native-windows/docs/getting-started&#34;&gt;Getting Started&lt;/a&gt; guide will set you up with a sample React Native for Windows app that you can begin editing right away.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re looking for sample code, just browse the RNTester folder in the GitHub web UI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The React Native Windows extension, including modifications to the original Facebook source code, and all newly contributed code is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/react-native-windows/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;. Portions of the React Native Windows extension derived from React Native are copyright Facebook.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>abseil/abseil-cpp</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/abseil/abseil-cpp</id>
    <link href="https://github.com/abseil/abseil-cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Abseil Common Libraries (C++)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Abseil - C++ Common Libraries&lt;/h1&gt; &#xA;&lt;p&gt;The repository contains the Abseil C++ library code. Abseil is an open-source collection of C++ code (compliant to C++11) designed to augment the C++ standard library.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#about&#34;&gt;About Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#build&#34;&gt;Building Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#codemap&#34;&gt;Codemap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#releases&#34;&gt;Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#links&#34;&gt;Links&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;about&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About Abseil&lt;/h2&gt; &#xA;&lt;p&gt;Abseil is an open-source collection of C++ library code designed to augment the C++ standard library. The Abseil library code is collected from Google&#39;s own C++ code base, has been extensively tested and used in production, and is the same code we depend on in our daily coding lives.&lt;/p&gt; &#xA;&lt;p&gt;In some cases, Abseil provides pieces missing from the C++ standard; in others, Abseil provides alternatives to the standard for special needs we&#39;ve found through usage in the Google code base. We denote those cases clearly within the library code we provide you.&lt;/p&gt; &#xA;&lt;p&gt;Abseil is not meant to be a competitor to the standard library; we&#39;ve just found that many of these utilities serve a purpose within our code base, and we now want to provide those resources to the C++ community as a whole.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;If you want to just get started, make sure you at least run through the &lt;a href=&#34;https://abseil.io/docs/cpp/quickstart&#34;&gt;Abseil Quickstart&lt;/a&gt;. The Quickstart contains information about setting up your development environment, downloading the Abseil code, running tests, and getting a simple binary working.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Building Abseil&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bazel.build&#34;&gt;Bazel&lt;/a&gt; and &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; are the official build systems for Abseil.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://abseil.io/docs/cpp/quickstart&#34;&gt;quickstart&lt;/a&gt; for more information on building Abseil using the Bazel build system.&lt;/p&gt; &#xA;&lt;p&gt;If you require CMake support, please check the &lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/CMake/README.md&#34;&gt;CMake build instructions&lt;/a&gt; and &lt;a href=&#34;https://abseil.io/docs/cpp/quickstart-cmake&#34;&gt;CMake Quickstart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Abseil is officially supported on many platforms. See the &lt;a href=&#34;https://abseil.io/docs/cpp/platforms/platforms&#34;&gt;Abseil platform support guide&lt;/a&gt; for details on supported operating systems, compilers, CPUs, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Codemap&lt;/h2&gt; &#xA;&lt;p&gt;Abseil contains the following C++ library components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/base/&#34;&gt;&lt;code&gt;base&lt;/code&gt;&lt;/a&gt; Abseil Fundamentals &lt;br&gt; The &lt;code&gt;base&lt;/code&gt; library contains initialization code and other code which all other Abseil code depends on. Code within &lt;code&gt;base&lt;/code&gt; may not depend on any other code (other than the C++ standard library).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/algorithm/&#34;&gt;&lt;code&gt;algorithm&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;algorithm&lt;/code&gt; library contains additions to the C++ &lt;code&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; library and container-based versions of such algorithms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/cleanup/&#34;&gt;&lt;code&gt;cleanup&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;cleanup&lt;/code&gt; library contains the control-flow-construct-like type &lt;code&gt;absl::Cleanup&lt;/code&gt; which is used for executing a callback on scope exit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/container/&#34;&gt;&lt;code&gt;container&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;container&lt;/code&gt; library contains additional STL-style containers, including Abseil&#39;s unordered &#34;Swiss table&#34; containers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/debugging/&#34;&gt;&lt;code&gt;debugging&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;debugging&lt;/code&gt; library contains code useful for enabling leak checks, and stacktrace and symbolization utilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/hash/&#34;&gt;&lt;code&gt;hash&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;hash&lt;/code&gt; library contains the hashing framework and default hash functor implementations for hashable types in Abseil.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/memory/&#34;&gt;&lt;code&gt;memory&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;memory&lt;/code&gt; library contains C++11-compatible versions of &lt;code&gt;std::make_unique()&lt;/code&gt; and related memory management facilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/meta/&#34;&gt;&lt;code&gt;meta&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;meta&lt;/code&gt; library contains C++11-compatible versions of type checks available within C++14 and C++17 versions of the C++ &lt;code&gt;&amp;lt;type_traits&amp;gt;&lt;/code&gt; library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/numeric/&#34;&gt;&lt;code&gt;numeric&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;numeric&lt;/code&gt; library contains C++11-compatible 128-bit integers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/profiling/&#34;&gt;&lt;code&gt;profiling&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;profiling&lt;/code&gt; library contains utility code for profiling C++ entities. It is currently a private dependency of other Abseil libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/status/&#34;&gt;&lt;code&gt;status&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;status&lt;/code&gt; contains abstractions for error handling, specifically &lt;code&gt;absl::Status&lt;/code&gt; and &lt;code&gt;absl::StatusOr&amp;lt;T&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/strings/&#34;&gt;&lt;code&gt;strings&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;strings&lt;/code&gt; library contains a variety of strings routines and utilities, including a C++11-compatible version of the C++17 &lt;code&gt;std::string_view&lt;/code&gt; type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/synchronization/&#34;&gt;&lt;code&gt;synchronization&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;synchronization&lt;/code&gt; library contains concurrency primitives (Abseil&#39;s &lt;code&gt;absl::Mutex&lt;/code&gt; class, an alternative to &lt;code&gt;std::mutex&lt;/code&gt;) and a variety of synchronization abstractions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/time/&#34;&gt;&lt;code&gt;time&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;time&lt;/code&gt; library contains abstractions for computing with absolute points in time, durations of time, and formatting and parsing time within time zones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/types/&#34;&gt;&lt;code&gt;types&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;types&lt;/code&gt; library contains non-container utility types, like a C++11-compatible version of the C++17 &lt;code&gt;std::optional&lt;/code&gt; type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/utility/&#34;&gt;&lt;code&gt;utility&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;utility&lt;/code&gt; library contains utility and helper code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;Abseil recommends users &#34;live-at-head&#34; (update to the latest commit from the master branch as often as possible). However, we realize this philosophy doesn&#39;t work for every project, so we also provide &lt;a href=&#34;https://github.com/abseil/abseil-cpp/releases&#34;&gt;Long Term Support Releases&lt;/a&gt; to which we backport fixes for severe bugs. See our &lt;a href=&#34;https://abseil.io/about/releases&#34;&gt;release management&lt;/a&gt; document for more details.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Abseil C++ library is licensed under the terms of the Apache license. See &lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;p&gt;For more information about Abseil:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consult our &lt;a href=&#34;https://abseil.io/about/intro&#34;&gt;Abseil Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read &lt;a href=&#34;https://abseil.io/about/philosophy&#34;&gt;Why Adopt Abseil&lt;/a&gt; to understand our design philosophy.&lt;/li&gt; &#xA; &lt;li&gt;Peruse our &lt;a href=&#34;https://abseil.io/about/compatibility&#34;&gt;Abseil Compatibility Guarantees&lt;/a&gt; to understand both what we promise to you, and what we expect of you in return.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>tinyobjloader/tinyobjloader</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/tinyobjloader/tinyobjloader</id>
    <link href="https://github.com/tinyobjloader/tinyobjloader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tiny but powerful single file wavefront obj loader&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tinyobjloader&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/tinyobjloader/tinyobjloader&#34;&gt;&lt;img src=&#34;https://travis-ci.org/tinyobjloader/tinyobjloader.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/tinyobjloader/tinyobjloader/_build/latest?definitionId=1&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/tinyobjloader/tinyobjloader/_apis/build/status/tinyobjloader.tinyobjloader?branchName=master&#34; alt=&#34;AZ Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ci.appveyor.com/project/syoyo/tinyobjloader-6e4qf/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/m6wfkvket7gth8wn/branch/master?svg=true&#34; alt=&#34;AppVeyor Build status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://coveralls.io/github/syoyo/tinyobjloader?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/github/syoyo/tinyobjloader/badge.svg?branch=master&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aur.archlinux.org/packages/tinyobjloader&#34;&gt;&lt;img src=&#34;https://img.shields.io/aur/version/tinyobjloader?logo=arch-linux&#34; alt=&#34;AUR version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tiny but powerful single file wavefront obj loader written in C++03. No dependency except for C++ STL. It can parse over 10M polygons with moderate memory and time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tinyobjloader&lt;/code&gt; is good for embedding .obj loader to your (global illumination) renderer ;-)&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for C89 version, please see &lt;a href=&#34;https://github.com/syoyo/tinyobjloader-c&#34;&gt;https://github.com/syoyo/tinyobjloader-c&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Version notice&lt;/h2&gt; &#xA;&lt;p&gt;We recommend to use &lt;code&gt;master&lt;/code&gt;(&lt;code&gt;main&lt;/code&gt;) branch. Its v2.0 release candidate. Most features are now nearly robust and stable(Remaining task for release v2.0 is polishing C++ and Python API).&lt;/p&gt; &#xA;&lt;p&gt;We have released new version v1.0.0 on 20 Aug, 2016. Old version is available as &lt;code&gt;v0.9.x&lt;/code&gt; branch &lt;a href=&#34;https://github.com/syoyo/tinyobjloader/tree/v0.9.x&#34;&gt;https://github.com/syoyo/tinyobjloader/tree/v0.9.x&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s new&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;29 Jul, 2021 : Added Mapbox&#39;s earcut for robust triangulation. Also fixes triangulation bug.&lt;/li&gt; &#xA; &lt;li&gt;19 Feb, 2020 : The repository has been moved to &lt;a href=&#34;https://github.com/tinyobjloader/tinyobjloader&#34;&gt;https://github.com/tinyobjloader/tinyobjloader&lt;/a&gt; !&lt;/li&gt; &#xA; &lt;li&gt;18 May, 2019 : Python binding!(See &lt;code&gt;python&lt;/code&gt; folder. Also see &lt;a href=&#34;https://pypi.org/project/tinyobjloader/&#34;&gt;https://pypi.org/project/tinyobjloader/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;14 Apr, 2019 : Bump version v2.0.0 rc0. New C++ API and python bindings!(1.x API still exists for backward compatibility)&lt;/li&gt; &#xA; &lt;li&gt;20 Aug, 2016 : Bump version v1.0.0. New data structure and API!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;C++03 compiler&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Old version&lt;/h3&gt; &#xA;&lt;p&gt;Previous old version is available in &lt;code&gt;v0.9.x&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tinyobjloader/tinyobjloader/master/images/rungholt.jpg&#34; alt=&#34;Rungholt&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;tinyobjloader can successfully load 6M triangles Rungholt scene. &lt;a href=&#34;http://casual-effects.com/data/index.html&#34;&gt;http://casual-effects.com/data/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tinyobjloader/tinyobjloader/master/images/sanmugel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tinyobjloader/tinyobjloader/master/examples/viewer&#34;&gt;examples/viewer/&lt;/a&gt; OpenGL .obj viewer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tinyobjloader/tinyobjloader/master/examples/callback_api/&#34;&gt;examples/callback_api/&lt;/a&gt; Callback API example&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tinyobjloader/tinyobjloader/master/examples/voxelize/&#34;&gt;examples/voxelize/&lt;/a&gt; Voxelizer example&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use case&lt;/h2&gt; &#xA;&lt;p&gt;TinyObjLoader is successfully used in ...&lt;/p&gt; &#xA;&lt;h3&gt;New version(v1.0.x)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Double precision support through &lt;code&gt;TINYOBJLOADER_USE_DOUBLE&lt;/code&gt; thanks to noma&lt;/li&gt; &#xA; &lt;li&gt;Loading models in Vulkan Tutorial &lt;a href=&#34;https://vulkan-tutorial.com/Loading_models&#34;&gt;https://vulkan-tutorial.com/Loading_models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;.obj viewer with Metal &lt;a href=&#34;https://github.com/middlefeng/NuoModelViewer/tree/master&#34;&gt;https://github.com/middlefeng/NuoModelViewer/tree/master&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vulkan Cookbook &lt;a href=&#34;https://github.com/PacktPublishing/Vulkan-Cookbook&#34;&gt;https://github.com/PacktPublishing/Vulkan-Cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cudabox: CUDA Solid Voxelizer Engine &lt;a href=&#34;https://github.com/gaspardzoss/cudavox&#34;&gt;https://github.com/gaspardzoss/cudavox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Drake: A planning, control, and analysis toolbox for nonlinear dynamical systems &lt;a href=&#34;https://github.com/RobotLocomotion/drake&#34;&gt;https://github.com/RobotLocomotion/drake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VFPR - a Vulkan Forward Plus Renderer : &lt;a href=&#34;https://github.com/WindyDarian/Vulkan-Forward-Plus-Renderer&#34;&gt;https://github.com/WindyDarian/Vulkan-Forward-Plus-Renderer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;glslViewer: &lt;a href=&#34;https://github.com/patriciogonzalezvivo/glslViewer&#34;&gt;https://github.com/patriciogonzalezvivo/glslViewer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Lighthouse2: &lt;a href=&#34;https://github.com/jbikker/lighthouse2&#34;&gt;https://github.com/jbikker/lighthouse2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;rayrender(an open source R package for raytracing scenes in created in R): &lt;a href=&#34;https://github.com/tylermorganwall/rayrender&#34;&gt;https://github.com/tylermorganwall/rayrender&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;liblava - A modern C++ and easy-to-use framework for the Vulkan API. [MIT]: &lt;a href=&#34;https://github.com/liblava/liblava&#34;&gt;https://github.com/liblava/liblava&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;rtxON - Simple Vulkan raytracing tutorials &lt;a href=&#34;https://github.com/iOrange/rtxON&#34;&gt;https://github.com/iOrange/rtxON&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;metal-ray-tracer - Writing ray-tracer using Metal Performance Shaders &lt;a href=&#34;https://github.com/sergeyreznik/metal-ray-tracer&#34;&gt;https://github.com/sergeyreznik/metal-ray-tracer&lt;/a&gt; &lt;a href=&#34;https://sergeyreznik.github.io/metal-ray-tracer/index.html&#34;&gt;https://sergeyreznik.github.io/metal-ray-tracer/index.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Your project here! (Letting us know via github issue is welcome!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Old version(v0.9.x)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;bullet3 &lt;a href=&#34;https://github.com/erwincoumans/bullet3&#34;&gt;https://github.com/erwincoumans/bullet3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pbrt-v2 &lt;a href=&#34;https://github.com/mmp/pbrt-v2&#34;&gt;https://github.com/mmp/pbrt-v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenGL game engine development &lt;a href=&#34;http://swarminglogic.com/jotting/2013_10_gamedev01&#34;&gt;http://swarminglogic.com/jotting/2013_10_gamedev01&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;mallie &lt;a href=&#34;https://lighttransport.github.io/mallie&#34;&gt;https://lighttransport.github.io/mallie&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;IBLBaker (Image Based Lighting Baker). &lt;a href=&#34;http://www.derkreature.com/iblbaker/&#34;&gt;http://www.derkreature.com/iblbaker/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stanford CS148 &lt;a href=&#34;http://web.stanford.edu/class/cs148/assignments/assignment3.pdf&#34;&gt;http://web.stanford.edu/class/cs148/assignments/assignment3.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Awesome Bump &lt;a href=&#34;http://awesomebump.besaba.com/about/&#34;&gt;http://awesomebump.besaba.com/about/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;sdlgl3-wavefront OpenGL .obj viewer &lt;a href=&#34;https://github.com/chrisliebert/sdlgl3-wavefront&#34;&gt;https://github.com/chrisliebert/sdlgl3-wavefront&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pbrt-v3 &lt;a href=&#34;https://github.com/mmp/pbrt-v3&#34;&gt;https://github.com/mmp/pbrt-v3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cocos2d-x &lt;a href=&#34;https://github.com/cocos2d/cocos2d-x/&#34;&gt;https://github.com/cocos2d/cocos2d-x/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Android Vulkan demo &lt;a href=&#34;https://github.com/SaschaWillems/Vulkan&#34;&gt;https://github.com/SaschaWillems/Vulkan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;voxelizer &lt;a href=&#34;https://github.com/karimnaaji/voxelizer&#34;&gt;https://github.com/karimnaaji/voxelizer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Probulator &lt;a href=&#34;https://github.com/kayru/Probulator&#34;&gt;https://github.com/kayru/Probulator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OptiX Prime baking &lt;a href=&#34;https://github.com/nvpro-samples/optix_prime_baking&#34;&gt;https://github.com/nvpro-samples/optix_prime_baking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FireRays SDK &lt;a href=&#34;https://github.com/GPUOpen-LibrariesAndSDKs/FireRays_SDK&#34;&gt;https://github.com/GPUOpen-LibrariesAndSDKs/FireRays_SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;parg, tiny C library of various graphics utilities and GL demos &lt;a href=&#34;https://github.com/prideout/parg&#34;&gt;https://github.com/prideout/parg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Opengl unit of ChronoEngine &lt;a href=&#34;https://github.com/projectchrono/chrono-opengl&#34;&gt;https://github.com/projectchrono/chrono-opengl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Point Based Global Illumination on modern GPU &lt;a href=&#34;https://pbgi.wordpress.com/code-source/&#34;&gt;https://pbgi.wordpress.com/code-source/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fast OBJ file importing and parsing in CUDA &lt;a href=&#34;http://researchonline.jcu.edu.au/42515/1/2015.CVM.OBJCUDA.pdf&#34;&gt;http://researchonline.jcu.edu.au/42515/1/2015.CVM.OBJCUDA.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sorted Shading for Uni-Directional Pathtracing by Joshua Bainbridge &lt;a href=&#34;https://nccastaff.bournemouth.ac.uk/jmacey/MastersProjects/MSc15/02Josh/joshua_bainbridge_thesis.pdf&#34;&gt;https://nccastaff.bournemouth.ac.uk/jmacey/MastersProjects/MSc15/02Josh/joshua_bainbridge_thesis.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GeeXLab &lt;a href=&#34;http://www.geeks3d.com/hacklab/20160531/geexlab-0-12-0-0-released-for-windows/&#34;&gt;http://www.geeks3d.com/hacklab/20160531/geexlab-0-12-0-0-released-for-windows/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Group(parse multiple group name)&lt;/li&gt; &#xA; &lt;li&gt;Vertex &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vertex color(as an extension: &lt;a href=&#34;https://blender.stackexchange.com/questions/31997/how-can-i-get-vertex-painted-obj-files-to-import-into-blender&#34;&gt;https://blender.stackexchange.com/questions/31997/how-can-i-get-vertex-painted-obj-files-to-import-into-blender&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Texcoord&lt;/li&gt; &#xA; &lt;li&gt;Normal&lt;/li&gt; &#xA; &lt;li&gt;Material &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unknown material attributes are returned as key-value(value is string) map.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Crease tag(&#39;t&#39;). This is OpenSubdiv specific(not in wavefront .obj specification)&lt;/li&gt; &#xA; &lt;li&gt;PBR material extension for .MTL. Its proposed here: &lt;a href=&#34;http://exocortex.com/blog/extending_wavefront_mtl_to_support_pbr&#34;&gt;http://exocortex.com/blog/extending_wavefront_mtl_to_support_pbr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Callback API for custom loading.&lt;/li&gt; &#xA; &lt;li&gt;Double precision support(for HPC application).&lt;/li&gt; &#xA; &lt;li&gt;Smoothing group&lt;/li&gt; &#xA; &lt;li&gt;Python binding : See &lt;code&gt;python&lt;/code&gt; folder. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Precompiled binary(manylinux1-x86_64 only) is hosted at pypi &lt;a href=&#34;https://pypi.org/project/tinyobjloader/&#34;&gt;https://pypi.org/project/tinyobjloader/&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Primitives&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; face(&lt;code&gt;f&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; lines(&lt;code&gt;l&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; points(&lt;code&gt;p&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; curve&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 2D curve&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; surface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Free form curve/surfaces&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix obj_sticker example.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More unit test codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Texture options&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;TinyObjLoader is licensed under MIT license.&lt;/p&gt; &#xA;&lt;h3&gt;Third party licenses.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pybind11 : BSD-style license.&lt;/li&gt; &#xA; &lt;li&gt;mapbox earcut.hpp: ISC License.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;One option is to simply copy the header file into your project and to make sure that &lt;code&gt;TINYOBJLOADER_IMPLEMENTATION&lt;/code&gt; is defined exactly once.&lt;/p&gt; &#xA;&lt;h3&gt;Building tinyobjloader - Using vcpkg(not recommended though)&lt;/h3&gt; &#xA;&lt;p&gt;Alghouth it is not a recommended way, you can download and install tinyobjloader using the &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Microsoft/vcpkg.git&#xA;cd vcpkg&#xA;./bootstrap-vcpkg.sh&#xA;./vcpkg integrate install&#xA;./vcpkg install tinyobjloader&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The tinyobjloader port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h3&gt;Data format&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;attrib_t&lt;/code&gt; contains single and linear array of vertex data(position, normal and texcoord).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;attrib_t::vertices =&amp;gt; 3 floats per vertex&#xA;&#xA;       v[0]        v[1]        v[2]        v[3]               v[n-1]&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;  | x | y | z | x | y | z | x | y | z | x | y | z | .... | x | y | z |&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;&#xA;attrib_t::normals =&amp;gt; 3 floats per vertex&#xA;&#xA;       n[0]        n[1]        n[2]        n[3]               n[n-1]&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;  | x | y | z | x | y | z | x | y | z | x | y | z | .... | x | y | z |&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;&#xA;attrib_t::texcoords =&amp;gt; 2 floats per vertex&#xA;&#xA;       t[0]        t[1]        t[2]        t[3]               t[n-1]&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;  |  u  |  v  |  u  |  v  |  u  |  v  |  u  |  v  | .... |  u  |  v  |&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;&#xA;attrib_t::colors =&amp;gt; 3 floats per vertex(vertex color. optional)&#xA;&#xA;       c[0]        c[1]        c[2]        c[3]               c[n-1]&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;  | x | y | z | x | y | z | x | y | z | x | y | z | .... | x | y | z |&#xA;  +-----------+-----------+-----------+-----------+      +-----------+&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each &lt;code&gt;shape_t::mesh_t&lt;/code&gt; does not contain vertex data but contains array index to &lt;code&gt;attrib_t&lt;/code&gt;. See &lt;code&gt;loader_example.cc&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;mesh_t::indices =&amp;gt; array of vertex indices.&#xA;&#xA;  +----+----+----+----+----+----+----+----+----+----+     +--------+&#xA;  | i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7 | i8 | i9 | ... | i(n-1) |&#xA;  +----+----+----+----+----+----+----+----+----+----+     +--------+&#xA;&#xA;Each index has an array index to attrib_t::vertices, attrib_t::normals and attrib_t::texcoords.&#xA;&#xA;mesh_t::num_face_vertices =&amp;gt; array of the number of vertices per face(e.g. 3 = triangle, 4 = quad , 5 or more = N-gons).&#xA;&#xA;&#xA;  +---+---+---+        +---+&#xA;  | 3 | 4 | 3 | ...... | 3 |&#xA;  +---+---+---+        +---+&#xA;    |   |   |            |&#xA;    |   |   |            +-----------------------------------------+&#xA;    |   |   |                                                      |&#xA;    |   |   +------------------------------+                       |&#xA;    |   |                                  |                       |&#xA;    |   +------------------+               |                       |&#xA;    |                      |               |                       |&#xA;    |/                     |/              |/                      |/&#xA;&#xA; mesh_t::indices&#xA;&#xA;  |    face[0]   |       face[1]     |    face[2]   |     |      face[n-1]           |&#xA;  +----+----+----+----+----+----+----+----+----+----+     +--------+--------+--------+&#xA;  | i0 | i1 | i2 | i3 | i4 | i5 | i6 | i7 | i8 | i9 | ... | i(n-3) | i(n-2) | i(n-1) |&#xA;  +----+----+----+----+----+----+----+----+----+----+     +--------+--------+--------+&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that when &lt;code&gt;triangulate&lt;/code&gt; flag is true in &lt;code&gt;tinyobj::LoadObj()&lt;/code&gt; argument, &lt;code&gt;num_face_vertices&lt;/code&gt; are all filled with 3(triangle).&lt;/p&gt; &#xA;&lt;h3&gt;float data type&lt;/h3&gt; &#xA;&lt;p&gt;TinyObjLoader now use &lt;code&gt;real_t&lt;/code&gt; for floating point data type. Default is &lt;code&gt;float(32bit)&lt;/code&gt;. You can enable &lt;code&gt;double(64bit)&lt;/code&gt; precision by using &lt;code&gt;TINYOBJLOADER_USE_DOUBLE&lt;/code&gt; define.&lt;/p&gt; &#xA;&lt;h3&gt;Robust triangulation&lt;/h3&gt; &#xA;&lt;p&gt;When you enable &lt;code&gt;triangulation&lt;/code&gt;(default is enabled), TinyObjLoader triangulate polygons(faces with 4 or more vertices).&lt;/p&gt; &#xA;&lt;p&gt;Built-in trinagulation code may not work well in some polygon shape.&lt;/p&gt; &#xA;&lt;p&gt;You can define &lt;code&gt;TINYOBJLOADER_USE_MAPBOX_EARCUT&lt;/code&gt; for robust triangulation using &lt;code&gt;mapbox/earcut.hpp&lt;/code&gt;. This requires C++11 compiler though. And you need to copy &lt;code&gt;mapbox/earcut.hpp&lt;/code&gt; to your project. If you have your own &lt;code&gt;mapbox/earcut.hpp&lt;/code&gt; file incuded in your project, you can define &lt;code&gt;TINYOBJLOADER_DONOT_INCLUDE_MAPBOX_EARCUT&lt;/code&gt; so that &lt;code&gt;mapbox/earcut.hpp&lt;/code&gt; is not included inside of &lt;code&gt;tiny_obj_loader.h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Example code (Deprecated API)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#define TINYOBJLOADER_IMPLEMENTATION // define this in only *one* .cc&#xA;// Optional. define TINYOBJLOADER_USE_MAPBOX_EARCUT gives robust trinagulation. Requires C++11&#xA;//#define TINYOBJLOADER_USE_MAPBOX_EARCUT&#xA;#include &#34;tiny_obj_loader.h&#34;&#xA;&#xA;std::string inputfile = &#34;cornell_box.obj&#34;;&#xA;tinyobj::attrib_t attrib;&#xA;std::vector&amp;lt;tinyobj::shape_t&amp;gt; shapes;&#xA;std::vector&amp;lt;tinyobj::material_t&amp;gt; materials;&#xA;&#xA;std::string warn;&#xA;std::string err;&#xA;&#xA;bool ret = tinyobj::LoadObj(&amp;amp;attrib, &amp;amp;shapes, &amp;amp;materials, &amp;amp;warn, &amp;amp;err, inputfile.c_str());&#xA;&#xA;if (!warn.empty()) {&#xA;  std::cout &amp;lt;&amp;lt; warn &amp;lt;&amp;lt; std::endl;&#xA;}&#xA;&#xA;if (!err.empty()) {&#xA;  std::cerr &amp;lt;&amp;lt; err &amp;lt;&amp;lt; std::endl;&#xA;}&#xA;&#xA;if (!ret) {&#xA;  exit(1);&#xA;}&#xA;&#xA;// Loop over shapes&#xA;for (size_t s = 0; s &amp;lt; shapes.size(); s++) {&#xA;  // Loop over faces(polygon)&#xA;  size_t index_offset = 0;&#xA;  for (size_t f = 0; f &amp;lt; shapes[s].mesh.num_face_vertices.size(); f++) {&#xA;    size_t fv = size_t(shapes[s].mesh.num_face_vertices[f]);&#xA;&#xA;    // Loop over vertices in the face.&#xA;    for (size_t v = 0; v &amp;lt; fv; v++) {&#xA;      // access to vertex&#xA;      tinyobj::index_t idx = shapes[s].mesh.indices[index_offset + v];&#xA;&#xA;      tinyobj::real_t vx = attrib.vertices[3*size_t(idx.vertex_index)+0];&#xA;      tinyobj::real_t vy = attrib.vertices[3*size_t(idx.vertex_index)+1];&#xA;      tinyobj::real_t vz = attrib.vertices[3*size_t(idx.vertex_index)+2];&#xA;&#xA;      // Check if `normal_index` is zero or positive. negative = no normal data&#xA;      if (idx.normal_index &amp;gt;= 0) {&#xA;        tinyobj::real_t nx = attrib.normals[3*size_t(idx.normal_index)+0];&#xA;        tinyobj::real_t ny = attrib.normals[3*size_t(idx.normal_index)+1];&#xA;        tinyobj::real_t nz = attrib.normals[3*size_t(idx.normal_index)+2];&#xA;      }&#xA;&#xA;      // Check if `texcoord_index` is zero or positive. negative = no texcoord data&#xA;      if (idx.texcoord_index &amp;gt;= 0) {&#xA;        tinyobj::real_t tx = attrib.texcoords[2*size_t(idx.texcoord_index)+0];&#xA;        tinyobj::real_t ty = attrib.texcoords[2*size_t(idx.texcoord_index)+1];&#xA;      }&#xA;      // Optional: vertex colors&#xA;      // tinyobj::real_t red   = attrib.colors[3*size_t(idx.vertex_index)+0];&#xA;      // tinyobj::real_t green = attrib.colors[3*size_t(idx.vertex_index)+1];&#xA;      // tinyobj::real_t blue  = attrib.colors[3*size_t(idx.vertex_index)+2];&#xA;    }&#xA;    index_offset += fv;&#xA;&#xA;    // per-face material&#xA;    shapes[s].mesh.material_ids[f];&#xA;  }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example code (New Object Oriented API)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#define TINYOBJLOADER_IMPLEMENTATION // define this in only *one* .cc&#xA;// Optional. define TINYOBJLOADER_USE_MAPBOX_EARCUT gives robust trinagulation. Requires C++11&#xA;//#define TINYOBJLOADER_USE_MAPBOX_EARCUT&#xA;#include &#34;tiny_obj_loader.h&#34;&#xA;&#xA;&#xA;std::string inputfile = &#34;cornell_box.obj&#34;;&#xA;tinyobj::ObjReaderConfig reader_config;&#xA;reader_config.mtl_search_path = &#34;./&#34;; // Path to material files&#xA;&#xA;tinyobj::ObjReader reader;&#xA;&#xA;if (!reader.ParseFromFile(inputfile, reader_config)) {&#xA;  if (!reader.Error().empty()) {&#xA;      std::cerr &amp;lt;&amp;lt; &#34;TinyObjReader: &#34; &amp;lt;&amp;lt; reader.Error();&#xA;  }&#xA;  exit(1);&#xA;}&#xA;&#xA;if (!reader.Warning().empty()) {&#xA;  std::cout &amp;lt;&amp;lt; &#34;TinyObjReader: &#34; &amp;lt;&amp;lt; reader.Warning();&#xA;}&#xA;&#xA;auto&amp;amp; attrib = reader.GetAttrib();&#xA;auto&amp;amp; shapes = reader.GetShapes();&#xA;auto&amp;amp; materials = reader.GetMaterials();&#xA;&#xA;// Loop over shapes&#xA;for (size_t s = 0; s &amp;lt; shapes.size(); s++) {&#xA;  // Loop over faces(polygon)&#xA;  size_t index_offset = 0;&#xA;  for (size_t f = 0; f &amp;lt; shapes[s].mesh.num_face_vertices.size(); f++) {&#xA;    size_t fv = size_t(shapes[s].mesh.num_face_vertices[f]);&#xA;&#xA;    // Loop over vertices in the face.&#xA;    for (size_t v = 0; v &amp;lt; fv; v++) {&#xA;      // access to vertex&#xA;      tinyobj::index_t idx = shapes[s].mesh.indices[index_offset + v];&#xA;      tinyobj::real_t vx = attrib.vertices[3*size_t(idx.vertex_index)+0];&#xA;      tinyobj::real_t vy = attrib.vertices[3*size_t(idx.vertex_index)+1];&#xA;      tinyobj::real_t vz = attrib.vertices[3*size_t(idx.vertex_index)+2];&#xA;&#xA;      // Check if `normal_index` is zero or positive. negative = no normal data&#xA;      if (idx.normal_index &amp;gt;= 0) {&#xA;        tinyobj::real_t nx = attrib.normals[3*size_t(idx.normal_index)+0];&#xA;        tinyobj::real_t ny = attrib.normals[3*size_t(idx.normal_index)+1];&#xA;        tinyobj::real_t nz = attrib.normals[3*size_t(idx.normal_index)+2];&#xA;      }&#xA;&#xA;      // Check if `texcoord_index` is zero or positive. negative = no texcoord data&#xA;      if (idx.texcoord_index &amp;gt;= 0) {&#xA;        tinyobj::real_t tx = attrib.texcoords[2*size_t(idx.texcoord_index)+0];&#xA;        tinyobj::real_t ty = attrib.texcoords[2*size_t(idx.texcoord_index)+1];&#xA;      }&#xA;&#xA;      // Optional: vertex colors&#xA;      // tinyobj::real_t red   = attrib.colors[3*size_t(idx.vertex_index)+0];&#xA;      // tinyobj::real_t green = attrib.colors[3*size_t(idx.vertex_index)+1];&#xA;      // tinyobj::real_t blue  = attrib.colors[3*size_t(idx.vertex_index)+2];&#xA;    }&#xA;    index_offset += fv;&#xA;&#xA;    // per-face material&#xA;    shapes[s].mesh.material_ids[f];&#xA;  }&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Optimized loader&lt;/h2&gt; &#xA;&lt;p&gt;Optimized multi-threaded .obj loader is available at &lt;code&gt;experimental/&lt;/code&gt; directory. If you want absolute performance to load .obj data, this optimized loader will fit your purpose. Note that the optimized loader uses C++11 thread and it does less error checks but may work most .obj data.&lt;/p&gt; &#xA;&lt;p&gt;Here is some benchmark result. Time are measured on MacBook 12(Early 2016, Core m5 1.2GHz).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rungholt scene(6M triangles) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;old version(v0.9.x): 15500 msecs.&lt;/li&gt; &#xA;   &lt;li&gt;baseline(v1.0.x): 6800 msecs(2.3x faster than old version)&lt;/li&gt; &#xA;   &lt;li&gt;optimised: 1500 msecs(10x faster than old version, 4.5x faster than baseline)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Python binding&lt;/h2&gt; &#xA;&lt;h3&gt;CI + PyPI upload&lt;/h3&gt; &#xA;&lt;p&gt;cibuildwheels + twine upload for each git tagging event is handled in Azure Pipeline.&lt;/p&gt; &#xA;&lt;h4&gt;How to bump version(For developer)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump version in CMakeLists.txt&lt;/li&gt; &#xA; &lt;li&gt;Update version in &lt;code&gt;python/setup.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Commit and push &lt;code&gt;master&lt;/code&gt;. Confirm C.I. build is OK.&lt;/li&gt; &#xA; &lt;li&gt;Create tag starting with &lt;code&gt;v&lt;/code&gt;(e.g. &lt;code&gt;v2.1.0&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git push --tags&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;cibuildwheels + pypi upload(through twine) will be automatically triggered in Azure Pipeline.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;Unit tests are provided in &lt;code&gt;tests&lt;/code&gt; directory. See &lt;code&gt;tests/README.md&lt;/code&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mamedev/mame</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/mamedev/mame</id>
    <link href="https://github.com/mamedev/mame" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MAME&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;MAME&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/mamedev/mame?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/mamedev/mame&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build status:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS/Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/GCC and clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Linux)/badge.svg?sanitize=true&#34; alt=&#34;CI (Linux)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows/MinGW GCC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Windows)/badge.svg?sanitize=true&#34; alt=&#34;CI (Windows)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS/clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(macOS)/badge.svg?sanitize=true&#34; alt=&#34;CI (macOS)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI Translations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Compile%20UI%20translations/badge.svg?sanitize=true&#34; alt=&#34;Compile UI translations&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Build%20documentation/badge.svg?sanitize=true&#34; alt=&#34;Build documentation&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Static analysis status for entire build (except for third-party parts of project):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scan.coverity.com/projects/mame-emulator&#34;&gt;&lt;img src=&#34;https://scan.coverity.com/projects/5727/badge.svg?flat=1&#34; alt=&#34;Coverity Scan Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is MAME?&lt;/h1&gt; &#xA;&lt;p&gt;MAME is a multi-purpose emulation framework.&lt;/p&gt; &#xA;&lt;p&gt;MAME&#39;s purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important &#34;vintage&#34; software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.&lt;/p&gt; &#xA;&lt;h1&gt;How to compile?&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re on a UNIX-like system (including Linux and macOS), it could be as easy as typing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MAME build,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=arcade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for an arcade-only build, or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=mess&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MESS build.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;http://docs.mamedev.org/initialsetup/compilingmame.html&#34;&gt;Compiling MAME&lt;/a&gt; page on our documentation site for more information, including prerequisites for macOS and popular Linux distributions.&lt;/p&gt; &#xA;&lt;p&gt;For recent versions of macOS you need to install &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; including command-line tools and &lt;a href=&#34;https://www.libsdl.org/download-2.0.php&#34;&gt;SDL 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Windows users, we provide a ready-made &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64.&lt;/p&gt; &#xA;&lt;p&gt;Visual Studio builds are also possible, but you still need &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64. In order to generate solution and project files just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or use this command to build it directly using msbuild&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019 MSBUILD=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Where can I find out more?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mamedev.org/&#34;&gt;Official MAME Development Team Site&lt;/a&gt; (includes binary downloads, wiki, forums, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mess.redump.net/&#34;&gt;Official MESS Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mametesters.org/&#34;&gt;MAME Testers&lt;/a&gt; (official bug tracker for MAME and MESS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Coding standard&lt;/h2&gt; &#xA;&lt;p&gt;MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.&lt;/p&gt; &#xA;&lt;p&gt;Some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#Allman_style&#34;&gt;Allman style&lt;/a&gt;; some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#K.26R_style&#34;&gt;K&amp;amp;R style&lt;/a&gt; -- mostly depending on who wrote the original version. &lt;strong&gt;Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source.&lt;/strong&gt; For new code, the majority tends to prefer Allman style, so if you don&#39;t care much, use that.&lt;/p&gt; &#xA;&lt;p&gt;All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD-3-Clause&lt;/a&gt; license, the &lt;a href=&#34;http://opensource.org/licenses/LGPL-2.1&#34;&gt;LGPL-2.1&lt;/a&gt;, or the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GPL-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The MAME project as a whole is made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GNU General Public License, version 2&lt;/a&gt; or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of the source files (over 90% including core files) are made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;3-clause BSD License&lt;/a&gt;, and we would encourage new contributors to make their contributions available under the terms of this license.&lt;/p&gt; &#xA;&lt;p&gt;Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the &#34;MAME&#34; name, logo, or wordmark.&lt;/p&gt; &#xA;&lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34; target=&#34;_blank&#34;&gt; &lt;img align=&#34;right&#34; src=&#34;http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (C) 1997-2021  MAMEDev and contributors&#xA;&#xA;This program is free software; you can redistribute it and/or modify it&#xA;under the terms of the GNU General Public License version 2, as provided in&#xA;docs/legal/GPL-2.0.&#xA;&#xA;This program is distributed in the hope that it will be useful, but WITHOUT&#xA;ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or&#xA;FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for&#xA;more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see COPYING for more details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neutralinojs/neutralinojs</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/neutralinojs/neutralinojs</id>
    <link href="https://github.com/neutralinojs/neutralinojs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Portable and lightweight cross-platform desktop application development framework&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://cdn.rawgit.com/neutralinojs/neutralinojs.github.io/b667f2c2/docs/nllogo.png&#34; style=&#34;width:300px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/neutralinojs/neutralinojs/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/neutralinojs/neutralinojs&#34; alt=&#34;GitHub release (latest by date)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/neutralinojs/neutralinojs/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/neutralinojs/neutralinojs.svg?sanitize=true&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/neutralinojs/neutralinojs/actions/workflows/test_suite.yml/badge.svg?sanitize=true&#34; alt=&#34;Build status&#34;&gt; &lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Fneutralinojs%2Fneutralinojs?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Fneutralinojs%2Fneutralinojs.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Neutralinojs is a lightweight and portable desktop application development framework. It lets you develop lightweight cross-platform desktop applications using JavaScript, HTML and CSS. Apps built with Neutralinojs can run on Linux, macOS, Windows, Web, and Chrome. Also, you can extend Neutralinojs with any programming language (via extensions IPC) and use Neutralinojs as a part of any source file (via child processes IPC).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Elanis/web-to-desktop-framework-comparison&#34;&gt;Neutralinojs vs Electron vs NW.JS vs Tauri vs NodeGui vs Flutter vs .Net MAUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/neutralinojs/evaluation&#34;&gt;Neutralinojs vs Electron vs NW.js (2018)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/neutralinojs/roadmap#roadmap-2022&#34;&gt;Roadmap for 2022&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Get started with the neu CLI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; # Creating a new app&#xA; npm i -g @neutralinojs/neu&#xA; neu create hello-world&#xA; cd hello-world&#xA; neu run&#xA; &#xA; # Building your app (No compilation - takes less than a second)&#xA; neu build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start building apps: &lt;a href=&#34;https://neutralino.js.org/docs&#34;&gt;neutralino.js.org/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why Neutralinojs?&lt;/h2&gt; &#xA;&lt;p&gt;In Electron and NWjs, you have to install NodeJs and hundreds of dependency libraries. Embedded Chromium and Node make simple apps bloaty. Neutralinojs offers a lightweight and portable SDK which is an alternative for Electron and NW.js. Neutralinojs doesn&#39;t bundle Chromium and uses the existing web browser library in the operating system (Eg: gtk-webkit2 on Linux). Neutralinojs implements a WebSocket connection for native operations and embeds a static web server to serve the web content. Also, it offers a built-in &lt;a href=&#34;https://github.com/neutralinojs/neutralino.js&#34;&gt;JavaScript client library&lt;/a&gt; for developers.&lt;/p&gt; &#xA;&lt;p&gt;Ask questions on StackOverflow using tag &lt;a href=&#34;https://stackoverflow.com/questions/tagged/neutralinojs&#34;&gt;neutralinojs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please check the &lt;a href=&#34;https://neutralino.js.org/docs/contributing/framework-developer-guide&#34;&gt;contribution guide&lt;/a&gt;. We use GitHub Discussions, Slack, and Discord for quick discussions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/cybpp4guTJ&#34;&gt;Join on Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/neutralinojs/neutralinojs/discussions&#34;&gt;Start a thread on Discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/neutralinojs/shared_invite/zt-b7mbivj5-pKpO6U5drmeT68vKD_pc6w&#34;&gt;Join on Slack&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Neutralinojs contributors:&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/neutralinojs/neutralinojs/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.firebaseapp.com/image?repo=neutralinojs/neutralinojs&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Image created with &lt;a href=&#34;https://contributors-img.firebaseapp.com&#34;&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Subprojects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Builds are powered by &lt;a href=&#34;https://codezri.org/docs/buildzri/intro&#34;&gt;BuildZri&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Releases are powered by &lt;a href=&#34;https://codezri.org/docs/releasezri/intro&#34;&gt;ReleaseZri&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sponsors and Donators&lt;/h2&gt; &#xA;&lt;p&gt;Organizations and individuals support Neutralinojs development. See: &lt;a href=&#34;https://codezri.org/sponsors&#34;&gt;https://codezri.org/sponsors&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you like to support our work, you can donate to Neutralinojs via &lt;a href=&#34;https://www.patreon.com/shalithasuranga&#34;&gt;Patreon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.patreon.com/shalithasuranga&#34;&gt;&lt;img src=&#34;https://c5.patreon.com/external/logo/become_a_patron_button.png&#34; alt=&#34;Become a Patreon&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Licenses and Copyrights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Neutralinojs core: MIT. Copyright (c) 2021 Neutralinojs and contributors.&lt;/li&gt; &#xA; &lt;li&gt;C++ websocket client/server library: BSD-3-Clause from &lt;a href=&#34;https://github.com/zaphoyd/websocketpp&#34;&gt;zaphoyd/websocketpp&lt;/a&gt;. Copyright (c) 2014, Peter Thorson. All rights reserved.&lt;/li&gt; &#xA; &lt;li&gt;JSON parser library: MIT from &lt;a href=&#34;https://github.com/nlohmann/json&#34;&gt;nlohmann/json&lt;/a&gt;. Copyright (c) 2013-2021 Niels Lohmann.&lt;/li&gt; &#xA; &lt;li&gt;Cross-platform webview library: MIT from &lt;a href=&#34;https://github.com/webview/webview&#34;&gt;webview/webview&lt;/a&gt;. Copyright (c) 2017 Serge Zaitsev.&lt;/li&gt; &#xA; &lt;li&gt;Cross-platform tray library: MIT from &lt;a href=&#34;https://github.com/zserge/tray&#34;&gt;zserge/tray&lt;/a&gt;. Copyright (c) 2017 Serge Zaitsev.&lt;/li&gt; &#xA; &lt;li&gt;Cross-platform GUI dialogs library: WTFPL from &lt;a href=&#34;https://github.com/samhocevar/portable-file-dialogs&#34;&gt;samhocevar/portable-file-dialogs&lt;/a&gt;. Copyright (c) 2018‚Äî2020 Sam Hocevar &lt;a href=&#34;mailto:sam@hocevar.net&#34;&gt;sam@hocevar.net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Base64 encoder/decoder library: MIT from &lt;a href=&#34;https://github.com/tobiaslocker/base64&#34;&gt;tobiaslocker/base64&lt;/a&gt;. Copyright (c) 2019 Tobias Locker.&lt;/li&gt; &#xA; &lt;li&gt;Cross-platform known platform directories API: MIT from &lt;a href=&#34;https://github.com/sago007/PlatformFolders&#34;&gt;sago007/PlatformFolders&lt;/a&gt;. Copyright (c) 2015 Poul Sander.&lt;/li&gt; &#xA; &lt;li&gt;C++ logging library: MIT from &lt;a href=&#34;https://github.com/amrayn/easyloggingpp&#34;&gt;amrayn/easyloggingpp&lt;/a&gt;. Copyright (c) 2012-2018 Amrayn Web Services. Copyright (c) 2012-2018 @abumusamq&lt;/li&gt; &#xA; &lt;li&gt;Cross-platform process library: MIT from &lt;a href=&#34;https://gitlab.com/eidheim/tiny-process-library&#34;&gt;eidheim/tiny-process-library&lt;/a&gt;. Copyright (c) 2015-2020 Ole Christian Eidheim.&lt;/li&gt; &#xA; &lt;li&gt;Asio standalone C++ library: &lt;a href=&#34;https://www.boost.org/LICENSE_1_0.txt&#34;&gt;Boost License v1.0&lt;/a&gt; from &lt;a href=&#34;https://github.com/chriskohlhoff/asio&#34;&gt;chriskohlhoff/asio&lt;/a&gt;. Copyright (c) 2003-2021 Christopher M. Kohlhoff (chris at kohlhoff dot com)&lt;/li&gt; &#xA; &lt;li&gt;Cross-platform C++ clipboard library: MIT from &lt;a href=&#34;https://github.com/dacap/clip&#34;&gt;dacap/clip&lt;/a&gt;. Copyright (c) 2015-2021 David Capello&lt;/li&gt; &#xA; &lt;li&gt;Logo design credits: &lt;a href=&#34;https://www.iconspng.com/image/2688/atom-orange&#34;&gt;IconsPng&lt;/a&gt;. Copyright free as mentioned in their website.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/neutralinojs/neutralinojs/main/LICENSE&#34;&gt;Complete license details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Fneutralinojs%2Fneutralinojs?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Fneutralinojs%2Fneutralinojs.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pixie-io/pixie</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/pixie-io/pixie</id>
    <link href="https://github.com/pixie-io/pixie" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Instant Kubernetes-Native Application Observability&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://px.dev&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/pixie-horizontal-color.png&#34; alt=&#34;Pixie!&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.px.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://slackin.px.dev&#34;&gt;&lt;img src=&#34;https://slackin.px.dev/badge.svg?sanitize=true&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/pixie_run&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/pixie_run.svg?style=social&amp;amp;label=Follow%20%40pixie_run&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ramitsurana/awesome-kubernetes&#34;&gt;&lt;img src=&#34;https://awesome.re/mentioned-badge.svg?sanitize=true&#34; alt=&#34;Mentioned in Awesome Kubernetes&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/avelino/awesome-go&#34;&gt;&lt;img src=&#34;https://awesome.re/mentioned-badge.svg?sanitize=true&#34; alt=&#34;Mentioned in Awesome Go&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jenkins.corp.pixielabs.ai/job/pixie-oss/job/build-and-test-all/&#34;&gt;&lt;img src=&#34;https://jenkins.corp.pixielabs.ai/buildStatus/icon?job=pixie-oss%2Fbuild-and-test-all&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/pixie-io/pixie&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/pixie-io/pixie/branch/main/graph/badge.svg?token=UG7P3QE5PQ&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.fossa.com/projects/custom%2B26327%2Fgithub.com%2Fpixie-io%2Fpixie?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/custom%2B26327%2Fgithub.com%2Fpixie-io%2Fpixie.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://artifacthub.io/packages/olm/community-operators/pixie-operator&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/pixie-operator&#34; alt=&#34;Artifact HUB&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/5027&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/5027/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://clomonitor.io/projects/cncf/pixie/pixie&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/pixie/pixie/badge&#34; alt=&#34;CLOMonitor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Pixie is an open-source observability tool for Kubernetes applications. Use Pixie to view the high-level state of your cluster (service maps, cluster resources, application traffic) and also drill down into more detailed views (pod state, flame graphs, individual full-body application requests).&lt;/p&gt; &#xA;&lt;h2&gt;Why Pixie?&lt;/h2&gt; &#xA;&lt;p&gt;Three features enable Pixie&#39;s magical developer experience:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Auto-telemetry:&lt;/strong&gt; Pixie uses eBPF to automatically collect telemetry data such as full-body requests, resource and network metrics, application profiles, and more. See the full list of data sources &lt;a href=&#34;https://docs.px.dev/about-pixie/data-sources/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;In-Cluster Edge Compute:&lt;/strong&gt; Pixie collects, stores and queries all telemetry data locally in the cluster. Pixie uses less than 5% of cluster CPU and in most cases less than 2%.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scriptability:&lt;/strong&gt; &lt;a href=&#34;https://docs.px.dev/reference/pxl/&#34;&gt;PxL&lt;/a&gt;, Pixie‚Äôs flexible Pythonic query language, can be used across Pixie‚Äôs UI, CLI, and client APIs.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use Cases&lt;/h2&gt; &#xA;&lt;h3&gt;Network Monitoring&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/net_flow_graph.png&#34; alt=&#34;Network Flow Graph&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Use Pixie to monitor your network, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The flow of network traffic within your cluster.&lt;/li&gt; &#xA; &lt;li&gt;The flow of DNS requests within your cluster.&lt;/li&gt; &#xA; &lt;li&gt;Individual full-body DNS requests and responses.&lt;/li&gt; &#xA; &lt;li&gt;A Map of TCP drops and TCP retransmits across your cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/pixie-101/network-monitoring/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/qIxzIPBhAUI&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h3&gt;Infrastructure Health&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/nodes.png&#34; alt=&#34;Infrastructure Monitoring&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Monitor your infrastructure alongside your network and application layer, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resource usage by Pod, Node, Namespace.&lt;/li&gt; &#xA; &lt;li&gt;CPU flame graphs per Pod, Node.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/pixie-101/infra-health/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/2dFIpiBryu8&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h3&gt;Service Performance&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/service.png&#34; alt=&#34;Service Performance&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Pixie automatically traces a &lt;a href=&#34;https://docs.px.dev/about-pixie/data-sources/&#34;&gt;variety of protocols&lt;/a&gt;. Get immediate visibility into the health of your services, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The flow of traffic between your services.&lt;/li&gt; &#xA; &lt;li&gt;Latency per service and endpoint.&lt;/li&gt; &#xA; &lt;li&gt;Sample of the slowest requests for an individual service.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/pixie-101/service-performance/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/Rex0yz_5vwc&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h3&gt;Database Query Profiling&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/sql_query.png&#34; alt=&#34;Database Query Profilling&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Pixie automatically traces several different &lt;a href=&#34;https://docs.px.dev/about-pixie/data-sources/#supported-protocols&#34;&gt;database protocols&lt;/a&gt;. Use Pixie to monitor the performance of your database requests:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Latency, error, and throughput (LET) rate for all pods.&lt;/li&gt; &#xA; &lt;li&gt;LET rate per normalized query.&lt;/li&gt; &#xA; &lt;li&gt;Latency per individual full-body query.&lt;/li&gt; &#xA; &lt;li&gt;Individual full-body requests and responses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/pixie-101/database-query-profiling/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/5NkU--hDXRQ&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h3&gt;Request Tracing&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/http_data_filtered.png&#34; alt=&#34;Request Tracing&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Pixie makes debugging this communication between microservices easy by providing immediate and deep (full-body) visibility into requests flowing through your cluster. See:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full-body requests and responses for &lt;a href=&#34;https://docs.px.dev/about-pixie/data-sources/#supported-protocols&#34;&gt;supported protocols&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Error rate per Service, Pod.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/pixie-101/request-tracing/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/Gl0so4rbwno&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h3&gt;Continuous Application Profiling&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/pod_flamegraph.png&#34; alt=&#34;Continuous Application Profiling&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Use Pixie&#39;s continuous profiling feature to identify performance issues within application code.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/pixie-101/profiler/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/Zr-s3EvAey8&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h3&gt;Distributed bpftrace Deployment&lt;/h3&gt; &#xA;&lt;p&gt;Use Pixie to deploy a &lt;a href=&#34;https://github.com/iovisor/bpftrace&#34;&gt;bpftrace&lt;/a&gt; program to all of the nodes in your cluster. After deploying the program, Pixie captures the output into a table and makes the data available to be queried and visualized in the Pixie UI. TCP Drops are pictured. For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/custom-data/distributed-bpftrace-deployment/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/xT7OYAgIV28&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Go Logging&lt;/h3&gt; &#xA;&lt;p&gt;Debug Go binaries deployed in production environments without needing to recompile and redeploy. For more details, check out the &lt;a href=&#34;https://docs.px.dev/tutorials/custom-data/dynamic-go-logging/&#34;&gt;tutorial&lt;/a&gt; or &lt;a href=&#34;https://youtu.be/aH7PHSsiIPM&#34;&gt;watch&lt;/a&gt; an overview.&lt;/p&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/.readme_assets/http_data.svg?sanitize=true&#34; alt=&#34;Request Tracing&#34; width=&#34;525&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;It takes just a few minutes to install Pixie. To get started, check out the &lt;a href=&#34;https://docs.px.dev/installing-pixie/install-guides/&#34;&gt;Install Guides&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Once installed, you can interact with Pixie using the:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.px.dev/using-pixie/using-live-ui/&#34;&gt;Web-based Live UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.px.dev/using-pixie/using-cli/&#34;&gt;CLI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.px.dev/using-pixie/api-quick-start/&#34;&gt;API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h2&gt;Get Involved&lt;/h2&gt; &#xA;&lt;p&gt;Pixie is a community-driven project; we welcome your contribution! For code contributions, please read our &lt;a href=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;File a &lt;a href=&#34;https://github.com/pixie-io/pixie/issues&#34;&gt;GitHub issue&lt;/a&gt; to report a bug or request a feature.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://slackin.px.dev&#34;&gt;Slack&lt;/a&gt; for live conversations and quick questions. We are also available on the &lt;a href=&#34;https://slack.cncf.io/app_redirect?channel=pixie%22&#34;&gt;CNCF slack&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Follow us on &lt;a href=&#34;https://twitter.com/pixie_run&#34;&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/channel/UCOMCDRvBVNIS0lCyOmst7eg&#34;&gt;YouTube&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Join our monthly &lt;a href=&#34;https://px.dev/community/#events&#34;&gt;community meetings&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Provide feedback on our &lt;a href=&#34;https://docs.px.dev/about-pixie/roadmap/&#34;&gt;roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br clear=&#34;all&#34;&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;The changelog is stored in annotated git tags.&lt;/p&gt; &#xA;&lt;p&gt;For vizier:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git for-each-ref refs/tags/release/vizier/$tagname --format=&#39;%(tag) %(contents)&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git for-each-ref refs/tags/release/cli/$tagname --format=&#39;%(tag) %(contents)&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These are also published on the &lt;a href=&#34;https://github.com/pixie-io/pixie/releases&#34;&gt;releases&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;The known adopters and users of Pixie are listed &lt;a href=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/ADOPTERS.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Software Bill of Materials&lt;/h2&gt; &#xA;&lt;p&gt;We publish a list of all the components Pixie depends on and the corresponding versions and licenses &lt;a href=&#34;https://storage.googleapis.com/pixie-dev-public/oss-licenses/latest.json&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About Pixie&lt;/h2&gt; &#xA;&lt;p&gt;Pixie was contributed by &lt;a href=&#34;https://newrelic.com/&#34;&gt;New Relic, Inc.&lt;/a&gt; to the &lt;a href=&#34;https://www.cncf.io/&#34;&gt;Cloud Native Computing Foundation&lt;/a&gt; as a Sandbox project in June 2021.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Pixie is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/pixie-io/pixie/main/LICENSE&#34;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cameron314/concurrentqueue</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/cameron314/concurrentqueue</id>
    <link href="https://github.com/cameron314/concurrentqueue" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast multi-producer, multi-consumer lock-free concurrent queue for C++11&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;moodycamel::ConcurrentQueue&#xA; &lt;t&gt;&lt;/t&gt;&lt;/h1&gt; &#xA;&lt;p&gt;An industrial-strength lock-free queue for C++.&lt;/p&gt; &#xA;&lt;p&gt;Note: If all you need is a single-producer, single-consumer queue, I have &lt;a href=&#34;https://github.com/cameron314/readerwriterqueue&#34;&gt;one of those too&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Knock-your-socks-off &lt;a href=&#34;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks&#34;&gt;blazing fast performance&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Single-header implementation. Just drop it in your project.&lt;/li&gt; &#xA; &lt;li&gt;Fully thread-safe lock-free queue. Use concurrently from any number of threads.&lt;/li&gt; &#xA; &lt;li&gt;C++11 implementation -- elements are moved (instead of copied) where possible.&lt;/li&gt; &#xA; &lt;li&gt;Templated, obviating the need to deal exclusively with pointers -- memory is managed for you.&lt;/li&gt; &#xA; &lt;li&gt;No artificial limitations on element types or maximum count.&lt;/li&gt; &#xA; &lt;li&gt;Memory can be allocated once up-front, or dynamically as needed.&lt;/li&gt; &#xA; &lt;li&gt;Fully portable (no assembly; all is done through standard C++11 primitives).&lt;/li&gt; &#xA; &lt;li&gt;Supports super-fast bulk operations.&lt;/li&gt; &#xA; &lt;li&gt;Includes a low-overhead blocking version (BlockingConcurrentQueue).&lt;/li&gt; &#xA; &lt;li&gt;Exception safe.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reasons to use&lt;/h2&gt; &#xA;&lt;p&gt;There are not that many full-fledged lock-free queues for C++. Boost has one, but it&#39;s limited to objects with trivial assignment operators and trivial destructors, for example. Intel&#39;s TBB queue isn&#39;t lock-free, and requires trivial constructors too. There&#39;re many academic papers that implement lock-free queues in C++, but usable source code is hard to find, and tests even more so.&lt;/p&gt; &#xA;&lt;p&gt;This queue not only has less limitations than others (for the most part), but &lt;a href=&#34;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks&#34;&gt;it&#39;s also faster&lt;/a&gt;. It&#39;s been fairly well-tested, and offers advanced features like &lt;strong&gt;bulk enqueueing/dequeueing&lt;/strong&gt; (which, with my new design, is much faster than one element at a time, approaching and even surpassing the speed of a non-concurrent queue even under heavy contention).&lt;/p&gt; &#xA;&lt;p&gt;In short, there was a lock-free queue shaped hole in the C++ open-source universe, and I set out to fill it with the fastest, most complete, and well-tested design and implementation I could. The result is &lt;code&gt;moodycamel::ConcurrentQueue&lt;/code&gt; :-)&lt;/p&gt; &#xA;&lt;h2&gt;Reasons &lt;em&gt;not&lt;/em&gt; to use&lt;/h2&gt; &#xA;&lt;p&gt;The fastest synchronization of all is the kind that never takes place. Fundamentally, concurrent data structures require some synchronization, and that takes time. Every effort was made, of course, to minimize the overhead, but if you can avoid sharing data between threads, do so!&lt;/p&gt; &#xA;&lt;p&gt;Why use concurrent data structures at all, then? Because they&#39;re gosh darn convenient! (And, indeed, sometimes sharing data concurrently is unavoidable.)&lt;/p&gt; &#xA;&lt;p&gt;My queue is &lt;strong&gt;not linearizable&lt;/strong&gt; (see the next section on high-level design). The foundations of its design assume that producers are independent; if this is not the case, and your producers co-ordinate amongst themselves in some fashion, be aware that the elements won&#39;t necessarily come out of the queue in the same order they were put in &lt;em&gt;relative to the ordering formed by that co-ordination&lt;/em&gt; (but they will still come out in the order they were put in by any &lt;em&gt;individual&lt;/em&gt; producer). If this affects your use case, you may be better off with another implementation; either way, it&#39;s an important limitation to be aware of.&lt;/p&gt; &#xA;&lt;p&gt;My queue is also &lt;strong&gt;not NUMA aware&lt;/strong&gt;, and does a lot of memory re-use internally, meaning it probably doesn&#39;t scale particularly well on NUMA architectures; however, I don&#39;t know of any other lock-free queue that &lt;em&gt;is&lt;/em&gt; NUMA aware (except for &lt;a href=&#34;http://webee.technion.ac.il/~idish/ftp/spaa049-gidron.pdf&#34;&gt;SALSA&lt;/a&gt;, which is very cool, but has no publicly available implementation that I know of).&lt;/p&gt; &#xA;&lt;p&gt;Finally, the queue is &lt;strong&gt;not sequentially consistent&lt;/strong&gt;; there &lt;em&gt;is&lt;/em&gt; a happens-before relationship between when an element is put in the queue and when it comes out, but other things (such as pumping the queue until it&#39;s empty) require more thought to get right in all eventualities, because explicit memory ordering may have to be done to get the desired effect. In other words, it can sometimes be difficult to use the queue correctly. This is why it&#39;s a good idea to follow the &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/samples.md&#34;&gt;samples&lt;/a&gt; where possible. On the other hand, the upside of this lack of sequential consistency is better performance.&lt;/p&gt; &#xA;&lt;h2&gt;High-level design&lt;/h2&gt; &#xA;&lt;p&gt;Elements are stored internally using contiguous blocks instead of linked lists for better performance. The queue is made up of a collection of sub-queues, one for each producer. When a consumer wants to dequeue an element, it checks all the sub-queues until it finds one that&#39;s not empty. All of this is largely transparent to the user of the queue, however -- it mostly just works&lt;sup&gt;TM&lt;/sup&gt;.&lt;/p&gt; &#xA;&lt;p&gt;One particular consequence of this design, however, (which seems to be non-intuitive) is that if two producers enqueue at the same time, there is no defined ordering between the elements when they&#39;re later dequeued. Normally this is fine, because even with a fully linearizable queue there&#39;d be a race between the producer threads and so you couldn&#39;t rely on the ordering anyway. However, if for some reason you do extra explicit synchronization between the two producer threads yourself, thus defining a total order between enqueue operations, you might expect that the elements would come out in the same total order, which is a guarantee my queue does not offer. At that point, though, there semantically aren&#39;t really two separate producers, but rather one that happens to be spread across multiple threads. In this case, you can still establish a total ordering with my queue by creating a single producer token, and using that from both threads to enqueue (taking care to synchronize access to the token, of course, but there was already extra synchronization involved anyway).&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve written a more detailed &lt;a href=&#34;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++&#34;&gt;overview of the internal design&lt;/a&gt;, as well as &lt;a href=&#34;http://moodycamel.com/blog/2014/detailed-design-of-a-lock-free-queue&#34;&gt;the full nitty-gritty details of the design&lt;/a&gt;, on my blog. Finally, the &lt;a href=&#34;https://github.com/cameron314/concurrentqueue&#34;&gt;source&lt;/a&gt; itself is available for perusal for those interested in its implementation.&lt;/p&gt; &#xA;&lt;h2&gt;Basic use&lt;/h2&gt; &#xA;&lt;p&gt;The entire queue&#39;s implementation is contained in &lt;strong&gt;one header&lt;/strong&gt;, &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/concurrentqueue.h&#34;&gt;&lt;code&gt;concurrentqueue.h&lt;/code&gt;&lt;/a&gt;. Simply download and include that to use the queue. The blocking version is in a separate header, &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/blockingconcurrentqueue.h&#34;&gt;&lt;code&gt;blockingconcurrentqueue.h&lt;/code&gt;&lt;/a&gt;, that depends on &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/concurrentqueue.h&#34;&gt;&lt;code&gt;concurrentqueue.h&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/lightweightsemaphore.h&#34;&gt;&lt;code&gt;lightweightsemaphore.h&lt;/code&gt;&lt;/a&gt;. The implementation makes use of certain key C++11 features, so it requires a relatively recent compiler (e.g. VS2012+ or g++ 4.8; note that g++ 4.6 has a known bug with &lt;code&gt;std::atomic&lt;/code&gt; and is thus not supported). The algorithm implementations themselves are platform independent.&lt;/p&gt; &#xA;&lt;p&gt;Use it like you would any other templated queue, with the exception that you can use it from many threads at once :-)&lt;/p&gt; &#xA;&lt;p&gt;Simple example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#include &#34;concurrentqueue.h&#34;&#xA;&#xA;moodycamel::ConcurrentQueue&amp;lt;int&amp;gt; q;&#xA;q.enqueue(25);&#xA;&#xA;int item;&#xA;bool found = q.try_dequeue(item);&#xA;assert(found &amp;amp;&amp;amp; item == 25);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Description of basic methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ConcurrentQueue(size_t initialSizeEstimate)&lt;/code&gt; Constructor which optionally accepts an estimate of the number of elements the queue will hold&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enqueue(T&amp;amp;&amp;amp; item)&lt;/code&gt; Enqueues one item, allocating extra space if necessary&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;try_enqueue(T&amp;amp;&amp;amp; item)&lt;/code&gt; Enqueues one item, but only if enough memory is already allocated&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;try_dequeue(T&amp;amp; item)&lt;/code&gt; Dequeues one item, returning true if an item was found or false if the queue appeared empty&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that it is up to the user to ensure that the queue object is completely constructed before being used by any other threads (this includes making the memory effects of construction visible, possibly via a memory barrier). Similarly, it&#39;s important that all threads have finished using the queue (and the memory effects have fully propagated) before it is destructed.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s usually two versions of each method, one &#34;explicit&#34; version that takes a user-allocated per-producer or per-consumer token, and one &#34;implicit&#34; version that works without tokens. Using the explicit methods is almost always faster (though not necessarily by a huge factor). Apart from performance, the primary distinction between them is their sub-queue allocation behaviour for enqueue operations: Using the implicit enqueue methods causes an automatically-allocated thread-local producer sub-queue to be allocated. Explicit producers, on the other hand, are tied directly to their tokens&#39; lifetimes (but are recycled internally).&lt;/p&gt; &#xA;&lt;p&gt;In order to avoid the number of sub-queues growing without bound, implicit producers are marked for reuse once their thread exits. However, this is not supported on all platforms. If using the queue from short-lived threads, it is recommended to use explicit producer tokens instead.&lt;/p&gt; &#xA;&lt;p&gt;Full API (pseudocode):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Allocates more memory if necessary&#xA;enqueue(item) : bool&#xA;enqueue(prod_token, item) : bool&#xA;enqueue_bulk(item_first, count) : bool&#xA;enqueue_bulk(prod_token, item_first, count) : bool&#xA;&#xA;# Fails if not enough memory to enqueue&#xA;try_enqueue(item) : bool&#xA;try_enqueue(prod_token, item) : bool&#xA;try_enqueue_bulk(item_first, count) : bool&#xA;try_enqueue_bulk(prod_token, item_first, count) : bool&#xA;&#xA;# Attempts to dequeue from the queue (never allocates)&#xA;try_dequeue(item&amp;amp;) : bool&#xA;try_dequeue(cons_token, item&amp;amp;) : bool&#xA;try_dequeue_bulk(item_first, max) : size_t&#xA;try_dequeue_bulk(cons_token, item_first, max) : size_t&#xA;&#xA;# If you happen to know which producer you want to dequeue from&#xA;try_dequeue_from_producer(prod_token, item&amp;amp;) : bool&#xA;try_dequeue_bulk_from_producer(prod_token, item_first, max) : size_t&#xA;&#xA;# A not-necessarily-accurate count of the total number of elements&#xA;size_approx() : size_t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Blocking version&lt;/h2&gt; &#xA;&lt;p&gt;As mentioned above, a full blocking wrapper of the queue is provided that adds &lt;code&gt;wait_dequeue&lt;/code&gt; and &lt;code&gt;wait_dequeue_bulk&lt;/code&gt; methods in addition to the regular interface. This wrapper is extremely low-overhead, but slightly less fast than the non-blocking queue (due to the necessary bookkeeping involving a lightweight semaphore).&lt;/p&gt; &#xA;&lt;p&gt;There are also timed versions that allow a timeout to be specified (either in microseconds or with a &lt;code&gt;std::chrono&lt;/code&gt; object).&lt;/p&gt; &#xA;&lt;p&gt;The only major caveat with the blocking version is that you must be careful not to destroy the queue while somebody is waiting on it. This generally means you need to know for certain that another element is going to come along before you call one of the blocking methods. (To be fair, the non-blocking version cannot be destroyed while in use either, but it can be easier to coordinate the cleanup.)&lt;/p&gt; &#xA;&lt;p&gt;Blocking example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#include &#34;blockingconcurrentqueue.h&#34;&#xA;&#xA;moodycamel::BlockingConcurrentQueue&amp;lt;int&amp;gt; q;&#xA;std::thread producer([&amp;amp;]() {&#xA;    for (int i = 0; i != 100; ++i) {&#xA;        std::this_thread::sleep_for(std::chrono::milliseconds(i % 10));&#xA;        q.enqueue(i);&#xA;    }&#xA;});&#xA;std::thread consumer([&amp;amp;]() {&#xA;    for (int i = 0; i != 100; ++i) {&#xA;        int item;&#xA;        q.wait_dequeue(item);&#xA;        assert(item == i);&#xA;        &#xA;        if (q.wait_dequeue_timed(item, std::chrono::milliseconds(5))) {&#xA;            ++i;&#xA;            assert(item == i);&#xA;        }&#xA;    }&#xA;});&#xA;producer.join();&#xA;consumer.join();&#xA;&#xA;assert(q.size_approx() == 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced features&lt;/h2&gt; &#xA;&lt;h4&gt;Tokens&lt;/h4&gt; &#xA;&lt;p&gt;The queue can take advantage of extra per-producer and per-consumer storage if it&#39;s available to speed up its operations. This takes the form of &#34;tokens&#34;: You can create a consumer token and/or a producer token for each thread or task (tokens themselves are not thread-safe), and use the methods that accept a token as their first parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;moodycamel::ConcurrentQueue&amp;lt;int&amp;gt; q;&#xA;&#xA;moodycamel::ProducerToken ptok(q);&#xA;q.enqueue(ptok, 17);&#xA;&#xA;moodycamel::ConsumerToken ctok(q);&#xA;int item;&#xA;q.try_dequeue(ctok, item);&#xA;assert(item == 17);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you happen to know which producer you want to consume from (e.g. in a single-producer, multi-consumer scenario), you can use the &lt;code&gt;try_dequeue_from_producer&lt;/code&gt; methods, which accept a producer token instead of a consumer token, and cut some overhead.&lt;/p&gt; &#xA;&lt;p&gt;Note that tokens work with the blocking version of the queue too.&lt;/p&gt; &#xA;&lt;p&gt;When producing or consuming many elements, the most efficient way is to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use the bulk methods of the queue with tokens&lt;/li&gt; &#xA; &lt;li&gt;Failing that, use the bulk methods without tokens&lt;/li&gt; &#xA; &lt;li&gt;Failing that, use the single-item methods with tokens&lt;/li&gt; &#xA; &lt;li&gt;Failing that, use the single-item methods without tokens&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Having said that, don&#39;t create tokens willy-nilly -- ideally there would be one token (of each kind) per thread. The queue will work with what it is given, but it performs best when used with tokens.&lt;/p&gt; &#xA;&lt;p&gt;Note that tokens aren&#39;t actually tied to any given thread; it&#39;s not technically required that they be local to the thread, only that they be used by a single producer/consumer at a time.&lt;/p&gt; &#xA;&lt;h4&gt;Bulk operations&lt;/h4&gt; &#xA;&lt;p&gt;Thanks to the &lt;a href=&#34;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++&#34;&gt;novel design&lt;/a&gt; of the queue, it&#39;s just as easy to enqueue/dequeue multiple items as it is to do one at a time. This means that overhead can be cut drastically for bulk operations. Example syntax:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;moodycamel::ConcurrentQueue&amp;lt;int&amp;gt; q;&#xA;&#xA;int items[] = { 1, 2, 3, 4, 5 };&#xA;q.enqueue_bulk(items, 5);&#xA;&#xA;int results[5];     // Could also be any iterator&#xA;size_t count = q.try_dequeue_bulk(results, 5);&#xA;for (size_t i = 0; i != count; ++i) {&#xA;    assert(results[i] == items[i]);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Preallocation (correctly using &lt;code&gt;try_enqueue&lt;/code&gt;)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;try_enqueue&lt;/code&gt;, unlike just plain &lt;code&gt;enqueue&lt;/code&gt;, will never allocate memory. If there&#39;s not enough room in the queue, it simply returns false. The key to using this method properly, then, is to ensure enough space is pre-allocated for your desired maximum element count.&lt;/p&gt; &#xA;&lt;p&gt;The constructor accepts a count of the number of elements that it should reserve space for. Because the queue works with blocks of elements, however, and not individual elements themselves, the value to pass in order to obtain an effective number of pre-allocated element slots is non-obvious.&lt;/p&gt; &#xA;&lt;p&gt;First, be aware that the count passed is rounded up to the next multiple of the block size. Note that the default block size is 32 (this can be changed via the traits). Second, once a slot in a block has been enqueued to, that slot cannot be re-used until the rest of the block has completely been completely filled up and then completely emptied. This affects the number of blocks you need in order to account for the overhead of partially-filled blocks. Third, each producer (whether implicit or explicit) claims and recycles blocks in a different manner, which again affects the number of blocks you need to account for a desired number of usable slots.&lt;/p&gt; &#xA;&lt;p&gt;Suppose you want the queue to be able to hold at least &lt;code&gt;N&lt;/code&gt; elements at any given time. Without delving too deep into the rather arcane implementation details, here are some simple formulas for the number of elements to request for pre-allocation in such a case. Note the division is intended to be arithmetic division and not integer division (in order for &lt;code&gt;ceil()&lt;/code&gt; to work).&lt;/p&gt; &#xA;&lt;p&gt;For explicit producers (using tokens to enqueue):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(ceil(N / BLOCK_SIZE) + 1) * MAX_NUM_PRODUCERS * BLOCK_SIZE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For implicit producers (no tokens):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(ceil(N / BLOCK_SIZE) - 1 + 2 * MAX_NUM_PRODUCERS) * BLOCK_SIZE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using mixed producer types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;((ceil(N / BLOCK_SIZE) - 1) * (MAX_EXPLICIT_PRODUCERS + 1) + 2 * (MAX_IMPLICIT_PRODUCERS + MAX_EXPLICIT_PRODUCERS)) * BLOCK_SIZE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If these formulas seem rather inconvenient, you can use the constructor overload that accepts the minimum number of elements (&lt;code&gt;N&lt;/code&gt;) and the maximum number of explicit and implicit producers directly, and let it do the computation for you.&lt;/p&gt; &#xA;&lt;p&gt;Finally, it&#39;s important to note that because the queue is only eventually consistent and takes advantage of weak memory ordering for speed, there&#39;s always a possibility that under contention &lt;code&gt;try_enqueue&lt;/code&gt; will fail even if the queue is correctly pre-sized for the desired number of elements. (e.g. A given thread may think that the queue&#39;s full even when that&#39;s no longer the case.) So no matter what, you still need to handle the failure case (perhaps looping until it succeeds), unless you don&#39;t mind dropping elements.&lt;/p&gt; &#xA;&lt;h4&gt;Exception safety&lt;/h4&gt; &#xA;&lt;p&gt;The queue is exception safe, and will never become corrupted if used with a type that may throw exceptions. The queue itself never throws any exceptions (operations fail gracefully (return false) if memory allocation fails instead of throwing &lt;code&gt;std::bad_alloc&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;It is important to note that the guarantees of exception safety only hold if the element type never throws from its destructor, and that any iterators passed into the queue (for bulk operations) never throw either. Note that in particular this means &lt;code&gt;std::back_inserter&lt;/code&gt; iterators must be used with care, since the vector being inserted into may need to allocate and throw a &lt;code&gt;std::bad_alloc&lt;/code&gt; exception from inside the iterator; so be sure to reserve enough capacity in the target container first if you do this.&lt;/p&gt; &#xA;&lt;p&gt;The guarantees are presently as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enqueue operations are rolled back completely if an exception is thrown from an element&#39;s constructor. For bulk enqueue operations, this means that elements are copied instead of moved (in order to avoid having only some of the objects be moved in the event of an exception). Non-bulk enqueues always use the move constructor if one is available.&lt;/li&gt; &#xA; &lt;li&gt;If the assignment operator throws during a dequeue operation (both single and bulk), the element(s) are considered dequeued regardless. In such a case, the dequeued elements are all properly destructed before the exception is propagated, but there&#39;s no way to get the elements themselves back.&lt;/li&gt; &#xA; &lt;li&gt;Any exception that is thrown is propagated up the call stack, at which point the queue is in a consistent state.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: If any of your type&#39;s copy constructors/move constructors/assignment operators don&#39;t throw, be sure to annotate them with &lt;code&gt;noexcept&lt;/code&gt;; this will avoid the exception-checking overhead in the queue where possible (even with zero-cost exceptions, there&#39;s still a code size impact that has to be taken into account).&lt;/p&gt; &#xA;&lt;h4&gt;Traits&lt;/h4&gt; &#xA;&lt;p&gt;The queue also supports a traits template argument which defines various types, constants, and the memory allocation and deallocation functions that are to be used by the queue. The typical pattern to providing your own traits is to create a class that inherits from the default traits and override only the values you wish to change. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct MyTraits : public moodycamel::ConcurrentQueueDefaultTraits&#xA;{&#xA;&#x9;static const size_t BLOCK_SIZE = 256;&#x9;&#x9;// Use bigger blocks&#xA;};&#xA;&#xA;moodycamel::ConcurrentQueue&amp;lt;int, MyTraits&amp;gt; q;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;How to dequeue types without calling the constructor&lt;/h4&gt; &#xA;&lt;p&gt;The normal way to dequeue an item is to pass in an existing object by reference, which is then assigned to internally by the queue (using the move-assignment operator if possible). This can pose a problem for types that are expensive to construct or don&#39;t have a default constructor; fortunately, there is a simple workaround: Create a wrapper class that copies the memory contents of the object when it is assigned by the queue (a poor man&#39;s move, essentially). Note that this only works if the object contains no internal pointers. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct MyObjectMover {&#xA;    inline void operator=(MyObject&amp;amp;&amp;amp; obj) {&#xA;        std::memcpy(data, &amp;amp;obj, sizeof(MyObject));&#xA;        &#xA;        // TODO: Cleanup obj so that when it&#39;s destructed by the queue&#xA;        // it doesn&#39;t corrupt the data of the object we just moved it into&#xA;    }&#xA;&#xA;    inline MyObject&amp;amp; obj() { return *reinterpret_cast&amp;lt;MyObject*&amp;gt;(data); }&#xA;&#xA;private:&#xA;    align(alignof(MyObject)) char data[sizeof(MyObject)];&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A less dodgy alternative, if moves are cheap but default construction is not, is to use a wrapper that defers construction until the object is assigned, enabling use of the move constructor:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct MyObjectMover {&#xA;    inline void operator=(MyObject&amp;amp;&amp;amp; x) {&#xA;        new (data) MyObject(std::move(x));&#xA;        created = true;&#xA;    }&#xA;&#xA;    inline MyObject&amp;amp; obj() {&#xA;        assert(created);&#xA;        return *reinterpret_cast&amp;lt;MyObject*&amp;gt;(data);&#xA;    }&#xA;&#xA;    ~MyObjectMover() {&#xA;        if (created)&#xA;            obj().~MyObject();&#xA;    }&#xA;&#xA;private:&#xA;    align(alignof(MyObject)) char data[sizeof(MyObject)];&#xA;    bool created = false;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Samples&lt;/h2&gt; &#xA;&lt;p&gt;There are some more detailed samples &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/samples.md&#34;&gt;here&lt;/a&gt;. The source of the &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/tree/master/tests/unittests&#34;&gt;unit tests&lt;/a&gt; and &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/tree/master/benchmarks&#34;&gt;benchmarks&lt;/a&gt; are available for reference as well.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;See my blog post for some &lt;a href=&#34;http://moodycamel.com/blog/2014/a-fast-general-purpose-lock-free-queue-for-c++#benchmarks&#34;&gt;benchmark results&lt;/a&gt; (including versus &lt;code&gt;boost::lockfree::queue&lt;/code&gt; and &lt;code&gt;tbb::concurrent_queue&lt;/code&gt;), or run the benchmarks yourself (requires MinGW and certain GnuWin32 utilities to build on Windows, or a recent g++ on Linux):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd build&#xA;make benchmarks&#xA;bin/benchmarks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The short version of the benchmarks is that it&#39;s so fast (especially the bulk methods), that if you&#39;re actually using the queue to &lt;em&gt;do&lt;/em&gt; anything, the queue won&#39;t be your bottleneck.&lt;/p&gt; &#xA;&lt;h2&gt;Tests (and bugs)&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve written quite a few unit tests as well as a randomized long-running fuzz tester. I also ran the core queue algorithm through the &lt;a href=&#34;http://demsky.eecs.uci.edu/c11modelchecker.html&#34;&gt;CDSChecker&lt;/a&gt; C++11 memory model model checker. Some of the inner algorithms were tested separately using the &lt;a href=&#34;http://www.1024cores.net/home/relacy-race-detector&#34;&gt;Relacy&lt;/a&gt; model checker, and full integration tests were also performed with Relacy. I&#39;ve tested on Linux (Fedora 19) and Windows (7), but only on x86 processors so far (Intel and AMD). The code was written to be platform-independent, however, and should work across all processors and OSes.&lt;/p&gt; &#xA;&lt;p&gt;Due to the complexity of the implementation and the difficult-to-test nature of lock-free code in general, there may still be bugs. If anyone is seeing buggy behaviour, I&#39;d like to hear about it! (Especially if a unit test for it can be cooked up.) Just open an issue on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Using vcpkg&lt;/h2&gt; &#xA;&lt;p&gt;You can download and install &lt;code&gt;moodycamel::ConcurrentQueue&lt;/code&gt; using the &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Microsoft/vcpkg.git&#xA;cd vcpkg&#xA;./bootstrap-vcpkg.sh&#xA;./vcpkg integrate install&#xA;vcpkg install concurrentqueue&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;moodycamel::ConcurrentQueue&lt;/code&gt; port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m releasing the source of this repository (with the exception of third-party code, i.e. the Boost queue (used in the benchmarks for comparison), Intel&#39;s TBB library (ditto), CDSChecker, Relacy, and Jeff Preshing&#39;s cross-platform semaphore, which all have their own licenses) under a simplified BSD license. I&#39;m also dual-licensing under the Boost Software License. See the &lt;a href=&#34;https://github.com/cameron314/concurrentqueue/raw/master/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; file for more details.&lt;/p&gt; &#xA;&lt;p&gt;Note that lock-free programming is a patent minefield, and this code may very well violate a pending patent (I haven&#39;t looked), though it does not to my present knowledge. I did design and implement this queue from scratch.&lt;/p&gt; &#xA;&lt;h2&gt;Diving into the code&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re interested in the source code itself, it helps to have a rough idea of how it&#39;s laid out. This section attempts to describe that.&lt;/p&gt; &#xA;&lt;p&gt;The queue is formed of several basic parts (listed here in roughly the order they appear in the source). There&#39;s the helper functions (e.g. for rounding to a power of 2). There&#39;s the default traits of the queue, which contain the constants and malloc/free functions used by the queue. There&#39;s the producer and consumer tokens. Then there&#39;s the queue&#39;s public API itself, starting with the constructor, destructor, and swap/assignment methods. There&#39;s the public enqueue methods, which are all wrappers around a small set of private enqueue methods found later on. There&#39;s the dequeue methods, which are defined inline and are relatively straightforward.&lt;/p&gt; &#xA;&lt;p&gt;Then there&#39;s all the main internal data structures. First, there&#39;s a lock-free free list, used for recycling spent blocks (elements are enqueued to blocks internally). Then there&#39;s the block structure itself, which has two different ways of tracking whether it&#39;s fully emptied or not (remember, given two parallel consumers, there&#39;s no way to know which one will finish first) depending on where it&#39;s used. Then there&#39;s a small base class for the two types of internal SPMC producer queues (one for explicit producers that holds onto memory but attempts to be faster, and one for implicit ones which attempt to recycle more memory back into the parent but is a little slower). The explicit producer is defined first, then the implicit one. They both contain the same general four methods: One to enqueue, one to dequeue, one to enqueue in bulk, and one to dequeue in bulk. (Obviously they have constructors and destructors too, and helper methods.) The main difference between them is how the block handling is done (they both use the same blocks, but in different ways, and map indices to them in different ways).&lt;/p&gt; &#xA;&lt;p&gt;Finally, there&#39;s the miscellaneous internal methods: There&#39;s the ones that handle the initial block pool (populated when the queue is constructed), and an abstract block pool that comprises the initial pool and any blocks on the free list. There&#39;s ones that handle the producer list (a lock-free add-only linked list of all the producers in the system). There&#39;s ones that handle the implicit producer lookup table (which is really a sort of specialized TLS lookup). And then there&#39;s some helper methods for allocating and freeing objects, and the data members of the queue itself, followed lastly by the free-standing swap functions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/pytorch</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/pytorch/pytorch</id>
    <link href="https://github.com/pytorch/pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/pytorch-logo-dark.png&#34; alt=&#34;PyTorch Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt; &#xA; &lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.&lt;/p&gt; &#xA;&lt;p&gt;Our trunk health (Continuous Integration signals) can be found at &lt;a href=&#34;https://hud.pytorch.org/ci/pytorch/pytorch/master&#34;&gt;hud.pytorch.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#more-about-pytorch&#34;&gt;More About PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#a-gpu-ready-tensor-library&#34;&gt;A GPU-Ready Tensor Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#dynamic-neural-networks-tape-based-autograd&#34;&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#python-first&#34;&gt;Python First&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#imperative-experiences&#34;&gt;Imperative Experiences&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#fast-and-lean&#34;&gt;Fast and Lean&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#extensions-without-pain&#34;&gt;Extensions Without Pain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#binaries&#34;&gt;Binaries&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#nvidia-jetson-platforms&#34;&gt;NVIDIA Jetson Platforms&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#from-source&#34;&gt;From Source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-dependencies&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#get-the-pytorch-source&#34;&gt;Get the PyTorch Source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-pytorch&#34;&gt;Install PyTorch&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#adjust-build-options-optional&#34;&gt;Adjust Build Options (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#docker-image&#34;&gt;Docker Image&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#using-pre-built-images&#34;&gt;Using pre-built images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-image-yourself&#34;&gt;Building the image yourself&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-documentation&#34;&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#previous-versions&#34;&gt;Previous Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#releases-and-contributing&#34;&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34;&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a Tensor library like NumPy, with strong GPU support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html&#34;&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/jit.html&#34;&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/nn.html&#34;&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/multiprocessing.html&#34;&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/data.html&#34;&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, PyTorch is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A replacement for NumPy to use the power of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;A GPU-Ready Tensor Library&lt;/h3&gt; &#xA;&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/tensor_illustration.png&#34; alt=&#34;Tensor illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.&lt;/p&gt; &#xA;&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt; &#xA;&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt; &#xA;&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as &lt;a href=&#34;https://github.com/twitter/torch-autograd&#34;&gt;torch-autograd&lt;/a&gt;, &lt;a href=&#34;https://github.com/HIPS/autograd&#34;&gt;autograd&lt;/a&gt;, &lt;a href=&#34;https://chainer.org&#34;&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;While this technique is not unique to PyTorch, it&#39;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif&#34; alt=&#34;Dynamic graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python First&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; / &lt;a href=&#34;https://www.scipy.org/&#34;&gt;SciPy&lt;/a&gt; / &lt;a href=&#34;https://scikit-learn.org&#34;&gt;scikit-learn&lt;/a&gt; etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt; and &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;. Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt; &#xA;&lt;h3&gt;Imperative Experiences&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn&#39;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Lean&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries such as &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel MKL&lt;/a&gt; and NVIDIA (&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.&lt;/p&gt; &#xA;&lt;p&gt;Hence, PyTorch is quite fast ‚Äì whether you run small or large neural networks.&lt;/p&gt; &#xA;&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We&#39;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions Without Pain&lt;/h3&gt; &#xA;&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch&#39;s Tensor API was designed to be straightforward and with minimal abstractions.&lt;/p&gt; &#xA;&lt;p&gt;You can write new neural network layers in Python using the torch API &lt;a href=&#34;https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html&#34;&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;a tutorial here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/extension-cpp&#34;&gt;an example here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;p&gt;Commands to install binaries via Conda or pip wheels are on our website: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;NVIDIA Jetson Platforms&lt;/h4&gt; &#xA;&lt;p&gt;Python wheels for NVIDIA&#39;s Jetson Nano, Jetson TX2, and Jetson AGX Xavier are provided &lt;a href=&#34;https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048&#34;&gt;here&lt;/a&gt; and the L4T container is published &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They require JetPack 4.2 and above, and &lt;a href=&#34;https://github.com/dusty-nv&#34;&gt;@dusty-nv&lt;/a&gt; and &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;@ptrblck&lt;/a&gt; are maintaining them.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;If you are installing from source, you will need Python 3.7 or later and a C++14 compiler. Also, we highly recommend installing an &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt; &#xA;&lt;p&gt;Once you have &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; installed, here are the instructions.&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with CUDA support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVIDIA CUDA&lt;/a&gt; 10.2 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;NVIDIA cuDNN&lt;/a&gt; v7 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/ax3l/9489132&#34;&gt;Compiler&lt;/a&gt; compatible with CUDA Note: You could refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf&#34;&gt;cuDNN Support Matrix&lt;/a&gt; for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardwares&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable CUDA support, export the environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are building for NVIDIA&#39;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are &lt;a href=&#34;https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/&#34;&gt;available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with ROCm support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html&#34;&gt;AMD ROCm&lt;/a&gt; 4.0 and above installation&lt;/li&gt; &#xA; &lt;li&gt;ROCm is currently supported only for Linux systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable ROCm support, export the environment variable &lt;code&gt;USE_ROCM=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Common&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# CUDA only: Add LAPACK support for the GPU if needed&#xA;conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed&#xA;conda install pkg-config libuv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed.&#xA;# Distributed package support on Windows is a prototype feature and is subject to changes.&#xA;conda install -c conda-forge libuv=1.39&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Get the PyTorch Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/pytorch/pytorch&#xA;cd pytorch&#xA;# if you are updating an existing checkout&#xA;git submodule sync&#xA;git submodule update --init --recursive --jobs 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are compiling for ROCm, you must run this command first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/amd_build/build_amd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are using &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt;, you may experience an error caused by the linker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized&#xA;collect2: error: ld returned 1 exit status&#xA;error: command &#39;g++&#39; failed with exit status 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is caused by &lt;code&gt;ld&lt;/code&gt; from Conda environment shadowing the system &lt;code&gt;ld&lt;/code&gt;. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.7.6+ and 3.8.1+.&lt;/p&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA is not supported on macOS.&lt;/p&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;p&gt;Choose Correct Visual Studio Version.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes there are regressions in new versions of Visual Studio, so it&#39;s best to use the same Visual Studio Version &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.circleci/scripts/vs_install.ps1&#34;&gt;16.8.5&lt;/a&gt; as Pytorch CI&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from &lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;https://visualstudio.microsoft.com/visual-cpp-build-tools/&lt;/a&gt;. The build tools &lt;em&gt;do not&lt;/em&gt; come with Visual Studio Code by default.&lt;/p&gt; &#xA;&lt;p&gt;If you want to build legacy python code, please refer to &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md#building-on-legacy-code-and-cuda&#34;&gt;Building on legacy code and CUDA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build with CPU&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s fairly easy to build with CPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;conda activate&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#39;ll need to manually download the library and set up the building environment by tweaking &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;LIB&lt;/code&gt;. The instruction &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/notes/windows.rst#building-from-source&#34;&gt;here&lt;/a&gt; is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.&lt;/p&gt; &#xA;&lt;p&gt;Build with CUDA&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm&#34;&gt;NVTX&lt;/a&gt; is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called &#34;Nsight Compute&#34;. To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. &lt;br&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.&lt;/p&gt; &#xA;&lt;p&gt;Additional libraries such as &lt;a href=&#34;https://developer.nvidia.com/magma&#34;&gt;Magma&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN, a.k.a MKLDNN or DNNL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/mozilla/sccache&#34;&gt;Sccache&lt;/a&gt; are often needed. Please refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/tree/master/.jenkins/pytorch/win-test-helpers/installation-helpers&#34;&gt;installation-helper&lt;/a&gt; to install them.&lt;/p&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat&#34;&gt;build_pytorch.bat&lt;/a&gt; script for some other environment variables configurations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmd&#xA;&#xA;:: Set the environment variables after you have downloaded and upzipped the mkl package,&#xA;:: else CMake would throw an error as `Could NOT find OpenMP`.&#xA;set CMAKE_INCLUDE_PATH={Your directory}\mkl\include&#xA;set LIB={Your directory}\mkl\lib;%LIB%&#xA;&#xA;:: Read the content in the previous section carefully before you proceed.&#xA;:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&#xA;:: &#34;Visual Studio 2019 Developer Command Prompt&#34; will be run automatically.&#xA;:: Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&#xA;set CMAKE_GENERATOR_TOOLSET_VERSION=14.27&#xA;set DISTUTILS_USE_SDK=1&#xA;for /f &#34;usebackq tokens=*&#34; %i in (`&#34;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&#34; -version [15^,17^) -products * -latest -property installationPath`) do call &#34;%i\VC\Auxiliary\Build\vcvarsall.bat&#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%&#xA;&#xA;:: [Optional] If you want to override the CUDA host compiler&#xA;set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe&#xA;&#xA;python setup.py install&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Adjust Build Options (Optional)&lt;/h5&gt; &#xA;&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.&lt;/p&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;h4&gt;Using pre-built images&lt;/h4&gt; &#xA;&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building the image yourself&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;# images are tagged as docker.io/${your_docker_username}/pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the Documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build documentation in various formats, you will need &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the &lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt; &#xA;&lt;p&gt;If you get a katex error run &lt;code&gt;npm install katex&lt;/code&gt;. If it persists, try &lt;code&gt;npm install -g katex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Previous Versions&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found on &lt;a href=&#34;https://pytorch.org/previous-versions&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Three-pointers to get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;Examples: easy to understand PyTorch code across all domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/&#34;&gt;The API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/GLOSSARY.md&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;PyTorch Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;Intro to Deep Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34;&gt;Intro to Machine Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-networks-with-pytorch&#34;&gt;Deep Neural Networks with PyTorch from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PyTorch&#34;&gt;PyTorch Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/&#34;&gt;PyTorch Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw&#34;&gt;PyTorch YouTube&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: Discuss implementations, research, etc. &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt; &#xA; &lt;li&gt;Slack: The &lt;a href=&#34;https://pytorch.slack.com/&#34;&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href=&#34;https://goo.gl/forms/PP1AGvNHpSaJP8to1&#34;&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: &lt;a href=&#34;https://eepurl.com/cbG0rv&#34;&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Page: Important announcements about PyTorch. &lt;a href=&#34;https://www.facebook.com/pytorch&#34;&gt;https://www.facebook.com/pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For brand guidelines, please visit our website at &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a 90-day release cycle (major releases). Please let us know if you encounter a bug by &lt;a href=&#34;https://github.com/pytorch/pytorch/issues&#34;&gt;filing an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about making a contribution to Pytorch, please see our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch is currently maintained by &lt;a href=&#34;https://apaszke.github.io/&#34;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&#34;https://github.com/colesbury&#34;&gt;Sam Gross&lt;/a&gt;, &lt;a href=&#34;http://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt; and &lt;a href=&#34;https://github.com/gchanan&#34;&gt;Gregory Chanan&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project is unrelated to &lt;a href=&#34;https://github.com/hughperkins/pytorch&#34;&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>trustwallet/wallet-core</title>
    <updated>2022-05-29T01:31:19Z</updated>
    <id>tag:github.com,2022-05-29:/trustwallet/wallet-core</id>
    <link href="https://github.com/trustwallet/wallet-core" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cross-platform, cross-blockchain wallet library.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/trustwallet/wallet-core/master/docs/banner.png&#34; align=&#34;center&#34; title=&#34;Trust logo&#34;&gt; &#xA;&lt;p&gt;Trust Wallet Core is an open source, cross-platform, mobile-focused library implementing low-level cryptographic wallet functionality for a high number of blockchains. It is a core part of the popular &lt;a href=&#34;https://trustwallet.com&#34;&gt;Trust Wallet&lt;/a&gt;, and some other projects. Most of the code is C++ with a set of strict C interfaces, and idiomatic interfaces for supported languages: Swift for iOS and Java (Kotlin) for Android.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/trustwallet/wallet-core/workflows/iOS%20CI/badge.svg?sanitize=true&#34; alt=&#34;iOS CI&#34;&gt; &lt;img src=&#34;https://github.com/trustwallet/wallet-core/workflows/Android%20CI/badge.svg?sanitize=true&#34; alt=&#34;Android CI&#34;&gt; &lt;img src=&#34;https://github.com/trustwallet/wallet-core/workflows/Linux%20CI/badge.svg?sanitize=true&#34; alt=&#34;Linux CI&#34;&gt; &lt;img src=&#34;https://github.com/trustwallet/wallet-core/workflows/Wasm%20CI/badge.svg?sanitize=true&#34; alt=&#34;Wasm CI&#34;&gt; &lt;img src=&#34;https://github.com/trustwallet/wallet-core/workflows/Docker%20CI/badge.svg?sanitize=true&#34; alt=&#34;Docker CI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/trustwallet/wallet-core&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod&#34; alt=&#34;Gitpod Ready-to-Code&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/TrustWallet/wallet-core.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/trustwallet/wallet-core&#34; alt=&#34;GitHub release (latest by date)&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/SPM-ready-blue&#34; alt=&#34;SPM&#34;&gt; &lt;img src=&#34;https://img.shields.io/cocoapods/v/TrustWalletCore.svg?sanitize=true&#34; alt=&#34;Cocoapods&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;For comprehensive documentation, see &lt;a href=&#34;https://developer.trustwallet.com/wallet-core&#34;&gt;developer.trustwallet.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Supported Blockchains&lt;/h1&gt; &#xA;&lt;p&gt;Wallet Core supports more than &lt;strong&gt;60&lt;/strong&gt; blockchains: Bitcoin, Ethereum, BNB, Cosmos, Solana, and most major blockchain platforms. The full list is &lt;a href=&#34;https://raw.githubusercontent.com/trustwallet/wallet-core/master/docs/registry.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;For build instructions, see &lt;a href=&#34;https://developer.trustwallet.com/wallet-core/building&#34;&gt;developer.trustwallet.com/wallet-core/building&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Using from your project&lt;/h1&gt; &#xA;&lt;p&gt;If you want to use wallet core in your project follow these instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Android&lt;/h2&gt; &#xA;&lt;p&gt;Android releases are hosted on &lt;a href=&#34;https://github.com/trustwallet/wallet-core/packages/700258&#34;&gt;GitHub packages&lt;/a&gt;, please checkout &lt;a href=&#34;https://docs.github.com/en/packages/guides/configuring-gradle-for-use-with-github-packages#installing-a-package&#34;&gt;this installation guide&lt;/a&gt;, you need to add GitHub access token to install it.&lt;/p&gt; &#xA;&lt;p&gt;Add this dependency to build.gradle and run &lt;code&gt;gradle install&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-groovy&#34;&gt;plugins {&#xA;    id &#39;maven&#39;&#xA;}&#xA;&#xA;dependencies {&#xA;    implementation &#39;com.trustwallet:wallet-core:x.y.z&#39;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace x.y.z with latest version: &lt;img src=&#34;https://img.shields.io/github/v/release/trustwallet/wallet-core&#34; alt=&#34;GitHub release (latest by date)&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;iOS&lt;/h2&gt; &#xA;&lt;p&gt;We currently support Swift Package Manager and CocoaPods (will discontinue in the future).&lt;/p&gt; &#xA;&lt;h3&gt;SPM&lt;/h3&gt; &#xA;&lt;p&gt;Download latest &lt;code&gt;Package.swift&lt;/code&gt; from &lt;a href=&#34;https://github.com/trustwallet/wallet-core/releases&#34;&gt;GitHub Releases&lt;/a&gt; and put it in a local &lt;code&gt;WalletCore&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Add this line to the &lt;code&gt;dependencies&lt;/code&gt; parameter in your &lt;code&gt;Package.swift&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;.package(name: &#34;WalletCore&#34;, path: &#34;../WalletCore&#34;),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or add remote url + &lt;code&gt;master&lt;/code&gt; branch, it points to recent (not always latest) binary release.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;.package(name: &#34;WalletCore&#34;, url: &#34;https://github.com/trustwallet/wallet-core&#34;, .branchItem(&#34;master&#34;)),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then add libraries to target&#39;s &lt;code&gt;dependencies&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;.product(name: &#34;WalletCore&#34;, package: &#34;WalletCore&#34;),&#xA;.product(name: &#34;SwiftProtobuf&#34;, package: &#34;WalletCore&#34;),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CocoaPods&lt;/h3&gt; &#xA;&lt;p&gt;Add this line to your Podfile and run &lt;code&gt;pod install&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;pod &#39;TrustWalletCore&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;NPM (beta)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;npm install @trustwallet/wallet-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Go (beta)&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://github.com/trustwallet/wallet-core/tree/master/samples/go&#34;&gt;Go integration sample&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Projects&lt;/h1&gt; &#xA;&lt;p&gt;Projects using Trust Wallet Core. Add yours too!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trustwallet.com&#34;&gt;&lt;img src=&#34;https://trustwallet.com/assets/images/trust_logotype.svg?sanitize=true&#34; alt=&#34;Trust Wallet&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://coinpaprika.com/&#34;&gt;Coinpaprika&lt;/a&gt; | &lt;a href=&#34;https://www.ifwallet.com/&#34;&gt;IFWallet&lt;/a&gt; | &lt;a href=&#34;https://crypto.com&#34;&gt;crypto.com&lt;/a&gt; | &lt;a href=&#34;https://www.alicedapp.com/&#34;&gt;Alice&lt;/a&gt; | &lt;a href=&#34;https://frontier.xyz/&#34;&gt;Frontier&lt;/a&gt; | &lt;a href=&#34;https://tokenary.io/&#34;&gt;Tokenary&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;There are a few community-maintained projects that extend Wallet Core to some additional platforms and languages. Note this is not an endorsement, please do your own research before using them:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Flutter binding &lt;a href=&#34;https://github.com/weishirongzhen/flutter_trust_wallet_core&#34;&gt;https://github.com/weishirongzhen/flutter_trust_wallet_core&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Python binding &lt;a href=&#34;https://github.com/phuang/wallet-core-python&#34;&gt;https://github.com/phuang/wallet-core-python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Wallet Core on Windows &lt;a href=&#34;https://github.com/kaetemi/wallet-core-windows&#34;&gt;https://github.com/kaetemi/wallet-core-windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;The best way to submit feedback and report bugs is to &lt;a href=&#34;https://github.com/trustwallet/wallet-core/issues/new&#34;&gt;open a GitHub issue&lt;/a&gt;. If you want to contribute code please see &lt;a href=&#34;https://developer.trustwallet.com/wallet-core/contributing&#34;&gt;Contributing&lt;/a&gt;. If you want to add support for a new blockchain also see &lt;a href=&#34;https://developer.trustwallet.com/wallet-core/newblockchain&#34;&gt;Adding Support for a New Blockchain&lt;/a&gt;, make sure you have read the &lt;a href=&#34;https://developer.trustwallet.com/wallet-core/newblockchain#requirements&#34;&gt;requirements&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to all the people who contribute. &lt;a href=&#34;https://github.com/trustwallet/wallet-core/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/wallet-core/contributors.svg?width=890&amp;amp;button=false&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Trust Wallet Core is available under the MIT license. See the &lt;a href=&#34;https://raw.githubusercontent.com/trustwallet/wallet-core/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for more info.&lt;/p&gt;</summary>
  </entry>
</feed>