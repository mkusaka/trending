<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-28T01:35:34Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Puellaquae/Desktop-Snake</title>
    <updated>2023-06-28T01:35:34Z</updated>
    <id>tag:github.com,2023-06-28:/Puellaquae/Desktop-Snake</id>
    <link href="https://github.com/Puellaquae/Desktop-Snake" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Snake Game Play With Desktop Icons&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Desktop-Snake&lt;/h1&gt; &#xA;&lt;p&gt;A Snake Game Play With Desktop Icons. Support multi-monitor with rectangle aligned layout.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>triple-Mu/YOLOv8-TensorRT</title>
    <updated>2023-06-28T01:35:34Z</updated>
    <id>tag:github.com,2023-06-28:/triple-Mu/YOLOv8-TensorRT</id>
    <link href="https://github.com/triple-Mu/YOLOv8-TensorRT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YOLOv8 using TensorRT accelerate !&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YOLOv8-TensorRT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;YOLOv8&lt;/code&gt; using TensorRT accelerate !&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/triple-Mu/YOLOv8-TensorRT&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fatrox%2Fsync-dotenv%2Fbadge&amp;amp;style=flat&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/triple-Mu/YOLOv8-TensorRT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.8--3.10-FFD43B?logo=python&#34; alt=&#34;Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;&lt;img src=&#34;https://badgen.net/badge/icon/tensorrt?icon=azurepipelines&amp;amp;label&#34; alt=&#34;img&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/triple-Mu/YOLOv8-TensorRT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/CPP-11%2F14-yellow&#34; alt=&#34;C++&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/triple-Mu/YOLOv8-TensorRT/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/github/license/triple-Mu/YOLOv8-TensorRT&#34; alt=&#34;img&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/triple-Mu/YOLOv8-TensorRT/pulls&#34;&gt;&lt;img src=&#34;https://badgen.net/github/prs/triple-Mu/YOLOv8-TensorRT&#34; alt=&#34;img&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/triple-Mu/YOLOv8-TensorRT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/triple-Mu/YOLOv8-TensorRT?color=ccf&#34; alt=&#34;img&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Prepare the environment&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;CUDA&lt;/code&gt; follow &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#download-the-nvidia-cuda-toolkit&#34;&gt;&lt;code&gt;CUDA official website&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;ðŸš€ RECOMMENDED &lt;code&gt;CUDA&lt;/code&gt; &amp;gt;= 11.4&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;TensorRT&lt;/code&gt; follow &lt;a href=&#34;https://developer.nvidia.com/nvidia-tensorrt-8x-download&#34;&gt;&lt;code&gt;TensorRT official website&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;ðŸš€ RECOMMENDED &lt;code&gt;TensorRT&lt;/code&gt; &amp;gt;= 8.4&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install python requirements.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;&lt;code&gt;ultralytics&lt;/code&gt;&lt;/a&gt; package for ONNX export or TensorRT API building.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install ultralytics&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare your own PyTorch weight such as &lt;code&gt;yolov8s.pt&lt;/code&gt; or &lt;code&gt;yolov8s-seg.pt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTICE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please use the latest &lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;TensorRT&lt;/code&gt;, so that you can achieve the fastest speed !&lt;/p&gt; &#xA;&lt;p&gt;If you have to use a lower version of &lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;TensorRT&lt;/code&gt;, please read the relevant issues carefully !&lt;/p&gt; &#xA;&lt;h1&gt;Normal Usage&lt;/h1&gt; &#xA;&lt;p&gt;If you get ONNX from origin &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;&lt;code&gt;ultralytics&lt;/code&gt;&lt;/a&gt; repo, you should build engine by yourself.&lt;/p&gt; &#xA;&lt;p&gt;You can only use the &lt;code&gt;c++&lt;/code&gt; inference code to deserialize the engine and do inference.&lt;/p&gt; &#xA;&lt;p&gt;You can get more information in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/docs/Normal.md&#34;&gt;&lt;code&gt;Normal.md&lt;/code&gt;&lt;/a&gt; !&lt;/p&gt; &#xA;&lt;p&gt;Besides, other scripts won&#39;t work.&lt;/p&gt; &#xA;&lt;h1&gt;Export End2End ONNX with NMS&lt;/h1&gt; &#xA;&lt;p&gt;You can export your onnx model by &lt;code&gt;ultralytics&lt;/code&gt; API and add postprocess such as bbox decoder and &lt;code&gt;NMS&lt;/code&gt; into ONNX model at the same time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 export-det.py \&#xA;--weights yolov8s.pt \&#xA;--iou-thres 0.65 \&#xA;--conf-thres 0.25 \&#xA;--topk 100 \&#xA;--opset 11 \&#xA;--sim \&#xA;--input-shape 1 3 640 640 \&#xA;--device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Description of all arguments&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--weights&lt;/code&gt; : The PyTorch model you trained.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--iou-thres&lt;/code&gt; : IOU threshold for NMS plugin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--conf-thres&lt;/code&gt; : Confidence threshold for NMS plugin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--topk&lt;/code&gt; : Max number of detection bboxes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--opset&lt;/code&gt; : ONNX opset version, default is 11.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sim&lt;/code&gt; : Whether to simplify your onnx model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--input-shape&lt;/code&gt; : Input shape for you model, should be 4 dimensions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--device&lt;/code&gt; : The CUDA deivce you export engine .&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will get an onnx model whose prefix is the same as input weights.&lt;/p&gt; &#xA;&lt;h3&gt;Just Taste First&lt;/h3&gt; &#xA;&lt;p&gt;If you just want to taste first, you can download the onnx model which are exported by &lt;code&gt;YOLOv8&lt;/code&gt; package and modified by me.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://triplemu.oss-cn-beijing.aliyuncs.com/YOLOv8/ONNX/yolov8n_nms.onnx?OSSAccessKeyId=LTAI5tN1dgmZD4PF8AJUXp3J&amp;amp;Expires=1772936700&amp;amp;Signature=r6HgJTTcCSAxQxD9bKO9qBTtigQ%3D&#34;&gt;&lt;strong&gt;YOLOv8-n&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://triplemu.oss-cn-beijing.aliyuncs.com/YOLOv8/ONNX/yolov8s_nms.onnx?OSSAccessKeyId=LTAI5tN1dgmZD4PF8AJUXp3J&amp;amp;Expires=1682936722&amp;amp;Signature=JjxQFx1YElcVdsCaMoj81KJ4a5s%3D&#34;&gt;&lt;strong&gt;YOLOv8-s&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://triplemu.oss-cn-beijing.aliyuncs.com/YOLOv8/ONNX/yolov8m_nms.onnx?OSSAccessKeyId=LTAI5tN1dgmZD4PF8AJUXp3J&amp;amp;Expires=1682936739&amp;amp;Signature=IRKBELdVFemD7diixxxgzMYqsWg%3D&#34;&gt;&lt;strong&gt;YOLOv8-m&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://triplemu.oss-cn-beijing.aliyuncs.com/YOLOv8/ONNX/yolov8l_nms.onnx?OSSAccessKeyId=LTAI5tN1dgmZD4PF8AJUXp3J&amp;amp;Expires=1682936763&amp;amp;Signature=RGkJ4G2XJ4J%2BNiki5cJi3oBkDnA%3D&#34;&gt;&lt;strong&gt;YOLOv8-l&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://triplemu.oss-cn-beijing.aliyuncs.com/YOLOv8/ONNX/yolov8x_nms.onnx?OSSAccessKeyId=LTAI5tN1dgmZD4PF8AJUXp3J&amp;amp;Expires=1673936778&amp;amp;Signature=3o%2F7QKhiZg1dW3I6sDrY4ug6MQU%3D&#34;&gt;&lt;strong&gt;YOLOv8-x&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Build End2End Engine from ONNX&lt;/h1&gt; &#xA;&lt;h3&gt;1. Build Engine by TensorRT ONNX Python api&lt;/h3&gt; &#xA;&lt;p&gt;You can export TensorRT engine from ONNX by &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/build.py&#34;&gt;&lt;code&gt;build.py&lt;/code&gt; &lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 build.py \&#xA;--weights yolov8s.onnx \&#xA;--iou-thres 0.65 \&#xA;--conf-thres 0.25 \&#xA;--topk 100 \&#xA;--fp16  \&#xA;--device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Description of all arguments&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--weights&lt;/code&gt; : The ONNX model you download.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--iou-thres&lt;/code&gt; : IOU threshold for NMS plugin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--conf-thres&lt;/code&gt; : Confidence threshold for NMS plugin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--topk&lt;/code&gt; : Max number of detection bboxes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--fp16&lt;/code&gt; : Whether to export half-precision engine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--device&lt;/code&gt; : The CUDA deivce you export engine .&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can modify &lt;code&gt;iou-thres&lt;/code&gt; &lt;code&gt;conf-thres&lt;/code&gt; &lt;code&gt;topk&lt;/code&gt; by yourself.&lt;/p&gt; &#xA;&lt;h3&gt;2. Export Engine by Trtexec Tools&lt;/h3&gt; &#xA;&lt;p&gt;You can export TensorRT engine by &lt;a href=&#34;https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec&#34;&gt;&lt;code&gt;trtexec&lt;/code&gt;&lt;/a&gt; tools.&lt;/p&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/usr/src/tensorrt/bin/trtexec \&#xA;--onnx=yolov8s.onnx \&#xA;--saveEngine=yolov8s.engine \&#xA;--fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you installed TensorRT by a debian package, then the installation path of &lt;code&gt;trtexec&lt;/code&gt; is &lt;code&gt;/usr/src/tensorrt/bin/trtexec&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you installed TensorRT by a tar package, then the installation path of &lt;code&gt;trtexec&lt;/code&gt; is under the &lt;code&gt;bin&lt;/code&gt; folder in the path you decompressed&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Build TensorRT Engine by TensorRT API&lt;/h1&gt; &#xA;&lt;p&gt;Please see more information in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/docs/API-Build.md&#34;&gt;&lt;code&gt;API-Build.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Notice !!!&lt;/strong&gt;&lt;/em&gt; We don&#39;t support YOLOv8-seg model now !!!&lt;/p&gt; &#xA;&lt;h1&gt;Inference&lt;/h1&gt; &#xA;&lt;h2&gt;1. Infer with python script&lt;/h2&gt; &#xA;&lt;p&gt;You can infer images with the engine by &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/infer-det.py&#34;&gt;&lt;code&gt;infer-det.py&lt;/code&gt;&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 infer-det.py \&#xA;--engine yolov8s.engine \&#xA;--imgs data \&#xA;--show \&#xA;--out-dir outputs \&#xA;--device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Description of all arguments&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--engine&lt;/code&gt; : The Engine you export.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--imgs&lt;/code&gt; : The images path you want to detect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--show&lt;/code&gt; : Whether to show detection results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--out-dir&lt;/code&gt; : Where to save detection results images. It will not work when use &lt;code&gt;--show&lt;/code&gt; flag.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--device&lt;/code&gt; : The CUDA deivce you use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--profile&lt;/code&gt; : Profile the TensorRT engine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;2. Infer with C++&lt;/h2&gt; &#xA;&lt;p&gt;You can infer with c++ in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/csrc/detect/end2end&#34;&gt;&lt;code&gt;csrc/detect/end2end&lt;/code&gt;&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h3&gt;Build:&lt;/h3&gt; &#xA;&lt;p&gt;Please set you own librarys in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/csrc/detect/end2end/CMakeLists.txt&#34;&gt;&lt;code&gt;CMakeLists.txt&lt;/code&gt;&lt;/a&gt; and modify &lt;code&gt;CLASS_NAMES&lt;/code&gt; and &lt;code&gt;COLORS&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/csrc/detect/end2end/main.cpp&#34;&gt;&lt;code&gt;main.cpp&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export root=${PWD}&#xA;cd csrc/detect/end2end&#xA;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;make&#xA;mv yolov8 ${root}&#xA;cd ${root}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# infer image&#xA;./yolov8 yolov8s.engine data/bus.jpg&#xA;# infer images&#xA;./yolov8 yolov8s.engine data&#xA;# infer video&#xA;./yolov8 yolov8s.engine data/test.mp4 # the video path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;TensorRT Segment Deploy&lt;/h1&gt; &#xA;&lt;p&gt;Please see more information in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/docs/Segment.md&#34;&gt;&lt;code&gt;Segment.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;TensorRT Pose Deploy&lt;/h1&gt; &#xA;&lt;p&gt;Please see more information in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/docs/Pose.md&#34;&gt;&lt;code&gt;Pose.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;DeepStream Detection Deploy&lt;/h1&gt; &#xA;&lt;p&gt;See more in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/csrc/deepstream/README.md&#34;&gt;&lt;code&gt;README.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Jetson Deploy&lt;/h1&gt; &#xA;&lt;p&gt;Only test on &lt;code&gt;Jetson-NX 4GB&lt;/code&gt;. See more in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/docs/Jetson.md&#34;&gt;&lt;code&gt;Jetson.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Profile you engine&lt;/h1&gt; &#xA;&lt;p&gt;If you want to profile the TensorRT engine:&lt;/p&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 trt-profile.py --engine yolov8s.engine --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Refuse To Use PyTorch for Model Inference !!!&lt;/h1&gt; &#xA;&lt;p&gt;If you need to break away from pytorch and use tensorrt inference, you can get more information in &lt;a href=&#34;https://raw.githubusercontent.com/triple-Mu/YOLOv8-TensorRT/main/infer-det-without-torch.py&#34;&gt;&lt;code&gt;infer-det-without-torch.py&lt;/code&gt;&lt;/a&gt;, the usage is the same as the pytorch version, but its performance is much worse.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;cuda-python&lt;/code&gt; or &lt;code&gt;pycuda&lt;/code&gt; for inference. Please install by such command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install cuda-python&#xA;# or&#xA;pip install pycuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 infer-det-without-torch.py \&#xA;--engine yolov8s.engine \&#xA;--imgs data \&#xA;--show \&#xA;--out-dir outputs \&#xA;--method cudart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Description of all arguments&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--engine&lt;/code&gt; : The Engine you export.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--imgs&lt;/code&gt; : The images path you want to detect.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--show&lt;/code&gt; : Whether to show detection results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--out-dir&lt;/code&gt; : Where to save detection results images. It will not work when use &lt;code&gt;--show&lt;/code&gt; flag.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--method&lt;/code&gt; : Choose &lt;code&gt;cudart&lt;/code&gt; or &lt;code&gt;pycuda&lt;/code&gt;, default is &lt;code&gt;cudart&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>