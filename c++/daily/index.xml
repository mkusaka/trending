<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-18T01:31:25Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rpng/open_vins</title>
    <updated>2023-04-18T01:31:25Z</updated>
    <id>tag:github.com,2023-04-18:/rpng/open_vins</id>
    <link href="https://github.com/rpng/open_vins" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source platform for visual-inertial navigation research.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenVINS&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rpng/open_vins/actions/workflows/build_ros1.yml&#34;&gt;&lt;img src=&#34;https://github.com/rpng/open_vins/actions/workflows/build_ros1.yml/badge.svg?sanitize=true&#34; alt=&#34;ROS 1 Workflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rpng/open_vins/actions/workflows/build_ros2.yml&#34;&gt;&lt;img src=&#34;https://github.com/rpng/open_vins/actions/workflows/build_ros2.yml/badge.svg?sanitize=true&#34; alt=&#34;ROS 2 Workflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rpng/open_vins/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/rpng/open_vins/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;ROS Free Workflow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the OpenVINS project! The OpenVINS project houses some core computer vision code along with a state-of-the art filter-based visual-inertial estimator. The core filter is an &lt;a href=&#34;https://en.wikipedia.org/wiki/Extended_Kalman_filter&#34;&gt;Extended Kalman filter&lt;/a&gt; which fuses inertial information with sparse visual feature tracks. These visual feature tracks are fused leveraging the &lt;a href=&#34;https://ieeexplore.ieee.org/document/4209642&#34;&gt;Multi-State Constraint Kalman Filter (MSCKF)&lt;/a&gt; sliding window formulation which allows for 3D features to update the state estimate without directly estimating the feature states in the filter. Inspired by graph-based optimization systems, the included filter has modularity allowing for convenient covariance management with a proper type-based state system. Please take a look at the feature list below for full details on what the system supports.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Github project page - &lt;a href=&#34;https://github.com/rpng/open_vins&#34;&gt;https://github.com/rpng/open_vins&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Documentation - &lt;a href=&#34;https://docs.openvins.com/&#34;&gt;https://docs.openvins.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Getting started guide - &lt;a href=&#34;https://docs.openvins.com/getting-started.html&#34;&gt;https://docs.openvins.com/getting-started.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Publication reference - &lt;a href=&#34;https://pgeneva.com/downloads/papers/Geneva2020ICRA.pdf&#34;&gt;https://pgeneva.com/downloads/papers/Geneva2020ICRA.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News / Events&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;April 15, 2023&lt;/strong&gt; - Minor update to v2.6.3 to support incremental feature triangulation of active features for downstream applications, faster zero-velocity update, small bug fixes, some example realsense configurations, and cached fast state prediction. Please check out the &lt;a href=&#34;https://github.com/rpng/open_vins/releases/tag/v2.6.3&#34;&gt;release page&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;April 3, 2023&lt;/strong&gt; - We have released a monocular plane-aided VINS, termed &lt;a href=&#34;https://github.com/rpng/ov_plane&#34;&gt;ov_plane&lt;/a&gt;, which leverages the OpenVINS project. Both now support the released &lt;a href=&#34;https://github.com/rpng/ar_table_dataset&#34;&gt;Indoor AR Table&lt;/a&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;July 14, 2022&lt;/strong&gt; - Improved feature extraction logic for &amp;gt;100hz tracking, some bug fixes and updated scripts. See v2.6.1 &lt;a href=&#34;https://github.com/rpng/open_vins/pull/259&#34;&gt;PR#259&lt;/a&gt; and v2.6.2 &lt;a href=&#34;https://github.com/rpng/open_vins/pull/264&#34;&gt;PR#264&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;March 14, 2022&lt;/strong&gt; - Initial dynamic initialization open sourcing, asynchronous subscription to inertial readings and publishing of odometry, support for lower frequency feature tracking. See v2.6 &lt;a href=&#34;https://github.com/rpng/open_vins/pull/232&#34;&gt;PR#232&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;December 13, 2021&lt;/strong&gt; - New YAML configuration system, ROS2 support, Docker images, robust static initialization based on disparity, internal logging system to reduce verbosity, image transport publishers, dynamic number of features support, and other small fixes. See v2.5 &lt;a href=&#34;https://github.com/rpng/open_vins/pull/209&#34;&gt;PR#209&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;July 19, 2021&lt;/strong&gt; - Camera classes, masking support, alignment utility, and other small fixes. See v2.4 &lt;a href=&#34;https://github.com/rpng/open_vins/pull/186&#34;&gt;PR#117&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;December 1, 2020&lt;/strong&gt; - Released improved memory management, active feature pointcloud publishing, limiting number of features in update to bound compute, and other small fixes. See v2.3 &lt;a href=&#34;https://github.com/rpng/open_vins/pull/117&#34;&gt;PR#117&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;November 18, 2020&lt;/strong&gt; - Released groundtruth generation utility package, &lt;a href=&#34;https://github.com/rpng/vicon2gt&#34;&gt;vicon2gt&lt;/a&gt; to enable creation of groundtruth trajectories in a motion capture room for evaulating VIO methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;July 7, 2020&lt;/strong&gt; - Released zero velocity update for vehicle applications and direct initialization when standing still. See &lt;a href=&#34;https://github.com/rpng/open_vins/pull/79&#34;&gt;PR#79&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;May 18, 2020&lt;/strong&gt; - Released secondary pose graph example repository &lt;a href=&#34;https://github.com/rpng/ov_secondary&#34;&gt;ov_secondary&lt;/a&gt; based on &lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Fusion&#34;&gt;VINS-Fusion&lt;/a&gt;. OpenVINS now publishes marginalized feature track, feature 3d position, and first camera intrinsics and extrinsics. See &lt;a href=&#34;https://github.com/rpng/open_vins/pull/66&#34;&gt;PR#66&lt;/a&gt; for details and discussion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;April 3, 2020&lt;/strong&gt; - Released &lt;a href=&#34;https://github.com/rpng/open_vins/releases/tag/v2.0&#34;&gt;v2.0&lt;/a&gt; update to the codebase with some key refactoring, ros-free building, improved dataset support, and single inverse depth feature representation. Please check out the &lt;a href=&#34;https://github.com/rpng/open_vins/releases/tag/v2.0&#34;&gt;release page&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;January 21, 2020&lt;/strong&gt; - Our paper has been accepted for presentation in &lt;a href=&#34;https://www.icra2020.org/&#34;&gt;ICRA 2020&lt;/a&gt;. We look forward to seeing everybody there! We have also added links to a few videos of the system running on different datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;October 23, 2019&lt;/strong&gt; - OpenVINS placed first in the &lt;a href=&#34;http://rpg.ifi.uzh.ch/uzh-fpv.html&#34;&gt;IROS 2019 FPV Drone Racing VIO Competition &lt;/a&gt;. We will be giving a short presentation at the &lt;a href=&#34;https://wp.nyu.edu/workshopiros2019mav/&#34;&gt;workshop&lt;/a&gt; at 12:45pm in Macau on November 8th.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;October 1, 2019&lt;/strong&gt; - We will be presenting at the &lt;a href=&#34;http://udel.edu/~ghuang/iros19-vins-workshop/index.html&#34;&gt;Visual-Inertial Navigation: Challenges and Applications &lt;/a&gt; workshop at &lt;a href=&#34;https://www.iros2019.org/&#34;&gt;IROS 2019&lt;/a&gt;. The submitted workshop paper can be found at &lt;a href=&#34;http://udel.edu/~ghuang/iros19-vins-workshop/papers/06.pdf&#34;&gt;this&lt;/a&gt; link.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;August 21, 2019&lt;/strong&gt; - Open sourced &lt;a href=&#34;https://github.com/rpng/ov_maplab&#34;&gt;ov_maplab&lt;/a&gt; for interfacing OpenVINS with the &lt;a href=&#34;https://github.com/ethz-asl/maplab&#34;&gt;maplab&lt;/a&gt; library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;August 15, 2019&lt;/strong&gt; - Initial release of OpenVINS repository and documentation website!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sliding window visual-inertial MSCKF&lt;/li&gt; &#xA; &lt;li&gt;Modular covariance type system&lt;/li&gt; &#xA; &lt;li&gt;Comprehensive documentation and derivations&lt;/li&gt; &#xA; &lt;li&gt;Extendable visual-inertial simulator &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On manifold SE(3) b-spline&lt;/li&gt; &#xA;   &lt;li&gt;Arbitrary number of cameras&lt;/li&gt; &#xA;   &lt;li&gt;Arbitrary sensor rate&lt;/li&gt; &#xA;   &lt;li&gt;Automatic feature generation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Five different feature representations &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Global XYZ&lt;/li&gt; &#xA;   &lt;li&gt;Global inverse depth&lt;/li&gt; &#xA;   &lt;li&gt;Anchored XYZ&lt;/li&gt; &#xA;   &lt;li&gt;Anchored inverse depth&lt;/li&gt; &#xA;   &lt;li&gt;Anchored MSCKF inverse depth&lt;/li&gt; &#xA;   &lt;li&gt;Anchored single inverse depth&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Calibration of sensor intrinsics and extrinsics &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Camera to IMU transform&lt;/li&gt; &#xA;   &lt;li&gt;Camera to IMU time offset&lt;/li&gt; &#xA;   &lt;li&gt;Camera intrinsics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Environmental SLAM feature &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenCV ARUCO tag SLAM features&lt;/li&gt; &#xA;   &lt;li&gt;Sparse feature SLAM features&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Visual tracking support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Monocular camera&lt;/li&gt; &#xA;   &lt;li&gt;Stereo camera (synchronized)&lt;/li&gt; &#xA;   &lt;li&gt;Binocular cameras (synchronized)&lt;/li&gt; &#xA;   &lt;li&gt;KLT or descriptor based&lt;/li&gt; &#xA;   &lt;li&gt;Masked tracking&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Static and dynamic state initialization&lt;/li&gt; &#xA; &lt;li&gt;Zero velocity detection and updates&lt;/li&gt; &#xA; &lt;li&gt;Out of the box evaluation on EurocMav, TUM-VI, UZH-FPV, KAIST Urban and VIO datasets&lt;/li&gt; &#xA; &lt;li&gt;Extensive evaluation suite (ATE, RPE, NEES, RMSE, etc..)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Codebase Extensions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/rpng/ov_plane&#34;&gt;ov_plane&lt;/a&gt;&lt;/strong&gt; - A real-time monocular visual-inertial odometry (VIO) system which leverages environmental planes. At the core it presents an efficient robust monocular-based plane detection algorithm which does not require additional sensing modalities such as a stereo, depth camera or neural network. The plane detection and tracking algorithm enables real-time regularization of point features to environmental planes which are either maintained in the state vector as long-lived planes, or marginalized for efficiency. Planar regularities are applied to both in-state SLAM and out-of-state MSCKF point features, enabling long-term point-to-plane loop-closures due to the large spacial volume of planes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/rpng/vicon2gt&#34;&gt;vicon2gt&lt;/a&gt;&lt;/strong&gt; - This utility was created to generate groundtruth trajectories using a motion capture system (e.g. Vicon or OptiTrack) for use in evaluating visual-inertial estimation systems. Specifically we calculate the inertial IMU state (full 15 dof) at camera frequency rate and generate a groundtruth trajectory similar to those provided by the EurocMav datasets. Performs fusion of inertial and motion capture information and estimates all unknown spacial-temporal calibrations between the two sensors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/rpng/ov_maplab&#34;&gt;ov_maplab&lt;/a&gt;&lt;/strong&gt; - This codebase contains the interface wrapper for exporting visual-inertial runs from &lt;a href=&#34;https://github.com/rpng/open_vins&#34;&gt;OpenVINS&lt;/a&gt; into the ViMap structure taken by &lt;a href=&#34;https://github.com/ethz-asl/maplab&#34;&gt;maplab&lt;/a&gt;. The state estimates and raw images are appended to the ViMap as OpenVINS runs through a dataset. After completion of the dataset, features are re-extract and triangulate with maplab&#39;s feature system. This can be used to merge multi-session maps, or to perform a batch optimization after first running the data through OpenVINS. Some example have been provided along with a helper script to export trajectories into the standard groundtruth format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/rpng/ov_secondary&#34;&gt;ov_secondary&lt;/a&gt;&lt;/strong&gt; - This is an example secondary thread which provides loop closure in a loosely coupled manner for &lt;a href=&#34;https://github.com/rpng/open_vins&#34;&gt;OpenVINS&lt;/a&gt;. This is a modification of the code originally developed by the HKUST aerial robotics group and can be found in their &lt;a href=&#34;https://github.com/HKUST-Aerial-Robotics/VINS-Fusion&#34;&gt;VINS-Fusion&lt;/a&gt; repository. Here we stress that this is a loosely coupled method, thus no information is returned to the estimator to improve the underlying OpenVINS odometry. This codebase has been modified in a few key areas including: exposing more loop closure parameters, subscribing to camera intrinsics, simplifying configuration such that only topics need to be supplied, and some tweaks to the loop closure detection to improve frequency.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo Videos&lt;/h2&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=KCX51GvYGss&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/KCX51GvYGss.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=Lc7VQHngSuQ&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/Lc7VQHngSuQ.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=vaia7iPaRW8&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/vaia7iPaRW8.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=MCzTF9ye2zw&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/MCzTF9ye2zw.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=eSQLWcNrx_I&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/eSQLWcNrx_I.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;br&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=187AXuuGNNw&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/187AXuuGNNw.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=oUoLlrFryk0&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/oUoLlrFryk0.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=ExPIGwORm4E&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/ExPIGwORm4E.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;http://www.youtube.com/watch?v=lXHl-qgLGl8&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rpng/open_vins/master/docs/youtube/lXHl-qgLGl8.jpg&#34; width=&#34;120&#34; height=&#34;90&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Credit / Licensing&lt;/h2&gt; &#xA;&lt;p&gt;This code was written by the &lt;a href=&#34;https://sites.udel.edu/robot/&#34;&gt;Robot Perception and Navigation Group (RPNG)&lt;/a&gt; at the University of Delaware. If you have any issues with the code please open an issue on our github page with relevant implementation details and references. For researchers that have leveraged or compared to this work, please cite the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@Conference{Geneva2020ICRA,&#xA;  Title      = {{OpenVINS}: A Research Platform for Visual-Inertial Estimation},&#xA;  Author     = {Patrick Geneva and Kevin Eckenhoff and Woosik Lee and Yulin Yang and Guoquan Huang},&#xA;  Booktitle  = {Proc. of the IEEE International Conference on Robotics and Automation},&#xA;  Year       = {2020},&#xA;  Address    = {Paris, France},&#xA;  Url        = {\url{https://github.com/rpng/open_vins}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The codebase and documentation is licensed under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.txt&#34;&gt;GNU General Public License v3 (GPL-3)&lt;/a&gt;. You must preserve the copyright and license notices in your derivative work and make available the complete source code with modifications under the same license (&lt;a href=&#34;https://choosealicense.com/licenses/gpl-3.0/&#34;&gt;see this&lt;/a&gt;; this is not legal advice).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>endless-sky/endless-sky</title>
    <updated>2023-04-18T01:31:25Z</updated>
    <id>tag:github.com,2023-04-18:/endless-sky/endless-sky</id>
    <link href="https://github.com/endless-sky/endless-sky" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Space exploration, trading, and combat game.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Endless Sky&lt;/h1&gt; &#xA;&lt;p&gt;Explore other star systems. Earn money by trading, carrying passengers, or completing missions. Use your earnings to buy a better ship or to upgrade the weapons and engines on your current one. Blow up pirates. Take sides in a civil war. Or leave human space behind and hope to find some friendly aliens whose culture is more civilized than your own...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Endless Sky is a sandbox-style space exploration game similar to Elite, Escape Velocity, or Star Control. You start out as the captain of a tiny spaceship and can choose what to do from there. The game includes a major plot line and many minor missions, but you can choose whether you want to play through the plot or strike out on your own as a merchant or bounty hunter or explorer.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/endless-sky/endless-sky/wiki/PlayersManual&#34;&gt;player&#39;s manual&lt;/a&gt; for more information, or the &lt;a href=&#34;https://endless-sky.github.io/&#34;&gt;home page&lt;/a&gt; for screenshots and the occasional blog post.&lt;/p&gt; &#xA;&lt;h2&gt;Installing the game&lt;/h2&gt; &#xA;&lt;p&gt;Official releases of Endless Sky are available as direct downloads from &lt;a href=&#34;https://github.com/endless-sky/endless-sky/releases/latest&#34;&gt;GitHub&lt;/a&gt;, on &lt;a href=&#34;https://store.steampowered.com/app/404410/Endless_Sky/&#34;&gt;Steam&lt;/a&gt;, and on &lt;a href=&#34;https://flathub.org/apps/details/io.github.endless_sky.endless_sky&#34;&gt;Flathub&lt;/a&gt;. Other package managers may also include the game, though the specific version provided may not be up-to-date.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Endless Sky has very minimal system requirements, meaning most systems should be able to run the game. The most restrictive requirement is likely that your device must support at least OpenGL 3.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Minimum&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Recommended&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RAM&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;500 MB&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Graphics&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;OpenGL 3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;OpenGL 3.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Storage Free&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 MB&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Building from source&lt;/h2&gt; &#xA;&lt;p&gt;Most development is done on Linux and Windows, using CMake (&lt;a href=&#34;https://raw.githubusercontent.com/endless-sky/endless-sky/master/readme-cmake.md&#34;&gt;build instructions&lt;/a&gt;) to compile the project. For those wishing to use an IDE, project files are provided for &lt;a href=&#34;https://www.codeblocks.org/&#34;&gt;Code::Blocks&lt;/a&gt; to simplify the project setup, and other IDEs are supported through their respective CMake integration. &lt;a href=&#34;https://scons.org/&#34;&gt;SCons&lt;/a&gt; was the primary build tool up until 0.9.16, and some files and information continue to be available for it. For full installation instructions, consult the &lt;a href=&#34;https://github.com/endless-sky/endless-sky/raw/master/readme-developer.md&#34;&gt;Build Instructions&lt;/a&gt; readme.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As a free and open source game, Endless Sky is the product of many people&#39;s work. Contributions of artwork, storylines, and other writing are most in-demand, though there is a loosely defined &lt;a href=&#34;https://github.com/endless-sky/endless-sky/wiki/DevelopmentRoadmap&#34;&gt;roadmap&lt;/a&gt;. Those who wish to &lt;a href=&#34;https://raw.githubusercontent.com/endless-sky/endless-sky/master/CONTRIBUTING.md&#34;&gt;contribute&lt;/a&gt; are encouraged to review the &lt;a href=&#34;https://github.com/endless-sky/endless-sky/wiki&#34;&gt;wiki&lt;/a&gt;, and to post in the &lt;a href=&#34;https://discord.gg/ZeuASSx&#34;&gt;community-run Discord&lt;/a&gt; beforehand. Those who prefer to use Steam can use its &lt;a href=&#34;https://steamcommunity.com/app/404410/discussions/&#34;&gt;discussion rooms&lt;/a&gt; as well, or GitHub&#39;s &lt;a href=&#34;https://github.com/endless-sky/endless-sky/discussions&#34;&gt;discussion zone&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Endless Sky&#39;s main discussion and development area was once &lt;a href=&#34;https://groups.google.com/g/endless-sky&#34;&gt;Google Groups&lt;/a&gt;, but due to factors outside our control, it is now inaccessible to new users, and should not be used anymore.&lt;/p&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;Endless Sky is a free, open source game. The &lt;a href=&#34;https://github.com/endless-sky/endless-sky/&#34;&gt;source code&lt;/a&gt; is available under the GPL v3 license, and all the artwork is either public domain or released under a variety of Creative Commons (and similarly permissive) licenses. (To determine the copyright status of any of the artwork, consult the &lt;a href=&#34;https://github.com/endless-sky/endless-sky/raw/master/copyright&#34;&gt;copyright file&lt;/a&gt;.)&lt;/p&gt;</summary>
  </entry>
</feed>