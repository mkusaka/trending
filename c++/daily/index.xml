<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-24T01:31:09Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Maknee/minigpt4.cpp</title>
    <updated>2023-07-24T01:31:09Z</updated>
    <id>tag:github.com,2023-07-24:/Maknee/minigpt4.cpp</id>
    <link href="https://github.com/Maknee/minigpt4.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of MiniGPT4 in C++ (4bit, 5bit, 6bit, 8bit, 16bit CPU inference with GGML)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minigpt4.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/maknee/minigpt4.cpp&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://colab.research.google.com/github/Maknee/minigpt4.cpp/blob/master/minigpt4/colab_webui.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Quickstart in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inference of &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT4&lt;/a&gt; in pure C/C++.&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of &lt;code&gt;minigpt4.cpp&lt;/code&gt; is to run minigpt4 using 4-bit quantization with using the &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Maknee/minigpt4.cpp/master/assets/webui_demo.png&#34; alt=&#34;minigpt1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Maknee/minigpt4.cpp/master/assets/minigpt4-demo1.gif&#34; alt=&#34;minigpt1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone repo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: &lt;a href=&#34;https://gitforwindows.org/&#34;&gt;git&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/Maknee/minigpt4.cpp&#xA;cd minigpt4.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Getting the library&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Download precompiled binary&lt;/h4&gt; &#xA;&lt;h5&gt;Windows / Linux / MacOS&lt;/h5&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://github.com/Maknee/minigpt4.cpp/releases&#34;&gt;Releases&lt;/a&gt; and extract &lt;code&gt;minigpt4&lt;/code&gt; library file into the repository directory.&lt;/p&gt; &#xA;&lt;h4&gt;Option 2: Build library manually&lt;/h4&gt; &#xA;&lt;h5&gt;Windows&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: &lt;a href=&#34;https://cmake.org/download/&#34;&gt;CMake&lt;/a&gt;, &lt;a href=&#34;https://visualstudio.microsoft.com/&#34;&gt;Visual Studio&lt;/a&gt; and &lt;a href=&#34;https://gitforwindows.org/&#34;&gt;Git&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;bin\Release\minigpt4.dll&lt;/code&gt; should be generated&lt;/p&gt; &#xA;&lt;h5&gt;Linux&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: CMake (Ubuntu: &lt;code&gt;sudo apt install cmake&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;minigpt4.so&lt;/code&gt; should be generated&lt;/p&gt; &#xA;&lt;h5&gt;MacOS&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: CMake (MacOS: &lt;code&gt;brew install cmake&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;minigpt4.dylib&lt;/code&gt; should be generated&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you build with opencv (allowing features such as loading and preprocessing image within the library itself), set &lt;code&gt;MINIGPT4_BUILD_WITH_OPENCV&lt;/code&gt; to &lt;code&gt;ON&lt;/code&gt; in &lt;code&gt;CMakeLists.txt&lt;/code&gt; or build with &lt;code&gt;-DMINIGPT4_BUILD_WITH_OPENCV=ON&lt;/code&gt; as a parameter to the cmake cli.&lt;/p&gt; &#xA;&lt;h3&gt;3. Obtaining the model&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Download pre-quantized MiniGPT4 model&lt;/h4&gt; &#xA;&lt;p&gt;Pre-quantized models are avaliable on Hugging Face ~ &lt;a href=&#34;https://huggingface.co/datasets/maknee/minigpt4-7b-ggml/tree/main&#34;&gt;7B&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/datasets/maknee/minigpt4-13b-ggml/tree/main&#34;&gt;13B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Recommended for reliable results, but slow inference speed: &lt;a href=&#34;https://huggingface.co/datasets/maknee/minigpt4-13b-ggml/blob/main/minigpt4-13B-f16.bin&#34;&gt;minigpt4-13B-f16.bin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Option 2: Convert and quantize PyTorch model&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.x&lt;/a&gt; and &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Clone the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt; repository and perform the setup&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd minigpt4&#xA;git clone https://github.com/Vision-CAIR/MiniGPT-4.git&#xA;cd MiniGPT-4&#xA;conda env create -f environment.yml&#xA;conda activate minigpt4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained checkpoint in the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt; repository under &lt;code&gt;Checkpoint Aligned with Vicuna 7B&lt;/code&gt; or &lt;code&gt;Checkpoint Aligned with Vicuna 13B&lt;/code&gt; or download them from &lt;a href=&#34;https://huggingface.co/datasets/maknee/minigpt4-7b-ggml/blob/main/pretrained_minigpt4_7b.pth&#34;&gt;Huggingface link for 7B&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/datasets/maknee/minigpt4-13b-ggml/blob/main/pretrained_minigpt4.pth&#34;&gt;13B&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Convert the model weights into ggml format&lt;/p&gt; &#xA;&lt;h5&gt;Windows&lt;/h5&gt; &#xA;&lt;p&gt;7B model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cd minigpt4&#xA;python convert.py C:\pretrained_minigpt4_7b.pth --ftype=f16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;13B model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cd minigpt4&#xA;python convert.py C:\pretrained_minigpt4.pth --ftype=f16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Linux / MacOS&lt;/h5&gt; &#xA;&lt;p&gt;7B model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python convert.py ~/Downloads/pretrained_minigpt4_7b.pth --outtype f16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;13B model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python convert.py ~/Downloads/pretrained_minigpt4.pth --outtype f16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;minigpt4-7B-f16.bin&lt;/code&gt; or &lt;code&gt;minigpt4-13B-f16.bin&lt;/code&gt; should be generated&lt;/p&gt; &#xA;&lt;h4&gt;4. Obtaining the vicuna model&lt;/h4&gt; &#xA;&lt;h4&gt;Option 1: Download pre-quantized vicuna-v0 model&lt;/h4&gt; &#xA;&lt;p&gt;Pre-quantized models are avaliable on &lt;a href=&#34;https://huggingface.co/datasets/maknee/ggml-vicuna-v0-quantized/tree/main&#34;&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Recommended for reliable results and decent inference speed: &lt;a href=&#34;https://huggingface.co/datasets/maknee/ggml-vicuna-v0-quantized/blob/main/ggml-vicuna-13B-v0-q5_k.bin&#34;&gt;ggml-vicuna-13B-v0-q5_k.bin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Option 2: Convert and quantize vicuna-v0 model&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.x&lt;/a&gt; and &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4/raw/main/PrepareVicuna.md&#34;&gt;guide from the MiniGPT4&lt;/a&gt; to obtain the vicuna-v0 model.&lt;/p&gt; &#xA;&lt;p&gt;Then, clone llama.cpp&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/ggerganov/llama.cpp&#xA;cd llama.cpp&#xA;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Convert the model to ggml&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python convert.py &amp;lt;path-to-model&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Quantize the model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python quanitize &amp;lt;path-to-model&amp;gt; &amp;lt;output-model&amp;gt; Q4_1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5. Running&lt;/h4&gt; &#xA;&lt;p&gt;Test if minigpt4 works by calling the following, replacing &lt;code&gt;minigpt4-13B-f16.bin&lt;/code&gt; and &lt;code&gt;ggml-vicuna-13B-v0-q5_k.bin&lt;/code&gt; with your respective models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd minigpt4&#xA;python minigpt4_library.py minigpt4-13B-f16.bin ggml-vicuna-13B-v0-q5_k.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Webui&lt;/h5&gt; &#xA;&lt;p&gt;Install the requirements for the webui&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run the webui, replacing &lt;code&gt;minigpt4-13B-f16.bin&lt;/code&gt; and &lt;code&gt;ggml-vicuna-13B-v0-q5_k.bin&lt;/code&gt; with your respective models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python webui.py minigpt4-13B-f16.bin ggml-vicuna-13B-v0-q5_k.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output should contain something like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;Running on local URL:  http://127.0.0.1:7860&#xA;&#xA;To create a public link, set `share=True` in `launch()`.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go to &lt;code&gt;http://127.0.0.1:7860&lt;/code&gt; in your browser and you should be able to interact with the webui.&lt;/p&gt;</summary>
  </entry>
</feed>