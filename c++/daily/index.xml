<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-02T01:25:58Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Mozilla-Ocho/llamafile</title>
    <updated>2023-12-02T01:25:58Z</updated>
    <id>tag:github.com,2023-12-02:/Mozilla-Ocho/llamafile</id>
    <link href="https://github.com/Mozilla-Ocho/llamafile" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Distribute and run LLMs with a single file.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llamafile&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/llamafile.png&#34; alt=&#34;llamafile-250&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;llamafile lets you distribute and run LLMs with a single file (&lt;a href=&#34;https://hacks.mozilla.org/2023/11/introducing-llamafile/&#34;&gt;blog post&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our goal is to make the &#34;build once anywhere, run anywhere&#34; dream come true for AI developers. We&#39;re doing that by combining &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; with &lt;a href=&#34;https://github.com/jart/cosmopolitan&#34;&gt;Cosmopolitan Libc&lt;/a&gt; into one framework that lets you build apps for LLMs as a single-file artifact that runs locally on most PCs and servers.&lt;/p&gt; &#xA;&lt;p&gt;First, your llamafiles can run on multiple CPU microarchitectures. We added runtime dispatching to llama.cpp that lets new Intel systems use modern CPU features without trading away support for older computers.&lt;/p&gt; &#xA;&lt;p&gt;Secondly, your llamafiles can run on multiple CPU architectures. We do that by concatenating AMD64 and ARM64 builds with a shell script that launches the appropriate one. Our file format is compatible with WIN32 and most UNIX shells. It&#39;s also able to be easily converted (by either you or your users) to the platform-native format, whenever required.&lt;/p&gt; &#xA;&lt;p&gt;Thirdly, your llamafiles can run on six OSes (macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD). You&#39;ll only need to build your code once, using a Linux-style toolchain. The GCC-based compiler we provide is itself an Actually Portable Executable, so you can build your software for all six OSes from the comfort of whichever one you prefer most for development.&lt;/p&gt; &#xA;&lt;p&gt;Lastly, the weights for your LLM can be embedded within your llamafile. We added support for PKZIP to the GGML library. This lets uncompressed weights be mapped directly into memory, similar to a self-extracting archive. It enables quantized weights distributed online to be prefixed with a compatible version of the llama.cpp software, thereby ensuring its originally observed behaviors can be reproduced indefinitely.&lt;/p&gt; &#xA;&lt;h2&gt;Binary Instructions&lt;/h2&gt; &#xA;&lt;p&gt;We provide example binaries that embed several different models. You can download these from Hugging Face via the links below. &#34;Command-line binaries&#34; run from the command line, just as if you were invoking llama.cpp&#39;s &#34;main&#34; function manually. &#34;Server binaries&#34; launch a local web server (at 127.0.0.1:8080) that provides a web-based chatbot.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Command-line binary&lt;/th&gt; &#xA;   &lt;th&gt;Server binary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral-7B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/mistral-7b.llamafile/resolve/main/mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile?download=true&#34;&gt;mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile (4.07 GB)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/mistral-7b.llamafile/resolve/main/mistral-7b-instruct-v0.1-Q4_K_M-server.llamafile?download=true&#34;&gt;mistral-7b-instruct-v0.1-Q4_K_M-server.llamafile (4.07 GB)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA 1.5&lt;/td&gt; &#xA;   &lt;td&gt;(Not provided because this model&#39;s features are best utilized via the web UI)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-server.llamafile?download=true&#34;&gt;llava-v1.5-7b-q4-server.llamafile (3.97 GB)&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python-13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b-main.llamafile?download=true&#34;&gt;wizardcoder-python-13b-main.llamafile (7.33 GB)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b-server.llamafile?download=true&#34;&gt;wizardcoder-python-13b-server.llamafile (7.33GB)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can also also download &lt;em&gt;just&lt;/em&gt; the llamafile software (without any weights included) from our releases page, or directly in your terminal or command prompt. This is mandatory currently on Windows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L https://github.com/Mozilla-Ocho/llamafile/releases/download/0.2.1/llamafile-server-0.2.1 &amp;gt;llamafile&#xA;chmod +x llamafile&#xA;./llamafile --help&#xA;./llamafile -m ~/weights/foo.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gotchas&lt;/h3&gt; &#xA;&lt;p&gt;On macOS with Apple Silicon you need to have Xcode installed for llamafile to be able to bootstrap itself.&lt;/p&gt; &#xA;&lt;p&gt;If you use zsh and have trouble running llamafile, try saying &lt;code&gt;sh -c ./llamafile&lt;/code&gt;. This is due to a bug that was fixed in zsh 5.9+. The same is the case for Python &lt;code&gt;subprocess&lt;/code&gt;, old versions of Fish, etc.&lt;/p&gt; &#xA;&lt;p&gt;On some Linux systems, you might get errors relating to &lt;code&gt;run-detectors&lt;/code&gt; or WINE. This is due to &lt;code&gt;binfmt_misc&lt;/code&gt; registrations. You can fix that by adding an additional registration for the APE file format llamafile uses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf&#xA;sudo chmod +x /usr/bin/ape&#xA;sudo sh -c &#34;echo &#39;:APE:M::MZqFpD::/usr/bin/ape:&#39; &amp;gt;/proc/sys/fs/binfmt_misc/register&#34;&#xA;sudo sh -c &#34;echo &#39;:APE-jart:M::jartsr::/usr/bin/ape:&#39; &amp;gt;/proc/sys/fs/binfmt_misc/register&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows, you may need to rename &lt;code&gt;llamafile&lt;/code&gt; to &lt;code&gt;llamafile.exe&lt;/code&gt; in order for it to run. Windows also has a maximum file size limit of 4GB for executables. The LLaVA server executable above is just 30MB shy of that limit, so it&#39;ll work on Windows, but with larger models like WizardCoder 13B, you need to store the weights in a separate file. Here&#39;s an example of how to do that. Let&#39;s say you want to try Mistral. In that case you can open PowerShell and run these commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -o llamafile.exe https://github.com/Mozilla-Ocho/llamafile/releases/download/0.2.1/llamafile-server-0.2.1&#xA;curl -o mistral.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf&#xA;.\llamafile.exe -m mistral.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On WSL, it&#39;s recommended that the WIN32 interop feature be disabled:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo sh -c &#34;echo -1 &amp;gt; /proc/sys/fs/binfmt_misc/WSLInterop&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On any platform, if your llamafile process is immediately killed, check if you have CrowdStrike and then ask to be whitelisted.&lt;/p&gt; &#xA;&lt;h3&gt;GPU Support&lt;/h3&gt; &#xA;&lt;p&gt;On Apple Silicon, everything should just work if Xcode is installed.&lt;/p&gt; &#xA;&lt;p&gt;On Linux, Nvidia cuBLAS GPU support will be compiled on the fly if (1) you have the &lt;code&gt;cc&lt;/code&gt; compiler installed, (2) you pass the &lt;code&gt;--n-gpu-layers 35&lt;/code&gt; flag (or whatever value is appropriate) to enable GPU, and (3) the CUDA developer toolkit is installed on your machine and the &lt;code&gt;nvcc&lt;/code&gt; compiler is on your path.&lt;/p&gt; &#xA;&lt;p&gt;On Windows, that usually means you need to open up the MSVC x64 native command prompt and run llamafile there, for the first invocation, so it can build a DLL with native GPU support. After that, &lt;code&gt;$CUDA_PATH/bin&lt;/code&gt; still usually needs to be on the &lt;code&gt;$PATH&lt;/code&gt; so the GGML DLL can find its other CUDA dependencies.&lt;/p&gt; &#xA;&lt;p&gt;In the event that GPU support couldn&#39;t be compiled and dynamically linked on the fly for any reason, llamafile will fall back to CPU inference.&lt;/p&gt; &#xA;&lt;h2&gt;Source Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how to build llamafile from source. First, you need the cosmocc toolchain, which is a fat portable binary version of GCC. Here&#39;s how you can download the latest release and add it to your path.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir -p cosmocc&#xA;cd cosmocc&#xA;curl -L https://github.com/jart/cosmopolitan/releases/download/3.1.3/cosmocc-3.1.3.zip &amp;gt;cosmocc.zip&#xA;unzip cosmocc.zip&#xA;cd ..&#xA;export PATH=&#34;$PWD/cosmocc/bin:$PATH&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now build the llamafile repository by running make:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to generate code for a libc function using the llama.cpp command line interface, utilizing &lt;a href=&#34;https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/tree/main&#34;&gt;WizardCoder-Python-13B&lt;/a&gt; (license: &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;) weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make -j8 o//llama.cpp/main/main&#xA;o//llama.cpp/main/main \&#xA;  -m ~/weights/wizardcoder-python-13b-v1.0.Q8_0.gguf \&#xA;  --temp 0 \&#xA;  -r $&#39;```\n&#39; \&#xA;  -p $&#39;```c\nvoid *memcpy_sse2(char *dst, const char *src, size_t size) {\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a similar example that instead utilizes &lt;a href=&#34;https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main&#34;&gt;Mistral-7B-Instruct&lt;/a&gt; (license: &lt;a href=&#34;https://choosealicense.com/licenses/apache-2.0/&#34;&gt;Apache 2.0&lt;/a&gt;) weights.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make -j8 o//llama.cpp/main/main&#xA;o//llama.cpp/main/main \&#xA;  -m ~/weights/mistral-7b-instruct-v0.1.Q4_K_M.gguf \&#xA;  --temp 0.7 \&#xA;  -r $&#39;\n&#39; \&#xA;  -p $&#39;### Instruction: Write a story about llamas\n### Response:\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to run llama.cpp&#39;s built-in HTTP server in such a way that the weights are embedded inside the executable. This example uses &lt;a href=&#34;https://huggingface.co/jartine/llava-v1.5-7B-GGUF/tree/main&#34;&gt;LLaVA v1.5-7B&lt;/a&gt; (license: &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI&lt;/a&gt;), a multimodal LLM that works with llama.cpp&#39;s recently-added support for image inputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make -j8&#xA;&#xA;o//llamafile/zipalign -j0 \&#xA;  o//llama.cpp/server/server \&#xA;  ~/weights/llava-v1.5-7b-Q8_0.gguf \&#xA;  ~/weights/llava-v1.5-7b-mmproj-Q8_0.gguf&#xA;&#xA;o//llama.cpp/server/server \&#xA;  -m llava-v1.5-7b-Q8_0.gguf \&#xA;  --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \&#xA;  --host 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command will launch a browser tab on your personal computer to display a web interface. It lets you chat with your LLM and upload images to it.&lt;/p&gt; &#xA;&lt;p&gt;If you want to be able to just say:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and have it run the web server without having to specify arguments (for the paths you already know are in there), then you can add a special &lt;code&gt;.args&lt;/code&gt; to the zip archive, which specifies the default arguments. In this case, we&#39;re going to try our luck with the normal &lt;code&gt;zip&lt;/code&gt; command, which requires we temporarily rename the file. First, let&#39;s create the arguments file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;.args&#xA;-m&#xA;llava-v1.5-7b-Q8_0.gguf&#xA;--mmproj&#xA;llava-v1.5-7b-mmproj-Q8_0.gguf&#xA;--host&#xA;0.0.0.0&#xA;...&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As we can see above, there&#39;s one argument per line. The &lt;code&gt;...&lt;/code&gt; argument optionally specifies where any additional CLI arguments passed by the user are to be inserted. Next, we&#39;ll add the argument file to the executable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mv o//llama.cpp/server/server server.com&#xA;zip server.com .args&#xA;mv server.com server&#xA;./server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Congratulations. You&#39;ve just made your own LLM executable that&#39;s easy to share with your friends.&lt;/p&gt; &#xA;&lt;p&gt;(Note that the examples provided above are not endorsements or recommendations of specific models, licenses, or data sets on the part of Mozilla.)&lt;/p&gt; &#xA;&lt;h3&gt;Security&lt;/h3&gt; &#xA;&lt;p&gt;llamafile adds pledge() and SECCOMP sandboxing to llama.cpp. This is enabled by default. It can be turned off by passing the &lt;code&gt;--unsecure&lt;/code&gt; flag. Sandboxing is currently only supported on Linux and OpenBSD; on other platforms it&#39;ll simply log a warning.&lt;/p&gt; &#xA;&lt;p&gt;Our approach to security has these benefits:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;After it starts up, your HTTP server isn&#39;t able to access the filesystem at all. This is good, since it means if someone discovers a bug in the llama.cpp server, then it&#39;s much less likely they&#39;ll be able to access sensitive information on your machine or make changes to its configuration. On Linux, we&#39;re able to sandbox things even further; the only networking related system call the HTTP server will allowed to use after starting up, is accept(). That further limits an attacker&#39;s ability to exfiltrate information, in the event that your HTTP server is compromised.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The main CLI command won&#39;t be able to access the network at all. This is enforced by the operating system kernel. It also won&#39;t be able to write to the file system. This keeps your computer safe in the event that a bug is ever discovered in the the GGUF file format that lets an attacker craft malicious weights files and post them online. The only exception to this rule is if you pass the &lt;code&gt;--prompt-cache&lt;/code&gt; flag without also specifying &lt;code&gt;--prompt-cache-ro&lt;/code&gt;. In that case, security currently needs to be weakened to allow &lt;code&gt;cpath&lt;/code&gt; and &lt;code&gt;wpath&lt;/code&gt; access, but network access will remain forbidden.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Therefore your llamafile is able to protect itself against the outside world, but that doesn&#39;t mean you&#39;re protected from llamafile. Sandboxing is self-imposed. If you obtained your llamafile from an untrusted source then its author could have simply modified it to not do that. In that case, you can run the untrusted llamafile inside another sandbox, such as a virtual machine, to make sure it behaves how you expect.&lt;/p&gt; &#xA;&lt;h2&gt;zipalign documentation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;SYNOPSIS&#xA;&#xA;  o//llamafile/zipalign ZIP FILE...&#xA;&#xA;DESCRIPTION&#xA;&#xA;  Adds aligned uncompressed files to PKZIP archive&#xA;&#xA;  This tool is designed to concatenate gigabytes of LLM weights to an&#xA;  executable. This command goes 10x faster than `zip -j0`. Unlike zip&#xA;  you are not required to use the .com file extension for it to work.&#xA;  But most importantly, this tool has a flag that lets you insert zip&#xA;  files that are aligned on a specific boundary. The result is things&#xA;  like GPUs that have specific memory alignment requirements will now&#xA;  be able to perform math directly on the zip file&#39;s mmap()&#39;d weights&#xA;&#xA;FLAGS&#xA;&#xA;  -h        help&#xA;  -N        nondeterministic mode&#xA;  -a INT    alignment (default 65536)&#xA;  -j        strip directory components&#xA;  -0        store uncompressed (currently default)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Technical Details&lt;/h2&gt; &#xA;&lt;p&gt;Here is a succinct overview of the tricks we used to create the fattest executable format ever. The long story short is llamafile is a shell script that launches itself and runs inference on embedded weights in milliseconds without needing to be copied or installed. What makes that possible is mmap(). Both the llama.cpp executable and the weights are concatenated onto the shell script. A tiny loader program is then extracted by the shell script, which maps the executable into memory. The llama.cpp executable then opens the shell script again as a file, and calls mmap() again to pull the weights into memory and make them directly accessible to both the CPU and GPU.&lt;/p&gt; &#xA;&lt;h3&gt;ZIP Weights Embedding&lt;/h3&gt; &#xA;&lt;p&gt;The trick to embedding weights inside llama.cpp executables is to ensure the local file is aligned on a page size boundary. That way, assuming the zip file is uncompressed, once it&#39;s mmap()&#39;d into memory we can pass pointers directly to GPUs like Apple Metal, which require that data be page size aligned. Since no existing ZIP archiving tool has an alignment flag, we had to write about &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/zipalign.c&#34;&gt;400 lines of code&lt;/a&gt; to insert the ZIP files ourselves. However, once there, every existing ZIP program should be able to read them, provided they support ZIP64. This makes the weights much more easily accessible than they otherwise would have been, had we invented our own file format for concatenated files.&lt;/p&gt; &#xA;&lt;h3&gt;Microarchitectural Portability&lt;/h3&gt; &#xA;&lt;p&gt;On Intel and AMD microprocessors, llama.cpp spends most of its time in the matmul quants, which are usually written thrice for SSSE3, AVX, and AVX2. llamafile pulls each of these functions out into a separate file that can be &lt;code&gt;#include&lt;/code&gt;ed multiple times, with varying &lt;code&gt;__attribute__((__target__(&#34;arch&#34;)))&lt;/code&gt; function attributes. Then, a wrapper function is added which uses Cosmopolitan&#39;s &lt;code&gt;X86_HAVE(FOO)&lt;/code&gt; feature to runtime dispatch to the appropriate implementation.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture Portability&lt;/h3&gt; &#xA;&lt;p&gt;llamafile solves architecture portability by building llama.cpp twice: once for AMD64 and again for ARM64. It then wraps them with a shell script which has an MZ prefix. On Windows, it&#39;ll run as a native binary. On Linux, it&#39;ll extract a small 8kb executable called &lt;a href=&#34;https://github.com/jart/cosmopolitan/raw/master/ape/loader.c&#34;&gt;APE Loader&lt;/a&gt; to &lt;code&gt;${TMPDIR:-${HOME:-.}}/.ape&lt;/code&gt; that&#39;ll map the binary portions of the shell script into memory. It&#39;s possible to avoid this process by running the &lt;a href=&#34;https://github.com/jart/cosmopolitan/raw/master/tool/build/assimilate.c&#34;&gt;&lt;code&gt;assimilate&lt;/code&gt;&lt;/a&gt; program that comes included with the &lt;code&gt;cosmocc&lt;/code&gt; compiler. What the &lt;code&gt;assimilate&lt;/code&gt; program does is turn the shell script executable into the host platform&#39;s native executable format. This guarantees a fallback path exists for traditional release processes when it&#39;s needed.&lt;/p&gt; &#xA;&lt;h3&gt;GPU Support&lt;/h3&gt; &#xA;&lt;p&gt;Cosmopolitan Libc uses static linking, since that&#39;s the only way to get the same executable to run on six OSes. This presents a challenge for llama.cpp, because it&#39;s not possible to statically link GPU support. The way we solve that is by checking if a compiler is installed on the host system. For Apple, that would be Xcode, and for other platforms, that would be &lt;code&gt;nvcc&lt;/code&gt;. llama.cpp has a single file implementation of each GPU module, named &lt;code&gt;ggml-metal.m&lt;/code&gt; (Objective C) and &lt;code&gt;ggml-cuda.cu&lt;/code&gt; (Nvidia C). llamafile embeds those source files within the zip archive and asks the platform compiler to build them at runtime, targeting the native GPU microarchitecture. If it works, then it&#39;s linked with platform C library dlopen() implementation. See &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/cuda.c&#34;&gt;llamafile/cuda.c&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/metal.c&#34;&gt;llamafile/metal.c&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to use the platform-specific dlopen() function, we need to ask the platform-specific compiler to build a small executable that exposes these interfaces. On ELF platforms, Cosmopolitan Libc maps this helper executable into memory along with the platform&#39;s ELF interpreter. The platform C library then takes care of linking all the GPU libraries, and then runs the helper program which longjmp()&#39;s back into Cosmopolitan. The executable program is now in a weird hybrid state where two separate C libraries exist which have different ABIs. For example, thread local storage works differently on each operating system, and programs will crash if the TLS register doesn&#39;t point to the appropriate memory. The way Cosmopolitan Libc solves that is by JITing a trampoline around each dlsym() import, which blocks signals using &lt;code&gt;sigprocmask()&lt;/code&gt; and changes the TLS register using &lt;code&gt;arch_prctl()&lt;/code&gt;. Under normal circumstances, aspecting each function call with four additional system calls would be prohibitively expensive, but for llama.cpp that cost is infinitesimal compared to the amount of compute used for LLM inference. Our technique has no noticeable slowdown. The major tradeoff is that, right now, you can&#39;t pass callback pointers to the dlopen()&#39;d module. Only one such function needed to be removed from the llama.cpp codebase, which was an API intended for customizing logging. In the future, Cosmoplitan will just trampoline signal handlers and code morph the TLS instructions to avoid these tradeoffs entirely. See &lt;a href=&#34;https://github.com/jart/cosmopolitan/raw/master/libc/dlopen/dlopen.c&#34;&gt;cosmopolitan/dlopen.c&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;While the llamafile project is Apache 2.0-licensed, our changes to llama.cpp are licensed under MIT (just like the llama.cpp project itself) so as to remain compatible and upstreamable in the future, should that be desired.&lt;/p&gt; &#xA;&lt;p&gt;The llamafile logo on this page was generated with the assistance of DALLÂ·E 3.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The 64-bit version of Windows has a 4GB file size limit. While llamafile will work fine on 64-bit Windows with the weights as a separate file, you&#39;ll get an error if you load them into the executable itself and try to run it.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>