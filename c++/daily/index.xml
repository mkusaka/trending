<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-20T01:30:44Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vitoplantamura/OnnxStream</title>
    <updated>2023-07-20T01:30:44Z</updated>
    <id>tag:github.com,2023-07-20:/vitoplantamura/OnnxStream</id>
    <link href="https://github.com/vitoplantamura/OnnxStream" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Running Stable Diffusion on a RPI Zero 2 (or in 260MB of RAM)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OnnxStream&lt;/h1&gt; &#xA;&lt;p&gt;The challenge is to run &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, which includes a large transformer model with almost 1 billion parameters, on a &lt;a href=&#34;https://www.raspberrypi.com/products/raspberry-pi-zero-2-w/&#34;&gt;Raspberry Pi Zero 2&lt;/a&gt;, which is a microcomputer with 512MB of RAM, without adding more swap space and without offloading intermediate results on disk. The recommended minimum RAM/VRAM for Stable Diffusion is typically 8GB.&lt;/p&gt; &#xA;&lt;p&gt;Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream.&lt;/p&gt; &#xA;&lt;p&gt;OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from &lt;code&gt;WeightsProvider&lt;/code&gt;. A &lt;code&gt;WeightsProvider&lt;/code&gt; specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom &lt;code&gt;WeightsProvider&lt;/code&gt; can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word &#34;Stream&#34; in &#34;OnnxStream&#34;). Two default &lt;code&gt;WeightsProviders&lt;/code&gt; are available: &lt;code&gt;DiskNoCache&lt;/code&gt; and &lt;code&gt;DiskPrefetch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OnnxStream can consume even 55x less memory than OnnxRuntime while being only 0.5-2x slower&lt;/strong&gt; (on CPU, see the Performance section below).&lt;/p&gt; &#xA;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;These images were generated by the Stable Diffusion example implementation included in this repo, using OnnxStream, at different precisions of the VAE decoder. The VAE decoder is the only model of Stable Diffusion that could not fit into the RAM of the Raspberry Pi Zero 2 in single or half precision. This is caused by the presence of residual connections and very big tensors and convolutions in the model. The only solution was static quantization (8 bit). The third image was generated by my RPI Zero 2 in about 3 hours. The first image was generated on my PC using the same latents generated by the RPI Zero 2, for comparison:&lt;/p&gt; &#xA;&lt;p&gt;VAE decoder in W16A16 precision:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png&#34; alt=&#34;W16A16 VAE Decoder&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VAE decoder in W8A32 precision:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png&#34; alt=&#34;W8A32 VAE Decoder&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VAE decoder in W8A8 precision (generated by my RPI Zero 2 in about 3 hours):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png&#34; alt=&#34;W8A8 VAE Decoder&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Features of OnnxStream&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference engine decoupled from the &lt;code&gt;WeightsProvider&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WeightsProvider&lt;/code&gt; can be &lt;code&gt;DiskNoCache&lt;/code&gt;, &lt;code&gt;DiskPrefetch&lt;/code&gt; or custom&lt;/li&gt; &#xA; &lt;li&gt;Attention slicing&lt;/li&gt; &#xA; &lt;li&gt;Dynamic quantization (8 bit unsigned, asymmetric, percentile)&lt;/li&gt; &#xA; &lt;li&gt;Static quantization (W8A8 unsigned, asymmetric, percentile)&lt;/li&gt; &#xA; &lt;li&gt;Easy calibration of a quantized model&lt;/li&gt; &#xA; &lt;li&gt;FP16 support (with or without FP16 arithmetic)&lt;/li&gt; &#xA; &lt;li&gt;24 ONNX operators implemented (the most common)&lt;/li&gt; &#xA; &lt;li&gt;Operations executed sequentially but all operators are multithreaded&lt;/li&gt; &#xA; &lt;li&gt;Single implementation file + header file&lt;/li&gt; &#xA; &lt;li&gt;XNNPACK calls wrapped in the &lt;code&gt;XnnPack&lt;/code&gt; class (for future replacement)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;OnnxStream depends on &lt;a href=&#34;https://github.com/google/XNNPACK&#34;&gt;XNNPACK&lt;/a&gt; for some (accelerated) primitives: MatMul, Convolution, element-wise Add/Sub/Mul/Div, Sigmoid and Softmax.&lt;/p&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Stable Diffusion consists of three models: &lt;strong&gt;a text encoder&lt;/strong&gt; (672 operations and 123 million parameters), the &lt;strong&gt;UNET model&lt;/strong&gt; (2050 operations and 854 million parameters) and the &lt;strong&gt;VAE decoder&lt;/strong&gt; (276 operations and 49 million parameters). Assuming that the batch size is equal to 1, a full image generation with 10 steps, which yields good results (with the Euler Ancestral scheduler), requires 2 runs of the text encoder, 20 (i.e. 2*10) runs of the UNET model and 1 run of the VAE decoder.&lt;/p&gt; &#xA;&lt;p&gt;This table shows the various inference times of the three models of Stable Diffusion, together with the memory consumption (i.e. the &lt;code&gt;Peak Working Set Size&lt;/code&gt; in Windows or the &lt;code&gt;Maximum Resident Set Size&lt;/code&gt; in Linux).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model / Library&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1st run&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;2nd run&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;3rd run&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16 UNET / OnnxStream&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.133 GB - 18.2 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.133 GB - 18.7 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.133 GB - 19.8 secs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP16 UNET / OnnxRuntime&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.085 GB - 12.8 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.353 GB - 7.28 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.353 GB - 7.96 secs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP32 Text Enc / OnnxStream&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147 GB - 1.26 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147 GB - 1.19 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.147 GB - 1.19 secs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP32 Text Enc / OnnxRuntime&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.641 GB - 1.02 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.641 GB - 0.06 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.641 GB - 0.07 secs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP32 VAE Dec / OnnxStream&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.004 GB - 20.9 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.004 GB - 20.6 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.004 GB - 21.2 secs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FP32 VAE Dec / OnnxRuntime&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.330 GB - 11.2 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.026 GB - 10.1 secs&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.026 GB - 11.1 secs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;In the case of the UNET model (when run in FP16 precision, with FP16 arithmetic enabled in OnnxStream), OnnxStream can consume even 55x less memory than OnnxRuntime while being 0.5-2x slower.&lt;/p&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The first run for OnnxRuntime is a warm up inference, since its &lt;code&gt;InferenceSession&lt;/code&gt; is created before the first run and reused for all the subsequent runs. No such thing as a warm up exists for OnnxStream since it is purely eager by design (however subsequent runs can benefit from the caching of the weights files by the OS).&lt;/li&gt; &#xA; &lt;li&gt;At the moment OnnxStream doesn&#39;t support inputs with a batch size != 1, unlike OnnxRuntime, which can greatly speed up the whole diffusion process using a batch size = 2 when running the UNET model.&lt;/li&gt; &#xA; &lt;li&gt;In my tests, changing OnnxRuntime&#39;s &lt;code&gt;SessionOptions&lt;/code&gt; (like &lt;code&gt;EnableCpuMemArena&lt;/code&gt; and &lt;code&gt;ExecutionMode&lt;/code&gt;) produces no significant difference in the results.&lt;/li&gt; &#xA; &lt;li&gt;Performance of OnnxRuntime is very similar to that of NCNN (the other framework I evaluated), both in terms of memory consumption and inference time. I&#39;ll include NCNN benchmarks in the future, if useful.&lt;/li&gt; &#xA; &lt;li&gt;Tests were run on my development machine: Windows Server 2019, 16GB RAM, 8750H cpu (AVX2), 970 EVO Plus SSD, 8 virtual cores on VMWare.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Attention Slicing and Quantization&lt;/h1&gt; &#xA;&lt;p&gt;The use of &#34;attention slicing&#34; when running the UNET model and the use of W8A8 quantization for the VAE decoder were crucial in reducing memory consumption to a level that allowed execution on a RPI Zero 2.&lt;/p&gt; &#xA;&lt;p&gt;While there is a lot of information on the internet about quantizing neural networks, little can be found about &#34;attention slicing&#34;. The idea is simple: the goal is to avoid materializing the full &lt;code&gt;Q @ K^T&lt;/code&gt; matrix when calculating the scaled dot-product attention of the various multi-head attentions in the UNET model. With an attention head count of 8 in the UNET model, &lt;code&gt;Q&lt;/code&gt; has a shape of (8,4096,40), while &lt;code&gt;K^T&lt;/code&gt; has a shape of (8,40,4096): so the result of the first MatMul has a final shape of (8,4096,4096), which is a 512MB tensor (in FP32 precision):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png&#34; alt=&#34;Attention Slicing&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The solution is to split &lt;code&gt;Q&lt;/code&gt; vertically and then to proceed with the attention operations normally on each chunk of &lt;code&gt;Q&lt;/code&gt;. &lt;code&gt;Q_sliced&lt;/code&gt; has a shape of (1,x,40), where x is 4096 (in this case) divided by &lt;code&gt;onnxstream::Model::m_attention_fused_ops_parts&lt;/code&gt; (which has a default value of 2, but can be customized). This simple trick allows to lower the overall consumed memory of the UNET model from 1.1GB to 300MB (when the model is run in FP32 precision). A possible alternative, certainly more efficient, would be to use FlashAttention, however FlashAttention would require writing a custom kernel for each supported architecture (AVX, NEON etc), bypassing XnnPack in our case.&lt;/p&gt; &#xA;&lt;h1&gt;How OnnxStream works&lt;/h1&gt; &#xA;&lt;p&gt;This code can run a model defined in the &lt;code&gt;path_to_model_folder/model.txt&lt;/code&gt;: (all the model operations are defined in the &lt;code&gt;model.txt&lt;/code&gt; text file; OnnxStream expects to find all the weights files in that same folder, as a series of &lt;code&gt;.bin&lt;/code&gt; files)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &#34;onnxstream.h&#34;&#xA;&#xA;using namespace onnxstream;&#xA;&#xA;int main()&#xA;{&#xA;    Model model;&#xA;&#xA;    //&#xA;    // Optional parameters that can be set on the Model object:&#xA;    //&#xA;    // model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)&#xA;    // model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)&#xA;    // model.write_range_data( ... ); // writes a range data file (useful after calibration)&#xA;    // model.m_range_data_calibrate = true; // calibrates the model&#xA;    // model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)&#xA;    // model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference&#xA;    // model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)&#xA;    // model.m_fuse_ops_in_attention = true; // enables attention slicing&#xA;    // model.m_attention_fused_ops_parts = ... ; // see the &#34;Attention Slicing&#34; section above&#xA;    //&#xA;&#xA;    model.read_file(&#34;path_to_model_folder/model.txt&#34;);&#xA;&#xA;    tensor_vector&amp;lt;float&amp;gt; data;&#xA;    &#xA;    ... // fill the tensor_vector with the tensor data. &#34;tensor_vector&#34; is just an alias to a std::vector with a custom allocator.&#xA;&#xA;    Tensor t;&#xA;    t.m_name = &#34;input&#34;;&#xA;    t.m_shape = { 1, 4, 64, 64 };&#xA;    t.set_vector(std::move(data));&#xA;    model.push_tensor(std::move(t));&#xA;&#xA;    model.run();&#xA;    &#xA;    auto&amp;amp; result = model.m_data[0].get_vector&amp;lt;float&amp;gt;();&#xA;    &#xA;    ... // process the result: &#34;result&#34; is a reference to the first result of the inference (a tensor_vector&amp;lt;float&amp;gt; as well).&#xA;&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;model.txt&lt;/code&gt; file contains all the model operations in ASCII format, as exported from the original ONNX file. Each line corresponds to an operation: for example this line represents a convolution in a quantized model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to export the &lt;code&gt;model.txt&lt;/code&gt; file and its weights (as a series of &lt;code&gt;.bin&lt;/code&gt; files) from an ONNX file for use in OnnxStream, a notebook (with a single cell) is provided (&lt;code&gt;onnx2txt.ipynb&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Some things must be considered when exporting a Pytorch &lt;code&gt;nn.Module&lt;/code&gt; (in our case) to ONNX for use in OnnxStream:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;When calling &lt;code&gt;torch.onnx.export&lt;/code&gt;, &lt;code&gt;dynamic_axes&lt;/code&gt; should be left empty, since OnnxStream doesn&#39;t support inputs with a dynamic shape.&lt;/li&gt; &#xA; &lt;li&gt;It is strongly recommended to run the excellent &lt;a href=&#34;https://github.com/daquexian/onnx-simplifier&#34;&gt;ONNX Simplifier&lt;/a&gt; on the exported ONNX file before its conversion to a &lt;code&gt;model.txt&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How to Build the Stable Diffusion example on Linux/Mac/Windows&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows only&lt;/strong&gt;: start the following command prompt: &lt;code&gt;Visual Studio Tools&lt;/code&gt; &amp;gt; &lt;code&gt;x64 Native Tools Command Prompt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mac only&lt;/strong&gt;: make sure to install cmake: &lt;code&gt;brew install cmake&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First you need to build &lt;a href=&#34;https://github.com/google/XNNPACK&#34;&gt;XNNPACK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Since the function prototypes of XnnPack can change at any time, I&#39;ve included a &lt;code&gt;git checkout&lt;/code&gt; ​​that ensures correct compilation of OnnxStream with a compatible version of XnnPack at the time of writing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/google/XNNPACK.git&#xA;cd XNNPACK&#xA;git rev-list -n 1 --before=&#34;2023-06-27 00:00&#34; master&#xA;git checkout &amp;lt;COMMIT_ID_FROM_THE_PREVIOUS_COMMAND&amp;gt;&#xA;mkdir build&#xA;cd build&#xA;cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can build the Stable Diffusion example.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&amp;gt;&lt;/code&gt; is for example &lt;code&gt;/home/vito/Desktop/XNNPACK&lt;/code&gt; or &lt;code&gt;C:\Projects\SD\XNNPACK&lt;/code&gt; (on Windows):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/vitoplantamura/OnnxStream.git&#xA;cd OnnxStream&#xA;cd src&#xA;mkdir build&#xA;cd build&#xA;cmake -DXNNPACK_DIR=&amp;lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&amp;gt; ..&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can run the Stable Diffusion example. The weights for the example can be downloaded from the Releases of this repo. These are the command line options of the Stable Diffusion example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--models-path       Sets the folder containing the Stable Diffusion models.&#xA;--ops-printf        During inference, writes the current operation to stdout.&#xA;--output            Sets the output PNG file.&#xA;--decode-latents    Skips the diffusion, and decodes the specified latents file.&#xA;--prompt            Sets the positive prompt.&#xA;--neg-prompt        Sets the negative prompt.&#xA;--steps             Sets the number of diffusion steps.&#xA;--save-latents      After the diffusion, saves the latents in the specified file.&#xA;--decoder-calibrate Calibrates the quantized version of the VAE decoder.&#xA;--decoder-fp16      During inference, uses the FP16 version of the VAE decoder.&#xA;--rpi               Configures the models to run on a Raspberry Pi Zero 2.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Stable Diffusion implementation in &lt;code&gt;sd.cpp&lt;/code&gt; is based on &lt;a href=&#34;https://github.com/fengwang/Stable-Diffusion-NCNN&#34;&gt;this project&lt;/a&gt;, which in turn is based on &lt;a href=&#34;https://github.com/EdVince/Stable-Diffusion-NCNN&#34;&gt;this project&lt;/a&gt; by @EdVince. The original code was modified in order to use OnnxStream instead of NCNN.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AcademySoftwareFoundation/openvdb</title>
    <updated>2023-07-20T01:30:44Z</updated>
    <id>tag:github.com,2023-07-20:/AcademySoftwareFoundation/openvdb</id>
    <link href="https://github.com/AcademySoftwareFoundation/openvdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenVDB - Sparse volume data structure and tools&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://www.openvdb.org/images/openvdb_logo.png&#34; alt=&#34;OpenVDB&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AcademySoftwareFoundation/openvdb/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/AcademySoftwareFoundation/openvdb&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/2774&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/2774/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://slack.aswf.io/&#34;&gt;&lt;img src=&#34;https://slack.aswf.io/badge.svg?sanitize=true&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OpenVDB&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AX&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Nano&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Houdini&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;core&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/ax.yml&#34;&gt;&lt;img src=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/ax.yml/badge.svg?sanitize=true&#34; alt=&#34;ax&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/nanovdb.yml&#34;&gt;&lt;img src=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/nanovdb.yml/badge.svg?sanitize=true&#34; alt=&#34;nano&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/houdini.yml&#34;&gt;&lt;img src=&#34;https://github.com/AcademySoftwareFoundation/openvdb/actions/workflows/houdini.yml/badge.svg?sanitize=true&#34; alt=&#34;hou&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.openvdb.org&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://github.com/AcademySoftwareFoundation/openvdb/discussions&#34;&gt;Discussion Forum&lt;/a&gt; | &lt;a href=&#34;https://www.openvdb.org/documentation/doxygen&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenVDB is an open source C++ library comprising a novel hierarchical data structure and a large suite of tools for the efficient storage and manipulation of sparse volumetric data discretized on three-dimensional grids. It was developed by DreamWorks Animation for use in volumetric applications typically encountered in feature film production.&lt;/p&gt; &#xA;&lt;h3&gt;Development Repository&lt;/h3&gt; &#xA;&lt;p&gt;This GitHub repository hosts the trunk of the OpenVDB development. This implies that it is the newest public version with the latest features and bug fixes. However, it also means that it has not undergone a lot of testing and is generally less stable than the &lt;a href=&#34;https://github.com/AcademySoftwareFoundation/openvdb/releases&#34;&gt;production releases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;OpenVDB is released under the &lt;a href=&#34;https://www.mozilla.org/MPL/2.0/&#34;&gt;Mozilla Public License Version 2.0&lt;/a&gt;, which is a free, open source software license developed and maintained by the Mozilla Foundation.&lt;/p&gt; &#xA;&lt;p&gt;The trademarks of any contributor to this project may not be used in association with the project without the contributor&#39;s express permission.&lt;/p&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;OpenVDB welcomes contributions to the OpenVDB project. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/AcademySoftwareFoundation/openvdb/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt; for details on how to make a contribution.&lt;/p&gt; &#xA;&lt;h3&gt;Developer Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The following provides basic installation examples for the core OpenVDB library. Other components, such as the python module, OpenVDB AX, NanoVDB and various executables, may require additional dependencies. See the &lt;a href=&#34;https://www.openvdb.org/documentation/doxygen/build.html&#34;&gt;build documentation&lt;/a&gt; for help with installations.&lt;/p&gt; &#xA;&lt;h4&gt;Linux&lt;/h4&gt; &#xA;&lt;h5&gt;Installing Dependencies (Boost, TBB, Blosc)&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# @note If your distribution does not have required versions, consider using&#xA;#   apt pinning. See the dependency documentation for more details.&#xA;apt-get install -y libboost-iostreams-dev&#xA;apt-get install -y libtbb-dev&#xA;apt-get install -y libblosc-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Building OpenVDB&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:AcademySoftwareFoundation/openvdb.git&#xA;cd openvdb&#xA;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j4 &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;macOS&lt;/h4&gt; &#xA;&lt;h5&gt;Installing Dependencies (Boost, TBB, Blosc)&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install boost&#xA;brew install tbb&#xA;brew install c-blosc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Building OpenVDB&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:AcademySoftwareFoundation/openvdb.git&#xA;cd openvdb&#xA;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j4 &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;h5&gt;Installing Dependencies (Boost, TBB, Blosc)&lt;/h5&gt; &#xA;&lt;p&gt;Note that the following commands have only been tested for 64bit systems/libraries. It is recommended to set the &lt;code&gt;VCPKG_DEFAULT_TRIPLET&lt;/code&gt; environment variable to &lt;code&gt;x64-windows&lt;/code&gt; to use 64-bit libraries by default. You will also require &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;Git&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; and &lt;a href=&#34;https://cmake.org/download/&#34;&gt;CMake&lt;/a&gt; to be installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vcpkg install zlib:x64-windows&#xA;vcpkg install blosc:x64-windows&#xA;vcpkg install tbb:x64-windows&#xA;vcpkg install boost-iostreams:x64-windows&#xA;vcpkg install boost-any:x64-windows&#xA;vcpkg install boost-algorithm:x64-windows&#xA;vcpkg install boost-uuid:x64-windows&#xA;vcpkg install boost-interprocess:x64-windows&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Building OpenVDB&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:AcademySoftwareFoundation/openvdb.git&#xA;cd openvdb&#xA;mkdir build&#xA;cd build&#xA;cmake -DCMAKE_TOOLCHAIN_FILE=&amp;lt;PATH_TO_VCPKG&amp;gt;\scripts\buildsystems\vcpkg.cmake -DVCPKG_TARGET_TRIPLET=x64-windows -A x64 ..&#xA;cmake --build . --parallel 4 --config Release --target install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Building OpenVDB AX&lt;/h4&gt; &#xA;&lt;p&gt;OpenVDB AX depends on the core OpenVDB library. See the &lt;a href=&#34;https://www.openvdb.org/documentation/doxygen/build.html&#34;&gt;build documentation&lt;/a&gt; for all available AX component options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:AcademySoftwareFoundation/openvdb.git&#xA;cd openvdb&#xA;mkdir build&#xA;cd build&#xA;cmake -DOPENVDB_BUILD_AX=ON ..&#xA;make -j4 &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Building NanoVDB&lt;/h4&gt; &#xA;&lt;p&gt;NanoVDB can be built with and without OpenVDB support. To see full build instructions see the &lt;a href=&#34;https://www.openvdb.org/documentation/doxygen/NanoVDB_HowToBuild.html&#34;&gt;NanoVDB build documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Building Without OpenVDB Support&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:AcademySoftwareFoundation/openvdb.git&#xA;cd openvdb/nanovdb/nanovdb  # Build from the subdirectory&#xA;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j4 &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Building With OpenVDB Support&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:AcademySoftwareFoundation/openvdb.git&#xA;cd openvdb&#xA;mkdir build&#xA;cd build&#xA;cmake -DOPENVDB_BUILD_NANOVDB=ON ..&#xA;make -j4 &amp;amp;&amp;amp; make install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>swig/swig</title>
    <updated>2023-07-20T01:30:44Z</updated>
    <id>tag:github.com,2023-07-20:/swig/swig</id>
    <link href="https://github.com/swig/swig" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;SWIG (Simplified Wrapper and Interface Generator)&lt;/p&gt; &#xA;&lt;p&gt;Tagline: SWIG is a compiler that integrates C and C++ with languages including Perl, Python, Tcl, Ruby, PHP, Java, C#, D, Go, Lua, Octave, R, Scheme (Guile, MzScheme/Racket), Scilab, Ocaml. SWIG can also export its parse tree into XML.&lt;/p&gt; &#xA;&lt;p&gt;SWIG reads annotated C/C++ header files and creates wrapper code (glue code) in order to make the corresponding C/C++ libraries available to the listed languages, or to extend C/C++ programs with a scripting language.&lt;/p&gt; &#xA;&lt;p&gt;Up-to-date SWIG related information can be found at&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    https://www.swig.org&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A SWIG FAQ and other hints can be found on the SWIG Wiki:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    https://github.com/swig/swig/wiki&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Please see the LICENSE file for details of the SWIG license. For further insight into the license including the license of SWIG&#39;s output code, please visit&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    https://www.swig.org/legal.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Release Notes&lt;/h1&gt; &#xA;&lt;p&gt;Please see the CHANGES.current file for a detailed list of bug fixes and new features for the current release. The CHANGES file contains bug fixes and new features for older versions. A summary of changes in each release can be found in the RELEASENOTES file.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;The Doc/Manual directory contains the most recent set of updated documentation for this release. The documentation is available in three different formats, each of which contains identical content. These format are, pdf (Doc/Manual/SWIGDocumentation.pdf), single page html (Doc/Manual/SWIGDocumentation.html) or multiple page html (other files in Doc/Manual). Please select your chosen format and copy/install to wherever takes your fancy.&lt;/p&gt; &#xA;&lt;p&gt;There is some technical developer documentation available in the Doc/Devel subdirectory. This is not necessarily up-to-date, but it has some information on SWIG internals.&lt;/p&gt; &#xA;&lt;p&gt;Documentation is also online at &lt;a href=&#34;https://www.swig.org/doc.html&#34;&gt;https://www.swig.org/doc.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Backwards Compatibility&lt;/h1&gt; &#xA;&lt;p&gt;The developers strive their best to preserve backwards compatibility between releases, but this is not always possible as the overriding aim is to provide the best wrapping experience. Where backwards compatibility is known to be broken, it is clearly marked as an incompatibility in the CHANGES and CHANGES.current files.&lt;/p&gt; &#xA;&lt;p&gt;See the documentation for details of the SWIG_VERSION preprocessor symbol if you have backward compatibility issues and need to use more than one version of SWIG.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Please read the Doc/Manual/Preface.html#Preface_installation for full installation instructions for Windows, Unix and Mac OS X using the release tarball/zip file. The INSTALL file has generic build and installation instructions for Unix users. Users wishing to build and install code from Github should visit &lt;a href=&#34;https://swig.org/svn.html&#34;&gt;https://swig.org/svn.html&lt;/a&gt; to obtain the more detailed instructions required for building code obtained from Github - extra steps are required compared to building from the release tarball.&lt;/p&gt; &#xA;&lt;h1&gt;Testing&lt;/h1&gt; &#xA;&lt;p&gt;The typical &#39;make -k check&#39; can be performed on Unix operating systems. Please read Doc/Manual/Preface.html#Preface_testing for details.&lt;/p&gt; &#xA;&lt;h1&gt;Examples&lt;/h1&gt; &#xA;&lt;p&gt;The Examples directory contains a variety of examples of using SWIG and it has some browsable documentation. Simply point your browser to the file &#34;Example/index.html&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The Examples directory also includes Visual C++ project 6 (.dsp) files for building some of the examples on Windows. Later versions of Visual Studio will convert these old style project files into a current solution file.&lt;/p&gt; &#xA;&lt;h1&gt;Known Issues&lt;/h1&gt; &#xA;&lt;p&gt;There are minor known bugs, details of which are in the bug tracker, see &lt;a href=&#34;https://www.swig.org/bugs.html&#34;&gt;https://www.swig.org/bugs.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;p&gt;In order to operate correctly, SWIG relies upon a set of library files. If after building SWIG, you get error messages like this,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ swig foo.i&#xA;:1. Unable to find &#39;swig.swg&#39;&#xA;:3. Unable to find &#39;tcl8.swg&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;it means that SWIG has either been incorrectly configured or installed. To fix this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;1.  Make sure you remembered to do a &#39;make install&#39; and that&#xA;    the installation actually worked.  Make sure you have&#xA;    write permission on the install directory.&#xA;&#xA;2.  If that doesn&#39;t work, type &#39;swig -swiglib&#39; to find out&#xA;    where SWIG thinks its library is located.&#xA;&#xA;3.  If the location is not where you expect, perhaps&#xA;    you supplied a bad option to configure.  Use&#xA;    ./configure --prefix=pathname to set the SWIG install&#xA;    location.   Also, make sure you don&#39;t include a shell&#xA;    escape character such as ~ when you specify the path.&#xA;&#xA;4.  The SWIG library can be changed by setting the SWIG_LIB&#xA;    environment variable.  However, you really shouldn&#39;t&#xA;    have to do this.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are having other troubles, you might look at the SWIG Wiki at &lt;a href=&#34;https://github.com/swig/swig/wiki&#34;&gt;https://github.com/swig/swig/wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Participate!&lt;/h1&gt; &#xA;&lt;p&gt;Please report any errors and submit patches (if possible)! We only have access to a limited variety of hardware (Linux, Solaris, OS-X, and Windows). All contributions help.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to join the SWIG development team or contribute a language module to the distribution, please contact the swig-devel mailing list, details at &lt;a href=&#34;https://www.swig.org/mail.html&#34;&gt;https://www.swig.org/mail.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;-- The SWIG Maintainers&lt;/p&gt;</summary>
  </entry>
</feed>