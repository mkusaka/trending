<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-07T01:28:38Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sz3/libcimbar</title>
    <updated>2024-09-07T01:28:38Z</updated>
    <id>tag:github.com,2024-09-07:/sz3/libcimbar</id>
    <link href="https://github.com/sz3/libcimbar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Optimized implementation for color-icon-matrix barcodes&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a href=&#34;https://github.com/sz3/cimbar&#34;&gt;INTRODUCTION&lt;/a&gt; | &lt;a href=&#34;https://github.com/sz3/cimbar/raw/master/ABOUT.md&#34;&gt;ABOUT&lt;/a&gt; | &lt;a href=&#34;https://github.com/sz3/cfc&#34;&gt;CFC&lt;/a&gt; | LIBCIMBAR&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/DETAILS.md&#34;&gt;DETAILS&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/PERFORMANCE.md&#34;&gt;PERFORMANCE&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/TODO.md&#34;&gt;TODO&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;libcimbar: Color Icon Matrix Barcodes&lt;/h2&gt; &#xA;&lt;p&gt;Behold: an experimental barcode format for air-gapped data transfer.&lt;/p&gt; &#xA;&lt;p&gt;It can sustain speeds of 850 kilobits/s (~106 KB/s) using just a computer monitor and a smartphone camera!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/sz3/cimbar-samples/raw/v0.6/b/4cecc30f.png&#34; width=&#34;70%&#34; title=&#34;A non-animated mode-B cimbar code&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Explain?&lt;/h2&gt; &#xA;&lt;p&gt;The encoder outputs an animated barcode to a computer or smartphone screen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Encoder web app: &lt;a href=&#34;https://cimbar.org&#34;&gt;https://cimbar.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;While the decoder is a cell phone app that uses the phone camera to read the animated barcode:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Decoder android app: &lt;a href=&#34;https://github.com/sz3/cfc&#34;&gt;https://github.com/sz3/cfc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;No internet/bluetooth/NFC/etc is used. All data is transmitted through the camera lens. You can try it out yourself, or take my word that it works. :)&lt;/p&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;cimbar&lt;/code&gt; is a high-density 2D barcode format. Data is stored in a grid of colored tiles -- bits are encoded based on which tile is chosen, and which color is chosen to draw the tile. Reed Solomon error correction is applied on the data, to account for the lossy nature of the video -&amp;gt; digital decoding. Sub-1% error rates are expected, and corrected.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;libcimbar&lt;/code&gt;, this optimized implementation, includes a simple protocol for file encoding built on fountain codes (&lt;code&gt;wirehair&lt;/code&gt;) and zstd compression. Files of up to 33MB (after compression!) are encoded in a series of cimbar codes, which can be output as images or a live video feed. Once enough distinct image frames have been decoded successfully, the file will be reconstructed and decompressed successfully. This is true even if the images are received out of order, or if some have been corrupted or are missing.&lt;/p&gt; &#xA;&lt;h2&gt;Platforms&lt;/h2&gt; &#xA;&lt;p&gt;The code is written in C++, and developed/tested on amd64+linux, arm64+android (decoder only), and emscripten+WASM (encoder only). It probably works, or can be made to work, on other platforms.&lt;/p&gt; &#xA;&lt;p&gt;Crucially, because the encoder compiles to asmjs and wasm, it can run on anything with a modern web browser. For offline use, you can either install cimbar.org as a progressive web app, or &lt;a href=&#34;https://github.com/sz3/libcimbar/releases/latest&#34;&gt;download the latest release&lt;/a&gt; of &lt;code&gt;cimbar_js.html&lt;/code&gt;, save it locally, and open it in your web browser.&lt;/p&gt; &#xA;&lt;h2&gt;Library dependencies&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencv.org/&#34;&gt;OpenCV&lt;/a&gt; and &lt;a href=&#34;https://github.com/glfw/glfw&#34;&gt;GLFW&lt;/a&gt; (+ OpenGL ES headers) must be installed before building. All other dependencies are included in the source tree.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;opencv - &lt;a href=&#34;https://opencv.org/&#34;&gt;https://opencv.org/&lt;/a&gt; (&lt;code&gt;libopencv-dev&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;GLFW - &lt;a href=&#34;https://github.com/glfw/glfw&#34;&gt;https://github.com/glfw/glfw&lt;/a&gt; (&lt;code&gt;libglfw3-dev&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;GLES3/gl3.h - &lt;code&gt;libgles2-mesa-dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;base - &lt;a href=&#34;https://github.com/r-lyeh-archived/base&#34;&gt;https://github.com/r-lyeh-archived/base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;catch2 - &lt;a href=&#34;https://github.com/catchorg/Catch2&#34;&gt;https://github.com/catchorg/Catch2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;concurrentqueue - &lt;a href=&#34;https://github.com/cameron314/concurrentqueue&#34;&gt;https://github.com/cameron314/concurrentqueue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cxxopts - &lt;a href=&#34;https://github.com/jarro2783/cxxopts&#34;&gt;https://github.com/jarro2783/cxxopts&lt;/a&gt; (used for command line tools)&lt;/li&gt; &#xA; &lt;li&gt;fmt - &lt;a href=&#34;https://github.com/fmtlib/fmt&#34;&gt;https://github.com/fmtlib/fmt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;intx - &lt;a href=&#34;https://github.com/chfast/intx&#34;&gt;https://github.com/chfast/intx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;libcorrect - &lt;a href=&#34;https://github.com/quiet/libcorrect&#34;&gt;https://github.com/quiet/libcorrect&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;libpopcnt - &lt;a href=&#34;https://github.com/kimwalisch/libpopcnt&#34;&gt;https://github.com/kimwalisch/libpopcnt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PicoSHA2 - &lt;a href=&#34;https://github.com/okdshin/PicoSHA2&#34;&gt;https://github.com/okdshin/PicoSHA2&lt;/a&gt; (used for testing)&lt;/li&gt; &#xA; &lt;li&gt;stb_image - &lt;a href=&#34;https://github.com/nothings/stb&#34;&gt;https://github.com/nothings/stb&lt;/a&gt; (for loading embedded pngs)&lt;/li&gt; &#xA; &lt;li&gt;wirehair - &lt;a href=&#34;https://github.com/catid/wirehair&#34;&gt;https://github.com/catid/wirehair&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;zstd - &lt;a href=&#34;https://github.com/facebook/zstd&#34;&gt;https://github.com/facebook/zstd&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;install opencv and GLFW. On ubuntu/debian, this looks like:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install libopencv-dev libglfw3-dev libgles2-mesa-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;run the cmake + make incantation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake .&#xA;make -j7&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, libcimbar will try to install build products under &lt;code&gt;./dist/bin/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build cimbar.js (what cimbar.org uses), see &lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/WASM.md&#34;&gt;WASM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Encode:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;large input files may fill up your disk with pngs!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;./cimbar --encode -i inputfile.txt -o outputprefix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Decode (extracts file into output directory):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./cimbar outputprefix*.png -o /tmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Decode a series of encoded images from stdin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo outputprefix*.png | ./cimbar -o /tmp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Encode and animate to window:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./cimbar_send inputfile.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also encode a file using &lt;a href=&#34;https://cimbar.org&#34;&gt;cimbar.org&lt;/a&gt;, or the latest &lt;a href=&#34;https://github.com/sz3/libcimbar/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance numbers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/PERFORMANCE.md&#34;&gt;PERFORMANCE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Implementation details&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/DETAILS.md&#34;&gt;DETAILS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Room for improvement/next steps&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sz3/libcimbar/master/TODO.md&#34;&gt;TODO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inspiration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohannesBuchner/imagehash/&#34;&gt;https://github.com/JohannesBuchner/imagehash/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/divan/txqr&#34;&gt;https://github.com/divan/txqr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/High_Capacity_Color_Barcode&#34;&gt;https://en.wikipedia.org/wiki/High_Capacity_Color_Barcode&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Would you like to know more?&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/sz3/cimbar&#34;&gt;INTRODUCTION&lt;/a&gt; | &lt;a href=&#34;https://github.com/sz3/cimbar/raw/master/ABOUT.md&#34;&gt;ABOUT&lt;/a&gt;&lt;/h3&gt;</summary>
  </entry>
  <entry>
    <title>RWKV/rwkv.cpp</title>
    <updated>2024-09-07T01:28:38Z</updated>
    <id>tag:github.com,2024-09-07:/RWKV/rwkv.cpp</id>
    <link href="https://github.com/RWKV/rwkv.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;INT4/INT5/INT8 and FP16 inference on CPU for RWKV language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;rwkv.cpp&lt;/h1&gt; &#xA;&lt;p&gt;This is a port of &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM&#34;&gt;BlinkDL/RWKV-LM&lt;/a&gt; to &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggerganov/ggml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Besides the usual &lt;strong&gt;FP32&lt;/strong&gt;, it supports &lt;strong&gt;FP16&lt;/strong&gt;, &lt;strong&gt;quantized INT4, INT5 and INT8&lt;/strong&gt; inference. This project is &lt;strong&gt;focused on CPU&lt;/strong&gt;, but cuBLAS is also supported.&lt;/p&gt; &#xA;&lt;p&gt;This project provides &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/rwkv.h&#34;&gt;a C library rwkv.h&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/python%2Frwkv_cpp%2Frwkv_cpp_model.py&#34;&gt;a convinient Python wrapper&lt;/a&gt; for it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13048&#34;&gt;RWKV&lt;/a&gt; is a large language model architecture, &lt;a href=&#34;https://huggingface.co/BlinkDL/rwkv-4-pile-14b&#34;&gt;with the largest model in the family having 14B parameters&lt;/a&gt;. In contrast to Transformer with &lt;code&gt;O(n^2)&lt;/code&gt; attention, RWKV requires only state from previous step to calculate logits. This makes RWKV very CPU-friendly on large context lenghts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/BlinkDL/rwkv-5-world&#34;&gt;RWKV v5&lt;/a&gt; is a major upgrade to RWKV architecture, making it competitive with Transformers in quality. RWKV v5 models are supported.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/BlinkDL/rwkv-6-world&#34;&gt;RWKV v6&lt;/a&gt; is a further improvement to RWKV architecture, with better quality. RWKV v6 models are supported.&lt;/p&gt; &#xA;&lt;p&gt;Loading LoRA checkpoints in &lt;a href=&#34;https://github.com/Blealtan/RWKV-LM-LoRA&#34;&gt;Blealtan&#39;s format&lt;/a&gt; is supported through &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/rwkv%2Fmerge_lora_into_ggml.py&#34;&gt;merge_lora_into_ggml.py script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quality and performance&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;rwkv.cpp&lt;/code&gt; for anything serious, please &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/rwkv%2Fmeasure_pexplexity.py&#34;&gt;test all available formats for perplexity and latency&lt;/a&gt; on a representative dataset, and decide which trade-off is best for you.&lt;/p&gt; &#xA;&lt;p&gt;In general, &lt;strong&gt;&lt;code&gt;RWKV v5&lt;/code&gt; models are as fast as &lt;code&gt;RWKV v4&lt;/code&gt; models&lt;/strong&gt;, with minor differencies in latency and memory consumption, and with having way higher quality than &lt;code&gt;v4&lt;/code&gt;. Therefore, it is recommended to use &lt;code&gt;RWKV v5&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Below table is for reference only. Measurements were made on 4C/8T x86 CPU with AVX2, 4 threads. The models are &lt;code&gt;RWKV v4 Pile 169M&lt;/code&gt;, &lt;code&gt;RWKV v4 Pile 1.5B&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;Perplexity (169M)&lt;/th&gt; &#xA;   &lt;th&gt;Latency, ms (1.5B)&lt;/th&gt; &#xA;   &lt;th&gt;File size, GB (1.5B)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;17.507&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;76&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.53&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;17.187&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;72&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q5_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;16.194&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;1.60&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q5_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15.851&lt;/td&gt; &#xA;   &lt;td&gt;81&lt;/td&gt; &#xA;   &lt;td&gt;1.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q8_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;15.652&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;89&lt;/td&gt; &#xA;   &lt;td&gt;2.13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FP16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;15.623&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;117&lt;/td&gt; &#xA;   &lt;td&gt;2.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;FP32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;15.623&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;198&lt;/td&gt; &#xA;   &lt;td&gt;5.64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;With cuBLAS&lt;/h3&gt; &#xA;&lt;p&gt;Measurements were made on Intel i7 13700K &amp;amp; NVIDIA 3060 Ti 8 GB. The model is &lt;code&gt;RWKV-4-Pile-169M&lt;/code&gt;, 12 layers were offloaded to GPU.&lt;/p&gt; &#xA;&lt;p&gt;Latency per token in ms shown.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;1 thread&lt;/th&gt; &#xA;   &lt;th&gt;2 threads&lt;/th&gt; &#xA;   &lt;th&gt;4 threads&lt;/th&gt; &#xA;   &lt;th&gt;8 threads&lt;/th&gt; &#xA;   &lt;th&gt;24 threads&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7.9&lt;/td&gt; &#xA;   &lt;td&gt;6.2&lt;/td&gt; &#xA;   &lt;td&gt;6.9&lt;/td&gt; &#xA;   &lt;td&gt;8.6&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7.8&lt;/td&gt; &#xA;   &lt;td&gt;6.7&lt;/td&gt; &#xA;   &lt;td&gt;6.9&lt;/td&gt; &#xA;   &lt;td&gt;8.6&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q5_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8.1&lt;/td&gt; &#xA;   &lt;td&gt;6.7&lt;/td&gt; &#xA;   &lt;td&gt;6.9&lt;/td&gt; &#xA;   &lt;td&gt;9.0&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;1 thread&lt;/th&gt; &#xA;   &lt;th&gt;2 threads&lt;/th&gt; &#xA;   &lt;th&gt;4 threads&lt;/th&gt; &#xA;   &lt;th&gt;8 threads&lt;/th&gt; &#xA;   &lt;th&gt;24 threads&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;59&lt;/td&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;59&lt;/td&gt; &#xA;   &lt;td&gt;51&lt;/td&gt; &#xA;   &lt;td&gt;49&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q5_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;77&lt;/td&gt; &#xA;   &lt;td&gt;69&lt;/td&gt; &#xA;   &lt;td&gt;67&lt;/td&gt; &#xA;   &lt;td&gt;72&lt;/td&gt; &#xA;   &lt;td&gt;101&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: since cuBLAS is supported only for &lt;code&gt;ggml_mul_mat()&lt;/code&gt;, we still need to use few CPU resources to execute remaining operations.&lt;/p&gt; &#xA;&lt;h3&gt;With hipBLAS&lt;/h3&gt; &#xA;&lt;p&gt;Measurements were made on CPU AMD Ryzen 9 5900X &amp;amp; GPU AMD Radeon RX 7900 XTX. The model is &lt;code&gt;RWKV-novel-4-World-7B-20230810-ctx128k&lt;/code&gt;, 32 layers were offloaded to GPU.&lt;/p&gt; &#xA;&lt;p&gt;Latency per token in ms shown.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;1 thread&lt;/th&gt; &#xA;   &lt;th&gt;2 threads&lt;/th&gt; &#xA;   &lt;th&gt;4 threads&lt;/th&gt; &#xA;   &lt;th&gt;8 threads&lt;/th&gt; &#xA;   &lt;th&gt;24 threads&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;f16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;94&lt;/td&gt; &#xA;   &lt;td&gt;91&lt;/td&gt; &#xA;   &lt;td&gt;94&lt;/td&gt; &#xA;   &lt;td&gt;106&lt;/td&gt; &#xA;   &lt;td&gt;944&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83&lt;/td&gt; &#xA;   &lt;td&gt;77&lt;/td&gt; &#xA;   &lt;td&gt;75&lt;/td&gt; &#xA;   &lt;td&gt;110&lt;/td&gt; &#xA;   &lt;td&gt;1692&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q4_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;85&lt;/td&gt; &#xA;   &lt;td&gt;80&lt;/td&gt; &#xA;   &lt;td&gt;85&lt;/td&gt; &#xA;   &lt;td&gt;93&lt;/td&gt; &#xA;   &lt;td&gt;1691&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Q5_1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;83&lt;/td&gt; &#xA;   &lt;td&gt;90&lt;/td&gt; &#xA;   &lt;td&gt;1115&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: same as cuBLAS, hipBLAS only supports &lt;code&gt;ggml_mul_mat()&lt;/code&gt;, we still need to use few CPU resources to execute remaining operations.&lt;/p&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone the repo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: &lt;a href=&#34;https://gitforwindows.org/&#34;&gt;git&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;git clone --recursive https://github.com/saharNooby/rwkv.cpp.git&#xA;cd rwkv.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Get the rwkv.cpp library&lt;/h3&gt; &#xA;&lt;h4&gt;Option 2.1. Download a pre-compiled library&lt;/h4&gt; &#xA;&lt;h5&gt;Windows / Linux / MacOS&lt;/h5&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp/releases&#34;&gt;Releases&lt;/a&gt;, download appropriate ZIP for your OS and CPU, extract &lt;code&gt;rwkv&lt;/code&gt; library file into the repository directory.&lt;/p&gt; &#xA;&lt;p&gt;On Windows: to check whether your CPU supports AVX2 or AVX-512, &lt;a href=&#34;https://www.cpuid.com/softwares/cpu-z.html&#34;&gt;use CPU-Z&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Option 2.2. Build the library yourself&lt;/h4&gt; &#xA;&lt;p&gt;This option is recommended for maximum performance, because the library would be built specifically for your CPU and OS.&lt;/p&gt; &#xA;&lt;h5&gt;Windows&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: &lt;a href=&#34;https://cmake.org/download/&#34;&gt;CMake&lt;/a&gt; or &lt;a href=&#34;https://anaconda.org/conda-forge/cmake&#34;&gt;CMake from anaconda&lt;/a&gt;, &lt;a href=&#34;https://visualstudio.microsoft.com/vs/older-downloads/&#34;&gt;Build Tools for Visual Studio 2019&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything went OK, &lt;code&gt;bin\Release\rwkv.dll&lt;/code&gt; file should appear.&lt;/p&gt; &#xA;&lt;h5&gt;Windows + cuBLAS&lt;/h5&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/docs%2FcuBLAS_on_Windows.md&#34;&gt;docs/cuBLAS_on_Windows.md&lt;/a&gt; for a comprehensive guide.&lt;/p&gt; &#xA;&lt;h5&gt;Windows + hipBLAS&lt;/h5&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/docs%2FhipBLAS_on_Windows.md&#34;&gt;docs/hipBLAS_on_Windows.md&lt;/a&gt; for a comprehensive guide.&lt;/p&gt; &#xA;&lt;h5&gt;Linux / MacOS&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: CMake (Linux: &lt;code&gt;sudo apt install cmake&lt;/code&gt;, MacOS: &lt;code&gt;brew install cmake&lt;/code&gt;, anaconoda: &lt;a href=&#34;https://anaconda.org/conda-forge/cmake&#34;&gt;cmake package&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cmake .&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anaconda &amp;amp; M1 users&lt;/strong&gt;: please verify that &lt;code&gt;CMAKE_SYSTEM_PROCESSOR: arm64&lt;/code&gt; after running &lt;code&gt;cmake .&lt;/code&gt; — if it detects &lt;code&gt;x86_64&lt;/code&gt;, edit the &lt;code&gt;CMakeLists.txt&lt;/code&gt; file under the &lt;code&gt;# Compile flags&lt;/code&gt; to add &lt;code&gt;set(CMAKE_SYSTEM_PROCESSOR &#34;arm64&#34;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If everything went OK, &lt;code&gt;librwkv.so&lt;/code&gt; (Linux) or &lt;code&gt;librwkv.dylib&lt;/code&gt; (MacOS) file should appear in the base repo folder.&lt;/p&gt; &#xA;&lt;h5&gt;Linux / MacOS + cuBLAS&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cmake . -DRWKV_CUBLAS=ON&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything went OK, &lt;code&gt;librwkv.so&lt;/code&gt; (Linux) or &lt;code&gt;librwkv.dylib&lt;/code&gt; (MacOS) file should appear in the base repo folder.&lt;/p&gt; &#xA;&lt;h3&gt;3. Get an RWKV model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.x with &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, download a model from &lt;a href=&#34;https://huggingface.co/BlinkDL&#34;&gt;Hugging Face&lt;/a&gt; like &lt;a href=&#34;https://huggingface.co/BlinkDL/rwkv-4-pile-169m/blob/main/RWKV-4-Pile-169M-20220807-8023.pth&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt;, convert it into &lt;code&gt;rwkv.cpp&lt;/code&gt; format using following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;# Windows&#xA;python python\convert_pytorch_to_ggml.py C:\RWKV-4-Pile-169M-20220807-8023.pth C:\rwkv.cpp-169M.bin FP16&#xA;&#xA;# Linux / MacOS&#xA;python python/convert_pytorch_to_ggml.py ~/Downloads/RWKV-4-Pile-169M-20220807-8023.pth ~/Downloads/rwkv.cpp-169M.bin FP16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optionally&lt;/strong&gt;, quantize the model into one of quantized formats from the table above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;# Windows&#xA;python python\quantize.py C:\rwkv.cpp-169M.bin C:\rwkv.cpp-169M-Q5_1.bin Q5_1&#xA;&#xA;# Linux / MacOS&#xA;python python/quantize.py ~/Downloads/rwkv.cpp-169M.bin ~/Downloads/rwkv.cpp-169M-Q5_1.bin Q5_1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Run the model&lt;/h3&gt; &#xA;&lt;h4&gt;Using the command line&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.x with &lt;a href=&#34;https://numpy.org/&#34;&gt;numpy&lt;/a&gt;. If using &lt;code&gt;Pile&lt;/code&gt; or &lt;code&gt;Raven&lt;/code&gt; models, &lt;a href=&#34;https://pypi.org/project/tokenizers/&#34;&gt;tokenizers&lt;/a&gt; is also required.&lt;/p&gt; &#xA;&lt;p&gt;To generate some text, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;# Windows&#xA;python python\generate_completions.py C:\rwkv.cpp-169M-Q5_1.bin&#xA;&#xA;# Linux / MacOS&#xA;python python/generate_completions.py ~/Downloads/rwkv.cpp-169M-Q5_1.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To chat with a bot, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;# Windows&#xA;python python\chat_with_bot.py C:\rwkv.cpp-169M-Q5_1.bin&#xA;&#xA;# Linux / MacOS&#xA;python python/chat_with_bot.py ~/Downloads/rwkv.cpp-169M-Q5_1.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/rwkv%2Fgenerate_completions.py&#34;&gt;generate_completions.py&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/rwkv%2Fchat_with_bot.py&#34;&gt;chat_with_bot.py&lt;/a&gt; to change prompts and sampling settings.&lt;/p&gt; &#xA;&lt;h4&gt;Using in your own code&lt;/h4&gt; &#xA;&lt;p&gt;The short and simple script &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/python%2Finference_example.py&#34;&gt;inference_example.py&lt;/a&gt; demostrates the use of &lt;code&gt;rwkv.cpp&lt;/code&gt; in Python.&lt;/p&gt; &#xA;&lt;p&gt;To use &lt;code&gt;rwkv.cpp&lt;/code&gt; in C/C++, include the header &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/rwkv.h&#34;&gt;rwkv.h&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use &lt;code&gt;rwkv.cpp&lt;/code&gt; in any other language, see &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/#Bindings&#34;&gt;Bindings&lt;/a&gt; section below. If your language is missing, you can try to bind to the C API using the tooling provided by your language.&lt;/p&gt; &#xA;&lt;h2&gt;Bindings&lt;/h2&gt; &#xA;&lt;p&gt;These projects wrap &lt;code&gt;rwkv.cpp&lt;/code&gt; for easier use in other languages/frameworks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Golang: &lt;a href=&#34;https://github.com/seasonjs/rwkv&#34;&gt;seasonjs/rwkv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Node.js: &lt;a href=&#34;https://github.com/Atome-FE/llama-node&#34;&gt;Atome-FE/llama-node&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;ggml&lt;/code&gt; moves fast, and can occasionally break compatibility with older file formats.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;rwkv.cpp&lt;/code&gt; will attempt it&#39;s best to explain why a model file can&#39;t be loaded and what next steps are available to the user.&lt;/p&gt; &#xA;&lt;p&gt;For reference only, here is a list of latest versions of &lt;code&gt;rwkv.cpp&lt;/code&gt; that have supported older formats. &lt;strong&gt;No support will be provided for these versions&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Q4_2&lt;/code&gt;, old layout of quantized formats &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp/commit/3ca9c7f7857a4b9f3de616ec938e71249cfb3f3f&#34;&gt;commit 3ca9c7f&lt;/a&gt;, &lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp/releases/tag/master-3ca9c7f&#34;&gt;release with prebuilt binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Q4_3&lt;/code&gt;, &lt;code&gt;Q4_1_O&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp/commit/c736ef5411606b529d3a74c139ee111ef1a28bb9&#34;&gt;commit c736ef5&lt;/a&gt;, &lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp/releases/tag/master-1c363e6&#34;&gt;release with prebuilt binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/docs/FILE_FORMAT.md&#34;&gt;docs/FILE_FORMAT.md&lt;/a&gt; for version numbers of &lt;code&gt;rwkv.cpp&lt;/code&gt; model files and their changelog.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the code style described in &lt;a href=&#34;https://raw.githubusercontent.com/RWKV/rwkv.cpp/master/docs/CODE_STYLE.md&#34;&gt;docs/CODE_STYLE.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>