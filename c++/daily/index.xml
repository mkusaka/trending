<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-11T01:29:33Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xtensor-stack/xtensor</title>
    <updated>2025-04-11T01:29:33Z</updated>
    <id>tag:github.com,2025-04-11:/xtensor-stack/xtensor</id>
    <link href="https://github.com/xtensor-stack/xtensor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ tensors with broadcasting and lazy computing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xtensor.svg?sanitize=true&#34; alt=&#34;xtensor&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xtensor/actions/workflows/linux.yml&#34;&gt;&lt;img src=&#34;https://github.com/xtensor-stack/xtensor/actions/workflows/linux.yml/badge.svg?sanitize=true&#34; alt=&#34;GHA Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xtensor-stack/xtensor/actions/workflows/osx.yml&#34;&gt;&lt;img src=&#34;https://github.com/xtensor-stack/xtensor/actions/workflows/osx.yml/badge.svg?sanitize=true&#34; alt=&#34;GHA OSX&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xtensor-stack/xtensor/actions/workflows/windows.yml&#34;&gt;&lt;img src=&#34;https://github.com/xtensor-stack/xtensor/actions/workflows/windows.yml/badge.svg?sanitize=true&#34; alt=&#34;GHA Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://xtensor.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;http://readthedocs.org/projects/xtensor/badge/?version=latest&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://xtensor-stack.github.io/xtensor&#34;&gt;&lt;img src=&#34;https://github.com/xtensor-stack/xtensor/workflows/gh-pages/badge.svg?sanitize=true&#34; alt=&#34;Doxygen -&gt; gh-pages&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/xtensor-stack/xtensor/stable?filepath=notebooks%2Fxtensor.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/QuantStack/Lobby?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the Gitter Chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Multi-dimensional arrays with broadcasting and lazy computing.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;xtensor&lt;/code&gt; is a C++ library meant for numerical analysis with multi-dimensional array expressions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;xtensor&lt;/code&gt; provides&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;an extensible expression system enabling &lt;strong&gt;lazy broadcasting&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;an API following the idioms of the &lt;strong&gt;C++ standard library&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools to manipulate array expressions and build upon &lt;code&gt;xtensor&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Containers of &lt;code&gt;xtensor&lt;/code&gt; are inspired by &lt;a href=&#34;http://www.numpy.org&#34;&gt;NumPy&lt;/a&gt;, the Python array programming library. &lt;strong&gt;Adaptors&lt;/strong&gt; for existing data structures to be plugged into our expression system can easily be written.&lt;/p&gt; &#xA;&lt;p&gt;In fact, &lt;code&gt;xtensor&lt;/code&gt; can be used to &lt;strong&gt;process NumPy data structures inplace&lt;/strong&gt; using Python&#39;s &lt;a href=&#34;https://docs.python.org/3/c-api/buffer.html&#34;&gt;buffer protocol&lt;/a&gt;. Similarly, we can operate on Julia and R arrays. For more details on the NumPy, Julia and R bindings, check out the &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-python&#34;&gt;xtensor-python&lt;/a&gt;, &lt;a href=&#34;https://github.com/xtensor-stack/Xtensor.jl&#34;&gt;xtensor-julia&lt;/a&gt; and &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-r&#34;&gt;xtensor-r&lt;/a&gt; projects respectively.&lt;/p&gt; &#xA;&lt;p&gt;Up to version 0.26.0, &lt;code&gt;xtensor&lt;/code&gt; requires a C++ compiler supporting C++14.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;xtensor&lt;/code&gt; 0.26.x requires a C++ compiler supporting C++17.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Package managers&lt;/h3&gt; &#xA;&lt;p&gt;We provide a package for the mamba (or conda) package manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mamba install -c conda-forge xtensor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install from sources&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;xtensor&lt;/code&gt; is a header-only library.&lt;/p&gt; &#xA;&lt;p&gt;You can directly install it from the sources:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -DCMAKE_INSTALL_PREFIX=your_install_prefix&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing xtensor using vcpkg&lt;/h3&gt; &#xA;&lt;p&gt;You can download and install xtensor using the &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Microsoft/vcpkg.git&#xA;cd vcpkg&#xA;./bootstrap-vcpkg.sh&#xA;./vcpkg integrate install&#xA;./vcpkg install xtensor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The xtensor port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h2&gt;Trying it online&lt;/h2&gt; &#xA;&lt;p&gt;You can play with &lt;code&gt;xtensor&lt;/code&gt; interactively in a Jupyter notebook right now! Just click on the binder link below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/xtensor-stack/xtensor/stable?filepath=notebooks/xtensor.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/binder-logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The C++ support in Jupyter is powered by the &lt;a href=&#34;https://github.com/jupyter-xeus/xeus-cling&#34;&gt;xeus-cling&lt;/a&gt; C++ kernel. Together with xeus-cling, xtensor enables a similar workflow to that of NumPy with the IPython Jupyter kernel.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xeus-cling-screenshot.png&#34; alt=&#34;xeus-cling&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For more information on using &lt;code&gt;xtensor&lt;/code&gt;, check out the reference documentation&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://xtensor.readthedocs.io/&#34;&gt;http://xtensor.readthedocs.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;xtensor&lt;/code&gt; depends on the &lt;a href=&#34;https://github.com/xtensor-stack/xtl&#34;&gt;xtl&lt;/a&gt; library and has an optional dependency on the &lt;a href=&#34;https://github.com/xtensor-stack/xsimd&#34;&gt;xsimd&lt;/a&gt; library:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;xtensor&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;xtl&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;xsimd&lt;/code&gt; (optional)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;master&lt;/td&gt; &#xA;   &lt;td&gt;^0.8.0&lt;/td&gt; &#xA;   &lt;td&gt;^13.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.26.0&lt;/td&gt; &#xA;   &lt;td&gt;^0.8.0&lt;/td&gt; &#xA;   &lt;td&gt;^13.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.25.0&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.5&lt;/td&gt; &#xA;   &lt;td&gt;^11.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.7&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^10.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.6&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^10.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.5&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^10.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.4&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^10.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.3&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^8.0.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.2&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^8.0.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.1&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^8.0.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.24.0&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^8.0.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.23.x&lt;/td&gt; &#xA;   &lt;td&gt;^0.7.0&lt;/td&gt; &#xA;   &lt;td&gt;^7.4.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.22.0&lt;/td&gt; &#xA;   &lt;td&gt;^0.6.23&lt;/td&gt; &#xA;   &lt;td&gt;^7.4.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The dependency on &lt;code&gt;xsimd&lt;/code&gt; is required if you want to enable SIMD acceleration in &lt;code&gt;xtensor&lt;/code&gt;. This can be done by defining the macro &lt;code&gt;XTENSOR_USE_XSIMD&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; including any header of &lt;code&gt;xtensor&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Basic usage&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Initialize a 2-D array and compute the sum of one of its rows and a 1-D array.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;&#xA;#include &#34;xtensor/xarray.hpp&#34;&#xA;#include &#34;xtensor/xio.hpp&#34;&#xA;#include &#34;xtensor/xview.hpp&#34;&#xA;&#xA;xt::xarray&amp;lt;double&amp;gt; arr1&#xA;  {{1.0, 2.0, 3.0},&#xA;   {2.0, 5.0, 7.0},&#xA;   {2.0, 5.0, 7.0}};&#xA;&#xA;xt::xarray&amp;lt;double&amp;gt; arr2&#xA;  {5.0, 6.0, 7.0};&#xA;&#xA;xt::xarray&amp;lt;double&amp;gt; res = xt::view(arr1, 1) + arr2;&#xA;&#xA;std::cout &amp;lt;&amp;lt; res;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{7, 11, 14}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Initialize a 1-D array and reshape it inplace.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;&#xA;#include &#34;xtensor/xarray.hpp&#34;&#xA;#include &#34;xtensor/xio.hpp&#34;&#xA;&#xA;xt::xarray&amp;lt;int&amp;gt; arr&#xA;  {1, 2, 3, 4, 5, 6, 7, 8, 9};&#xA;&#xA;arr.reshape({3, 3});&#xA;&#xA;std::cout &amp;lt;&amp;lt; arr;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{{1, 2, 3},&#xA; {4, 5, 6},&#xA; {7, 8, 9}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Index Access&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;#include &amp;lt;iostream&amp;gt;&#xA;#include &#34;xtensor/xarray.hpp&#34;&#xA;#include &#34;xtensor/xio.hpp&#34;&#xA;&#xA;xt::xarray&amp;lt;double&amp;gt; arr1&#xA;  {{1.0, 2.0, 3.0},&#xA;   {2.0, 5.0, 7.0},&#xA;   {2.0, 5.0, 7.0}};&#xA;&#xA;std::cout &amp;lt;&amp;lt; arr1(0, 0) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;xt::xarray&amp;lt;int&amp;gt; arr2&#xA;  {1, 2, 3, 4, 5, 6, 7, 8, 9};&#xA;&#xA;std::cout &amp;lt;&amp;lt; arr2(0);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;1.0&#xA;1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;The NumPy to xtensor cheat sheet&lt;/h3&gt; &#xA;&lt;p&gt;If you are familiar with NumPy APIs, and you are interested in xtensor, you can check out the &lt;a href=&#34;https://xtensor.readthedocs.io/en/latest/numpy.html&#34;&gt;NumPy to xtensor cheat sheet&lt;/a&gt; provided in the documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Lazy broadcasting with &lt;code&gt;xtensor&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Xtensor can operate on arrays of different shapes of dimensions in an element-wise fashion. Broadcasting rules of xtensor are similar to those of &lt;a href=&#34;http://www.numpy.org&#34;&gt;NumPy&lt;/a&gt; and &lt;a href=&#34;http://libdynd.org&#34;&gt;libdynd&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Broadcasting rules&lt;/h3&gt; &#xA;&lt;p&gt;In an operation involving two arrays of different dimensions, the array with the lesser dimensions is broadcast across the leading dimensions of the other.&lt;/p&gt; &#xA;&lt;p&gt;For example, if &lt;code&gt;A&lt;/code&gt; has shape &lt;code&gt;(2, 3)&lt;/code&gt;, and &lt;code&gt;B&lt;/code&gt; has shape &lt;code&gt;(4, 2, 3)&lt;/code&gt;, the result of a broadcasted operation with &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; has shape &lt;code&gt;(4, 2, 3)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   (2, 3) # A&#xA;(4, 2, 3) # B&#xA;---------&#xA;(4, 2, 3) # Result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same rule holds for scalars, which are handled as 0-D expressions. If &lt;code&gt;A&lt;/code&gt; is a scalar, the equation becomes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;       () # A&#xA;(4, 2, 3) # B&#xA;---------&#xA;(4, 2, 3) # Result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If matched up dimensions of two input arrays are different, and one of them has size &lt;code&gt;1&lt;/code&gt;, it is broadcast to match the size of the other. Let&#39;s say B has the shape &lt;code&gt;(4, 2, 1)&lt;/code&gt; in the previous example, so the broadcasting happens as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   (2, 3) # A&#xA;(4, 2, 1) # B&#xA;---------&#xA;(4, 2, 3) # Result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Universal functions, laziness and vectorization&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;code&gt;xtensor&lt;/code&gt;, if &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; are arrays of &lt;em&gt;broadcastable shapes&lt;/em&gt;, the return type of an expression such as &lt;code&gt;x + y * sin(z)&lt;/code&gt; is &lt;strong&gt;not an array&lt;/strong&gt;. It is an &lt;code&gt;xexpression&lt;/code&gt; object offering the same interface as an N-dimensional array, which does not hold the result. &lt;strong&gt;Values are only computed upon access or when the expression is assigned to an xarray object&lt;/strong&gt;. This allows to operate symbolically on very large arrays and only compute the result for the indices of interest.&lt;/p&gt; &#xA;&lt;p&gt;We provide utilities to &lt;strong&gt;vectorize any scalar function&lt;/strong&gt; (taking multiple scalar arguments) into a function that will perform on &lt;code&gt;xexpression&lt;/code&gt;s, applying the lazy broadcasting rules which we just described. These functions are called &lt;em&gt;xfunction&lt;/em&gt;s. They are &lt;code&gt;xtensor&lt;/code&gt;&#39;s counterpart to NumPy&#39;s universal functions.&lt;/p&gt; &#xA;&lt;p&gt;In &lt;code&gt;xtensor&lt;/code&gt;, arithmetic operations (&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;) and all special functions are &lt;em&gt;xfunction&lt;/em&gt;s.&lt;/p&gt; &#xA;&lt;h3&gt;Iterating over &lt;code&gt;xexpression&lt;/code&gt;s and broadcasting Iterators&lt;/h3&gt; &#xA;&lt;p&gt;All &lt;code&gt;xexpression&lt;/code&gt;s offer two sets of functions to retrieve iterator pairs (and their &lt;code&gt;const&lt;/code&gt; counterpart).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;begin()&lt;/code&gt; and &lt;code&gt;end()&lt;/code&gt; provide instances of &lt;code&gt;xiterator&lt;/code&gt;s which can be used to iterate over all the elements of the expression. The order in which elements are listed is &lt;code&gt;row-major&lt;/code&gt; in that the index of last dimension is incremented first.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;begin(shape)&lt;/code&gt; and &lt;code&gt;end(shape)&lt;/code&gt; are similar but take a &lt;em&gt;broadcasting shape&lt;/em&gt; as an argument. Elements are iterated upon in a row-major way, but certain dimensions are repeated to match the provided shape as per the rules described above. For an expression &lt;code&gt;e&lt;/code&gt;, &lt;code&gt;e.begin(e.shape())&lt;/code&gt; and &lt;code&gt;e.begin()&lt;/code&gt; are equivalent.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Runtime vs compile-time dimensionality&lt;/h3&gt; &#xA;&lt;p&gt;Two container classes implementing multi-dimensional arrays are provided: &lt;code&gt;xarray&lt;/code&gt; and &lt;code&gt;xtensor&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;xarray&lt;/code&gt; can be reshaped dynamically to any number of dimensions. It is the container that is the most similar to NumPy arrays.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;xtensor&lt;/code&gt; has a dimension set at compilation time, which enables many optimizations. For example, shapes and strides of &lt;code&gt;xtensor&lt;/code&gt; instances are allocated on the stack instead of the heap.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;xarray&lt;/code&gt; and &lt;code&gt;xtensor&lt;/code&gt; container are both &lt;code&gt;xexpression&lt;/code&gt;s and can be involved and mixed in universal functions, assigned to each other etc...&lt;/p&gt; &#xA;&lt;p&gt;Besides, two access operators are provided:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The variadic template &lt;code&gt;operator()&lt;/code&gt; which can take multiple integral arguments or none.&lt;/li&gt; &#xA; &lt;li&gt;And the &lt;code&gt;operator[]&lt;/code&gt; which takes a single multi-index argument, which can be of size determined at runtime. &lt;code&gt;operator[]&lt;/code&gt; also supports access with braced initializers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performances&lt;/h2&gt; &#xA;&lt;p&gt;Xtensor operations make use of SIMD acceleration depending on what instruction sets are available on the platform at hand (SSE, AVX, AVX512, Neon).&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xsimd&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xsimd-small.svg?sanitize=true&#34; alt=&#34;xsimd&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/xtensor-stack/xsimd&#34;&gt;xsimd&lt;/a&gt; project underlies the detection of the available instruction sets, and provides generic high-level wrappers and memory allocators for client libraries such as xtensor.&lt;/p&gt; &#xA;&lt;h3&gt;Continuous benchmarking&lt;/h3&gt; &#xA;&lt;p&gt;Xtensor operations are continuously benchmarked, and are significantly improved at each new version. Current performances on statically dimensioned tensors match those of the Eigen library. Dynamically dimension tensors for which the shape is heap allocated come at a small additional cost.&lt;/p&gt; &#xA;&lt;h3&gt;Stack allocation for shapes and strides&lt;/h3&gt; &#xA;&lt;p&gt;More generally, the library implement a &lt;code&gt;promote_shape&lt;/code&gt; mechanism at build time to determine the optimal sequence type to hold the shape of an expression. The shape type of a broadcasting expression whose members have a dimensionality determined at compile time will have a stack allocated sequence type. If at least one note of a broadcasting expression has a dynamic dimension (for example an &lt;code&gt;xarray&lt;/code&gt;), it bubbles up to the entire broadcasting expression which will have a heap allocated shape. The same hold for views, broadcast expressions, etc...&lt;/p&gt; &#xA;&lt;p&gt;Therefore, when building an application with xtensor, we recommend using statically-dimensioned containers whenever possible to improve the overall performance of the application.&lt;/p&gt; &#xA;&lt;h2&gt;Language bindings&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xtensor-python&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xtensor-python-small.svg?sanitize=true&#34; alt=&#34;xtensor-python&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-python&#34;&gt;xtensor-python&lt;/a&gt; project provides the implementation of two &lt;code&gt;xtensor&lt;/code&gt; containers, &lt;code&gt;pyarray&lt;/code&gt; and &lt;code&gt;pytensor&lt;/code&gt; which effectively wrap NumPy arrays, allowing inplace modification, including reshapes.&lt;/p&gt; &#xA;&lt;p&gt;Utilities to automatically generate NumPy-style universal functions, exposed to Python from scalar functions are also provided.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xtensor-julia&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xtensor-julia-small.svg?sanitize=true&#34; alt=&#34;xtensor-julia&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-julia&#34;&gt;xtensor-julia&lt;/a&gt; project provides the implementation of two &lt;code&gt;xtensor&lt;/code&gt; containers, &lt;code&gt;jlarray&lt;/code&gt; and &lt;code&gt;jltensor&lt;/code&gt; which effectively wrap julia arrays, allowing inplace modification, including reshapes.&lt;/p&gt; &#xA;&lt;p&gt;Like in the Python case, utilities to generate NumPy-style universal functions are provided.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xtensor-r&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xtensor-r-small.svg?sanitize=true&#34; alt=&#34;xtensor-r&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-r&#34;&gt;xtensor-r&lt;/a&gt; project provides the implementation of two &lt;code&gt;xtensor&lt;/code&gt; containers, &lt;code&gt;rarray&lt;/code&gt; and &lt;code&gt;rtensor&lt;/code&gt; which effectively wrap R arrays, allowing inplace modification, including reshapes.&lt;/p&gt; &#xA;&lt;p&gt;Like for the Python and Julia bindings, utilities to generate NumPy-style universal functions are provided.&lt;/p&gt; &#xA;&lt;h2&gt;Library bindings&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xtensor-blas&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xtensor-blas-small.svg?sanitize=true&#34; alt=&#34;xtensor-blas&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-blas&#34;&gt;xtensor-blas&lt;/a&gt; project provides bindings to BLAS libraries, enabling linear-algebra operations on xtensor expressions.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/xtensor-stack/xtensor-io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/docs/source/xtensor-io-small.svg?sanitize=true&#34; alt=&#34;xtensor-io&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/xtensor-stack/xtensor-io&#34;&gt;xtensor-io&lt;/a&gt; project enables the loading of a variety of file formats into xtensor expressions, such as image files, sound files, HDF5 files, as well as NumPy npy and npz files.&lt;/p&gt; &#xA;&lt;h2&gt;Building and running the tests&lt;/h2&gt; &#xA;&lt;p&gt;Building the tests requires the &lt;a href=&#34;https://github.com/google/googletest&#34;&gt;GTest&lt;/a&gt; testing framework and &lt;a href=&#34;https://cmake.org&#34;&gt;cmake&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;gtest and cmake are available as packages for most Linux distributions. Besides, they can also be installed with the &lt;code&gt;conda&lt;/code&gt; package manager (even on windows):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c conda-forge gtest cmake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once &lt;code&gt;gtest&lt;/code&gt; and &lt;code&gt;cmake&lt;/code&gt; are installed, you can build and run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -DBUILD_TESTS=ON ../&#xA;make xtest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use CMake to download the source of &lt;code&gt;gtest&lt;/code&gt;, build it, and use the generated libraries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -DBUILD_TESTS=ON -DDOWNLOAD_GTEST=ON ../&#xA;make xtest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building the HTML documentation&lt;/h2&gt; &#xA;&lt;p&gt;xtensor&#39;s documentation is built with three tools&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.doxygen.org&#34;&gt;doxygen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;sphinx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://breathe.readthedocs.io&#34;&gt;breathe&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;While doxygen must be installed separately, you can install breathe by typing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install breathe sphinx_rtd_theme&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Breathe can also be installed with &lt;code&gt;conda&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c conda-forge breathe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, go to &lt;code&gt;docs&lt;/code&gt; subdirectory and build the documentation with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;We use a shared copyright model that enables all contributors to maintain the copyright on their contributions.&lt;/p&gt; &#xA;&lt;p&gt;This software is licensed under the BSD-3-Clause license. See the &lt;a href=&#34;https://raw.githubusercontent.com/xtensor-stack/xtensor/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>