<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-15T01:29:29Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HanGuo97/flute</title>
    <updated>2025-04-15T01:29:29Z</updated>
    <id>tag:github.com,2025-04-15:/HanGuo97/flute</id>
    <link href="https://github.com/HanGuo97/flute" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast Matrix Multiplications for Lookup Table-Quantized LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/assets/flute-logo.png&#34; alt=&#34;&#34; width=&#34;40%&#34; align=&#34;top&#34; style=&#34;border-radius: 10px; padding-left: 120px; padding-right: 120px; background-color: white;&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;&lt;strong&gt;FLUTE&lt;/strong&gt;: Flexible Lookup Table Engine for LUT-quantized LLMs &lt;br&gt;&lt;/em&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/HanGuo97/flute&#34; alt=&#34;GitHub License&#34;&gt; &lt;a href=&#34;https://pypi.org/project/flute-kernel/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/flute-kernel&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2407.10960&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2407.10960-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#background&#34;&gt;Background&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#benchmarks&#34;&gt;Benchmarks&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#support-and-compatibility&#34;&gt;Compatibility&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Update&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;February, 2024.&lt;/strong&gt; HIGGS will appear in NAACL 2025.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;January 9, 2025.&lt;/strong&gt; Added (very) experimental support for removing specialization on shapes + GPU via auto-tune.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;December 12, 2024.&lt;/strong&gt; Added support for Hadamard Transform (via &lt;a href=&#34;https://pytorch.org/blog/hadacore/&#34;&gt;HadaCore&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;November 26, 2024.&lt;/strong&gt; Added support for &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/flute/integrations/higgs.py&#34;&gt;vector (de)quantization&lt;/a&gt; (&lt;code&gt;vector_size=2&lt;/code&gt;), as part of &lt;a href=&#34;https://arxiv.org/abs/2411.17525&#34;&gt;HIGGS&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;October 5, 2024.&lt;/strong&gt; FLUTE will appear in EMNLP 2024 (Findings).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;September 15, 2024.&lt;/strong&gt; Added &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#flute--huggingface&#34;&gt;experimental support&lt;/a&gt; for loading pre-quantized FLUTE models in HuggingFace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;September 6, 2024.&lt;/strong&gt; Added (unlearned) NF-quantized LLaMA-3.1 (405B) models: &lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-405B-FLUTE/tree/nf_w4g64&#34;&gt;base&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-405B-Instruct-FLUTE/tree/nf_w4g64&#34;&gt;instruction tuned&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;August 31, 2024.&lt;/strong&gt; Added &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#learned-normal-float-quantization-nfl&#34;&gt;support&lt;/a&gt; and &lt;a href=&#34;https://github.com/HanGuo97/flute/raw/main/examples/learnable_scales_eval.ipynb&#34;&gt;example&lt;/a&gt; for the Learned Normal Float (NFL) quantization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;August 26, 2024.&lt;/strong&gt; Added &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#converting-bitsandbytes-model-into-flute-model&#34;&gt;support&lt;/a&gt; for converting &lt;code&gt;bitsandbytes&lt;/code&gt; model into FLUTE model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;August 5, 2024.&lt;/strong&gt; Added quantized LLaMA-3.1 (8B/70B) models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;August 2, 2024.&lt;/strong&gt; Added support for RTX4090.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;July 27, 2024.&lt;/strong&gt; Added support for LLaMA-3.1 (405B) and tuned BF16 performance. FP16 is still the recommended data type, especially for 3-bit settings.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Install FLUTE with pip or &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#build-from-source&#34;&gt;from source&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For CUDA 12.1&#xA;pip install flute-kernel&#xA;# For CUDA 11.8&#xA;pip install flute-kernel -i https://flute-ai.github.io/whl/cu118&#xA;# For CUDA 12.4&#xA;pip install flute-kernel -i https://flute-ai.github.io/whl/cu124&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Head over to &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; and try it out!&lt;/p&gt; &#xA;&lt;h1&gt;Background&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uniform quantization&lt;/strong&gt; converts full precision weights to lower-precision intervals of equal size. &lt;strong&gt;Lookup table (LUT) quantization&lt;/strong&gt; is a flexible variant of non-uniform quantization which can map intervals to arbitrary values via a lookup table.&lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Uniform (Integer) Quantization&lt;/th&gt; &#xA;   &lt;th&gt;Lookup Table Quantization&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;p&gt;$$\widehat{\mathbf{W}} = \mathtt{float}(\mathbf{Q}) \cdot \mathbf{s}$$&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;p&gt;$$\widehat{\mathbf{W}} = \mathtt{tableLookup}(\mathbf{Q}, \mathtt{table}) \cdot \mathbf{s}$$&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;where $\mathbf{Q}$ denote the quantized weight, $\mathbf{s}$ the (group-wise) scales, and $\widehat{\mathbf{W}}$ the de-quantized weight. Here are some examples of the lookup table suppored in FLUTE.&lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Examples&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;&lt;code&gt;int4&lt;/code&gt;, &lt;code&gt;int3&lt;/code&gt;, &lt;code&gt;int2&lt;/code&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;recovers uniform/integer quantization&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;&lt;code&gt;fp4&lt;/code&gt;, &lt;code&gt;fp3&lt;/code&gt;, &lt;code&gt;fp2&lt;/code&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;&lt;code&gt;nf4&lt;/code&gt;, &lt;code&gt;nf3&lt;/code&gt;, &lt;code&gt;nf2&lt;/code&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;generalizes the &lt;code&gt;nf4&lt;/code&gt; data-format introduced in QLoRA&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;any arbitrary table&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt; &lt;p&gt;you could even learn it!&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;New Models Powered by FLUTE&lt;/h3&gt; &#xA;&lt;p&gt;The flexibility of the kernel could lead to new quantization algorithms. As a proof of concept, we are releasing a few &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#models&#34;&gt;models&lt;/a&gt; quantized using &lt;strong&gt;Learned Normal Float (NFL)&lt;/strong&gt; --- a simple extension to the &lt;code&gt;nf4&lt;/code&gt; data format introduced in QLoRA. NFL initialized the lookup table and the scales with those from NF quantization. Then, it uses calibration data to learn the scales via straight through estimation for for the gradient with respect to the scales.&lt;/p&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;p&gt;For additional benchmarks, detailed breakdowns, and corresponding instruction-tuned models, please refer to the paper and the &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#model-zoo&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/assets/intro-figure.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;LLaMA-3.1&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki PPL&lt;/th&gt; &#xA;   &lt;th&gt;C4 PPL&lt;/th&gt; &#xA;   &lt;th&gt;LLM Eval Avg.&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki PPL&lt;/th&gt; &#xA;   &lt;th&gt;C4 PPL&lt;/th&gt; &#xA;   &lt;th&gt;LLM Eval Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-3.1 (8B)&lt;/td&gt; &#xA;   &lt;td&gt;6.31&lt;/td&gt; &#xA;   &lt;td&gt;9.60&lt;/td&gt; &#xA;   &lt;td&gt;69.75&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-3.1 (70B)&lt;/td&gt; &#xA;   &lt;td&gt;2.82&lt;/td&gt; &#xA;   &lt;td&gt;7.18&lt;/td&gt; &#xA;   &lt;td&gt;75.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.24&lt;/td&gt; &#xA;   &lt;td&gt;10.06&lt;/td&gt; &#xA;   &lt;td&gt;69.13&lt;/td&gt; &#xA;   &lt;td&gt;+ NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;3.09&lt;/td&gt; &#xA;   &lt;td&gt;7.53&lt;/td&gt; &#xA;   &lt;td&gt;74.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.23&lt;/td&gt; &#xA;   &lt;td&gt;11.83&lt;/td&gt; &#xA;   &lt;td&gt;65.66&lt;/td&gt; &#xA;   &lt;td&gt;+ NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;4.29&lt;/td&gt; &#xA;   &lt;td&gt;8.91&lt;/td&gt; &#xA;   &lt;td&gt;72.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Gemma-2&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki PPL&lt;/th&gt; &#xA;   &lt;th&gt;C4 PPL&lt;/th&gt; &#xA;   &lt;th&gt;LLM Eval Avg.&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki PPL&lt;/th&gt; &#xA;   &lt;th&gt;C4 PPL&lt;/th&gt; &#xA;   &lt;th&gt;LLM Eval Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma-2 (9B)&lt;/td&gt; &#xA;   &lt;td&gt;6.88&lt;/td&gt; &#xA;   &lt;td&gt;10.12&lt;/td&gt; &#xA;   &lt;td&gt;73.12&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2 (27B)&lt;/td&gt; &#xA;   &lt;td&gt;5.70&lt;/td&gt; &#xA;   &lt;td&gt;8.98&lt;/td&gt; &#xA;   &lt;td&gt;75.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.49&lt;/td&gt; &#xA;   &lt;td&gt;10.35&lt;/td&gt; &#xA;   &lt;td&gt;72.50&lt;/td&gt; &#xA;   &lt;td&gt;+ NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;5.69&lt;/td&gt; &#xA;   &lt;td&gt;9.31&lt;/td&gt; &#xA;   &lt;td&gt;74.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;FLUTE + vLLM&lt;/h2&gt; &#xA;&lt;p&gt;FLUTE-quantized models (&lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#models&#34;&gt;Model Zoo&lt;/a&gt;) can be directly served using exisiting frameworks such as vLLM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- python -m vllm.entrypoints.openai.api_server \&#xA;+ python -m flute.integrations.vllm vllm.entrypoints.openai.api_server \&#xA;    --model [MODEL] \&#xA;    --revision [REVISION] \&#xA;    --tensor-parallel-size [TP_SIZE] \&#xA;+   --quantization flute&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, the following commmand runs the FLUTE-quantized LLaMA-3.1 (8B) on a single GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m flute.integrations.vllm vllm.entrypoints.openai.api_server \&#xA;    --model radi-cho/Meta-Llama-3.1-8B-FLUTE \&#xA;    --quantization flute&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can then query the vLLM server as usual.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:8000/v1/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -d &#39;{&#xA;        &#34;model&#34;: &#34;radi-cho/Meta-Llama-3.1-8B-FLUTE&#34;,&#xA;        &#34;prompt&#34;: &#34;San Francisco is a&#34;,&#xA;        &#34;max_tokens&#34;: 7,&#xA;        &#34;temperature&#34;: 0&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FLUTE + HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;FLUTE also runs out of the box with HuggingFace and its &lt;code&gt;accelerate&lt;/code&gt; extension. This integration is mostly experimental and not optimized. Users sensitive to performance considerations should use the &lt;code&gt;vLLM&lt;/code&gt; integration instead.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Loading a pre-quantized FLUTE model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;import flute.integrations.huggingface&#xA;&#xA;- model = AutoModelForCausalLM.from_pretrained(&#xA;+ model = flute.integrations.huggingface.from_pretrained(&#xA;    &#34;radi-cho/Meta-Llama-3.1-8B-FLUTE&#34;,&#xA;    # all of your favoriate HF flags will be forwarded&#xA;    device_map=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Loading and quantizing a dense model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import flute.integrations.base&#xA;flute.integrations.base.prepare_model_flute(&#xA;    name=&#34;model.model.layers&#34;,&#xA;    module=model.model.layers,  # for LLaMA-3 and Gemma-2&#xA;    num_bits=num_bits,&#xA;    group_size=group_size,&#xA;    fake=False,&#xA;    handle_hooks=True)  # for `accelerate` hooks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this, the model can be used as normal. Please checkout the quantization &lt;a href=&#34;https://raw.githubusercontent.com/HanGuo97/flute/main/#quantizing-your-own-models&#34;&gt;guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Support and Compatibility&lt;/h1&gt; &#xA;&lt;h2&gt;Kernel&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Supported (via pip)&lt;/th&gt; &#xA;   &lt;th&gt;Supported (build from source)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Input dtypes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.float16&lt;/code&gt; &lt;code&gt;torch.bfloat16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bits&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;4bit&lt;/code&gt; &lt;code&gt;3bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;2bit&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Group Sizes&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;32&lt;/code&gt; &lt;code&gt;64&lt;/code&gt; &lt;code&gt;128&lt;/code&gt; &lt;code&gt;256&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❓&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPUs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;A100&lt;/code&gt; &lt;code&gt;A6000&lt;/code&gt; &lt;code&gt;RTX 4090&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;H100&lt;/code&gt; (unoptimized)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] In the current release, we noticed &lt;code&gt;torch.bfloat16&lt;/code&gt; is slower than &lt;code&gt;torch.float16&lt;/code&gt;. This likely because of lack of tuning, and that Ampere GPUs lack a hardware acceleration for &lt;code&gt;bfloat16&lt;/code&gt; &lt;a href=&#34;https://github.com/HanGuo97/flute/raw/main/flute/csrc/cutlass_extensions_bf16.h#L27&#34;&gt;vectorized atomic-add&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] We noticed several numerically unstable situations using &lt;code&gt;bits=4, group-size=256, GPU=A100&lt;/code&gt;, though this is relatively rare (8 of 9360 test cases failed). We also noticed correctness issues in some situations with &lt;code&gt;bits=4, group-size=256, dtype=bfloat16, GPU=RTX4090&lt;/code&gt; (1 of 52 test cases failed). We will be looking into this, but we suggest avoiding these particular use cases (&lt;code&gt;W4G256&lt;/code&gt;) for now.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] As of the current release, the kernel is shape-specialized due to legacy reasons (i.e., we tune tile sizes etc for each matrix shape). Please see the below chart for the supported use cases, as different platform and tensor parallel size changes the matrix shapes. We plan to add supports for a broad range of shapes in the near future. In the meantime, please let us know if you have any specific models in mind and we are happy to add support for them.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Single GPU / Pipeline Parallel&lt;/th&gt; &#xA;   &lt;th&gt;Tensor Parallel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-3/3.1 (8B)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-3/3.1 (70B)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;2 or 4 GPUs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-3.1 (405B)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;4 or 8 GPUs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma-2 (9B)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma-2 (27B)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;2 or 4 GPUs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] The models we release here are trained on more data and hence different from those in the paper.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The HuggingFace Hub links are for &lt;code&gt;NFL W4G64&lt;/code&gt; quantization by default. To use the &lt;code&gt;NFL W3G64&lt;/code&gt; quantization, add &lt;code&gt;--revision nfl_w3g64&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-8B-FLUTE&#34;&gt;LLaMA-3.1 (8B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;ARC-E&lt;/th&gt; &#xA;   &lt;th&gt;ARC-C&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Wino&lt;/th&gt; &#xA;   &lt;th&gt;Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unquantized&lt;/td&gt; &#xA;   &lt;td&gt;6.31&lt;/td&gt; &#xA;   &lt;td&gt;9.60&lt;/td&gt; &#xA;   &lt;td&gt;79.16&lt;/td&gt; &#xA;   &lt;td&gt;82.20&lt;/td&gt; &#xA;   &lt;td&gt;52.65&lt;/td&gt; &#xA;   &lt;td&gt;60.71&lt;/td&gt; &#xA;   &lt;td&gt;74.03&lt;/td&gt; &#xA;   &lt;td&gt;69.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.24&lt;/td&gt; &#xA;   &lt;td&gt;10.06&lt;/td&gt; &#xA;   &lt;td&gt;79.38&lt;/td&gt; &#xA;   &lt;td&gt;81.61&lt;/td&gt; &#xA;   &lt;td&gt;51.54&lt;/td&gt; &#xA;   &lt;td&gt;59.57&lt;/td&gt; &#xA;   &lt;td&gt;73.56&lt;/td&gt; &#xA;   &lt;td&gt;69.13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.23&lt;/td&gt; &#xA;   &lt;td&gt;11.83&lt;/td&gt; &#xA;   &lt;td&gt;77.91&lt;/td&gt; &#xA;   &lt;td&gt;76.98&lt;/td&gt; &#xA;   &lt;td&gt;46.33&lt;/td&gt; &#xA;   &lt;td&gt;56.74&lt;/td&gt; &#xA;   &lt;td&gt;70.32&lt;/td&gt; &#xA;   &lt;td&gt;65.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-70B-FLUTE&#34;&gt;LLaMA-3.1 (70B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;ARC-E&lt;/th&gt; &#xA;   &lt;th&gt;ARC-C&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Wino&lt;/th&gt; &#xA;   &lt;th&gt;Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unquantized&lt;/td&gt; &#xA;   &lt;td&gt;2.82&lt;/td&gt; &#xA;   &lt;td&gt;7.18&lt;/td&gt; &#xA;   &lt;td&gt;82.81&lt;/td&gt; &#xA;   &lt;td&gt;85.31&lt;/td&gt; &#xA;   &lt;td&gt;59.64&lt;/td&gt; &#xA;   &lt;td&gt;67.49&lt;/td&gt; &#xA;   &lt;td&gt;82.00&lt;/td&gt; &#xA;   &lt;td&gt;75.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;3.09&lt;/td&gt; &#xA;   &lt;td&gt;7.53&lt;/td&gt; &#xA;   &lt;td&gt;83.03&lt;/td&gt; &#xA;   &lt;td&gt;85.52&lt;/td&gt; &#xA;   &lt;td&gt;58.19&lt;/td&gt; &#xA;   &lt;td&gt;67.04&lt;/td&gt; &#xA;   &lt;td&gt;80.43&lt;/td&gt; &#xA;   &lt;td&gt;74.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;4.29&lt;/td&gt; &#xA;   &lt;td&gt;8.91&lt;/td&gt; &#xA;   &lt;td&gt;82.04&lt;/td&gt; &#xA;   &lt;td&gt;83.29&lt;/td&gt; &#xA;   &lt;td&gt;54.78&lt;/td&gt; &#xA;   &lt;td&gt;64.99&lt;/td&gt; &#xA;   &lt;td&gt;78.14&lt;/td&gt; &#xA;   &lt;td&gt;72.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-405B-FLUTE&#34;&gt;LLaMA-3.1 (405B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Note that the weights are in the branch &lt;code&gt;nf_w4g64&lt;/code&gt; and thus &lt;code&gt;--revision nf_w4g64&lt;/code&gt; is needed since these are not on the default branch.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-8B-Instruct-FLUTE&#34;&gt;LLaMA-3.1 Instruct (8B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.78&lt;/td&gt; &#xA;   &lt;td&gt;11.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.73&lt;/td&gt; &#xA;   &lt;td&gt;12.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-70B-Instruct-FLUTE&#34;&gt;LLaMA-3.1 Instruct (70B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;4.15&lt;/td&gt; &#xA;   &lt;td&gt;9.18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;4.74&lt;/td&gt; &#xA;   &lt;td&gt;9.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3.1-405B-Instruct-FLUTE&#34;&gt;LLaMA-3.1 Instruct (405B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Note that the weights are in the branch &lt;code&gt;nf_w4g64&lt;/code&gt; and thus &lt;code&gt;--revision nf_w4g64&lt;/code&gt; is needed since these are not on the default branch.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3-8B-FLUTE&#34;&gt;LLaMA-3 (8B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;ARC-E&lt;/th&gt; &#xA;   &lt;th&gt;ARC-C&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Wino&lt;/th&gt; &#xA;   &lt;th&gt;Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unquantized&lt;/td&gt; &#xA;   &lt;td&gt;6.1&lt;/td&gt; &#xA;   &lt;td&gt;9.2&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;50.4&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;72.8&lt;/td&gt; &#xA;   &lt;td&gt;68.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.11&lt;/td&gt; &#xA;   &lt;td&gt;9.38&lt;/td&gt; &#xA;   &lt;td&gt;79.33&lt;/td&gt; &#xA;   &lt;td&gt;79.79&lt;/td&gt; &#xA;   &lt;td&gt;49.74&lt;/td&gt; &#xA;   &lt;td&gt;59.22&lt;/td&gt; &#xA;   &lt;td&gt;73.95&lt;/td&gt; &#xA;   &lt;td&gt;68.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.13&lt;/td&gt; &#xA;   &lt;td&gt;11.06&lt;/td&gt; &#xA;   &lt;td&gt;78.78&lt;/td&gt; &#xA;   &lt;td&gt;76.22&lt;/td&gt; &#xA;   &lt;td&gt;44.37&lt;/td&gt; &#xA;   &lt;td&gt;56.69&lt;/td&gt; &#xA;   &lt;td&gt;70.32&lt;/td&gt; &#xA;   &lt;td&gt;65.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3-70B-FLUTE&#34;&gt;LLaMA-3 (70B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;ARC-E&lt;/th&gt; &#xA;   &lt;th&gt;ARC-C&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Wino&lt;/th&gt; &#xA;   &lt;th&gt;Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unquantized&lt;/td&gt; &#xA;   &lt;td&gt;2.9&lt;/td&gt; &#xA;   &lt;td&gt;6.9&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;86.9&lt;/td&gt; &#xA;   &lt;td&gt;60.3&lt;/td&gt; &#xA;   &lt;td&gt;66.4&lt;/td&gt; &#xA;   &lt;td&gt;80.6&lt;/td&gt; &#xA;   &lt;td&gt;75.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;3.03&lt;/td&gt; &#xA;   &lt;td&gt;7.03&lt;/td&gt; &#xA;   &lt;td&gt;82.15&lt;/td&gt; &#xA;   &lt;td&gt;85.98&lt;/td&gt; &#xA;   &lt;td&gt;57.85&lt;/td&gt; &#xA;   &lt;td&gt;66.17&lt;/td&gt; &#xA;   &lt;td&gt;79.79&lt;/td&gt; &#xA;   &lt;td&gt;74.39&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;4.15&lt;/td&gt; &#xA;   &lt;td&gt;8.10&lt;/td&gt; &#xA;   &lt;td&gt;80.74&lt;/td&gt; &#xA;   &lt;td&gt;83.71&lt;/td&gt; &#xA;   &lt;td&gt;55.29&lt;/td&gt; &#xA;   &lt;td&gt;64.05&lt;/td&gt; &#xA;   &lt;td&gt;78.45&lt;/td&gt; &#xA;   &lt;td&gt;72.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3-8B-Instruct-FLUTE&#34;&gt;LLaMA-3 Instruct (8B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.78&lt;/td&gt; &#xA;   &lt;td&gt;10.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.75&lt;/td&gt; &#xA;   &lt;td&gt;12.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/Meta-Llama-3-70B-Instruct-FLUTE&#34;&gt;LLaMA-3 Instruct (70B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;3.67&lt;/td&gt; &#xA;   &lt;td&gt;7.95&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;4.90&lt;/td&gt; &#xA;   &lt;td&gt;10.86&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/gemma-2-9b-FLUTE&#34;&gt;Gemma-2 (9B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;ARC-E&lt;/th&gt; &#xA;   &lt;th&gt;ARC-C&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Wino&lt;/th&gt; &#xA;   &lt;th&gt;Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unquantized&lt;/td&gt; &#xA;   &lt;td&gt;6.88&lt;/td&gt; &#xA;   &lt;td&gt;10.12&lt;/td&gt; &#xA;   &lt;td&gt;81.39&lt;/td&gt; &#xA;   &lt;td&gt;87.37&lt;/td&gt; &#xA;   &lt;td&gt;61.35&lt;/td&gt; &#xA;   &lt;td&gt;61.23&lt;/td&gt; &#xA;   &lt;td&gt;74.27&lt;/td&gt; &#xA;   &lt;td&gt;73.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.49&lt;/td&gt; &#xA;   &lt;td&gt;10.35&lt;/td&gt; &#xA;   &lt;td&gt;81.28&lt;/td&gt; &#xA;   &lt;td&gt;86.24&lt;/td&gt; &#xA;   &lt;td&gt;59.30&lt;/td&gt; &#xA;   &lt;td&gt;60.40&lt;/td&gt; &#xA;   &lt;td&gt;75.30&lt;/td&gt; &#xA;   &lt;td&gt;72.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.06&lt;/td&gt; &#xA;   &lt;td&gt;11.14&lt;/td&gt; &#xA;   &lt;td&gt;80.52&lt;/td&gt; &#xA;   &lt;td&gt;83.16&lt;/td&gt; &#xA;   &lt;td&gt;55.46&lt;/td&gt; &#xA;   &lt;td&gt;58.28&lt;/td&gt; &#xA;   &lt;td&gt;72.69&lt;/td&gt; &#xA;   &lt;td&gt;70.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/gemma-2-27b-FLUTE&#34;&gt;Gemma-2 (27B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;ARC-E&lt;/th&gt; &#xA;   &lt;th&gt;ARC-C&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Wino&lt;/th&gt; &#xA;   &lt;th&gt;Avg.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unquantized&lt;/td&gt; &#xA;   &lt;td&gt;5.70&lt;/td&gt; &#xA;   &lt;td&gt;8.98&lt;/td&gt; &#xA;   &lt;td&gt;83.24&lt;/td&gt; &#xA;   &lt;td&gt;87.84&lt;/td&gt; &#xA;   &lt;td&gt;62.88&lt;/td&gt; &#xA;   &lt;td&gt;65.35&lt;/td&gt; &#xA;   &lt;td&gt;79.24&lt;/td&gt; &#xA;   &lt;td&gt;75.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;5.69&lt;/td&gt; &#xA;   &lt;td&gt;9.31&lt;/td&gt; &#xA;   &lt;td&gt;82.53&lt;/td&gt; &#xA;   &lt;td&gt;86.45&lt;/td&gt; &#xA;   &lt;td&gt;59.22&lt;/td&gt; &#xA;   &lt;td&gt;64.13&lt;/td&gt; &#xA;   &lt;td&gt;78.21&lt;/td&gt; &#xA;   &lt;td&gt;74.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/gemma-2-9b-it-FLUTE&#34;&gt;Gemma-2 Instruct (9B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;6.88&lt;/td&gt; &#xA;   &lt;td&gt;11.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W3G64&lt;/td&gt; &#xA;   &lt;td&gt;7.35&lt;/td&gt; &#xA;   &lt;td&gt;11.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://huggingface.co/radi-cho/gemma-2-27b-it-FLUTE&#34;&gt;Gemma-2 Instruct (27B)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Wiki&lt;/th&gt; &#xA;   &lt;th&gt;C4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NFL W4G64&lt;/td&gt; &#xA;   &lt;td&gt;5.91&lt;/td&gt; &#xA;   &lt;td&gt;9.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quantizing Your Own Models&lt;/h2&gt; &#xA;&lt;p&gt;We provide two APIs to quantize a custom models. The easist way is to use the command line interface.&lt;/p&gt; &#xA;&lt;h3&gt;Simple Normal Float Quantization&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m flute.integrations.base \&#xA;    --pretrained_model_name_or_path meta-llama/Meta-Llama-3-70B-Instruct \&#xA;    --save_directory Meta-Llama-3-70B-Instruct-NF4 \&#xA;    --num_bits 4 \&#xA;    --group_size 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The CLI essentially wraps around the following Python API,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import (&#xA;    LlamaForCausalLM,&#xA;    Gemma2ForCausalLM,&#xA;    AutoModelForCausalLM)&#xA;import flute.integrations.base&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    pretrained_model_name_or_path,&#xA;    device_map=&#34;cpu&#34;,&#xA;    torch_dtype=&#34;auto&#34;)&#xA;&#xA;if isinstance(model, (LlamaForCausalLM, Gemma2ForCausalLM)):&#xA;    flute.integrations.base.prepare_model_flute(&#xA;        name=&#34;model.model.layers&#34;,&#xA;        module=model.model.layers,&#xA;        num_bits=num_bits,&#xA;        group_size=group_size,&#xA;        fake=False)&#xA;else:&#xA;    # more models to come&#xA;    raise NotImplementedError&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Converting &lt;code&gt;bitsandbytes&lt;/code&gt; Model into FLUTE Model&lt;/h3&gt; &#xA;&lt;p&gt;While FLUTE has its own Normal Float (NF) implementation, we could convert an existing HuggingFace model quantized via &lt;code&gt;bitsandbytes&lt;/code&gt; into FLUTE format. To do so, just add two lines to the Python API,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;flute.integrations.base.prepare_model_flute(&#xA;    name=&#34;model.model.layers&#34;,&#xA;    module=model.model.layers,&#xA;    num_bits=num_bits,&#xA;    group_size=group_size,&#xA;    fake=False,&#xA;+   prepare_bnb_layers=True,&#xA;+   default_bnb_dtype=torch.float16,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s worth noting that we do not support double quantization, and the conversion will materialize the first-level scales.&lt;/p&gt; &#xA;&lt;h3&gt;Learned Normal Float Quantization (NFL)&lt;/h3&gt; &#xA;&lt;p&gt;NFL initialized the lookup table and the scales with those from NF quantization. Then, it uses calibration data to learn the scales via straight through estimation for for the gradient with respect to the scales.&lt;/p&gt; &#xA;&lt;p&gt;To use NFL quantization, call the following function before &lt;code&gt;prepare_model_flute&lt;/code&gt;. We also provide an &lt;a href=&#34;https://github.com/HanGuo97/flute/raw/main/examples/learnable_scales_eval.ipynb&#34;&gt;example jupyter notebook&lt;/a&gt; to illustrate the entire process.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import flute.integrations.learnable&#xA;&#xA;flute.integrations.learnable.learn_scales(&#xA;    model=model,&#xA;    tokenizer=tokenizer,&#xA;    num_bits=num_bits,&#xA;    group_size=group_size,&#xA;    custom_corpora=list_of_corpora,&#xA;    samples=num_samples,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Extending to New Models (Experimental)&lt;/h1&gt; &#xA;&lt;p&gt;At the moment, FLUTE kernel is specialized to the combination of GPU, matrix shapes, data types, bits, and group sizes. This means adding supporting new models requires tuning the kernel configurations for the corresponding use cases. We are hoping to add support for just-in-time tuning, but in the meantime, here are the ways to tune the kernel ahead-of-time.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Build the &lt;code&gt;raw&lt;/code&gt; version of the library that exposes all templates.&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Reset the previously tuned kernel,&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp flute/csrc/qgemm_kernel_generated.template.cu flute/csrc/qgemm_kernel_generated.cu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Un-comment the combination(s) to tune in &lt;code&gt;flute/csrc/qgemm_kernel_raw_generated.cu&lt;/code&gt;,&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;INSTANTIATE_TEMPLATE(NUM_SMs, DTYPE, cute::uint16_t, __half2, BITS, GROUP_SIZE);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Example for W4G64 on A100 &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;-// INSTANTIATE_TEMPLATE(108, cute::half_t    , cute::uint16_t, __half2       , 4, 64);&#xA;+INSTANTIATE_TEMPLATE(108, cute::half_t    , cute::uint16_t, __half2       , 4, 64);&#xA;&#xA;-// INSTANTIATE_TEMPLATE(108, cute::bfloat16_t, cute::uint16_t, __nv_bfloat162, 4, 64);&#xA;+INSTANTIATE_TEMPLATE(108, cute::bfloat16_t, cute::uint16_t, __nv_bfloat162, 4, 64);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Remove settings &lt;em&gt;not tuned&lt;/em&gt; in &lt;code&gt;flute/csrc/qgemm.cpp&lt;/code&gt;, &lt;code&gt;flute/__init__.py&lt;/code&gt;, and &lt;code&gt;flute/ops.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Although including other settings could still build, it could break the linking process and require re-compiling the library.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Example for W4G64 on A100 &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/flute/csrc/qgemm.cpp b/flute/csrc/qgemm.cpp&#xA;index 84bae95..c4a0236 100644&#xA;--- a/flute/csrc/qgemm.cpp&#xA;+++ b/flute/csrc/qgemm.cpp&#xA;@@ -314,3 +313,0 @@ qgemm_raw_simple(const at::Tensor&amp;amp; input,&#xA;-        case 32:                                      \&#xA;-            RUN_QGEMM_RAW(T, NUM_BITS, 32);           \&#xA;-            break;                                    \&#xA;@@ -320,6 +316,0 @@ qgemm_raw_simple(const at::Tensor&amp;amp; input,&#xA;-        case 128:                                     \&#xA;-            RUN_QGEMM_RAW(T, NUM_BITS, 128);          \&#xA;-            break;                                    \&#xA;-        case 256:                                     \&#xA;-            RUN_QGEMM_RAW(T, NUM_BITS, 256);          \&#xA;-            break;                                    \&#xA;@@ -335,6 +325,0 @@ qgemm_raw_simple(const at::Tensor&amp;amp; input,&#xA;-        case 2:                                          \&#xA;-            RUN_QGEMM_RAW_SWITCH_GROUP_SIZE(T, 2);       \&#xA;-            break;                                       \&#xA;-        case 3:                                          \&#xA;-            RUN_QGEMM_RAW_SWITCH_GROUP_SIZE(T, 3);       \&#xA;-            break;                                       \&#xA;@@ -381 +366 @@ TORCH_LIBRARY(flute, m) {&#xA;-    // m.def(&#34;qgemm_raw_simple_80(Tensor input, Tensor weight, Tensor(a!) output, Tensor scales, Tensor table, Tensor table2, Tensor(b!) workspace, int&#xA; num_bits, int group_size, int template_id) -&amp;gt; ()&#34;);&#xA;+    m.def(&#34;qgemm_raw_simple_80(Tensor input, Tensor weight, Tensor(a!) output, Tensor scales, Tensor table, Tensor table2, Tensor(b!) workspace, &#xA;int num_bits, int group_size, int template_id) -&amp;gt; ()&#34;);&#xA;@@ -391 +376 @@ TORCH_LIBRARY_IMPL(flute, CUDA, m) {&#xA;-    // m.impl(&#34;qgemm_raw_simple_80&#34;, &amp;amp;qgemm_raw_simple&amp;lt;cute::Int&amp;lt;108&amp;gt;&amp;gt;);&#xA;+    m.impl(&#34;qgemm_raw_simple_80&#34;, &amp;amp;qgemm_raw_simple&amp;lt;cute::Int&amp;lt;108&amp;gt;&amp;gt;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/flute/__init__.py b/flute/__init__.py&#xA;index 34b1a26..f524841 100644&#xA;--- a/flute/__init__.py&#xA;+++ b/flute/__init__.py&#xA;@@ -69 +69 @@ QGEMM_SIMPLE_DICT = {&#xA;-# QGEMM_RAW_SIMPLE_DICT = {&#xA;+QGEMM_RAW_SIMPLE_DICT = {&#xA;@@ -71 +71 @@ QGEMM_SIMPLE_DICT = {&#xA;-#     108: cast(QGEMM_RAW_SIMPLE_TYPE, torch.ops.flute.qgemm_raw_simple_80),&#xA;+    108: cast(QGEMM_RAW_SIMPLE_TYPE, torch.ops.flute.qgemm_raw_simple_80),&#xA;@@ -73 +73 @@ QGEMM_SIMPLE_DICT = {&#xA;-# }&#xA;+}&#xA;@@ -76 +76 @@ qgemm_simple     = QGEMM_SIMPLE_DICT[NUM_SMS]&#xA;-qgemm_raw_simple = None  # QGEMM_RAW_SIMPLE_DICT[NUM_SMS]&#xA;+qgemm_raw_simple = QGEMM_RAW_SIMPLE_DICT[NUM_SMS]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/flute/ops.py b/flute/ops.py&#xA;index 9fd91a2..80782ea 100644&#xA;--- a/flute/ops.py&#xA;+++ b/flute/ops.py&#xA;@@ -124 +124 @@ def _qgemm_simple_89_abstract(&#xA;-# @torch.library.register_fake(&#34;flute::qgemm_raw_simple_80&#34;)&#xA;+@torch.library.register_fake(&#34;flute::qgemm_raw_simple_80&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Build from source (see instructions below).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e . --no-build-isolation  # `--no-build-isolation` is optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Depending on the number of configurations to tune, this could take time in the order of tens of minutes to hours.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2: Tune FLUTE on the new matrix shapes.&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from flute.tune import TuneTask, tune_tasks_legacy&#xA;&#xA;tasks = [&#xA;    TuneTask(&#xA;        M=1,                  # batch size (x sequence length, usually 1 for token-by-token generation)&#xA;        N=1024,               # parameter dimension (note when using tensor-parallelism, this could change)&#xA;        K=4096,               # parameter dimension (note when using tensor-parallelism, this could change)&#xA;        num_bits=4,           # number of bits&#xA;        group_size=64,        # group size&#xA;        num_sms=108,          # number of streaming multiprocessors of the GPU&#xA;        dtype=torch.float16,  # data type&#xA;        device=torch.device(&#34;cuda:0&#34;)&#xA;    ),&#xA;]&#xA;&#xA;tune_tasks_legacy(tasks)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this step is complete, artifacts will be saved in &lt;code&gt;flute/data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Build the newly-tuned kernel&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# remove changes&#xA;git checkout -- flute/csrc/&#xA;&#xA;# generating new dispatching logic based on tuning artifacts&#xA;bash scripts/codegen_tuned.sh&#xA;&#xA;# remove changes&#xA;git checkout -- \&#xA;    flute/ops.py \&#xA;    flute/__init__.py&#xA;&#xA;# Build&#xA;pip install -e . --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if only one data type is tuned, you will also need to edit &lt;code&gt;flute/utils.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Example &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;diff --git a/flute/utils.py b/flute/utils.py&#xA;index 5add543..13f49c0 100644&#xA;--- a/flute/utils.py&#xA;+++ b/flute/utils.py&#xA;@@ -270,7 +270,7 @@ def pack(&#xA; &#xA;         K, N = W.shape&#xA;         template_ids = []&#xA;-        for dtype in [torch.float16, torch.bfloat16]:&#xA;+        for dtype in [torch.float16]:&#xA;             template_id = TEMPLATE_TUNED_WITHOUT_M_CONFIGS[(&#xA;                 NUM_SMS,&#xA;                 num_bits,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Finally, please follow the examples in &lt;code&gt;tests/&lt;/code&gt; to verify that the kernel is working correctly.&lt;/p&gt; &#xA;&lt;h1&gt;Build From Source&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the CUTLASS library.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Unfortunately, the path is hard-coded as of now. If you install CUTLASS&#xA;# in a different directory, please make sure the corresponding path in&#xA;# `setup.py` is updated.&#xA;cd /workspace&#xA;&#xA;git clone https://github.com/NVIDIA/cutlass.git&#xA;cd cutlass&#xA;git checkout v3.4.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Build.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/HanGuo97/flute&#xA;cd flute&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the build process requires having the local CUDA version (&lt;code&gt;nvcc --version&lt;/code&gt;) match PyTorch&#39;s CUDA. In situations in which the build process throws an error related to CUDA version mismatch, try adding &lt;code&gt;--no-build-isolation&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement and Citation&lt;/h1&gt; &#xA;&lt;p&gt;Special thanks to Dmytro Ivchenko, Yijie Bei, and the Fireworks AI team for helpful discussion. If you find any of the models or code in this repo useful, please feel free to cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{flute2024,&#xA;  title={Fast Matrix Multiplications for Lookup Table-Quantized LLMs},&#xA;  author={Guo, Han and Brandon, William and Cholakov, Radostin and Ragan-Kelley, Jonathan and Xing, Eric and Kim, Yoon},&#xA;  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},&#xA;  pages={12419--12433},&#xA;  year={2024}&#xA;}&#xA;&#xA;@article{higgs2024,&#xA;  title={Pushing the Limits of Large Language Model Quantization via the Linearity Theorem},&#xA;  author={Malinovskii, Vladimir and Panferov, Andrei and Ilin, Ivan and Guo, Han and Richt{\&#39;a}rik, Peter and Alistarh, Dan},&#xA;  journal={arXiv preprint arXiv:2411.17525},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>