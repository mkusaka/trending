<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-10T01:31:33Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/onnxruntime</title>
    <updated>2022-06-10T01:31:33Z</updated>
    <id>tag:github.com,2022-06-10:/microsoft/onnxruntime</id>
    <link href="https://github.com/microsoft/onnxruntime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/images/ONNX_Runtime_logo_dark.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime is a cross-platform inference and training machine-learning accelerator&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime inference&lt;/strong&gt; can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing&#34;&gt;Learn more →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime training&lt;/strong&gt; can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-training&#34;&gt;Learn more →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;General Information&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai&#34;&gt;onnxruntime.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage documention and tutorials&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai/docs&#34;&gt;onnxruntime.ai/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Companion sample repositories&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ONNX Runtime Inferencing: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-inference-examples&#34;&gt;microsoft/onnxruntime-inference-examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ONNX Runtime Training: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-training-examples&#34;&gt;microsoft/onnxruntime-training-examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build Pipeline Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;EPs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20x64%20NoContribops%20CI%20Pipeline?label=Linux+CPU+x64+No+Contrib+Ops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=78&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/centos7_cpu?label=Linux+CentOS7&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=140&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-distributed?label=Distributed+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20NUPHAR%20CI%20Pipeline?label=Linux+NUPHAR&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=55&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline?label=Linux+OpenVINO&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mac&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=65&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20NoContribops%20CI%20Pipeline?label=MacOS+NoContribops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Android&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;iOS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WebAssembly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20WebAssembly%20CI%20Pipeline?label=WASM&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data/Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Windows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/Privacy.md&#34;&gt;privacy statement&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and Feedback&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For feature requests or bug reports, please file a &lt;a href=&#34;https://github.com/Microsoft/onnxruntime/issues&#34;&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general discussion or questions, please use &lt;a href=&#34;https://github.com/microsoft/onnxruntime/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>onnx/onnx</title>
    <updated>2022-06-10T01:31:33Z</updated>
    <id>tag:github.com,2022-06-10:/onnx/onnx</id>
    <link href="https://github.com/onnx/onnx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open standard for machine learning interoperability&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;40%&#34; src=&#34;https://github.com/onnx/onnx/raw/main/docs/ONNX_logo_main.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=5&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnx-pipelines/onnx/_apis/build/status/Windows-CI?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=7&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnx-pipelines/onnx/_apis/build/status/Linux-CI?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=6&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnx-pipelines/onnx/_apis/build/status/MacOS-CI?branchName=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/3313&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/3313/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://onnx.ai&#34;&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt; is an open ecosystem that empowers AI developers to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types. Currently we focus on the capabilities needed for inferencing (scoring).&lt;/p&gt; &#xA;&lt;p&gt;ONNX is &lt;a href=&#34;http://onnx.ai/supported-tools&#34;&gt;widely supported&lt;/a&gt; and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.&lt;/p&gt; &#xA;&lt;h1&gt;Use ONNX&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/tutorials&#34;&gt;Tutorials for creating ONNX models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/models&#34;&gt;Pre-trained ONNX models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Learn about the ONNX spec&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/Overview.md&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/IR.md&#34;&gt;ONNX intermediate representation spec&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/Versioning.md&#34;&gt;Versioning principles of the spec&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/Operators.md&#34;&gt;Operators documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/PythonAPIOverview.md&#34;&gt;Python API Overview&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Programming utilities for working with ONNX Graphs&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/ShapeInference.md&#34;&gt;Shape and Type Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/optimizer&#34;&gt;Graph Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/VersionConverter.md&#34;&gt;Opset Version Conversion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribute&lt;/h1&gt; &#xA;&lt;p&gt;ONNX is a &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/community/readme.md&#34;&gt;community project&lt;/a&gt;. We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/community/sigs.md&#34;&gt;Special Interest Groups&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/community/working-groups.md&#34;&gt;Working Groups&lt;/a&gt; to shape the future of ONNX.&lt;/p&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;p&gt;If you think some operator should be added to ONNX specification, please read &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/AddNewOp.md&#34;&gt;this document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Discuss&lt;/h1&gt; &#xA;&lt;p&gt;We encourage you to open &lt;a href=&#34;https://github.com/onnx/onnx/issues&#34;&gt;Issues&lt;/a&gt;, or use &lt;a href=&#34;https://lfaifoundation.slack.com/&#34;&gt;Slack&lt;/a&gt; (If you have not joined yet, please use this &lt;a href=&#34;https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA&#34;&gt;link&lt;/a&gt; to join the group) for more real-time discussion.&lt;/p&gt; &#xA;&lt;h1&gt;Follow Us&lt;/h1&gt; &#xA;&lt;p&gt;Stay up to date with the latest ONNX news. [&lt;a href=&#34;https://www.facebook.com/onnxai/&#34;&gt;Facebook&lt;/a&gt;] [&lt;a href=&#34;https://twitter.com/onnxai&#34;&gt;Twitter&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Official Python packages&lt;/h2&gt; &#xA;&lt;p&gt;ONNX released packages are published in PyPi.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://test.pypi.org/project/onnx-weekly/&#34;&gt;Weekly packages&lt;/a&gt; are published in test pypi to enable experimentation and early testing.&lt;/p&gt; &#xA;&lt;h2&gt;Conda packages&lt;/h2&gt; &#xA;&lt;p&gt;A binary build of ONNX is available from &lt;a href=&#34;https://conda.io&#34;&gt;Conda&lt;/a&gt;, in &lt;a href=&#34;https://conda-forge.org/&#34;&gt;conda-forge&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c conda-forge onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the &lt;a href=&#34;https://hub.docker.com/r/onnx/onnx-dev&#34;&gt;onnx-dev docker image&lt;/a&gt; for a Linux-based installation without having to worry about dependency versioning.&lt;/p&gt; &#xA;&lt;h2&gt;Build ONNX from Source&lt;/h2&gt; &#xA;&lt;p&gt;Before building from source uninstall any existing versions of onnx &lt;code&gt;pip uninstall onnx&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Generally speaking, you need to install &lt;a href=&#34;https://github.com/protocolbuffers/protobuf&#34;&gt;protobuf C/C++ libraries and tools&lt;/a&gt; before proceeding forward. Then depending on how you installed protobuf, you need to set environment variable CMAKE_ARGS to &#34;-DONNX_USE_PROTOBUF_SHARED_LIBS=ON&#34; or &#34;-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF&#34;. For example, you may need to run the following command:&lt;/p&gt; &#xA;&lt;p&gt;Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_ARGS=&#34;-DONNX_USE_PROTOBUF_SHARED_LIBS=ON&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;set CMAKE_ARGS=&#34;-DONNX_USE_PROTOBUF_SHARED_LIBS=ON&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The ON/OFF depends on what kind of protobuf library you have. Shared libraries are files ending with *.dll/*.so/*.dylib. Static libraries are files ending with *.a/*.lib. This option depends on how you get your protobuf library and how it was built. And it is default OFF. You don&#39;t need to run the commands above if you&#39;d prefer to use a static protobuf library.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;If you are building ONNX from source, it is recommended that you also build Protobuf locally as a static library. The version distributed with conda-forge is a DLL, but ONNX expects it to be a static library. Building protobuf locally also lets you control the version of protobuf. The tested and recommended version is 3.16.0.&lt;/p&gt; &#xA;&lt;p&gt;The instructions in this README assume you are using Visual Studio. It is recommended that you run all the commands from a shell started from &#34;x64 Native Tools Command Prompt for VS 2019&#34; and keep the build system generator for cmake (e.g., cmake -G &#34;Visual Studio 16 2019&#34;) consistent while building protobuf as well as ONNX.&lt;/p&gt; &#xA;&lt;p&gt;You can get protobuf by running the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;git clone https://github.com/protocolbuffers/protobuf.git&#xA;cd protobuf&#xA;git checkout v3.16.0&#xA;cd cmake&#xA;cmake -G &#34;Visual Studio 16 2019&#34; -A x64 -DCMAKE_INSTALL_PREFIX=&amp;lt;protobuf_install_dir&amp;gt; -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_SHARED_LIBS=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF .&#xA;msbuild protobuf.sln /m /p:Configuration=Release&#xA;msbuild INSTALL.vcxproj /p:Configuration=Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then it will be built as a static library and installed to &amp;lt;protobuf_install_dir&amp;gt;. Please add the bin directory(which contains protoc.exe) to your PATH.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;set PATH=&amp;lt;protobuf_install_dir&amp;gt;/bin;%PATH%&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note: if your protobuf_install_dir contains spaces, &lt;strong&gt;do not&lt;/strong&gt; add quotation marks around it.&lt;/p&gt; &#xA;&lt;p&gt;Alternative: if you don&#39;t want to change your PATH, you can set ONNX_PROTOC_EXECUTABLE instead.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bat&#34;&gt;set CMAKE_ARGS=-DONNX_PROTOC_EXECUTABLE=&amp;lt;full_path_to_protoc.exe&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can build ONNX as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/onnx/onnx.git&#xA;cd onnx&#xA;git submodule update --init --recursive&#xA;# prefer lite proto&#xA;set CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;First, you need to install protobuf.&lt;/p&gt; &#xA;&lt;p&gt;Ubuntu users: the quickest way to install protobuf is to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get install python3-pip python3-dev libprotobuf-dev protobuf-compiler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can build ONNX as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export CMAKE_ARGS=&#34;-DONNX_USE_PROTOBUF_SHARED_LIBS=ON&#34;&#xA;git clone --recursive https://github.com/onnx/onnx.git&#xA;cd onnx&#xA;# prefer lite proto&#xA;set CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, you may need to install it from source. You can use the following commands to do it:&lt;/p&gt; &#xA;&lt;p&gt;Debian/Ubuntu:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/protocolbuffers/protobuf.git&#xA;cd protobuf&#xA;git checkout v3.16.0&#xA;git submodule update --init --recursive&#xA;mkdir build_source &amp;amp;&amp;amp; cd build_source&#xA;cmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release&#xA;make -j$(nproc)&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CentOS/RHEL/Fedora:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/protocolbuffers/protobuf.git&#xA;cd protobuf&#xA;git checkout v3.16.0&#xA;git submodule update --init --recursive&#xA;mkdir build_source &amp;amp;&amp;amp; cd build_source&#xA;cmake ../cmake  -DCMAKE_INSTALL_LIBDIR=lib64 -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release&#xA;make -j$(nproc)&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here &#34;-DCMAKE_POSITION_INDEPENDENT_CODE=ON&#34; is crucial. By default static libraries are built without &#34;-fPIC&#34; flag, they are not position independent code. But shared libraries must be position independent code. Python C/C++ extensions(like ONNX) are shared libraries. So if a static library was not built with &#34;-fPIC&#34;, it can&#39;t be linked to such a shared library.&lt;/p&gt; &#xA;&lt;p&gt;Once build is successful, update PATH to include protobuf paths.&lt;/p&gt; &#xA;&lt;p&gt;Then you can build ONNX as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/onnx/onnx.git&#xA;cd onnx&#xA;git submodule update --init --recursive&#xA;# prefer lite proto&#xA;set CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mac&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;export NUM_CORES=`sysctl -n hw.ncpu`&#xA;brew update&#xA;brew install autoconf &amp;amp;&amp;amp; brew install automake&#xA;wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protobuf-cpp-3.16.0.tar.gz&#xA;tar -xvf protobuf-cpp-3.16.0.tar.gz&#xA;cd protobuf-3.16.0&#xA;mkdir build_source &amp;amp;&amp;amp; cd build_source&#xA;cmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release&#xA;make -j${NUM_CORES}&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once build is successful, update PATH to include protobuf paths.&lt;/p&gt; &#xA;&lt;p&gt;Then you can build ONNX as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/onnx/onnx.git&#xA;cd onnx&#xA;# prefer lite proto&#xA;set CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Verify Installation&lt;/h2&gt; &#xA;&lt;p&gt;After installation, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -c &#34;import onnx&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to verify it works.&lt;/p&gt; &#xA;&lt;h2&gt;Common Build Options&lt;/h2&gt; &#xA;&lt;p&gt;For full list refer to CMakeLists.txt &lt;strong&gt;Environment variables&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;USE_MSVC_STATIC_RUNTIME&lt;/code&gt; should be 1 or 0, not ON or OFF. When set to 1 onnx links statically to runtime library. &lt;strong&gt;Default&lt;/strong&gt;: USE_MSVC_STATIC_RUNTIME=0&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;DEBUG&lt;/code&gt; should be 0 or 1. When set to 1 onnx is built in debug mode. or debug versions of the dependencies, you need to open the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/CMakeLists.txt&#34;&gt;CMakeLists file&lt;/a&gt; and append a letter &lt;code&gt;d&lt;/code&gt; at the end of the package name lines. For example, &lt;code&gt;NAMES protobuf-lite&lt;/code&gt; would become &lt;code&gt;NAMES protobuf-lited&lt;/code&gt;. &lt;strong&gt;Default&lt;/strong&gt;: Debug=0&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;CMake variables&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ONNX_USE_PROTOBUF_SHARED_LIBS&lt;/code&gt; should be ON or OFF. &lt;strong&gt;Default&lt;/strong&gt;: ONNX_USE_PROTOBUF_SHARED_LIBS=OFF USE_MSVC_STATIC_RUNTIME=0 &lt;code&gt;ONNX_USE_PROTOBUF_SHARED_LIBS&lt;/code&gt; determines how onnx links to protobuf libraries.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;When set to ON - onnx will dynamically link to protobuf shared libs, PROTOBUF_USE_DLLS will be defined as described &lt;a href=&#34;https://github.com/protocolbuffers/protobuf/raw/master/cmake/README.md#dlls-vs-static-linking&#34;&gt;here&lt;/a&gt;, Protobuf_USE_STATIC_LIBS will be set to OFF and &lt;code&gt;USE_MSVC_STATIC_RUNTIME&lt;/code&gt; must be 0.&lt;/li&gt; &#xA;   &lt;li&gt;When set to OFF - onnx will link statically to protobuf, and Protobuf_USE_STATIC_LIBS will be set to ON (to force the use of the static libraries) and &lt;code&gt;USE_MSVC_STATIC_RUNTIME&lt;/code&gt; can be 0 or 1.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ONNX_USE_LITE_PROTO&lt;/code&gt; should be ON or OFF. When set to ON onnx uses lite protobuf instead of full protobuf. &lt;strong&gt;Default&lt;/strong&gt;: ONNX_USE_LITE_PROTO=OFF&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ONNX_WERROR&lt;/code&gt; should be ON or OFF. When set to ON warnings are treated as errors. &lt;strong&gt;Default&lt;/strong&gt;: ONNX_WERROR=OFF in local builds, ON in CI and release pipelines.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Common Errors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Note: the &lt;code&gt;import onnx&lt;/code&gt; command does not work from the source checkout directory; in this case you&#39;ll see &lt;code&gt;ModuleNotFoundError: No module named &#39;onnx.onnx_cpp2py_export&#39;&lt;/code&gt;. Change into another directory to fix this error.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Building ONNX on Ubuntu works well, but on CentOS/RHEL and other ManyLinux systems, you might need to open the [CMakeLists file][CMakeLists] and replace all instances of &lt;code&gt;/lib&lt;/code&gt; with &lt;code&gt;/lib64&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Testing&lt;/h1&gt; &#xA;&lt;p&gt;ONNX uses &lt;a href=&#34;https://docs.pytest.org&#34;&gt;pytest&lt;/a&gt; as test driver. In order to run tests, you will first need to install pytest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pytest nbval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installing pytest, use the following command to run tests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/docs/CONTRIBUTING.md&#34;&gt;contributor guide&lt;/a&gt; for instructions.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/onnx/main/LICENSE&#34;&gt;Apache License v2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://onnx.ai/codeofconduct.html&#34;&gt;ONNX Open Source Code of Conduct&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ZLMediaKit/ZLMediaKit</title>
    <updated>2022-06-10T01:31:33Z</updated>
    <id>tag:github.com,2022-06-10:/ZLMediaKit/ZLMediaKit</id>
    <link href="https://github.com/ZLMediaKit/ZLMediaKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WebRTC/RTSP/RTMP/HTTP/HLS/HTTP-FLV/WebSocket-FLV/HTTP-TS/HTTP-fMP4/WebSocket-TS/WebSocket-fMP4/GB28181 server and client framework based on C++11&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xia-chu/ZLMediaKit/master/www/logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;一个基于C++11的高性能运营级流媒体服务框架&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/license-MIT-green.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://en.cppreference.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-c++-red.svg?sanitize=true&#34; alt=&#34;C++&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-blue.svg?sanitize=true&#34; alt=&#34;platform&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-yellow.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.org/xia-chu/ZLMediaKit&#34;&gt;&lt;img src=&#34;https://travis-ci.org/xia-chu/ZLMediaKit.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目特点&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基于C++11开发，避免使用裸指针，代码稳定可靠，性能优越。&lt;/li&gt; &#xA; &lt;li&gt;支持多种协议(RTSP/RTMP/HLS/HTTP-FLV/WebSocket-FLV/GB28181/HTTP-TS/WebSocket-TS/HTTP-fMP4/WebSocket-fMP4/MP4/WebRTC),支持协议互转。&lt;/li&gt; &#xA; &lt;li&gt;使用多路复用/多线程/异步网络IO模式开发，并发性能优越，支持海量客户端连接。&lt;/li&gt; &#xA; &lt;li&gt;代码经过长期大量的稳定性、性能测试，已经在线上商用验证已久。&lt;/li&gt; &#xA; &lt;li&gt;支持linux、macos、ios、android、windows全平台。&lt;/li&gt; &#xA; &lt;li&gt;支持画面秒开、极低延时(&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/%E5%BB%B6%E6%97%B6%E6%B5%8B%E8%AF%95&#34;&gt;500毫秒内，最低可达100毫秒&lt;/a&gt;)。&lt;/li&gt; &#xA; &lt;li&gt;提供完善的标准&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/api/include&#34;&gt;C API&lt;/a&gt;,可以作SDK用，或供其他语言调用。&lt;/li&gt; &#xA; &lt;li&gt;提供完整的&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/server&#34;&gt;MediaServer&lt;/a&gt;服务器，可以免开发直接部署为商用服务器。&lt;/li&gt; &#xA; &lt;li&gt;提供完善的&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-API&#34;&gt;restful api&lt;/a&gt;以及&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-HOOK-API&#34;&gt;web hook&lt;/a&gt;，支持丰富的业务逻辑。&lt;/li&gt; &#xA; &lt;li&gt;打通了视频监控协议栈与直播协议栈，对RTSP/RTMP支持都很完善。&lt;/li&gt; &#xA; &lt;li&gt;全面支持H265/H264/AAC/G711/OPUS。&lt;/li&gt; &#xA; &lt;li&gt;功能完善，支持集群、按需转协议、按需推拉流、先播后推、断连续推等功能。&lt;/li&gt; &#xA; &lt;li&gt;极致性能，单机10W级别播放器，100Gb/s级别io带宽能力。&lt;/li&gt; &#xA; &lt;li&gt;极致体验，&lt;a href=&#34;https://github.com/ZLMediaKit/ZLMediaKit/wiki/ZLMediakit%E7%8B%AC%E5%AE%B6%E7%89%B9%E6%80%A7%E4%BB%8B%E7%BB%8D&#34;&gt;独家特性&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZLMediaKit/ZLMediaKit/issues/511&#34;&gt;谁在使用zlmediakit?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;全面支持ipv6网络&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;项目定位&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;移动嵌入式跨平台流媒体解决方案。&lt;/li&gt; &#xA; &lt;li&gt;商用级流媒体服务器。&lt;/li&gt; &#xA; &lt;li&gt;网络编程二次开发SDK。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;功能清单&lt;/h2&gt; &#xA;&lt;h3&gt;功能一览&lt;/h3&gt; &#xA;&lt;img width=&#34;800&#34; alt=&#34;功能一览&#34; src=&#34;https://user-images.githubusercontent.com/11495632/114176523-d50fce80-996d-11eb-81f8-0a2e2715ba7b.png&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;RTSP[S]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RTSP[S] 服务器，支持RTMP/MP4/HLS转RTSP[S],支持亚马逊echo show这样的设备&lt;/li&gt; &#xA;   &lt;li&gt;RTSP[S] 播放器，支持RTSP代理，支持生成静音音频&lt;/li&gt; &#xA;   &lt;li&gt;RTSP[S] 推流客户端与服务器&lt;/li&gt; &#xA;   &lt;li&gt;支持 &lt;code&gt;rtp over udp&lt;/code&gt; &lt;code&gt;rtp over tcp&lt;/code&gt; &lt;code&gt;rtp over http&lt;/code&gt; &lt;code&gt;rtp组播&lt;/code&gt; 四种RTP传输方式&lt;/li&gt; &#xA;   &lt;li&gt;服务器/客户端完整支持Basic/Digest方式的登录鉴权，全异步可配置化的鉴权接口&lt;/li&gt; &#xA;   &lt;li&gt;支持H265编码&lt;/li&gt; &#xA;   &lt;li&gt;服务器支持RTSP推流(包括&lt;code&gt;rtp over udp&lt;/code&gt; &lt;code&gt;rtp over tcp&lt;/code&gt;方式)&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码，其他编码能转发但不能转协议&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RTMP[S]&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RTMP[S] 播放服务器，支持RTSP/MP4/HLS转RTMP&lt;/li&gt; &#xA;   &lt;li&gt;RTMP[S] 发布服务器，支持录制发布流&lt;/li&gt; &#xA;   &lt;li&gt;RTMP[S] 播放器，支持RTMP代理，支持生成静音音频&lt;/li&gt; &#xA;   &lt;li&gt;RTMP[S] 推流客户端&lt;/li&gt; &#xA;   &lt;li&gt;支持http[s]-flv直播&lt;/li&gt; &#xA;   &lt;li&gt;支持websocket-flv直播&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码，其他编码能转发但不能转协议&lt;/li&gt; &#xA;   &lt;li&gt;支持&lt;a href=&#34;https://github.com/ksvc/FFmpeg/wiki&#34;&gt;RTMP-H265&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;支持&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/RTMP%E5%AF%B9H265%E5%92%8COPUS%E7%9A%84%E6%94%AF%E6%8C%81&#34;&gt;RTMP-OPUS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HLS&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持HLS文件生成，自带HTTP文件服务器&lt;/li&gt; &#xA;   &lt;li&gt;通过cookie追踪技术，可以模拟HLS播放为长连接，可以实现HLS按需拉流、播放统计等业务&lt;/li&gt; &#xA;   &lt;li&gt;支持HLS播发器，支持拉流HLS转rtsp/rtmp/mp4&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;TS&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持http[s]-ts直播&lt;/li&gt; &#xA;   &lt;li&gt;支持ws[s]-ts直播&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;fMP4&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持http[s]-fmp4直播&lt;/li&gt; &#xA;   &lt;li&gt;支持ws[s]-fmp4直播&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;HTTP[S]与WebSocket&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;服务器支持&lt;code&gt;目录索引生成&lt;/code&gt;,&lt;code&gt;文件下载&lt;/code&gt;,&lt;code&gt;表单提交请求&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;客户端提供&lt;code&gt;文件下载器(支持断点续传)&lt;/code&gt;,&lt;code&gt;接口请求器&lt;/code&gt;,&lt;code&gt;文件上传器&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;完整HTTP API服务器，可以作为web后台开发框架&lt;/li&gt; &#xA;   &lt;li&gt;支持跨域访问&lt;/li&gt; &#xA;   &lt;li&gt;支持http客户端、服务器cookie&lt;/li&gt; &#xA;   &lt;li&gt;支持WebSocket服务器和客户端&lt;/li&gt; &#xA;   &lt;li&gt;支持http文件访问鉴权&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GB28181与RTP推流&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持UDP/TCP国标RTP(PS或TS)推流服务器，可以转换成RTSP/RTMP/HLS等协议&lt;/li&gt; &#xA;   &lt;li&gt;支持RTSP/RTMP/HLS转国标推流客户端，支持TCP/UDP模式，提供相应restful api&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码&lt;/li&gt; &#xA;   &lt;li&gt;支持海康ehome推流&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;MP4点播与录制&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持录制为FLV/HLS/MP4&lt;/li&gt; &#xA;   &lt;li&gt;RTSP/RTMP/HTTP-FLV/WS-FLV支持MP4文件点播，支持seek&lt;/li&gt; &#xA;   &lt;li&gt;支持H264/H265/AAC/G711/OPUS编码&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;WebRTC&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持WebRTC推流，支持转其他协议&lt;/li&gt; &#xA;   &lt;li&gt;支持WebRTC播放，支持其他协议转WebRTC&lt;/li&gt; &#xA;   &lt;li&gt;支持双向echo test&lt;/li&gt; &#xA;   &lt;li&gt;支持simulcast推流&lt;/li&gt; &#xA;   &lt;li&gt;支持上下行rtx/nack丢包重传&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;支持单端口、多线程、客户端网络连接迁移(开源界唯一)&lt;/strong&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;支持TWCC rtcp动态调整码率&lt;/li&gt; &#xA;   &lt;li&gt;支持remb/pli/sr/rr rtcp&lt;/li&gt; &#xA;   &lt;li&gt;支持rtp扩展解析&lt;/li&gt; &#xA;   &lt;li&gt;支持GOP缓冲，webrtc播放秒开&lt;/li&gt; &#xA;   &lt;li&gt;支持datachannel&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;其他&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持丰富的restful api以及web hook事件&lt;/li&gt; &#xA;   &lt;li&gt;支持简单的telnet调试&lt;/li&gt; &#xA;   &lt;li&gt;支持配置文件热加载&lt;/li&gt; &#xA;   &lt;li&gt;支持流量统计、推拉流鉴权等事件&lt;/li&gt; &#xA;   &lt;li&gt;支持虚拟主机,可以隔离不同域名&lt;/li&gt; &#xA;   &lt;li&gt;支持按需拉流，无人观看自动关断拉流&lt;/li&gt; &#xA;   &lt;li&gt;支持先播放后推流，提高及时推流画面打开率&lt;/li&gt; &#xA;   &lt;li&gt;提供c api sdk&lt;/li&gt; &#xA;   &lt;li&gt;支持FFmpeg拉流代理任意格式的流&lt;/li&gt; &#xA;   &lt;li&gt;支持http api生成并返回实时截图&lt;/li&gt; &#xA;   &lt;li&gt;支持按需解复用、转协议，当有人观看时才开启转协议，降低cpu占用率&lt;/li&gt; &#xA;   &lt;li&gt;支持溯源模式的集群部署，溯源方式支持rtsp/rtmp/hls/http-ts, 边沿站支持hls, 源站支持多个(采用round robin方式溯源)&lt;/li&gt; &#xA;   &lt;li&gt;rtsp/rtmp/webrtc推流异常断开后，可以在超时时间内重连推流，播放器无感知&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;编译以及测试&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;编译前务必仔细参考wiki:&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B&#34;&gt;快速开始&lt;/a&gt;操作!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;怎么使用&lt;/h2&gt; &#xA;&lt;p&gt;你有三种方法使用ZLMediaKit，分别是：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1、使用c api，作为sdk使用，请参考&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/api/include&#34;&gt;这里&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;2、作为独立的流媒体服务器使用，不想做c/c++开发的，可以参考 &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-API&#34;&gt;restful api&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/MediaServer%E6%94%AF%E6%8C%81%E7%9A%84HTTP-HOOK-API&#34;&gt;web hook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;3、如果想做c/c++开发，添加业务逻辑增加功能，可以参考这里的&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/tree/master/tests&#34;&gt;测试程序&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker 镜像&lt;/h2&gt; &#xA;&lt;p&gt;你可以从Docker Hub下载已经编译好的镜像并启动它：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#此镜像为zlmediakit开发团队提供，推荐&#xA;docker run -id -p 1935:1935 -p 8080:80 -p 8554:554 -p 10000:10000 -p 10000:10000/udp -p 8000:8000/udp zlmediakit/zlmediakit:Release.last&#xA;&#xA;#此镜像委托第三方提供&#xA;docker run -id -p 1935:1935 -p 8080:80 -p 8554:554 -p 10000:10000 -p 10000:10000/udp panjjo/zlmediakit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;你也可以根据Dockerfile编译镜像：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash build_docker_images.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;合作项目&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;可视化管理网站&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/langmansh/AKStreamNVR&#34;&gt;最新的前后端分离web项目,支持webrtc播放&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://gitee.com/kkkkk5G/MediaServerUI&#34;&gt;基于ZLMediaKit主线的管理WEB网站&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/chenxiaolei/ZLMediaKit_NVR_UI&#34;&gt;基于ZLMediaKit分支的管理WEB网站&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/MingZhuLiu/ZLMediaServerManagent&#34;&gt;一个非常漂亮的可视化后台管理系统&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;流媒体管理平台&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/648540858/wvp-GB28181-pro&#34;&gt;GB28181完整解决方案,自带web管理网站,支持webrtc、h265播放&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/chatop2020/AKStream&#34;&gt;功能强大的流媒体控制管理接口平台,支持GB28181&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/panjjo/gosip&#34;&gt;Go实现的GB28181服务器&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://gitee.com/hfwudao/GB28181_Node_Http&#34;&gt;node-js版本的GB28181平台&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tsingeye/FreeEhome&#34;&gt;Go实现的海康ehome服务器&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;客户端&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/hctym1995/ZLM_ApiDemo&#34;&gt;基于C SDK实现的推流客户端&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/chengxiaosheng/ZLMediaKit.HttpApi&#34;&gt;C#版本的Http API与Hook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/MingZhuLiu/ZLMediaKit.DotNetCore.Sdk&#34;&gt;DotNetCore的RESTful客户端&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;播放器&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/numberwolf/h265web.js&#34;&gt;基于wasm支持H265的播放器&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/v354412101/wsPlayer&#34;&gt;基于MSE的websocket-fmp4播放器&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/metartc/metaRTC&#34;&gt;全国产webrtc sdk(metaRTC)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;授权协议&lt;/h2&gt; &#xA;&lt;p&gt;本项目自有代码使用宽松的MIT协议，在保留版权信息的情况下可以自由应用于各自商用、非商业的项目。 但是本项目也零碎的使用了一些其他的开源代码，在商用的情况下请自行替代或剔除； 由于使用本项目而产生的商业纠纷或侵权行为一概与本项目及开发者无关，请自行承担法律风险。 在使用本项目代码时，也应该在授权协议中同时表明本项目依赖的第三方库的协议。&lt;/p&gt; &#xA;&lt;h2&gt;联系方式&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;邮箱：&lt;a href=&#34;mailto:1213642868@qq.com&#34;&gt;1213642868@qq.com&lt;/a&gt;(本项目相关或流媒体相关问题请走issue流程，否则恕不邮件答复)&lt;/li&gt; &#xA; &lt;li&gt;QQ群：qq群号在wiki中，请阅读wiki后再加群&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;怎么提问？&lt;/h2&gt; &#xA;&lt;p&gt;如果要对项目有相关疑问，建议您这么做：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1、仔细看下readme、wiki，如果有必要可以查看下issue.&lt;/li&gt; &#xA; &lt;li&gt;2、如果您的问题还没解决，可以提issue.&lt;/li&gt; &#xA; &lt;li&gt;3、有些问题，如果不具备参考性的，无需在issue提的，可以在qq群提.&lt;/li&gt; &#xA; &lt;li&gt;4、QQ私聊一般不接受无偿技术咨询和支持(&lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/wiki/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%BB%BA%E8%AE%AEQQ%E7%A7%81%E8%81%8A%E5%92%A8%E8%AF%A2%E9%97%AE%E9%A2%98%EF%BC%9F&#34;&gt;为什么不提倡QQ私聊&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;特别感谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目采用了&lt;a href=&#34;https://github.com/ireader&#34;&gt;老陈&lt;/a&gt; 的 &lt;a href=&#34;https://github.com/ireader/media-server&#34;&gt;media-server&lt;/a&gt; 库， 本项目的 ts/fmp4/mp4/ps 容器格式的复用解复用都依赖media-server库。在实现本项目诸多功能时，老陈多次给予了无私热情关键的帮助， 特此对他表示诚挚的感谢！&lt;/p&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;感谢以下各位对本项目包括但不限于代码贡献、问题反馈、资金捐赠等各种方式的支持！以下排名不分先后：&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ireader&#34;&gt;老陈&lt;/a&gt; &lt;a href=&#34;https://github.com/gemfield&#34;&gt;Gemfield&lt;/a&gt; &lt;a href=&#34;https://github.com/nanguantong2&#34;&gt;南冠彤&lt;/a&gt; &lt;a href=&#34;https://github.com/tsingeye&#34;&gt;凹凸慢&lt;/a&gt; &lt;a href=&#34;https://github.com/chenxiaolei&#34;&gt;chenxiaolei&lt;/a&gt; &lt;a href=&#34;https://github.com/zqsong&#34;&gt;史前小虫&lt;/a&gt; &lt;a href=&#34;https://github.com/baiyfcu&#34;&gt;清涩绿茶&lt;/a&gt; &lt;a href=&#34;https://github.com/3503207480&#34;&gt;3503207480&lt;/a&gt; &lt;a href=&#34;https://github.com/DroidChow&#34;&gt;DroidChow&lt;/a&gt; &lt;a href=&#34;https://github.com/HuoQiShuai&#34;&gt;阿塞&lt;/a&gt; &lt;a href=&#34;https://github.com/ChinaCCF&#34;&gt;火宣&lt;/a&gt; &lt;a href=&#34;https://github.com/JerryLinGd&#34;&gt;γ瑞γミ&lt;/a&gt; &lt;a href=&#34;https://www.linkingvision.com/&#34;&gt;linkingvision&lt;/a&gt; &lt;a href=&#34;https://github.com/taotaobujue2008&#34;&gt;茄子&lt;/a&gt; &lt;a href=&#34;mailto:409257224@qq.com&#34;&gt;好心情&lt;/a&gt; &lt;a href=&#34;https://github.com/MingZhuLiu&#34;&gt;浮沉&lt;/a&gt; &lt;a href=&#34;https://github.com/wasphin&#34;&gt;Xiaofeng Wang&lt;/a&gt; &lt;a href=&#34;https://github.com/doodoocoder&#34;&gt;doodoocoder&lt;/a&gt; &lt;a href=&#34;https://github.com/Colibrow&#34;&gt;qingci&lt;/a&gt; &lt;a href=&#34;https://github.com/swwheihei&#34;&gt;swwheihei&lt;/a&gt; &lt;a href=&#34;https://gitee.com/kkkkk5G&#34;&gt;KKKKK5G&lt;/a&gt; &lt;a href=&#34;mailto:zhouweimin@supremind.com&#34;&gt;Zhou Weimin&lt;/a&gt; &lt;a href=&#34;https://github.com/jim-king-2000&#34;&gt;Jim Jin&lt;/a&gt; &lt;a href=&#34;mailto:392293307@qq.com&#34;&gt;西瓜丶&lt;/a&gt; &lt;a href=&#34;https://github.com/MingZhuLiu&#34;&gt;MingZhuLiu&lt;/a&gt; &lt;a href=&#34;https://github.com/chengxiaosheng&#34;&gt;chengxiaosheng&lt;/a&gt; &lt;a href=&#34;mailto:2381267071@qq.com&#34;&gt;big panda&lt;/a&gt; &lt;a href=&#34;https://github.com/tanningzhong&#34;&gt;tanningzhong&lt;/a&gt; &lt;a href=&#34;https://github.com/hctym1995&#34;&gt;hctym1995&lt;/a&gt; &lt;a href=&#34;https://gitee.com/kingyuanyuan&#34;&gt;hewenyuan&lt;/a&gt; &lt;a href=&#34;mailto:sunhui200475@163.com&#34;&gt;sunhui&lt;/a&gt; &lt;a href=&#34;mailto:fangpengcheng@bilibili.com&#34;&gt;mirs&lt;/a&gt; &lt;a href=&#34;mailto:kevin__cheng@outlook.com&#34;&gt;Kevin Cheng&lt;/a&gt; &lt;a href=&#34;mailto:root@oopy.org&#34;&gt;Liu Jiang&lt;/a&gt; &lt;a href=&#34;https://github.com/alongl&#34;&gt;along&lt;/a&gt; &lt;a href=&#34;mailto:xpy66swsry@gmail.com&#34;&gt;qingci&lt;/a&gt; &lt;a href=&#34;mailto:zh.ghlong@qq.com&#34;&gt;lyg1949&lt;/a&gt; &lt;a href=&#34;mailto:zh.ghlong@qq.com&#34;&gt;zhlong&lt;/a&gt; &lt;a href=&#34;mailto:3503207480@qq.com&#34;&gt;大裤衩&lt;/a&gt; &lt;a href=&#34;mailto:droid.chow@gmail.com&#34;&gt;droid.chow&lt;/a&gt; &lt;a href=&#34;https://github.com/musicwood&#34;&gt;陈晓林&lt;/a&gt; &lt;a href=&#34;https://github.com/CharleyWangHZ&#34;&gt;CharleyWangHZ&lt;/a&gt; &lt;a href=&#34;https://github.com/johzzy&#34;&gt;Johnny&lt;/a&gt; &lt;a href=&#34;https://github.com/DoubleX69&#34;&gt;DoubleX69&lt;/a&gt; &lt;a href=&#34;https://github.com/lawrencehj&#34;&gt;lawrencehj&lt;/a&gt; &lt;a href=&#34;mailto:xyyangkun@163.com&#34;&gt;yangkun&lt;/a&gt; &lt;a href=&#34;mailto:holychaossword@hotmail.com&#34;&gt;Xinghua Zhao&lt;/a&gt; &lt;a href=&#34;https://github.com/brokensword2018&#34;&gt;hejilin&lt;/a&gt; &lt;a href=&#34;https://github.com/rqb500&#34;&gt;rqb500&lt;/a&gt; &lt;a href=&#34;https://github.com/alexliyu7352&#34;&gt;Alex&lt;/a&gt; &lt;a href=&#34;https://github.com/Dw9&#34;&gt;Dw9&lt;/a&gt; &lt;a href=&#34;mailto:mingyuejingque@gmail.com&#34;&gt;明月惊鹊&lt;/a&gt; &lt;a href=&#34;mailto:2958580318@qq.com&#34;&gt;cgm&lt;/a&gt; &lt;a href=&#34;mailto:1724010622@qq.com&#34;&gt;hejilin&lt;/a&gt; &lt;a href=&#34;mailto:liyu7352@gmail.com&#34;&gt;alexliyu7352&lt;/a&gt; &lt;a href=&#34;mailto:2958580318@qq.com&#34;&gt;cgm&lt;/a&gt; &lt;a href=&#34;https://github.com/HaoruiWang&#34;&gt;haorui wang&lt;/a&gt; &lt;a href=&#34;mailto:joshuafc@foxmail.com&#34;&gt;joshuafc&lt;/a&gt; &lt;a href=&#34;https://github.com/JayChen0519&#34;&gt;JayChen0519&lt;/a&gt; &lt;a href=&#34;mailto:zuoxue@qq.com&#34;&gt;zx&lt;/a&gt; &lt;a href=&#34;mailto:wangcker@163.com&#34;&gt;wangcker&lt;/a&gt; &lt;a href=&#34;mailto:wp@zafu.edu.cn&#34;&gt;WuPeng&lt;/a&gt; &lt;a href=&#34;https://github.com/starry&#34;&gt;starry&lt;/a&gt; &lt;a href=&#34;https://github.com/mtdxc&#34;&gt;mtdxc&lt;/a&gt; &lt;a href=&#34;https://github.com/hugangfeng333&#34;&gt;胡刚风&lt;/a&gt; &lt;a href=&#34;https://github.com/zhao85&#34;&gt;zhao85&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;使用案例&lt;/h2&gt; &#xA;&lt;p&gt;本项目已经得到不少公司和个人开发者的认可，据作者不完全统计， 使用本项目的公司包括知名的互联网巨头、国内排名前列的云服务公司、多家知名的AI独角兽公司， 以及一系列中小型公司。使用者可以通过在 &lt;a href=&#34;https://github.com/xia-chu/ZLMediaKit/issues/511&#34;&gt;issue&lt;/a&gt; 上粘贴公司的大名和相关项目介绍为本项目背书，感谢支持！&lt;/p&gt;</summary>
  </entry>
</feed>