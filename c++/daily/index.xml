<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-13T01:29:27Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qiayuanl/legged_control</title>
    <updated>2024-07-13T01:29:27Z</updated>
    <id>tag:github.com,2024-07-13:/qiayuanl/legged_control</id>
    <link href="https://github.com/qiayuanl/legged_control" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Nonlinear MPC and WBC framework for legged robot based on OCS2 and ros-controls&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;legged_control&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;legged_control is an NMPC-WBC legged robot control stack and framework based on &lt;a href=&#34;https://github.com/leggedrobotics/ocs2&#34;&gt;OCS2&lt;/a&gt; and &lt;a href=&#34;http://wiki.ros.org/ros_control&#34;&gt;ros-control&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The advantage shows below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To the author&#39;s best knowledge, this framework is probably the best-performing open-source legged robot MPC control framework;&lt;/li&gt; &#xA; &lt;li&gt;You can deploy this framework in your A1 robot within a few hours;&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the ros-control interface, you can easily use this framework for your custom robot.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;I believe this framework can provide a high-performance and easy-to-use model-based baseline for the legged robot community.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/21256355/192135828-8fa7d9bb-9b4d-41f9-907a-68d34e6809d8.mp4&#34;&gt;https://user-images.githubusercontent.com/21256355/192135828-8fa7d9bb-9b4d-41f9-907a-68d34e6809d8.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Source code&lt;/h3&gt; &#xA;&lt;p&gt;The source code is hosted on GitHub: &lt;a href=&#34;https://github.com/qiayuanliao/legged_control&#34;&gt;qiayuanliao/legged_control&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone legged_control&#xA;git clone git@github.com:qiayuanliao/legged_control.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OCS2&lt;/h3&gt; &#xA;&lt;p&gt;OCS2 is a huge monorepo; &lt;strong&gt;DO NOT&lt;/strong&gt; try to compile the whole repo. You only need to compile &lt;code&gt;ocs2_legged_robot_ros&lt;/code&gt; and its dependencies following the step below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You are supposed to clone the OCS2, pinocchio, and hpp-fcl as described in the documentation of OCS2. &lt;pre&gt;&lt;code&gt;# Clone OCS2&#xA;git clone git@github.com:leggedrobotics/ocs2.git&#xA;# Clone pinocchio&#xA;git clone --recurse-submodules https://github.com/leggedrobotics/pinocchio.git&#xA;# Clone hpp-fcl&#xA;git clone --recurse-submodules https://github.com/leggedrobotics/hpp-fcl.git&#xA;# Clone ocs2_robotic_assets&#xA;git clone https://github.com/leggedrobotics/ocs2_robotic_assets.git&#xA;# Install dependencies&#xA;sudo apt install liburdfdom-dev liboctomap-dev libassimp-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Compile the &lt;code&gt;ocs2_legged_robot_ros&lt;/code&gt; package with &lt;a href=&#34;https://catkin-tools.readthedocs.io/en/latest/&#34;&gt;catkin tools&lt;/a&gt; instead of &lt;code&gt;catkin_make&lt;/code&gt;. It will take you about ten minutes. &lt;pre&gt;&lt;code&gt;catkin config -DCMAKE_BUILD_TYPE=RelWithDebInfo&#xA;catkin build ocs2_legged_robot_ros ocs2_self_collision_visualization&#xA;&lt;/code&gt;&lt;/pre&gt; Ensure you can command the ANYmal as shown in the &lt;a href=&#34;https://leggedrobotics.github.io/ocs2/robotic_examples.html#legged-robot&#34;&gt;document&lt;/a&gt; and below. &lt;img src=&#34;https://leggedrobotics.github.io/ocs2/_images/legged_robot.gif&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;Build the source code of &lt;code&gt;legged_control&lt;/code&gt; by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;catkin build legged_controllers legged_unitree_description&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the simulation (&lt;strong&gt;DO NOT&lt;/strong&gt; run on the onboard computer)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;catkin build legged_gazebo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the hardware interface real robot. If you use your computer only for simulation, you &lt;strong&gt;DO NOT&lt;/strong&gt; need to compile &lt;code&gt;legged_unitree_hw&lt;/code&gt; (TODO: add a legged prefix to the package name)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;catkin build legged_unitree_hw&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set your robot type as an environment variable: ROBOT_TYPE&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;export ROBOT_TYPE=a1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the simulation:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch legged_unitree_description empty_world.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or on the robot hardware:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch legged_unitree_hw legged_unitree_hw.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Load the controller:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch legged_controllers load_controller.launch cheater:=false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Start the &lt;code&gt;legged_controller&lt;/code&gt; or &lt;code&gt;legged_cheater_controller&lt;/code&gt;, &lt;strong&gt;NOTE that you are not allowed to start the &lt;code&gt;legged_cheater_controller&lt;/code&gt; in real hardware!&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosservice call /controller_manager/switch_controller &#34;start_controllers: [&#39;controllers/legged_controller&#39;]                   &#xA;stop_controllers: [&#39;&#39;]&#xA;strictness: 0&#xA;start_asap: false&#xA;timeout: 0.0&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, you can start the controller using &lt;code&gt;rqt_controller_manager&lt;/code&gt; GUI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install ros-noetic-rqt-controller-manager&#xA;rosrun rqt_controller_manager rqt_controller_manager&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Set the gait in the terminal of &lt;code&gt;load_controller.launch&lt;/code&gt;, then use RViz (you need to add what you want to display by yourself) and control the robot by &lt;code&gt;cmd_vel&lt;/code&gt; and &lt;code&gt;move_base_simple/goal&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2022/07/27/lBzdeRa1gmvwx9C.gif&#34; alt=&#34;ezgif-5-684a1e1e23.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Note&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;THE GAIT AND THE GOAL ARE COMPLETELY DIFFERENT AND SEPARATED!&lt;/strong&gt; You don&#39;t need to type stance while the robot is lying on the ground &lt;strong&gt;with four foot touching the ground&lt;/strong&gt;; it&#39;s completely wrong since the robot is already in the stance gait.&lt;/li&gt; &#xA; &lt;li&gt;The target_trajectories_publisher is for demonstration. You can combine the trajectory publisher and gait command into a very simple node to add gamepad and keyboard input for different gaits and torso heights and to start/stop controller (by ros service).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Framework&lt;/h2&gt; &#xA;&lt;p&gt;The system framework diagram is shown below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/qiayuanl/legged_control/master/docs/system_diagram.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The robot torso&#39;s desired velocity or position goal is converted to state trajectory and then sent to the NMPC;&lt;/li&gt; &#xA; &lt;li&gt;The NMPC will evaluate an optimized system state and input.&lt;/li&gt; &#xA; &lt;li&gt;The Whole-body Controller (WBC) figures out the joint torques according to the optimized states and inputs from the NMPC.&lt;/li&gt; &#xA; &lt;li&gt;The torque is set as a feed-forward term and is sent to the robot&#39;s motor controller. Low-gain joint-space position and velocity PD commands are sent to the robot&#39;s motors to reduce the shock during foot contact and for better tracking performance.&lt;/li&gt; &#xA; &lt;li&gt;The NMPC and WBC need to know the current robot state, the base orientation, and the joint state, all obtained directly from the IMU and the motors. Running in the same loop with WBC, a linear Kalman filter[1] estimates the base position and velocity from base orientation, base acceleration, and joint foot position measurements.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Module&lt;/h2&gt; &#xA;&lt;p&gt;The main module of the entire control framework is NMPC and WBC, and the following is only a very brief introduction.&lt;/p&gt; &#xA;&lt;h3&gt;NMPC&lt;/h3&gt; &#xA;&lt;p&gt;The NMPC part solves the following optimization problems at each cycle through the formulation and solving interfaces provided by OCS2:&lt;/p&gt; &#xA;&lt;p&gt;$$ \begin{split} \begin{cases} \underset{\mathbf u(.)}{\min} \ \ \phi(\mathbf x(t_I)) + \displaystyle \int_{t_0}^{t_I} l(\mathbf x(t), \mathbf u(t), t) , dt \ \text{s.t.} \ \ \mathbf x(t_0) = \mathbf x_0 ,\hspace{11.5em} \text{initial state} \ \ \ \ \ \ \dot{\mathbf x}(t) = \mathbf f(\mathbf x(t), \mathbf u(t), t) \hspace{7.5em} \text{system flow map} \ \ \ \ \ \ \mathbf g_1(\mathbf x(t), \mathbf u(t), t) = \mathbf{0} \hspace{8.5em} \text{state-input equality constraints} \ \ \ \ \ \ \mathbf g_2(\mathbf x(t), t) = \mathbf{0} \hspace{10.5em} \text{state-only equality constraints} \ \ \ \ \ \ \mathbf h(\mathbf x(t), \mathbf u(t), t) \geq \mathbf{0} \hspace{8.5em} \text{inequality constraints} \end{cases}\end{split} $$&lt;/p&gt; &#xA;&lt;p&gt;For this framework, we defined system state $\mathbf{x}$ and input $\mathbf{u}$ as:&lt;/p&gt; &#xA;&lt;p&gt;$$ \begin{equation} \mathbf{x}= [\mathbf{h}_{com}^T, \mathbf{q}_b^T, \mathbf{q}_j^T]^T, \mathbf{u} = [\mathbf{f}_c^T, \mathbf{v}_j^T]^T \end{equation} $$&lt;/p&gt; &#xA;&lt;p&gt;where $\mathbf{h}_{com} \in \mathbb{R}^6$ is the collection of the normalized centroidal momentum, $\mathbf{q}=[\mathbf{q}_b^T, \mathbf{q}_j^T]^T$ is the generalized coordinate. $\mathbf{f}_c \in \mathbb{R}^{12}$ consists of contact forces at four contact points, i.e., four ground reaction forces of the foot. $\mathbf{q}_j$ and $\mathbf{v}_j$ are the joint positions and velocities. While the cost function is simply the quadratic cost of tracking the error of all states and the input, the system dynamics uses centroidal dynamics with the following constraints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Friction cone;&lt;/li&gt; &#xA; &lt;li&gt;No motion at the standing foot;&lt;/li&gt; &#xA; &lt;li&gt;The z-axis position of the swinging foot satisfies the gait-generated curve.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To solve this optimal control problem, a multiple shooting is formulated to transcribe the optimal control problem to a nonlinear program (NLP) problem, and the NLP problem is solved using Sequential Quadratic Programming (SQP). The QP subproblem is solved using HPIPM. For more details [2, 3]&lt;/p&gt; &#xA;&lt;h3&gt;WBC&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/qiayuanl/legged_control/master/docs/tasks.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;WBC only considers the current moment. Several tasks are defined in the table above. Each task is the equality constraints or inequality constraints on decision variables. The decision variables of WBC are:&lt;/p&gt; &#xA;&lt;p&gt;$$ \mathbf{x}_{wbc} = [\ddot{\mathbf{q}}^T, \mathbf{f}_c^T, \mathbf{\tau}^T]^T $$&lt;/p&gt; &#xA;&lt;p&gt;where $\ddot{\mathbf{q}}$ is acceleration of generalized coordinate, $\mathbf{\tau}$ is the joint torque. The WBC solves the QP problem in the null space of the higher priority tasks&#39; linear constraints and tries to minimize the slacking variables of inequality constraints. This approach can consider the full nonlinear rigid body dynamics and ensure strict hierarchy results. For more details [4].&lt;/p&gt; &#xA;&lt;h2&gt;Deploy and Develop&lt;/h2&gt; &#xA;&lt;h3&gt;A1 robot&lt;/h3&gt; &#xA;&lt;p&gt;People with ROS foundation should be able to run through simulation and real machine deployment within a few hours. The following shows some known laboratories that have run through this framework on their own A1 objects. and the time spent The table below shows the labs successfully deploying this repo in their &lt;strong&gt;real A1&lt;/strong&gt;; feel free to open a PR to update it. (because the code they got at the time was not stable, so the spend time cannot represent the their level).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Lab&lt;/th&gt; &#xA;   &lt;th&gt;XPeng Robotics&lt;/th&gt; &#xA;   &lt;th&gt;Unitree&lt;/th&gt; &#xA;   &lt;th&gt;Hybrid Robotics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spend Time&lt;/td&gt; &#xA;   &lt;td&gt;1 day&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;2 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;I recommended to use an external computing device such as NUC to run this control framework. The author uses the 11th generation of NUC, and the computing frequency of NMPC can be close to 200Hz.&lt;/p&gt; &#xA;&lt;h3&gt;Your costum rorbots&lt;/h3&gt; &#xA;&lt;p&gt;Deploying this framework to your robot is very simple, the steps are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imitate the &lt;code&gt;UnitreeHW&lt;/code&gt; class in legged_examples/legged_unitree/legged_unitree_hw , inherit &lt;code&gt;LeggedHW&lt;/code&gt; and implement the &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; functions of the hardware interface;&lt;/li&gt; &#xA; &lt;li&gt;Imitate the legged_examples/legged_unitree/legged_unitree_description, write the xarco of the robot and generate the URDF file, note that the names of the joint and link need to be the same as legged_unitree_description.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;[1] T. Flayols, A. Del Prete, P. Wensing, A. Mifsud, M. Benallegue, and O. Stasse, “Experimental evaluation of simple estimators for humanoid robots,” IEEE-RAS Int. Conf. Humanoid Robot., pp. 889–895, 2017, doi: 10.1109/HUMANOIDS.2017.8246977.&lt;/p&gt; &#xA;&lt;p&gt;[2] J. P. Sleiman, F. Farshidian, M. V. Minniti, and M. Hutter, “A Unified MPC Framework for Whole-Body Dynamic Locomotion and Manipulation,” IEEE Robot. Autom. Lett., vol. 6, no. 3, pp. 4688–4695, 2021, doi: 10.1109/LRA.2021.3068908.&lt;/p&gt; &#xA;&lt;p&gt;[3] R. Grandia, F. Jenelten, S. Yang, F. Farshidian, and M. Hutter, “Perceptive Locomotion through Nonlinear Model Predictive Control,” (submitted to) IEEE Trans. Robot., no. August, 2022, doi: 10.48550/arXiv.2208.08373.&lt;/p&gt; &#xA;&lt;p&gt;[4] C. Dario Bellicoso, C. Gehring, J. Hwangbo, P. Fankhauser, and M. Hutter, “Perception-less terrain adaptation through whole body control and hierarchical optimization,” in IEEE-RAS International Conference on Humanoid Robots, 2016, pp. 558–564, doi: 10.1109/HUMANOIDS.2016.7803330.&lt;/p&gt;</summary>
  </entry>
</feed>