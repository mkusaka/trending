<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-30T01:29:18Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hku-mars/FAST-LIVO</title>
    <updated>2024-10-30T01:29:18Z</updated>
    <id>tag:github.com,2024-10-30:/hku-mars/FAST-LIVO</id>
    <link href="https://github.com/hku-mars/FAST-LIVO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry (LIVO).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FAST-LIVO&lt;/h1&gt; &#xA;&lt;h2&gt;Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;3 July 2024&lt;/strong&gt;: We are excited to announce the upcoming release of &lt;a href=&#34;https://github.com/hku-mars/FAST-LIVO2&#34;&gt;&lt;strong&gt;FAST-LIVO2&lt;/strong&gt;&lt;/a&gt; (some high-resolution results are already showcased). This new version delivers a &lt;strong&gt;overwhelming enhancement&lt;/strong&gt; over &lt;strong&gt;FAST-LIVO&lt;/strong&gt;, establishing an &lt;strong&gt;undisputed state-of-the-art&lt;/strong&gt; in accuracy &lt;strong&gt;(pixel-level)&lt;/strong&gt;, efficiency &lt;strong&gt;(the first LIVO system applied for fully onboard autonomous UAV navigation)&lt;/strong&gt;, and robustness &lt;strong&gt;(validated with over 2TB data, demonstrating exceptional performance in numerous degenerated LiDAR and camera scenarios)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;7 Dec 2023&lt;/strong&gt;: A &lt;strong&gt;detailed step-by-step guide&lt;/strong&gt; for hard synchronization between Livox Mid-360/Avia and camera is published at &lt;a href=&#34;https://github.com/sheng00125/LIV_handhold&#34;&gt;&lt;strong&gt;LIV_hanheld&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FAST-LIVO&lt;/strong&gt; is a fast LiDAR-Inertial-Visual odometry system, which builds on two tightly-coupled and direct odometry subsystems: a VIO subsystem and a LIO subsystem. The LIO subsystem registers raw points (instead of feature points on e.g., edges or planes) of a new scan to an incrementally-built point cloud map. The map points are additionally attached with image patches, which are then used in the VIO subsystem to align a new image by minimizing the direct photometric errors without extracting any visual features (e.g., ORB or FAST corner features).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contributors&lt;/strong&gt;: &lt;a href=&#34;https://github.com/xuankuzcr&#34;&gt;Chunran Zheng 郑纯然&lt;/a&gt;， &lt;a href=&#34;https://github.com/ZQYKAWAYI&#34;&gt;Qingyan Zhu 朱清岩&lt;/a&gt;， &lt;a href=&#34;https://github.com/XW-HKU&#34;&gt;Wei Xu 徐威&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hku-mars/FAST-LIVO/main/img/Framework.svg?sanitize=true&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;1.1 Our paper&lt;/h3&gt; &#xA;&lt;p&gt;Our paper has been accepted to &lt;strong&gt;IROS2022&lt;/strong&gt;, which is now available on &lt;strong&gt;arXiv&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2203.00893&#34;&gt;FAST-LIVO: Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If our code is used in your project, please cite our paper following the bibtex below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zheng2022fast,&#xA;  title={FAST-LIVO: Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry},&#xA;  author={Zheng, Chunran and Zhu, Qingyan and Xu, Wei and Liu, Xiyuan and Guo, Qizhi and Zhang, Fu},&#xA;  journal={arXiv preprint arXiv:2203.00893},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;1.2 Our related video&lt;/h3&gt; &#xA;&lt;p&gt;Our accompanying videos are now available on &lt;strong&gt;YouTube&lt;/strong&gt; (click below images to open) and &lt;a href=&#34;https://www.bilibili.com/video/BV15q4y1i7sj?spm_id_from=333.337.search-card.all.click&#34;&gt;&lt;strong&gt;Bilibili&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=C6Pb_0W9E_g&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hku-mars/FAST-LIVO/main/img/cover.bmp&#34; alt=&#34;video&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;2. Prerequisited&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 Ubuntu and ROS&lt;/h3&gt; &#xA;&lt;p&gt;Ubuntu 16.04~20.04. &lt;a href=&#34;http://wiki.ros.org/ROS/Installation&#34;&gt;ROS Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;2.2 PCL &amp;amp;&amp;amp; Eigen &amp;amp;&amp;amp; OpenCV&lt;/h3&gt; &#xA;&lt;p&gt;PCL&amp;gt;=1.6, Follow &lt;a href=&#34;https://pointclouds.org/&#34;&gt;PCL Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Eigen&amp;gt;=3.3.4, Follow &lt;a href=&#34;https://eigen.tuxfamily.org/index.php?title=Main_Page&#34;&gt;Eigen Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;OpenCV&amp;gt;=3.2, Follow &lt;a href=&#34;http://opencv.org/&#34;&gt;Opencv Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;2.3 Sophus&lt;/h3&gt; &#xA;&lt;p&gt;Sophus Installation for the non-templated/double-only version.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/strasdat/Sophus.git&#xA;cd Sophus&#xA;git checkout a621ff&#xA;mkdir build &amp;amp;&amp;amp; cd build &amp;amp;&amp;amp; cmake ..&#xA;make&#xA;sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.4 Vikit&lt;/h3&gt; &#xA;&lt;p&gt;Vikit contains camera models, some math and interpolation functions that we need. Vikit is a catkin project, therefore, download it into your catkin workspace source folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd catkin_ws/src&#xA;git clone https://github.com/uzh-rpg/rpg_vikit.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.5 &lt;strong&gt;livox_ros_driver&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://github.com/Livox-SDK/livox_ros_driver&#34;&gt;livox_ros_driver Installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;3. Build&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository and catkin_make:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ~/catkin_ws/src&#xA;git clone https://github.com/hku-mars/FAST-LIVO&#xA;cd ../&#xA;catkin_make&#xA;source ~/catkin_ws/devel/setup.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. Run the package&lt;/h2&gt; &#xA;&lt;p&gt;Please note that our system can only work in the hard synchronized LiDAR-Inertial-Visual dataset at present due to the unestimated time offset between the camera and IMU. The frame headers of the camera and the LiDAR are at the same physical trigger time.&lt;/p&gt; &#xA;&lt;h3&gt;4.1 Important parameters&lt;/h3&gt; &#xA;&lt;p&gt;Edit &lt;code&gt;config/xxx.yaml&lt;/code&gt; to set the below parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;lid_topic&lt;/code&gt;: The topic name of LiDAR data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;imu_topic&lt;/code&gt;: The topic name of IMU data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;img_topic&lt;/code&gt;: The topic name of camera data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;img_enable&lt;/code&gt;: Enbale vio submodule.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lidar_enable&lt;/code&gt;: Enbale lio submodule.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;point_filter_num&lt;/code&gt;: The sampling interval for a new scan. It is recommended that &lt;code&gt;3~4&lt;/code&gt; for faster odometry, and &lt;code&gt;1~2&lt;/code&gt; for denser map.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;outlier_threshold&lt;/code&gt;: The outlier threshold value of photometric error (square) of a single pixel. It is recommended that &lt;code&gt;50~250&lt;/code&gt; for the darker scenes, and &lt;code&gt;500~1000&lt;/code&gt; for the brighter scenes. The smaller the value is, the faster the vio submodule is, but the weaker the anti-degradation ability is.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;img_point_cov&lt;/code&gt;: The covariance of photometric errors per pixel.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;laser_point_cov&lt;/code&gt;: The covariance of point-to-plane redisual per point.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;filter_size_surf&lt;/code&gt;: Downsample the points in a new scan. It is recommended that &lt;code&gt;0.05~0.15&lt;/code&gt; for indoor scenes, &lt;code&gt;0.3~0.5&lt;/code&gt; for outdoor scenes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;filter_size_map&lt;/code&gt;: Downsample the points in LiDAR global map. It is recommended that &lt;code&gt;0.15~0.3&lt;/code&gt; for indoor scenes, &lt;code&gt;0.4~0.5&lt;/code&gt; for outdoor scenes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pcd_save_en&lt;/code&gt;: If &lt;code&gt;true&lt;/code&gt;, save point clouds to the PCD folder. Save RGB-colored points if &lt;code&gt;img_enable&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt;, intensity-colored points if &lt;code&gt;img_enable&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delta_time&lt;/code&gt;: The time offset between the camera and LiDAR, which is used to correct timestamp misalignment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After setting the appropriate topic name and parameters, you can directly run &lt;strong&gt;FAST-LIVO&lt;/strong&gt; on the dataset.&lt;/p&gt; &#xA;&lt;h3&gt;4.2 Run on private dataset&lt;/h3&gt; &#xA;&lt;p&gt;Download our collected rosbag files via OneDrive (&lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:f:/g/personal/zhengcr_connect_hku_hk/Esiqlmaql0dPreuOhiHlXl4Bqu5RRRIViK1EyuR4h1_n4w?e=fZdVn0&#34;&gt;FAST-LIVO-Datasets&lt;/a&gt;) containing &lt;strong&gt;4&lt;/strong&gt; rosbag files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch fast_livo mapping_avia.launch&#xA;rosbag play YOUR_DOWNLOADED.bag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4.3 Run on benchmark dataset&lt;/h3&gt; &#xA;&lt;p&gt;NTU-VIRAL&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch fast_livo mapping_avia_ntu.launch&#xA;rosbag play YOUR_DOWNLOADED.bag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MARS-LVIG&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch fast_livo mapping_avia_marslvig.launch&#xA;rosbag play YOUR_DOWNLOADED.bag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. Our hard sychronized equipment&lt;/h2&gt; &#xA;&lt;p&gt;To support the robotics community and enhance the reproducibility of our work, we provide CAD files for our handheld device, available in &#34;.SLDPRT&#34; and &#34;.SLDASM&#34; formats. These files can be opened and edited using Solidworks. Each module is designed for compatibility with FDM (Fused Deposition Modeling) technology, ensuring ease of 3D printing. Additionally, we open-source our &lt;strong&gt;hardware synchronization scheme&lt;/strong&gt;, the &lt;strong&gt;STM32 source code&lt;/strong&gt;, detailed &lt;strong&gt;hardware wiring configuration instructions&lt;/strong&gt;, and &lt;strong&gt;sensor ros driver&lt;/strong&gt;. Access these resources at our repository: &lt;a href=&#34;https://github.com/sheng00125/LIV_handhold&#34;&gt;&lt;strong&gt;LIV_handhold&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hku-mars/FAST-LIVO/main/img/cover.jpg&#34; alt=&#34;principle&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;6. Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for &lt;a href=&#34;https://github.com/hku-mars/FAST_LIO&#34;&gt;FAST-LIO2&lt;/a&gt; and &lt;a href=&#34;https://github.com/uzh-rpg/rpg_svo_pro_open&#34;&gt;SVO2.0&lt;/a&gt;. Thanks for &lt;a href=&#34;https://www.livoxtech.com/&#34;&gt;Livox_Technology&lt;/a&gt; for equipment support.&lt;/p&gt; &#xA;&lt;p&gt;Thanks &lt;a href=&#34;https://github.com/ziv-lin&#34;&gt;Jiarong Lin&lt;/a&gt; for the help in the experiments.&lt;/p&gt; &#xA;&lt;h2&gt;7. License&lt;/h2&gt; &#xA;&lt;p&gt;The source code of this package is released under &lt;a href=&#34;http://www.gnu.org/licenses/&#34;&gt;&lt;strong&gt;GPLv2&lt;/strong&gt;&lt;/a&gt; license. We only allow it free for &lt;strong&gt;academic usage&lt;/strong&gt;. For commercial use, please contact Dr. Fu Zhang &lt;a href=&#34;mailto:fuzhang@hku.hk&#34;&gt;fuzhang@hku.hk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For any technical issues, please contact me via email &lt;a href=&#34;mailto:zhengcr@connect.hku.hk&#34;&gt;zhengcr@connect.hku.hk&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>