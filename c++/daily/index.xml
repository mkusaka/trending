<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-25T01:30:49Z</updated>
  <subtitle>Daily Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ropfuscator/ropfuscator</title>
    <updated>2023-03-25T01:30:49Z</updated>
    <id>tag:github.com,2023-03-25:/ropfuscator/ropfuscator</id>
    <link href="https://github.com/ropfuscator/ropfuscator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ROPfuscator is a fine-grained code obfuscation framework for C/C++ programs using ROP (return-oriented programming).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;ROPfuscator is a research proof of concept and is not intended for production use. The authors do not take any responsibility or liability for the use of the software. Please exercise caution and use at your own risk.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ROPfuscator &lt;a href=&#34;https://github.com/ropfuscator/ropfuscator/actions/workflows/main.yaml&#34;&gt;&lt;img src=&#34;https://github.com/ropfuscator/ropfuscator/actions/workflows/main.yaml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/logo.png&#34; alt=&#34;logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ROPfuscator is a fine-grained code obfuscation framework for LLVM-supported languages using ROP (return-oriented programming). ROPfuscator obfuscates a program at the assembly code level by transforming regular instructions into ROP chains, thwarting our natural conception of normal control flow. It is implemented as an extension to LLVM (10.0.1) x86 backend.&lt;/p&gt; &#xA;&lt;p&gt;For build, usage and implementation, see individual documents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Building ROPfuscator: &lt;a href=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/build.md&#34;&gt;build.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Using ROPfuscator to obfuscate programs: &lt;a href=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/usage.md&#34;&gt;usage.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Obfuscation algorithm details: &lt;a href=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/algorithm.md&#34;&gt;algorithm.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Implementation details: &lt;a href=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/implementation.md&#34;&gt;implementation.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Key Improvements&lt;/h2&gt; &#xA;&lt;p&gt;This project aims to provide an improved version of ROPfuscator with a strong focus on reproducibility and ease of integration. We have made several key enhancements in this repository, as outlined below.&lt;/p&gt; &#xA;&lt;h3&gt;Nix Package Manager&lt;/h3&gt; &#xA;&lt;p&gt;ROPfuscator now leverages &lt;a href=&#34;https://nixos.org/&#34;&gt;Nix&lt;/a&gt;, a powerful declarative package manager that allows for reliable and reproducible builds. Nix provides several benefits:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensures dependencies and build environments are consistent across systems.&lt;/li&gt; &#xA; &lt;li&gt;Allows for isolated build environments, eliminating conflicts with other installed packages.&lt;/li&gt; &#xA; &lt;li&gt;Supports rollbacks to previous package versions, making it easier to recover from failed updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ROPfuscator provides a Nix flake exposing ROPfuscator&#39;s &lt;code&gt;stdenv&lt;/code&gt;s and various helper functions used to natively compile Nix derivations, without applying any modification to the build system of the project to be built.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation Process&lt;/h3&gt; &#xA;&lt;p&gt;The evaluation process has been rewritten from scratch, taking full advantage of the Nix package manager. This ensures a more reliable and transparent evaluation, which will be the foundation for future work on ROPfuscator.&lt;/p&gt; &#xA;&lt;h3&gt;Transparent Obfuscation for Upstream Nix Packages&lt;/h3&gt; &#xA;&lt;p&gt;ROPfuscator can now transparently attempt to obfuscate &lt;em&gt;any&lt;/em&gt; package present in the upstream Nix package repository, &lt;a href=&#34;https://github.com/NixOS/nixpkgs&#34;&gt;nixpkgs&lt;/a&gt;, without requiring any modifications. This allows Nix users to seamlessly integrate ROPfuscator into their existing workflows and test its capabilities.&lt;/p&gt; &#xA;&lt;p&gt;ROPfuscator can target a single project, obfuscating only the object files pertinent to the project itself, or it can obfuscate the target along with all its dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation and White Paper&lt;/h2&gt; &#xA;&lt;p&gt;Please note that the evaluation code present in this repository has not been used to produce any artifact. The original evaluation from the deprecated repository is still valid and it is used in the original white paper.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;h3&gt;Using Nix (recommended)&lt;/h3&gt; &#xA;&lt;h4&gt;Step 0: Install Nix&lt;/h4&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://nix.dev/tutorials/install-nix&#34;&gt;Nix&lt;/a&gt; (the package manager) and make sure that its daemon is running.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1: Enable Nix to use Flakes&lt;/h4&gt; &#xA;&lt;p&gt;Flakes allow you to specify your code&#39;s dependencies in a declarative way and they allow to easily specify inputs and outputs for projects. ROPfuscator exposes different outputs hence we need to enable Nix to use flakes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nixos.wiki/wiki/Flakes&#34;&gt;Here&lt;/a&gt; is a step-by-step process on how to enable them.&lt;/p&gt; &#xA;&lt;h4&gt;Step 2: Add ROPfuscator cache repository to Nix&#39;s channels (optional)&lt;/h4&gt; &#xA;&lt;p&gt;This step allows leveraging ROPfuscator&#39;s cache repository to avoid recompiling the project and all its dependencies from scratch. This step is optional but recommended.&lt;/p&gt; &#xA;&lt;p&gt;To enable ROPfuscator&#39;s cache, first install &lt;code&gt;cachix&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nix-env -iA cachix -f https://cachix.org/api/v1/install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, configure &lt;code&gt;nix.conf&lt;/code&gt; to use the binary cache:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cachix use ropfuscator&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 3: Build and use ROPfuscator&lt;/h4&gt; &#xA;&lt;p&gt;The final step is to build ROPfuscator. This can be achieved by invoking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nix build github:ropfuscator/ropfuscator -L&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to drop in a shell configured to use ROPfuscator by default, just invoke:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nix shell github:ropfuscator/ropfuscator&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ROPfuscator Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/architecture.svg?sanitize=true&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We combine the following obfuscation layers to achieve robust obfuscation against several attacks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ROP Transformation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Convert each instruction into one or more ROP gadgets, and translate the entire code to ROP chains.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Opaque Predicate Insertion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Translate ROP gadget address(es) and stack pushed values into opaque constants, which are composition of multiple opaque predicates.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;ROPfuscator can be used to obfuscate packages that are present in the Nixpkgs repository. Currently, we are using a custom fork because some upstream packages were not properly configured for cross-compilation. Although we have already submitted some of the patches upstream, there is still some work to be done for a seamless experience.&lt;/p&gt; &#xA;&lt;p&gt;To get started, follow the first two steps listed above and install Nix. Then, copy &lt;code&gt;flake-example.nix&lt;/code&gt; into a directory, renaming it to &lt;code&gt;flake.nix&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; mkdir -p ropfuscator-example &amp;amp;&amp;amp; cd ropfuscator-example&#xA; cp ../flake-example.nix flake.nix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At this point, you can build the two packages defined in the flake: &lt;code&gt;hello&lt;/code&gt; and &lt;code&gt;obfuscatedHello&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build &lt;code&gt;obfuscatedHello&lt;/code&gt;, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; nix build .#obfuscatedHello -L&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly, to build &lt;code&gt;hello&lt;/code&gt; run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; nix build .#hello -L&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can use ROPfuscator to obfuscate packages present in the Nixpkgs repository. We currently are locked into a custom fork as some packages upstream were not well configured for cross-compilation. We pushed some of the patches upstream but there is more work to have a seemless experience.&lt;/p&gt; &#xA;&lt;p&gt;Follow step 0 and step 1 from above and install Nix. Then, copy the &lt;code&gt;flake-example.nix&lt;/code&gt; into a directory. At this point we can build the two packages defined in the flake: &lt;code&gt;hello&lt;/code&gt; and &lt;code&gt;obfuscatedHello&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build obfuscatedHello we have to invoke&lt;/p&gt; &#xA;&lt;p&gt;nix build .#obfuscatedHello -L&lt;/p&gt; &#xA;&lt;p&gt;Similarly, for hello:&lt;/p&gt; &#xA;&lt;p&gt;nix build .#hello -L&lt;/p&gt; &#xA;&lt;h2&gt;Configurations&lt;/h2&gt; &#xA;&lt;p&gt;ROPfuscator can be configured through TOML configuration files. &lt;a href=&#34;https://github.com/ropfuscator/utilities/tree/master/configs&#34;&gt;This repository&lt;/a&gt; includes the following pre-made configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ROP Only&lt;/strong&gt;: does not obfuscate gadget addresses, stack values, immediate operands, or branch targets, and does not use opaque predicates.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;All Addresses&lt;/strong&gt;: obfuscates all gadget addresses and uses opaque predicates for all opaque constants.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Half addresses&lt;/strong&gt;: obfuscates 50% of gadget addresses and uses opaque predicates for all opaque constants.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Full&lt;/strong&gt;: obfuscates all gadget addresses, stack values, immediate operands, and branch targets, and uses opaque predicates for all opaque constants.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each configuration can be further customized with the options available in the configuration table in the &lt;a href=&#34;https://github.com/ropfuscator/utilities/tree/master/configs&#34;&gt;README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux 32-bit x86 binaries are the only supported target (as of now)&lt;/li&gt; &#xA; &lt;li&gt;For detailed limitations, see &lt;a href=&#34;https://raw.githubusercontent.com/ropfuscator/ropfuscator/master/docs/limitation.md&#34;&gt;limitation.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Interested in working on ROPfuscator?&lt;/h2&gt; &#xA;&lt;p&gt;We encourage collaboration and are open to discussing potential extensions or improvements to the project. If you are interested in contributing, please reach out to us or open an issue.&lt;/p&gt; &#xA;&lt;p&gt;Thank you for your support!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/cuda-quantum</title>
    <updated>2023-03-25T01:30:49Z</updated>
    <id>tag:github.com,2023-03-25:/NVIDIA/cuda-quantum</id>
    <link href="https://github.com/NVIDIA/cuda-quantum" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ and Python support for the CUDA Quantum programming model for heterogeneous quantum-classical workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the CUDA Quantum repository&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img align=&#34;right&#34; width=&#34;200&#34; src=&#34;https://developer.nvidia.com/sites/default/files/akamai/nvidia-cuquantum-icon.svg?sanitize=true&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The CUDA Quantum Platform for hybrid quantum-classical computers enables integration and programming of quantum processing units (QPUs), GPUs, and CPUs in one system. This repository contains the source code for all C++ and Python tools provided by the CUDA Quantum toolkit, including the NVQ++ compiler, the CUDA Quantum runtime, as well as a selection of integrated CPU and GPU backends for rapid application development and testing.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about how to work with CUDA Quantum, please take a look at the &lt;a href=&#34;https://nvidia.github.io/cuda-quantum/&#34;&gt;CUDA Quantum Documentation&lt;/a&gt;. The page also contains &lt;a href=&#34;https://nvidia.github.io/cuda-quantum/install.html&#34;&gt;installation instructions&lt;/a&gt; for officially released packages.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to install the latest iteration under development in this repository and/or add your own modifications, please build CUDA Quantum from source following &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/cuda-quantum/main/Building.md&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways in which you can get involved with CUDA Quantum. If you are interested in developing quantum applications with CUDA Quantum, this repository is a great place to get started! For more information about contributing to the CUDA Quantum platform, please take a look at &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/cuda-quantum/main/Contributing.md&#34;&gt;Contributing.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/cuda-quantum/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Contributing a pull request to this repo requires accepting the Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. A CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately. Simply follow the instructions provided by the bot. You will only need to do this once.&lt;/p&gt; &#xA;&lt;h2&gt;Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Please let us know your feedback and ideas for the CUDA Quantum platform in the &lt;a href=&#34;https://github.com/NVIDIA/cuda-quantum/discussions&#34;&gt;Discussions&lt;/a&gt; tab of this repository, or file an &lt;a href=&#34;https://github.com/NVIDIA/cuda-quantum/issues&#34;&gt;issue&lt;/a&gt;. To report security concerns or &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/cuda-quantum/main/Code_of_Conduct.md&#34;&gt;Code of Conduct&lt;/a&gt; violations, please reach out to &lt;a href=&#34;mailto:cuda-quantum@nvidia.com&#34;&gt;cuda-quantum@nvidia.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>interpretml/interpret</title>
    <updated>2023-03-25T01:30:49Z</updated>
    <id>tag:github.com,2023-03-25:/interpretml/interpret</id>
    <link href="https://github.com/interpretml/interpret" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fit interpretable models. Explain blackbox machine learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InterpretML&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/interpretml/interpret.svg?style=flat-square&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/interpretml/interpret/develop?labpath=examples%2Fpython%2FInterpretable_Classification_Methods.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/interpret.svg?style=flat-square&#34; alt=&#34;Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/interpret.svg?style=flat-square&#34; alt=&#34;Package Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/conda/v/conda-forge/interpret&#34; alt=&#34;Conda&#34;&gt; &lt;img src=&#34;https://img.shields.io/azure-devops/build/ms/interpret/293/develop.svg?style=flat-square&#34; alt=&#34;Build Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/azure-devops/coverage/ms/interpret/293/develop.svg?style=flat-square&#34; alt=&#34;Coverage&#34;&gt; &lt;img src=&#34;https://img.shields.io/maintenance/yes/2023?style=flat-square&#34; alt=&#34;Maintenance&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;In the beginning machines learned in darkness, and data scientists struggled in the void to explain them.&lt;/h3&gt; &#xA; &lt;h3&gt;Let there be light.&lt;/h3&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model&#39;s global behavior, or understand the reasons behind individual predictions.&lt;/p&gt; &#xA;&lt;p&gt;Interpretability is essential for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model debugging - Why did my model make this mistake?&lt;/li&gt; &#xA; &lt;li&gt;Feature Engineering - How can I improve my model?&lt;/li&gt; &#xA; &lt;li&gt;Detecting fairness issues - Does my model discriminate?&lt;/li&gt; &#xA; &lt;li&gt;Human-AI cooperation - How can I understand and trust the model&#39;s decisions?&lt;/li&gt; &#xA; &lt;li&gt;Regulatory compliance - Does my model satisfy legal requirements?&lt;/li&gt; &#xA; &lt;li&gt;High-risk applications - Healthcare, finance, judicial, ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/interpretml/interpretml.github.io/raw/master/interpret-highlight.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Python 3.7+ | Linux, Mac, Windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install interpret&#xA;# OR&#xA;conda install -c conda-forge interpret&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Introducing the Explainable Boosting Machine (EBM)&lt;/h1&gt; &#xA;&lt;p&gt;EBM is an interpretable model developed at Microsoft Research&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/interpretml/interpret/develop/#citations&#34;&gt;*&lt;/a&gt;&lt;/sup&gt;. It uses modern machine learning techniques like bagging, gradient boosting, and automatic interaction detection to breathe new life into traditional GAMs (Generalized Additive Models). This makes EBMs as accurate as state-of-the-art techniques like random forests and gradient boosted trees. However, unlike these blackbox models, EBMs produce exact explanations and are editable by domain experts.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset/AUROC&lt;/th&gt; &#xA;   &lt;th&gt;Domain&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Logistic Regression&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Random Forest&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;XGBoost&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Explainable Boosting Machine&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Adult Income&lt;/td&gt; &#xA;   &lt;td&gt;Finance&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.907±.003&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.903±.002&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.927±.001&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.928±.002&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Heart Disease&lt;/td&gt; &#xA;   &lt;td&gt;Medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.895±.030&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.890±.008&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.851±.018&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.898±.013&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Breast Cancer&lt;/td&gt; &#xA;   &lt;td&gt;Medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.995±.005&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.992±.009&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.992±.010&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.995±.006&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Telecom Churn&lt;/td&gt; &#xA;   &lt;td&gt;Business&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.849±.005&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.824±.004&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.828±.010&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.852±.006&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Credit Fraud&lt;/td&gt; &#xA;   &lt;td&gt;Security&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.979±.002&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.950±.007&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.981±.003&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;em&gt;.981±.003&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/benchmarks/EBM%20Classification%20Comparison.ipynb&#34;&gt;&lt;em&gt;Notebook for reproducing table&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Supported Techniques&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Interpretability Technique&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/ebm.html&#34;&gt;Explainable Boosting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;glassbox model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/dt.html&#34;&gt;Decision Tree&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;glassbox model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/dr.html&#34;&gt;Decision Rule List&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;glassbox model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/lr.html&#34;&gt;Linear/Logistic Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;glassbox model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/shap.html&#34;&gt;SHAP Kernel Explainer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;blackbox explainer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/lime.html&#34;&gt;LIME&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;blackbox explainer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/msa.html&#34;&gt;Morris Sensitivity Analysis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;blackbox explainer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://interpret.ml/docs/pdp.html&#34;&gt;Partial Dependence&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;blackbox explainer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Train a glassbox model&lt;/h1&gt; &#xA;&lt;p&gt;Let&#39;s fit an Explainable Boosting Machine&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from interpret.glassbox import ExplainableBoostingClassifier&#xA;&#xA;ebm = ExplainableBoostingClassifier()&#xA;ebm.fit(X_train, y_train)&#xA;&#xA;# or substitute with LogisticRegression, DecisionTreeClassifier, RuleListClassifier, ...&#xA;# EBM supports pandas dataframes, numpy arrays, and handles &#34;string&#34; data natively.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Understand the model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from interpret import show&#xA;&#xA;ebm_global = ebm.explain_global()&#xA;show(ebm_global)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/interpretml/interpret/develop/examples/python/assets/readme_ebm_global_specific.PNG?raw=true&#34; alt=&#34;Global Explanation Image&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Understand individual predictions&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ebm_local = ebm.explain_local(X_test, y_test)&#xA;show(ebm_local)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/interpretml/interpret/develop/examples/python/assets/readme_ebm_local_specific.PNG?raw=true&#34; alt=&#34;Local Explanation Image&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;And if you have multiple model explanations, compare them&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;show([logistic_regression_global, decision_tree_global])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/interpretml/interpret/develop/examples/python/assets/readme_dashboard.PNG?raw=true&#34; alt=&#34;Dashboard Image&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;If you need to keep your data private, use Differentially Private EBMs (see &lt;a href=&#34;http://proceedings.mlr.press/v139/nori21a/nori21a.pdf&#34;&gt;DP-EBMs&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from interpret.privacy import DPExplainableBoostingClassifier, DPExplainableBoostingRegressor&#xA;&#xA;dp_ebm = DPExplainableBoostingClassifier(epsilon=1, delta=1e-5) # Specify privacy parameters&#xA;dp_ebm.fit(X_train, y_train)&#xA;&#xA;show(dp_ebm.explain_global()) # Identical function calls to standard EBMs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For more information, see the &lt;a href=&#34;https://interpret.ml/docs/getting-started.html&#34;&gt;documentation&lt;/a&gt;. &lt;br&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;InterpretML was originally created by (equal contributions): Samuel Jenkins, Harsha Nori, Paul Koch, and Rich Caruana&lt;/p&gt; &#xA;&lt;p&gt;EBMs are fast derivative of GA2M, invented by: Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker&lt;/p&gt; &#xA;&lt;p&gt;Many people have supported us along the way. Check out &lt;a href=&#34;https://raw.githubusercontent.com/interpretml/interpret/develop/ACKNOWLEDGEMENTS.md&#34;&gt;ACKNOWLEDGEMENTS.md&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;We also build on top of many great packages. Please check them out!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/plotly/plotly.py&#34;&gt;plotly&lt;/a&gt; | &lt;a href=&#34;https://github.com/plotly/dash&#34;&gt;dash&lt;/a&gt; | &lt;a href=&#34;https://github.com/scikit-learn/scikit-learn&#34;&gt;scikit-learn&lt;/a&gt; | &lt;a href=&#34;https://github.com/marcotcr/lime&#34;&gt;lime&lt;/a&gt; | &lt;a href=&#34;https://github.com/slundberg/shap&#34;&gt;shap&lt;/a&gt; | &lt;a href=&#34;https://github.com/SALib/SALib&#34;&gt;salib&lt;/a&gt; | &lt;a href=&#34;https://github.com/scikit-learn-contrib/skope-rules&#34;&gt;skope-rules&lt;/a&gt; | &lt;a href=&#34;https://github.com/andosa/treeinterpreter&#34;&gt;treeinterpreter&lt;/a&gt; | &lt;a href=&#34;https://github.com/gevent/gevent&#34;&gt;gevent&lt;/a&gt; | &lt;a href=&#34;https://github.com/joblib/joblib&#34;&gt;joblib&lt;/a&gt; | &lt;a href=&#34;https://github.com/pytest-dev/pytest&#34;&gt;pytest&lt;/a&gt; | &lt;a href=&#34;https://github.com/jupyter/notebook&#34;&gt;jupyter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a name=&#34;citations&#34;&gt;Citations&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;strong&gt;InterpretML&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details open&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;InterpretML: A Unified Framework for Machine Learning Interpretability&#34; (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{nori2019interpretml,&#xA;  title={InterpretML: A Unified Framework for Machine Learning Interpretability},&#xA;  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},&#xA;  journal={arXiv preprint arXiv:1909.09223},&#xA;  year={2019}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/1909.09223.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Explainable Boosting&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission&#34; (R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad 2015)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{caruana2015intelligible,&#xA;  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},&#xA;  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},&#xA;  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},&#xA;  pages={1721--1730},&#xA;  year={2015},&#xA;  organization={ACM}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Accurate intelligible models with pairwise interactions&#34; (Y. Lou, R. Caruana, J. Gehrke, and G. Hooker 2013)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{lou2013accurate,&#xA;  title={Accurate intelligible models with pairwise interactions},&#xA;  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},&#xA;  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},&#xA;  pages={623--631},&#xA;  year={2013},&#xA;  organization={ACM}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;http://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Intelligible models for classification and regression&#34; (Y. Lou, R. Caruana, and J. Gehrke 2012)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{lou2012intelligible,&#xA;  title={Intelligible models for classification and regression},&#xA;  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes},&#xA;  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},&#xA;  pages={150--158},&#xA;  year={2012},&#xA;  organization={ACM}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values&#34; (Zijie J. Wang, Alex Kale, Harsha Nori, Peter Stella, Mark E. Nunnally, Duen Horng Chau, Mihaela Vorvoreanu, Jennifer Wortman Vaughan, Rich Caruana 2022)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{wang2022interpretability,&#xA;  title={Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values},&#xA;  author={Wang, Zijie J and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E and Chau, Duen Horng and Vorvoreanu, Mihaela and Vaughan, Jennifer Wortman and Caruana, Rich},&#xA;  journal={arXiv preprint arXiv:2206.15465},&#xA;  year={2022}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/2206.15465.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Axiomatic Interpretability for Multiclass Additive Models&#34; (X. Zhang, S. Tan, P. Koch, Y. Lou, U. Chajewska, and R. Caruana 2019)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{zhang2019axiomatic,&#xA;  title={Axiomatic Interpretability for Multiclass Additive Models},&#xA;  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},&#xA;  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp;amp; Data Mining},&#xA;  pages={226--234},&#xA;  year={2019},&#xA;  organization={ACM}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/1810.09092.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Distill-and-compare: auditing black-box models using transparent model distillation&#34; (S. Tan, R. Caruana, G. Hooker, and Y. Lou 2018)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{tan2018distill,&#xA;  title={Distill-and-compare: auditing black-box models using transparent model distillation},&#xA;  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},&#xA;  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},&#xA;  pages={303--310},&#xA;  year={2018},&#xA;  organization={ACM}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/1710.06169&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models&#34; (B. Lengerich, S. Tan, C. Chang, G. Hooker, R. Caruana 2019)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{lengerich2019purifying,&#xA;  title={Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},&#xA;  author={Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},&#xA;  journal={arXiv preprint arXiv:1911.04974},&#xA;  year={2019}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/1911.04974.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Interpreting Interpretability: Understanding Data Scientists&#39; Use of Interpretability Tools for Machine Learning&#34; (H. Kaur, H. Nori, S. Jenkins, R. Caruana, H. Wallach, J. Wortman Vaughan 2020)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{kaur2020interpreting,&#xA;  title={Interpreting Interpretability: Understanding Data Scientists&#39; Use of Interpretability Tools for Machine Learning},&#xA;  author={Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},&#xA;  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},&#xA;  pages={1--14},&#xA;  year={2020}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;http://www-personal.umich.edu/~harmank/Papers/CHI2020_Interpretability.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;How Interpretable and Trustworthy are GAMs?&#34; (C. Chang, S. Tan, B. Lengerich, A. Goldenberg, R. Caruana 2020)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{chang2020interpretable,&#xA;  title={How Interpretable and Trustworthy are GAMs?},&#xA;  author={Chang, Chun-Hao and Tan, Sarah and Lengerich, Ben and Goldenberg, Anna and Caruana, Rich},&#xA;  journal={arXiv preprint arXiv:2006.06466},&#xA;  year={2020}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/2006.06466.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Differential Privacy&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Accuracy, Interpretability, and Differential Privacy via Explainable Boosting&#34; (H. Nori, R. Caruana, Z. Bu, J. Shen, J. Kulkarni 2021)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{pmlr-v139-nori21a,&#xA;  title = &#x9; {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},&#xA;  author =       {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},&#xA;  booktitle = &#x9; {Proceedings of the 38th International Conference on Machine Learning},&#xA;  pages = &#x9; {8227--8237},&#xA;  year = &#x9; {2021},&#xA;  volume = &#x9; {139},&#xA;  series = &#x9; {Proceedings of Machine Learning Research},&#xA;  publisher =    {PMLR}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;http://proceedings.mlr.press/v139/nori21a/nori21a.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;LIME&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Why should i trust you?: Explaining the predictions of any classifier&#34; (M. T. Ribeiro, S. Singh, and C. Guestrin 2016)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@inproceedings{ribeiro2016should,&#xA;  title={Why should i trust you?: Explaining the predictions of any classifier},&#xA;  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},&#xA;  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},&#xA;  pages={1135--1144},&#xA;  year={2016},&#xA;  organization={ACM}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/1602.04938.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;SHAP&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;A Unified Approach to Interpreting Model Predictions&#34; (S. M. Lundberg and S.-I. Lee 2017)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@incollection{NIPS2017_7062,&#xA; title = {A Unified Approach to Interpreting Model Predictions},&#xA; author = {Lundberg, Scott M and Lee, Su-In},&#xA; booktitle = {Advances in Neural Information Processing Systems 30},&#xA; editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},&#xA; pages = {4765--4774},&#xA; year = {2017},&#xA; publisher = {Curran Associates, Inc.},&#xA; url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Consistent individualized feature attribution for tree ensembles&#34; (Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In 2018)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{lundberg2018consistent,&#xA;  title={Consistent individualized feature attribution for tree ensembles},&#xA;  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},&#xA;  journal={arXiv preprint arXiv:1802.03888},&#xA;  year={2018}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://arxiv.org/pdf/1802.03888&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Explainable machine-learning predictions for the prevention of hypoxaemia during surgery&#34; (S. M. Lundberg et al. 2018)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{lundberg2018explainable,&#xA;  title={Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},&#xA;  author={Lundberg, Scott M and Nair, Bala and Vavilala, Monica S and Horibe, Mayumi and Eisses, Michael J and Adams, Trevor and Liston, David E and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and others},&#xA;  journal={Nature Biomedical Engineering},&#xA;  volume={2},&#xA;  number={10},&#xA;  pages={749},&#xA;  year={2018},&#xA;  publisher={Nature Publishing Group}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6467492/pdf/nihms-1505578.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Sensitivity Analysis&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;SALib: An open-source Python library for Sensitivity Analysis&#34; (J. D. Herman and W. Usher 2017)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{herman2017salib,&#xA;  title={SALib: An open-source Python library for Sensitivity Analysis.},&#xA;  author={Herman, Jonathan D and Usher, Will},&#xA;  journal={J. Open Source Software},&#xA;  volume={2},&#xA;  number={9},&#xA;  pages={97},&#xA;  year={2017}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://www.researchgate.net/profile/Will_Usher/publication/312204236_SALib_An_open-source_Python_library_for_Sensitivity_Analysis/links/5ac732d64585151e80a39547/SALib-An-open-source-Python-library-for-Sensitivity-Analysis.pdf?origin=publication_detail&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Factorial sampling plans for preliminary computational experiments&#34; (M. D. Morris 1991)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{morris1991factorial,&#xA;  title={},&#xA;  author={Morris, Max D},&#xA;  journal={Technometrics},&#xA;  volume={33},&#xA;  number={2},&#xA;  pages={161--174},&#xA;  year={1991},&#xA;  publisher={Taylor \&amp;amp; Francis Group}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Partial Dependence&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Greedy function approximation: a gradient boosting machine&#34; (J. H. Friedman 2001)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{friedman2001greedy,&#xA;  title={Greedy function approximation: a gradient boosting machine},&#xA;  author={Friedman, Jerome H},&#xA;  journal={Annals of statistics},&#xA;  pages={1189--1232},&#xA;  year={2001},&#xA;  publisher={JSTOR}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Open Source Software&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;hr&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Scikit-learn: Machine learning in Python&#34; (F. Pedregosa et al. 2011)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{pedregosa2011scikit,&#xA;  title={Scikit-learn: Machine learning in Python},&#xA;  author={Pedregosa, Fabian and Varoquaux, Ga{\&#34;e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},&#xA;  journal={Journal of machine learning research},&#xA;  volume={12},&#xA;  number={Oct},&#xA;  pages={2825--2830},&#xA;  year={2011}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf&#34;&gt;Paper link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Collaborative data science&#34; (Plotly Technologies Inc. 2015)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@online{plotly, &#xA;  author = {Plotly Technologies Inc.}, &#xA;  title = {Collaborative data science}, &#xA;  publisher = {Plotly Technologies Inc.}, &#xA;  address = {Montreal, QC}, &#xA;  year = {2015}, &#xA;  url = {https://plot.ly}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://plot.ly&#34;&gt;Link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;em&gt;&#34;Joblib: running python function as pipeline jobs&#34; (G. Varoquaux and O. Grisel 2009)&lt;/em&gt; &lt;/summary&gt; &#xA;  &lt;br&gt; &#xA;  &lt;pre&gt;&#xA;@article{varoquaux2009joblib,&#xA;  title={Joblib: running python function as pipeline jobs},&#xA;  author={Varoquaux, Ga{\&#34;e}l and Grisel, O},&#xA;  journal={packages. python. org/joblib},&#xA;  year={2009}&#xA;}&#xA;    &lt;/pre&gt; &#xA;  &lt;a href=&#34;https://joblib.readthedocs.io/en/latest/&#34;&gt;Link&lt;/a&gt; &#xA; &lt;/details&gt; &#xA; &lt;hr&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Videos&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=MREiHgHgl0k&#34;&gt;The Science Behind InterpretML: Explainable Boosting Machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WwBeKMQ0-I8&#34;&gt;How to Explain Models with InterpretML Deep Dive&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/7uzNKY8pEhQ&#34;&gt;Black-Box and Glass-Box Explanation in Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qPn9m30ojfc&#34;&gt;Explainable AI explained! By-design interpretable models with Microsofts InterpretML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ERNuFfsknhk&#34;&gt;Interpreting Machine Learning Models with InterpretML&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;External links&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192&#34;&gt;Interpretable or Accurate? Why Not Both?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/the-explainable-boosting-machine-f24152509ebb&#34;&gt;The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.oakbits.com/ebm-algorithm.html&#34;&gt;Performance And Explainability With EBM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/interpretml-another-way-to-explain-your-model-b7faf0a384f8&#34;&gt;InterpretML: Another Way to Explain Your Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fiddler.ai/blog/a-gentle-introduction-to-ga2ms-a-white-box-model&#34;&gt;A gentle introduction to GA2Ms, a white box model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@sand.mayur/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae&#34;&gt;Model Interpretation with Microsoft’s Interpret ML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@mariusvadeika/explaining-model-pipelines-with-interpretml-a9214f75400b&#34;&gt;Explaining Model Pipelines With InterpretML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4&#34;&gt;Explain Your Model with Microsoft’s InterpretML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html&#34;&gt;On Model Explainability: From LIME, SHAP, to Explainable Boosting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mikewlange.github.io/ImbalancedData-/index.html&#34;&gt;Dealing with Imbalanced Data (Mortgage loans defaults)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254&#34;&gt;The right way to compute your Shapley Values&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727&#34;&gt;The Art of Sprezzatura for Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95&#34;&gt;Mixing Art into the Science of Model Explainability&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Papers that use or compare EBMs&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.02910.pdf&#34;&gt;Federated Boosted Decision Trees with Differential Privacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.09123.pdf&#34;&gt;GAM(E) CHANGER OR NOT? AN EVALUATION OF INTERPRETABLE MACHINE LEARNING MODELS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.14165.pdf&#34;&gt;GAM Coach: Towards Interactive and User-centered Algorithmic Recourse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.10332.pdf&#34;&gt;Revealing the Galaxy-Halo Connection Through Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.springer.com/content/pdf/10.1007/s40313-021-00858-y.pdf&#34;&gt;Explainable Artificial Intelligence for COVID-19 Diagnosis Through Blood Test Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-93736-2_40&#34;&gt;Using Explainable Boosting Machines (EBMs) to Detect Common Flaws in Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://assets.amazon.science/fa/3a/a62ba73f4bbda1d880b678c39193/differentially-private-gradient-boosting-on-linear-learners-for-tabular-data-analysis.pdf&#34;&gt;Differentially Private Gradient Boosting on Linear Learners for Tabular Data Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2214509523000244/pdfft?md5=171c275b6bcae8897cef03d931e908e2&amp;amp;pid=1-s2.0-S2214509523000244-main.pdf&#34;&gt;Concrete compressive strength prediction using an explainable boosting machine model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2301.04652.pdf&#34;&gt;Estimate Deformation Capacity of Non-Ductile RC Shear Walls Using Explainable Boosting Machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-15037-1_11&#34;&gt;Introducing the Rank-Biased Overlap as Similarity Measure for Feature Importance in Explainable Machine Learning: A Case Study on Parkinson’s Disease&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9768181/pdf/frai-05-1015604.pdf&#34;&gt;Targeting resources efficiently and justifiably by combining causal machine learning and theory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2212.10707.pdf&#34;&gt;Extractive Text Summarization Using Generalized Additive Models with Interactions for Sentence Selection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.medrxiv.org/content/medrxiv/early/2022/11/28/2022.04.30.22274520.full.pdf&#34;&gt;Death by Round Numbers: Glass-Box Machine Learning Uncovers Biases in Medical Practice&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cs.jhu.edu/~xzhan138/papers/BLACK2022.pdf&#34;&gt;Post-Hoc Interpretation of Transformer Hyperparameters with Explainable Boosting Machines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9771385/pdf/frai-05-1059033.pdf&#34;&gt;Interpretable machine learning for predicting pathologic complete response in patients treated with chemoradiation therapy for rectal adenocarcinoma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deliverypdf.ssrn.com/delivery.php?ID=998105006000069122073098120102102121021040051018055094125029122011041003059093125102072122106122077081069015087124028097016003127095087091028087010007035098086102086081014043013113004081117108011028041097095064071100112069081100069120077067116088100069070097093080074087115080072064086111126&amp;amp;EXT=pdf&amp;amp;INDEX=TRUE&#34;&gt;Exploring the Balance between Interpretability and Performance with carefully designed Constrainable Neural Additive Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2211.08991.pdf&#34;&gt;Estimating Discontinuous Time-Varying Risk Factors and Treatment Benefits for COVID-19 with Interpretable ML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.07723.pdf&#34;&gt;Pest Presence Prediction Using Interpretable Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.10.17.512613v1.full.pdf&#34;&gt;epitope1D: Accurate Taxonomy-Aware B-Cell Linear Epitope Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/4991/htm&#34;&gt;Explainable Boosting Machines for Slope Failure Spatial Predictive Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2109.13770.pdf&#34;&gt;Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.06.30.20143651v1.full.pdf&#34;&gt;Identifying main and interaction effects of risk factors to predict intensive care admission in patients hospitalized with COVID-19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deliverypdf.ssrn.com/delivery.php?ID=760122118067103094108090123091079011028032009009023085005014014002123105085114025022024005047078031019089073120012025117073002064031071072113006066035001068125027021087087083085026100009018045107092063001023068071002124070107120120007014102094103069089119026110104107005031095001092090&amp;amp;EXT=pdf&amp;amp;INDEX=TRUE&#34;&gt;Comparing the interpretability of machine learning classifiers for brain tumour survival prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2207.05322.pdf&#34;&gt;Using Interpretable Machine Learning to Predict Maternal and Fetal Outcomes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2207.13770.pdf&#34;&gt;Calibrate: Interactive Analysis of Probabilistic Model Output&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.13912.pdf&#34;&gt;Neural Additive Models: Interpretable Machine Learning with Neural Nets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.01613.pdf&#34;&gt;NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.14108v1.pdf&#34;&gt;Scalable Interpretability via Polynomials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.14120.pdf&#34;&gt;Neural Basis Models for Interpretability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2206.00473.pdf&#34;&gt;ILMART: Interpretable Ranking with Constrained LambdaMART&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9097874&#34;&gt;Integrating Co-Clustering and Interpretable Machine Learning for the Prediction of Intravenous Immunoglobulin Resistance in Kawasaki Disease&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2003.07132v1.pdf&#34;&gt;GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2208.08149.pdf&#34;&gt;A Concept and Argumentation based Interpretable Model in High Risk Domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3485447.3512277&#34;&gt;Analyzing the Differences between Professional and Amateur Esports through Win Probability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9132761/pdf/11682_2022_Article_688.pdf&#34;&gt;Explainable machine learning with pairwise interactions for the classifcation of Parkinson’s disease and SWEDD from clinical and imaging features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://statsbomb.com/wp-content/uploads/2019/10/decroos-interpretability-statsbomb.pdf&#34;&gt;Interpretable Prediction of Goals in Soccer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.05131.pdf&#34;&gt;Extending the Tsetlin Machine with Integer-Weighted Clauses for Increased Interpretability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.04176.pdf&#34;&gt;In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.04012.pdf&#34;&gt;From Shapley Values to Generalized Additive Models and back&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.12778.pdf&#34;&gt;An Explainable Machine Learning Approach to Visual-Interactive Labeling: A Case Study on Non-communicable Disease Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2021.11.01.21265700v1.full.pdf&#34;&gt;Development and Validation of an Interpretable 3-day Intensive Care Unit Readmission Prediction Model Using Explainable Boosting Machines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2022.04.30.22274520v1.full.pdf&#34;&gt;Death by Round Numbers and Sharp Thresholds: How to Avoid Dangerous AI EHR Recommendations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9037972/pdf/11517_2022_Article_2568.pdf&#34;&gt;Building a predictive model to identify clinical indicators for COVID-19 using machine learning method&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8777022/pdf/fcvm-08-797002.pdf&#34;&gt;Using Innovative Machine Learning Methods to Screen and Identify Predictors of Congenital Heart Diseases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86993-9_31&#34;&gt;Explainable Boosting Machine for Predicting Alzheimer’s Disease from MRI Hippocampal Subfields&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.09903.pdf&#34;&gt;Impact of Accuracy on Model Interpretations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Books that discuss EBMs&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Interpretable-Machine-Learning-Python-hands/dp/180020390X&#34;&gt;Interpretable Machine Learning with Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Explainable-Artificial-Intelligence_-An-Introduction-to-Interpretable-XAI/dp/3030833550&#34;&gt;Explainable Artificial Intelligence: An Introduction to Interpretable Machine Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/machine-learning-for/9781098102425/&#34;&gt;Machine Learning for High-Risk Applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154&#34;&gt;Applied Machine Learning Explainability Techniques&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/eXplainable-I-Python-examples-ebook/dp/B0B4F98MN6&#34;&gt;The eXplainable A.I.: With Python examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Explainable-Recipes-Implement-Explainability-Interpretability-ebook/dp/B0BSF5NBY7&#34;&gt;Explainable AI Recipes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;External tools&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/interpretml/ebm2onnx&#34;&gt;EBM to Onnx converter by SoftAtHome&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/interpretml/gam-changer&#34;&gt;GAM Changer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kaspersgit/ml_2_sql&#34;&gt;ML 2 SQL (experimental)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contact us&lt;/h1&gt; &#xA;&lt;p&gt;There are multiple ways to get in touch:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Email us at &lt;a href=&#34;mailto:interpret@microsoft.com&#34;&gt;interpret@microsoft.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Or, feel free to raise a GitHub issue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;If a tree fell in your random forest, would anyone notice?&lt;/h3&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>