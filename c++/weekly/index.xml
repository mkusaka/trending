<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-21T01:52:15Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rhasspy/piper</title>
    <updated>2024-01-21T01:52:15Z</updated>
    <id>tag:github.com,2024-01-21:/rhasspy/piper</id>
    <link href="https://github.com/rhasspy/piper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast, local neural text to speech system&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/etc/logo.png&#34; alt=&#34;Piper logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A fast, local neural text to speech system that sounds great and is optimized for the Raspberry Pi 4. Piper is used in a &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/#people-using-piper&#34;&gt;variety of projects&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;Welcome to the world of speech synthesis!&#39; | \&#xA;  ./piper --model en_US-lessac-medium.onnx --output_file welcome.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://rhasspy.github.io/piper-samples&#34;&gt;Listen to voice samples&lt;/a&gt; and check out a &lt;a href=&#34;https://youtu.be/rjq5eZoWWSo&#34;&gt;video tutorial by Thorsten Müller&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nabucasa.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/etc/nabu_casa_sponsored.png&#34; alt=&#34;Sponsored by Nabu Casa&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Voices are trained with &lt;a href=&#34;https://github.com/jaywalnut310/vits/&#34;&gt;VITS&lt;/a&gt; and exported to the &lt;a href=&#34;https://onnxruntime.ai/&#34;&gt;onnxruntime&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Voices&lt;/h2&gt; &#xA;&lt;p&gt;Our goal is to support Home Assistant and the &lt;a href=&#34;https://www.home-assistant.io/blog/2022/12/20/year-of-voice/&#34;&gt;Year of Voice&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/rhasspy/piper-voices/tree/v1.0.0&#34;&gt;Download voices&lt;/a&gt; for the supported languages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Arabic (ar_JO)&lt;/li&gt; &#xA; &lt;li&gt;Catalan (ca_ES)&lt;/li&gt; &#xA; &lt;li&gt;Czech (cs_CZ)&lt;/li&gt; &#xA; &lt;li&gt;Danish (da_DK)&lt;/li&gt; &#xA; &lt;li&gt;German (de_DE)&lt;/li&gt; &#xA; &lt;li&gt;Greek (el_GR)&lt;/li&gt; &#xA; &lt;li&gt;English (en_GB, en_US)&lt;/li&gt; &#xA; &lt;li&gt;Spanish (es_ES, es_MX)&lt;/li&gt; &#xA; &lt;li&gt;Finnish (fi_FI)&lt;/li&gt; &#xA; &lt;li&gt;French (fr_FR)&lt;/li&gt; &#xA; &lt;li&gt;Hungarian (hu_HU)&lt;/li&gt; &#xA; &lt;li&gt;Icelandic (is_IS)&lt;/li&gt; &#xA; &lt;li&gt;Italian (it_IT)&lt;/li&gt; &#xA; &lt;li&gt;Georgian (ka_GE)&lt;/li&gt; &#xA; &lt;li&gt;Kazakh (kk_KZ)&lt;/li&gt; &#xA; &lt;li&gt;Luxembourgish (lb_LU)&lt;/li&gt; &#xA; &lt;li&gt;Nepali (ne_NP)&lt;/li&gt; &#xA; &lt;li&gt;Dutch (nl_BE, nl_NL)&lt;/li&gt; &#xA; &lt;li&gt;Norwegian (no_NO)&lt;/li&gt; &#xA; &lt;li&gt;Polish (pl_PL)&lt;/li&gt; &#xA; &lt;li&gt;Portuguese (pt_BR, pt_PT)&lt;/li&gt; &#xA; &lt;li&gt;Romanian (ro_RO)&lt;/li&gt; &#xA; &lt;li&gt;Russian (ru_RU)&lt;/li&gt; &#xA; &lt;li&gt;Serbian (sr_RS)&lt;/li&gt; &#xA; &lt;li&gt;Swedish (sv_SE)&lt;/li&gt; &#xA; &lt;li&gt;Swahili (sw_CD)&lt;/li&gt; &#xA; &lt;li&gt;Turkish (tr_TR)&lt;/li&gt; &#xA; &lt;li&gt;Ukrainian (uk_UA)&lt;/li&gt; &#xA; &lt;li&gt;Vietnamese (vi_VN)&lt;/li&gt; &#xA; &lt;li&gt;Chinese (zh_CN)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will need two files per voice:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A &lt;code&gt;.onnx&lt;/code&gt; model file, such as &lt;a href=&#34;https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/lessac/medium/en_US-lessac-medium.onnx&#34;&gt;&lt;code&gt;en_US-lessac-medium.onnx&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A &lt;code&gt;.onnx.json&lt;/code&gt; config file, such as &lt;a href=&#34;https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/lessac/medium/en_US-lessac-medium.onnx.json&#34;&gt;&lt;code&gt;en_US-lessac-medium.onnx.json&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The &lt;code&gt;MODEL_CARD&lt;/code&gt; file for each voice contains important licensing information. Piper is intended for text to speech research, and does not impose any additional restrictions on voice models. Some voices may have restrictive licenses, however, so please review them carefully!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/#running-in-python&#34;&gt;run Piper with Python&lt;/a&gt; or download a binary release:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_amd64.tar.gz&#34;&gt;amd64&lt;/a&gt; (64-bit desktop Linux)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_arm64.tar.gz&#34;&gt;arm64&lt;/a&gt; (64-bit Raspberry Pi 4)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_armv7.tar.gz&#34;&gt;armv7&lt;/a&gt; (32-bit Raspberry Pi 3/4)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to build from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/Makefile&#34;&gt;Makefile&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/src/cpp&#34;&gt;C++ source&lt;/a&gt;. You must download and extract &lt;a href=&#34;https://github.com/rhasspy/piper-phonemize&#34;&gt;piper-phonemize&lt;/a&gt; to &lt;code&gt;lib/Linux-$(uname -m)/piper_phonemize&lt;/code&gt; before building. For example, &lt;code&gt;lib/Linux-x86_64/piper_phonemize/lib/libpiper_phonemize.so&lt;/code&gt; should exist for AMD/Intel machines (as well as everything else from &lt;code&gt;libpiper_phonemize-amd64.tar.gz&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/#voices&#34;&gt;Download a voice&lt;/a&gt; and extract the &lt;code&gt;.onnx&lt;/code&gt; and &lt;code&gt;.onnx.json&lt;/code&gt; files&lt;/li&gt; &#xA; &lt;li&gt;Run the &lt;code&gt;piper&lt;/code&gt; binary with text on standard input, &lt;code&gt;--model /path/to/your-voice.onnx&lt;/code&gt;, and &lt;code&gt;--output_file output.wav&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;Welcome to the world of speech synthesis!&#39; | \&#xA;  ./piper --model en_US-lessac-medium.onnx --output_file welcome.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For multi-speaker models, use &lt;code&gt;--speaker &amp;lt;number&amp;gt;&lt;/code&gt; to change speakers (default: 0).&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;piper --help&lt;/code&gt; for more options.&lt;/p&gt; &#xA;&lt;h3&gt;Streaming Audio&lt;/h3&gt; &#xA;&lt;p&gt;Piper can stream raw audio to stdout as its produced:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;This sentence is spoken first. This sentence is synthesized while the first sentence is spoken.&#39; | \&#xA;  ./piper --model en_US-lessac-medium.onnx --output-raw | \&#xA;  aplay -r 22050 -f S16_LE -t raw -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is &lt;strong&gt;raw&lt;/strong&gt; audio and not a WAV file, so make sure your audio player is set to play 16-bit mono PCM samples at the correct sample rate for the voice.&lt;/p&gt; &#xA;&lt;h3&gt;JSON Input&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;piper&lt;/code&gt; executable can accept JSON input when using the &lt;code&gt;--json-input&lt;/code&gt; flag. Each line of input must be a JSON object with &lt;code&gt;text&lt;/code&gt; field. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &#34;text&#34;: &#34;First sentence to speak.&#34; }&#xA;{ &#34;text&#34;: &#34;Second sentence to speak.&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optional fields include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;speaker&lt;/code&gt; - string &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name of the speaker to use from &lt;code&gt;speaker_id_map&lt;/code&gt; in config (multi-speaker voices only)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;speaker_id&lt;/code&gt; - number &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Id of speaker to use from 0 to number of speakers - 1 (multi-speaker voices only, overrides &#34;speaker&#34;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output_file&lt;/code&gt; - string &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Path to output WAV file&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following example writes two sentences with different speakers to different files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{ &#34;text&#34;: &#34;First speaker.&#34;, &#34;speaker_id&#34;: 0, &#34;output_file&#34;: &#34;/tmp/speaker_0.wav&#34; }&#xA;{ &#34;text&#34;: &#34;Second speaker.&#34;, &#34;speaker_id&#34;: 1, &#34;output_file&#34;: &#34;/tmp/speaker_1.wav&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;People using Piper&lt;/h2&gt; &#xA;&lt;p&gt;Piper has been used in the following projects/papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/home-assistant/addons/raw/master/piper/README.md&#34;&gt;Home Assistant&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rhasspy/rhasspy3/&#34;&gt;Rhasspy 3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvaccess.org/post/in-process-8th-may-2023/#voices&#34;&gt;NVDA - NonVisual Desktop Access&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.techrxiv.org/articles/preprint/Image_Captioning_for_the_Visually_Impaired_and_Blind_A_Recipe_for_Low-Resource_Languages/22133894&#34;&gt;Image Captioning for the Visually Impaired and Blind: A Recipe for Low-Resource Languages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenVoiceOS/ovos-tts-plugin-piper&#34;&gt;Open Voice Operating System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shahizat/jetsonGPT&#34;&gt;JetsonGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI&#34;&gt;LocalAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lernstick.ch/&#34;&gt;Lernstick EDU / EXAM: reading clipboard content aloud with language detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/TRAINING.md&#34;&gt;training guide&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/src/python&#34;&gt;source code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Pretrained checkpoints are available on &lt;a href=&#34;https://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main&#34;&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running in Python&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/rhasspy/piper/master/src/python_run&#34;&gt;src/python_run&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install piper-tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo &#39;Welcome to the world of speech synthesis!&#39; | piper \&#xA;  --model en_US-lessac-medium \&#xA;  --output_file welcome.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will automatically download &lt;a href=&#34;https://huggingface.co/rhasspy/piper-voices/tree/v1.0.0&#34;&gt;voice files&lt;/a&gt; the first time they&#39;re used. Use &lt;code&gt;--data-dir&lt;/code&gt; and &lt;code&gt;--download-dir&lt;/code&gt; to adjust where voices are found/downloaded.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to use a GPU, install the &lt;code&gt;onnxruntime-gpu&lt;/code&gt; package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;.venv/bin/pip3 install onnxruntime-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then run &lt;code&gt;piper&lt;/code&gt; with the &lt;code&gt;--cuda&lt;/code&gt; argument. You will need to have a functioning CUDA environment, such as what&#39;s available in &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch&#34;&gt;NVIDIA&#39;s PyTorch containers&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pybind/pybind11</title>
    <updated>2024-01-21T01:52:15Z</updated>
    <id>tag:github.com,2024-01-21:/pybind/pybind11</id>
    <link href="https://github.com/pybind/pybind11" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Seamless operability between C++11 and Python&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. figure:: &lt;a href=&#34;https://github.com/pybind/pybind11/raw/master/docs/pybind11-logo.png&#34;&gt;https://github.com/pybind/pybind11/raw/master/docs/pybind11-logo.png&lt;/a&gt; :alt: pybind11 logo&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pybind11 — Seamless operability between C++11 and Python&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;|Latest Documentation Status| |Stable Documentation Status| |Gitter chat| |GitHub Discussions| |CI| |Build status|&lt;/p&gt; &#xA;&lt;p&gt;|Repology| |PyPI package| |Conda-forge| |Python Versions|&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Setuptools example &amp;lt;https://github.com/pybind/python_example&amp;gt;&lt;/code&gt;_ • &lt;code&gt;Scikit-build example &amp;lt;https://github.com/pybind/scikit_build_example&amp;gt;&lt;/code&gt;_ • &lt;code&gt;CMake example &amp;lt;https://github.com/pybind/cmake_example&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;.. start&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pybind11&lt;/strong&gt; is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. Its goals and syntax are similar to the excellent &lt;code&gt;Boost.Python &amp;lt;http://www.boost.org/doc/libs/1_58_0/libs/python/doc/&amp;gt;&lt;/code&gt;_ library by David Abrahams: to minimize boilerplate code in traditional extension modules by inferring type information using compile-time introspection.&lt;/p&gt; &#xA;&lt;p&gt;The main issue with Boost.Python—and the reason for creating such a similar project—is Boost. Boost is an enormously large and complex suite of utility libraries that works with almost every C++ compiler in existence. This compatibility has its cost: arcane template tricks and workarounds are necessary to support the oldest and buggiest of compiler specimens. Now that C++11-compatible compilers are widely available, this heavy machinery has become an excessively large and unnecessary dependency.&lt;/p&gt; &#xA;&lt;p&gt;Think of this library as a tiny self-contained version of Boost.Python with everything stripped away that isn&#39;t relevant for binding generation. Without comments, the core header files only require ~4K lines of code and depend on Python (3.6+, or PyPy) and the C++ standard library. This compact implementation was possible thanks to some of the new C++11 language features (specifically: tuples, lambda functions and variadic templates). Since its creation, this library has grown beyond Boost.Python in many ways, leading to dramatically simpler binding code in many common situations.&lt;/p&gt; &#xA;&lt;p&gt;Tutorial and reference documentation is provided at &lt;code&gt;pybind11.readthedocs.io &amp;lt;https://pybind11.readthedocs.io/en/latest&amp;gt;&lt;/code&gt;&lt;em&gt;. A PDF version of the manual is available &lt;code&gt;here &amp;lt;https://pybind11.readthedocs.io/_/downloads/en/latest/pdf/&amp;gt;&lt;/code&gt;&lt;/em&gt;. And the source code is always available at &lt;code&gt;github.com/pybind/pybind11 &amp;lt;https://github.com/pybind/pybind11&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Core features&lt;/h2&gt; &#xA;&lt;p&gt;pybind11 can map the following core C++ features to Python:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Functions accepting and returning custom data structures per value, reference, or pointer&lt;/li&gt; &#xA; &lt;li&gt;Instance methods and static methods&lt;/li&gt; &#xA; &lt;li&gt;Overloaded functions&lt;/li&gt; &#xA; &lt;li&gt;Instance attributes and static attributes&lt;/li&gt; &#xA; &lt;li&gt;Arbitrary exception types&lt;/li&gt; &#xA; &lt;li&gt;Enumerations&lt;/li&gt; &#xA; &lt;li&gt;Callbacks&lt;/li&gt; &#xA; &lt;li&gt;Iterators and ranges&lt;/li&gt; &#xA; &lt;li&gt;Custom operators&lt;/li&gt; &#xA; &lt;li&gt;Single and multiple inheritance&lt;/li&gt; &#xA; &lt;li&gt;STL data structures&lt;/li&gt; &#xA; &lt;li&gt;Smart pointers with reference counting like &lt;code&gt;std::shared_ptr&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Internal references with correct reference counting&lt;/li&gt; &#xA; &lt;li&gt;C++ classes with virtual (and pure virtual) methods can be extended in Python&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Goodies&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the core functionality, pybind11 provides some extra goodies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Python 3.6+, and PyPy3 7.3 are supported with an implementation-agnostic interface (pybind11 2.9 was the last version to support Python 2 and 3.5).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It is possible to bind C++11 lambda functions with captured variables. The lambda capture data is stored inside the resulting Python function object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;pybind11 uses C++11 move constructors and move assignment operators whenever possible to efficiently transfer custom data types.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It&#39;s easy to expose the internal storage of custom data types through Pythons&#39; buffer protocols. This is handy e.g.&amp;nbsp;for fast conversion between C++ matrix classes like Eigen and NumPy without expensive copy operations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;pybind11 can automatically vectorize functions so that they are transparently applied to all entries of one or more NumPy array arguments.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Python&#39;s slice-based access and assignment operations can be supported with just a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Everything is contained in just a few header files; there is no need to link against any additional libraries.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Binaries are generally smaller by a factor of at least 2 compared to equivalent bindings generated by Boost.Python. A recent pybind11 conversion of PyRosetta, an enormous Boost.Python binding project, &lt;code&gt;reported &amp;lt;https://graylab.jhu.edu/Sergey/2016.RosettaCon/PyRosetta-4.pdf&amp;gt;&lt;/code&gt;_ a binary size reduction of &lt;strong&gt;5.4x&lt;/strong&gt; and compile time reduction by &lt;strong&gt;5.8x&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Function signatures are precomputed at compile time (using &lt;code&gt;constexpr&lt;/code&gt;), leading to smaller binaries.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;With little extra effort, C++ types can be pickled and unpickled similar to regular Python objects.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported compilers&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clang/LLVM 3.3 or newer (for Apple Xcode&#39;s clang, this is 5.0.0 or newer)&lt;/li&gt; &#xA; &lt;li&gt;GCC 4.8 or newer&lt;/li&gt; &#xA; &lt;li&gt;Microsoft Visual Studio 2017 or newer&lt;/li&gt; &#xA; &lt;li&gt;Intel classic C++ compiler 18 or newer (ICC 20.2 tested in CI)&lt;/li&gt; &#xA; &lt;li&gt;Cygwin/GCC (previously tested on 2.5.1)&lt;/li&gt; &#xA; &lt;li&gt;NVCC (CUDA 11.0 tested in CI)&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA PGI (20.9 tested in CI)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This project was created by &lt;code&gt;Wenzel Jakob &amp;lt;http://rgl.epfl.ch/people/wjakob&amp;gt;&lt;/code&gt;_. Significant features and/or improvements to the code were contributed by Jonas Adler, Lori A. Burns, Sylvain Corlay, Eric Cousineau, Aaron Gokaslan, Ralf Grosse-Kunstleve, Trent Houliston, Axel Huebl, @hulucc, Yannick Jadoul, Sergey Lyskov, Johan Mabille, Tomasz Miąsko, Dean Moldovan, Ben Pritchard, Jason Rhinelander, Boris Schäling, Pim Schellart, Henry Schreiner, Ivan Smirnov, Boris Staletic, and Patrick Stewart.&lt;/p&gt; &#xA;&lt;p&gt;We thank Google for a generous financial contribution to the continuous integration infrastructure used by this project.&lt;/p&gt; &#xA;&lt;p&gt;Contributing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;See the `contributing&#xA;guide &amp;lt;https://github.com/pybind/pybind11/blob/master/.github/CONTRIBUTING.md&amp;gt;`_&#xA;for information on building and contributing to pybind11.&#xA;&#xA;License&#xA;~~~~~~~&#xA;&#xA;pybind11 is provided under a BSD-style license that can be found in the&#xA;`LICENSE &amp;lt;https://github.com/pybind/pybind11/blob/master/LICENSE&amp;gt;`_&#xA;file. By using, distributing, or contributing to this project, you agree&#xA;to the terms and conditions of this license.&#xA;&#xA;.. |Latest Documentation Status| image:: https://readthedocs.org/projects/pybind11/badge?version=latest&#xA;   :target: http://pybind11.readthedocs.org/en/latest&#xA;.. |Stable Documentation Status| image:: https://img.shields.io/badge/docs-stable-blue.svg&#xA;   :target: http://pybind11.readthedocs.org/en/stable&#xA;.. |Gitter chat| image:: https://img.shields.io/gitter/room/gitterHQ/gitter.svg&#xA;   :target: https://gitter.im/pybind/Lobby&#xA;.. |CI| image:: https://github.com/pybind/pybind11/workflows/CI/badge.svg&#xA;   :target: https://github.com/pybind/pybind11/actions&#xA;.. |Build status| image:: https://ci.appveyor.com/api/projects/status/riaj54pn4h08xy40?svg=true&#xA;   :target: https://ci.appveyor.com/project/wjakob/pybind11&#xA;.. |PyPI package| image:: https://img.shields.io/pypi/v/pybind11.svg&#xA;   :target: https://pypi.org/project/pybind11/&#xA;.. |Conda-forge| image:: https://img.shields.io/conda/vn/conda-forge/pybind11.svg&#xA;   :target: https://github.com/conda-forge/pybind11-feedstock&#xA;.. |Repology| image:: https://repology.org/badge/latest-versions/python:pybind11.svg&#xA;   :target: https://repology.org/project/python:pybind11/versions&#xA;.. |Python Versions| image:: https://img.shields.io/pypi/pyversions/pybind11.svg&#xA;   :target: https://pypi.org/project/pybind11/&#xA;.. |GitHub Discussions| image:: https://img.shields.io/static/v1?label=Discussions&amp;amp;message=Ask&amp;amp;color=blue&amp;amp;logo=github&#xA;   :target: https://github.com/pybind/pybind11/discussions&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/faiss</title>
    <updated>2024-01-21T01:52:15Z</updated>
    <id>tag:github.com,2024-01-21:/facebookresearch/faiss</id>
    <link href="https://github.com/facebookresearch/faiss" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library for efficient similarity search and clustering of dense vectors.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Faiss&lt;/h1&gt; &#xA;&lt;p&gt;Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta&#39;s &lt;a href=&#34;https://ai.facebook.com/&#34;&gt;Fundamental AI Research&lt;/a&gt; group.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/faiss/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for detailed information about latest features.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.&lt;/p&gt; &#xA;&lt;p&gt;Some of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server. Other methods, like HNSW and NSG add an indexing structure on top of the raw vectors to make searching more efficient.&lt;/p&gt; &#xA;&lt;p&gt;The GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace &lt;code&gt;IndexFlatL2&lt;/code&gt; with &lt;code&gt;GpuIndexFlatL2&lt;/code&gt;) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported.&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;Faiss comes with precompiled libraries for Anaconda in Python, see &lt;a href=&#34;https://anaconda.org/pytorch/faiss-cpu&#34;&gt;faiss-cpu&lt;/a&gt; and &lt;a href=&#34;https://anaconda.org/pytorch/faiss-gpu&#34;&gt;faiss-gpu&lt;/a&gt;. The library is mostly implemented in C++, the only dependency is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&#34;&gt;BLAS&lt;/a&gt; implementation. Optional GPU support is provided via CUDA, and the Python interface is also optional. It compiles with cmake. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/faiss/main/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;How Faiss works&lt;/h2&gt; &#xA;&lt;p&gt;Faiss is built around an index type that stores a set of vectors, and provides a function to search in them with L2 and/or dot product vector comparison. Some index types are simple baselines, such as exact search. Most of the available indexing structures correspond to various trade-offs with respect to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;search time&lt;/li&gt; &#xA; &lt;li&gt;search quality&lt;/li&gt; &#xA; &lt;li&gt;memory used per index vector&lt;/li&gt; &#xA; &lt;li&gt;training time&lt;/li&gt; &#xA; &lt;li&gt;adding time&lt;/li&gt; &#xA; &lt;li&gt;need for external data for unsupervised training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The optional GPU implementation provides what is likely (as of March 2017) the fastest exact and approximate (compressed-domain) nearest neighbor search implementation for high-dimensional vectors, fastest Lloyd&#39;s k-means, and fastest small k-selection algorithm known. &lt;a href=&#34;https://arxiv.org/abs/1702.08734&#34;&gt;The implementation is detailed here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Full documentation of Faiss&lt;/h2&gt; &#xA;&lt;p&gt;The following are entry points for documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the full documentation can be found on the &lt;a href=&#34;http://github.com/facebookresearch/faiss/wiki&#34;&gt;wiki page&lt;/a&gt;, including a &lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Getting-started&#34;&gt;tutorial&lt;/a&gt;, a &lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; and a &lt;a href=&#34;https://github.com/facebookresearch/faiss/wiki/Troubleshooting&#34;&gt;troubleshooting section&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;the &lt;a href=&#34;https://faiss.ai/&#34;&gt;doxygen documentation&lt;/a&gt; gives per-class information extracted from code comments&lt;/li&gt; &#xA; &lt;li&gt;to reproduce results from our research papers, &lt;a href=&#34;https://arxiv.org/abs/1609.01882&#34;&gt;Polysemous codes&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1702.08734&#34;&gt;Billion-scale similarity search with GPUs&lt;/a&gt;, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/faiss/main/benchs/README.md&#34;&gt;benchmarks README&lt;/a&gt;. For &lt;a href=&#34;https://arxiv.org/abs/1804.09996&#34;&gt; Link and code: Fast indexing with graphs and compact regression codes&lt;/a&gt;, see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/faiss/main/benchs/link_and_code&#34;&gt;link_and_code README&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;The main authors of Faiss are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jegou&#34;&gt;Hervé Jégou&lt;/a&gt; initiated the Faiss project and wrote its first implementation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mdouze&#34;&gt;Matthijs Douze&lt;/a&gt; implemented most of the CPU Faiss&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wickedfoo&#34;&gt;Jeff Johnson&lt;/a&gt; implemented all of the GPU Faiss&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/beauby&#34;&gt;Lucas Hosseini&lt;/a&gt; implemented the binary indexes and the build system&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KinglittleQ&#34;&gt;Chengqi Deng&lt;/a&gt; implemented NSG, NNdescent and much of the additive quantization code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexanderguzhva&#34;&gt;Alexandr Guzhva&lt;/a&gt; many optimizations: SIMD, memory allocation and layout, fast decoding kernels for vector codecs, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/algoriddle&#34;&gt;Gergely Szilvasy&lt;/a&gt; build system, benchmarking framework.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;References to cite when you use Faiss in a research paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{douze2024faiss,&#xA;      title={The Faiss library},&#xA;      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},&#xA;      year={2024},&#xA;      eprint={2401.08281},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the GPU version of Faiss, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{johnson2019billion,&#xA;  title={Billion-scale similarity search with {GPUs}},&#xA;  author={Johnson, Jeff and Douze, Matthijs and J{\&#39;e}gou, Herv{\&#39;e}},&#xA;  journal={IEEE Transactions on Big Data},&#xA;  volume={7},&#xA;  number={3},&#xA;  pages={535--547},&#xA;  year={2019},&#xA;  publisher={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Join the Faiss community&lt;/h2&gt; &#xA;&lt;p&gt;For public discussion of Faiss or for questions, there is a Facebook group at &lt;a href=&#34;https://www.facebook.com/groups/faissusers/&#34;&gt;https://www.facebook.com/groups/faissusers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We monitor the &lt;a href=&#34;http://github.com/facebookresearch/faiss/issues&#34;&gt;issues page&lt;/a&gt; of the repository. You can report bugs, ask questions, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Legal&lt;/h2&gt; &#xA;&lt;p&gt;Faiss is MIT-licensed, refer to the &lt;a href=&#34;https://github.com/facebookresearch/faiss/raw/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt; in the top level directory.&lt;/p&gt; &#xA;&lt;p&gt;Copyright © Meta Platforms, Inc. See the &lt;a href=&#34;https://opensource.fb.com/legal/terms/&#34;&gt;Terms of Use&lt;/a&gt; and &lt;a href=&#34;https://opensource.fb.com/legal/privacy/&#34;&gt;Privacy Policy&lt;/a&gt; for this project.&lt;/p&gt;</summary>
  </entry>
</feed>