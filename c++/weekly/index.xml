<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-27T01:51:19Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>leejet/stable-diffusion.cpp</title>
    <updated>2023-08-27T01:51:19Z</updated>
    <id>tag:github.com,2023-08-27:/leejet/stable-diffusion.cpp</id>
    <link href="https://github.com/leejet/stable-diffusion.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion in pure C/C++&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/a%20lovely%20cat.png&#34; width=&#34;256x&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;stable-diffusion.cpp&lt;/h1&gt; &#xA;&lt;p&gt;Inference of &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; in pure C/C++&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation based on &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;, working in the same way as &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16-bit, 32-bit float support&lt;/li&gt; &#xA; &lt;li&gt;4-bit, 5-bit and 8-bit integer quantization support&lt;/li&gt; &#xA; &lt;li&gt;Accelerated memory-efficient CPU inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Only requires ~2.3GB when using txt2img with fp16 precision to generate a 512x512 image&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;AVX, AVX2 and AVX512 support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;Original &lt;code&gt;txt2img&lt;/code&gt; and &lt;code&gt;img2img&lt;/code&gt; mode&lt;/li&gt; &#xA; &lt;li&gt;Negative prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt; style tokenizer (not all the features, only token weighting for now)&lt;/li&gt; &#xA; &lt;li&gt;Sampling method &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;Euler A&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Supported platforms &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux&lt;/li&gt; &#xA;   &lt;li&gt;Mac OS&lt;/li&gt; &#xA;   &lt;li&gt;Windows&lt;/li&gt; &#xA;   &lt;li&gt;Android (via Termux)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TODO&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More sampling methods&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GPU support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make inference faster &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The current implementation of ggml_conv_2d is slow and has high memory usage&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Continuing to reduce memory usage (quantizing the weights of ggml_conv_2d)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LoRA support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; k-quants support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cross-platform reproducibility (perhaps ensuring consistency with the original SD)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Adapting to more weight formats&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/leejet/stable-diffusion.cpp&#xA;cd stable-diffusion.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have already cloned the repository, you can use the following command to update the repository to the latest code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd stable-diffusion.cpp&#xA;git pull origin master&#xA;git submodule init&#xA;git submodule update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Convert weights&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;download original weights(.ckpt or .safetensors). For example&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Stable Diffusion v1.4 from &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&#34;&gt;https://huggingface.co/CompVis/stable-diffusion-v-1-4-original&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Stable Diffusion v1.5 from &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;https://huggingface.co/runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -L -O https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt&#xA;# curl -L -O https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;convert weights to ggml model format&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd models&#xA;pip install -r requirements.txt&#xA;python convert.py [path to weights] --out_type [output precision]&#xA;# For example, python convert.py sd-v1-4.ckpt --out_type f16&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can specify the output model format using the --out_type parameter&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;f16&lt;/code&gt; for 16-bit floating-point&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;f32&lt;/code&gt; for 32-bit floating-point&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q8_0&lt;/code&gt; for 8-bit integer quantization&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q5_0&lt;/code&gt; or &lt;code&gt;q5_1&lt;/code&gt; for 5-bit integer quantization&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;q4_0&lt;/code&gt; or &lt;code&gt;q4_1&lt;/code&gt; for 4-bit integer quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;h4&gt;Build from scratch&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Using OpenBLAS&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake .. -DGGML_OPENBLAS=ON&#xA;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: ./bin/sd [arguments]&#xA;&#xA;arguments:&#xA;  -h, --help                         show this help message and exit&#xA;  -M, --mode [txt2img or img2img]    generation mode (default: txt2img)&#xA;  -t, --threads N                    number of threads to use during computation (default: -1).&#xA;                                     If threads &amp;lt;= 0, then threads will be set to the number of CPU physical cores&#xA;  -m, --model [MODEL]                path to model&#xA;  -i, --init-img [IMAGE]             path to the input image, required by img2img&#xA;  -o, --output OUTPUT                path to write result image to (default: .\output.png)&#xA;  -p, --prompt [PROMPT]              the prompt to render&#xA;  -n, --negative-prompt PROMPT       the negative prompt (default: &#34;&#34;)&#xA;  --cfg-scale SCALE                  unconditional guidance scale: (default: 7.0)&#xA;  --strength STRENGTH                strength for noising/unnoising (default: 0.75)&#xA;                                     1.0 corresponds to full destruction of information in init image&#xA;  -H, --height H                     image height, in pixel space (default: 512)&#xA;  -W, --width W                      image width, in pixel space (default: 512)&#xA;  --sample-method SAMPLE_METHOD      sample method (default: &#34;eular a&#34;)&#xA;  --steps  STEPS                     number of sample steps (default: 20)&#xA;  -s SEED, --seed SEED               RNG seed (default: 42, use random seed for &amp;lt; 0)&#xA;  -v, --verbose                      print extra info&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;txt2img example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;./bin/sd -m ../models/sd-v1-4-ggml-model-f16.bin -p &#34;a lovely cat&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using formats of different precisions will yield results of varying quality.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;f32&lt;/th&gt; &#xA;   &lt;th&gt;f16&lt;/th&gt; &#xA;   &lt;th&gt;q8_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_1&lt;/th&gt; &#xA;   &lt;th&gt;q4_0&lt;/th&gt; &#xA;   &lt;th&gt;q4_1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/f32.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/f16.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q8_0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q5_0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q5_1.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q4_0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/q4_1.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;img2img example&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;./output.png&lt;/code&gt; is the image generated from the above txt2img pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;./bin/sd --mode img2img -m ../models/sd-v1-4-ggml-model-f16.bin -p &#34;cat with blue eyes&#34; -i ./output.png -o ./img2img_output.png --strength 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/leejet/stable-diffusion.cpp/master/assets/img2img_output.png&#34; width=&#34;256x&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;h4&gt;Building using Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build -t sd .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -v /path/to/models:/models -v /path/to/output/:/output sd [args...]&#xA;# For example&#xA;# docker run -v ./models:/models -v ./build:/output sd -m /models/sd-v1-4-ggml-model-f16.bin -p &#34;a lovely cat&#34; -v -o /output/output.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Memory/Disk Requirements&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;f32&lt;/th&gt; &#xA;   &lt;th&gt;f16&lt;/th&gt; &#xA;   &lt;th&gt;q8_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_0&lt;/th&gt; &#xA;   &lt;th&gt;q5_1&lt;/th&gt; &#xA;   &lt;th&gt;q4_0&lt;/th&gt; &#xA;   &lt;th&gt;q4_1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Disk&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.7G&lt;/td&gt; &#xA;   &lt;td&gt;2.0G&lt;/td&gt; &#xA;   &lt;td&gt;1.7G&lt;/td&gt; &#xA;   &lt;td&gt;1.6G&lt;/td&gt; &#xA;   &lt;td&gt;1.6G&lt;/td&gt; &#xA;   &lt;td&gt;1.5G&lt;/td&gt; &#xA;   &lt;td&gt;1.5G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Memory&lt;/strong&gt;(txt2img - 512 x 512)&lt;/td&gt; &#xA;   &lt;td&gt;~2.8G&lt;/td&gt; &#xA;   &lt;td&gt;~2.3G&lt;/td&gt; &#xA;   &lt;td&gt;~2.1G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;   &lt;td&gt;~2.0G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;k-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aseprite/aseprite</title>
    <updated>2023-08-27T01:51:19Z</updated>
    <id>tag:github.com,2023-08-27:/aseprite/aseprite</id>
    <link href="https://github.com/aseprite/aseprite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Animated sprite editor &amp; pixel art tool (Windows, macOS, Linux)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Aseprite&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aseprite/aseprite/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/aseprite/aseprite/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://community.aseprite.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discourse-community-brightgreen.svg?style=flat&#34; alt=&#34;Discourse Community&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/Yb2CeX8&#34;&gt;&lt;img src=&#34;https://discordapp.com/api/guilds/324979738533822464/embed.png&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aseprite&lt;/strong&gt; is a program to create animated sprites. Its main features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sprites are composed of &lt;a href=&#34;https://www.aseprite.org/docs/timeline/&#34;&gt;layers &amp;amp; frames&lt;/a&gt; as separated concepts.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://www.aseprite.org/docs/color-profile/&#34;&gt;color profiles&lt;/a&gt; and different &lt;a href=&#34;https://www.aseprite.org/docs/color-mode/&#34;&gt;color modes&lt;/a&gt;: RGBA, Indexed (palettes up to 256 colors), Grayscale.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aseprite.org/docs/animation/&#34;&gt;Animation facilities&lt;/a&gt;, with real-time &lt;a href=&#34;https://www.aseprite.org/docs/preview-window/&#34;&gt;preview&lt;/a&gt; and &lt;a href=&#34;https://www.aseprite.org/docs/onion-skinning/&#34;&gt;onion skinning&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aseprite.org/docs/exporting/&#34;&gt;Export/import&lt;/a&gt; animations to/from &lt;a href=&#34;https://www.aseprite.org/docs/sprite-sheet/&#34;&gt;sprite sheets&lt;/a&gt;, GIF files, or sequence of PNG files (and FLC, FLI, JPG, BMP, PCX, TGA).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aseprite.org/docs/workspace/#drag-and-drop-tabs&#34;&gt;Multiple editors&lt;/a&gt; support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imgur.com/x3OKkGj&#34;&gt;Layer groups&lt;/a&gt; for organizing your work, and &lt;a href=&#34;https://twitter.com/aseprite/status/806889204601016325&#34;&gt;reference layers&lt;/a&gt; for rotoscoping.&lt;/li&gt; &#xA; &lt;li&gt;Pixel-art specific tools like &lt;a href=&#34;https://imgur.com/0fdlNau&#34;&gt;Pixel Perfect freehand mode&lt;/a&gt;, &lt;a href=&#34;https://www.aseprite.org/docs/shading/&#34;&gt;Shading ink&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/aseprite/status/1196883990080344067&#34;&gt;Custom Brushes&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/aseprite/status/1126548469865431041&#34;&gt;Outlines&lt;/a&gt;, &lt;a href=&#34;https://imgur.com/1yZKUcs&#34;&gt;Wide Pixels&lt;/a&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;Other special drawing tools like &lt;a href=&#34;https://twitter.com/aseprite/status/1253770784708886533&#34;&gt;Pressure sensitivity&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/aseprite/status/659709226747625472&#34;&gt;Symmetry Tool&lt;/a&gt;, &lt;a href=&#34;https://imgur.com/7JZQ81o&#34;&gt;Stroke and Fill&lt;/a&gt; selection, &lt;a href=&#34;https://twitter.com/aseprite/status/1126549217856622597&#34;&gt;Gradients&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/pixel__toast/status/1132079817736695808&#34;&gt;Tiled mode&lt;/a&gt; useful to draw patterns and textures.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/aseprite/status/1170007034651172866&#34;&gt;Transform multiple frames/layers&lt;/a&gt; at the same time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aseprite.org/docs/scripting/&#34;&gt;Lua scripting capabilities&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aseprite.org/docs/cli/&#34;&gt;CLI - Command Line Interface&lt;/a&gt; to automatize tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aseprite.org/quickref/&#34;&gt;Quick Reference / Cheat Sheet&lt;/a&gt; keyboard shortcuts (&lt;a href=&#34;https://imgur.com/rvAUxyF&#34;&gt;customizable keys&lt;/a&gt; and &lt;a href=&#34;https://imgur.com/oNqFqVb&#34;&gt;mouse wheel&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/aseprite/status/1202641475256881153&#34;&gt;Reopen closed files&lt;/a&gt; and &lt;a href=&#34;https://www.aseprite.org/docs/data-recovery/&#34;&gt;recover data&lt;/a&gt; in case of crash.&lt;/li&gt; &#xA; &lt;li&gt;Undo/Redo for every operation and support for &lt;a href=&#34;https://imgur.com/9I42fZK&#34;&gt;non-linear undo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/aseprite/status/1124442198651678720&#34;&gt;More features &amp;amp; tips&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;There is a list of &lt;a href=&#34;https://github.com/aseprite/aseprite/issues&#34;&gt;Known Issues&lt;/a&gt; (things to be fixed or that aren&#39;t yet implemented).&lt;/p&gt; &#xA;&lt;p&gt;If you found a bug or have a new idea/feature for the program, &lt;a href=&#34;https://github.com/aseprite/aseprite/issues/new&#34;&gt;you can report them&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;You can ask for help in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://community.aseprite.org/&#34;&gt;Aseprite Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/Yb2CeX8&#34;&gt;Aseprite Discord Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Official support: &lt;a href=&#34;mailto:support@aseprite.org&#34;&gt;support@aseprite.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Social networks and community-driven places: &lt;a href=&#34;https://twitter.com/aseprite/&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://facebook.com/aseprite/&#34;&gt;Facebook&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/user/aseprite&#34;&gt;YouTube&lt;/a&gt;, &lt;a href=&#34;https://www.instagram.com/aseprite/&#34;&gt;Instagram&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;Aseprite is being developed by &lt;a href=&#34;https://igara.com/&#34;&gt;Igara Studio&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://davidcapello.com/&#34;&gt;David Capello&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Gasparoken&#34;&gt;Gaspar Capello&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/martincapello&#34;&gt;Martín Capello&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;The default Aseprite theme was introduced in v0.8, created by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ilkke.net/&#34;&gt;Ilija Melentijevic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A modified dark version of this theme introduced in v1.3-beta1 was created by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/MapleGecko&#34;&gt;Nicolas Desilets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/davidcapello&#34;&gt;David Capello&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Aseprite includes color palettes created by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pixeljoint.com/p/23821.htm&#34;&gt;Richard &#34;DawnBringer&#34; Fhager&lt;/a&gt;, &lt;a href=&#34;http://pixeljoint.com/forum/forum_posts.asp?TID=12795&#34;&gt;16 colors&lt;/a&gt;, &lt;a href=&#34;http://pixeljoint.com/forum/forum_posts.asp?TID=16247&#34;&gt;32 colors&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://androidarts.com/&#34;&gt;Arne Niklas Jansson&lt;/a&gt;, &lt;a href=&#34;http://androidarts.com/palette/16pal.htm&#34;&gt;16 colors&lt;/a&gt;, &lt;a href=&#34;http://wayofthepixel.net/index.php?topic=15824.msg144494&#34;&gt;32 colors&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/ENDESGA&#34;&gt;ENDESGA Studios&lt;/a&gt;, &lt;a href=&#34;https://forums.tigsource.com/index.php?topic=46126.msg1279124#msg1279124&#34;&gt;EDG16 and EDG32&lt;/a&gt;, and &lt;a href=&#34;https://twitter.com/ENDESGA/status/865812366931353600&#34;&gt;other palettes&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/Hyohnoo&#34;&gt;Hyohnoo Games&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/Hyohnoo/status/797472587974639616&#34;&gt;mail24&lt;/a&gt; palette.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/DavitMasia&#34;&gt;Davit Masia&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/DavitMasia/status/834862452164612096&#34;&gt;matriax8c&lt;/a&gt; palette.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/Xavier_Gd&#34;&gt;Javier Guerrero&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/Xavier_Gd/status/868519467864686594&#34;&gt;nyx8&lt;/a&gt; palette.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/adigunpolack&#34;&gt;Adigun A. Polack&lt;/a&gt;, &lt;a href=&#34;http://pixeljoint.com/pixelart/119466.htm&#34;&gt;AAP-64&lt;/a&gt;, &lt;a href=&#34;http://pixeljoint.com/pixelart/120714.htm&#34;&gt;AAP-Splendor128&lt;/a&gt;, &lt;a href=&#34;http://pixeljoint.com/pixelart/119844.htm&#34;&gt;SimpleJPC-16&lt;/a&gt;, and &lt;a href=&#34;http://pixeljoint.com/pixelart/121151.htm&#34;&gt;AAP-Micro12&lt;/a&gt; palette.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PineTreePizza&#34;&gt;PineTreePizza&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/PineTreePizza/status/1006536191955623938&#34;&gt;Rosy-42&lt;/a&gt; palette.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It tries to replicate some pixel-art algorithms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://forums.sonicretro.org/index.php?showtopic=8848&amp;amp;st=15&amp;amp;p=159754&amp;amp;#entry159754&#34;&gt;RotSprite&lt;/a&gt; by Xenowhirl.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deepnight.net/blog/tools/pixel-perfect-drawing/&#34;&gt;Pixel perfect drawing algorithm&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/deepnightfr&#34;&gt;Sébastien Bénard&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/CarduusHimself/status/420554200737935361&#34;&gt;Carduus&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://raw.githubusercontent.com/aseprite/aseprite/main/docs/LICENSES.md&#34;&gt;third-party open source projects&lt;/a&gt;, to &lt;a href=&#34;https://www.aseprite.org/contributors/&#34;&gt;contributors&lt;/a&gt;, and all the people who have contributed ideas, patches, bugs report, feature requests, donations, and help me to develop Aseprite.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This program is distributed under three different licenses:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Source code and official releases/binaries are distributed under our &lt;a href=&#34;https://raw.githubusercontent.com/aseprite/aseprite/main/EULA.txt&#34;&gt;End-User License Agreement for Aseprite (EULA)&lt;/a&gt;. Please check that there are &lt;a href=&#34;https://raw.githubusercontent.com/aseprite/aseprite/main/src/README.md&#34;&gt;modules/libraries in the source code&lt;/a&gt; that are distributed under the MIT license (e.g. &lt;a href=&#34;https://github.com/aseprite/laf&#34;&gt;laf&lt;/a&gt;, &lt;a href=&#34;https://github.com/aseprite/clip&#34;&gt;clip&lt;/a&gt;, &lt;a href=&#34;https://github.com/aseprite/undo&#34;&gt;undo&lt;/a&gt;, &lt;a href=&#34;https://github.com/aseprite/observable&#34;&gt;observable&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/aseprite/aseprite/main/src/ui&#34;&gt;ui&lt;/a&gt;, etc.).&lt;/li&gt; &#xA; &lt;li&gt;You can request a special &lt;a href=&#34;https://www.aseprite.org/faq/#is-there-an-educational-license&#34;&gt;educational license&lt;/a&gt; in case you are a teacher in an educational institution and want to use Aseprite in your classroom (in-situ).&lt;/li&gt; &#xA; &lt;li&gt;Steam releases are distributed under the terms of the &lt;a href=&#34;http://store.steampowered.com/subscriber_agreement/&#34;&gt;Steam Subscriber Agreement&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can get more information about Aseprite license in the &lt;a href=&#34;https://www.aseprite.org/faq/#licensing-&amp;amp;-commercial&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alibaba/GraphScope</title>
    <updated>2023-08-27T01:51:19Z</updated>
    <id>tag:github.com,2023-08-27:/alibaba/GraphScope</id>
    <link href="https://github.com/alibaba/GraphScope" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🔨 🍇 💻 🚀 GraphScope: A One-Stop Large-Scale Graph Computing System from Alibaba 来自阿里巴巴的一站式大规模图计算系统 图分析 图查询 图机器学习&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://graphscope.io/assets/images/graphscope-logo.svg?sanitize=true&#34; width=&#34;400&#34; alt=&#34;graphscope-logo&#34;&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A One-Stop Large-Scale Graph Computing System from Alibaba &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/alibaba/GraphScope/actions/workflows/local-ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/alibaba/GraphScope/actions/workflows/local-ci.yml/badge.svg?sanitize=true&#34; alt=&#34;GraphScope CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/alibaba/GraphScope&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/alibaba/GraphScope/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://try.graphscope.app&#34;&gt;&lt;img src=&#34;https://shields.io/badge/JupyterLab-Try%20GraphScope%20Now!-F37626?logo=jupyter&#34; alt=&#34;Playground&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/alibaba/GraphScope&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://artifacthub.io/packages/helm/graphscope/graphscope&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/graphscope&#34; alt=&#34;Artifact HUB&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://graphscope.io/docs&#34;&gt;&lt;img src=&#34;https://shields.io/badge/Docs-English-blue?logo=Read%20The%20Docs&#34; alt=&#34;Docs-en&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://graphscope.io/docs/frequently_asked_questions.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-FAQ-blue?logo=Read%20The%20Docs&#34; alt=&#34;FAQ-en&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://graphscope.io/docs/zh/&#34;&gt;&lt;img src=&#34;https://shields.io/badge/Docs-%E4%B8%AD%E6%96%87-blue?logo=Read%20The%20Docs&#34; alt=&#34;Docs-zh&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://graphscope.io/docs/zh/frequently_asked_questions.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-FAQ%E4%B8%AD%E6%96%87-blue?logo=Read%20The%20Docs&#34; alt=&#34;FAQ-zh&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/README-zh.md&#34;&gt;&lt;img src=&#34;https://shields.io/badge/README-%E4%B8%AD%E6%96%87-blue&#34; alt=&#34;README-zh&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dl.acm.org/doi/10.14778/3476311.3476369&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ACM%20DL-10.14778%2F3476311.3476369-blue&#34; alt=&#34;ACM DL&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🎉 See our ongoing &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/flex/&#34;&gt;GraphScope Flex&lt;/a&gt;: a LEGO-inspired, modular, and user-friendly GraphScope evolution. 🎉&lt;/p&gt; &#xA;&lt;p&gt;GraphScope is a unified distributed graph computing platform that provides a one-stop environment for performing diverse graph operations on a cluster of computers through a user-friendly Python interface. GraphScope makes multi-staged processing of large-scale graph data on compute clusters simply by combining several important pieces of Alibaba technology: including &lt;a href=&#34;https://github.com/alibaba/libgrape-lite&#34;&gt;GRAPE&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/interactive_engine/&#34;&gt;MaxGraph&lt;/a&gt;, and &lt;a href=&#34;https://github.com/alibaba/graph-learn&#34;&gt;Graph-Learn&lt;/a&gt; (GL) for analytics, interactive, and graph neural networks (GNN) computation, respectively, and the &lt;a href=&#34;https://github.com/v6d-io/v6d&#34;&gt;Vineyard&lt;/a&gt; store that offers efficient in-memory data transfers.&lt;/p&gt; &#xA;&lt;p&gt;Visit our website at &lt;a href=&#34;https://graphscope.io&#34;&gt;graphscope.io&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[20/07/2023] GraphScope achieved record-breaking results on the &lt;a href=&#34;https://ldbcouncil.org/benchmarks/snb-interactive/&#34;&gt;LDBC Social Network Benchmark Interactive workload&lt;/a&gt;, with a 2.45× higher throughput on SF300 than the previous record holder! 🏆&lt;/li&gt; &#xA; &lt;li&gt;[04/07/2023] GraphScope Flex tech preview released with &lt;a href=&#34;https://github.com/alibaba/GraphScope/releases/tag/v0.23.0&#34;&gt;v0.23.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#installation-for-standalone-mode&#34;&gt;Installation for Standalone Mode&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#demo-node-classification-on-citation-network&#34;&gt;Demo: Node Classification on Citation Network&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#loading-a-graph&#34;&gt;Loading a graph&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#interactive-query&#34;&gt;Interactive query&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#graph-analytics&#34;&gt;Graph analytics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#graph-neural-networks-gnns&#34;&gt;Graph neural networks (GNNs)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#processing-large-graph-on-kubernetes-cluster&#34;&gt;Graph Processing on Kubernetes&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#creating-a-session&#34;&gt;Creating a session&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#loading-a-graph-and-processing-computation-tasks&#34;&gt;Loading graphs and graph computation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#closing-the-session&#34;&gt;Closing the session&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#development&#34;&gt;Development&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#building-on-local&#34;&gt;Building from source&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#building-docker-images&#34;&gt;Building Docker images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#building-client-library&#34;&gt;Building the client library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#publications&#34;&gt;Publications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/#contributing&#34;&gt;Joining our Community!&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://try.graphscope.app&#34;&gt;Playground&lt;/a&gt; with a managed JupyterLab. &lt;a href=&#34;https://try.graphscope.app&#34;&gt;Try GraphScope&lt;/a&gt; straight away in your browser!&lt;/p&gt; &#xA;&lt;p&gt;GraphScope supports running in standalone mode or on clusters managed by &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; within containers. For quickly getting started, let&#39;s begin with the standalone mode.&lt;/p&gt; &#xA;&lt;h3&gt;Installation for Standalone Mode&lt;/h3&gt; &#xA;&lt;p&gt;GraphScope pre-compiled package is distributed as a python package and can be easily installed with &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install graphscope&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;graphscope&lt;/code&gt; requires &lt;code&gt;Python&lt;/code&gt; &amp;gt;= &lt;code&gt;3.8&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; &amp;gt;= &lt;code&gt;19.3&lt;/code&gt;. The package is built for and tested on the most popular Linux (Ubuntu 20.04+ / CentOS 7+) and macOS 11+ (Intel) / macOS 12+ (Apple silicon) distributions. For Windows users, you may want to install Ubuntu on WSL2 to use this package.&lt;/p&gt; &#xA;&lt;p&gt;Next, we will walk you through a concrete example to illustrate how GraphScope can be used by data scientists to effectively analyze large graphs.&lt;/p&gt; &#xA;&lt;h2&gt;Demo: Node Classification on Citation Network&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ogb.stanford.edu/docs/nodeprop/#ogbn-mag&#34;&gt;&lt;code&gt;ogbn-mag&lt;/code&gt;&lt;/a&gt; is a heterogeneous network composed of a subset of the Microsoft Academic Graph. It contains 4 types of entities(i.e., papers, authors, institutions, and fields of study), as well as four types of directed relations connecting two entities.&lt;/p&gt; &#xA;&lt;p&gt;Given the heterogeneous &lt;code&gt;ogbn-mag&lt;/code&gt; data, the task is to predict the class of each paper. Node classification can identify papers in multiple venues, which represent different groups of scientific work on different topics. We apply both the attribute and structural information to classify papers. In the graph, each paper node contains a 128-dimensional word2vec vector representing its content, which is obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are pre-trained. The structural information is computed on-the-fly.&lt;/p&gt; &#xA;&lt;h3&gt;Loading a graph&lt;/h3&gt; &#xA;&lt;p&gt;GraphScope models graph data as property graph, in which the edges/vertices are labeled and have many properties. Taking &lt;code&gt;ogbn-mag&lt;/code&gt; as example, the figure below shows the model of the property graph.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://graphscope.io/docs/_images/sample_pg.png&#34; width=&#34;600&#34; alt=&#34;sample-of-property-graph&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This graph has four kinds of vertices, labeled as &lt;code&gt;paper&lt;/code&gt;, &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;institution&lt;/code&gt; and &lt;code&gt;field_of_study&lt;/code&gt;. There are four kinds of edges connecting them, each kind of edges has a label and specifies the vertex labels for its two ends. For example, &lt;code&gt;cites&lt;/code&gt; edges connect two vertices labeled &lt;code&gt;paper&lt;/code&gt;. Another example is &lt;code&gt;writes&lt;/code&gt;, it requires the source vertex is labeled &lt;code&gt;author&lt;/code&gt; and the destination is a &lt;code&gt;paper&lt;/code&gt; vertex. All the vertices and edges may have properties. e.g., &lt;code&gt;paper&lt;/code&gt; vertices have properties like features, publish year, subject label, etc.&lt;/p&gt; &#xA;&lt;p&gt;To load this graph to GraphScope with our retrieval module, please use these code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import graphscope&#xA;from graphscope.dataset import load_ogbn_mag&#xA;&#xA;g = load_ogbn_mag()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a set of functions to load graph datasets from &lt;a href=&#34;https://ogb.stanford.edu/docs/dataset_overview/&#34;&gt;ogb&lt;/a&gt; and &lt;a href=&#34;https://snap.stanford.edu/data/index.html&#34;&gt;snap&lt;/a&gt; for convenience. Please find all the available graphs &lt;a href=&#34;https://github.com/alibaba/GraphScope/tree/docs/python/graphscope/dataset&#34;&gt;here&lt;/a&gt;. If you want to use your own graph data, please refer &lt;a href=&#34;https://graphscope.io/docs/loading_graph.html&#34;&gt;this doc&lt;/a&gt; to load vertices and edges by labels.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive query&lt;/h3&gt; &#xA;&lt;p&gt;Interactive queries allow users to directly explore, examine, and present graph data in an &lt;em&gt;exploratory&lt;/em&gt; manner in order to locate specific or in-depth information in time. GraphScope adopts a high-level language called &lt;a href=&#34;http://tinkerpop.apache.org/&#34;&gt;Gremlin&lt;/a&gt; for graph traversal, and provides &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/interactive_engine/benchmark/&#34;&gt;efficient execution&lt;/a&gt; at scale.&lt;/p&gt; &#xA;&lt;p&gt;In this example, we use graph traversal to count the number of papers two given authors have co-authored. To simplify the query, we assume the authors can be uniquely identified by ID &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;4307&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# get the endpoint for submitting Gremlin queries on graph g.&#xA;interactive = graphscope.gremlin(g)&#xA;&#xA;# count the number of papers two authors (with id 2 and 4307) have co-authored&#xA;papers = interactive.execute(&#34;g.V().has(&#39;author&#39;, &#39;id&#39;, 2).out(&#39;writes&#39;).where(__.in(&#39;writes&#39;).has(&#39;id&#39;, 4307)).count()&#34;).one()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Graph analytics&lt;/h3&gt; &#xA;&lt;p&gt;Graph analytics is widely used in real world. Many algorithms, like community detection, paths and connectivity, centrality are proven to be very useful in various businesses. GraphScope ships with a set of &lt;a href=&#34;https://graphscope.io/docs/analytics_engine.html#built-in-algorithms&#34;&gt;built-in algorithms&lt;/a&gt;, enables users easily analysis their graph data.&lt;/p&gt; &#xA;&lt;p&gt;Continuing our example, below we first derive a subgraph by extracting publications in specific time out of the entire graph (using Gremlin!), and then run k-core decomposition and triangle counting to generate the structural features of each paper node.&lt;/p&gt; &#xA;&lt;p&gt;Please note that many algorithms may only work on &lt;em&gt;homogeneous&lt;/em&gt; graphs, and therefore, to evaluate these algorithms over a property graph, we need to project it into a simple graph at first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# extract a subgraph of publication within a time range&#xA;sub_graph = interactive.subgraph(&#34;g.V().has(&#39;year&#39;, gte(2014).and(lte(2020))).outE(&#39;cites&#39;)&#34;)&#xA;&#xA;# project the projected graph to simple graph.&#xA;simple_g = sub_graph.project(vertices={&#34;paper&#34;: []}, edges={&#34;cites&#34;: []})&#xA;&#xA;ret1 = graphscope.k_core(simple_g, k=5)&#xA;ret2 = graphscope.triangles(simple_g)&#xA;&#xA;# add the results as new columns to the citation graph&#xA;sub_graph = sub_graph.add_column(ret1, {&#34;kcore&#34;: &#34;r&#34;})&#xA;sub_graph = sub_graph.add_column(ret2, {&#34;tc&#34;: &#34;r&#34;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition, users can write their own algorithms in GraphScope. Currently, GraphScope supports users to write their own algorithms in Pregel model and PIE model.&lt;/p&gt; &#xA;&lt;h3&gt;Graph neural networks (GNNs)&lt;/h3&gt; &#xA;&lt;p&gt;Graph neural networks (GNNs) combines superiority of both graph analytics and machine learning. GNN algorithms can compress both structural and attribute information in a graph into low-dimensional embedding vectors on each node. These embeddings can be further fed into downstream machine learning tasks.&lt;/p&gt; &#xA;&lt;p&gt;In our example, we train a GCN model to classify the nodes (papers) into 349 categories, each of which represents a venue (e.g. pre-print and conference). To achieve this, first we launch a learning engine and build a graph with features following the last step.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;# define the features for learning&#xA;paper_features = [f&#34;feat_{i}&#34; for i in range(128)]&#xA;&#xA;paper_features.append(&#34;kcore&#34;)&#xA;paper_features.append(&#34;tc&#34;)&#xA;&#xA;# launch a learning engine.&#xA;lg = graphscope.graphlearn(sub_graph, nodes=[(&#34;paper&#34;, paper_features)],&#xA;                  edges=[(&#34;paper&#34;, &#34;cites&#34;, &#34;paper&#34;)],&#xA;                  gen_labels=[&#xA;                      (&#34;train&#34;, &#34;paper&#34;, 100, (0, 75)),&#xA;                      (&#34;val&#34;, &#34;paper&#34;, 100, (75, 85)),&#xA;                      (&#34;test&#34;, &#34;paper&#34;, 100, (85, 100))&#xA;                  ])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then we define the training process, and run it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Note: Here we use tensorflow as NN backend to train GNN model. so please&#xA;# install tensorflow.&#xA;try:&#xA;    # https://www.tensorflow.org/guide/migrate&#xA;    import tensorflow.compat.v1 as tf&#xA;    tf.disable_v2_behavior()&#xA;except ImportError:&#xA;    import tensorflow as tf&#xA;&#xA;import graphscope.learning&#xA;from graphscope.learning.examples import EgoGraphSAGE&#xA;from graphscope.learning.examples import EgoSAGESupervisedDataLoader&#xA;from graphscope.learning.examples.tf.trainer import LocalTrainer&#xA;&#xA;# supervised GCN.&#xA;def train_gcn(graph, node_type, edge_type, class_num, features_num,&#xA;              hops_num=2, nbrs_num=[25, 10], epochs=2,&#xA;              hidden_dim=256, in_drop_rate=0.5, learning_rate=0.01,&#xA;):&#xA;    graphscope.learning.reset_default_tf_graph()&#xA;&#xA;    dimensions = [features_num] + [hidden_dim] * (hops_num - 1) + [class_num]&#xA;    model = EgoGraphSAGE(dimensions, act_func=tf.nn.relu, dropout=in_drop_rate)&#xA;&#xA;    # prepare train dataset&#xA;    train_data = EgoSAGESupervisedDataLoader(&#xA;        graph, graphscope.learning.Mask.TRAIN,&#xA;        node_type=node_type, edge_type=edge_type, nbrs_num=nbrs_num, hops_num=hops_num,&#xA;    )&#xA;    train_embedding = model.forward(train_data.src_ego)&#xA;    train_labels = train_data.src_ego.src.labels&#xA;    loss = tf.reduce_mean(&#xA;        tf.nn.sparse_softmax_cross_entropy_with_logits(&#xA;            labels=train_labels, logits=train_embedding,&#xA;        )&#xA;    )&#xA;    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)&#xA;&#xA;    # prepare test dataset&#xA;    test_data = EgoSAGESupervisedDataLoader(&#xA;        graph, graphscope.learning.Mask.TEST,&#xA;        node_type=node_type, edge_type=edge_type, nbrs_num=nbrs_num, hops_num=hops_num,&#xA;    )&#xA;    test_embedding = model.forward(test_data.src_ego)&#xA;    test_labels = test_data.src_ego.src.labels&#xA;    test_indices = tf.math.argmax(test_embedding, 1, output_type=tf.int32)&#xA;    test_acc = tf.div(&#xA;        tf.reduce_sum(tf.cast(tf.math.equal(test_indices, test_labels), tf.float32)),&#xA;        tf.cast(tf.shape(test_labels)[0], tf.float32),&#xA;    )&#xA;&#xA;    # train and test&#xA;    trainer = LocalTrainer()&#xA;    trainer.train(train_data.iterator, loss, optimizer, epochs=epochs)&#xA;    trainer.test(test_data.iterator, test_acc)&#xA;&#xA;train_gcn(lg, node_type=&#34;paper&#34;, edge_type=&#34;cites&#34;,&#xA;          class_num=349,  # output dimension&#xA;          features_num=130,  # input dimension, 128 + kcore + triangle count&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A Python script with the entire process is available &lt;a href=&#34;https://colab.research.google.com/github/alibaba/GraphScope/blob/main/tutorials/1_node_classification_on_citation.ipynb&#34;&gt;here&lt;/a&gt;, you may try it out by yourself.&lt;/p&gt; &#xA;&lt;h2&gt;Processing Large Graph on Kubernetes Cluster&lt;/h2&gt; &#xA;&lt;p&gt;GraphScope is designed for processing large graphs, which are usually hard to fit in the memory of a single machine. With &lt;a href=&#34;https://github.com/v6d-io/v6d&#34;&gt;Vineyard&lt;/a&gt; as the distributed in-memory data manager, GraphScope supports running on a cluster managed by Kubernetes(k8s).&lt;/p&gt; &#xA;&lt;p&gt;To continue this tutorial, please ensure that you have a k8s-managed cluster and know the credentials for the cluster. (e.g., address of k8s API server, usually stored a &lt;code&gt;~/.kube/config&lt;/code&gt; file.)&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can set up a local k8s cluster for testing with &lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;Kind&lt;/a&gt;. We provide a script for setup this environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for usage, type -h&#xA;./scripts/install_deps.sh --k8s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you did not install the &lt;code&gt;graphscope&lt;/code&gt; package in the above step, you can install a subset of the whole package with client functions only.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install graphscope-client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, let&#39;s revisit the example by running on a cluster instead.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://graphscope.io/docs/_images/how-it-works.png&#34; width=&#34;600&#34; alt=&#34;how-it-works&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The figure shows the flow of execution in the cluster mode. When users run code in the python client, it will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Step 1&lt;/em&gt;. Create a session or workspace in GraphScope.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Step 2 - Step 5&lt;/em&gt;. Load a graph, query, analysis and run learning task on this graph via Python interface. These steps are the same to local mode, thus users process huge graphs in a distributed setting just like analysis a small graph on a single machine.(Note that &lt;code&gt;graphscope.gremlin&lt;/code&gt; and &lt;code&gt;graphscope.graphlearn&lt;/code&gt; need to be changed to &lt;code&gt;sess.gremlin&lt;/code&gt; and &lt;code&gt;sess.graphlearn&lt;/code&gt;, respectively. &lt;code&gt;sess&lt;/code&gt; is the name of the &lt;code&gt;Session&lt;/code&gt; instance user created.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Step 6&lt;/em&gt;. Close the session.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Creating a session&lt;/h3&gt; &#xA;&lt;p&gt;To use GraphScope in a distributed setting, we need to establish a session in a python interpreter.&lt;/p&gt; &#xA;&lt;p&gt;For convenience, we provide several demo datasets, and an option &lt;code&gt;with_dataset&lt;/code&gt; to mount the dataset in the graphscope cluster. The datasets will be mounted to &lt;code&gt;/dataset&lt;/code&gt; in the pods. If you want to use your own data on k8s cluster, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/docs/deployment.rst&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import graphscope&#xA;&#xA;sess = graphscope.session(with_dataset=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For macOS, the session needs to establish with the LoadBalancer service type (which is NodePort by default).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import graphscope&#xA;&#xA;sess = graphscope.session(with_dataset=True, k8s_service_type=&#34;LoadBalancer&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A session tries to launch a &lt;code&gt;coordinator&lt;/code&gt;, which is the entry for the back-end engines. The coordinator manages a cluster of resources (k8s pods), and the interactive/analytical/learning engines ran on them. For each pod in the cluster, there is a vineyard instance at service for distributed data in memory.&lt;/p&gt; &#xA;&lt;h3&gt;Loading a graph and processing computation tasks&lt;/h3&gt; &#xA;&lt;p&gt;Similar to the standalone mode, we can still use the functions to load a graph easily.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from graphscope.dataset import load_ogbn_mag&#xA;&#xA;# Note we have mounted the demo datasets to /dataset,&#xA;# There are several datasets including ogbn_mag_small,&#xA;# User can attach to the engine container and explore the directory.&#xA;g = load_ogbn_mag(sess, &#34;/dataset/ogbn_mag_small/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, the &lt;code&gt;g&lt;/code&gt; is loaded in parallel via vineyard and stored in vineyard instances in the cluster managed by the session.&lt;/p&gt; &#xA;&lt;p&gt;Next, we can conduct graph queries with Gremlin, invoke various graph algorithms, or run graph-based neural network tasks like we did in the standalone mode. We do not repeat code here, but a &lt;code&gt;.ipynb&lt;/code&gt; processing the classification task on k8s is available on the &lt;a href=&#34;https://try.graphscope.app/&#34;&gt;Playground&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Closing the session&lt;/h3&gt; &#xA;&lt;p&gt;Another additional step in the distribution is session close. We close the session after processing all graph tasks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sess.close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This operation will notify the backend engines and vineyard to safely unload graphs and their applications, Then, the coordinator will release all the applied resources in the k8s cluster.&lt;/p&gt; &#xA;&lt;p&gt;Please note that we have not hardened this release for production use and it lacks important security features such as authentication and encryption, and therefore &lt;strong&gt;it is NOT recommended for production use (yet)!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Building on local&lt;/h3&gt; &#xA;&lt;p&gt;To build graphscope Python package and the engine binaries, you need to install some dependencies and build tools.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./gs install-deps dev&#xA;&#xA;# With argument --cn to speed up the download if you are in China.&#xA;./gs install-deps dev --cn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can build GraphScope with pre-configured &lt;code&gt;make&lt;/code&gt; commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# to make graphscope whole package, including python package + engine binaries.&#xA;sudo make install&#xA;&#xA;# or make the engine components&#xA;# make interactive&#xA;# make analytical&#xA;# make learning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building Docker images&lt;/h3&gt; &#xA;&lt;p&gt;GraphScope ships with a &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/GraphScope/main/k8s/dockerfiles/graphscope-dev.Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; that can build docker images for releasing. The images are built on a &lt;code&gt;builder&lt;/code&gt; image with all dependencies installed and copied to a &lt;code&gt;runtime-base&lt;/code&gt; image. To build images with latest version of GraphScope, go to the &lt;code&gt;k8s/internal&lt;/code&gt; directory under root directory and run this command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# by default, the built image is tagged as graphscope/graphscope:SHORTSHA&#xA;# cd k8s&#xA;make graphscope&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building client library&lt;/h3&gt; &#xA;&lt;p&gt;GraphScope python interface is separate with the engines image. If you are developing python client and not modifying the protobuf files, the engines image doesn&#39;t require to be rebuilt.&lt;/p&gt; &#xA;&lt;p&gt;You may want to re-install the python client on local.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the learning engine client has C/C++ extensions modules and setting up the build environment is a bit tedious. By default the locally-built client library doesn&#39;t include the support for learning engine. If you want to build client library with learning engine enabled, please refer &lt;a href=&#34;https://graphscope.io/docs/developer_guide.html#build-python-wheels&#34;&gt;Build Python Wheels&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;To verify the correctness of your developed features, your code changes should pass our tests.&lt;/p&gt; &#xA;&lt;p&gt;You may run the whole test suite with commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Documentation can be generated using Sphinx. Users can build the documentation using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# build the docs&#xA;make graphscope-docs&#xA;&#xA;# to open preview on local&#xA;open docs/_build/latest/html/index.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The latest version of online documentation can be found at &lt;a href=&#34;https://graphscope.io/docs&#34;&gt;https://graphscope.io/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;GraphScope is released under &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License 2.0&lt;/a&gt;. Please note that third-party libraries may not have the same license as GraphScope.&lt;/p&gt; &#xA;&lt;h2&gt;Publications&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wenfei Fan, Tao He, Longbin Lai, Xue Li, Yong Li, Zhao Li, Zhengping Qian, Chao Tian, Lei Wang, Jingbo Xu, Youyang Yao, Qiang Yin, Wenyuan Yu, Jingren Zhou, Diwen Zhu, Rong Zhu. &lt;a href=&#34;http://vldb.org/pvldb/vol14/p2879-qian.pdf&#34;&gt;GraphScope: A Unified Engine For Big Graph Processing&lt;/a&gt;. The 47th International Conference on Very Large Data Bases (VLDB), industry, 2021.&lt;/li&gt; &#xA; &lt;li&gt;Jingbo Xu, Zhanning Bai, Wenfei Fan, Longbin Lai, Xue Li, Zhao Li, Zhengping Qian, Lei Wang, Yanyan Wang, Wenyuan Yu, Jingren Zhou. &lt;a href=&#34;http://vldb.org/pvldb/vol14/p2703-xu.pdf&#34;&gt;GraphScope: A One-Stop Large Graph Processing System&lt;/a&gt;. The 47th International Conference on Very Large Data Bases (VLDB), demo, 2021&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you use this software, please cite our paper using the following metadata:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{fan2021graphscope,&#xA;  title={GraphScope: a unified engine for big graph processing},&#xA;  author={Fan, Wenfei and He, Tao and Lai, Longbin and Li, Xue and Li, Yong and Li, Zhao and Qian, Zhengping and Tian, Chao and Wang, Lei and Xu, Jingbo and others},&#xA;  journal={Proceedings of the VLDB Endowment},&#xA;  volume={14},&#xA;  number={12},&#xA;  pages={2879--2892},&#xA;  year={2021},&#xA;  publisher={VLDB Endowment}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join in the &lt;a href=&#34;http://slack.graphscope.io&#34;&gt;Slack channel&lt;/a&gt; for discussion.&lt;/li&gt; &#xA; &lt;li&gt;Please report bugs by submitting a GitHub issue.&lt;/li&gt; &#xA; &lt;li&gt;Please submit contributions using pull requests.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>