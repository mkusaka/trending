<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-09T01:49:25Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VowpalWabbit/vowpal_wabbit</title>
    <updated>2023-04-09T01:49:25Z</updated>
    <id>tag:github.com,2023-04-09:/VowpalWabbit/vowpal_wabbit</id>
    <link href="https://github.com/VowpalWabbit/vowpal_wabbit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VowpalWabbit/vowpal_wabbit/master/logo_assets/vowpal-wabbits-github-logo@3x.png&#34; height=&#34;auto&#34; width=&#34;100%&#34; alt=&#34;Vowpal Wabbit&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/vowpalwabbit/Vowpal%20Wabbit/_build/latest?definitionId=23&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://img.shields.io/azure-devops/build/vowpalwabbit/3934113c-9e2b-4dbc-8972-72ab9b9b4342/23?label=Linux%20build&amp;amp;logo=Azure%20Devops&#34; alt=&#34;Linux build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/vowpalwabbit/Vowpal%20Wabbit/_build/latest?definitionId=14&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://img.shields.io/azure-devops/build/vowpalwabbit/3934113c-9e2b-4dbc-8972-72ab9b9b4342/14?label=Windows%20build&amp;amp;logo=Azure%20Devops&#34; alt=&#34;Windows build status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/VowpalWabbit/vowpal_wabbit&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/VowpalWabbit/vowpal_wabbit/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/JohnLangford/vowpal_wabbit/alerts/&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/alerts/g/JohnLangford/vowpal_wabbit.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Total Alerts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the &lt;em&gt;Vowpal Wabbit&lt;/em&gt; fast online learning code.&lt;/p&gt; &#xA;&lt;h2&gt;Why Vowpal Wabbit?&lt;/h2&gt; &#xA;&lt;p&gt;Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning. There is a specific focus on reinforcement learning with several contextual bandit algorithms implemented and the online nature lending to the problem well. Vowpal Wabbit is a destination for implementing and maturing state of the art algorithms with performance in mind.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Input Format.&lt;/strong&gt; The input format for the learning algorithm is substantially more flexible than might be expected. Examples can have features consisting of free form text, which is interpreted in a bag-of-words way. There can even be multiple sets of free form text in different namespaces.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speed.&lt;/strong&gt; The learning algorithm is fast -- similar to the few other online algorithm implementations out there. There are several optimization algorithms available with the baseline being sparse gradient descent (GD) on a loss function.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalability.&lt;/strong&gt; This is not the same as fast. Instead, the important characteristic here is that the memory footprint of the program is bounded independent of data. This means the training set is not loaded into main memory before learning starts. In addition, the size of the set of features is bounded independent of the amount of training data using the hashing trick.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature Interaction.&lt;/strong&gt; Subsets of features can be internally paired so that the algorithm is linear in the cross-product of the subsets. This is useful for ranking problems. The alternative of explicitly expanding the features before feeding them into the learning algorithm can be both computation and space intensive, depending on how it&#39;s handled.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit/wiki&#34;&gt;Visit the wiki to learn more.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;For the most up to date instructions for getting started on Windows, MacOS or Linux &lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Getting-started&#34;&gt;please see the wiki&lt;/a&gt;. This includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Getting-started&#34;&gt;Installing with a package manager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Tutorial&#34;&gt;Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>wangzhaode/ChatGLM-MNN</title>
    <updated>2023-04-09T01:49:25Z</updated>
    <id>tag:github.com,2023-04-09:/wangzhaode/ChatGLM-MNN</id>
    <link href="https://github.com/wangzhaode/ChatGLM-MNN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pure C++, Easy Deploy ChatGLM-6B.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM-MNN&lt;/h1&gt; &#xA;&lt;h2&gt;Describe&lt;/h2&gt; &#xA;&lt;p&gt;该项目将模型&lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;ChatGLM-6B&lt;/a&gt;转换到&lt;code&gt;MNN&lt;/code&gt;并使用&lt;code&gt;C++&lt;/code&gt;进行推理。 在实现上做了如下优化：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;对其中的词表做了部分删减，删除了模型中未使用的前20000个图片词；将&lt;code&gt;vocab&lt;/code&gt;, &lt;code&gt;embedding&lt;/code&gt;, &lt;code&gt;lm_head&lt;/code&gt;的大小从150528缩小到130528;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Embedding&lt;/code&gt;操作调用次数较少，使用&lt;code&gt;fseek&lt;/code&gt;, &lt;code&gt;fread&lt;/code&gt;加载的方式降低内存;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lm_head&lt;/code&gt;操作为&lt;code&gt;[num, 4096] @ [4096, 130528]&lt;/code&gt;，转换为&lt;code&gt;[130528, 4096] @ [4096, 1]&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;原模型对显存要求较高；将模型按层拆分成28个模型，可以根据用户显存大小动态将计算任务分配给GPU和CPU，充分利用GPU与CPU内存与算力; 即使小显存显卡也可以加速生成。&lt;/li&gt; &#xA; &lt;li&gt;针对端侧设备可以逐次加载计算，&lt;code&gt;2G&lt;/code&gt;内存的Android设备也可以执行推理（速度较慢）。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;现已提供&lt;a href=&#34;https://raw.githubusercontent.com/wangzhaode/ChatGLM-MNN/master/export_script&#34;&gt;ONNX导出方法&lt;/a&gt;和模型，可以使用其他框架部署；&lt;a href=&#34;https://github.com/wangzhaode/ChatGLM-MNN/releases/tag/v0.4&#34;&gt;点击下载&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;目前支持命令行对话与Web UI对话两种形式的Demo &lt;img src=&#34;https://raw.githubusercontent.com/wangzhaode/ChatGLM-MNN/master/resource/web/web_demo.png&#34; alt=&#34;web_demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Speed&lt;/h2&gt; &#xA;&lt;p&gt;移动端：将分段模型逐个加载推理可以在内存大小大于&lt;code&gt;2G&lt;/code&gt;的设备执行推理，实验性测试性能较差；目前性能约为：63 &lt;code&gt;s/word&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;PC测试平台：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Memory: 32G (+32G Swap)&lt;/li&gt; &#xA; &lt;li&gt;CPU: AMD Ryzen 9 3900X 12-Core Processor&lt;/li&gt; &#xA; &lt;li&gt;GPU: GeForce RTX 2080 Ti&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;FP Model&lt;/h3&gt; &#xA;&lt;p&gt;仅测试浮点模型(CPU: fp32/ GPU: fp16)，输入&lt;code&gt;你好&lt;/code&gt;，在回复完内容相同的情况下，平均生成一个词语的时间(&lt;code&gt;s/word&lt;/code&gt;)对比如下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;impl&lt;/th&gt; &#xA;   &lt;th&gt;GPU + CPU&lt;/th&gt; &#xA;   &lt;th&gt;CPU only&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MNN&lt;/td&gt; &#xA;   &lt;td&gt;0.292&lt;/td&gt; &#xA;   &lt;td&gt;0.877&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pytorch&lt;/td&gt; &#xA;   &lt;td&gt;out of memory&lt;/td&gt; &#xA;   &lt;td&gt;1.344&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Quantize Model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;TODO&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;1. Compile MNN library&lt;/h3&gt; &#xA;&lt;p&gt;从源码编译MNN&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/alibaba/MNN.git&#xA;cd MNN&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake .. # CPU only&#xA;cmake .. -DMNN_CUDA=ON # using CUDA&#xA;cmake .. -DMNN_OPENCL=ON -DMNN_USE_SYSTEM_LIB=ON -DMNN_SEP_BUILD=OFF # using OPENCL&#xA;make -j8&#xA;cp -r ../include /path/to/ChatGLM-MNN/&#xA;cp libMNN.so /path/to/ChatGLM-MNN/libs&#xA;cp express/libMNN_Express.so /path/to/ChatGLM-MNN/libs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Download Models&lt;/h3&gt; &#xA;&lt;p&gt;从 &lt;code&gt;github release&lt;/code&gt; 下载模型文件到 &lt;code&gt;/path/to/ChatGLM-MNN/resource/models&lt;/code&gt;， 如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd resource/models&#xA;./download_models_fp16.sh # 下载fp16权值模型&#xA;./download_models_int8.sh # 下载int8权值模型，推荐使用&#xA;./download_models_int4.sh # 下载int4权值模型，有精度损失&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Build and Run&lt;/h3&gt; &#xA;&lt;p&gt;Mac/Linux/Windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j8&#xA;./cli_demo # cli demo&#xA;./web_demo # web ui demo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Android:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir build&#xA;cd build&#xA;../android_build.sh&#xA;make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Using GPU&lt;/h4&gt; &#xA;&lt;p&gt;默认用法为使用&lt;code&gt;CPU&lt;/code&gt;, 使用&lt;code&gt;NVIDIA GPU&lt;/code&gt;需要在编译MNN时添加宏&lt;code&gt;-DMNN_CUDA=ON&lt;/code&gt;，使用其他GPU需要在编译MNN时添加宏&lt;code&gt;-DMNN_OPENCL=ON -DMNN_USE_SYSTEM_LIB=ON -DMNN_SEP_BUILD=OFF&lt;/code&gt;； 在创建&lt;code&gt;ChatGLM&lt;/code&gt;时指定显存大小，如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// 8G CUDA Memory&#xA;ChatGLM chatglm(8);&#xA;// 命令式&#xA;std::cout &amp;lt;&amp;lt; chatglm.response(&#34;你好&#34;);&#xA;// 交互式&#xA;chatglm.chat();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;Q: 用Python帮我写一段快速排序代码&lt;/p&gt; &#xA;&lt;p&gt;A: 当然可以!以下是使用Python实现快速排序算法的示例代码:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quick_sort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    else:&#xA;        pivot = arr[0]&#xA;        left = []&#xA;        right = []&#xA;        for i in range(1, len(arr)):&#xA;            if arr[i] &amp;lt; pivot:&#xA;                left.append(arr[i])&#xA;            else:&#xA;                right.append(arr[i])&#xA;        return quick_sort(left) + [pivot] + quick_sort(right)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这个实现中,我们首先检查传入的数组长度是否小于等于1。如果是,我们直接返回该数组,因为这种情况下快速排序是不稳定的。否则,我们选择数组的第一个元素作为基准点(pivot),并将数组分为两个部分:左边的部分的所有元素都小于基准点,右边的部分的所有元素都大于基准点。然后我们递归地对左边和右边的部分执行快速排序,并将结果拼接起来。最后,我们递归地对左右两个部分执行相同的操作,并返回结果。&lt;/p&gt; &#xA;&lt;p&gt;你可以像这样调用这个函数:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;arr = [3, 6, 1, 9, 2, 5, 8, 4]&#xA;sorted_arr = quick_sort(arr)&#xA;print(sorted_arr)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这将输出 &lt;code&gt;[1, 2, 3, 4, 5, 6, 7, 8, 9]&lt;/code&gt;,表示快速排序已成功地对数组进行了排序。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Q: 内存泄露如何定位&lt;/p&gt; &#xA;&lt;p&gt;A: 内存泄漏(Memory Leak)是指程序在运行过程中,占用的内存空间不断增加,最终导致程序无法继续运行,或者运行效率降低。&lt;/p&gt; &#xA;&lt;p&gt;以下是一些常见的内存泄漏检查方法:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;使用内存分析工具:例如Eclipse Memory Analyzer、VisualVM等,这些工具可以帮助检测程序中未释放的内存空间,以及内存分配的异常。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;检查变量的生命周期:如果一个变量在它的生命周期内没有被释放,那么它的内存空间就会被一直占用,最终导致内存泄漏。可以通过查看变量的生命周期信息来检查是否存在这个问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;检查代码中的异常:如果程序在执行过程中出现了异常,而没有释放所占用的内存空间,也会导致内存泄漏。可以通过调试程序,查找并释放这些内存空间。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;优化代码:如果程序中的代码存在内存泄漏问题,那么可能是因为程序的性能问题或者内存占用过高导致的。可以通过优化代码,减少内存分配和释放的次数,来解决这个问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;定期清理无用的数据:如果程序中存在大量的无用数据,也会导致内存泄漏。可以通过定期清理这些数据,释放内存空间。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;内存泄漏问题的解决需要从多个方面入手,通过不断地调试和优化程序,来找到内存泄漏的根本原因,并有效地解决问题。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Q: 北京有什么特色小吃？&lt;/p&gt; &#xA;&lt;p&gt;A: 北京是中国历史文化名城,也是中国美食之都之一,有许多特色小吃。以下是一些著名的北京特色小吃:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;炸酱面:炸酱面是中国传统面食之一,以黄酱和肉末为主要材料,配以豆瓣酱、黄瓜丝和豆芽等配料,味道鲜美。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;烤鸭:烤鸭是北京最著名的美食之一,以薄饼和鸭肉为主要材料,烤制过程中还会加入葱、姜等调料,口感鲜美。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;豆汁:豆汁是一种传统的北京小吃,以黄豆为主要原料,配以辣椒油、醋、蒜泥等调料,味道酸甜可口。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;羊蝎子:羊蝎子是一道以羊肉和羊肝为主要材料的炖菜,口感鲜美,营养丰富。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;糖葫芦:糖葫芦是一种传统的北京小吃,以草莓、山楂等水果为主料,沾上糖浆,口感酸甜可口。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;煎饼果子:煎饼果子是一种流行的中式早餐,以薄饼和蛋、肉松、油条等为主要材料,口感酥脆。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;驴打滚:驴打滚是一种传统的北京小吃,以糯米粉和豆沙为主要材料,通过卷起来和炸的方式制作,口感香甜。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;这只是北京众多特色小吃中的一小部分,北京还有很多其他美食,如北京火锅、北京炸酱面、北京小吃街等等,值得一试。&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yhirose/cpp-httplib&#34;&gt;cpp-httplib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xqdoo00o/chatgpt-web&#34;&gt;chatgpt-web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yanyiwu/cppjieba&#34;&gt;cppjieba&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Alexays/Waybar</title>
    <updated>2023-04-09T01:49:25Z</updated>
    <id>tag:github.com,2023-04-09:/Alexays/Waybar</id>
    <link href="https://github.com/Alexays/Waybar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Highly customizable Wayland bar for Sway and Wlroots based compositors. ✌️ 🎉&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Waybar &lt;a href=&#34;https://raw.githubusercontent.com/Alexays/Waybar/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;Licence&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paypal.me/ARouillard&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Donate-Paypal-2244dd.svg?sanitize=true&#34; alt=&#34;Paypal Donate&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alexays/waybar/master/preview-2.png&#34; alt=&#34;Waybar&#34;&gt;&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Highly customizable Wayland bar for Sway and Wlroots based compositors.&lt;br&gt; Available in Arch &lt;a href=&#34;https://www.archlinux.org/packages/community/x86_64/waybar/&#34;&gt;community&lt;/a&gt; or &lt;a href=&#34;https://aur.archlinux.org/packages/waybar-git/&#34;&gt;AUR&lt;/a&gt;, &lt;a href=&#34;https://packages.gentoo.org/packages/gui-apps/waybar&#34;&gt;Gentoo&lt;/a&gt;, &lt;a href=&#34;https://build.opensuse.org/package/show/X11:Wayland/waybar&#34;&gt;openSUSE&lt;/a&gt;, and &lt;a href=&#34;https://pkgs.alpinelinux.org/packages?name=waybar&#34;&gt;Alpine Linux&lt;/a&gt;.&lt;br&gt; &lt;em&gt;Waybar &lt;a href=&#34;https://github.com/Alexays/Waybar/wiki/Examples&#34;&gt;examples&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Current features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sway (Workspaces, Binding mode, Focused window name)&lt;/li&gt; &#xA; &lt;li&gt;River (Mapping mode, Tags, Focused window name)&lt;/li&gt; &#xA; &lt;li&gt;Hyprland (Focused window name)&lt;/li&gt; &#xA; &lt;li&gt;Tray &lt;a href=&#34;https://github.com/Alexays/Waybar/issues/21&#34;&gt;#21&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Local time&lt;/li&gt; &#xA; &lt;li&gt;Battery&lt;/li&gt; &#xA; &lt;li&gt;UPower&lt;/li&gt; &#xA; &lt;li&gt;Network&lt;/li&gt; &#xA; &lt;li&gt;Bluetooth&lt;/li&gt; &#xA; &lt;li&gt;Pulseaudio&lt;/li&gt; &#xA; &lt;li&gt;Wireplumber&lt;/li&gt; &#xA; &lt;li&gt;Disk&lt;/li&gt; &#xA; &lt;li&gt;Memory&lt;/li&gt; &#xA; &lt;li&gt;Cpu load average&lt;/li&gt; &#xA; &lt;li&gt;Temperature&lt;/li&gt; &#xA; &lt;li&gt;MPD&lt;/li&gt; &#xA; &lt;li&gt;Custom scripts&lt;/li&gt; &#xA; &lt;li&gt;Custom image&lt;/li&gt; &#xA; &lt;li&gt;Multiple output configuration&lt;/li&gt; &#xA; &lt;li&gt;And many more customizations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Configuration and Styling&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alexays/Waybar/wiki&#34;&gt;See the wiki for more details&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Waybar is available from a number of Linux distributions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repology.org/project/waybar/versions&#34;&gt;&lt;img src=&#34;https://repology.org/badge/vertical-allrepos/waybar.svg?sanitize=true&#34; alt=&#34;Packaging status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An Ubuntu PPA with more recent versions is available &lt;a href=&#34;https://launchpad.net/~nschloe/+archive/ubuntu/waybar&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building from source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/Alexays/Waybar&#xA;$ cd Waybar&#xA;$ meson build&#xA;$ ninja -C build&#xA;$ ./build/waybar&#xA;# If you want to install it&#xA;$ ninja -C build install&#xA;$ waybar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gtkmm3&#xA;jsoncpp&#xA;libsigc++&#xA;fmt&#xA;wayland&#xA;chrono-date&#xA;spdlog&#xA;libgtk-3-dev [gtk-layer-shell]&#xA;gobject-introspection [gtk-layer-shell]&#xA;libgirepository1.0-dev [gtk-layer-shell]&#xA;libpulse [Pulseaudio module]&#xA;libnl [Network module]&#xA;libappindicator-gtk3 [Tray module]&#xA;libdbusmenu-gtk3 [Tray module]&#xA;libmpdclient [MPD module]&#xA;libsndio [sndio module]&#xA;libevdev [KeyboardState module]&#xA;xkbregistry&#xA;upower [UPower battery module]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Build dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake&#xA;meson&#xA;scdoc&#xA;wayland-protocols&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Ubuntu, you can install all the relevant dependencies using this command (tested with 19.10 and 20.04):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install \&#xA;  clang-tidy \&#xA;  gobject-introspection \&#xA;  libdbusmenu-gtk3-dev \&#xA;  libevdev-dev \&#xA;  libfmt-dev \&#xA;  libgirepository1.0-dev \&#xA;  libgtk-3-dev \&#xA;  libgtkmm-3.0-dev \&#xA;  libinput-dev \&#xA;  libjsoncpp-dev \&#xA;  libmpdclient-dev \&#xA;  libnl-3-dev \&#xA;  libnl-genl-3-dev \&#xA;  libpulse-dev \&#xA;  libsigc++-2.0-dev \&#xA;  libspdlog-dev \&#xA;  libwayland-dev \&#xA;  scdoc \&#xA;  upower \&#xA;  libxkbregistry-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Contributions welcome!&lt;br&gt; Have fun :)&lt;br&gt; The style guidelines are &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google&#39;s&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Waybar is licensed under the MIT license. &lt;a href=&#34;https://github.com/Alexays/Waybar/raw/master/LICENSE&#34;&gt;See LICENSE for more information&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>