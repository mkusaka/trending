<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-30T01:58:07Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>changkun/modern-cpp-tutorial</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/changkun/modern-cpp-tutorial</id>
    <link href="https://github.com/changkun/modern-cpp-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ“š Modern C++ Tutorial: C++11/14/17/20 On the Fly | https://changkun.de/modern-cpp/&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/assets/cover-2nd-en.png&#34; alt=&#34;logo&#34; height=&#34;550&#34; align=&#34;right&#34;&gt; &#xA;&lt;h1&gt;Modern C++ Tutorial: C++11/14/17/20 On the Fly&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/travis/changkun/modern-cpp-tutorial/master?style=flat-square&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-English-blue.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/README-zh-cn.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-red.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/assets/donate.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E2%82%AC-donate-ff69b4.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Purpose&lt;/h2&gt; &#xA;&lt;p&gt;The book claims to be &#34;On the Fly&#34;. Its intent is to provide a comprehensive introduction to the relevant features regarding modern C++ (before 2020s). Readers can choose interesting content according to the following table of content to learn and quickly familiarize the new features you would like to learn. Readers should be aware that not all of these features are required. Instead, it should be learned when you really need it.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, instead of coding-only, the book introduces the historical background of its technical requirements (as simple as possible), which provides great help in understanding why these features came out.&lt;/p&gt; &#xA;&lt;p&gt;In addition, the author would like to encourage readers to use modern C++ directly in their new projects and migrate their old projects to modern C++ gradually after reading the book.&lt;/p&gt; &#xA;&lt;h2&gt;Targets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This book assumes that readers are already familiar with traditional C++ (i.e. C++98 or earlier), or at least that they do not have any difficulty in reading traditional C++ code. In other words, those who have long experience in traditional C++ and people who desire to quickly understand the features of modern C++ in a short period of time are well suited to read the book.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This book introduces, to a certain extent, the dark magic of modern C++. However, these magic tricks are very limited, they are not suitable for readers who want to learn advanced C++. The purpose of this book is offering a quick start for modern C++. Of course, advanced readers can also use this book to review and examine themselves on modern C++.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Start&lt;/h2&gt; &#xA;&lt;p&gt;You can choose from the following reading methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/book/en-us/toc.md&#34;&gt;GitHub Online&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp/pdf/modern-cpp-tutorial-en-us.pdf&#34;&gt;PDF document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp/epub/modern-cpp-tutorial-en-us.epub&#34;&gt;EPUB document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp&#34;&gt;Website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code&lt;/h2&gt; &#xA;&lt;p&gt;Each chapter of this book contains a lot of code. If you encounter problems while writing your own code with the introductory features of the book, reading the source code attached to the book might be of help. You can find the book &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/code&#34;&gt;here&lt;/a&gt;. All the code is organized by chapter, the folder name is the chapter number.&lt;/p&gt; &#xA;&lt;h2&gt;Exercises&lt;/h2&gt; &#xA;&lt;p&gt;There are few exercises at the end of each chapter of the book. These are meant to test whether you have mastered the knowledge in the current chapter. You can find the possible answer to the problem &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/exercises&#34;&gt;here&lt;/a&gt;. Again, the folder name is the chapter number.&lt;/p&gt; &#xA;&lt;h2&gt;Website&lt;/h2&gt; &#xA;&lt;p&gt;The source code of the &lt;a href=&#34;https://changkun.de/modern-cpp&#34;&gt;website&lt;/a&gt; of this book can be found &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/website&#34;&gt;here&lt;/a&gt;, which is built by &lt;a href=&#34;https://hexo.io&#34;&gt;hexo&lt;/a&gt; and &lt;a href=&#34;https://vuejs.org&#34;&gt;vuejs&lt;/a&gt;. The website provides you another way of reading the book, it also adapts to mobile browsers.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in building everything locally, it is recommended using &lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Docker&lt;/a&gt;. To build, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This book was originally written in Chinese by &lt;a href=&#34;https://changkun.de&#34;&gt;Changkun Ou&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The author has limited time and language skills. If readers find any mistakes in the book or any language improvements, please feel free to open an &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/issues&#34;&gt;Issue&lt;/a&gt; or start a &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/pulls&#34;&gt;Pull request&lt;/a&gt;. For detailed guidelines and checklist, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/CONTRIBUTING.md&#34;&gt;How to contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The author is grateful to all contributors, including but not limited to &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is also supported by:&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://www.digitalocean.com/?refcode=834a3bbc951b&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=CopyPaste&#34;&gt; &lt;img src=&#34;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg?sanitize=true&#34; width=&#34;201px&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;This work was written by &lt;a href=&#34;https://changkun.de&#34;&gt;Ou Changkun&lt;/a&gt; and licensed under a &lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License&lt;/a&gt;. The code of this repository is open sourced under the &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>applenob/Cpp_Primer_Practice</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/applenob/Cpp_Primer_Practice</id>
    <link href="https://github.com/applenob/Cpp_Primer_Practice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æå®šC++ğŸ‘Šã€‚C++ Primer ä¸­æ–‡ç‰ˆç¬¬5ç‰ˆå­¦ä¹ ä»“åº“ï¼ŒåŒ…æ‹¬ç¬”è®°å’Œè¯¾åç»ƒä¹ ç­”æ¡ˆã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cpp Primer å­¦ä¹ &lt;/h1&gt; &#xA;&lt;h2&gt;ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;ã€ŠC++ Primer ä¸­æ–‡ç‰ˆï¼ˆç¬¬ 5 ç‰ˆï¼‰ã€‹å­¦ä¹ ä»“åº“ï¼ŒåŒ…æ‹¬&lt;strong&gt;ç¬”è®°&lt;/strong&gt;å’Œ&lt;strong&gt;è¯¾åç»ƒä¹ ç­”æ¡ˆ&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ç¯å¢ƒ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;system: ubuntu 16.04&lt;/li&gt; &#xA; &lt;li&gt;IDE: VS Code&lt;/li&gt; &#xA; &lt;li&gt;compiler: g++&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://book.douban.com/subject/25708312/&#34;&gt;è±†ç“£é“¾æ¥&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ç›®å½•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç¬¬1ç«  : å¼€å§‹ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch01.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch01.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ I éƒ¨åˆ† : C++åŸºç¡€ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ç¬¬2ç«  : å˜é‡å’ŒåŸºæœ¬ç±»å‹ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch02.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch02.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬3ç«  : å­—ç¬¦ä¸²ã€å‘é‡å’Œæ•°ç»„ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch03.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch03.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬4ç«  : è¡¨è¾¾å¼ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch04.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch04.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬5ç«  : è¯­å¥ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch05.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch05.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬6ç«  : å‡½æ•° &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch06.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch06.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬7ç«  : ç±» &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch07.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch07.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ II éƒ¨åˆ† : C++æ ‡å‡†åº“ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ç¬¬8ç«  : IOåº“ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch08.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch08.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬9ç«  : é¡ºåºå®¹å™¨ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch09.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch09.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬10ç«  : æ³›å‹ç®—æ³• &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch10.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch10.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬11ç«  : å…³è”å®¹å™¨ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch11.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch11.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬12ç«  : åŠ¨æ€å†…å­˜ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch12.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch12.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ III éƒ¨åˆ† : ç±»è®¾è®¡è€…çš„å·¥å…· &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ç¬¬13ç«  : æ‹·è´æ§åˆ¶ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch13.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch13.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬14ç«  : é‡è½½ä¸ç±»å‹è½¬æ¢ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch14.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch14.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬15ç«  : é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch15.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch15.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬16ç«  : æ¨¡ç‰ˆä¸æ³›å‹ç¼–ç¨‹ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch16.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch16.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç¬¬ IV éƒ¨åˆ† : é«˜çº§ä¸»é¢˜ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ç¬¬17ç«  : æ ‡å‡†åº“ä¸ç‰¹æ®Šè®¾æ–½ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch17.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch17.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬18ç«  : ç”¨äºå¤§å‹ç¨‹åºçš„å·¥å…· &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch18.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch18.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ç¬¬19ç«  : ç‰¹æ®Šå·¥å…·ä¸æŠ€æœ¯ &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch19.md&#34;&gt;ç¬”è®°&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch19.md&#34;&gt;ç»ƒä¹ &lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å‚è€ƒ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mooophy/Cpp-Primer&#34;&gt;C++ Primer 5 Answers(C++11/14)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huangmingchuan/Cpp_Primer_Answers&#34;&gt;ã€ŠC++ Primerã€‹ç¬¬äº”ç‰ˆä¸­æ–‡ç‰ˆä¹ é¢˜ç­”æ¡ˆ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å‚ä¸è´¡çŒ®&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬ä»“åº“ç”±å¤šä½å°ä¼™ä¼´ä¸€èµ·å‚ä¸ç¼–å†™ï¼Œæ¬¢è¿å¤§å®¶å¯¹æœ¬ä»“åº“è¿›è¡Œè¡¥å……ï¼Œä¸€èµ·å¸®å¤§å®¶æ›´å¥½åœ°ç†è§£è¿™æœ¬â€œå¤§éƒ¨å¤´â€ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zaphoyd/websocketpp</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/zaphoyd/websocketpp</id>
    <link href="https://github.com/zaphoyd/websocketpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ websocket client/server library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WebSocket++ (0.8.2)&lt;/h1&gt; &#xA;&lt;p&gt;WebSocket++ is a header only C++ library that implements RFC6455 The WebSocket Protocol. It allows integrating WebSocket client and server functionality into C++ programs. It uses interchangeable network transport modules including one based on raw char buffers, one based on C++ iostreams, and one based on Asio (either via Boost or standalone). End users can write additional transport policies to support other networking or event libraries as needed.&lt;/p&gt; &#xA;&lt;h1&gt;Major Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full support for RFC6455&lt;/li&gt; &#xA; &lt;li&gt;Partial support for Hixie 76 / Hybi 00, 07-17 draft specs (server only)&lt;/li&gt; &#xA; &lt;li&gt;Message/event based interface&lt;/li&gt; &#xA; &lt;li&gt;Supports secure WebSockets (TLS), IPv6, and explicit proxies.&lt;/li&gt; &#xA; &lt;li&gt;Flexible dependency management (C++11 Standard Library or Boost)&lt;/li&gt; &#xA; &lt;li&gt;Interchangeable network transport modules (raw, iostream, Asio, or custom)&lt;/li&gt; &#xA; &lt;li&gt;Portable/cross platform (Posix/Windows, 32/64bit, Intel/ARM/PPC)&lt;/li&gt; &#xA; &lt;li&gt;Thread-safe&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Get Involved&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/zaphoyd/websocketpp&#34;&gt;&lt;img src=&#34;https://travis-ci.org/zaphoyd/websocketpp.png&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Project Website&lt;/strong&gt; &lt;a href=&#34;http://www.zaphoyd.com/websocketpp/&#34;&gt;http://www.zaphoyd.com/websocketpp/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User Manual&lt;/strong&gt; &lt;a href=&#34;http://docs.websocketpp.org/&#34;&gt;http://docs.websocketpp.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GitHub Repository&lt;/strong&gt; &lt;a href=&#34;https://github.com/zaphoyd/websocketpp/&#34;&gt;https://github.com/zaphoyd/websocketpp/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub pull requests should be submitted to the &lt;code&gt;develop&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Announcements Mailing List&lt;/strong&gt; &lt;a href=&#34;http://groups.google.com/group/websocketpp-announcements/&#34;&gt;http://groups.google.com/group/websocketpp-announcements/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IRC Channel&lt;/strong&gt; #websocketpp (freenode)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discussion / Development / Support Mailing List / Forum&lt;/strong&gt; &lt;a href=&#34;http://groups.google.com/group/websocketpp/&#34;&gt;http://groups.google.com/group/websocketpp/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Author&lt;/h1&gt; &#xA;&lt;p&gt;Peter Thorson - &lt;a href=&#34;mailto:websocketpp@zaphoyd.com&#34;&gt;websocketpp@zaphoyd.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mamedev/mame</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/mamedev/mame</id>
    <link href="https://github.com/mamedev/mame" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MAME&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;MAME&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/mamedev/mame?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/mamedev/mame&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build status:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS/Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/GCC and clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Linux)/badge.svg?sanitize=true&#34; alt=&#34;CI (Linux)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows/MinGW GCC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Windows)/badge.svg?sanitize=true&#34; alt=&#34;CI (Windows)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS/clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(macOS)/badge.svg?sanitize=true&#34; alt=&#34;CI (macOS)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI Translations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Compile%20UI%20translations/badge.svg?sanitize=true&#34; alt=&#34;Compile UI translations&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Build%20documentation/badge.svg?sanitize=true&#34; alt=&#34;Build documentation&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Static analysis status for entire build (except for third-party parts of project):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scan.coverity.com/projects/mame-emulator&#34;&gt;&lt;img src=&#34;https://scan.coverity.com/projects/5727/badge.svg?flat=1&#34; alt=&#34;Coverity Scan Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is MAME?&lt;/h1&gt; &#xA;&lt;p&gt;MAME is a multi-purpose emulation framework.&lt;/p&gt; &#xA;&lt;p&gt;MAME&#39;s purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important &#34;vintage&#34; software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.&lt;/p&gt; &#xA;&lt;h1&gt;How to compile?&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re on a UNIX-like system (including Linux and macOS), it could be as easy as typing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MAME build,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=arcade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for an arcade-only build, or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=mess&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MESS build.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;http://docs.mamedev.org/initialsetup/compilingmame.html&#34;&gt;Compiling MAME&lt;/a&gt; page on our documentation site for more information, including prerequisites for macOS and popular Linux distributions.&lt;/p&gt; &#xA;&lt;p&gt;For recent versions of macOS you need to install &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; including command-line tools and &lt;a href=&#34;https://www.libsdl.org/download-2.0.php&#34;&gt;SDL 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Windows users, we provide a ready-made &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64.&lt;/p&gt; &#xA;&lt;p&gt;Visual Studio builds are also possible, but you still need &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64. In order to generate solution and project files just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or use this command to build it directly using msbuild&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019 MSBUILD=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Where can I find out more?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mamedev.org/&#34;&gt;Official MAME Development Team Site&lt;/a&gt; (includes binary downloads, wiki, forums, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mess.redump.net/&#34;&gt;Official MESS Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mametesters.org/&#34;&gt;MAME Testers&lt;/a&gt; (official bug tracker for MAME and MESS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Coding standard&lt;/h2&gt; &#xA;&lt;p&gt;MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.&lt;/p&gt; &#xA;&lt;p&gt;Some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#Allman_style&#34;&gt;Allman style&lt;/a&gt;; some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#K.26R_style&#34;&gt;K&amp;amp;R style&lt;/a&gt; -- mostly depending on who wrote the original version. &lt;strong&gt;Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source.&lt;/strong&gt; For new code, the majority tends to prefer Allman style, so if you don&#39;t care much, use that.&lt;/p&gt; &#xA;&lt;p&gt;All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD-3-Clause&lt;/a&gt; license, the &lt;a href=&#34;http://opensource.org/licenses/LGPL-2.1&#34;&gt;LGPL-2.1&lt;/a&gt;, or the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GPL-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The MAME project as a whole is made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GNU General Public License, version 2&lt;/a&gt; or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of the source files (over 90% including core files) are made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;3-clause BSD License&lt;/a&gt;, and we would encourage new contributors to make their contributions available under the terms of this license.&lt;/p&gt; &#xA;&lt;p&gt;Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the &#34;MAME&#34; name, logo, or wordmark.&lt;/p&gt; &#xA;&lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34; target=&#34;_blank&#34;&gt; &lt;img align=&#34;right&#34; src=&#34;http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (C) 1997-2021  MAMEDev and contributors&#xA;&#xA;This program is free software; you can redistribute it and/or modify it&#xA;under the terms of the GNU General Public License version 2, as provided in&#xA;docs/legal/GPL-2.0.&#xA;&#xA;This program is distributed in the hope that it will be useful, but WITHOUT&#xA;ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or&#xA;FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for&#xA;more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see COPYING for more details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleSpeech</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/PaddlePaddle/PaddleSpeech</id>
    <link href="https://github.com/PaddlePaddle/PaddleSpeech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use Speech Toolkit including SOTA/Streaming ASR with punctuation, influential TTS with text frontend, Speaker Verification System and End-to-End Speech Simultaneous Translation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/README_cn.md&#34;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;|English)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/PaddleSpeech_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;support os&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleSpeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/paddlespeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt; Quick Start &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt; Quick Start Server &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt; Quick Start Streaming Server&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#documents&#34;&gt; Documents &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt; Models List &lt;/a&gt; | &lt;a href=&#34;https://aistudio.baidu.com/aistudio/education/group/info/25130&#34;&gt; AIStudio Courses &lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2205.12007&#34;&gt; Paper &lt;/a&gt; | &lt;a href=&#34;https://gitee.com/paddlepaddle/PaddleSpeech&#34;&gt; Gitee &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleSpeech&lt;/strong&gt; is an open-source toolkit on &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt; platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models.&lt;/p&gt; &#xA;&lt;h5&gt;Speech Recognition&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Recognition Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;I knocked at the door on the ancient side of the building.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;æˆ‘è®¤ä¸ºè·‘æ­¥æœ€é‡è¦çš„å°±æ˜¯ç»™æˆ‘å¸¦æ¥äº†èº«ä½“å¥åº·ã€‚&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Speech Translation (English to Chinese)&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Translations Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;æˆ‘ åœ¨ è¿™æ ‹ å»ºç­‘ çš„ å¤è€ é—¨ä¸Š æ•²é—¨ã€‚&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Text-to-Speech&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Input Text&lt;/th&gt; &#xA;    &lt;th&gt;Synthetic Audio&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Life was like a box of chocolates, you never know what you&#39;re gonna get.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;æ—©ä¸Šå¥½ï¼Œä»Šå¤©æ˜¯2020/10/29ï¼Œæœ€ä½æ¸©åº¦æ˜¯-3Â°Cã€‚&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;å­£å§¬å¯‚ï¼Œé›†é¸¡ï¼Œé¸¡å³æ£˜é¸¡ã€‚æ£˜é¸¡é¥¥å½ï¼Œå­£å§¬åŠç®•ç¨·æµé¸¡ã€‚é¸¡æ—¢æµï¼Œè·»å§¬ç¬ˆï¼Œå­£å§¬å¿Œï¼Œæ€¥å’­é¸¡ï¼Œé¸¡æ€¥ï¼Œç»§åœ¾å‡ ï¼Œå­£å§¬æ€¥ï¼Œå³ç±ç®•å‡»é¸¡ï¼Œç®•ç–¾å‡»å‡ ä¼ï¼Œä¼å³é½‘ï¼Œé¸¡å½é›†å‡ åŸºï¼Œå­£å§¬æ€¥æå±å‡»é¸¡ï¼Œé¸¡æ—¢æ®›ï¼Œå­£å§¬æ¿€ï¼Œå³è®°ã€Šå­£å§¬å‡»é¸¡è®°ã€‹ã€‚&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For more synthesized audios, please refer to &lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;PaddleSpeech Text-to-Speech samples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Punctuation Restoration&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Input Text &lt;/th&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Output Text &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ä»Šå¤©çš„å¤©æ°”çœŸä¸é”™å•Šä½ ä¸‹åˆæœ‰ç©ºå—æˆ‘æƒ³çº¦ä½ ä¸€èµ·å»åƒé¥­&lt;/td&gt; &#xA;    &lt;td&gt;ä»Šå¤©çš„å¤©æ°”çœŸä¸é”™å•Šï¼ä½ ä¸‹åˆæœ‰ç©ºå—ï¼Ÿæˆ‘æƒ³çº¦ä½ ä¸€èµ·å»åƒé¥­ã€‚&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Via the easy-to-use, efficient, flexible and scalable implementation, our vision is to empower both industrial application and academic research, including training, inference &amp;amp; testing modules, and deployment process. To be more specific, this toolkit features at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ“¦ &lt;strong&gt;Ease of Use&lt;/strong&gt;: low barriers to install, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt;CLI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt;Server&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt;Streaming Server&lt;/a&gt; is available to quick-start your journey.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ† &lt;strong&gt;Align to the State-of-the-Art&lt;/strong&gt;: we provide high-speed and ultra-lightweight models, and also cutting-edge technology.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ† &lt;strong&gt;Streaming ASR and TTS System&lt;/strong&gt;: we provide production ready streaming asr and streaming tts system.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ’¯ &lt;strong&gt;Rule-based Chinese frontend&lt;/strong&gt;: our frontend contains Text Normalization and Grapheme-to-Phoneme (G2P, including Polyphone and Tone Sandhi). Moreover, we use self-defined linguistic rules to adapt Chinese context.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“¦ &lt;strong&gt;Varieties of Functions that Vitalize both Industrial and Academia&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ğŸ›ï¸ &lt;em&gt;Implementation of critical audio tasks&lt;/em&gt;: this toolkit contains audio functions like Automatic Speech Recognition, Text-to-Speech Synthesis, Speaker Verfication, KeyWord Spotting, Audio Classification, and Speech Translation, etc.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ”¬ &lt;em&gt;Integration of mainstream models and datasets&lt;/em&gt;: the toolkit implements modules that participate in the whole pipeline of the speech tasks, and uses mainstream datasets like LibriSpeech, LJSpeech, AIShell, CSMSC, etc. See also &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt;model list&lt;/a&gt; for more details.&lt;/li&gt; &#xA;   &lt;li&gt;ğŸ§© &lt;em&gt;Cascaded models application&lt;/em&gt;: as an extension of the typical traditional audio tasks, we combine the workflows of the aforementioned tasks with other fields like Natural language processing (NLP) and Computer Vision (CV).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recent Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ‘‘ 2022.05.13: Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/PPASR.md&#34;&gt;PP-ASR&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/PPTTS.md&#34;&gt;PP-TTS&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/vpr/PPVPR.md&#34;&gt;PP-VPR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘ğŸ» 2022.05.06: &lt;code&gt;Streaming ASR&lt;/code&gt; with &lt;code&gt;Punctuation Restoration&lt;/code&gt; and &lt;code&gt;Token Timestamp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘ğŸ» 2022.05.06: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;, and &lt;code&gt;Punctuation Restoration&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘ğŸ» 2022.04.28: &lt;code&gt;Streaming Server&lt;/code&gt; is available for &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘ğŸ» 2022.03.28: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘ğŸ» 2022.03.28: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ¤— 2021.12.14: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS&lt;/a&gt; Demos on Hugging Face Spaces are available!&lt;/li&gt; &#xA; &lt;li&gt;ğŸ‘ğŸ» 2021.12.10: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt;, &lt;code&gt;Speech Translation (English to Chinese)&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scan the QR code below with your Wechat, you can access to official technical exchange group and get the bonus ( more than 20GB learning materials, such as papers, codes and videos ) and the live link of the lessons. Look forward to your participation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/23690325/169763015-cbd8e28d-602c-4723-810d-dbc6da49441e.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We strongly recommend our users to install PaddleSpeech in &lt;strong&gt;Linux&lt;/strong&gt; with &lt;em&gt;python&amp;gt;=3.7&lt;/em&gt;. Up to now, &lt;strong&gt;Linux&lt;/strong&gt; supports CLI for the all our tasks, &lt;strong&gt;Mac OSX&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; only supports PaddleSpeech CLI for Audio Classification, Speech-to-Text and Text-to-Speech. To install &lt;code&gt;PaddleSpeech&lt;/code&gt;, please see &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our models with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/cli/README.md&#34;&gt;PaddleSpeech Command Line&lt;/a&gt;. Change &lt;code&gt;--input&lt;/code&gt; to test your own audio/text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech cls --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech vector --task spk --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Speech Recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech asr --lang zh --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Automatic Speech Recognition is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Translation&lt;/strong&gt; (English to Chinese) (not support for Mac and Windows now)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech st --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech tts --input &#34;ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨é£æ¡¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Text to Speech is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Postprocessing&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Punctuation Restoration &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;paddlespeech text --task punc --input ä»Šå¤©çš„å¤©æ°”çœŸä¸é”™å•Šä½ ä¸‹åˆæœ‰ç©ºå—æˆ‘æƒ³çº¦ä½ ä¸€èµ·å»åƒé¥­&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Batch Process&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo -e &#34;1 æ¬¢è¿å…‰ä¸´ã€‚\n2 è°¢è°¢æƒ é¡¾ã€‚&#34; | paddlespeech tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shell Pipeline&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ASR + Punctuation Restoration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech asr --input ./zh.wav | paddlespeech text --task punc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos&#34;&gt;demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try more functions like training and tuning, please have a look at &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Speech-to-Text Quick Start&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our speech server with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/server/README.md&#34;&gt;PaddleSpeech Server Command Line&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_server start --config_file ./paddlespeech/server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input &#34;æ‚¨å¥½ï¼Œæ¬¢è¿ä½¿ç”¨ç™¾åº¦é£æ¡¨è¯­éŸ³åˆæˆæœåŠ¡ã€‚&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Audio Classification Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about server command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server&#34;&gt;speech server demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartstreamingserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Streaming Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt; server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Speech Recognition Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Text to Speech Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input &#34;æ‚¨å¥½ï¼Œæ¬¢è¿ä½¿ç”¨ç™¾åº¦é£æ¡¨è¯­éŸ³åˆæˆæœåŠ¡ã€‚&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information please see: &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;ModelList&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech supports a series of most popular models. They are summarized in &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;released models&lt;/a&gt; and attached with available pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeechToText&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt; contains &lt;em&gt;Acoustic Model&lt;/em&gt;, &lt;em&gt;Language Model&lt;/em&gt;, and &lt;em&gt;Speech Translation&lt;/em&gt;, with the following details:&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Speech-to-Text Module Type&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Speech Recogination&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Aishell&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeech2 RNN + Conv based Models&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr0&#34;&gt;deepspeech2-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr1&#34;&gt;u2.transformer.conformer-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Librispeech&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr0&#34;&gt;deepspeech2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr1&#34;&gt;transformer.conformer.u2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr2&#34;&gt;transformer.conformer.u2-kaldi-librispeech&lt;/a&gt; &lt;/td&gt;  &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TIMIT&lt;/td&gt; &#xA;   &lt;td&gt;Unified Streaming &amp;amp; Non-streaming Two-pass&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/timit/asr1&#34;&gt; u2-timit&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment&lt;/td&gt; &#xA;   &lt;td&gt;THCHS30&lt;/td&gt; &#xA;   &lt;td&gt;MFA&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/.examples/thchs30/align0&#34;&gt;mfa-thchs30&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Language Model&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Ngram Language Model&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ngram_lm&#34;&gt;kenlm&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Speech Translation (English to Chinese)&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;TED En-Zh&lt;/td&gt; &#xA;   &lt;td&gt;Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st0&#34;&gt;transformer-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FAT + Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st1&#34;&gt;fat-st-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;TextToSpeech&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; in PaddleSpeech mainly contains three modules: &lt;em&gt;Text Frontend&lt;/em&gt;, &lt;em&gt;Acoustic Model&lt;/em&gt; and &lt;em&gt;Vocoder&lt;/em&gt;. Acoustic Model and Vocoder models are listed as follow:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Text-to-Speech Module Type &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Text Frontend &lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt; â€ƒ &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/tn&#34;&gt;tn&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/g2p&#34;&gt;g2p&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Acoustic Model&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts0&#34;&gt;tacotron2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts0&#34;&gt;tacotron2-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer TTS&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts1&#34;&gt;transformer-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeedySpeech&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts2&#34;&gt;speedyspeech-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts3&#34;&gt;fastspeech2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/tts3&#34;&gt;fastspeech2-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts3&#34;&gt;fastspeech2-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/tts3&#34;&gt;fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;6&#34;&gt;Vocoder&lt;/td&gt; &#xA;   &lt;td&gt;WaveFlow&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc0&#34;&gt;waveflow-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parallel WaveGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc1&#34;&gt;PWGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc1&#34;&gt;PWGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc1&#34;&gt;PWGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc1&#34;&gt;PWGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi Band MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc3&#34;&gt;Multi Band MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Style MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc4&#34;&gt;Style MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiFiGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc5&#34;&gt;HiFiGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc5&#34;&gt;HiFiGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc5&#34;&gt;HiFiGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc5&#34;&gt;HiFiGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WaveRNN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc6&#34;&gt;WaveRNN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Voice Cloning&lt;/td&gt; &#xA;   &lt;td&gt;GE2E&lt;/td&gt; &#xA;   &lt;td&gt;Librispeech, etc.&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ge2e&#34;&gt;ge2e&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc0&#34;&gt;ge2e-tacotron2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc1&#34;&gt;ge2e-fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;AudioClassification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio Classification&lt;/td&gt; &#xA;   &lt;td&gt;ESC-50&lt;/td&gt; &#xA;   &lt;td&gt;PANN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/esc50/cls0&#34;&gt;pann-esc50&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeakerVerification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;VoxCeleb12&lt;/td&gt; &#xA;   &lt;td&gt;ECAPA-TDNN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/voxceleb/sv0&#34;&gt;ecapa-tdnn-voxceleb12&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;PunctuationRestoration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Punctuation Restoration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Punctuation Restoration&lt;/td&gt; &#xA;   &lt;td&gt;IWLST2012_zh&lt;/td&gt; &#xA;   &lt;td&gt;Ernie Linear&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/iwslt2012/punc0&#34;&gt;iwslt2012-punc0&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;p&gt;Normally, &lt;a href=&#34;https://paperswithcode.com/area/speech&#34;&gt;Speech SoTA&lt;/a&gt;, &lt;a href=&#34;https://paperswithcode.com/area/audio&#34;&gt;Audio SoTA&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/area/music&#34;&gt;Music SoTA&lt;/a&gt; give you an overview of the hot academic topics in the related area. To focus on the tasks in PaddleSpeech, you will find the following guidelines are helpful to grasp the core ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/README.md&#34;&gt;Some Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Automatic Speech Recognition&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/data_preparation.md&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/ngram_lm.md&#34;&gt;Ngram LM&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/advanced_usage.md&#34;&gt;Advanced Usage&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/zh_text_frontend.md&#34;&gt;Chinese Rule Based Text Frontend&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;Test Audio Samples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Speaker Verification &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_searching/README.md&#34;&gt;Audio Searching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speaker_verification/README.md&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_tagging/README.md&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_translation/README.md&#34;&gt;Speech Translation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_server/README.md&#34;&gt;Speech Server&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;Released Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeechToText&#34;&gt;Speech-to-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#TextToSpeech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#AudioClassification&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeakerVerification&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#PunctuationRestoration&#34;&gt;Punctuation Restoration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#contribution&#34;&gt;Welcome to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Text-to-Speech module is originally called &lt;a href=&#34;https://github.com/PaddlePaddle/Parakeet&#34;&gt;Parakeet&lt;/a&gt;, and now merged with this repository. If you are interested in academic research about this task, please see &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview&#34;&gt;TTS research overview&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/raw/develop/docs/source/tts/models_introduction.md&#34;&gt;this document&lt;/a&gt; is a good guideline for the pipeline components.&lt;/p&gt; &#xA;&lt;h2&gt;â­ Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt;: Use PaddleSpeech TTS to generate virtual human voice.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web&#34;&gt;&lt;img src=&#34;https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/demo_video.html&#34;&gt;PaddleSpeech Demo Video&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt;: Use PaddleSpeech TTS and ASR to clone voice from videos.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jerryuhoo/VTuberTalk/main/gui/gui.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;To cite PaddleSpeech for research, please use the following format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{zhang2022paddlespeech,&#xA;    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},&#xA;    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},&#xA;    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},&#xA;    year = {2022},&#xA;    publisher = {Association for Computational Linguistics},&#xA;}&#xA;&#xA;@inproceedings{zheng2021fused,&#xA;  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},&#xA;  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},&#xA;  booktitle={International Conference on Machine Learning},&#xA;  pages={12736--12746},&#xA;  year={2021},&#xA;  organization={PMLR}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;contribution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to PaddleSpeech&lt;/h2&gt; &#xA;&lt;p&gt;You are warmly welcome to submit questions in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/discussions&#34;&gt;discussions&lt;/a&gt; and bug reports in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;issues&lt;/a&gt;! Also, we highly appreciate if you are willing to contribute to this project!&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zh794390558&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3038472?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackwaterveg&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/87408988?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yt605155624&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24568452?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kuke&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3064195?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinghai-sun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7038341?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pkuyym&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5782283?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KPatr1ck&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22954146?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LittleChenCc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10339970?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/745165806&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/20623194?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Mingxue-Xu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/92848346?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chrisxu2016&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18379485?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfchener&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6771821?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luotao1&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6836917?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wanghaoshuang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7534971?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gongel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24390500?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mmglove&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/38800877?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/iclementine&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/16222986?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ZeyuChen&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1371212?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81195143?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qingqing01&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7845005?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ericxk&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/4719594?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kvinwang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6442159?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jiqiren11&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/82639260?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AshishKarel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/58069375?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chesterkuo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6285069?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tensor-tang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/21351065?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hysunflower&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/52739577?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wwhu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6081200?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lispc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/2833376?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24245709?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harisankarh&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1307053?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackiexiao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18050469?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/limpidezza&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/71760778?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/yeyupiaoling&#34;&gt;yeyupiaoling&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PPASR&#34;&gt;PPASR&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech&#34;&gt;PaddlePaddle-DeepSpeech&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle&#34;&gt;VoiceprintRecognition-PaddlePaddle&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle&#34;&gt;AudioClassification-PaddlePaddle&lt;/a&gt; for years of attention, constructive advice and great help.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/mymagicpower&#34;&gt;mymagicpower&lt;/a&gt; for the Java implementation of ASR upon &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk&#34;&gt;short&lt;/a&gt; and &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk&#34;&gt;long&lt;/a&gt; audio files.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/JiehangXie&#34;&gt;JiehangXie&lt;/a&gt;/&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt; for developing Virtual Uploader(VUP)/Virtual YouTuber(VTuber) with PaddleSpeech TTS function.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;745165806&lt;/a&gt;/&lt;a href=&#34;https://github.com/745165806/PaddleSpeechTask&#34;&gt;PaddleSpeechTask&lt;/a&gt; for contributing Punctuation Restoration model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;kslz&lt;/a&gt; for supplementary Chinese documents.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/awmmmm&#34;&gt;awmmmm&lt;/a&gt; for contributing fastspeech2 aishell3 conformer pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/phecda-xu&#34;&gt;phecda-xu&lt;/a&gt;/&lt;a href=&#34;https://github.com/phecda-xu/PaddleDubbing&#34;&gt;PaddleDubbing&lt;/a&gt; for developing a dubbing tool with GUI based on PaddleSpeech TTS model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;jerryuhoo&lt;/a&gt;/&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt; for developing a GUI tool based on PaddleSpeech TTS and code for making datasets from videos based on PaddleSpeech ASR.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Besides, PaddleSpeech depends on a lot of open source repositories. See &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/reference.md&#34;&gt;references&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/rocksdb</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/facebook/rocksdb</id>
    <link href="https://github.com/facebook/rocksdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library that provides an embeddable, persistent key-value store for fast storage.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;RocksDB: A Persistent Key-Value Store for Flash and RAM Storage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebook/rocksdb.svg?style=svg&#34; alt=&#34;CircleCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/github/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/facebook/rocksdb.svg?branch=main&#34; alt=&#34;TravisCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/Facebook/rocksdb/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/fbgfu0so3afcno78/branch/main?svg=true&#34; alt=&#34;Appveyor Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://140-211-168-68-openstack.osuosl.org:8080/job/rocksdb&#34;&gt;&lt;img src=&#34;http://140-211-168-68-openstack.osuosl.org:8080/buildStatus/icon?job=rocksdb&amp;amp;style=plastic&#34; alt=&#34;PPC64le Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RocksDB is developed and maintained by Facebook Database Engineering Team. It is built on earlier work on &lt;a href=&#34;https://github.com/google/leveldb&#34;&gt;LevelDB&lt;/a&gt; by Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;This code is a library that forms the core building block for a fast key-value server, especially suited for storing data on flash drives. It has a Log-Structured-Merge-Database (LSM) design with flexible tradeoffs between Write-Amplification-Factor (WAF), Read-Amplification-Factor (RAF) and Space-Amplification-Factor (SAF). It has multi-threaded compactions, making it especially suitable for storing multiple terabytes of data in a single database.&lt;/p&gt; &#xA;&lt;p&gt;Start with example usage here: &lt;a href=&#34;https://github.com/facebook/rocksdb/tree/main/examples&#34;&gt;https://github.com/facebook/rocksdb/tree/main/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki&#34;&gt;github wiki&lt;/a&gt; for more explanation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in &lt;code&gt;include/&lt;/code&gt;. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Questions and discussions are welcome on the &lt;a href=&#34;https://www.facebook.com/groups/rocksdb.dev/&#34;&gt;RocksDB Developers Public&lt;/a&gt; Facebook group and &lt;a href=&#34;https://groups.google.com/g/rocksdb&#34;&gt;email list&lt;/a&gt; on Google Groups.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;RocksDB is dual-licensed under both the GPLv2 (found in the COPYING file in the root directory) and Apache 2.0 License (found in the LICENSE.Apache file in the root directory). You may select, at your option, one of the above-listed licenses.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Akebi-Group/Akebi-GC</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/Akebi-Group/Akebi-GC</id>
    <link href="https://github.com/Akebi-Group/Akebi-GC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The great software for some game that exploiting anime girls (and boys).&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Akebi GC&lt;/h1&gt; The great software for some game that exploiting anime girls (and boys). &#xA;&lt;hr&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;p&gt;It is reccomended to use &lt;a href=&#34;https://visualstudio.microsoft.com/&#34;&gt;Visual Studio 2022.&lt;/a&gt; As well as setting up &lt;strong&gt;&lt;code&gt;cheat-library&lt;/code&gt;&lt;/strong&gt; as startup project. &lt;strong&gt;The following is a recommended procedure, but others may be used.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone repository with &lt;code&gt;git clone --recurse-submodules https://github.com/Akebi-Group/Akebi-GC.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open &lt;code&gt;Akebi-GC/akebi-gc.sln&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build solution &lt;code&gt;akebi-gc.sln&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Release&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Head over to the releases page&lt;/li&gt; &#xA; &lt;li&gt;Download the latest binaries&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;(1-2 are optional if you didn&#39;t build from source)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;code&gt;/bin&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open Compiled version (debug, release)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Insure that &lt;code&gt;CLibrary.dll&lt;/code&gt; is in the same folder that &lt;code&gt;injector.exe&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;injector.exe&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Features&lt;/h1&gt; &#xA;&lt;h4&gt;General&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Protection Bypass&lt;/li&gt; &#xA; &lt;li&gt;In-Game GUI&lt;/li&gt; &#xA; &lt;li&gt;Hotkeys&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Player&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;God Mode&lt;/li&gt; &#xA; &lt;li&gt;Unlimited Stamina&lt;/li&gt; &#xA; &lt;li&gt;Dumb Enemies (Enemies don&#39;t attack)&lt;/li&gt; &#xA; &lt;li&gt;Player&lt;/li&gt; &#xA; &lt;li&gt;Multiply Attacks&lt;/li&gt; &#xA; &lt;li&gt;No Cooldown Skill/Ultimate&lt;/li&gt; &#xA; &lt;li&gt;No Cooldown Sprint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;World&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto Loot&lt;/li&gt; &#xA; &lt;li&gt;Auto Talk&lt;/li&gt; &#xA; &lt;li&gt;Killaura&lt;/li&gt; &#xA; &lt;li&gt;Auto Tree Farm&lt;/li&gt; &#xA; &lt;li&gt;Mob Vacuum&lt;/li&gt; &#xA; &lt;li&gt;Auto Fish&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Teleport&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chest/Oculi Teleport (Teleports to nearest)&lt;/li&gt; &#xA; &lt;li&gt;Map Teleport (Teleport to mark on map)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Visuals&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ESP&lt;/li&gt; &#xA; &lt;li&gt;Interactive Map&lt;/li&gt; &#xA; &lt;li&gt;Elemental Sight&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Debugging&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Entity List&lt;/li&gt; &#xA; &lt;li&gt;Position Info&lt;/li&gt; &#xA; &lt;li&gt;FPS Graph&lt;/li&gt; &#xA; &lt;li&gt;Packet Sniffer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Demo&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Map Teleportation&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/map-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Noclip&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/noclip-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TP to Oculi&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/oculi-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TP to Chests&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/chest-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Rapid Fire&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/rapid-fire-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Auto Talk&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/auto-talk-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cutscene Skipping&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create database for chests, oculi, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Adding a feature&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the Project&lt;/li&gt; &#xA; &lt;li&gt;Create your Feature Branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Commit your Changes (&lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the Branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Open a Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Suggestions&lt;/h2&gt; &#xA;&lt;p&gt;Open an issue with the title of the suggesstion you want to make. In the description, make sure it is descriptive enough so our devs can understand what you want and how you want it.&lt;/p&gt; &#xA;&lt;h2&gt;Bugs&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the short explanation for bug reporting, as well as the bug report template.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Find a bug and write down what happened, as well as your first thoughts on what you think caused it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Try to reproduce the bug. For this you need to understand what actually happened, leading up to the bug and when the actual bug happened. To make sure you get all this information correctly taking various forms of documentations, such as video, screenshots etc is essential. These steps makes it a lot easier to try and figure out what actually happened. Try to replicate the scenario where the bug appeared, as close to the original as possible. What we would recommend for this step is using the bug reporting template which can be found on page 2 and simply adding the information you have / find in there.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;can it be reproduced? Yes or no. If yes: Explain in as much detail as possible what happens when the bug occurs and why it occurs. Try and explain it as cleanly and as concise as possible to make sure that the coders donâ€™t have to read an essay to understand what could be a simple bug with a simple fix. For this, remember that information is very subjective so it is much better to over communicate than to risk confusion. If no: Try to provide as much information about the bug as possible, so that the testers will be able to replicate the scenario in which the bug occurred more easily so we can try to reproduce the bug.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Tell us which version you are using. Otherwise we would be getting bug reports on the same issue, that has been infact fixed in the latest commits. copy the SHA / Version Number of the latest commit when you built the mod. For example: &lt;code&gt;bd17a00ec388f3b93624280cde9e1c66e740edf9&lt;/code&gt; / Release 0.7&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Notes: Please remember to always record your testing sessions on your local hard drive and then upload them unlisted to youtube to conserve memory space on your computer and to give us easy access to your replays. This is to ensure that the optimal amount of documentation is available for the bug testers and coders to use as a guideline for either replicating scenarios, reproducing bugs or fixing them.&lt;/p&gt; &#xA;&lt;p&gt;TL:DR Record all your stuff while playing the mod and report any bugs to the issues section of this repository.&lt;/p&gt; &#xA;&lt;h3&gt;Bug reporting template&lt;/h3&gt; &#xA;&lt;p&gt;Title: e.g. â€œInstantly kill enemy with Shacklesâ€œ Description: â€œGame crashed if x, y, zâ€œ&lt;/p&gt; &#xA;&lt;p&gt;-- Footer -- Date Occured: 5 / 3 / 2022 Is it reproducible: Yes / Occasionally / No Latest Commit used: &lt;code&gt;bd17a00ec388f3b93624280cde9e1c66e740edf9&lt;/code&gt; Release Version: 0.7&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/onnxruntime</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/microsoft/onnxruntime</id>
    <link href="https://github.com/microsoft/onnxruntime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/images/ONNX_Runtime_logo_dark.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime is a cross-platform inference and training machine-learning accelerator&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime inference&lt;/strong&gt; can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing&#34;&gt;Learn more â†’&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime training&lt;/strong&gt; can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-training&#34;&gt;Learn more â†’&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;General Information&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai&#34;&gt;onnxruntime.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage documention and tutorials&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai/docs&#34;&gt;onnxruntime.ai/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Companion sample repositories&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ONNX Runtime Inferencing: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-inference-examples&#34;&gt;microsoft/onnxruntime-inference-examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ONNX Runtime Training: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-training-examples&#34;&gt;microsoft/onnxruntime-training-examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build Pipeline Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;EPs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20x64%20NoContribops%20CI%20Pipeline?label=Linux+CPU+x64+No+Contrib+Ops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=78&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/centos7_cpu?label=Linux+CentOS7&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=140&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-distributed?label=Distributed+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20NUPHAR%20CI%20Pipeline?label=Linux+NUPHAR&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=55&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline?label=Linux+OpenVINO&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mac&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=65&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20NoContribops%20CI%20Pipeline?label=MacOS+NoContribops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Android&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;iOS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WebAssembly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20WebAssembly%20CI%20Pipeline?label=WASM&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data/Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Windows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/Privacy.md&#34;&gt;privacy statement&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and Feedback&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For feature requests or bug reports, please file a &lt;a href=&#34;https://github.com/Microsoft/onnxruntime/issues&#34;&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general discussion or questions, please use &lt;a href=&#34;https://github.com/microsoft/onnxruntime/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>protocolbuffers/protobuf</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/protocolbuffers/protobuf</id>
    <link href="https://github.com/protocolbuffers/protobuf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2008 Google Inc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf&#39;s documentation on the Google Developers site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Compiler Installation&lt;/h2&gt; &#xA;&lt;p&gt;The protocol compiler is written in C++. If you are using C++, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; &#xA;&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf/releases&#34;&gt;https://github.com/protocolbuffers/protobuf/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the maven repo here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&#34;&gt;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; &#xA;&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&#34;&gt;src&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&#34;&gt;java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&#34;&gt;objectivec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C#&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&#34;&gt;csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&#34;&gt;ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf-go&#34;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&#34;&gt;php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dart-lang/protobuf&#34;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The best way to learn how to use protobuf is to follow the tutorials in our developer guide:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;https://developers.google.com/protocol-buffers/docs/tutorials&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The complete documentation for Protocol Buffers is available via the web at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Project-OSRM/osrm-backend</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/Project-OSRM/osrm-backend</id>
    <link href="https://github.com/Project-OSRM/osrm-backend" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source Routing Machine - C++ backend&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Open Source Routing Machine&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linux / macOS&lt;/th&gt; &#xA;   &lt;th&gt;Windows&lt;/th&gt; &#xA;   &lt;th&gt;Code Coverage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend/actions/workflows/osrm-backend.yml&#34;&gt;&lt;img src=&#34;https://github.com/Project-OSRM/osrm-backend/actions/workflows/osrm-backend.yml/badge.svg?sanitize=true&#34; alt=&#34;osrm-backend CI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ci.appveyor.com/project/DennisOSRM/osrm-backend&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/4iuo3s9gxprmcjjh&#34; alt=&#34;AppVeyor&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://codecov.io/gh/Project-OSRM/osrm-backend&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/Project-OSRM/osrm-backend/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;High performance routing engine written in C++14 designed to run on OpenStreetMap data.&lt;/p&gt; &#xA;&lt;p&gt;The following services are available via HTTP API, C++ library interface and NodeJs wrapper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nearest - Snaps coordinates to the street network and returns the nearest matches&lt;/li&gt; &#xA; &lt;li&gt;Route - Finds the fastest route between coordinates&lt;/li&gt; &#xA; &lt;li&gt;Table - Computes the duration or distances of the fastest route between all pairs of supplied coordinates&lt;/li&gt; &#xA; &lt;li&gt;Match - Snaps noisy GPS traces to the road network in the most plausible way&lt;/li&gt; &#xA; &lt;li&gt;Trip - Solves the Traveling Salesman Problem using a greedy heuristic&lt;/li&gt; &#xA; &lt;li&gt;Tile - Generates Mapbox Vector Tiles with internal routing metadata&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To quickly try OSRM use our &lt;a href=&#34;http://map.project-osrm.org&#34;&gt;demo server&lt;/a&gt; which comes with both the backend and a frontend on top.&lt;/p&gt; &#xA;&lt;p&gt;For a quick introduction about how the road network is represented in OpenStreetMap and how to map specific road network features have a look at &lt;a href=&#34;https://www.mapbox.com/mapping/mapping-for-navigation/&#34;&gt;this guide about mapping for navigation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Related &lt;a href=&#34;https://github.com/Project-OSRM&#34;&gt;Project-OSRM&lt;/a&gt; repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Project-OSRM/osrm-frontend&#34;&gt;osrm-frontend&lt;/a&gt; - User-facing frontend with map. The demo server runs this on top of the backend&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Project-OSRM/osrm-text-instructions&#34;&gt;osrm-text-instructions&lt;/a&gt; - Text instructions from OSRM route response&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/osrm/osrm-backend/&#34;&gt;osrm-backend-docker&lt;/a&gt; - Ready to use Docker images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Full documentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://project-osrm.org&#34;&gt;Hosted documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/http.md&#34;&gt;osrm-routed HTTP API documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/libosrm.md&#34;&gt;libosrm API documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IRC: &lt;code&gt;irc.oftc.net&lt;/code&gt;, channel: &lt;code&gt;#osrm&lt;/code&gt; (&lt;a href=&#34;https://webchat.oftc.net&#34;&gt;Webchat&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mailinglist: &lt;code&gt;https://lists.openstreetmap.org/listinfo/osrm-talk&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The easiest and quickest way to setup your own routing engine is to use Docker images we provide.&lt;/p&gt; &#xA;&lt;p&gt;There are two pre-processing pipelines available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Contraction Hierarchies (CH)&lt;/li&gt; &#xA; &lt;li&gt;Multi-Level Dijkstra (MLD)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;we recommend using MLD by default except for special use-cases such as very large distance matrices where CH is still a better fit for the time being. In the following we explain the MLD pipeline. If you want to use the CH pipeline instead replace &lt;code&gt;osrm-partition&lt;/code&gt; and &lt;code&gt;osrm-customize&lt;/code&gt; with a single &lt;code&gt;osrm-contract&lt;/code&gt; and change the algorithm option for &lt;code&gt;osrm-routed&lt;/code&gt; to &lt;code&gt;--algorithm ch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;p&gt;We base our Docker images (&lt;a href=&#34;https://hub.docker.com/r/osrm/osrm-backend/&#34;&gt;backend&lt;/a&gt;, &lt;a href=&#34;https://hub.docker.com/r/osrm/osrm-frontend/&#34;&gt;frontend&lt;/a&gt;) on Debian and make sure they are as lightweight as possible.&lt;/p&gt; &#xA;&lt;p&gt;Download OpenStreetMap extracts for example from &lt;a href=&#34;http://download.geofabrik.de/&#34;&gt;Geofabrik&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget http://download.geofabrik.de/europe/germany/berlin-latest.osm.pbf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pre-process the extract with the car profile and start a routing engine HTTP server on port 5000&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -t -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-extract -p /opt/car.lua /data/berlin-latest.osm.pbf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The flag &lt;code&gt;-v &#34;${PWD}:/data&#34;&lt;/code&gt; creates the directory &lt;code&gt;/data&lt;/code&gt; inside the docker container and makes the current working directory &lt;code&gt;&#34;${PWD}&#34;&lt;/code&gt; available there. The file &lt;code&gt;/data/berlin-latest.osm.pbf&lt;/code&gt; inside the container is referring to &lt;code&gt;&#34;${PWD}/berlin-latest.osm.pbf&#34;&lt;/code&gt; on the host.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -t -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-partition /data/berlin-latest.osrm&#xA;docker run -t -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-customize /data/berlin-latest.osrm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;berlin-latest.osrm&lt;/code&gt; has a different file extension.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -t -i -p 5000:5000 -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-routed --algorithm mld /data/berlin-latest.osrm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make requests against the HTTP server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl &#34;http://127.0.0.1:5000/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally start a user-friendly frontend on port 9966, and open it up in your browser&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 9966:9966 osrm/osrm-frontend&#xA;xdg-open &#39;http://127.0.0.1:9966&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In case Docker complains about not being able to connect to the Docker daemon make sure you are in the &lt;code&gt;docker&lt;/code&gt; group.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo usermod -aG docker $USER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After adding yourself to the &lt;code&gt;docker&lt;/code&gt; group make sure to log out and back in again with your terminal.&lt;/p&gt; &#xA;&lt;p&gt;We support the following images on Docker Cloud:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;master&lt;/code&gt; compiled with release flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest-assertions&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;master&lt;/code&gt; compiled with with release flag, assertions enabled and debug symbols&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest-debug&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;master&lt;/code&gt; compiled with debug flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;tag&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;specific tag compiled with release flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;tag&amp;gt;-debug&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;specific tag compiled with debug flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Building from Source&lt;/h3&gt; &#xA;&lt;p&gt;The following targets Ubuntu 16.04. For instructions how to build on different distributions, macOS or Windows see our &lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend/wiki&#34;&gt;Wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install build-essential git cmake pkg-config \&#xA;libbz2-dev libxml2-dev libzip-dev libboost-all-dev \&#xA;lua5.2 liblua5.2-dev libtbb-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Compile and install OSRM binaries&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build&#xA;cd build&#xA;cmake ..&#xA;cmake --build .&#xA;sudo cmake --build . --target install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Request Against the Demo Server&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend/wiki/Demo-server&#34;&gt;API usage policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Simple query with instructions and alternatives on Berlin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl &#34;https://router.project-osrm.org/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true&amp;amp;alternatives=true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the Node.js Bindings&lt;/h3&gt; &#xA;&lt;p&gt;The Node.js bindings provide read-only access to the routing engine. We provide API documentation and examples &lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/nodejs/api.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will need a modern &lt;code&gt;libstdc++&lt;/code&gt; toolchain (&lt;code&gt;&amp;gt;= GLIBCXX_3.4.20&lt;/code&gt;) for binary compatibility if you want to use the pre-built binaries. For older Ubuntu systems you can upgrade your standard library for example with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:ubuntu-toolchain-r/test&#xA;sudo apt-get update -y&#xA;sudo apt-get install -y libstdc++-5-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can install the Node.js bindings via &lt;code&gt;npm install osrm&lt;/code&gt; or from this repository either via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will check and use pre-built binaries if they&#39;re available for this release and your Node version, or via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install --build-from-source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to always force building the Node.js bindings from source.&lt;/p&gt; &#xA;&lt;p&gt;For usage details have a look &lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/nodejs/api.md&#34;&gt;these API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;An exemplary implementation by a 3rd party with Docker and Node.js can be found &lt;a href=&#34;https://github.com/door2door-io/osrm-express-server-demo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;References in publications&lt;/h2&gt; &#xA;&lt;p&gt;When using the code in a (scientific) publication, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{luxen-vetter-2011,&#xA; author = {Luxen, Dennis and Vetter, Christian},&#xA; title = {Real-time routing with OpenStreetMap data},&#xA; booktitle = {Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},&#xA; series = {GIS &#39;11},&#xA; year = {2011},&#xA; isbn = {978-1-4503-1031-4},&#xA; location = {Chicago, Illinois},&#xA; pages = {513--516},&#xA; numpages = {4},&#xA; url = {http://doi.acm.org/10.1145/2093973.2094062},&#xA; doi = {10.1145/2093973.2094062},&#xA; acmid = {2094062},&#xA; publisher = {ACM},&#xA; address = {New York, NY, USA},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>wenet-e2e/wenet</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/wenet-e2e/wenet</id>
    <link href="https://github.com/wenet-e2e/wenet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Production First and Production Ready End-to-End Speech Recognition Toolkit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WeNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/raw/main/README_CN.md&#34;&gt;&lt;strong&gt;ä¸­æ–‡ç‰ˆ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7%7C3.8-brightgreen&#34; alt=&#34;Python-Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/discussions&#34;&gt;&lt;strong&gt;Discussions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/papers.html&#34;&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/server/x86&#34;&gt;&lt;strong&gt;Runtime (x86)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet&#34;&gt;&lt;strong&gt;Runtime (android)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/pretrained_models.md&#34;&gt;&lt;strong&gt;Pretrained Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We&lt;/strong&gt; share neural &lt;strong&gt;Net&lt;/strong&gt; together.&lt;/p&gt; &#xA;&lt;p&gt;The main motivation of WeNet is to close the gap between research and production end-to-end (E2E) speech recognition models, to reduce the effort of productionizing E2E models, and to explore better E2E models for production.&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Production first and production ready&lt;/strong&gt;: The core design principle of WeNet. WeNet provides full stack solutions for speech recognition.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Unified solution for streaming and non-streaming ASR&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/pdf/2012.05481.pdf&#34;&gt;U2 framework&lt;/a&gt;--develop, train, and deploy only once.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Runtime solution&lt;/em&gt;: built-in server &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/server/x86&#34;&gt;x86&lt;/a&gt; and on-device &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet&#34;&gt;android&lt;/a&gt; runtime solution.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Model exporting solution&lt;/em&gt;: built-in solution to export model to LibTorch/ONNX for inference.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;LM solution&lt;/em&gt;: built-in production-level &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/lm.md&#34;&gt;LM solution&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Other production solutions&lt;/em&gt;: built-in contextual biasing, time stamp, endpoint, and n-best solutions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Accurate&lt;/strong&gt;: WeNet achieves SOTA results on a lot of public speech datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Light weight&lt;/strong&gt;: WeNet is easy to install, easy to use, well designed, and well documented.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;code&gt;examples/$dataset/s0/README.md&lt;/code&gt; for benchmark on different speech datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Installation(Python Only)&lt;/h2&gt; &#xA;&lt;p&gt;If you just want to use WeNet as a python package for speech recognition application, just install it by &lt;code&gt;pip&lt;/code&gt;, please note python 3.6+ is required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install wenet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And please see &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/runtime/binding/python/README.md&#34;&gt;doc&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;h2&gt;Installation(Training and Developing)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/wenet-e2e/wenet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Conda: please see &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create Conda env:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n wenet python=3.8&#xA;conda activate wenet&#xA;pip install -r requirements.txt&#xA;conda install pytorch=1.10.0 torchvision torchaudio=0.10.0 cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optionally, if you want to use x86 runtime or language model(LM), you have to build the runtime as follows. Otherwise, you can just ignore this step.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# runtime build requires cmake 3.14 or above&#xA;cd runtime/server/x86&#xA;mkdir build &amp;amp;&amp;amp; cd build &amp;amp;&amp;amp; cmake .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; &#xA;&lt;p&gt;Please visit &lt;a href=&#34;https://github.com/wenet-e2e/wenet/discussions&#34;&gt;Discussions&lt;/a&gt; for further discussion.&lt;/p&gt; &#xA;&lt;p&gt;For Chinese users, you can aslo scan the QR code on the left to follow our offical account of WeNet. We created a WeChat group for better discussion and quicker response. Please scan the personal QR code on the right, and the guy is responsible for inviting you to the chat group.&lt;/p&gt; &#xA;&lt;p&gt;If you can not access the QR image, please access it on &lt;a href=&#34;https://gitee.com/robin1001/qr/tree/master&#34;&gt;gitee&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/wenet.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/binbin.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Or you can directly discuss on &lt;a href=&#34;https://github.com/wenet-e2e/wenet/issues&#34;&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.chumenwenwen.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/chumenwenwen.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://lxie.npu-aslp.org&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/colleges/nwpu.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://www.aishelltech.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/aishelltech.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://www.ximalaya.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/ximalaya.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.jd.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/jd.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://horizon.ai&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/hobot.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://thuhcsi.github.io&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/colleges/thu.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nvidia.com/en-us&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/nvidia.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPnet&lt;/a&gt; for transformer based modeling.&lt;/li&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;Kaldi&lt;/a&gt; for WFST based decoding for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred &lt;a href=&#34;https://github.com/srvk/eesen&#34;&gt;EESEN&lt;/a&gt; for building TLG based graph for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred to &lt;a href=&#34;https://github.com/ZhengkunTian/OpenTransformer/&#34;&gt;OpenTransformer&lt;/a&gt; for python batch inference of e2e models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{yao2021wenet,&#xA;  title={WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit},&#xA;  author={Yao, Zhuoyuan and Wu, Di and Wang, Xiong and Zhang, Binbin and Yu, Fan and Yang, Chao and Peng, Zhendong and Chen, Xiaoyu and Xie, Lei and Lei, Xin},&#xA;  booktitle={Proc. Interspeech},&#xA;  year={2021},&#xA;  address={Brno, Czech Republic }&#xA;  organization={IEEE}&#xA;}&#xA;&#xA;@article{zhang2022wenet,&#xA;  title={WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit},&#xA;  author={Zhang, Binbin and Wu, Di and Peng, Zhendong and Song, Xingchen and Yao, Zhuoyuan and Lv, Hang and Xie, Lei and Yang, Chao and Pan, Fuping and Niu, Jianwei},&#xA;  journal={arXiv preprint arXiv:2203.15455},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/tensorflow</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/tensorflow/tensorflow</id>
    <link href="https://github.com/tensorflow/tensorflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open Source Machine Learning Framework for Everyone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/tensorflow.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.5281/zenodo.4724125&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/api-reference-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of &lt;a href=&#34;https://www.tensorflow.org/resources/tools&#34;&gt;tools&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/resources/libraries-extensions&#34;&gt;libraries&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;community&lt;/a&gt; resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google&#39;s Machine Intelligence Research organization to conduct machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow provides stable &lt;a href=&#34;https://www.tensorflow.org/api_docs/python&#34;&gt;Python&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/api_docs/cc&#34;&gt;C++&lt;/a&gt; APIs, as well as non-guaranteed backward compatible API for &lt;a href=&#34;https://www.tensorflow.org/api_docs&#34;&gt;other languages&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep up-to-date with release announcements and security updates by subscribing to &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/announce&#34;&gt;announce@tensorflow.org&lt;/a&gt;. See all the &lt;a href=&#34;https://www.tensorflow.org/community/forums&#34;&gt;mailing lists&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;TensorFlow install guide&lt;/a&gt; for the &lt;a href=&#34;https://www.tensorflow.org/install/pip&#34;&gt;pip package&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;enable GPU support&lt;/a&gt;, use a &lt;a href=&#34;https://www.tensorflow.org/install/docker&#34;&gt;Docker container&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/install/source&#34;&gt;build from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To install the current release, which includes support for &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;CUDA-enabled GPU cards&lt;/a&gt; &lt;em&gt;(Ubuntu and Windows)&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A smaller CPU-only package is also available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update TensorFlow to the latest version, add &lt;code&gt;--upgrade&lt;/code&gt; flag to the above commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Nightly binaries are available for testing using the &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly&#34;&gt;tf-nightly&lt;/a&gt; and &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly-cpu&#34;&gt;tf-nightly-cpu&lt;/a&gt; packages on PyPi.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Try your first TensorFlow program&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&#xA;&amp;gt;&amp;gt;&amp;gt; tf.add(1, 2).numpy()&#xA;3&#xA;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant(&#39;Hello, TensorFlow!&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; hello.numpy()&#xA;b&#39;Hello, TensorFlow!&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution guidelines&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want to contribute to TensorFlow, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;. This project adheres to TensorFlow&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We use &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues&#34;&gt;GitHub issues&lt;/a&gt; for tracking requests and bugs, please see &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss&#34;&gt;TensorFlow Discuss&lt;/a&gt; for general questions and discussion, and please direct specific questions to &lt;a href=&#34;https://stackoverflow.com/questions/tagged/tensorflow&#34;&gt;Stack Overflow&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The TensorFlow project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:tensorflow&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/1486&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/1486/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Continuous build status&lt;/h2&gt; &#xA;&lt;p&gt;You can find more community-supported platforms and configurations in the &lt;a href=&#34;https://github.com/tensorflow/build#community-supported-tensorflow-builds&#34;&gt;TensorFlow SIG Build community builds table&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Official Builds&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Artifacts&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux XLA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bintray.com/google/tensorflow/tensorflow/_latestVersion&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 0 and 1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 2 and 3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow MacOS CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;TensorFlow.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official&#34;&gt;TensorFlow Official Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/examples&#34;&gt;TensorFlow Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-in-practice&#34;&gt;DeepLearning.AI TensorFlow Developer Professional Certificate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-data-and-deployment&#34;&gt;TensorFlow: Data and Deployment from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/getting-started-with-tensor-flow2&#34;&gt;Getting Started with TensorFlow 2 from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-advanced-techniques&#34;&gt;TensorFlow: Advanced Techniques from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow2-deeplearning&#34;&gt;TensorFlow 2 for Deep Learning Specialization from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/introduction-tensorflow&#34;&gt;Intro to TensorFlow for A.I, M.L, and D.L from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187&#34;&gt;Intro to TensorFlow for Deep Learning from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-lite--ud190&#34;&gt;Introduction to TensorFlow Lite from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-tensorflow-gcp&#34;&gt;Machine Learning with TensorFlow on GCP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codelabs.developers.google.com/?cat=TensorFlow&#34;&gt;TensorFlow Codelabs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.tensorflow.org&#34;&gt;TensorFlow Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/resources/learn-ml&#34;&gt;Learn ML with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/tensorflow&#34;&gt;TensorFlow Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ&#34;&gt;TensorFlow YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/model_optimization/guide/roadmap&#34;&gt;TensorFlow model optimization roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/about/bib&#34;&gt;TensorFlow White Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorboard&#34;&gt;TensorBoard Visualization Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about the &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;TensorFlow community&lt;/a&gt; and how to &lt;a href=&#34;https://www.tensorflow.org/community/contribute&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SerenityOS/serenity</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/SerenityOS/serenity</id>
    <link href="https://github.com/SerenityOS/serenity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Serenity Operating System ğŸ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SerenityOS&lt;/h1&gt; &#xA;&lt;p&gt;Graphical Unix-like operating system for x86 computers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SerenityOS/serenity/actions?query=workflow%3A%22Build%2C%20lint%2C%20and%20test%22&#34;&gt;&lt;img src=&#34;https://github.com/SerenityOS/serenity/workflows/Build,%20lint,%20and%20test/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_build/latest?definitionId=1&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_apis/build/status/CI?branchName=master&#34; alt=&#34;Azure DevOps Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:serenity&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/serenity.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=SerenityOS_serenity&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=SerenityOS_serenity&amp;amp;metric=ncloc&#34; alt=&#34;Sonar Cube Static Analysis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/830522505605283862.svg?logo=discord&amp;amp;logoColor=white&amp;amp;logoWidth=20&amp;amp;labelColor=7289DA&amp;amp;label=Discord&amp;amp;color=17cf48&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is a love letter to &#39;90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.&lt;/p&gt; &#xA;&lt;p&gt;Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix. This is a system by us, for us, based on the things we like.&lt;/p&gt; &#xA;&lt;p&gt;You can watch videos of the system being developed on YouTube:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/andreaskling&#34;&gt;Andreas Kling&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/linusgroh&#34;&gt;Linus Groh&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;FAQ&lt;/strong&gt;: &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshot&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Meta/Screenshots/screenshot-b36968c.png&#34; alt=&#34;Screenshot as of b36968c.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modern x86 32-bit and 64-bit kernel with pre-emptive multi-threading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Applications/Browser/&#34;&gt;Browser&lt;/a&gt; with JavaScript, WebAssembly, and more (check the spec compliance for &lt;a href=&#34;https://libjs.dev/test262/&#34;&gt;JS&lt;/a&gt;, &lt;a href=&#34;https://css.tobyase.de/&#34;&gt;CSS&lt;/a&gt;, and &lt;a href=&#34;https://libjs.dev/wasm/&#34;&gt;WASM&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Security features (hardware protections, limited userland capabilities, W^X memory, &lt;code&gt;pledge&lt;/code&gt; &amp;amp; &lt;code&gt;unveil&lt;/code&gt;, (K)ASLR, OOM-resistance, web-content isolation, state-of-the-art TLS algorithms, ...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Services/&#34;&gt;System services&lt;/a&gt; (WindowServer, LoginServer, AudioServer, WebServer, RequestServer, CrashServer, ...) and modern IPC&lt;/li&gt; &#xA; &lt;li&gt;Good POSIX compatibility (&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/LibC/&#34;&gt;LibC&lt;/a&gt;, Shell, syscalls, signals, pseudoterminals, filesystem notifications, standard Unix &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Utilities/&#34;&gt;utilities&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;POSIX-like virtual file systems (/proc, /dev, /sys, /tmp, ...) and ext2 file system&lt;/li&gt; &#xA; &lt;li&gt;Network stack and applications with support for IPv4, TCP, UDP; DNS, HTTP, Gemini, IMAP, NTP&lt;/li&gt; &#xA; &lt;li&gt;Profiling, debugging and other development tools (Kernel-supported profiling, detailed program analysis with software emulation in UserspaceEmulator, CrashReporter, interactive GUI playground, HexEditor, HackStudio IDE for C++ and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/&#34;&gt;Libraries&lt;/a&gt; for everything from cryptography to OpenGL, audio, JavaScript, GUI, playing chess, ...&lt;/li&gt; &#xA; &lt;li&gt;Support for many common and uncommon file formats (PNG, JPEG, GIF, MP3, WAV, FLAC, ZIP, TAR, PDF, QOI, Gemini, ...)&lt;/li&gt; &#xA; &lt;li&gt;Unified style and design philosophy, flexible theming system, &lt;a href=&#34;https://fonts.serenityos.net/font-family&#34;&gt;custom (bitmap and vector) fonts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Games/&#34;&gt;Games&lt;/a&gt; (Solitaire, Minesweeper, 2048, chess, Conway&#39;s Game of Life, ...) and &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Demos/&#34;&gt;demos&lt;/a&gt; (CatDog, Starfield, Eyes, mandelbrot set, WidgetGallery, ...)&lt;/li&gt; &#xA; &lt;li&gt;Every-day GUI programs and utilities (Spreadsheet with JavaScript, TextEditor, Terminal, PixelPaint, various multimedia viewers and players, Mail, Assistant, Calculator, ...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... and all of the above are right in this repository, no extra dependencies, built from-scratch by us :^)&lt;/p&gt; &#xA;&lt;p&gt;Additionally, there are &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Ports/AvailablePorts.md&#34;&gt;over two hundred ports of popular open-source software&lt;/a&gt;, including games, compilers, Unix tools, multimedia apps and more.&lt;/p&gt; &#xA;&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; &#xA;&lt;p&gt;Man pages are available online at &lt;a href=&#34;https://man.serenityos.org&#34;&gt;man.serenityos.org&lt;/a&gt;. These pages are generated from the Markdown source files in &lt;a href=&#34;https://github.com/SerenityOS/serenity/tree/master/Base/usr/share/man&#34;&gt;&lt;code&gt;Base/usr/share/man&lt;/code&gt;&lt;/a&gt; and updated automatically.&lt;/p&gt; &#xA;&lt;p&gt;When running SerenityOS you can use &lt;code&gt;man&lt;/code&gt; for the terminal interface, or &lt;code&gt;help&lt;/code&gt; for the GUI.&lt;/p&gt; &#xA;&lt;p&gt;Code-related documentation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Documentation/&#34;&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;How do I build and run this?&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/BuildInstructions.md&#34;&gt;SerenityOS build instructions&lt;/a&gt;. Serenity runs on Linux, macOS (aarch64 might be a challenge), Windows (with WSL2) and many other *Nixes with hardware or software virtualization.&lt;/p&gt; &#xA;&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; &#xA;&lt;p&gt;Join our Discord server: &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;SerenityOS Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before opening an issue, please see the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/CONTRIBUTING.md#issue-policy&#34;&gt;issue policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A general guide for contributing can be found in &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andreas Kling&lt;/strong&gt; - &lt;a href=&#34;https://twitter.com/awesomekling&#34;&gt;awesomekling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robin Burchell&lt;/strong&gt; - &lt;a href=&#34;https://github.com/rburchell&#34;&gt;rburchell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conrad Pankoff&lt;/strong&gt; - &lt;a href=&#34;https://github.com/deoxxa&#34;&gt;deoxxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sergey Bugaev&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bugaevc&#34;&gt;bugaevc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Liav A&lt;/strong&gt; - &lt;a href=&#34;https://github.com/supercomputer7&#34;&gt;supercomputer7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linus Groh&lt;/strong&gt; - &lt;a href=&#34;https://github.com/linusg&#34;&gt;linusg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ali Mohammad Pur&lt;/strong&gt; - &lt;a href=&#34;https://github.com/alimpfard&#34;&gt;alimpfard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shannon Booth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/shannonbooth&#34;&gt;shannonbooth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;HÃ¼seyin ASLITÃœRK&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asliturk&#34;&gt;asliturk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Matthew Olsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mattco98&#34;&gt;mattco98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nico Weber&lt;/strong&gt; - &lt;a href=&#34;https://github.com/nico&#34;&gt;nico&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brian Gianforcaro&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bgianfo&#34;&gt;bgianfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ben Wiederhake&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BenWiederhake&#34;&gt;BenWiederhake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tom&lt;/strong&gt; - &lt;a href=&#34;https://github.com/tomuta&#34;&gt;tomuta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Paul Scharnofske&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asynts&#34;&gt;asynts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Itamar Shenhar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/itamar8910&#34;&gt;itamar8910&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Luke Wilde&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Lubrsi&#34;&gt;Lubrsi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brendan Coles&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bcoles&#34;&gt;bcoles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andrew Kaster&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ADKaster&#34;&gt;ADKaster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;thankyouverycool&lt;/strong&gt; - &lt;a href=&#34;https://github.com/thankyouverycool&#34;&gt;thankyouverycool&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Idan Horowitz&lt;/strong&gt; - &lt;a href=&#34;https://github.com/IdanHo&#34;&gt;IdanHo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gunnar Beutner&lt;/strong&gt; - &lt;a href=&#34;https://github.com/gunnarbeutner&#34;&gt;gunnarbeutner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Flynn&lt;/strong&gt; - &lt;a href=&#34;https://github.com/trflynn89&#34;&gt;trflynn89&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jean-Baptiste Boric&lt;/strong&gt; - &lt;a href=&#34;https://github.com/boricj&#34;&gt;boricj&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stephan Unverwerth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sunverwerth&#34;&gt;sunverwerth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Wipfli&lt;/strong&gt; - &lt;a href=&#34;https://github.com/MaxWipfli&#34;&gt;MaxWipfli&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Daniel Bertalan&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BertalanD&#34;&gt;BertalanD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jelle Raaijmakers&lt;/strong&gt; - &lt;a href=&#34;https://github.com/GMTA&#34;&gt;GMTA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sam Atkins&lt;/strong&gt; - &lt;a href=&#34;https://github.com/AtkinsSJ&#34;&gt;AtkinsSJ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tobias Christiansen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/TobyAsE&#34;&gt;TobyAsE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lenny Maiorani&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ldm5180&#34;&gt;ldm5180&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sin-ack&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sin-ack&#34;&gt;sin-ack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jesse Buhagiar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Quaker762&#34;&gt;Quaker762&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Peter Elliott&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Petelliott&#34;&gt;Petelliott&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Karol Kosek&lt;/strong&gt; - &lt;a href=&#34;https://github.com/krkk&#34;&gt;krkk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mustafa Quraish&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mustafaquraish&#34;&gt;mustafaquraish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;David Tuin&lt;/strong&gt; - &lt;a href=&#34;https://github.com/davidot&#34;&gt;davidot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leon Albrecht&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Hendiadyoin1&#34;&gt;Hendiadyoin1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Schumacher&lt;/strong&gt; - &lt;a href=&#34;https://github.com/timschumi&#34;&gt;timschumi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Marcus Nilsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/metmo&#34;&gt;metmo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gegga Thor&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Xexxa&#34;&gt;Xexxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;kleines FilmrÃ¶llchen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kleinesfilmroellchen&#34;&gt;kleinesfilmroellchen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kenneth Myhra&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kennethmyhra&#34;&gt;kennethmyhra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maciej&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sppmacd&#34;&gt;sppmacd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sahan Fernando&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ccapitalK&#34;&gt;ccapitalK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And many more! &lt;a href=&#34;https://github.com/SerenityOS/serenity/graphs/contributors&#34;&gt;See here&lt;/a&gt; for a full contributor list. The people listed above have landed more than 100 commits in the project. :^)&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is licensed under a 2-clause BSD license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CMU-Perceptual-Computing-Lab/openpose</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/CMU-Perceptual-Computing-Lab/openpose</id>
    <link href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/Logo_main_black.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Build Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Linux&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;MacOS&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Windows&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Build Status&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34;&gt;&lt;strong&gt;OpenPose&lt;/strong&gt;&lt;/a&gt; has represented the &lt;strong&gt;first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is &lt;strong&gt;authored by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;GinÃ©s Hidalgo&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~zhecao&#34;&gt;&lt;strong&gt;Zhe Cao&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34;&gt;&lt;strong&gt;Tomas Simon&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=sFQD3k4AAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Shih-En Wei&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://jhugestar.github.io&#34;&gt;&lt;strong&gt;Hanbyul Joo&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;http://www.cs.cmu.edu/~yaser&#34;&gt;&lt;strong&gt;Yaser Sheikh&lt;/strong&gt;&lt;/a&gt;. It is &lt;strong&gt;maintained by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;GinÃ©s Hidalgo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;. OpenPose would not be possible without the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34;&gt;&lt;strong&gt;CMU Panoptic Studio dataset&lt;/strong&gt;&lt;/a&gt;. We would also like to thank all the people who &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/09_authors_and_contributors.md&#34;&gt;has helped OpenPose in any way&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face_hands.gif&#34; width=&#34;480&#34;&gt; &lt;br&gt; &lt;sup&gt;Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;GinÃ©s Hidalgo&lt;/a&gt; (left) and &lt;a href=&#34;https://jhugestar.github.io&#34; target=&#34;_blank&#34;&gt;Hanbyul Joo&lt;/a&gt; (right) in front of the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34; target=&#34;_blank&#34;&gt;CMU Panoptic Studio&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#related-work&#34;&gt;Related Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#quick-start-overview&#34;&gt;Quick Start Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#send-us-feedback&#34;&gt;Send Us Feedback!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Whole-body (Body, Foot, Face, and Hands) 2D Pose Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/dance_foot.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_hands.gif&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;Testing OpenPose: (Left) &lt;a href=&#34;https://www.youtube.com/watch?v=2DiQUX11YaY&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;Crazy Uptown Funk flashmob in Sydney&lt;/i&gt;&lt;/a&gt; video sequence. (Center and right) Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;GinÃ©s Hidalgo&lt;/a&gt; and &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34; target=&#34;_blank&#34;&gt;Tomas Simon&lt;/a&gt; testing face and hands&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Whole-body 3D Pose Reconstruction and Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose3d.gif&#34; width=&#34;360&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; testing the OpenPose 3D Module&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Unity Plugin&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_main.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_body_foot.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_hand_face.png&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; and &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;GinÃ©s Hidalgo&lt;/a&gt; testing the &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34; target=&#34;_blank&#34;&gt;OpenPose Unity Plugin&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Runtime Analysis&lt;/h3&gt; &#xA;&lt;p&gt;We show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose_vs_competition.png&#34; width=&#34;360&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Main Functionality&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;2D real-time multi-person keypoint detection&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;15, 18 or &lt;strong&gt;25-keypoint body/foot keypoint estimation&lt;/strong&gt;, including &lt;strong&gt;6 foot keypoints&lt;/strong&gt;. &lt;strong&gt;Runtime invariant to number of detected people&lt;/strong&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;2x21-keypoint hand keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;70-keypoint face keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/3d_reconstruction_module.md&#34;&gt;&lt;strong&gt;3D real-time single-person keypoint detection&lt;/strong&gt;&lt;/a&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3D triangulation from multiple single views.&lt;/li&gt; &#xA;     &lt;li&gt;Synchronization of Flir cameras handled.&lt;/li&gt; &#xA;     &lt;li&gt;Compatible with Flir/Point Grey cameras.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/calibration_module.md&#34;&gt;&lt;strong&gt;Calibration toolbox&lt;/strong&gt;&lt;/a&gt;: Estimation of distortion, intrinsic, and extrinsic camera parameters.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Single-person tracking&lt;/strong&gt; for further speedup or visual smoothing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware compatibility&lt;/strong&gt;: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Usage Alternatives&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/01_demo.md&#34;&gt;&lt;strong&gt;Command-line demo&lt;/strong&gt;&lt;/a&gt; for built-in functionality.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/04_cpp_api.md/&#34;&gt;&lt;strong&gt;C++ API&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/03_python_api.md&#34;&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/a&gt; for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For further details, check the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/07_major_released_features.md&#34;&gt;major released features&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/08_release_notes.md&#34;&gt;release notes&lt;/a&gt; docs.&lt;/p&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose training code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/&#34;&gt;&lt;strong&gt;OpenPose foot dataset&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34;&gt;&lt;strong&gt;OpenPose Unity Plugin&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenPose papers published in &lt;strong&gt;IEEE TPAMI and CVPR&lt;/strong&gt;. Cite them in your publications if OpenPose helps your research! (Links and more details in the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt; section below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use OpenPose without installing or writing any code, simply &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#windows-portable-demo&#34;&gt;download and use the latest Windows portable version of OpenPose&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, you could &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source&#34;&gt;build OpenPose from source&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installation doc&lt;/a&gt; for all the alternatives.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Overview&lt;/h2&gt; &#xA;&lt;p&gt;Simply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also add any of the available flags in any order. E.g., the following example runs on a video (&lt;code&gt;--video {PATH}&lt;/code&gt;), enables face (&lt;code&gt;--face&lt;/code&gt;) and hands (&lt;code&gt;--hand&lt;/code&gt;), and saves the output keypoints on JSON files on disk (&lt;code&gt;--write_json {PATH}&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, you can also extend OpenPose&#39;s functionality from its Python and C++ APIs. After &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installing&lt;/a&gt; OpenPose, check its &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/00_index.md&#34;&gt;official doc&lt;/a&gt; for a quick overview of all the alternatives and tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;Send Us Feedback!&lt;/h2&gt; &#xA;&lt;p&gt;Our library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.&lt;/li&gt; &#xA; &lt;li&gt;Want to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/10_community_projects.md&#34;&gt;Community-based Projects&lt;/a&gt; section or even integrate it with OpenPose!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;, while the hand and face detectors also use &lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt; (the face detector was trained using the same procedure than the hand detector).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{8765346,&#xA;  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},&#xA;  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},&#xA;  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2019}&#xA;}&#xA;&#xA;@inproceedings{simon2017hand,&#xA;  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{cao2017realtime,&#xA;  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{wei2016cpm,&#xA;  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Convolutional pose machines},&#xA;  year = {2016}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Paper links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8765346&#34;&gt;IEEE TPAMI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;ArXiv&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.08050&#34;&gt;Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.00134&#34;&gt;Convolutional Pose Machines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/LICENSE&#34;&gt;license&lt;/a&gt; for further details. Interested in a commercial license? Check this &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt;. For commercial queries, use the &lt;code&gt;Contact&lt;/code&gt; section from the &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt; and also send a copy of that message to &lt;a href=&#34;mailto:yaser@cs.cmu.edu&#34;&gt;Yaser Sheikh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/leveldb</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/google/leveldb</id>
    <link href="https://github.com/google/leveldb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;ci&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Authors: Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keys and values are arbitrary byte arrays.&lt;/li&gt; &#xA; &lt;li&gt;Data is stored sorted by key.&lt;/li&gt; &#xA; &lt;li&gt;Callers can provide a custom comparison function to override the sort order.&lt;/li&gt; &#xA; &lt;li&gt;The basic operations are &lt;code&gt;Put(key,value)&lt;/code&gt;, &lt;code&gt;Get(key)&lt;/code&gt;, &lt;code&gt;Delete(key)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiple changes can be made in one atomic batch.&lt;/li&gt; &#xA; &lt;li&gt;Users can create a transient snapshot to get a consistent view of data.&lt;/li&gt; &#xA; &lt;li&gt;Forward and backward iteration is supported over the data.&lt;/li&gt; &#xA; &lt;li&gt;Data is automatically compressed using the &lt;a href=&#34;https://google.github.io/snappy/&#34;&gt;Snappy compression library&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/raw/main/doc/index.md&#34;&gt;LevelDB library documentation&lt;/a&gt; is online and bundled with the source code.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is not a SQL database. It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.&lt;/li&gt; &#xA; &lt;li&gt;Only a single process (possibly multi-threaded) can access a particular database at a time.&lt;/li&gt; &#xA; &lt;li&gt;There is no client-server support builtin to the library. An application that needs such support will have to wrap their own server around the library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting the Source&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/google/leveldb.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;This project supports &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; out of the box.&lt;/p&gt; &#xA;&lt;h3&gt;Build for POSIX&lt;/h3&gt; &#xA;&lt;p&gt;Quick start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake -DCMAKE_BUILD_TYPE=Release .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building for Windows&lt;/h3&gt; &#xA;&lt;p&gt;First generate the Visual Studio 2017 project/solution files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -G &#34;Visual Studio 15&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default default will build for x86. For 64-bit run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmake -G &#34;Visual Studio 15 Win64&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile the Windows solution from the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;devenv /build Debug leveldb.sln&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or open leveldb.sln in Visual Studio and build from within.&lt;/p&gt; &#xA;&lt;p&gt;Please see the CMake documentation and &lt;code&gt;CMakeLists.txt&lt;/code&gt; for more advanced usage.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing to the leveldb Project&lt;/h1&gt; &#xA;&lt;p&gt;The leveldb project welcomes contributions. leveldb&#39;s primary goal is to be a reliable and fast key/value store. Changes that are in line with the features/limitations outlined above, and meet the requirements below, will be considered.&lt;/p&gt; &#xA;&lt;p&gt;Contribution requirements:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tested platforms only&lt;/strong&gt;. We &lt;em&gt;generally&lt;/em&gt; will only accept changes for platforms that are compiled and tested. This means POSIX (for Linux and macOS) or Windows. Very small changes will sometimes be accepted, but consider that more of an exception than the rule.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stable API&lt;/strong&gt;. We strive very hard to maintain a stable API. Changes that require changes for projects using leveldb &lt;em&gt;might&lt;/em&gt; be rejected without sufficient benefit to the project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tests&lt;/strong&gt;: All changes must be accompanied by a new (or changed) test, or a sufficient explanation as to why a new (or changed) test is not required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Style&lt;/strong&gt;: This project conforms to the &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google C++ Style Guide&lt;/a&gt;. To ensure your changes are properly formatted please run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clang-format -i --style=file &amp;lt;file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We are unlikely to accept contributions to the build configuration files, such as &lt;code&gt;CMakeLists.txt&lt;/code&gt;. We are focused on maintaining a build configuration that allows us to test that the project works in a few supported configurations inside Google. We are not currently interested in supporting other requirements, such as different operating systems, compilers, or build systems.&lt;/p&gt; &#xA;&lt;h2&gt;Submitting a Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;Before any pull request will be accepted the author must first sign a Contributor License Agreement (CLA) at &lt;a href=&#34;https://cla.developers.google.com/&#34;&gt;https://cla.developers.google.com/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to keep the commit timeline linear &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits&#34;&gt;squash&lt;/a&gt; your changes down to a single commit and &lt;a href=&#34;https://git-scm.com/docs/git-rebase&#34;&gt;rebase&lt;/a&gt; on google/leveldb/main. This keeps the commit timeline linear and more easily sync&#39;ed with the internal repository at Google. More information at GitHub&#39;s &lt;a href=&#34;https://help.github.com/articles/about-git-rebase/&#34;&gt;About Git rebase&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Here is a performance report (with explanations) from the run of the included db_bench program. The results are somewhat noisy, but should be enough to get a ballpark performance estimate.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use a database with a million entries. Each entry has a 16 byte key, and a 100 byte value. Values used by the benchmark compress to about half their original size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LevelDB:    version 1.1&#xA;Date:       Sun May  1 12:11:26 2011&#xA;CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz&#xA;CPUCache:   4096 KB&#xA;Keys:       16 bytes each&#xA;Values:     100 bytes each (50 bytes after compression)&#xA;Entries:    1000000&#xA;Raw Size:   110.6 MB (estimated)&#xA;File Size:  62.9 MB (estimated)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Write performance&lt;/h2&gt; &#xA;&lt;p&gt;The &#34;fill&#34; benchmarks create a brand new database, in either sequential, or random order. The &#34;fillsync&#34; benchmark flushes data from the operating system to the disk after every operation; the other write operations leave the data sitting in the operating system buffer cache for a while. The &#34;overwrite&#34; benchmark does random writes that update existing keys in the database.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fillseq      :       1.765 micros/op;   62.7 MB/s&#xA;fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)&#xA;fillrandom   :       2.460 micros/op;   45.0 MB/s&#xA;overwrite    :       2.380 micros/op;   46.5 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each &#34;op&#34; above corresponds to a write of a single key/value pair. I.e., a random write benchmark goes at approximately 400,000 writes per second.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;fillsync&#34; operation costs much less (0.3 millisecond) than a disk seek (typically 10 milliseconds). We suspect that this is because the hard disk itself is buffering the update in its memory and responding before the data has been written to the platter. This may or may not be safe based on whether or not the hard disk has enough power to save its memory in the event of a power failure.&lt;/p&gt; &#xA;&lt;h2&gt;Read performance&lt;/h2&gt; &#xA;&lt;p&gt;We list the performance of reading sequentially in both the forward and reverse direction, and also the performance of a random lookup. Note that the database created by the benchmark is quite small. Therefore the report characterizes the performance of leveldb when the working set fits in memory. The cost of reading a piece of data that is not present in the operating system buffer cache will be dominated by the one or two disk seeks needed to fetch the data from disk. Write performance will be mostly unaffected by whether or not the working set fits in memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)&#xA;readseq     :  0.476 micros/op;  232.3 MB/s&#xA;readreverse :  0.724 micros/op;  152.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LevelDB compacts its underlying storage data in the background to improve read performance. The results listed above were done immediately after a lot of random writes. The results after compactions (which are usually triggered automatically) are better.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)&#xA;readseq     :  0.423 micros/op;  261.8 MB/s&#xA;readreverse :  0.663 micros/op;  166.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the high cost of reads comes from repeated decompression of blocks read from disk. If we supply enough cache to the leveldb so it can hold the uncompressed blocks in memory, the read performance improves again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)&#xA;readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository contents&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/index.md&#34;&gt;doc/index.md&lt;/a&gt; for more explanation. See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/impl.md&#34;&gt;doc/impl.md&lt;/a&gt; for a brief overview of the implementation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in include/leveldb/*.h. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Guide to header files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/db.h&lt;/strong&gt;: Main interface to the DB: Start here.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/options.h&lt;/strong&gt;: Control over the behavior of an entire database, and also control over the behavior of individual reads and writes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/comparator.h&lt;/strong&gt;: Abstraction for user-specified comparison function. If you want just bytewise comparison of keys, you can use the default comparator, but clients can write their own comparator implementations if they want custom ordering (e.g. to handle different character encodings, etc.).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/iterator.h&lt;/strong&gt;: Interface for iterating over data. You can get an iterator from a DB object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/write_batch.h&lt;/strong&gt;: Interface for atomically applying multiple updates to a database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/slice.h&lt;/strong&gt;: A simple module for maintaining a pointer and a length into some other byte array.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/status.h&lt;/strong&gt;: Status is returned from many of the public interfaces and is used to report success and various kinds of errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/env.h&lt;/strong&gt;: Abstraction of the OS environment. A posix implementation of this interface is in util/env_posix.cc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/table.h, include/leveldb/table_builder.h&lt;/strong&gt;: Lower-level modules that most clients probably won&#39;t use directly.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>electron/electron</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/electron/electron</id>
    <link href="https://github.com/electron/electron" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://electronjs.org&#34;&gt;&lt;img src=&#34;https://electronjs.org/images/electron-logo.svg?sanitize=true&#34; alt=&#34;Electron Logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/electron/electron/tree/main&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/electron/electron/tree/main.svg?style=shield&#34; alt=&#34;CircleCI Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/electron-bot/electron-ljo26/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/4lggi9dpjc1qob7k/branch/main?svg=true&#34; alt=&#34;AppVeyor Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/APGC3k5yaH&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/745037351163527189?color=%237289DA&amp;amp;label=chat&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Electron Discord Invite&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Available Translations: ğŸ‡¨ğŸ‡³ ğŸ‡§ğŸ‡· ğŸ‡ªğŸ‡¸ ğŸ‡¯ğŸ‡µ ğŸ‡·ğŸ‡º ğŸ‡«ğŸ‡· ğŸ‡ºğŸ‡¸ ğŸ‡©ğŸ‡ª. View these docs in other languages at &lt;a href=&#34;https://github.com/electron/i18n/tree/master/content/&#34;&gt;electron/i18n&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Electron framework lets you write cross-platform desktop applications using JavaScript, HTML and CSS. It is based on &lt;a href=&#34;https://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; and &lt;a href=&#34;https://www.chromium.org&#34;&gt;Chromium&lt;/a&gt; and is used by the &lt;a href=&#34;https://github.com/atom/atom&#34;&gt;Atom editor&lt;/a&gt; and many other &lt;a href=&#34;https://electronjs.org/apps&#34;&gt;apps&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/electronjs&#34;&gt;@ElectronJS&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; &#xA;&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&#34;https://github.com/electron/electron/tree/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&#34;mailto:coc@electronjs.org&#34;&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href=&#34;https://docs.npmjs.com/&#34;&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;. The preferred method is to install Electron as a development dependency in your app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install electron --save-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more installation options and troubleshooting tips, see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/installation.md&#34;&gt;installation&lt;/a&gt;. For info on how to manage Electron versions in your apps, see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/electron-versioning.md&#34;&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Platform support&lt;/h2&gt; &#xA;&lt;p&gt;Each Electron release provides binaries for macOS, Windows, and Linux.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS (El Capitan and up): Electron provides 64-bit Intel and ARM binaries for macOS. Apple Silicon support was added in Electron 11.&lt;/li&gt; &#xA; &lt;li&gt;Windows (Windows 7 and up): Electron provides &lt;code&gt;ia32&lt;/code&gt; (&lt;code&gt;x86&lt;/code&gt;), &lt;code&gt;x64&lt;/code&gt; (&lt;code&gt;amd64&lt;/code&gt;), and &lt;code&gt;arm64&lt;/code&gt; binaries for Windows. Windows on ARM support was added in Electron 5.0.8.&lt;/li&gt; &#xA; &lt;li&gt;Linux: The prebuilt binaries of Electron are built on Ubuntu 20.04. They have also been verified to work on: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ubuntu 14.04 and newer&lt;/li&gt; &#xA;   &lt;li&gt;Fedora 24 and newer&lt;/li&gt; &#xA;   &lt;li&gt;Debian 8 and newer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start &amp;amp; Electron Fiddle&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://github.com/electron/fiddle&#34;&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt; to build, run, and package small Electron experiments, to see code examples for all of Electron&#39;s APIs, and to try out different versions of Electron. It&#39;s designed to make the start of your journey with Electron easier.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, clone and run the &lt;a href=&#34;https://github.com/electron/electron-quick-start&#34;&gt;electron/electron-quick-start&lt;/a&gt; repository to see a minimal Electron app in action:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/electron/electron-quick-start&#xA;cd electron-quick-start&#xA;npm install&#xA;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources for learning Electron&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://electronjs.org/docs&#34;&gt;electronjs.org/docs&lt;/a&gt; - All of Electron&#39;s documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/fiddle&#34;&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/electron-quick-start&#34;&gt;electron/electron-quick-start&lt;/a&gt; - A very basic starter Electron app&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://electronjs.org/community#boilerplates&#34;&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Programmatic usage&lt;/h2&gt; &#xA;&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the binary. Use this to spawn Electron from Node scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const electron = require(&#39;electron&#39;)&#xA;const proc = require(&#39;child_process&#39;)&#xA;&#xA;// will print something similar to /Users/maf/.../Electron&#xA;console.log(electron)&#xA;&#xA;// spawn Electron&#xA;const child = proc.spawn(electron)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mirrors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://npmmirror.com/mirrors/electron/&#34;&gt;China&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.electronjs.org/docs/latest/tutorial/installation#mirror&#34;&gt;Advanced Installation Instructions&lt;/a&gt; to learn how to use a custom mirror.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation translations&lt;/h2&gt; &#xA;&lt;p&gt;We crowdsource translations for our documentation via &lt;a href=&#34;https://crowdin.com/project/electron&#34;&gt;Crowdin&lt;/a&gt;. We currently accept translations for Chinese (Simplified), French, German, Japanese, Portuguese, Russian, and Spanish.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we&#39;re looking for and how to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps, and more can be found on the &lt;a href=&#34;https://www.electronjs.org/community&#34;&gt;Community page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/electron/electron/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;When using Electron logos, make sure to follow &lt;a href=&#34;https://openjsf.org/wp-content/uploads/sites/84/2021/01/OpenJS-Foundation-Trademark-Policy-2021-01-12.docx.pdf&#34;&gt;OpenJS Foundation Trademark Policy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PolyMC/PolyMC</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/PolyMC/PolyMC</id>
    <link href="https://github.com/PolyMC/PolyMC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A custom launcher for Minecraft that allows you to easily manage multiple installations of Minecraft at once (Fork of MultiMC)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PolyMC/PolyMC/develop/program_info/polymc-header-black.svg#gh-light-mode-only&#34; alt=&#34;PolyMC logo&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PolyMC/PolyMC/develop/program_info/polymc-header.svg#gh-dark-mode-only&#34; alt=&#34;PolyMC logo&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;PolyMC is a custom launcher for Minecraft that focuses on predictability, long term stability and simplicity.&lt;/p&gt; &#xA;&lt;p&gt;This is a &lt;strong&gt;fork&lt;/strong&gt; of the MultiMC Launcher and not endorsed by MultiMC. If you want to read about why this fork was created, check out &lt;a href=&#34;https://polymc.org/wiki/overview/faq/&#34;&gt;our FAQ page&lt;/a&gt;. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All downloads and instructions for PolyMC can be found &lt;a href=&#34;https://polymc.org/download/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Last build status: &lt;a href=&#34;https://github.com/PolyMC/PolyMC/actions&#34;&gt;https://github.com/PolyMC/PolyMC/actions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development Builds&lt;/h2&gt; &#xA;&lt;p&gt;There are per-commit development builds available &lt;a href=&#34;https://github.com/PolyMC/PolyMC/actions&#34;&gt;here&lt;/a&gt;. These have debug information in the binaries, so their file sizes are relatively larger. Portable builds are provided for AppImage on Linux, Windows, and macOS.&lt;/p&gt; &#xA;&lt;p&gt;For Debian and Arch, you can use these packages for the latest development versions:&lt;br&gt; &lt;a href=&#34;https://aur.archlinux.org/packages/polymc-git/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/aur-polymc--git-blue&#34; alt=&#34;polymc-git&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mpr.makedeb.org/packages/polymc-git&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mpr-polymc--git-orange&#34; alt=&#34;polymc-git&#34;&gt;&lt;/a&gt;&lt;br&gt; For flatpak, you can use &lt;a href=&#34;https://discourse.flathub.org/t/how-to-use-flathub-beta/2111&#34;&gt;flathub-beta&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Help &amp;amp; Support&lt;/h1&gt; &#xA;&lt;p&gt;Feel free to create an issue if you need help. However, you might find it easier to ask in the Discord server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/xq7fxrgtMP&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/923671181020766230?label=PolyMC%20Discord&#34; alt=&#34;PolyMC Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For people who don&#39;t want to use Discord, we have a Matrix Space which is bridged to the Discord server:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#polymc:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc:matrix.org?label=PolyMC%20space&#34; alt=&#34;PolyMC Space&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If there are any issues with the space or you are using a client that does not support the feature here are the individual rooms:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#polymc-development:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-development:matrix.org?label=PolyMC%20Development&#34; alt=&#34;Development&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-discussion:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-discussion:matrix.org?label=PolyMC%20Discussion&#34; alt=&#34;Discussion&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-github:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-github:matrix.org?label=PolyMC%20Github&#34; alt=&#34;Github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-maintainers:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-maintainers:matrix.org?label=PolyMC%20Maintainers&#34; alt=&#34;Maintainers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-news:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-news:matrix.org?label=PolyMC%20News&#34; alt=&#34;News&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-offtopic:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-offtopic:matrix.org?label=PolyMC%20Offtopic&#34; alt=&#34;Offtopic&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-support:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-support:matrix.org?label=PolyMC%20Support&#34; alt=&#34;Support&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-voice:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-voice:matrix.org?label=PolyMC%20Voice&#34; alt=&#34;Voice&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;we also have a subreddit you can post your issues and suggestions on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.reddit.com/r/PolyMCLauncher/&#34;&gt;r/PolyMCLauncher&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;If you want to contribute to PolyMC you might find it useful to join our Discord Server or Matrix Space.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;If you want to build PolyMC yourself, check &lt;a href=&#34;https://polymc.org/wiki/development/build-instructions/&#34;&gt;Build Instructions&lt;/a&gt; for build instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Code formatting&lt;/h2&gt; &#xA;&lt;p&gt;Just follow the existing formatting.&lt;/p&gt; &#xA;&lt;p&gt;In general, in order of importance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure your IDE is not messing up line endings or whitespace and avoid using linters.&lt;/li&gt; &#xA; &lt;li&gt;Prefer readability over dogma.&lt;/li&gt; &#xA; &lt;li&gt;Keep to the existing formatting.&lt;/li&gt; &#xA; &lt;li&gt;Indent with 4 space unless it&#39;s in a submodule.&lt;/li&gt; &#xA; &lt;li&gt;Keep lists (of arguments, parameters, initializers...) as lists, not paragraphs. It should either read from top to bottom, or left to right. Not both.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;p&gt;The translation effort for PolyMC is hosted on &lt;a href=&#34;https://hosted.weblate.org/projects/polymc/polymc/&#34;&gt;Weblate&lt;/a&gt; and information about translating PolyMC is available at &lt;a href=&#34;https://github.com/PolyMC/Translations&#34;&gt;https://github.com/PolyMC/Translations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download information&lt;/h2&gt; &#xA;&lt;p&gt;To modify download information or change packaging information send a pull request or issue to the website &lt;a href=&#34;https://github.com/PolyMC/polymc.github.io/raw/master/src/download.md&#34;&gt;Here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Forking/Redistributing/Custom builds policy&lt;/h2&gt; &#xA;&lt;p&gt;Do whatever you want, we don&#39;t care. Just follow the license. If you have any questions about this feel free to ask in an issue.&lt;/p&gt; &#xA;&lt;p&gt;Be aware that if you build this software without removing the provided API keys in &lt;a href=&#34;https://raw.githubusercontent.com/PolyMC/PolyMC/develop/CMakeLists.txt&#34;&gt;CMakeLists.txt&lt;/a&gt; you are accepting the following terms and conditions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/legal/microsoft-identity-platform/terms-of-use&#34;&gt;Microsoft Identity Platform Terms of Use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://support.curseforge.com/en/support/solutions/articles/9000207405-curse-forge-3rd-party-api-terms-and-conditions&#34;&gt;CurseForge 3rd Party API Terms and Conditions&lt;/a&gt; If you do not agree with these terms and conditions, then remove the associated API keys from the &lt;a href=&#34;https://raw.githubusercontent.com/PolyMC/PolyMC/develop/CMakeLists.txt&#34;&gt;CMakeLists.txt&lt;/a&gt; file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All launcher code is available under the GPL-3.0-only license.&lt;/p&gt; &#xA;&lt;p&gt;The logo and related assets are under the CC BY-SA 4.0 license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/folly</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/facebook/folly</id>
    <link href="https://github.com/facebook/folly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source C++ library developed and used at Facebook.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Folly: Facebook Open-source Library&lt;/h1&gt; &#xA;&lt;a href=&#34;https://opensource.facebook.com/support-ukraine&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine - Help Provide Humanitarian Aid to Ukraine.&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;What is &lt;code&gt;folly&lt;/code&gt;?&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebook/folly/main/static/logo.svg?sanitize=true&#34; alt=&#34;Logo Folly&#34; width=&#34;15%&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;Folly (acronymed loosely after Facebook Open Source Library) is a library of C++14 components designed with practicality and efficiency in mind. &lt;strong&gt;Folly contains a variety of core library components used extensively at Facebook&lt;/strong&gt;. In particular, it&#39;s often a dependency of Facebook&#39;s other open source C++ efforts and place where those projects can share code.&lt;/p&gt; &#xA;&lt;p&gt;It complements (as opposed to competing against) offerings such as Boost and of course &lt;code&gt;std&lt;/code&gt;. In fact, we embark on defining our own component only when something we need is either not available, or does not meet the needed performance profile. We endeavor to remove things from folly if or when &lt;code&gt;std&lt;/code&gt; or Boost obsoletes them.&lt;/p&gt; &#xA;&lt;p&gt;Performance concerns permeate much of Folly, sometimes leading to designs that are more idiosyncratic than they would otherwise be (see e.g. &lt;code&gt;PackedSyncPtr.h&lt;/code&gt;, &lt;code&gt;SmallLocks.h&lt;/code&gt;). Good performance at large scale is a unifying theme in all of Folly.&lt;/p&gt; &#xA;&lt;h2&gt;Check it out in the intro video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Wr_IfOICYSs&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/Wr_IfOICYSs/0.jpg&#34; alt=&#34;Explain Like Iâ€™m 5: Folly&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Logical Design&lt;/h1&gt; &#xA;&lt;p&gt;Folly is a collection of relatively independent components, some as simple as a few symbols. There is no restriction on internal dependencies, meaning that a given folly module may use any other folly components.&lt;/p&gt; &#xA;&lt;p&gt;All symbols are defined in the top-level namespace &lt;code&gt;folly&lt;/code&gt;, except of course macros. Macro names are ALL_UPPERCASE and should be prefixed with &lt;code&gt;FOLLY_&lt;/code&gt;. Namespace &lt;code&gt;folly&lt;/code&gt; defines other internal namespaces such as &lt;code&gt;internal&lt;/code&gt; or &lt;code&gt;detail&lt;/code&gt;. User code should not depend on symbols in those namespaces.&lt;/p&gt; &#xA;&lt;p&gt;Folly has an &lt;code&gt;experimental&lt;/code&gt; directory as well. This designation connotes primarily that we feel the API may change heavily over time. This code, typically, is still in heavy use and is well tested.&lt;/p&gt; &#xA;&lt;h1&gt;Physical Design&lt;/h1&gt; &#xA;&lt;p&gt;At the top level Folly uses the classic &#34;stuttering&#34; scheme &lt;code&gt;folly/folly&lt;/code&gt; used by Boost and others. The first directory serves as an installation root of the library (with possible versioning a la &lt;code&gt;folly-1.0/&lt;/code&gt;), and the second is to distinguish the library when including files, e.g. &lt;code&gt;#include &amp;lt;folly/FBString.h&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The directory structure is flat (mimicking the namespace structure), i.e. we don&#39;t have an elaborate directory hierarchy (it is possible this will change in future versions). The subdirectory &lt;code&gt;experimental&lt;/code&gt; contains files that are used inside folly and possibly at Facebook but not considered stable enough for client use. Your code should not use files in &lt;code&gt;folly/experimental&lt;/code&gt; lest it may break when you update Folly.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;folly/folly/test&lt;/code&gt; subdirectory includes the unittests for all components, usually named &lt;code&gt;ComponentXyzTest.cpp&lt;/code&gt; for each &lt;code&gt;ComponentXyz.*&lt;/code&gt;. The &lt;code&gt;folly/folly/docs&lt;/code&gt; directory contains documentation.&lt;/p&gt; &#xA;&lt;h1&gt;What&#39;s in it?&lt;/h1&gt; &#xA;&lt;p&gt;Because of folly&#39;s fairly flat structure, the best way to see what&#39;s in it is to look at the headers in &lt;a href=&#34;https://github.com/facebook/folly/tree/main/folly&#34;&gt;top level &lt;code&gt;folly/&lt;/code&gt; directory&lt;/a&gt;. You can also check the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/folly/main/folly/docs&#34;&gt;&lt;code&gt;docs&lt;/code&gt; folder&lt;/a&gt; for documentation, starting with the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/folly/main/folly/docs/Overview.md&#34;&gt;overview&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Folly is published on GitHub at &lt;a href=&#34;https://github.com/facebook/folly&#34;&gt;https://github.com/facebook/folly&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Build Notes&lt;/h1&gt; &#xA;&lt;p&gt;Because folly does not provide any ABI compatibility guarantees from commit to commit, we generally recommend building folly as a static library.&lt;/p&gt; &#xA;&lt;p&gt;folly supports gcc (5.1+), clang, or MSVC. It should run on Linux (x86-32, x86-64, and ARM), iOS, macOS, and Windows (x86-64). The CMake build is only tested on some of these platforms; at a minimum, we aim to support macOS and Linux (on the latest Ubuntu LTS release or newer.)&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;getdeps.py&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This script is used by many of Meta&#39;s OSS tools. It will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s written in python so you&#39;ll need python3.6 or later on your PATH. It works on Linux, macOS and Windows.&lt;/p&gt; &#xA;&lt;p&gt;The settings for folly&#39;s cmake build are held in its getdeps manifest &lt;code&gt;build/fbcode_builder/manifests/folly&lt;/code&gt;, which you can edit locally if desired.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;If on Linux or MacOS (with homebrew installed) you can install system dependencies to save building them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo&#xA;git clone https://github.com/facebook/folly&#xA;# Install dependencies&#xA;cd folly&#xA;sudo ./build/fbcode_builder/getdeps.py install-system-deps --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to see the packages before installing them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build/fbcode_builder/getdeps.py install-system-deps --dry-run --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On other platforms or if on Linux and without system dependencies &lt;code&gt;getdeps.py&lt;/code&gt; will mostly download and build them for you during the build step.&lt;/p&gt; &#xA;&lt;p&gt;Some of the dependencies &lt;code&gt;getdeps.py&lt;/code&gt; uses and installs are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a version of boost compiled with C++14 support.&lt;/li&gt; &#xA; &lt;li&gt;googletest is required to build and run folly&#39;s tests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;This script will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; currently requires python 3.6+ to be on your path.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; will invoke cmake etc&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo&#xA;git clone https://github.com/facebook/folly&#xA;cd folly&#xA;# Build, using system dependencies if available&#xA;python3 ./build/fbcode_builder/getdeps.py --allow-system-packages build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It puts output in its scratch area:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;installed/folly/lib/libfolly.a&lt;/code&gt;: Library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also specify a &lt;code&gt;--scratch-path&lt;/code&gt; argument to control the location of the scratch directory used for the build. You can find the default scratch install location from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-inst-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are also &lt;code&gt;--install-dir&lt;/code&gt; and &lt;code&gt;--install-prefix&lt;/code&gt; arguments to provide some more fine-grained control of the installation directories. However, given that folly provides no compatibility guarantees between commits we generally recommend building and installing the libraries to a temporary location, and then pointing your project&#39;s build at this temporary location, rather than installing folly in the traditional system installation directories. e.g., if you are building with CMake you can use the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; variable to allow CMake to find folly in this temporary installation directory when building your project.&lt;/p&gt; &#xA;&lt;p&gt;If you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run tests&lt;/h3&gt; &#xA;&lt;p&gt;By default &lt;code&gt;getdeps.py&lt;/code&gt; will build the tests for folly. To run them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd folly&#xA;python3 ./build/fbcode_builder/getdeps.py --allow-system-packages test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;build.sh&lt;/code&gt;/&lt;code&gt;build.bat&lt;/code&gt; wrapper&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;build.sh&lt;/code&gt; can be used on Linux and MacOS, on Windows use the &lt;code&gt;build.bat&lt;/code&gt; script instead. Its a wrapper around &lt;code&gt;getdeps.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build with cmake directly&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t want to let getdeps invoke cmake for you then by default, building the tests is disabled as part of the CMake &lt;code&gt;all&lt;/code&gt; target. To build the tests, specify &lt;code&gt;-DBUILD_TESTS=ON&lt;/code&gt; to CMake at configure time.&lt;/p&gt; &#xA;&lt;p&gt;NB if you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate on a &lt;code&gt;getdeps.py&lt;/code&gt; build, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch-path build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Running tests with ctests also works if you cd to the build dir, e.g. &lt;code&gt; &lt;/code&gt;(cd $(python3 ./build/fbcode_builder/getdeps.py show-build-dir) &amp;amp;&amp;amp; ctest)`&lt;/p&gt; &#xA;&lt;h3&gt;Finding dependencies in non-default locations&lt;/h3&gt; &#xA;&lt;p&gt;If you have boost, gtest, or other dependencies installed in a non-default location, you can use the &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;CMAKE_LIBRARY_PATH&lt;/code&gt; variables to make CMAKE look also look for header files and libraries in non-standard locations. For example, to also search the directories &lt;code&gt;/alt/include/path1&lt;/code&gt; and &lt;code&gt;/alt/include/path2&lt;/code&gt; for header files and the directories &lt;code&gt;/alt/lib/path1&lt;/code&gt; and &lt;code&gt;/alt/lib/path2&lt;/code&gt; for libraries, you can invoke &lt;code&gt;cmake&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake \&#xA;  -DCMAKE_INCLUDE_PATH=/alt/include/path1:/alt/include/path2 \&#xA;  -DCMAKE_LIBRARY_PATH=/alt/lib/path1:/alt/lib/path2 ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ubuntu LTS, CentOS Stream, Fedora&lt;/h2&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;getdeps.py&lt;/code&gt; approach above. We test in CI on Ubuntu LTS, and occasionally on other distros.&lt;/p&gt; &#xA;&lt;p&gt;If you find the set of system packages is not quite right for your chosen distro, you can specify distro version specific overrides in the dependency manifests (e.g. &lt;a href=&#34;https://github.com/facebook/folly/raw/main/build/fbcode_builder/manifests/boost&#34;&gt;https://github.com/facebook/folly/blob/main/build/fbcode_builder/manifests/boost&lt;/a&gt; ). You could probably make it work on most recent Ubuntu/Debian or Fedora/Redhat derived distributions.&lt;/p&gt; &#xA;&lt;p&gt;At time of writing (Dec 2021) there is a build break on GCC 11.x based systems in lang_badge_test. If you don&#39;t need badge functionality you can work around by commenting it out from CMakeLists.txt (unfortunately fbthrift does need it)&lt;/p&gt; &#xA;&lt;h2&gt;Windows (Vcpkg)&lt;/h2&gt; &#xA;&lt;p&gt;Note that many tests are disabled for folly Windows builds, you can see them in the log from the cmake configure step, or by looking for WINDOWS_DISABLED in &lt;code&gt;CMakeLists.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;That said, &lt;code&gt;getdeps.py&lt;/code&gt; builds work on Windows and are tested in CI.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer, you can try Vcpkg. folly is available in &lt;a href=&#34;https://github.com/Microsoft/vcpkg#vcpkg&#34;&gt;Vcpkg&lt;/a&gt; and releases may be built via &lt;code&gt;vcpkg install folly:x64-windows&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also use &lt;code&gt;vcpkg install folly:x64-windows --head&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;macOS&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; builds work on macOS and are tested in CI, however if you prefer, you can try one of the macOS package managers&lt;/p&gt; &#xA;&lt;h3&gt;Homebrew&lt;/h3&gt; &#xA;&lt;p&gt;folly is available as a Formula and releases may be built via &lt;code&gt;brew install folly&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also use &lt;code&gt;folly/build/bootstrap-osx-homebrew.sh&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  ./folly/build/bootstrap-osx-homebrew.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a build directory &lt;code&gt;_build&lt;/code&gt; in the top-level.&lt;/p&gt; &#xA;&lt;h3&gt;MacPorts&lt;/h3&gt; &#xA;&lt;p&gt;Install the required packages from MacPorts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  sudo port install \&#xA;    boost \&#xA;    cmake \&#xA;    gflags \&#xA;    git \&#xA;    google-glog \&#xA;    libevent \&#xA;    libtool \&#xA;    lz4 \&#xA;    lzma \&#xA;    openssl \&#xA;    snappy \&#xA;    xz \&#xA;    zlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and install double-conversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/google/double-conversion.git&#xA;  cd double-conversion&#xA;  cmake -DBUILD_SHARED_LIBS=ON .&#xA;  make&#xA;  sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and install folly with the parameters listed below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/facebook/folly.git&#xA;  cd folly&#xA;  mkdir _build&#xA;  cd _build&#xA;  cmake ..&#xA;  make&#xA;  sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>PCSX2/pcsx2</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/PCSX2/pcsx2</id>
    <link href="https://github.com/PCSX2/pcsx2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PCSX2 - The Playstation 2 Emulator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PCSX2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%96%A5%EF%B8%8F%20Windows%20Builds/master?label=Windows%20Builds&#34; alt=&#34;Windows Build Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%90%A7%20Linux%20Builds/master?label=Linux%20Builds&#34; alt=&#34;Linux Build Status&#34;&gt; &lt;a href=&#34;https://www.codacy.com/gh/PCSX2/pcsx2/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=PCSX2/pcsx2&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/1f7c0d75fec74d6daa6adb084e5b4f71&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/TCz3t9k&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/309643527816609793?color=%235CA8FA&amp;amp;label=PCSX2%20Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCSX2 is a free and open-source PlayStation 2 (PS2) emulator. Its purpose is to emulate the PS2&#39;s hardware, using a combination of MIPS CPU &lt;a href=&#34;https://en.wikipedia.org/wiki/Interpreter_(computing)&#34;&gt;Interpreters&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_recompilation&#34;&gt;Recompilers&lt;/a&gt; and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Virtual_machine&#34;&gt;Virtual Machine&lt;/a&gt; which manages hardware states and PS2 system memory. This allows you to play PS2 games on your PC, with many additional features and benefits.&lt;/p&gt; &#xA;&lt;h2&gt;Project Details&lt;/h2&gt; &#xA;&lt;p&gt;The PCSX2 project has been running for more than ten years. Past versions could only run a few public domain game demos, but newer versions can run most games at full speed, including popular titles such as Final Fantasy X and Devil May Cry 3. Visit the &lt;a href=&#34;https://pcsx2.net/compat/&#34;&gt;PCSX2 compatibility list&lt;/a&gt; to check the latest compatibility status of games (with more than 2500 titles tested), or ask for help in the &lt;a href=&#34;https://forums.pcsx2.net/&#34;&gt;official forums&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The latest officially released stable version is version 1.6.0.&lt;/p&gt; &#xA;&lt;p&gt;Installers and binaries for both stable and development builds are available from &lt;a href=&#34;https://pcsx2.net/downloads/&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Minimum&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 8.1 or newer (64 bit) &lt;br&gt; - Ubuntu 18.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports SSE4.1 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 1600 &lt;br&gt; - Two physical cores, with hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D10 support &lt;br&gt; - OpenGL 3.x support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 3000 (GeForce GTX 750) &lt;br&gt; - 2 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;4 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended Single Thread Performance is based on moderately complex games. Games that pushed the PS2 hardware to its limits will struggle on CPUs at this level. Some release titles and 2D games which underutilized the PS2 hardware may run on CPUs rated as low as 1200. A quick reference for CPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:CPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;, &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-The-Most-CPU-Intensive-Games&#34;&gt;Forum&lt;/a&gt; and CPU &lt;strong&gt;light&lt;/strong&gt; games: &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-Games-that-don-t-need-a-strong-CPU-to-emulate&#34;&gt;Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommended&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 10 (64 bit) &lt;br&gt; - Ubuntu 19.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports AVX2 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 2100 &lt;br&gt; - Four physical cores, with or without hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D11 support &lt;br&gt; - OpenGL 4.6 support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 6000 (GeForce GTX 1050 Ti) &lt;br&gt; - 4 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended GPU is based on 3x Internal, ~1080p resolution requirements. Higher resolutions will require stronger cards; 6x Internal, ~4K resolution will require a &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 12000 (GeForce GTX 1070 Ti). Just like CPU requirements, this is also highly game dependent. A quick reference for GPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:GPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Technical Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need the &lt;a href=&#34;https://support.microsoft.com/en-us/help/2977003/&#34;&gt;Visual C++ 2019 x86 Redistributables&lt;/a&gt; to run PCSX2.&lt;/li&gt; &#xA; &lt;li&gt;Windows XP and Direct3D9 support was dropped after stable release 1.4.0.&lt;/li&gt; &#xA; &lt;li&gt;Windows 7 and Windows 8.0 support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;32 bit support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to update your operating system and drivers to ensure you have the best experience possible. Having a newer GPU is also recommended so you have the latest supported drivers.&lt;/li&gt; &#xA; &lt;li&gt;Because of copyright issues, and the complexity of trying to work around it, you need a BIOS dump extracted from a legitimately-owned PS2 console to use the emulator. For more information about the BIOS and how to get it from your console, visit &lt;a href=&#34;https://raw.githubusercontent.com/PCSX2/pcsx2/master/pcsx2/Docs/PCSX2_FAQ.md#question-13-where-do-i-get-a-ps2-bios&#34;&gt;this page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PCSX2 uses two CPU cores for emulation by default. A third core can be used via the MTVU speed hack, which is compatible with most games. This can be a significant speedup on CPUs with 3+ cores, but it may be a slowdown on GS-limited games (or on CPUs with fewer than 2 cores). Software renderers will then additionally use however many rendering threads it is set to and will need higher core counts to run efficiently.&lt;/li&gt; &#xA; &lt;li&gt;Requirements benchmarks are based on a statistic from the Passmark CPU bench marking software. When we say &#34;STR&#34;, we are referring to Passmark&#39;s &#34;Single Thread Rating&#34; statistic. You can look up your CPU on &lt;a href=&#34;https://cpubenchmark.net&#34;&gt;Passmark&#39;s website for CPUs&lt;/a&gt; to see how it compares to PCSX2&#39;s requirements.&lt;/li&gt; &#xA; &lt;li&gt;Vulkan requires an up-to-date GPU driver; old drivers may cause graphical problems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want more? &lt;a href=&#34;https://pcsx2.net/&#34;&gt;Check out the PCSX2 website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/pytorch</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/pytorch/pytorch</id>
    <link href="https://github.com/pytorch/pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/pytorch-logo-dark.png&#34; alt=&#34;PyTorch Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt; &#xA; &lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.&lt;/p&gt; &#xA;&lt;p&gt;Our trunk health (Continuous Integration signals) can be found at &lt;a href=&#34;https://hud.pytorch.org/ci/pytorch/pytorch/master&#34;&gt;hud.pytorch.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#more-about-pytorch&#34;&gt;More About PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#a-gpu-ready-tensor-library&#34;&gt;A GPU-Ready Tensor Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#dynamic-neural-networks-tape-based-autograd&#34;&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#python-first&#34;&gt;Python First&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#imperative-experiences&#34;&gt;Imperative Experiences&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#fast-and-lean&#34;&gt;Fast and Lean&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#extensions-without-pain&#34;&gt;Extensions Without Pain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#binaries&#34;&gt;Binaries&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#nvidia-jetson-platforms&#34;&gt;NVIDIA Jetson Platforms&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#from-source&#34;&gt;From Source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-dependencies&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#get-the-pytorch-source&#34;&gt;Get the PyTorch Source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-pytorch&#34;&gt;Install PyTorch&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#adjust-build-options-optional&#34;&gt;Adjust Build Options (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#docker-image&#34;&gt;Docker Image&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#using-pre-built-images&#34;&gt;Using pre-built images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-image-yourself&#34;&gt;Building the image yourself&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-documentation&#34;&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#previous-versions&#34;&gt;Previous Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#releases-and-contributing&#34;&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34;&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a Tensor library like NumPy, with strong GPU support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html&#34;&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/jit.html&#34;&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/nn.html&#34;&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/multiprocessing.html&#34;&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/data.html&#34;&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, PyTorch is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A replacement for NumPy to use the power of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;A GPU-Ready Tensor Library&lt;/h3&gt; &#xA;&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/tensor_illustration.png&#34; alt=&#34;Tensor illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.&lt;/p&gt; &#xA;&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt; &#xA;&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt; &#xA;&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as &lt;a href=&#34;https://github.com/twitter/torch-autograd&#34;&gt;torch-autograd&lt;/a&gt;, &lt;a href=&#34;https://github.com/HIPS/autograd&#34;&gt;autograd&lt;/a&gt;, &lt;a href=&#34;https://chainer.org&#34;&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;While this technique is not unique to PyTorch, it&#39;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif&#34; alt=&#34;Dynamic graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python First&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; / &lt;a href=&#34;https://www.scipy.org/&#34;&gt;SciPy&lt;/a&gt; / &lt;a href=&#34;https://scikit-learn.org&#34;&gt;scikit-learn&lt;/a&gt; etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt; and &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;. Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt; &#xA;&lt;h3&gt;Imperative Experiences&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn&#39;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Lean&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries such as &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel MKL&lt;/a&gt; and NVIDIA (&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.&lt;/p&gt; &#xA;&lt;p&gt;Hence, PyTorch is quite fast â€“ whether you run small or large neural networks.&lt;/p&gt; &#xA;&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We&#39;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions Without Pain&lt;/h3&gt; &#xA;&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch&#39;s Tensor API was designed to be straightforward and with minimal abstractions.&lt;/p&gt; &#xA;&lt;p&gt;You can write new neural network layers in Python using the torch API &lt;a href=&#34;https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html&#34;&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;a tutorial here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/extension-cpp&#34;&gt;an example here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;p&gt;Commands to install binaries via Conda or pip wheels are on our website: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;NVIDIA Jetson Platforms&lt;/h4&gt; &#xA;&lt;p&gt;Python wheels for NVIDIA&#39;s Jetson Nano, Jetson TX2, and Jetson AGX Xavier are provided &lt;a href=&#34;https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048&#34;&gt;here&lt;/a&gt; and the L4T container is published &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They require JetPack 4.2 and above, and &lt;a href=&#34;https://github.com/dusty-nv&#34;&gt;@dusty-nv&lt;/a&gt; and &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;@ptrblck&lt;/a&gt; are maintaining them.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;If you are installing from source, you will need Python 3.7 or later and a C++14 compiler. Also, we highly recommend installing an &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt; &#xA;&lt;p&gt;Once you have &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; installed, here are the instructions.&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with CUDA support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVIDIA CUDA&lt;/a&gt; 10.2 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;NVIDIA cuDNN&lt;/a&gt; v7 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/ax3l/9489132&#34;&gt;Compiler&lt;/a&gt; compatible with CUDA Note: You could refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf&#34;&gt;cuDNN Support Matrix&lt;/a&gt; for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardwares&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable CUDA support, export the environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are building for NVIDIA&#39;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are &lt;a href=&#34;https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/&#34;&gt;available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with ROCm support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html&#34;&gt;AMD ROCm&lt;/a&gt; 4.0 and above installation&lt;/li&gt; &#xA; &lt;li&gt;ROCm is currently supported only for Linux systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable ROCm support, export the environment variable &lt;code&gt;USE_ROCM=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Common&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# CUDA only: Add LAPACK support for the GPU if needed&#xA;conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed&#xA;conda install pkg-config libuv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed.&#xA;# Distributed package support on Windows is a prototype feature and is subject to changes.&#xA;conda install -c conda-forge libuv=1.39&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Get the PyTorch Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/pytorch/pytorch&#xA;cd pytorch&#xA;# if you are updating an existing checkout&#xA;git submodule sync&#xA;git submodule update --init --recursive --jobs 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are compiling for ROCm, you must run this command first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/amd_build/build_amd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are using &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt;, you may experience an error caused by the linker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized&#xA;collect2: error: ld returned 1 exit status&#xA;error: command &#39;g++&#39; failed with exit status 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is caused by &lt;code&gt;ld&lt;/code&gt; from Conda environment shadowing the system &lt;code&gt;ld&lt;/code&gt;. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.7.6+ and 3.8.1+.&lt;/p&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA is not supported on macOS.&lt;/p&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;p&gt;Choose Correct Visual Studio Version.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes there are regressions in new versions of Visual Studio, so it&#39;s best to use the same Visual Studio Version &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.circleci/scripts/vs_install.ps1&#34;&gt;16.8.5&lt;/a&gt; as Pytorch CI&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from &lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;https://visualstudio.microsoft.com/visual-cpp-build-tools/&lt;/a&gt;. The build tools &lt;em&gt;do not&lt;/em&gt; come with Visual Studio Code by default.&lt;/p&gt; &#xA;&lt;p&gt;If you want to build legacy python code, please refer to &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md#building-on-legacy-code-and-cuda&#34;&gt;Building on legacy code and CUDA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build with CPU&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s fairly easy to build with CPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;conda activate&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#39;ll need to manually download the library and set up the building environment by tweaking &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;LIB&lt;/code&gt;. The instruction &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/notes/windows.rst#building-from-source&#34;&gt;here&lt;/a&gt; is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.&lt;/p&gt; &#xA;&lt;p&gt;Build with CUDA&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm&#34;&gt;NVTX&lt;/a&gt; is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called &#34;Nsight Compute&#34;. To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. &lt;br&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.&lt;/p&gt; &#xA;&lt;p&gt;Additional libraries such as &lt;a href=&#34;https://developer.nvidia.com/magma&#34;&gt;Magma&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN, a.k.a MKLDNN or DNNL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/mozilla/sccache&#34;&gt;Sccache&lt;/a&gt; are often needed. Please refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/tree/master/.jenkins/pytorch/win-test-helpers/installation-helpers&#34;&gt;installation-helper&lt;/a&gt; to install them.&lt;/p&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat&#34;&gt;build_pytorch.bat&lt;/a&gt; script for some other environment variables configurations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmd&#xA;&#xA;:: Set the environment variables after you have downloaded and upzipped the mkl package,&#xA;:: else CMake would throw an error as `Could NOT find OpenMP`.&#xA;set CMAKE_INCLUDE_PATH={Your directory}\mkl\include&#xA;set LIB={Your directory}\mkl\lib;%LIB%&#xA;&#xA;:: Read the content in the previous section carefully before you proceed.&#xA;:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&#xA;:: &#34;Visual Studio 2019 Developer Command Prompt&#34; will be run automatically.&#xA;:: Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&#xA;set CMAKE_GENERATOR_TOOLSET_VERSION=14.27&#xA;set DISTUTILS_USE_SDK=1&#xA;for /f &#34;usebackq tokens=*&#34; %i in (`&#34;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&#34; -version [15^,17^) -products * -latest -property installationPath`) do call &#34;%i\VC\Auxiliary\Build\vcvarsall.bat&#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%&#xA;&#xA;:: [Optional] If you want to override the CUDA host compiler&#xA;set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe&#xA;&#xA;python setup.py install&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Adjust Build Options (Optional)&lt;/h5&gt; &#xA;&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.&lt;/p&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;h4&gt;Using pre-built images&lt;/h4&gt; &#xA;&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building the image yourself&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;# images are tagged as docker.io/${your_docker_username}/pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the Documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build documentation in various formats, you will need &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the &lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt; &#xA;&lt;p&gt;If you get a katex error run &lt;code&gt;npm install katex&lt;/code&gt;. If it persists, try &lt;code&gt;npm install -g katex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Previous Versions&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found on &lt;a href=&#34;https://pytorch.org/previous-versions&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Three-pointers to get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;Examples: easy to understand PyTorch code across all domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/&#34;&gt;The API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/GLOSSARY.md&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;PyTorch Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;Intro to Deep Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34;&gt;Intro to Machine Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-networks-with-pytorch&#34;&gt;Deep Neural Networks with PyTorch from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PyTorch&#34;&gt;PyTorch Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/&#34;&gt;PyTorch Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw&#34;&gt;PyTorch YouTube&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: Discuss implementations, research, etc. &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt; &#xA; &lt;li&gt;Slack: The &lt;a href=&#34;https://pytorch.slack.com/&#34;&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href=&#34;https://goo.gl/forms/PP1AGvNHpSaJP8to1&#34;&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: &lt;a href=&#34;https://eepurl.com/cbG0rv&#34;&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Page: Important announcements about PyTorch. &lt;a href=&#34;https://www.facebook.com/pytorch&#34;&gt;https://www.facebook.com/pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For brand guidelines, please visit our website at &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a 90-day release cycle (major releases). Please let us know if you encounter a bug by &lt;a href=&#34;https://github.com/pytorch/pytorch/issues&#34;&gt;filing an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about making a contribution to Pytorch, please see our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch is currently maintained by &lt;a href=&#34;https://apaszke.github.io/&#34;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&#34;https://github.com/colesbury&#34;&gt;Sam Gross&lt;/a&gt;, &lt;a href=&#34;http://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt; and &lt;a href=&#34;https://github.com/gchanan&#34;&gt;Gregory Chanan&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project is unrelated to &lt;a href=&#34;https://github.com/hughperkins/pytorch&#34;&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openvinotoolkit/openvino</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/openvinotoolkit/openvino</id>
    <link href="https://github.com/openvinotoolkit/openvino" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenVINOâ„¢ Toolkit repository&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/docs/img/openvino-logo-purple-black.png&#34; width=&#34;400px&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/releases/tag/2022.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-2022.1-green.svg?sanitize=true&#34; alt=&#34;Stable release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Apache License Version 2.0&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/checks-status/openvinotoolkit/openvino/master?label=GitHub%20checks&#34; alt=&#34;GitHub branch checks state&#34;&gt; &lt;img src=&#34;https://img.shields.io/azure-devops/build/openvinoci/b2bab62f-ab2f-4871-a538-86ea1be7d20f/13?label=Public%20CI&#34; alt=&#34;Azure DevOps builds (branch)&#34;&gt; &lt;a href=&#34;https://badge.fury.io/py/openvino&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/openvino.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/openvino&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/openvino&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#what-is-openvino-toolkit&#34;&gt;What is OpenVINO?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#supported-hardware-matrix&#34;&gt;Supported Hardware matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#products-which-use-openvino&#34;&gt;Products which use OpenVINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#system-requirements&#34;&gt;System requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#how-to-build&#34;&gt;How to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#how-to-contribute&#34;&gt;How to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#get-a-support&#34;&gt;Get a support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#see-also&#34;&gt;See also&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is OpenVINO toolkit?&lt;/h2&gt; &#xA;&lt;p&gt;OpenVINOâ„¢ is an open-source toolkit for optimizing and deploying AI inference.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Boost deep learning performance in computer vision, automatic speech recognition, natural language processing and other common tasks&lt;/li&gt; &#xA; &lt;li&gt;Use models trained with popular frameworks like TensorFlow, PyTorch and more&lt;/li&gt; &#xA; &lt;li&gt;Reduce resource demands and efficiently deploy on a range of IntelÂ® platforms from edge to cloud&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This open-source version includes several components: namely &lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html&#34;&gt;Model Optimizer&lt;/a&gt;, &lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html&#34;&gt;OpenVINOâ„¢ Runtime&lt;/a&gt;, &lt;a href=&#34;https://docs.openvino.ai/latest/pot_introduction.html&#34;&gt;Post-Training Optimization Tool&lt;/a&gt;, as well as CPU, GPU, MYRIAD, multi device and heterogeneous plugins to accelerate deep learning inferencing on IntelÂ® CPUs and IntelÂ® Processor Graphics. It supports pre-trained models from the &lt;a href=&#34;https://github.com/openvinotoolkit/open_model_zoo&#34;&gt;Open Model Zoo&lt;/a&gt;, along with 100+ open source and public models in popular formats such as TensorFlow, ONNX, PaddlePaddle, MXNet, Caffe, Kaldi.&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html&#34;&gt;OpenVINOâ„¢ Runtime&lt;/a&gt; - is a set of C++ libraries with C and Python bindings providing a common API to deliver inference solutions on the platform of your choice. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/core&#34;&gt;core&lt;/a&gt; - provides the base API for model representation and modification.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/inference&#34;&gt;inference&lt;/a&gt; - provides an API to infer models on device.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/common/transformations&#34;&gt;transformations&lt;/a&gt; - contains the set of common transformations which are used in OpenVINO plugins.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/common/low_precision_transformations&#34;&gt;low precision transformations&lt;/a&gt; - contains the set of transformations which are used in low precision models&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings&#34;&gt;bindings&lt;/a&gt; - contains all awailable OpenVINO bindings which are maintained by OpenVINO team. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings/c&#34;&gt;c&lt;/a&gt; - provides C API for OpenVINOâ„¢ Runtime&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings/python&#34;&gt;python&lt;/a&gt; - Python API for OpenVINOâ„¢ Runtime&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins&#34;&gt;Plugins&lt;/a&gt; - contains OpenVINO plugins which are maintained in open-source by OpenVINO team. For more information please taje a look to the &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#supported-hardware-matrix&#34;&gt;list of supported devices&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/frontends&#34;&gt;Frontends&lt;/a&gt; - contains available OpenVINO frontends which allow to read model from native framework format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html&#34;&gt;Model Optimizer&lt;/a&gt; - is a cross-platform command-line tool that facilitates the transition between training and deployment environments, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/pot_introduction.html&#34;&gt;Post-Training Optimization Tool&lt;/a&gt; - is designed to accelerate the inference of deep learning models by applying special methods without model retraining or fine-tuning, for example, post-training 8-bit quantization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/samples&#34;&gt;Samples&lt;/a&gt; - applications on C, C++ and Python languages which shows basic use cases of OpenVINO usages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Hardware matrix&lt;/h2&gt; &#xA;&lt;p&gt;The OpenVINOâ„¢ Runtime can infer models on different hardware devices. This section provides the list of supported devices.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;ShortDescription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_CPU.html#doxid-openvino-docs-o-v-u-g-supported-plugins-c-p-u&#34;&gt;Intel CPU&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_cpu&#34;&gt;openvino_intel_cpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Xeon with IntelÂ® Advanced Vector Extensions 2 (IntelÂ® AVX2), IntelÂ® Advanced Vector Extensions 512 (IntelÂ® AVX-512), and AVX512_BF16, Intel Core Processors with Intel AVX2, Intel Atom Processors with IntelÂ® Streaming SIMD Extensions (IntelÂ® SSE)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_ARM_CPU.html&#34;&gt;ARM CPU&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_contrib/tree/master/modules/arm_plugin&#34;&gt;openvino_arm_cpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Raspberry Piâ„¢ 4 Model B, AppleÂ® Mac mini with M1 chip, NVIDIAÂ® Jetson Nanoâ„¢, Androidâ„¢ devices &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_GPU.html#doxid-openvino-docs-o-v-u-g-supported-plugins-g-p-u&#34;&gt;Intel GPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_gpu&#34;&gt;openvino_intel_gpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Processor Graphics, including Intel HD Graphics and Intel Iris Graphics&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_GNA.html#doxid-openvino-docs-o-v-u-g-supported-plugins-g-n-a&#34;&gt;Intel GNA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_gna&#34;&gt;openvino_intel_gna_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Speech Enabling Developer Kit, Amazon Alexa* Premium Far-Field Developer Kit, Intel Pentium Silver J5005 Processor, Intel Pentium Silver N5000 Processor, Intel Celeron J4005 Processor, Intel Celeron J4105 Processor, Intel Celeron Processor N4100, Intel Celeron Processor N4000, Intel Core i3-8121U Processor, Intel Core i7-1065G7 Processor, Intel Core i7-1060G7 Processor, Intel Core i5-1035G4 Processor, Intel Core i5-1035G7 Processor, Intel Core i5-1035G1 Processor, Intel Core i5-1030G7 Processor, Intel Core i5-1030G4 Processor, Intel Core i3-1005G1 Processor, Intel Core i3-1000G1 Processor, Intel Core i3-1000G4 Processor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_IE_DG_supported_plugins_VPU.html#doxid-openvino-docs-i-e-d-g-supported-plugins-v-p-u&#34;&gt;Myriad plugin&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_myriad&#34;&gt;openvino_intel_myriad_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IntelÂ® Neural Compute Stick 2 powered by the IntelÂ® Movidiusâ„¢ Myriadâ„¢ X&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Also OpenVINOâ„¢ Toolkit contains several plugins which should simplify to load model on several hardware devices:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;ShortDescription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_IE_DG_supported_plugins_AUTO.html#doxid-openvino-docs-i-e-d-g-supported-plugins-a-u-t-o&#34;&gt;Auto&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto&#34;&gt;openvino_auto_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto plugin enables selecting Intel device for inference automatically&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Automatic_Batching.html&#34;&gt;Auto Batch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto_batch&#34;&gt;openvino_auto_batch_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto batch plugin performs on-the-fly automatic batching (i.e. grouping inference requests together) to improve device utilization, with no programming effort from the user&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Hetero_execution.html#doxid-openvino-docs-o-v-u-g-hetero-execution&#34;&gt;Hetero&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/hetero&#34;&gt;openvino_hetero_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Heterogeneous execution enables automatic inference splitting between several devices&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Running_on_multiple_devices.html#doxid-openvino-docs-o-v-u-g-running-on-multiple-devices&#34;&gt;Multi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto&#34;&gt;openvino_auto_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi plugin enables simultaneous inference of the same model on several devices in parallel&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenVINOâ„¢ Toolkit is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt;. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;User documentation&lt;/h3&gt; &#xA;&lt;p&gt;The latest documentation for OpenVINOâ„¢ Toolkit is availabe &lt;a href=&#34;https://docs.openvino.ai/&#34;&gt;here&lt;/a&gt;. This documentation contains detailed information about all OpenVINO components and provides all important information which could be needed if you create an application which is based on binary OpenVINO distribution or own OpenVINO version without source code modification.&lt;/p&gt; &#xA;&lt;h3&gt;Developer documentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#todo-add&#34;&gt;Developer documentation&lt;/a&gt; contains information about architectural decisions which are applied inside the OpenVINO components. This documentation has all necessary information which could be needed in order to contribute to OpenVINO.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;The list of OpenVINO tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_notebooks&#34;&gt;Jupiter notebooks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Products which use OpenVINO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opencv.org/&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onnxruntime.ai/&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/build/ovtfoverview.html&#34;&gt;OpenVINOâ„¢ Integration with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/TNN/tree/master&#34;&gt;TNN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;The full information about system requirements depends on platform and available in section &lt;code&gt;System requirement&lt;/code&gt; on dedicated pages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_linux.html&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_windows.html&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_macos.html&#34;&gt;macOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_raspbian.html&#34;&gt;Raspbian&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;p&gt;Please take a look to &lt;a href=&#34;https://github.com/openvinotoolkit/openvino/wiki#how-to-build&#34;&gt;OpenVINO Wiki&lt;/a&gt; to get more information about OpenVINO build process.&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for details. Thank you!&lt;/p&gt; &#xA;&lt;h2&gt;Get a support&lt;/h2&gt; &#xA;&lt;p&gt;Please report questions, issues and suggestions using:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/issues&#34;&gt;GitHub* Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://stackoverflow.com/questions/tagged/openvino&#34;&gt;&lt;code&gt;openvino&lt;/code&gt;&lt;/a&gt; tag on StackOverflow*&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/forums/computer-vision&#34;&gt;Forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;See also&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/wiki&#34;&gt;OpenVINO Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storage.openvinotoolkit.org/&#34;&gt;OpenVINO Storage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Additional OpenVINOâ„¢ toolkit modules: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_contrib&#34;&gt;openvino_contrib&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html&#34;&gt;IntelÂ® Distribution of OpenVINOâ„¢ toolkit Product Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/articles/OpenVINO-RelNotes&#34;&gt;IntelÂ® Distribution of OpenVINOâ„¢ toolkit Release Notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/nncf&#34;&gt;Neural Network Compression Framework (NNCF)&lt;/a&gt; - a suite of advanced algorithms for model inference optimization including quantization, filter pruning, binarization and sparsity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/training_extensions&#34;&gt;OpenVINOâ„¢ Training Extensions (OTE)&lt;/a&gt; - convenient environment to train Deep Learning models and convert them using OpenVINO for optimized inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/model_server&#34;&gt;OpenVINOâ„¢ Model Server (OVMS)&lt;/a&gt; - a scalable, high-performance solution for serving deep learning models optimized for Intel architectures&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/workbench_docs_Workbench_DG_Introduction.html&#34;&gt;DL Workbench&lt;/a&gt; - An alternative, web-based version of OpenVINO designed to make production of pretrained deep learning models significantly easier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/cvat&#34;&gt;Computer Vision Annotation Tool (CVAT)&lt;/a&gt; - an online, interactive video and image annotation tool for computer vision purposes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/datumaro&#34;&gt;Dataset Management Framework (Datumaro)&lt;/a&gt; - a framework and CLI tool to build, transform, and analyze datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;* Other names and brands may be claimed as the property of others.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ethereum/solidity</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/ethereum/solidity</id>
    <link href="https://github.com/ethereum/solidity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solidity, the Smart Contract Programming Language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Solidity Contract-Oriented Programming Language&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#ethereum_solidity:gitter.im&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Matrix%20-chat-brightgreen?style=plastic&amp;amp;logo=matrix&#34; alt=&#34;Matrix Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/ethereum/solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitter%20-chat-brightgreen?style=plastic&amp;amp;logo=gitter&#34; alt=&#34;Gitter Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.soliditylang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Solidity_Forum%20-discuss-brightgreen?style=plastic&amp;amp;logo=discourse&#34; alt=&#34;Solidity&amp;nbsp;Forum&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/solidity_lang&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/solidity_lang?style=plastic&amp;amp;logo=twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/000335908?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;logo=mastodon&amp;amp;style=plastic&#34; alt=&#34;Mastodon Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can talk to us on Gitter and Matrix, tweet at us on Twitter or create a new topic in the Solidity forum. Questions, feedback, and suggestions are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Solidity is a statically typed, contract-oriented, high-level language for implementing smart contracts on the Ethereum platform.&lt;/p&gt; &#xA;&lt;p&gt;For a good overview and starting point, please check out the official &lt;a href=&#34;https://soliditylang.org&#34;&gt;Solidity Language Portal&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#build-and-install&#34;&gt;Build and Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#maintainers&#34;&gt;Maintainers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#security&#34;&gt;Security&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is a statically-typed curly-braces programming language designed for developing smart contracts that run on the Ethereum Virtual Machine. Smart contracts are programs that are executed inside a peer-to-peer network where nobody has special authority over the execution, and thus they allow to implement tokens of value, ownership, voting, and other kinds of logic.&lt;/p&gt; &#xA;&lt;p&gt;When deploying contracts, you should use the latest released version of Solidity. This is because breaking changes, as well as new features and bug fixes are introduced regularly. We currently use a 0.x version number &lt;a href=&#34;https://semver.org/#spec-item-4&#34;&gt;to indicate this fast pace of change&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build and Install&lt;/h2&gt; &#xA;&lt;p&gt;Instructions about how to build and install the Solidity compiler can be found in the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/installing-solidity.html#building-from-source&#34;&gt;Solidity documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;A &#34;Hello World&#34; program in Solidity is of even less use than in other languages, but still:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-solidity&#34;&gt;// SPDX-License-Identifier: MIT&#xA;pragma solidity &amp;gt;=0.6.0 &amp;lt;0.9.0;&#xA;&#xA;contract HelloWorld {&#xA;    function helloWorld() external pure returns (string memory) {&#xA;        return &#34;Hello, World!&#34;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get started with Solidity, you can use &lt;a href=&#34;https://remix.ethereum.org/&#34;&gt;Remix&lt;/a&gt;, which is a browser-based IDE. Here are some example contracts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#voting&#34;&gt;Voting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#blind-auction&#34;&gt;Blind Auction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#safe-remote-purchase&#34;&gt;Safe remote purchase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#micropayment-channel&#34;&gt;Micropayment Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The Solidity documentation is hosted at &lt;a href=&#34;https://docs.soliditylang.org&#34;&gt;Read the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is still under development. Contributions are always welcome! Please follow the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/contributing.html&#34;&gt;Developers Guide&lt;/a&gt; if you want to help.&lt;/p&gt; &#xA;&lt;p&gt;You can find our current feature and bug priorities for forthcoming releases in the &lt;a href=&#34;https://github.com/ethereum/solidity/projects&#34;&gt;projects section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/axic&#34;&gt;@axic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chriseth&#34;&gt;@chriseth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/LICENSE.txt&#34;&gt;GNU General Public License v3.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some third-party code has its &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/cmake/templates/license.h.in&#34;&gt;own licensing terms&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;The security policy may be &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/SECURITY.md&#34;&gt;found here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>falcosecurity/falco</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/falcosecurity/falco</id>
    <link href="https://github.com/falcosecurity/falco" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cloud Native Runtime Security&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/falcosecurity/community/master/logo/primary-logo.png&#34; width=&#34;360&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;Cloud Native Runtime Security.&lt;/b&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/falcosecurity/falco&#34;&gt;&lt;img src=&#34;https://img.shields.io/circleci/build/github/falcosecurity/falco/master?style=for-the-badge&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/2317&#34;&gt;&lt;img src=&#34;https://img.shields.io/cii/summary/2317?label=CCI%20Best%20Practices&amp;amp;style=for-the-badge&#34; alt=&#34;CII Best Practices Summary&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/falcosecurity/falco/master/COPYING&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/falcosecurity/falco?style=for-the-badge&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Want to talk? Join us on the &lt;a href=&#34;https://kubernetes.slack.com/messages/falco&#34;&gt;#falco&lt;/a&gt; channel in the &lt;a href=&#34;https://slack.k8s.io&#34;&gt;Kubernetes Slack&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Latest releases&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/falcosecurity/falco/master/CHANGELOG.md&#34;&gt;change log&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- &#xA;Badges in the following table are constructed by using the&#xA;https://img.shields.io/badge/dynamic/xml endpoint.&#xA;&#xA;Parameters are configured for fetching packages from S3 before &#xA;(filtered by prefix, sorted in ascending order) and for picking &#xA;the latest package by using an XPath selector after.&#xA;&#xA;- Common query parameters:&#xA;&#xA;color=#300aec7&#xA;style=flat-square&#xA;label=Falco&#xA;&#xA;- DEB packages parameters:&#xA;&#xA;url=https://falco-distribution.s3-eu-west-1.amazonaws.com/?prefix=packages/deb/stable/falco-&#xA;query=substring-before(substring-after((/*[name()=&#39;ListBucketResult&#39;]/*[name()=&#39;Contents&#39;])[last()]/*[name()=&#39;Key&#39;],&#34;falco-&#34;),&#34;.asc&#34;)&#xA;&#xA;- RPM packages parameters:&#xA;&#xA;url=https://falco-distribution.s3-eu-west-1.amazonaws.com/?prefix=packages/rpm/falco-&#xA;query=substring-before(substring-after((/*[name()=&#39;ListBucketResult&#39;]/*[name()=&#39;Contents&#39;])[last()]/*[name()=&#39;Key&#39;],&#34;falco-&#34;),&#34;.asc&#34;)&#xA;&#xA;- BIN packages parameters:&#xA;&#xA;url=https://falco-distribution.s3-eu-west-1.amazonaws.com/?prefix=packages/bin/x86_64/falco-&#xA;query=substring-after((/*[name()=&#39;ListBucketResult&#39;]/*[name()=&#39;Contents&#39;])[last()]/*[name()=&#39;Key&#39;], &#34;falco-&#34;)&#xA;&#xA;Notes:&#xA; - if more than 1000 items are present under as S3 prefix, &#xA;   the actual latest package will be not picked;&#xA;   see https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html&#xA; - for `-dev` packages, the S3 prefix is modified accordingly&#xA; - finally, all parameters are URL encoded and appended to the badge endpoint&#xA;&#xA;--&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;development&lt;/th&gt; &#xA;   &lt;th&gt;stable&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rpm&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.falco.org/?prefix=packages/rpm-dev/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/xml?color=%2300aec7&amp;amp;style=flat-square&amp;amp;label=Falco&amp;amp;query=substring-before%28substring-after%28%28%2F%2A%5Bname%28%29%3D%27ListBucketResult%27%5D%2F%2A%5Bname%28%29%3D%27Contents%27%5D%29%5Blast%28%29%5D%2F%2A%5Bname%28%29%3D%27Key%27%5D%2C%22falco-%22%29%2C%22.asc%22%29&amp;amp;url=https%3A%2F%2Ffalco-distribution.s3-eu-west-1.amazonaws.com%2F%3Fprefix%3Dpackages%2Frpm-dev%2Ffalco-&#34; alt=&#34;rpm-dev&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.falco.org/?prefix=packages/rpm/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/xml?color=%2300aec7&amp;amp;style=flat-square&amp;amp;label=Falco&amp;amp;query=substring-before%28substring-after%28%28%2F%2A%5Bname%28%29%3D%27ListBucketResult%27%5D%2F%2A%5Bname%28%29%3D%27Contents%27%5D%29%5Blast%28%29%5D%2F%2A%5Bname%28%29%3D%27Key%27%5D%2C%22falco-%22%29%2C%22.asc%22%29&amp;amp;url=https%3A%2F%2Ffalco-distribution.s3-eu-west-1.amazonaws.com%2F%3Fprefix%3Dpackages%2Frpm%2Ffalco-&#34; alt=&#34;rpm&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deb&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.falco.org/?prefix=packages/deb-dev/stable/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/xml?color=%2300aec7&amp;amp;style=flat-square&amp;amp;label=Falco&amp;amp;query=substring-before%28substring-after%28%28%2F%2A%5Bname%28%29%3D%27ListBucketResult%27%5D%2F%2A%5Bname%28%29%3D%27Contents%27%5D%29%5Blast%28%29%5D%2F%2A%5Bname%28%29%3D%27Key%27%5D%2C%22falco-%22%29%2C%22.asc%22%29&amp;amp;url=https%3A%2F%2Ffalco-distribution.s3-eu-west-1.amazonaws.com%2F%3Fprefix%3Dpackages%2Fdeb-dev%2Fstable%2Ffalco-&#34; alt=&#34;deb-dev&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.falco.org/?prefix=packages/deb/stable/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/xml?color=%2300aec7&amp;amp;style=flat-square&amp;amp;label=Falco&amp;amp;query=substring-before%28substring-after%28%28%2F%2A%5Bname%28%29%3D%27ListBucketResult%27%5D%2F%2A%5Bname%28%29%3D%27Contents%27%5D%29%5Blast%28%29%5D%2F%2A%5Bname%28%29%3D%27Key%27%5D%2C%22falco-%22%29%2C%22.asc%22%29&amp;amp;url=https%3A%2F%2Ffalco-distribution.s3-eu-west-1.amazonaws.com%2F%3Fprefix%3Dpackages%2Fdeb%2Fstable%2Ffalco-&#34; alt=&#34;deb&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;binary&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.falco.org/?prefix=packages/bin-dev/x86_64/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/xml?color=%2300aec7&amp;amp;style=flat-square&amp;amp;label=Falco&amp;amp;query=substring-after%28%28%2F%2A%5Bname%28%29%3D%27ListBucketResult%27%5D%2F%2A%5Bname%28%29%3D%27Contents%27%5D%29%5Blast%28%29%5D%2F%2A%5Bname%28%29%3D%27Key%27%5D%2C%20%22falco-%22%29&amp;amp;url=https%3A%2F%2Ffalco-distribution.s3-eu-west-1.amazonaws.com%2F%3Fprefix%3Dpackages%2Fbin-dev%2Fx86_64%2Ffalco-&#34; alt=&#34;bin-dev&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.falco.org/?prefix=packages/bin/x86_64/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/xml?color=%2300aec7&amp;amp;style=flat-square&amp;amp;label=Falco&amp;amp;query=substring-after%28%28%2F%2A%5Bname%28%29%3D%27ListBucketResult%27%5D%2F%2A%5Bname%28%29%3D%27Contents%27%5D%29%5Blast%28%29%5D%2F%2A%5Bname%28%29%3D%27Key%27%5D%2C%20%22falco-%22%29&amp;amp;url=https%3A%2F%2Ffalco-distribution.s3-eu-west-1.amazonaws.com%2F%3Fprefix%3Dpackages%2Fbin%2Fx86_64%2Ffalco-&#34; alt=&#34;bin&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The Falco Project, originally created by &lt;a href=&#34;https://sysdig.com&#34;&gt;Sysdig&lt;/a&gt;, is an incubating &lt;a href=&#34;https://cncf.io&#34;&gt;CNCF&lt;/a&gt; open source cloud native runtime security tool. Falco makes it easy to consume kernel events, and enrich those events with information from Kubernetes and the rest of the cloud native stack. Falco can also be extended to other data sources by using plugins. Falco has a rich set of security rules specifically built for Kubernetes, Linux, and cloud-native. If a rule is violated in a system, Falco will send an alert notifying the user of the violation and its severity.&lt;/p&gt; &#xA;&lt;h3&gt;What can Falco detect?&lt;/h3&gt; &#xA;&lt;p&gt;Falco can detect and alert on any behavior that involves making Linux system calls. Falco alerts can be triggered by the use of specific system calls, their arguments, and by properties of the calling process. For example, Falco can easily detect incidents including but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A shell is running inside a container or pod in Kubernetes.&lt;/li&gt; &#xA; &lt;li&gt;A container is running in privileged mode, or is mounting a sensitive path, such as &lt;code&gt;/proc&lt;/code&gt;, from the host.&lt;/li&gt; &#xA; &lt;li&gt;A server process is spawning a child process of an unexpected type.&lt;/li&gt; &#xA; &lt;li&gt;Unexpected read of a sensitive file, such as &lt;code&gt;/etc/shadow&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A non-device file is written to &lt;code&gt;/dev&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A standard system binary, such as &lt;code&gt;ls&lt;/code&gt;, is making an outbound network connection.&lt;/li&gt; &#xA; &lt;li&gt;A privileged pod is started in a Kubernetes cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installing Falco&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to run Falco in &lt;strong&gt;production&lt;/strong&gt; please adhere to the &lt;a href=&#34;https://falco.org/docs/getting-started/installation/&#34;&gt;official installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Kubernetes&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tool&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Helm&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/falcosecurity/charts/tree/master/falco#introduction&#34;&gt;Chart Repository&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The Falco community offers regular helm chart releases.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Minikube&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://falco.org/docs/getting-started/third-party/#minikube&#34;&gt;Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The Falco driver has been baked into minikube for easy deployment.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kind&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://falco.org/docs/getting-started/third-party/#kind&#34;&gt;Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Running Falco with kind requires a driver on the host system.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://falco.org/docs/getting-started/third-party/#gke&#34;&gt;Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;We suggest using the eBPF driver for running Falco on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Developing&lt;/h3&gt; &#xA;&lt;p&gt;Falco is designed to be extensible such that it can be built into cloud-native applications and infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;Falco has a &lt;a href=&#34;https://falco.org/docs/grpc/&#34;&gt;gRPC&lt;/a&gt; endpoint and an API defined in &lt;a href=&#34;https://github.com/falcosecurity/falco/raw/master/userspace/falco/outputs.proto&#34;&gt;protobuf&lt;/a&gt;. The Falco Project supports various SDKs for this endpoint.&lt;/p&gt; &#xA;&lt;h5&gt;SDKs&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Repository&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/falcosecurity/client-go&#34;&gt;client-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rust&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/falcosecurity/client-rs&#34;&gt;client-rs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/falcosecurity/client-py&#34;&gt;client-py&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Plugins&lt;/h3&gt; &#xA;&lt;p&gt;Falco comes with a &lt;a href=&#34;https://falco.org/docs/plugins/&#34;&gt;plugin framework&lt;/a&gt; that extends it to potentially any cloud detection scenario. Plugins are shared libraries that conform to a documented API and allow for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adding new event sources that can be used in rules;&lt;/li&gt; &#xA; &lt;li&gt;Adding the ability to define new fields and extract information from events.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Falco Project maintains &lt;a href=&#34;https://github.com/falcosecurity/plugins&#34;&gt;various plugins&lt;/a&gt; and provides SDKs for plugin development.&lt;/p&gt; &#xA;&lt;h5&gt;SDKs&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Repository&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/falcosecurity/plugin-sdk-go&#34;&gt;falcosecurity/plugin-sdk-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://falco.org/docs/&#34;&gt;Official Documentation&lt;/a&gt; is the best resource to learn about Falco.&lt;/p&gt; &#xA;&lt;h3&gt;Join the Community&lt;/h3&gt; &#xA;&lt;p&gt;To get involved with The Falco Project please visit &lt;a href=&#34;https://github.com/falcosecurity/community&#34;&gt;the community repository&lt;/a&gt; to find more.&lt;/p&gt; &#xA;&lt;p&gt;How to reach out?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://kubernetes.slack.com/messages/falco&#34;&gt;#falco&lt;/a&gt; channel on the &lt;a href=&#34;https://slack.k8s.io&#34;&gt;Kubernetes Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lists.cncf.io/g/cncf-falco-dev&#34;&gt;Join the Falco mailing list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://falco.org/docs/&#34;&gt;Read the Falco documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/falcosecurity/.github/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Security Audit&lt;/h3&gt; &#xA;&lt;p&gt;A third party security audit was performed by Cure53, you can see the full report &lt;a href=&#34;https://raw.githubusercontent.com/falcosecurity/falco/master/audits/SECURITY_AUDIT_2019_07.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Reporting security vulnerabilities&lt;/h3&gt; &#xA;&lt;p&gt;Please report security vulnerabilities following the community process documented &lt;a href=&#34;https://github.com/falcosecurity/.github/raw/master/SECURITY.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;License Terms&lt;/h3&gt; &#xA;&lt;p&gt;Falco is licensed to you under the &lt;a href=&#34;https://raw.githubusercontent.com/falcosecurity/falco/master/COPYING&#34;&gt;Apache 2.0&lt;/a&gt; open source license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>iovisor/bpftrace</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/iovisor/bpftrace</id>
    <link href="https://github.com/iovisor/bpftrace" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-level tracing language for Linux eBPF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bpftrace&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/iovisor/bpftrace/actions?query=workflow%3ACI+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/iovisor/bpftrace/workflows/CI/badge.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://webchat.oftc.net/?channels=bpftrace&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IRC-bpftrace-blue.svg?sanitize=true&#34; alt=&#34;IRC#bpftrace&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/iovisor/bpftrace/alerts/&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/alerts/g/iovisor/bpftrace.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Total alerts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;bpftrace is a high-level tracing language for Linux enhanced Berkeley Packet Filter (eBPF) available in recent Linux kernels (4.x). bpftrace uses LLVM as a backend to compile scripts to BPF-bytecode and makes use of &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;BCC&lt;/a&gt; for interacting with the Linux BPF system, as well as existing Linux tracing capabilities: kernel dynamic tracing (kprobes), user-level dynamic tracing (uprobes), and tracepoints. The bpftrace language is inspired by awk and C, and predecessor tracers such as DTrace and SystemTap. bpftrace was created by &lt;a href=&#34;https://github.com/ajor&#34;&gt;Alastair Robertson&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about bpftrace, see the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/man/adoc/bpftrace.adoc&#34;&gt;Manual&lt;/a&gt; the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/reference_guide.md&#34;&gt;Reference Guide&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/tutorial_one_liners.md&#34;&gt;One-Liner Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;One-Liners&lt;/h2&gt; &#xA;&lt;p&gt;The following one-liners demonstrate different capabilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Files opened by process&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_enter_open { printf(&#34;%s %s\n&#34;, comm, str(args-&amp;gt;filename)); }&#39;&#xA;&#xA;# Syscall count by program&#xA;bpftrace -e &#39;tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }&#39;&#xA;&#xA;# Read bytes by process:&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_exit_read /args-&amp;gt;ret/ { @[comm] = sum(args-&amp;gt;ret); }&#39;&#xA;&#xA;# Read size distribution by process:&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_exit_read { @[comm] = hist(args-&amp;gt;ret); }&#39;&#xA;&#xA;# Show per-second syscall rates:&#xA;bpftrace -e &#39;tracepoint:raw_syscalls:sys_enter { @ = count(); } interval:s:1 { print(@); clear(@); }&#39;&#xA;&#xA;# Trace disk size by process&#xA;bpftrace -e &#39;tracepoint:block:block_rq_issue { printf(&#34;%d %s %d\n&#34;, pid, comm, args-&amp;gt;bytes); }&#39;&#xA;&#xA;# Count page faults by process&#xA;bpftrace -e &#39;software:faults:1 { @[comm] = count(); }&#39;&#xA;&#xA;# Count LLC cache misses by process name and PID (uses PMCs):&#xA;bpftrace -e &#39;hardware:cache-misses:1000000 { @[comm, pid] = count(); }&#39;&#xA;&#xA;# Profile user-level stacks at 99 Hertz, for PID 189:&#xA;bpftrace -e &#39;profile:hz:99 /pid == 189/ { @[ustack] = count(); }&#39;&#xA;&#xA;# Files opened, for processes in the root cgroup-v2&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_enter_openat /cgroup == cgroupid(&#34;/sys/fs/cgroup/unified/mycg&#34;)/ { printf(&#34;%s\n&#34;, str(args-&amp;gt;filename)); }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More powerful scripts can easily be constructed. See &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools&#34;&gt;Tools&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;For build and install instructions, see &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;bpftrace contains various tools, which also serve as examples of programming in the bpftrace language.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bashreadline.bt&#34;&gt;bashreadline.bt&lt;/a&gt;: Print entered bash commands system wide. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bashreadline_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biolatency.bt&#34;&gt;biolatency.bt&lt;/a&gt;: Block I/O latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biolatency_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biosnoop.bt&#34;&gt;biosnoop.bt&lt;/a&gt;: Block I/O tracing tool, showing per I/O latency. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biosnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biostacks.bt&#34;&gt;biostacks.bt&lt;/a&gt;: Show disk I/O latency with initialization stacks. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biostacks_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bitesize.bt&#34;&gt;bitesize.bt&lt;/a&gt;: Show disk I/O size as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bitesize_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/capable.bt&#34;&gt;capable.bt&lt;/a&gt;: Trace security capability checks. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/capable_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/cpuwalk.bt&#34;&gt;cpuwalk.bt&lt;/a&gt;: Sample which CPUs are executing processes. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/cpuwalk_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/dcsnoop.bt&#34;&gt;dcsnoop.bt&lt;/a&gt;: Trace directory entry cache (dcache) lookups. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/dcsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/execsnoop.bt&#34;&gt;execsnoop.bt&lt;/a&gt;: Trace new processes via exec() syscalls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/execsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/gethostlatency.bt&#34;&gt;gethostlatency.bt&lt;/a&gt;: Show latency for getaddrinfo/gethostbyname[2] calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/gethostlatency_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/killsnoop.bt&#34;&gt;killsnoop.bt&lt;/a&gt;: Trace signals issued by the kill() syscall. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/killsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/loads.bt&#34;&gt;loads.bt&lt;/a&gt;: Print load averages. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/loads_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/mdflush.bt&#34;&gt;mdflush.bt&lt;/a&gt;: Trace md flush events. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/mdflush_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/naptime.bt&#34;&gt;naptime.bt&lt;/a&gt;: Show voluntary sleep calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/naptime_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/opensnoop.bt&#34;&gt;opensnoop.bt&lt;/a&gt;: Trace open() syscalls showing filenames. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/opensnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/oomkill.bt&#34;&gt;oomkill.bt&lt;/a&gt;: Trace OOM killer. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/oomkill_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/pidpersec.bt&#34;&gt;pidpersec.bt&lt;/a&gt;: Count new processes (via fork). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/pidpersec_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlat.bt&#34;&gt;runqlat.bt&lt;/a&gt;: CPU scheduler run queue latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlat_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlen.bt&#34;&gt;runqlen.bt&lt;/a&gt;: CPU scheduler run queue length as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlen_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/setuids.bt&#34;&gt;setuids.bt&lt;/a&gt;: Trace the setuid syscalls: privilege escalation. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/setuids_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/statsnoop.bt&#34;&gt;statsnoop.bt&lt;/a&gt;: Trace stat() syscalls for general debugging. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/statsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/swapin.bt&#34;&gt;swapin.bt&lt;/a&gt;: Show swapins by process. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/swapin_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syncsnoop.bt&#34;&gt;syncsnoop.bt&lt;/a&gt;: Trace sync() variety of syscalls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syncsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syscount.bt&#34;&gt;syscount.bt&lt;/a&gt;: Count system calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syscount_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpaccept.bt&#34;&gt;tcpaccept.bt&lt;/a&gt;: Trace TCP passive connections (accept()). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpaccept_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpconnect.bt&#34;&gt;tcpconnect.bt&lt;/a&gt;: Trace TCP active connections (connect()). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpconnect_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpdrop.bt&#34;&gt;tcpdrop.bt&lt;/a&gt;: Trace kernel-based TCP packet drops with details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpdrop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcplife.bt&#34;&gt;tcplife.bt&lt;/a&gt;: Trace TCP session lifespans with connection details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcplife_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpretrans.bt&#34;&gt;tcpretrans.bt&lt;/a&gt;: Trace TCP retransmits. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpretrans_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpsynbl.bt&#34;&gt;tcpsynbl.bt&lt;/a&gt;: Show TCP SYN backlog as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpsynbl_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/threadsnoop.bt&#34;&gt;threadsnoop.bt&lt;/a&gt;: List new thread creation. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/threadsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfscount.bt&#34;&gt;vfscount.bt&lt;/a&gt;: Count VFS calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfscount_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfsstat.bt&#34;&gt;vfsstat.bt&lt;/a&gt;: Count some VFS calls, with per-second summaries. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfsstat_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/writeback.bt&#34;&gt;writeback.bt&lt;/a&gt;: Trace file system writeback events with details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/writeback_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/xfsdist.bt&#34;&gt;xfsdist.bt&lt;/a&gt;: Summarize XFS operation latency distribution as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/xfsdist_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more eBPF observability tools, see &lt;a href=&#34;https://github.com/iovisor/bcc#tools&#34;&gt;bcc tools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Probe types&lt;/h2&gt; &#xA;&lt;center&gt;&#xA; &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/images/bpftrace_probes_2018.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/images/bpftrace_probes_2018.png&#34; border=&#34;0&#34; width=&#34;700&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/reference_guide.md&#34;&gt;Reference Guide&lt;/a&gt; for more detail.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;For additional help / discussion, please use our &lt;a href=&#34;https://github.com/iovisor/bpftrace/discussions&#34;&gt;discussions&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Have ideas for new bpftrace tools? &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/CONTRIBUTING-TOOLS.md&#34;&gt;CONTRIBUTING-TOOLS.md&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bugs reports and feature requests: &lt;a href=&#34;https://github.com/iovisor/bpftrace/issues&#34;&gt;Issue Tracker&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;bpftrace development IRC: #bpftrace at irc.oftc.net&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;For build &amp;amp; test directly in docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For build in docker then test directly on host&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build-static.sh&#xA;$ ./build-static/src/bpftrace&#xA;$ ./build-static/tests/bpftrace_test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vagrant&lt;/h3&gt; &#xA;&lt;p&gt;For development and testing a &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/Vagrantfile&#34;&gt;Vagrantfile&lt;/a&gt; is available.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have the &lt;code&gt;vbguest&lt;/code&gt; plugin installed, it is required to correctly install the shared file system driver on the ubuntu boxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant plugin install vagrant-vbguest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start VM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant status&#xA;$ vagrant up $YOUR_CHOICE&#xA;$ vagrant ssh $YOUR_CHOICE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2019 Alastair Robertson&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/MMKV</title>
    <updated>2022-05-30T01:58:07Z</updated>
    <id>tag:github.com,2022-05-30:/Tencent/MMKV</id>
    <link href="https://github.com/Tencent/MMKV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An efficient, small mobile key-value storage framework developed by WeChat. Works on Android, iOS, macOS, Windows, and POSIX.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/MMKV/raw/master/LICENSE.TXT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD_3-brightgreen.svg?style=flat&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-1.2.13-brightgreen.svg?sanitize=true&#34; alt=&#34;Release Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Platform-%20Android%20%7C%20iOS%2FmacOS%20%7C%20Win32%20%7C%20POSIX-brightgreen.svg?sanitize=true&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸­æ–‡ç‰ˆæœ¬è¯·å‚çœ‹&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/README_CN.md&#34;&gt;è¿™é‡Œ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MMKV is an &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;small&lt;/strong&gt;, &lt;strong&gt;easy-to-use&lt;/strong&gt; mobile key-value storage framework used in the WeChat application. It&#39;s currently available on &lt;strong&gt;Android&lt;/strong&gt;, &lt;strong&gt;iOS/macOS&lt;/strong&gt;, &lt;strong&gt;Win32&lt;/strong&gt; and &lt;strong&gt;POSIX&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for Android&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of Android to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;apply&lt;/code&gt; calls needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 50K in binary size&lt;/strong&gt;: MMKV adds about 50K per architecture on App size, and much less when zipped (APK).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via Maven&lt;/h3&gt; &#xA;&lt;p&gt;Add the following lines to &lt;code&gt;build.gradle&lt;/code&gt; on your app module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-gradle&#34;&gt;dependencies {&#xA;    implementation &#39;com.tencent:mmkv:1.2.13&#39;&#xA;    // replace &#34;1.2.13&#34; with any available version&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Starting from v1.2.8, MMKV has been &lt;strong&gt;migrated to Maven Central&lt;/strong&gt;.&lt;br&gt; For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_setup&#34;&gt;Android Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;apply&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say your &lt;code&gt;Application&lt;/code&gt; class, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;public void onCreate() {&#xA;    super.onCreate();&#xA;&#xA;    String rootDir = MMKV.initialize(this);&#xA;    System.out.println(&#34;mmkv root: &#34; + rootDir);&#xA;    //â€¦â€¦&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;import com.tencent.mmkv.MMKV;&#xA;    &#xA;MMKV kv = MMKV.defaultMMKV();&#xA;&#xA;kv.encode(&#34;bool&#34;, true);&#xA;boolean bValue = kv.decodeBool(&#34;bool&#34;);&#xA;&#xA;kv.encode(&#34;int&#34;, Integer.MIN_VALUE);&#xA;int iValue = kv.decodeInt(&#34;int&#34;);&#xA;&#xA;kv.encode(&#34;string&#34;, &#34;Hello from mmkv&#34;);&#xA;String str = kv.decodeString(&#34;string&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_tutorial&#34;&gt;Android Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Writing random &lt;code&gt;int&lt;/code&gt; for 1000 times, we get this chart:&lt;br&gt; &lt;img src=&#34;https://github.com/Tencent/MMKV/wiki/assets/profile_android_mini.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; For more benchmark data, please refer to &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_benchmark&#34;&gt;our benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for iOS/macOS&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of iOS/macOS to achieve the best performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go, no configurations are needed. All changes are saved immediately, no &lt;code&gt;synchronize&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains encode/decode helpers and mmap logics and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Less than 30K in binary size&lt;/strong&gt;: MMKV adds less than 30K per architecture on App size, and much less when zipped (IPA).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via CocoaPods:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://guides.CocoaPods.org/using/getting-started.html&#34;&gt;CocoaPods&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Open the terminal, &lt;code&gt;cd&lt;/code&gt; to your project directory, run &lt;code&gt;pod repo update&lt;/code&gt; to make CocoaPods aware of the latest available MMKV versions;&lt;/li&gt; &#xA; &lt;li&gt;Edit your Podfile, add &lt;code&gt;pod &#39;MMKV&#39;&lt;/code&gt; to your app target;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pod install&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;.xcworkspace&lt;/code&gt; file generated by CocoaPods;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;#import &amp;lt;MMKV/MMKV.h&amp;gt;&lt;/code&gt; to your source file and we are done.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_setup&#34;&gt;iOS/macOS Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go, no configurations are needed. All changes are saved immediately, no &lt;code&gt;synchronize&lt;/code&gt; calls are needed. Setup MMKV on App startup, in your &lt;code&gt;-[MyApp application: didFinishLaunchingWithOptions:]&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-objective-c&#34;&gt;- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {&#xA;    // init MMKV in the main thread&#xA;    [MMKV initializeMMKV:nil];&#xA;&#xA;    //...&#xA;    return YES;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-objective-c&#34;&gt;MMKV *mmkv = [MMKV defaultMMKV];&#xA;    &#xA;[mmkv setBool:YES forKey:@&#34;bool&#34;];&#xA;BOOL bValue = [mmkv getBoolForKey:@&#34;bool&#34;];&#xA;    &#xA;[mmkv setInt32:-1024 forKey:@&#34;int32&#34;];&#xA;int32_t iValue = [mmkv getInt32ForKey:@&#34;int32&#34;];&#xA;    &#xA;[mmkv setString:@&#34;hello, mmkv&#34; forKey:@&#34;string&#34;];&#xA;NSString *str = [mmkv getStringForKey:@&#34;string&#34;];&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_tutorial&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Writing random &lt;code&gt;int&lt;/code&gt; for 10000 times, we get this chart:&lt;br&gt; &lt;img src=&#34;https://github.com/Tencent/MMKV/wiki/assets/profile_mini.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; For more benchmark data, please refer to &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_benchmark&#34;&gt;our benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for Win32&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of Windows to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;save&lt;/code&gt;, no &lt;code&gt;sync&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 10K in binary size&lt;/strong&gt;: MMKV adds about 10K on application size, and much less when zipped.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via Source&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Getting source code from git repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/MMKV.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;Win32/MMKV/MMKV.vcxproj&lt;/code&gt; to your solution;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;MMKV&lt;/code&gt; project to your project&#39;s dependencies;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;$(OutDir)include&lt;/code&gt; to your project&#39;s &lt;code&gt;C/C++&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Include Directories&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;$(OutDir)&lt;/code&gt; to your project&#39;s &lt;code&gt;Linker&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Library Directories&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;MMKV.lib&lt;/code&gt; to your project&#39;s &lt;code&gt;Linker&lt;/code&gt; -&amp;gt; &lt;code&gt;Input&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Dependencies&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;#include &amp;lt;MMKV/MMKV.h&amp;gt;&lt;/code&gt; to your source file and we are done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;MMKV is compiled with &lt;code&gt;MT/MTd&lt;/code&gt; runtime by default. If your project uses &lt;code&gt;MD/MDd&lt;/code&gt;, you should change MMKV&#39;s setting to match your project&#39;s (&lt;code&gt;C/C++&lt;/code&gt; -&amp;gt; &lt;code&gt;Code Generation&lt;/code&gt; -&amp;gt; &lt;code&gt;Runtime Library&lt;/code&gt;), or vice versa.&lt;/li&gt; &#xA; &lt;li&gt;MMKV is developed with Visual Studio 2017, change the &lt;code&gt;Platform Toolset&lt;/code&gt; if you use a different version of Visual Studio.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/windows_setup&#34;&gt;Win32 Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;save&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say in your &lt;code&gt;main()&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;#include &amp;lt;MMKV/MMKV.h&amp;gt;&#xA;&#xA;int main() {&#xA;    std::wstring rootDir = getYourAppDocumentDir();&#xA;    MMKV::initializeMMKV(rootDir);&#xA;    //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto mmkv = MMKV::defaultMMKV();&#xA;&#xA;mmkv-&amp;gt;set(true, &#34;bool&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;bool = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getBool(&#34;bool&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(1024, &#34;int32&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;int32 = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getInt32(&#34;int32&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(&#34;Hello, MMKV for Win32&#34;, &#34;string&#34;);&#xA;std::string result;&#xA;mmkv-&amp;gt;getString(&#34;string&#34;, result);&#xA;std::cout &amp;lt;&amp;lt; &#34;string = &#34; &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/windows_tutorial&#34;&gt;Win32 Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for POSIX&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of POSIX to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;save&lt;/code&gt;, no &lt;code&gt;sync&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 7K in binary size&lt;/strong&gt;: MMKV adds about 7K on application size, and much less when zipped.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via CMake&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Getting source code from the git repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/MMKV.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Edit your &lt;code&gt;CMakeLists.txt&lt;/code&gt;, add those lines:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;add_subdirectory(mmkv/POSIX/src mmkv)&#xA;target_link_libraries(MyApp&#xA;    mmkv)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;#include &#34;MMKV.h&#34;&lt;/code&gt; to your source file and we are done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/posix_setup&#34;&gt;POSIX Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;save&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say in your &lt;code&gt;main()&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;#include &#34;MMKV.h&#34;&#xA;&#xA;int main() {&#xA;    std::string rootDir = getYourAppDocumentDir();&#xA;    MMKV::initializeMMKV(rootDir);&#xA;    //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto mmkv = MMKV::defaultMMKV();&#xA;&#xA;mmkv-&amp;gt;set(true, &#34;bool&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;bool = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getBool(&#34;bool&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(1024, &#34;int32&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;int32 = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getInt32(&#34;int32&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(&#34;Hello, MMKV for Win32&#34;, &#34;string&#34;);&#xA;std::string result;&#xA;mmkv-&amp;gt;getString(&#34;string&#34;, result);&#xA;std::cout &amp;lt;&amp;lt; &#34;string = &#34; &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/posix_tutorial&#34;&gt;POSIX Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MMKV is published under the BSD 3-Clause license. For details check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/LICENSE.TXT&#34;&gt;LICENSE.TXT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for details of change history.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, also join our &lt;a href=&#34;https://opensource.tencent.com/contribution&#34;&gt;Tencent OpenSource Plan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To give clarity of what is expected of our members, MMKV has adopted the code of conduct defined by the Contributor Covenant, which is widely used. And we think it articulates our values well. For more, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ &amp;amp; Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; first. Should there be any questions, don&#39;t hesitate to create &lt;a href=&#34;https://github.com/Tencent/MMKV/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Personal Information Protection Rules&lt;/h2&gt; &#xA;&lt;p&gt;User privacy is taken very seriously: MMKV does not obtain, collect or upload any personal information. Please refer to the &lt;a href=&#34;https://support.weixin.qq.com/cgi-bin/mmsupportacctnodeweb-bin/pages/aY5BAtRiO1BpoHxo&#34;&gt;MMKV SDK Personal Information Protection Rules&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
</feed>