<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-03T01:46:09Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qinguoyi/TinyWebServer</title>
    <updated>2023-09-03T01:46:09Z</updated>
    <id>tag:github.com,2023-09-03:/qinguoyi/TinyWebServer</id>
    <link href="https://github.com/qinguoyi/TinyWebServer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🔥 Linux下C++轻量级WebServer服务器&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinyWebServer&lt;/h1&gt; &#xA;&lt;p&gt;Linux下C++轻量级Web服务器，助力初学者快速实践网络编程，搭建属于自己的服务器.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用 &lt;strong&gt;线程池 + 非阻塞socket + epoll(ET和LT均实现) + 事件处理(Reactor和模拟Proactor均实现)&lt;/strong&gt; 的并发模型&lt;/li&gt; &#xA; &lt;li&gt;使用&lt;strong&gt;状态机&lt;/strong&gt;解析HTTP请求报文，支持解析&lt;strong&gt;GET和POST&lt;/strong&gt;请求&lt;/li&gt; &#xA; &lt;li&gt;访问服务器数据库实现web端用户&lt;strong&gt;注册、登录&lt;/strong&gt;功能，可以请求服务器&lt;strong&gt;图片和视频文件&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;实现&lt;strong&gt;同步/异步日志系统&lt;/strong&gt;，记录服务器运行状态&lt;/li&gt; &#xA; &lt;li&gt;经Webbench压力测试可以实现&lt;strong&gt;上万的并发连接&lt;/strong&gt;数据交换&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;写在前面&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本项目开发维护过程中，很多童鞋曾发红包支持，我都一一谢绝。我现在不会，将来也不会将本项目包装成任何课程售卖，更不会开通任何支持通道。&lt;/li&gt; &#xA; &lt;li&gt;目前网络上有人或对本项目，或对游双大佬的项目包装成课程售卖。请各位童鞋擦亮眼，辨识各大学习/求职网站的C++服务器项目，不要盲目付费。&lt;/li&gt; &#xA; &lt;li&gt;有面试官大佬通过项目信息在公司内找到我，发现很多童鞋简历上都用了这个项目。但，在面试过程中发现&lt;code&gt;很多童鞋通过本项目入门了，但是对于一些东西还是属于知其然不知其所以然的状态，需要加强下基础知识的学习&lt;/code&gt;，推荐认真阅读下 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;《unix环境高级编程》&lt;/li&gt; &#xA;   &lt;li&gt;《unix网络编程》&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;感谢各位大佬，各位朋友，各位童鞋的认可和支持。如果本项目能带你入门，将是我莫大的荣幸。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E6%A6%82%E8%BF%B0&#34;&gt;概述&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E6%A1%86%E6%9E%B6&#34;&gt;框架&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#Demo%E6%BC%94%E7%A4%BA&#34;&gt;Demo演示&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95&#34;&gt;压力测试&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97&#34;&gt;更新日志&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E6%BA%90%E7%A0%81%E4%B8%8B%E8%BD%BD&#34;&gt;源码下载&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E5%BF%AB%E9%80%9F%E8%BF%90%E8%A1%8C&#34;&gt;快速运行&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E4%B8%AA%E6%80%A7%E5%8C%96%E8%BF%90%E8%A1%8C&#34;&gt;个性化运行&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E5%BA%96%E4%B8%81%E8%A7%A3%E7%89%9B&#34;&gt;庖丁解牛&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#CPP11%E5%AE%9E%E7%8E%B0&#34;&gt;CPP11实现&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/qinguoyi/TinyWebServer/master/#%E8%87%B4%E8%B0%A2&#34;&gt;致谢&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;概述&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;C/C++&lt;/li&gt; &#xA;  &lt;li&gt;B/S模型&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/lock&#34;&gt;线程同步机制包装类&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/http&#34;&gt;http连接请求处理类&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/threadpool&#34;&gt;半同步/半反应堆线程池&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/timer&#34;&gt;定时器处理非活动连接&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/log&#34;&gt;同步/异步日志系统 &lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/CGImysql&#34;&gt;数据库连接池&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/CGImysql&#34;&gt;同步线程注册和登录校验&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/master/test_presure&#34;&gt;简易服务器压力测试&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;框架&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1ge0j1atq5hj30g60lm0w4.jpg&#34; height=&#34;765&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Demo演示&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;注册演示&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1ge0iz0dkleg30m80bxjyj.gif&#34; height=&#34;429&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;登录演示&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/qinguoyi/TinyWebServer/raw/master/root/login.gif&#34; height=&#34;429&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;请求图片文件演示(6M)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1ge0juxrnlfg30go07x4qr.gif&#34; height=&#34;429&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;请求视频文件演示(39M)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1ge0jtxie8ng30go07xb2b.gif&#34; height=&#34;429&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;压力测试&lt;/h2&gt; &#xA;&lt;p&gt;在关闭日志后，使用Webbench对服务器进行压力测试，对listenfd和connfd分别采用ET和LT模式，均可实现上万的并发连接，下面列出的是两者组合后的测试结果.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Proactor，LT + LT，93251 QPS&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1gfjqu2hptkj30gz07474n.jpg&#34; height=&#34;201&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Proactor，LT + ET，97459 QPS&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1gfjr1xppdgj30h206zdg6.jpg&#34; height=&#34;201&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Proactor，ET + LT，80498 QPS&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1gfjr24vmjtj30gz0720t3.jpg&#34; height=&#34;201&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Proactor，ET + ET，92167 QPS&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1gfjrflrebdj30gz06z0t3.jpg&#34; height=&#34;201&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Reactor，LT + ET，69175 QPS&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;http://ww1.sinaimg.cn/large/005TJ2c7ly1gfjr1humcbj30h207474n.jpg&#34; height=&#34;201&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;并发连接总数：10500&lt;/li&gt; &#xA;  &lt;li&gt;访问服务器时间：5s&lt;/li&gt; &#xA;  &lt;li&gt;所有访问均成功&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; 使用本项目的webbench进行压测时，若报错显示webbench命令找不到，将可执行文件webbench删除后，重新编译即可。&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 解决请求服务器上大文件的Bug&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 增加请求视频文件的页面&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 解决数据库同步校验内存泄漏&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 实现非阻塞模式下的ET和LT触发，并完成压力测试&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 完善&lt;code&gt;lock.h&lt;/code&gt;中的封装类，统一使用该同步机制&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 改进代码结构，更新局部变量懒汉单例模式&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 优化数据库连接池信号量与代码结构&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 使用RAII机制优化数据库连接的获取与释放&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 优化代码结构，封装工具类以减少全局变量&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 编译一次即可，命令行进行个性化测试更加友好&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; main函数封装重构&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 新增命令行日志开关，关闭日志后更新压力测试结果&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 改进编译方式，只配置一次SQL信息即可&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 新增Reactor模式，并完成压力测试&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;源码下载&lt;/h2&gt; &#xA;&lt;p&gt;目前有两个版本，版本间的代码结构有较大改动，文档和代码运行方法也不一致。重构版本更简洁，原始版本(raw_version)更大保留游双代码的原汁原味，从原始版本更容易入手.&lt;/p&gt; &#xA;&lt;p&gt;如果遇到github代码下载失败，或访问太慢，可以从以下链接下载，与Github最新提交同步.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;重构版本下载地址 : &lt;a href=&#34;https://pan.baidu.com/s/1PozKji8Oop-1BYcfixZR0g&#34;&gt;BaiduYun&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;提取码 : vsqq&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;原始版本(raw_version)下载地址 : &lt;a href=&#34;https://pan.baidu.com/s/1asMNDW-zog92DZY1Oa4kaQ&#34;&gt;BaiduYun&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;提取码 : 9wye&lt;/li&gt; &#xA;   &lt;li&gt;原始版本运行请参考&lt;a href=&#34;https://github.com/qinguoyi/TinyWebServer/tree/raw_version&#34;&gt;原始文档&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速运行&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;服务器测试环境&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ubuntu版本16.04&lt;/li&gt; &#xA;   &lt;li&gt;MySQL版本5.7.29&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;浏览器测试环境&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows、Linux均可&lt;/li&gt; &#xA;   &lt;li&gt;Chrome&lt;/li&gt; &#xA;   &lt;li&gt;FireFox&lt;/li&gt; &#xA;   &lt;li&gt;其他浏览器暂无测试&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;测试前确认已安装MySQL数据库&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;// 建立yourdb库&#xA;create database yourdb;&#xA;&#xA;// 创建user表&#xA;USE yourdb;&#xA;CREATE TABLE user(&#xA;    username char(50) NULL,&#xA;    passwd char(50) NULL&#xA;)ENGINE=InnoDB;&#xA;&#xA;// 添加数据&#xA;INSERT INTO user(username, passwd) VALUES(&#39;name&#39;, &#39;passwd&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;修改main.cpp中的数据库初始化信息&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;//数据库登录名,密码,库名&#xA;string user = &#34;root&#34;;&#xA;string passwd = &#34;root&#34;;&#xA;string databasename = &#34;yourdb&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;build&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;sh ./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;启动server&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;./server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;浏览器端&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;ip:9006&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;个性化运行&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;./server [-p port] [-l LOGWrite] [-m TRIGMode] [-o OPT_LINGER] [-s sql_num] [-t thread_num] [-c close_log] [-a actor_model]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;温馨提示:以上参数不是非必须，不用全部使用，根据个人情况搭配选用即可.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;-p，自定义端口号 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;默认9006&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-l，选择日志写入方式，默认同步写入 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;0，同步写入&lt;/li&gt; &#xA;   &lt;li&gt;1，异步写入&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-m，listenfd和connfd的模式组合，默认使用LT + LT &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;0，表示使用LT + LT&lt;/li&gt; &#xA;   &lt;li&gt;1，表示使用LT + ET&lt;/li&gt; &#xA;   &lt;li&gt;2，表示使用ET + LT&lt;/li&gt; &#xA;   &lt;li&gt;3，表示使用ET + ET&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-o，优雅关闭连接，默认不使用 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;0，不使用&lt;/li&gt; &#xA;   &lt;li&gt;1，使用&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-s，数据库连接数量 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;默认为8&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-t，线程数量 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;默认为8&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-c，关闭日志，默认打开 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;0，打开日志&lt;/li&gt; &#xA;   &lt;li&gt;1，关闭日志&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;-a，选择反应堆模型，默认Proactor &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;0，Proactor模型&lt;/li&gt; &#xA;   &lt;li&gt;1，Reactor模型&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;测试示例命令与含义&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;./server -p 9007 -l 1 -m 0 -o 1 -s 10 -t 10 -c 1 -a 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 端口9007&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 异步写入日志&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 使用LT + LT组合&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 使用优雅关闭连接&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 数据库连接池内有10条连接&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 线程池内有10条线程&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 关闭日志&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Reactor反应堆模型&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;庖丁解牛&lt;/h2&gt; &#xA;&lt;p&gt;近期版本迭代较快，以下内容多以旧版本(raw_version)代码为蓝本进行详解.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huixxi.github.io/2020/06/02/%E5%B0%8F%E7%99%BD%E8%A7%86%E8%A7%92%EF%BC%9A%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82%E7%A4%BE%E9%95%BF%E7%9A%84TinyWebServer/#more&#34;&gt;小白视角：一文读懂社长的TinyWebServer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxNzU2MzcwMw==&amp;amp;mid=2649274278&amp;amp;idx=3&amp;amp;sn=5840ff698e3f963c7855d702e842ec47&amp;amp;chksm=83ffbefeb48837e86fed9754986bca6db364a6fe2e2923549a378e8e5dec6e3cf732cdb198e2&amp;amp;scene=0&amp;amp;xtrack=1#rd&#34;&gt;最新版Web服务器项目详解 - 01 线程同步机制封装类&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxNzU2MzcwMw==&amp;amp;mid=2649274278&amp;amp;idx=4&amp;amp;sn=caa323faf0c51d882453c0e0c6a62282&amp;amp;chksm=83ffbefeb48837e841a6dbff292217475d9075e91cbe14042ad6e55b87437dcd01e6d9219e7d&amp;amp;scene=0&amp;amp;xtrack=1#rd&#34;&gt;最新版Web服务器项目详解 - 02 半同步半反应堆线程池（上）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/PB8vMwi8sB4Jw3WzAKpWOQ&#34;&gt;最新版Web服务器项目详解 - 03 半同步半反应堆线程池（下）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/BfnNl-3jc_x5WPrWEJGdzQ&#34;&gt;最新版Web服务器项目详解 - 04 http连接处理（上）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/wAQHU-QZiRt1VACMZZjNlw&#34;&gt;最新版Web服务器项目详解 - 05 http连接处理（中）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/451xNaSFHxcxfKlPBV3OCg&#34;&gt;最新版Web服务器项目详解 - 06 http连接处理（下）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/mmXLqh_NywhBXJvI45hchA&#34;&gt;最新版Web服务器项目详解 - 07 定时器处理非活动连接（上）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/fb_OUnlV1SGuOUdrGrzVgg&#34;&gt;最新版Web服务器项目详解 - 08 定时器处理非活动连接（下）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/IWAlPzVDkR2ZRI5iirEfCg&#34;&gt;最新版Web服务器项目详解 - 09 日志系统（上）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/f-ujwFyCe1LZa3EB561ehA&#34;&gt;最新版Web服务器项目详解 - 10 日志系统（下）&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxNzU2MzcwMw==&amp;amp;mid=2649274326&amp;amp;idx=1&amp;amp;sn=5af78e2bf6552c46ae9ab2aa22faf839&amp;amp;chksm=83ffbe8eb4883798c3abb82ddd124c8100a39ef41ab8d04abe42d344067d5e1ac1b0cac9d9a3&amp;amp;token=1450918099&amp;amp;lang=zh_CN#rd&#34;&gt;最新版Web服务器项目详解 - 11 数据库连接池&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxNzU2MzcwMw==&amp;amp;mid=2649274431&amp;amp;idx=4&amp;amp;sn=7595a70f06a79cb7abaebcd939e0cbee&amp;amp;chksm=83ffb167b4883871ce110aeb23e04acf835ef41016517247263a2c3ab6f8e615607858127ea6&amp;amp;token=1686112912&amp;amp;lang=zh_CN#rd&#34;&gt;最新版Web服务器项目详解 - 12 注册登录&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxNzU2MzcwMw==&amp;amp;mid=2649274431&amp;amp;idx=1&amp;amp;sn=2dd28c92f5d9704a57c001a3d2630b69&amp;amp;chksm=83ffb167b48838715810b27b8f8b9a576023ee5c08a8e5d91df5baf396732de51268d1bf2a4e&amp;amp;token=1686112912&amp;amp;lang=zh_CN#rd&#34;&gt;最新版Web服务器项目详解 - 13 踩坑与面试题&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;已更新完毕&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#qinguoyi/TinyWebServer&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=qinguoyi/TinyWebServer&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CPP11实现&lt;/h2&gt; &#xA;&lt;p&gt;更简洁，更优雅的CPP11实现：&lt;a href=&#34;https://github.com/markparticle/WebServer&#34;&gt;Webserver&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;Linux高性能服务器编程，游双著.&lt;/p&gt; &#xA;&lt;p&gt;感谢以下朋友的PR和帮助: &lt;a href=&#34;https://github.com/RownH&#34;&gt;@RownH&lt;/a&gt;，&lt;a href=&#34;https://github.com/mapleFU&#34;&gt;@mapleFU&lt;/a&gt;，&lt;a href=&#34;https://github.com/ZWiley&#34;&gt;@ZWiley&lt;/a&gt;，&lt;a href=&#34;https://github.com/zjuHong&#34;&gt;@zjuHong&lt;/a&gt;，&lt;a href=&#34;https://github.com/mamil&#34;&gt;@mamil&lt;/a&gt;，&lt;a href=&#34;https://github.com/byfate&#34;&gt;@byfate&lt;/a&gt;，&lt;a href=&#34;https://github.com/MaJun827&#34;&gt;@MaJun827&lt;/a&gt;，&lt;a href=&#34;https://github.com/BBLiu-coder&#34;&gt;@BBLiu-coder&lt;/a&gt;，&lt;a href=&#34;https://github.com/smoky96&#34;&gt;@smoky96&lt;/a&gt;，&lt;a href=&#34;https://github.com/yfBong&#34;&gt;@yfBong&lt;/a&gt;，&lt;a href=&#34;https://github.com/liuwuyao&#34;&gt;@liuwuyao&lt;/a&gt;，&lt;a href=&#34;https://github.com/Huixxi&#34;&gt;@Huixxi&lt;/a&gt;，&lt;a href=&#34;https://github.com/markparticle&#34;&gt;@markparticle&lt;/a&gt;，&lt;a href=&#34;https://github.com/Blogg9ggg&#34;&gt;@blogg9ggg&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/puerts</title>
    <updated>2023-09-03T01:46:09Z</updated>
    <id>tag:github.com,2023-09-03:/Tencent/puerts</id>
    <link href="https://github.com/Tencent/puerts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PUER(普洱) Typescript. Let&#39;s write your game in UE or Unity with TypeScript.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/pic/puerts_logo.png&#34; alt=&#34;Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/puerts/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD_3_Clause-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/puerts/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/puerts/releases/tag/Unreal_v1.0.5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/unreal-v1.0.5-blue.svg?sanitize=true&#34; alt=&#34;unreal&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/zhcn/install.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/unity(stable)-v2.0.0-blue.svg?sanitize=true&#34; alt=&#34;unity&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/Tencent/puerts/workflows/unity%20unittest/badge.svg?sanitize=true&#34; alt=&#34;Unity_Test&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/#what---%E6%99%AE%E6%B4%B1ts%E6%98%AF%E4%BB%80%E4%B9%88&#34;&gt;跳转中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;WHAT is PuerTS (PUER Typescript)?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;PuerTS&lt;/code&gt; is a TypeScript programming solution in Unity/Unreal/DotNet.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;provides a JavaScript Runtime.&lt;/li&gt; &#xA; &lt;li&gt;allows TypeScript to access the host engine with the help of TypeScript declarations generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WHY should I use PuerTS?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Facilitates game-building processes by combining JavaScript/Node.js ecosystem and professional game engines&lt;/li&gt; &#xA; &lt;li&gt;In contrast to Lua script, TypeScript supports static type checking, which significantly improves code robustness and maintainability.&lt;/li&gt; &#xA; &lt;li&gt;High efficiency: supports reflection call throughout the host - no extra steps needed for interop with C++/C#.&lt;/li&gt; &#xA; &lt;li&gt;High performance: supports static wrapper generation - handles complex scenes with high-performance demands.&lt;/li&gt; &#xA; &lt;li&gt;Talented WebGL Support: massive advantage in performance and dev efficiency compared to Lua, even faster than pure C# in some cases.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;HOW can I start to use PuerTS&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://puerts.github.io/en&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/faq.md&#34;&gt;general faq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/en/faq.md&#34;&gt;unreal faq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/en/faq.md&#34;&gt;unity faq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/en/install.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/en/install.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changelog&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/en/changelog.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/unity/Assets/core/upm/changelog.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/en/bugs.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/en/bugs.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Avaliable on these Engine&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;unreal engine 4.22 ~ latest&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;unity 5 ~ latest&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Any .net project&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Available on these Platform&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iOS&lt;/li&gt; &#xA; &lt;li&gt;Android&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA; &lt;li&gt;Macos&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ask for help&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/RYRY7D833n&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/puerts/discussions&#34;&gt;Github Discussion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;WHAT - 普洱TS是什么?&lt;/h2&gt; &#xA;&lt;p&gt;PuerTS是 Unity/Unreal/Dotnet 下的TypeScript编程解决方案&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;提供了一个JavaScript运行时&lt;/li&gt; &#xA; &lt;li&gt;提供TypeScript声明文件生成能力，易于通过TypeScript访问宿主引擎，&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WHY - 为什么我该用普洱TS?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;JavaScript生态有众多的库和工具链，结合专业商业引擎的渲染能力，快速打造游戏&lt;/li&gt; &#xA; &lt;li&gt;相比游戏领域常用的lua脚本，TypeScript的静态类型检查有助于编写更健壮，可维护性更好的程序&lt;/li&gt; &#xA; &lt;li&gt;高效：全引擎，全平台支持反射调用，无需额外步骤即可与宿主C++/C#通信。&lt;/li&gt; &#xA; &lt;li&gt;高性能：全引擎，全平台支持生成静态调用桥梁，兼顾了高性能的场景。&lt;/li&gt; &#xA; &lt;li&gt;WebGL平台下的天生优势：相比Lua脚本在WebGL版本的表现，PuerTS在性能和效率上都有极大提升，目前极限情况甚至比C#更快。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;HOW - 我该怎么开始&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://puerts.github.io&#34;&gt;官方文档&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/faq.md&#34;&gt;通用 faq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/zhcn/faq.md&#34;&gt;unreal faq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/zhcn/faq.md&#34;&gt;unity faq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;最新版本安装&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/zhcn/install.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/zhcn/install.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;改动日志&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/zhcn/changelog.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/unity/Assets/core/upm/changelog-hans.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;已知问题与解决办法&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unreal/zhcn/bugs.md&#34;&gt;unreal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/puerts/master/doc/unity/zhcn/bugs.md&#34;&gt;unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;可用引擎&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;unreal engine 4.22 ~ latest&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;unity 5 ~ latest&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;任意.net环境&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;可用平台&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iOS&lt;/li&gt; &#xA; &lt;li&gt;Android&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA; &lt;li&gt;Macos&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ask for help | 技术支持&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/RYRY7D833n&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/puerts/discussions&#34;&gt;Github Discussion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;QQ频道：cees1s2p9p&lt;/p&gt; &#xA;&lt;p&gt;QQ群：942696334&lt;/p&gt; &#xA;&lt;p&gt;UE4专属群：689643903&lt;/p&gt; &#xA;&lt;h2&gt;开发博客&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/column/c_1355534112468402176&#34;&gt;知乎专栏&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InternLM/lmdeploy</title>
    <updated>2023-09-03T01:46:09Z</updated>
    <id>tag:github.com,2023-09-03:/InternLM/lmdeploy</id>
    <link href="https://github.com/InternLM/lmdeploy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LMDeploy is a toolkit for compressing, deploying, and serving LLMs.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/resources/lmdeploy-logo.png&#34; width=&#34;450&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/actions&#34;&gt;&lt;img src=&#34;https://github.com/InternLM/lmdeploy/workflows/lint/badge.svg?sanitize=true&#34; alt=&#34;badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lmdeploy&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lmdeploy&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/tree/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/InternLM/lmdeploy.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/InternLM/lmdeploy&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 👋 join us on &lt;a href=&#34;https://twitter.com/intern_lm&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://discord.gg/xa29JuW87d&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; and &lt;a href=&#34;https://r.vansin.top/?r=internwx&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;News 🎉&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/08] TurboMind supports flash-attention2.&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] TurboMind supports Qwen-7B, dynamic NTK-RoPE scaling and dynamic logN scaling&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] TurboMind supports Windows (tp=1)&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] TurboMind supports 4-bit inference, 2.4x faster than FP16, the fastest open-source implementation🚀. Check &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/w4a16.md&#34;&gt;this&lt;/a&gt; guide for detailed info&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] LMDeploy has launched on the &lt;a href=&#34;https://huggingface.co/lmdeploy&#34;&gt;HuggingFace Hub&lt;/a&gt;, providing ready-to-use 4-bit models.&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] LMDeploy supports 4-bit quantization using the &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; algorithm.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07] TurboMind supports Llama-2 70B with GQA.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07] TurboMind supports Llama-2 7B/13B.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07] TurboMind supports tensor-parallel inference of InternLM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;LMDeploy is a toolkit for compressing, deploying, and serving LLM, developed by the &lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt; teams. It has the following core features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Inference Engine (TurboMind)&lt;/strong&gt;: Based on &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt;, we have implemented an efficient inference engine - TurboMind, which supports the inference of LLaMA and its variant models on NVIDIA GPUs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive Inference Mode&lt;/strong&gt;: By caching the k/v of attention during multi-round dialogue processes, it remembers dialogue history, thus avoiding repetitive processing of historical sessions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-GPU Model Deployment and Quantization&lt;/strong&gt;: We provide comprehensive model deployment and quantification support, and have been validated at different scales.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Persistent Batch Inference&lt;/strong&gt;: Further optimization of model execution efficiency.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/InternLM/lmdeploy/assets/67539920/e3876167-0671-44fc-ac52-5a0f9382493e&#34; alt=&#34;PersistentBatchInference&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;LMDeploy&lt;/code&gt; has two inference backends, &lt;code&gt;Pytorch&lt;/code&gt; and &lt;code&gt;TurboMind&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;TurboMind&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;br&gt; W4A16 inference requires Nvidia GPU with Ampere architecture or above.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tensor Parallel&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FP16&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;KV INT8&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;W4A16&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;W8A8&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InternLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pytorch&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tensor Parallel&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FP16&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;KV INT8&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;W4A16&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;W8A8&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InternLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Case I&lt;/strong&gt;: output token throughput with fixed input token and output token number (1, 2048)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Case II&lt;/strong&gt;: request throughput with real conversation data&lt;/p&gt; &#xA;&lt;p&gt;Test Setting: LLaMA-7B, NVIDIA A100(80G)&lt;/p&gt; &#xA;&lt;p&gt;The output token throughput of TurboMind exceeds 2000 tokens/s, which is about 5% - 15% higher than DeepSpeed overall and outperforms huggingface transformers by up to 2.3x. And the request throughput of TurboMind is 30% higher than vLLM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/InternLM/lmdeploy/assets/4560679/7775c518-608e-4e5b-be73-7645a444e774&#34; alt=&#34;benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Install lmdeploy with pip ( python 3.8+) or &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/build.md&#34;&gt;from source&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install lmdeploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy InternLM&lt;/h3&gt; &#xA;&lt;h4&gt;Get InternLM model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 1. Download InternLM model&#xA;&#xA;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://huggingface.co/internlm/internlm-chat-7b /path/to/internlm-chat-7b&#xA;&#xA;# if you want to clone without large files – just their pointers&#xA;# prepend your git clone with the following env var:&#xA;GIT_LFS_SKIP_SMUDGE=1&#xA;&#xA;# 2. Convert InternLM model to turbomind&#39;s format, which will be in &#34;./workspace&#34; by default&#xA;python3 -m lmdeploy.serve.turbomind.deploy internlm-chat-7b /path/to/internlm-chat-7b&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Inference by TurboMind&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m lmdeploy.turbomind.chat ./workspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;br&gt; When inferring with FP16 precision, the InternLM-7B model requires at least 15.7G of GPU memory overhead on TurboMind. &lt;br&gt; It is recommended to use NVIDIA cards such as 3090, V100, A100, etc. Disable GPU ECC can free up 10% memory, try &lt;code&gt;sudo nvidia-smi --ecc-config=0&lt;/code&gt; and reboot system.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;br&gt; Tensor parallel is available to perform inference on multiple GPUs. Add &lt;code&gt;--tp=&amp;lt;num_gpu&amp;gt;&lt;/code&gt; on &lt;code&gt;chat&lt;/code&gt; to enable runtime TP.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Serving with gradio&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m lmdeploy.serve.gradio.app ./workspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/InternLM/lmdeploy/assets/67539920/08d1e6f2-3767-44d5-8654-c85767cec2ab&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Serving with Restful API&lt;/h4&gt; &#xA;&lt;p&gt;Launch inference server by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m lmdeploy.serve.openai.api_server ./workspace server_ip server_port --instance_num 32 --tp 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can communicate with it by command line,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# restful_api_url is what printed in api_server.py, e.g. http://localhost:23333&#xA;python -m lmdeploy.serve.openai.api_client restful_api_url&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or webui,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# restful_api_url is what printed in api_server.py, e.g. http://localhost:23333&#xA;# server_ip and server_port here are for gradio ui&#xA;# example: python -m lmdeploy.serve.gradio.app http://localhost:23333 localhost 6006 --restful_api True&#xA;python -m lmdeploy.serve.gradio.app restful_api_url server_ip --restful_api True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/restful_api.md&#34;&gt;restful_api.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h4&gt;Serving with Triton Inference Server&lt;/h4&gt; &#xA;&lt;p&gt;Launch inference server by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash workspace/service_docker_up.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can communicate with the inference server by command line,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m lmdeploy.serve.client {server_ip_addresss}:33337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or webui,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m lmdeploy.serve.gradio.app {server_ip_addresss}:33337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the deployment of other supported models, such as LLaMA, LLaMA-2, vicuna and so on, you can find the guide from &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/serving.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Inference with PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;For detailed instructions on Inference pytorch models, see &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/pytorch.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Single GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m lmdeploy.pytorch.chat $NAME_OR_PATH_TO_HF_MODEL \&#xA;    --max_new_tokens 64 \&#xA;    --temperture 0.8 \&#xA;    --top_p 0.95 \&#xA;    --seed 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Tensor Parallel with DeepSpeed&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;deepspeed --module --num_gpus 2 lmdeploy.pytorch.chat \&#xA;    $NAME_OR_PATH_TO_HF_MODEL \&#xA;    --max_new_tokens 64 \&#xA;    --temperture 0.8 \&#xA;    --top_p 0.95 \&#xA;    --seed 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You need to install deepspeed first to use this feature.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;h4&gt;Weight INT4 Quantization&lt;/h4&gt; &#xA;&lt;p&gt;LMDeploy uses &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; algorithm for model weight quantization&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/zh_cn/w4a16.md&#34;&gt;Click here&lt;/a&gt; to view the test results for weight int4 usage.&lt;/p&gt; &#xA;&lt;h4&gt;KV Cache INT8 Quantization&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/zh_cn/kv_int8.md&#34;&gt;Click here&lt;/a&gt; to view the usage method, implementation formula, and test results for kv int8.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;&lt;br&gt; runtime Tensor Parallel for quantilized model is not available. Please setup &lt;code&gt;--tp&lt;/code&gt; on &lt;code&gt;deploy&lt;/code&gt; to enable static TP.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to LMDeploy. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mit-han-lab/llm-awq&#34;&gt;llm-awq&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>