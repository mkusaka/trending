<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-06T01:39:24Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ashishps1/awesome-low-level-design</title>
    <updated>2025-04-06T01:39:24Z</updated>
    <id>tag:github.com,2025-04-06:/ashishps1/awesome-low-level-design</id>
    <link href="https://github.com/ashishps1/awesome-low-level-design" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn Low Level Design (LLD) and prepare for interviews using free resources.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/images/lld-repo-logo.png&#34; width=&#34;350&#34; height=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://blog.algomaster.io/&#34;&gt;Join Free Newsletter&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;This repository contains resources to learn Low Level Design (LLD) / Object Oriented Design (OOD) and prepare for interviews.&lt;/p&gt; &#xA;&lt;p&gt;üëâ For a better and more comprehensive experience, checkout the &lt;a href=&#34;https://algomaster.io/learn/lld&#34;&gt;LLD page at AlgoMaster.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìå Fundamental Concepts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/p/basic-oop-concepts-explained-with-code&#34;&gt;Basics OOP Concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/backticks-tildes/the-s-o-l-i-d-principles-in-pictures-b34ce2f1e898&#34;&gt;SOLID Principles with Pictures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/p/solid-principles-explained-with-code&#34;&gt;SOLID Principles with Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/p/082450d8-0e7b-4447-a8dc-b7308e45f048&#34;&gt;DRY Principle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/p/8c3c7da7-885b-4a9c-a6e4-70ee02de4772&#34;&gt;YAGNI Principle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/p/21b57678-b351-4ed4-b390-3b6308af2f7d&#34;&gt;KISS Principle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/object-oriented-design&#34;&gt;Coursera - Object-Oriented Design&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Design Patterns&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Creational Patterns&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Structural Patterns&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Behavioral Patterns&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://blog.algomaster.io/p/singleton-design-pattern&#34;&gt;Singleton&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/adapter&#34;&gt;Adapter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/iterator&#34;&gt;Iterator&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/factory-method&#34;&gt;Factory Method&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/bridge&#34;&gt;Bridge&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/observer&#34;&gt;Observer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/abstract-factory&#34;&gt;Abstract Factory&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/composite&#34;&gt;Composite&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/strategy&#34;&gt;Strategy&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/builder&#34;&gt;Builder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/decorator&#34;&gt;Decorator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/command&#34;&gt;Command&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/prototype&#34;&gt;Prototype&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/facade&#34;&gt;Facade&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/state&#34;&gt;State&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/flyweight&#34;&gt;Flyweight&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/template-method&#34;&gt;Template Method&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/proxy&#34;&gt;Proxy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/visitor&#34;&gt;Visitor&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/mediator&#34;&gt;Mediator&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/memento&#34;&gt;Memento&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://refactoring.guru/design-patterns/chain-of-responsibility&#34;&gt;Chain of Responsibility&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üóÇÔ∏è UML&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/p/uml-class-diagram-explained-with-examples&#34;&gt;Class Diagram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.visual-paradigm.com/guide/uml-unified-modeling-language/what-is-use-case-diagram/&#34;&gt;Use Case Diagram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.visual-paradigm.com/guide/uml-unified-modeling-language/what-is-sequence-diagram/&#34;&gt;Sequence Diagram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.visual-paradigm.com/guide/uml-unified-modeling-language/what-is-activity-diagram/&#34;&gt;Activity Diagram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.visual-paradigm.com/guide/uml-unified-modeling-language/what-is-state-machine-diagram/&#34;&gt;State Machine Diagram&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚úÖ &lt;a href=&#34;https://blog.algomaster.io/p/how-to-answer-a-lld-interview-problem&#34;&gt;How to Answer a LLD Interview Problem&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/images/interview-template.png&#34; width=&#34;350&#34; height=&#34;250&#34;&gt; &#xA;&lt;h2&gt;üíª Low Level Design Interview Problems&lt;/h2&gt; &#xA;&lt;h3&gt;Easy Problems&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/parking-lot.md&#34;&gt;Design Parking Lot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/stack-overflow.md&#34;&gt;Design Stack Overflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/vending-machine.md&#34;&gt;Design a Vending Machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/logging-framework.md&#34;&gt;Design Logging Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/traffic-signal.md&#34;&gt;Design Traffic Signal Control System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/coffee-vending-machine.md&#34;&gt;Design Coffee Vending Machine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/task-management-system.md&#34;&gt;Design a Task Management System&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Medium Problems&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/atm.md&#34;&gt;Design ATM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/linkedin.md&#34;&gt;Design LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/lru-cache.md&#34;&gt;Design LRU Cache&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/tic-tac-toe.md&#34;&gt;Design Tic Tac Toe Game&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/pub-sub-system.md&#34;&gt;Design Pub Sub System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/elevator-system.md&#34;&gt;Design an Elevator System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/car-rental-system.md&#34;&gt;Design Car Rental System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/online-auction-system.md&#34;&gt;Design an Online Auction System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/hotel-management-system.md&#34;&gt;Design Hotel Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/digital-wallet-service.md&#34;&gt;Design a Digital Wallet Service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/airline-management-system.md&#34;&gt;Design Airline Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/library-management-system.md&#34;&gt;Design a Library Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/social-networking-service.md&#34;&gt;Design a Social Network like Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/restaurant-management-system.md&#34;&gt;Design Restaurant Management System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/concert-ticket-booking-system.md&#34;&gt;Design a Concert Ticket Booking System&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hard Problems&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/cricinfo.md&#34;&gt;Design CricInfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/splitwise.md&#34;&gt;Design Splitwise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/chess-game.md&#34;&gt;Design Chess Game&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/snake-and-ladder.md&#34;&gt;Design a Snake and Ladder game&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/ride-sharing-service.md&#34;&gt;Design Ride-Sharing Service like Uber&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/course-registration-system.md&#34;&gt;Design Course Registration System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/movie-ticket-booking-system.md&#34;&gt;Design Movie Ticket Booking System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/online-shopping-service.md&#34;&gt;Design Online Shopping System like Amazon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/online-stock-brokerage-system.md&#34;&gt;Design Online Stock Brokerage System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/music-streaming-service.md&#34;&gt;Design Music Streaming Service like Spotify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ashishps1/awesome-low-level-design/main/problems/food-delivery-service.md&#34;&gt;Design Online Food Delivery Service like Swiggy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Books&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.in/dp/9385889753&#34;&gt;Head First Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.in/dp/B001GSTOAM&#34;&gt;Clean Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.in/dp/0134757599&#34;&gt;Refactoring: Improving the Design of Existing Code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì© Newsletter&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.algomaster.io/&#34;&gt;AlgoMaster Newsletter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Additional resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/design-patterns&#34;&gt;Coursera - Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DovAmir/awesome-design-patterns&#34;&gt;Github - Awesome Design Patterns&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! If you&#39;d like to add a new problem, improve existing content, or fix errors:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository&lt;/li&gt; &#xA; &lt;li&gt;Create a feature branch: &lt;code&gt;git checkout -b feature/your-feature-name&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes: &lt;code&gt;git commit -m &#39;Add some feature&#39;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch: &lt;code&gt;git push origin feature/your-feature-name&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please make sure to update Readme files and documentation as appropriate.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;If you find this resource helpful, please give it a star and share it with others!&lt;/i&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HIllya51/LunaTranslator</title>
    <updated>2025-04-06T01:39:24Z</updated>
    <id>tag:github.com,2025-04-06:/HIllya51/LunaTranslator</id>
    <link href="https://github.com/HIllya51/LunaTranslator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GalgameÁøªËØëÂô®ÔºåÊîØÊåÅHOOK„ÄÅOCR„ÄÅÂâ™Ë¥¥ÊùøÁ≠â„ÄÇVisual Novel Translator , support HOOK / OCR / clipboard&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LunaTranslator&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;‰∏ÄÊ¨ægalgameÁøªËØëÂô®&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;ÁÆÄ‰Ωì‰∏≠Êñá | &lt;a href=&#34;https://raw.githubusercontent.com/HIllya51/LunaTranslator/main/README_en.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/HIllya51/LunaTranslator/main/otherlang.md&#34;&gt;Other Language&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://docs.lunatranslator.org/&#34;&gt;‰ΩøÁî®ËØ¥Êòé&lt;/a&gt; | &lt;a href=&#34;https://space.bilibili.com/592120404/video&#34;&gt;ËßÜÈ¢ëÊïôÁ®ã&lt;/a&gt; | &lt;a href=&#34;https://docs.lunatranslator.org/README.html&#34;&gt;ËΩØ‰ª∂‰∏ãËΩΩ&lt;/a&gt; | &lt;a href=&#34;https://qm.qq.com/q/I5rr3uEpi2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/QQ%E7%BE%A4-963119821-FF007C?style=for-the-badge&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/ErtDwVeAbB&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1262692128031772733?label=Discord&amp;amp;logo=discord&amp;amp;color=FF007C&amp;amp;style=for-the-badge&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;ÂäüËÉΩÊîØÊåÅ&lt;/h2&gt; &#xA;&lt;h4&gt;ÊñáÊú¨ËæìÂÖ•&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HOOK&lt;/strong&gt; ÊîØÊåÅ‰ΩøÁî®HOOKÊñπÂºèËé∑ÂèñÊñáÊú¨„ÄÇÂØπ‰∫éÈÉ®ÂàÜÊ∏∏ÊàèÂºïÊìéÔºåËøòÊîØÊåÅ&lt;a href=&#34;https://docs.lunatranslator.org/embedtranslate.html&#34;&gt;ÂÜÖÂµåÁøªËØë&lt;/a&gt;„ÄÇËøòÊîØÊåÅÊèêÂèñÈÉ®ÂàÜ&lt;a href=&#34;https://docs.lunatranslator.org/emugames.html&#34;&gt;Ê®°ÊãüÂô®&lt;/a&gt;‰∏äËøêË°åÁöÑÊ∏∏ÊàèÁöÑÊñáÊú¨„ÄÇÂØπ‰∫é‰∏çÊîØÊåÅÊàñÊîØÊåÅ‰∏çÂ•ΩÁöÑÊ∏∏ÊàèÔºåËØ∑&lt;a href=&#34;https://github.com/HIllya51/LunaTranslator/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=01_game_request.yaml&#34;&gt;Êèê‰∫§ÂèçÈ¶à&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OCR&lt;/strong&gt; ÊîØÊåÅ &lt;strong&gt;&lt;a href=&#34;https://docs.lunatranslator.org/useapis/ocrapi.html&#34;&gt;Á¶ªÁ∫øOCR&lt;/a&gt;&lt;/strong&gt; ( ÂÜÖÁΩÆOCRÂºïÊìé„ÄÅWindowsOCR„ÄÅTesseract5„ÄÅmanga-ocr„ÄÅWeChat/QQ OCR ) Âíå &lt;strong&gt;&lt;a href=&#34;https://docs.lunatranslator.org/useapis/ocrapi.html&#34;&gt;Âú®Á∫øOCR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ââ™Ë¥¥Êùø&lt;/strong&gt; ÊîØÊåÅ‰ªéÂâ™Ë¥¥Êùø‰∏≠Ëé∑ÂèñÊñáÊú¨ËøõË°åÁøªËØëÔºå‰πüÂèØ‰ª•Â∞ÜÊèêÂèñÁöÑÊñáÊú¨ËæìÂá∫Âà∞Ââ™Ë¥¥Êùø&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;ÁøªËØëÂô®&lt;/h4&gt; &#xA;&lt;p&gt;ÊîØÊåÅÂá†‰πéÊâÄÊúâËÉΩÊÉ≥ÂæóÂà∞ÁöÑÁøªËØëÂºïÊìéÔºåÂåÖÊã¨Ôºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Âú®Á∫øÁøªËØë&lt;/strong&gt; ÊîØÊåÅÂ§ßÈáèÂÖçÊ≥®ÂÜåÂºÄÁÆ±Âç≥Áî®ÁöÑÂú®Á∫øÁøªËØëÊé•Âè£Ôºå‰πüÊîØÊåÅ‰ΩøÁî®Áî®Êà∑Ê≥®ÂÜåÁöÑAPIÁöÑ &lt;strong&gt;&lt;a href=&#34;https://docs.lunatranslator.org/useapis/tsapi.html&#34;&gt;‰º†ÁªüÁøªËØë&lt;/a&gt;&lt;/strong&gt; Âíå &lt;strong&gt;&lt;a href=&#34;https://docs.lunatranslator.org/guochandamoxing.html&#34;&gt;Â§ßÊ®°ÂûãÁøªËØë&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¶ªÁ∫øÁøªËØë&lt;/strong&gt; ÊîØÊåÅÂ∏∏ËßÅ &lt;strong&gt;‰º†ÁªüÁøªËØë&lt;/strong&gt; ÂºïÊìéÂíåÁ¶ªÁ∫øÈÉ®ÁΩ≤ÁöÑ &lt;strong&gt;&lt;a href=&#34;https://docs.lunatranslator.org/offlinellm.html&#34;&gt;Â§ßÊ®°ÂûãÁøªËØë&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;È¢ÑÁøªËØë&lt;/strong&gt; ÊîØÊåÅËØªÂèñÈ¢ÑÁøªËØëÊñá‰ª∂ÔºåÊîØÊåÅÁøªËØëÁºìÂ≠ò&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÊîØÊåÅËá™ÂÆö‰πâÁøªËØëÊâ©Â±ï&lt;/strong&gt; ÊîØÊåÅ‰ΩøÁî®pythonËØ≠Ë®ÄÊâ©Â±ïÂÖ∂‰ªñÁøªËØëÊé•Âè£&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;ËØ≠Èü≥ÂêàÊàê&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Á¶ªÁ∫øTTS&lt;/strong&gt; ÊîØÊåÅWindowsTTS„ÄÅVoiceRoid2/VoiceRoid+„ÄÅNeoSpeech„ÄÅVOICEVOX„ÄÅVITS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Âú®Á∫øTTS&lt;/strong&gt; ÊîØÊåÅÁÅ´Â±±TTS„ÄÅÊúâÈÅìTTS„ÄÅEdgeTTS„ÄÅË∞∑Ê≠åTTS„ÄÅOpenAI&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Êó•ËØ≠Â≠¶‰π†&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êó•ËØ≠ÂàÜËØçÂèäÂÅáÂêçÊòæÁ§∫&lt;/strong&gt; ÊîØÊåÅ‰ΩøÁî® Mecab Á≠âÂàÜËØçÂíåÊòæÁ§∫ÂÅáÂêç&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êü•ËØç&lt;/strong&gt; ÊîØÊåÅ‰ΩøÁî® &lt;strong&gt;Á¶ªÁ∫øËæû‰π¶&lt;/strong&gt; ( MDICT ) Âíå &lt;strong&gt;Âú®Á∫øËæû‰π¶&lt;/strong&gt; ËøõË°åÂçïËØçÊü•ËØ¢&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Anki&lt;/strong&gt; ÊîØÊåÅ‰ΩøÁî®‰∏ÄÈîÆÊ∑ªÂä†ÂçïËØçÂà∞anki‰∏≠&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Âä†ËΩΩÊµèËßàÂô®Êèí‰ª∂&lt;/strong&gt; ÂèØ‰ª•Âú®ËΩØ‰ª∂ÂÜÖÂä†ËΩΩYomitanÁ≠âÊµèËßàÂô®Êèí‰ª∂‰ª•ËæÖÂä©ÂÆûÁé∞‰∏Ä‰∫õÂÖ∂‰ªñÂäüËÉΩ&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ÊîØÊåÅ‰ΩúËÄÖ&lt;/h2&gt; &#xA;&lt;p&gt;Â¶ÇÊûú‰Ω†ÊÑüËßâËØ•ËΩØ‰ª∂ÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåÊ¨¢ËøéÂæÆ‰ø°Êâ´Á†ÅËµûÂä©ÔºåË∞¢Ë∞¢Ôºå‰πà‰πàÂìí~ &lt;img src=&#34;https://raw.githubusercontent.com/HIllya51/LunaTranslator/src/files/static/zan.jpg&#34; style=&#34;height: 400px !important;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ÂºÄÊ∫êËÆ∏ÂèØ&lt;/h2&gt; &#xA;&lt;p&gt;LunaTranslator‰ΩøÁî® &lt;a href=&#34;https://raw.githubusercontent.com/HIllya51/LunaTranslator/LICENSE&#34;&gt;GPLv3&lt;/a&gt; ËÆ∏ÂèØËØÅ„ÄÇ&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ÂºïÁî®ÁöÑÈ°πÁõÆ&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/Artikash/Textractor&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/Artikash/Textractor&#34;&gt;Artikash/Textractor&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/RapidAI/RapidOcrOnnx&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/RapidAI/RapidOcrOnnx&#34;&gt;RapidAI/RapidOcrOnnx&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/PaddlePaddle/PaddleOCR&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddlePaddle/PaddleOCR&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/Blinue/Magpie&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/Blinue/Magpie&#34;&gt;Blinue/Magpie&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/nanokina/ebyroid&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/nanokina/ebyroid&#34;&gt;nanokina/ebyroid&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/xupefei/Locale-Emulator&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/xupefei/Locale-Emulator&#34;&gt;xupefei/Locale-Emulator&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/InWILL/Locale_Remulator&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/InWILL/Locale_Remulator&#34;&gt;InWILL/Locale_Remulator&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/zxyacb/ntlea&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/zxyacb/ntlea&#34;&gt;zxyacb/ntlea&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/Chuyu-Team/YY-Thunks&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/Chuyu-Team/YY-Thunks&#34;&gt;Chuyu-Team/YY-Thunks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/Chuyu-Team/VC-LTL5&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/Chuyu-Team/VC-LTL5&#34;&gt;Chuyu-Team/VC-LTL5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/uyjulian/AtlasTranslate&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/uyjulian/AtlasTranslate&#34;&gt;uyjulian/AtlasTranslate&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/ilius/pyglossary&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/ilius/pyglossary&#34;&gt;ilius/pyglossary&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/ikegami-yukino/mecab&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/ikegami-yukino/mecab&#34;&gt;ikegami-yukino/mecab&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/AngusJohnson/Clipper2&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/AngusJohnson/Clipper2&#34;&gt;AngusJohnson/Clipper2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/rapidfuzz/rapidfuzz-cpp&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/rapidfuzz/rapidfuzz-cpp&#34;&gt;rapidfuzz/rapidfuzz-cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/TsudaKageyu/minhook&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/TsudaKageyu/minhook&#34;&gt;TsudaKageyu/minhook&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/lobehub/lobe-icons&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/lobehub/lobe-icons&#34;&gt;lobehub/lobe-icons&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/kokke/tiny-AES-c&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/kokke/tiny-AES-c&#34;&gt;kokke/tiny-AES-c&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/TPN-Team/OCR&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/TPN-Team/OCR&#34;&gt;TPN-Team/OCR&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/AuroraWright/owocr&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/AuroraWright/owocr&#34;&gt;AuroraWright/owocr&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;img src=&#34;https://img.shields.io/github/license/b1tg/win11-oneocr&#34; alt=&#34;img&#34;&gt; &lt;a href=&#34;https://github.com/b1tg/win11-oneocr&#34;&gt;b1tg/win11-oneocr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>ggml-org/whisper.cpp</title>
    <updated>2025-04-06T01:39:24Z</updated>
    <id>tag:github.com,2025-04-06:/ggml-org/whisper.cpp</id>
    <link href="https://github.com/ggml-org/whisper.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of OpenAI&#39;s Whisper model in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;whisper.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1991296/235238348-05d0f6a4-da44-4900-a1de-d0707e75b763.jpeg&#34; alt=&#34;whisper.cpp&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/actions&#34;&gt;&lt;img src=&#34;https://github.com/ggml-org/whisper.cpp/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://conan.io/center/whisper-cpp&#34;&gt;&lt;img src=&#34;https://shields.io/conan/v/whisper-cpp&#34; alt=&#34;Conan Center&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/whisper.cpp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/whisper.cpp.svg?sanitize=true&#34; alt=&#34;npm&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stable: &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/releases/tag/v1.7.5&#34;&gt;v1.7.5&lt;/a&gt; / &lt;a href=&#34;https://github.com/orgs/ggml-org/projects/4/&#34;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;High-performance inference of &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI&#39;s Whisper&lt;/a&gt; automatic speech recognition (ASR) model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without dependencies&lt;/li&gt; &#xA; &lt;li&gt;Apple Silicon first-class citizen - optimized via ARM NEON, Accelerate framework, Metal and &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#core-ml-support&#34;&gt;Core ML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AVX intrinsics support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#power-vsx-intrinsics&#34;&gt;VSX intrinsics support for POWER architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mixed F16 / F32 precision&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#quantization&#34;&gt;Integer quantization support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Zero memory allocations at runtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#vulkan-gpu-support&#34;&gt;Vulkan support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support for CPU-only inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#nvidia-gpu-support&#34;&gt;Efficient GPU support for NVIDIA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#openvino-support&#34;&gt;OpenVINO Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#ascend-npu-support&#34;&gt;Ascend NPU Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/raw/master/include/whisper.h&#34;&gt;C-style API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mac OS (Intel and Arm)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.objc&#34;&gt;iOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.android&#34;&gt;Android&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/bindings/java/README.md&#34;&gt;Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Linux / &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/issues/56#issuecomment-1350920264&#34;&gt;FreeBSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.wasm&#34;&gt;WebAssembly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Windows (&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/raw/master/.github/workflows/build.yml#L117-L144&#34;&gt;MSVC&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/issues/168&#34;&gt;MinGW&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/166&#34;&gt;Raspberry Pi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/pkgs/container/whisper.cpp&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The entire high-level implementation of the model is contained in &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/include/whisper.h&#34;&gt;whisper.h&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/src/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;. The rest of the code is part of the &lt;a href=&#34;https://github.com/ggml-org/ggml&#34;&gt;&lt;code&gt;ggml&lt;/code&gt;&lt;/a&gt; machine learning library.&lt;/p&gt; &#xA;&lt;p&gt;Having such a lightweight implementation of the model allows to easily integrate it in different platforms and applications. As an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.objc&#34;&gt;whisper.objc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also easily make your own offline voice assistant application: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/command&#34;&gt;command&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On Apple Silicon, the inference runs fully on the GPU via Metal:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/assets/1991296/c82e8f86-60dc-49f2-b048-d2fdbd6b5225&#34;&gt;https://github.com/ggml-org/whisper.cpp/assets/1991296/c82e8f86-60dc-49f2-b048-d2fdbd6b5225&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;First clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ggml-org/whisper.cpp.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate into the directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd whisper.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, download one of the Whisper &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/models/README.md&#34;&gt;models&lt;/a&gt; converted in &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/#ggml-format&#34;&gt;&lt;code&gt;ggml&lt;/code&gt; format&lt;/a&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh ./models/download-ggml-model.sh base.en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now build the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/cli&#34;&gt;whisper-cli&lt;/a&gt; example and transcribe an audio file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# build the project&#xA;cmake -B build&#xA;cmake --build build --config Release&#xA;&#xA;# transcribe an audio file&#xA;./build/bin/whisper-cli -f samples/jfk.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For a quick demo, simply run &lt;code&gt;make base.en&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The command downloads the &lt;code&gt;base.en&lt;/code&gt; model converted to custom &lt;code&gt;ggml&lt;/code&gt; format and runs the inference on all &lt;code&gt;.wav&lt;/code&gt; samples in the folder &lt;code&gt;samples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage instructions, run: &lt;code&gt;./build/bin/whisper-cli -h&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/cli&#34;&gt;whisper-cli&lt;/a&gt; example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool. For example, you can use &lt;code&gt;ffmpeg&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More audio samples&lt;/h2&gt; &#xA;&lt;p&gt;If you want some extra audio samples to play with, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make -j samples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via &lt;code&gt;ffmpeg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download and run the other models as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make -j tiny.en&#xA;make -j tiny&#xA;make -j base.en&#xA;make -j base&#xA;make -j small.en&#xA;make -j small&#xA;make -j medium.en&#xA;make -j medium&#xA;make -j large-v1&#xA;make -j large-v2&#xA;make -j large-v3&#xA;make -j large-v3-turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Memory usage&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Disk&lt;/th&gt; &#xA;   &lt;th&gt;Mem&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tiny&lt;/td&gt; &#xA;   &lt;td&gt;75 MiB&lt;/td&gt; &#xA;   &lt;td&gt;~273 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;142 MiB&lt;/td&gt; &#xA;   &lt;td&gt;~388 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;466 MiB&lt;/td&gt; &#xA;   &lt;td&gt;~852 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;medium&lt;/td&gt; &#xA;   &lt;td&gt;1.5 GiB&lt;/td&gt; &#xA;   &lt;td&gt;~2.1 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;2.9 GiB&lt;/td&gt; &#xA;   &lt;td&gt;~3.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;POWER VSX Intrinsics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;whisper.cpp&lt;/code&gt; supports POWER architectures and includes code which significantly speeds operation on Linux running on POWER9/10, making it capable of faster-than-realtime transcription on underclocked Raptor Talos II. Ensure you have a BLAS package installed, and replace the standard cmake setup with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# build with GGML_BLAS defined&#xA;cmake -B build -DGGML_BLAS=1&#xA;cmake --build build --config Release&#xA;./build/bin/whisper-cli [ .. etc .. ]&#xA;&#xA;## Quantization&#xA;&#xA;`whisper.cpp` supports integer quantization of the Whisper `ggml` models.&#xA;Quantized models require less memory and disk space and depending on the hardware can be processed more efficiently.&#xA;&#xA;Here are the steps for creating and using a quantized model:&#xA;&#xA;```bash&#xA;# quantize a model with Q5_0 method&#xA;cmake -B build&#xA;cmake --build build --config Release&#xA;./build/bin/quantize models/ggml-base.en.bin models/ggml-base.en-q5_0.bin q5_0&#xA;&#xA;# run the examples as usual, specifying the quantized model file&#xA;./build/bin/whisper-cli -m models/ggml-base.en-q5_0.bin ./samples/gb0.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Core ML support&lt;/h2&gt; &#xA;&lt;p&gt;On Apple Silicon devices, the Encoder inference can be executed on the Apple Neural Engine (ANE) via Core ML. This can result in significant speed-up - more than x3 faster compared with CPU-only execution. Here are the instructions for generating a Core ML model and using it with &lt;code&gt;whisper.cpp&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Python dependencies needed for the creation of the Core ML model:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ane_transformers&#xA;pip install openai-whisper&#xA;pip install coremltools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To ensure &lt;code&gt;coremltools&lt;/code&gt; operates correctly, please confirm that &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; is installed and execute &lt;code&gt;xcode-select --install&lt;/code&gt; to install the command-line tools.&lt;/li&gt; &#xA;   &lt;li&gt;Python 3.11 is recommended.&lt;/li&gt; &#xA;   &lt;li&gt;MacOS Sonoma (version 14) or newer is recommended, as older versions of MacOS might experience issues with transcription hallucination.&lt;/li&gt; &#xA;   &lt;li&gt;[OPTIONAL] It is recommended to utilize a Python version management system, such as &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt; for this step: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;To create an environment, use: &lt;code&gt;conda create -n py311-whisper python=3.11 -y&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;To activate the environment, use: &lt;code&gt;conda activate py311-whisper&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate a Core ML model. For example, to generate a &lt;code&gt;base.en&lt;/code&gt; model, use:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./models/generate-coreml-model.sh base.en&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will generate the folder &lt;code&gt;models/ggml-base.en-encoder.mlmodelc&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build &lt;code&gt;whisper.cpp&lt;/code&gt; with Core ML support:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# using CMake&#xA;cmake -B build -DWHISPER_COREML=1&#xA;cmake --build build -j --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the examples as usual. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ ./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/jfk.wav&#xA;&#xA;...&#xA;&#xA;whisper_init_state: loading Core ML model from &#39;models/ggml-base.en-encoder.mlmodelc&#39;&#xA;whisper_init_state: first run on a device may take a while ...&#xA;whisper_init_state: Core ML model loaded&#xA;&#xA;system_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | COREML = 1 |&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first run on a device is slow, since the ANE service compiles the Core ML model to some device-specific format. Next runs are faster.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information about the Core ML implementation please refer to PR &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/pull/566&#34;&gt;#566&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenVINO support&lt;/h2&gt; &#xA;&lt;p&gt;On platforms that support &lt;a href=&#34;https://github.com/openvinotoolkit/openvino&#34;&gt;OpenVINO&lt;/a&gt;, the Encoder inference can be executed on OpenVINO-supported devices including x86 CPUs and Intel GPUs (integrated &amp;amp; discrete).&lt;/p&gt; &#xA;&lt;p&gt;This can result in significant speedup in encoder performance. Here are the instructions for generating the OpenVINO model and using it with &lt;code&gt;whisper.cpp&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;First, setup python virtual env. and install python dependencies. Python 3.10 is recommended.&lt;/p&gt; &lt;p&gt;Windows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd models&#xA;python -m venv openvino_conv_env&#xA;openvino_conv_env\Scripts\activate&#xA;python -m pip install --upgrade pip&#xA;pip install -r requirements-openvino.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd models&#xA;python3 -m venv openvino_conv_env&#xA;source openvino_conv_env/bin/activate&#xA;python -m pip install --upgrade pip&#xA;pip install -r requirements-openvino.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate an OpenVINO encoder model. For example, to generate a &lt;code&gt;base.en&lt;/code&gt; model, use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python convert-whisper-to-openvino.py --model base.en&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will produce ggml-base.en-encoder-openvino.xml/.bin IR model files. It&#39;s recommended to relocate these to the same folder as &lt;code&gt;ggml&lt;/code&gt; models, as that is the default location that the OpenVINO extension will search at runtime.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build &lt;code&gt;whisper.cpp&lt;/code&gt; with OpenVINO support:&lt;/p&gt; &lt;p&gt;Download OpenVINO package from &lt;a href=&#34;https://github.com/openvinotoolkit/openvino/releases&#34;&gt;release page&lt;/a&gt;. The recommended version to use is &lt;a href=&#34;https://github.com/openvinotoolkit/openvino/releases/tag/2023.0.0&#34;&gt;2023.0.0&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After downloading &amp;amp; extracting package onto your development system, set up required environment by sourcing setupvars script. For example:&lt;/p&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source /path/to/l_openvino_toolkit_ubuntu22_2023.0.0.10926.b4452d56304_x86_64/setupvars.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Windows (cmd):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;C:\Path\To\w_openvino_toolkit_windows_2023.0.0.10926.b4452d56304_x86_64\setupvars.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And then build the project using cmake:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -B build -DWHISPER_OPENVINO=1&#xA;cmake --build build -j --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the examples as usual. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ ./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/jfk.wav&#xA;&#xA;...&#xA;&#xA;whisper_ctx_init_openvino_encoder: loading OpenVINO model from &#39;models/ggml-base.en-encoder-openvino.xml&#39;&#xA;whisper_ctx_init_openvino_encoder: first run on a device may take a while ...&#xA;whisper_openvino_init: path_model = models/ggml-base.en-encoder-openvino.xml, device = GPU, cache_dir = models/ggml-base.en-encoder-openvino-cache&#xA;whisper_ctx_init_openvino_encoder: OpenVINO model loaded&#xA;&#xA;system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 1 |&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first time run on an OpenVINO device is slow, since the OpenVINO framework will compile the IR (Intermediate Representation) model to a device-specific &#39;blob&#39;. This device-specific blob will get cached for the next run.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information about the OpenVINO implementation please refer to PR &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/pull/1037&#34;&gt;#1037&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;NVIDIA GPU support&lt;/h2&gt; &#xA;&lt;p&gt;With NVIDIA cards the processing of the models is done efficiently on the GPU via cuBLAS and custom CUDA kernels. First, make sure you have installed &lt;code&gt;cuda&lt;/code&gt;: &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;https://developer.nvidia.com/cuda-downloads&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now build &lt;code&gt;whisper.cpp&lt;/code&gt; with CUDA support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_CUDA=1&#xA;cmake --build build -j --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Vulkan GPU support&lt;/h2&gt; &#xA;&lt;p&gt;Cross-vendor solution which allows you to accelerate workload on your GPU. First, make sure your graphics card driver provides support for Vulkan API.&lt;/p&gt; &#xA;&lt;p&gt;Now build &lt;code&gt;whisper.cpp&lt;/code&gt; with Vulkan support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_VULKAN=1&#xA;cmake --build build -j --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;BLAS CPU support via OpenBLAS&lt;/h2&gt; &#xA;&lt;p&gt;Encoder processing can be accelerated on the CPU via OpenBLAS. First, make sure you have installed &lt;code&gt;openblas&lt;/code&gt;: &lt;a href=&#34;https://www.openblas.net/&#34;&gt;https://www.openblas.net/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now build &lt;code&gt;whisper.cpp&lt;/code&gt; with OpenBLAS support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_BLAS=1&#xA;cmake --build build -j --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ascend NPU support&lt;/h2&gt; &#xA;&lt;p&gt;Ascend NPU provides inference acceleration via &lt;a href=&#34;https://www.hiascend.com/en/software/cann&#34;&gt;&lt;code&gt;CANN&lt;/code&gt;&lt;/a&gt; and AI cores.&lt;/p&gt; &#xA;&lt;p&gt;First, check if your Ascend NPU device is supported:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Verified devices&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Ascend NPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Atlas 300T A2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Then, make sure you have installed &lt;a href=&#34;https://www.hiascend.com/en/software/cann/community&#34;&gt;&lt;code&gt;CANN toolkit&lt;/code&gt;&lt;/a&gt; . The lasted version of CANN is recommanded.&lt;/p&gt; &#xA;&lt;p&gt;Now build &lt;code&gt;whisper.cpp&lt;/code&gt; with CANN support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_CANN=1&#xA;cmake --build build -j --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the inference examples as usual, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build/bin/whisper-cli -f samples/jfk.wav -m models/ggml-base.en.bin -t 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Notes:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have trouble with Ascend NPU device, please create a issue with &lt;strong&gt;[CANN]&lt;/strong&gt; prefix/tag.&lt;/li&gt; &#xA; &lt;li&gt;If you run successfully with your Ascend NPU device, please help update the table &lt;code&gt;Verified devices&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker must be installed and running on your system.&lt;/li&gt; &#xA; &lt;li&gt;Create a folder to store big models &amp;amp; intermediate files (ex. /whisper/models)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Images&lt;/h3&gt; &#xA;&lt;p&gt;We have two Docker images available for this project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;ghcr.io/ggml-org/whisper.cpp:main&lt;/code&gt;: This image includes the main executable file as well as &lt;code&gt;curl&lt;/code&gt; and &lt;code&gt;ffmpeg&lt;/code&gt;. (platforms: &lt;code&gt;linux/amd64&lt;/code&gt;, &lt;code&gt;linux/arm64&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ghcr.io/ggml-org/whisper.cpp:main-cuda&lt;/code&gt;: Same as &lt;code&gt;main&lt;/code&gt; but compiled with CUDA support. (platforms: &lt;code&gt;linux/amd64&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# download model and persist it in a local folder&#xA;docker run -it --rm \&#xA;  -v path/to/models:/models \&#xA;  whisper.cpp:main &#34;./models/download-ggml-model.sh base /models&#34;&#xA;# transcribe an audio file&#xA;docker run -it --rm \&#xA;  -v path/to/models:/models \&#xA;  -v path/to/audios:/audios \&#xA;  whisper.cpp:main &#34;./main -m /models/ggml-base.bin -f /audios/jfk.wav&#34;&#xA;# transcribe an audio file in samples folder&#xA;docker run -it --rm \&#xA;  -v path/to/models:/models \&#xA;  whisper.cpp:main &#34;./main -m /models/ggml-base.bin -f ./samples/jfk.wav&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installing with Conan&lt;/h2&gt; &#xA;&lt;p&gt;You can install pre-built binaries for whisper.cpp or build it from source using &lt;a href=&#34;https://conan.io/&#34;&gt;Conan&lt;/a&gt;. Use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conan install --requires=&#34;whisper-cpp/[*]&#34; --build=missing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed instructions on how to use Conan, please refer to the &lt;a href=&#34;https://docs.conan.io/2/&#34;&gt;Conan documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference only&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Real-time audio input example&lt;/h2&gt; &#xA;&lt;p&gt;This is a naive example of performing real-time inference on audio from your microphone. The &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/stream&#34;&gt;stream&lt;/a&gt; tool samples the audio every half a second and runs the transcription continuously. More info is available in &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/issues/10&#34;&gt;issue #10&lt;/a&gt;. You will need to have &lt;a href=&#34;https://wiki.libsdl.org/SDL2/Installation&#34;&gt;sdl2&lt;/a&gt; installed for it to work properly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake -B build -DWHISPER_SDL2=ON&#xA;cmake --build build --config Release&#xA;./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Confidence color-coding&lt;/h2&gt; &#xA;&lt;p&gt;Adding the &lt;code&gt;--print-colors&lt;/code&gt; argument will print the transcribed text using an experimental color coding strategy to highlight words with high or low confidence:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/gb0.wav --print-colors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;965&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png&#34;&gt; &#xA;&lt;h2&gt;Controlling the length of the generated text segments (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;For example, to limit the line length to a maximum of 16 characters, simply add &lt;code&gt;-ml 16&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ ./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16&#xA;&#xA;whisper_model_load: loading model from &#39;./models/ggml-base.en.bin&#39;&#xA;...&#xA;system_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |&#xA;&#xA;main: processing &#39;./samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00:00.000 --&amp;gt; 00:00:00.850]   And so my&#xA;[00:00:00.850 --&amp;gt; 00:00:01.590]   fellow&#xA;[00:00:01.590 --&amp;gt; 00:00:04.140]   Americans, ask&#xA;[00:00:04.140 --&amp;gt; 00:00:05.660]   not what your&#xA;[00:00:05.660 --&amp;gt; 00:00:06.840]   country can do&#xA;[00:00:06.840 --&amp;gt; 00:00:08.430]   for you, ask&#xA;[00:00:08.430 --&amp;gt; 00:00:09.440]   what you can do&#xA;[00:00:09.440 --&amp;gt; 00:00:10.020]   for your&#xA;[00:00:10.020 --&amp;gt; 00:00:11.000]   country.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Word-level timestamp (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;--max-len&lt;/code&gt; argument can be used to obtain word-level timestamps. Simply use &lt;code&gt;-ml 1&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ ./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1&#xA;&#xA;whisper_model_load: loading model from &#39;./models/ggml-base.en.bin&#39;&#xA;...&#xA;system_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |&#xA;&#xA;main: processing &#39;./samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00:00.000 --&amp;gt; 00:00:00.320]&#xA;[00:00:00.320 --&amp;gt; 00:00:00.370]   And&#xA;[00:00:00.370 --&amp;gt; 00:00:00.690]   so&#xA;[00:00:00.690 --&amp;gt; 00:00:00.850]   my&#xA;[00:00:00.850 --&amp;gt; 00:00:01.590]   fellow&#xA;[00:00:01.590 --&amp;gt; 00:00:02.850]   Americans&#xA;[00:00:02.850 --&amp;gt; 00:00:03.300]  ,&#xA;[00:00:03.300 --&amp;gt; 00:00:04.140]   ask&#xA;[00:00:04.140 --&amp;gt; 00:00:04.990]   not&#xA;[00:00:04.990 --&amp;gt; 00:00:05.410]   what&#xA;[00:00:05.410 --&amp;gt; 00:00:05.660]   your&#xA;[00:00:05.660 --&amp;gt; 00:00:06.260]   country&#xA;[00:00:06.260 --&amp;gt; 00:00:06.600]   can&#xA;[00:00:06.600 --&amp;gt; 00:00:06.840]   do&#xA;[00:00:06.840 --&amp;gt; 00:00:07.010]   for&#xA;[00:00:07.010 --&amp;gt; 00:00:08.170]   you&#xA;[00:00:08.170 --&amp;gt; 00:00:08.190]  ,&#xA;[00:00:08.190 --&amp;gt; 00:00:08.430]   ask&#xA;[00:00:08.430 --&amp;gt; 00:00:08.910]   what&#xA;[00:00:08.910 --&amp;gt; 00:00:09.040]   you&#xA;[00:00:09.040 --&amp;gt; 00:00:09.320]   can&#xA;[00:00:09.320 --&amp;gt; 00:00:09.440]   do&#xA;[00:00:09.440 --&amp;gt; 00:00:09.760]   for&#xA;[00:00:09.760 --&amp;gt; 00:00:10.020]   your&#xA;[00:00:10.020 --&amp;gt; 00:00:10.510]   country&#xA;[00:00:10.510 --&amp;gt; 00:00:11.000]  .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speaker segmentation via tinydiarize (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;More information about this approach is available here: &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/pull/1058&#34;&gt;https://github.com/ggml-org/whisper.cpp/pull/1058&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sample usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# download a tinydiarize compatible model&#xA;./models/download-ggml-model.sh small.en-tdrz&#xA;&#xA;# run as usual, adding the &#34;-tdrz&#34; command-line argument&#xA;./build/bin/whisper-cli -f ./samples/a13.wav -m ./models/ggml-small.en-tdrz.bin -tdrz&#xA;...&#xA;main: processing &#39;./samples/a13.wav&#39; (480000 samples, 30.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, tdrz = 1, timestamps = 1 ...&#xA;...&#xA;[00:00:00.000 --&amp;gt; 00:00:03.800]   Okay Houston, we&#39;ve had a problem here. [SPEAKER_TURN]&#xA;[00:00:03.800 --&amp;gt; 00:00:06.200]   This is Houston. Say again please. [SPEAKER_TURN]&#xA;[00:00:06.200 --&amp;gt; 00:00:08.260]   Uh Houston we&#39;ve had a problem.&#xA;[00:00:08.260 --&amp;gt; 00:00:11.320]   We&#39;ve had a main beam up on a volt. [SPEAKER_TURN]&#xA;[00:00:11.320 --&amp;gt; 00:00:13.820]   Roger main beam interval. [SPEAKER_TURN]&#xA;[00:00:13.820 --&amp;gt; 00:00:15.100]   Uh uh [SPEAKER_TURN]&#xA;[00:00:15.100 --&amp;gt; 00:00:18.020]   So okay stand, by thirteen we&#39;re looking at it. [SPEAKER_TURN]&#xA;[00:00:18.020 --&amp;gt; 00:00:25.740]   Okay uh right now uh Houston the uh voltage is uh is looking good um.&#xA;[00:00:27.620 --&amp;gt; 00:00:29.940]   And we had a a pretty large bank or so.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Karaoke-style movie generation (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/cli&#34;&gt;whisper-cli&lt;/a&gt; example provides support for output of karaoke-style movies, where the currently pronounced word is highlighted. Use the &lt;code&gt;-wts&lt;/code&gt; argument and run the generated bash script. This requires to have &lt;code&gt;ffmpeg&lt;/code&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;Here are a few &lt;em&gt;&#34;typical&#34;&lt;/em&gt; examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts&#xA;source ./samples/jfk.wav.wts&#xA;ffplay ./samples/jfk.wav.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts&#xA;source ./samples/mm0.wav.wts&#xA;ffplay ./samples/mm0.wav.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts&#xA;source ./samples/gb0.wav.wts&#xA;ffplay ./samples/gb0.wav.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Video comparison of different models&lt;/h2&gt; &#xA;&lt;p&gt;Use the &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/raw/master/scripts/bench-wts.sh&#34;&gt;scripts/bench-wts.sh&lt;/a&gt; script to generate a video in the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/bench-wts.sh samples/jfk.wav&#xA;ffplay ./samples/jfk.wav.all.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;In order to have an objective comparison of the performance of the inference across different system configurations, use the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/bench&#34;&gt;whisper-bench&lt;/a&gt; tool. The tool simply runs the Encoder part of the model and prints how much time it took to execute it. The results are summarized in the following Github issue:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/issues/89&#34;&gt;Benchmark results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Additionally a script to run whisper.cpp with different models and audio files is provided &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/scripts/bench.py&#34;&gt;bench.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can run it with the following command, by default it will run against any standard model in the models folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 scripts/bench.py -f samples/jfk.wav -t 2,4,8 -p 1,2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is written in python with the intention of being easy to modify and extend for your benchmarking use case.&lt;/p&gt; &#xA;&lt;p&gt;It outputs a csv file with the results of the benchmarking.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;ggml&lt;/code&gt; format&lt;/h2&gt; &#xA;&lt;p&gt;The original models are converted to a custom binary format. This allows to pack everything needed into a single file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model parameters&lt;/li&gt; &#xA; &lt;li&gt;mel filters&lt;/li&gt; &#xA; &lt;li&gt;vocabulary&lt;/li&gt; &#xA; &lt;li&gt;weights&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can download the converted models using the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/models/download-ggml-model.sh&#34;&gt;models/download-ggml-model.sh&lt;/a&gt; script or manually from here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ggerganov/whisper.cpp&#34;&gt;https://huggingface.co/ggerganov/whisper.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see the conversion script &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/models/convert-pt-to-ggml.py&#34;&gt;models/convert-pt-to-ggml.py&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/models/README.md&#34;&gt;models/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/categories/bindings&#34;&gt;Bindings&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Rust: &lt;a href=&#34;https://github.com/tazz4843/whisper-rs&#34;&gt;tazz4843/whisper-rs&lt;/a&gt; | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/310&#34;&gt;#310&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; JavaScript: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/bindings/javascript&#34;&gt;bindings/javascript&lt;/a&gt; | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/309&#34;&gt;#309&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;React Native (iOS / Android): &lt;a href=&#34;https://github.com/mybigday/whisper.rn&#34;&gt;whisper.rn&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Go: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/bindings/go&#34;&gt;bindings/go&lt;/a&gt; | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/312&#34;&gt;#312&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Java: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/GiviMAD/whisper-jni&#34;&gt;GiviMAD/whisper-jni&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Ruby: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/bindings/ruby&#34;&gt;bindings/ruby&lt;/a&gt; | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/507&#34;&gt;#507&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Objective-C / Swift: &lt;a href=&#34;https://github.com/ggml-org/whisper.spm&#34;&gt;ggml-org/whisper.spm&lt;/a&gt; | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/313&#34;&gt;#313&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/exPHAT/SwiftWhisper&#34;&gt;exPHAT/SwiftWhisper&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .NET: | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/422&#34;&gt;#422&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/sandrohanea/whisper.net&#34;&gt;sandrohanea/whisper.net&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NickDarvey/whisper&#34;&gt;NickDarvey/whisper&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Python: | &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/issues/9&#34;&gt;#9&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/stlukey/whispercpp.py&#34;&gt;stlukey/whispercpp.py&lt;/a&gt; (Cython)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/AIWintermuteAI/whispercpp&#34;&gt;AIWintermuteAI/whispercpp&lt;/a&gt; (Updated fork of aarnphm/whispercpp)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/aarnphm/whispercpp&#34;&gt;aarnphm/whispercpp&lt;/a&gt; (Pybind11)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/abdeladim-s/pywhispercpp&#34;&gt;abdeladim-s/pywhispercpp&lt;/a&gt; (Pybind11)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; R: &lt;a href=&#34;https://github.com/bnosac/audio.whisper&#34;&gt;bnosac/audio.whisper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Unity: &lt;a href=&#34;https://github.com/Macoron/whisper.unity&#34;&gt;macoron/whisper.unity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;XCFramework&lt;/h2&gt; &#xA;&lt;p&gt;The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;// swift-tools-version: 5.10&#xA;// The swift-tools-version declares the minimum version of Swift required to build this package.&#xA;&#xA;import PackageDescription&#xA;&#xA;let package = Package(&#xA;    name: &#34;Whisper&#34;,&#xA;    targets: [&#xA;        .executableTarget(&#xA;            name: &#34;Whisper&#34;,&#xA;            dependencies: [&#xA;                &#34;WhisperFramework&#34;&#xA;            ]),&#xA;        .binaryTarget(&#xA;            name: &#34;WhisperFramework&#34;,&#xA;            url: &#34;https://github.com/ggml-org/whisper.cpp/releases/download/v1.7.5/whisper-v1.7.5-xcframework.zip&#34;,&#xA;            checksum: &#34;c7faeb328620d6012e130f3d705c51a6ea6c995605f2df50f6e1ad68c59c6c4a&#34;&#xA;        )&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;There are various examples of using the library for different projects in the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples&#34;&gt;examples&lt;/a&gt; folder. Some of the examples are even ported to run in the browser using WebAssembly. Check them out!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;   &lt;th&gt;Web&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/cli&#34;&gt;whisper-cli&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.wasm&#34;&gt;whisper.wasm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tool for translating and transcribing audio using Whisper&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/bench&#34;&gt;whisper-bench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/bench.wasm&#34;&gt;bench.wasm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Benchmark the performance of Whisper on your machine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/stream&#34;&gt;whisper-stream&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/stream.wasm&#34;&gt;stream.wasm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Real-time transcription of raw microphone capture&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/command&#34;&gt;whisper-command&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/command.wasm&#34;&gt;command.wasm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Basic voice assistant example for receiving voice commands from the mic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/server&#34;&gt;whisper-server&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HTTP transcription server with OAI-like API&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/talk-llama&#34;&gt;whisper-talk-llama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Talk with a LLaMA bot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.objc&#34;&gt;whisper.objc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;iOS mobile application using whisper.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.swiftui&#34;&gt;whisper.swiftui&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SwiftUI iOS / macOS application using whisper.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.android&#34;&gt;whisper.android&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Android mobile application using whisper.cpp&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/whisper.nvim&#34;&gt;whisper.nvim&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Speech-to-text plugin for Neovim&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/generate-karaoke.sh&#34;&gt;generate-karaoke.sh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Helper script to easily &lt;a href=&#34;https://youtu.be/uj7hVta4blM&#34;&gt;generate a karaoke video&lt;/a&gt; of raw audio capture&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/livestream.sh&#34;&gt;livestream.sh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/issues/185&#34;&gt;Livestream audio transcription&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/yt-wsp.sh&#34;&gt;yt-wsp.sh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Download + transcribe and/or translate any VOD &lt;a href=&#34;https://gist.github.com/DaniruKun/96f763ec1a037cc92fe1a059b643b818&#34;&gt;(original)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/wchess&#34;&gt;wchess&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/whisper.cpp/master/examples/wchess&#34;&gt;wchess.wasm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Voice-controlled chess&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you have any kind of feedback about this project feel free to use the Discussions section and open a new topic. You can use the &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/categories/show-and-tell&#34;&gt;Show and tell&lt;/a&gt; category to share your own projects that use &lt;code&gt;whisper.cpp&lt;/code&gt;. If you have a question, make sure to check the &lt;a href=&#34;https://github.com/ggml-org/whisper.cpp/discussions/126&#34;&gt;Frequently asked questions (#126)&lt;/a&gt; discussion.&lt;/p&gt;</summary>
  </entry>
</feed>