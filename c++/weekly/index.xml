<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-25T01:38:49Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/T-MAC</title>
    <updated>2024-08-25T01:38:49Z</updated>
    <id>tag:github.com,2024-08-25:/microsoft/T-MAC</id>
    <link href="https://github.com/microsoft/T-MAC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Low-bit LLM inference on CPU with lookup table&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T-MAC&lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/assets/demo.gif&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&#34;&gt;BitNet&lt;/a&gt; on T-MAC (LUT-based) vs llama.cpp (dequantization-based)&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;08/21/2024 ðŸŽ‰ðŸŽ‰: T-MAC paper is accepted by EuroSys 2025.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;08/17/2024 ðŸš€: T-MAC now supports 1/2/4-bit quantized models of (almost) any architecture in GPTQ format.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;08/14/2024 ðŸš€: The T-MAC GEMM (N&amp;gt;1) kernels are now integrated into llama.cpp to accelerate prefill. Check &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/#prefill-speedup&#34;&gt;Prefill speedup&lt;/a&gt; for speedup.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;07/27/2024 âœ¨: We&#39;ve noted that T-MAC is even faster than the NPU in token generation speed on the latest Snapdragon X Elite chipset! Check &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/#compared-to-npu&#34;&gt;Compared to NPU&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;07/23/2024 ðŸš€ðŸš€: We&#39;ve enabled the execution of any 2-bit quantized Llama model in GPTQ format via T-MAC! Test it using the pretrained models released by &lt;a href=&#34;https://github.com/OpenGVLab/EfficientQAT&#34;&gt;EfficientQAT&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;07/22/2024 ðŸš€ðŸš€: We&#39;ve added native deployment support for Windows on ARM. T-MAC demonstrates a substantial 5x speedup on the Surface Laptop 7.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;T-MAC is a kernel library to directly support mixed-precision matrix multiplication (int1/2/3/4 x int8/fp16/fp32) without the need for dequantization by utilizing lookup tables. T-MAC aims to boost low-bit LLM inference on CPUs. T-MAC already offers support for various low-bit models, including W4A16 from GPTQ/gguf, W2A16 from &lt;a href=&#34;https://github.com/DD-DuDa/BitDistiller&#34;&gt;BitDistiller&lt;/a&gt;/&lt;a href=&#34;https://github.com/OpenGVLab/EfficientQAT&#34;&gt;EfficientQAT&lt;/a&gt; and W1(.58)A8 from &lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&#34;&gt;BitNet&lt;/a&gt; on OSX/Linux/Windows equipped with ARM/Intel CPUs.&lt;/p&gt; &#xA;&lt;p&gt;T-MAC achieves a token generation throughput of 20 tokens/sec with a single core and 48 tokens/sec with four cores on Surface Laptop 7 for 3B BitNet, which is a 4~5x speedup compared to SOTA CPU low-bit framework (&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;). T-MAC can even reach 11 tokens/sec on lower-end devices like Raspberry Pi 5.&lt;/p&gt; &#xA;&lt;h2&gt;End-2-End Speedup&lt;/h2&gt; &#xA;&lt;p&gt;We evaluate the token generation performance of different models on five different devices: Surface Laptop 7, Apple M2-Ultra, Jetson AGX Orin, Raspberry Pi 5 and Surface Book 3. Check &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/docs/profiling_data.md&#34;&gt;datasheet&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We evaluate BitNet-3B and Llama-2-7B (W2) with T-MAC 2-bit and llama.cpp Q2_K, and evaluate Llama-2-7B (W4) with T-MAC 4-bit and llama.cpp Q4_0.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In addition to providing a significant speedup, T-MAC can also match the same performance using fewer CPU cores. For instance, to reach 40 tokens/sec, a throughput that greatly surpasses human reading speed, T-MAC only requires 2 cores, while llama.cpp requires 8 cores. On Jetson AGX Orin, to achieve 10 tokens/sec, a throughput that already meets human reading speed, T-MAC only requires 2 cores, while llama.cpp uses all 12 cores. T-MAC can meet real-time requirements on less powerful devices equipped with fewer CPU cores like Raspberry Pi 5. By using fewer cores, T-MAC can reserve computational resources for other applications and significantly reduce power and energy consumption, both of which are crucial for edge devices.&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/assets/e2e_threads.png&#34;&gt; &lt;p&gt;T-MAC achieves significant speedup at single-threads and consumes much less CPU cores to reach the same throughput&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The throughputs of T-MAC are obtained without fast-aggregation. Users can toggle on fast-aggregation through &lt;code&gt;-fa&lt;/code&gt; to achieve an additional speedup of 10%~20% with.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The figure above shows that when the model size is increased to 7B-4bit, the multi-threading throughput of llama.cpp on Surface Laptop 7 becomes highly unstable due to the thermal threshold under &lt;em&gt;Better Performance&lt;/em&gt; mode. This instability is not observed with T-MAC, as LUT is more energy-efficient compared to multiply-add operations. To establish a more solid baseline, we re-profile the performance under the &lt;em&gt;Best Performance&lt;/em&gt; mode:&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/assets/e2e_threads_surface_max.png&#34;&gt; &lt;p&gt;The throughput of T-MAC and llama.cpp both increase by maximizing CPU frequency&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;However, under real-world situations, CPUs can&#39;t maintain maximum frequency consistently on edge devices. The performance of llama.cpp will degrade as indicated by the results under the &lt;em&gt;Better Performance&lt;/em&gt; mode.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Prefill Speedup&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;TODO: add more results&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We have compared the prefill throughput (input_len=256) for Llama-2-7b (W2) on Surface Laptop 7 with two baselines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;llama.cpp: llama.cpp optimized dequant-based low-bit kernels&lt;/li&gt; &#xA; &lt;li&gt;llama.cpp (OpenBLAS): llama.cpp OpenBLAS backend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;NUM_THREADS&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;T-MAC (tokens/sec)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;llama.cpp (OpenBLAS)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;llama.cpp&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-7b (W2)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;50.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama-2-7b (W2)&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;94.4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;37.7&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;21.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Kernel-level Speedup&lt;/h2&gt; &#xA;&lt;p&gt;Our GEMM kernels demonstrate superior performance over SOTA low-bit GEMM on CPU. The following figure shows the speedup compared to llama.cpp for llama-7b kernels during token generation (NUM_THREADS=1):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/assets/gemv_t1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;llama.cpp doesn&#39;t provide 1-bit kernel implementation, but we can deduce it from the 2-bit, as it won&#39;t bring additional speedup according to the 2/3/4-bit results.&lt;/p&gt; &#xA; &lt;p&gt;Surface stands for Surface Book 3 in this section.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;T-MAC can achieve significant speedup for multi-batch (N&amp;gt;1) GEMM due to reduced computaional cost, which ensures superior performance on prompt evaluation and multi-batch token generation. The following figures shows the speedup compared to llama.cpp using OpenBLAS backend (NUM_THREADS=1):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/assets/gemm.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;M2-Ultra is an exception as it is equipped with a specially designed &lt;a href=&#34;https://github.com/corsix/amx&#34;&gt;AMX coprocessor&lt;/a&gt; to accelerate multi-batch GEMM. However, T-MAC can still achieve comparable performance at 2-bit.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Energy and Power Saving&lt;/h2&gt; &#xA;&lt;p&gt;By replacing heavy fused-multiply-add instructions with table lookup instructions, T-MAC significantly reduces power consumption. Combined with the speedup, T-MAC ultimately results in a substantial decrease in total energy consumption.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/assets/e2e_power.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;Multi-threading power/energy consumption on M2-Ultra for three models, M1: Llama-2-7B (W4), M2: Llama-2-7B (W2) and M3: BitNet-3B&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Data sampled with &lt;a href=&#34;https://www.unix.com/man-page/osx/1/powermetrics/&#34;&gt;powermetrics&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Compared to NPU&lt;/h3&gt; &#xA;&lt;p&gt;On the latest Snapdragon X Elite chipset, CPU through T-MAC achieves better performance compared to NPU through Qualcomm Snapdragon Neural Processing Engine (NPE).&lt;/p&gt; &#xA;&lt;p&gt;When deploying the llama-2-7b-4bit model on it, the NPU can only generate 10.4 tokens/sec (according to the data released &lt;a href=&#34;https://aihub.qualcomm.com/models/llama_v2_7b_chat_quantized&#34;&gt;here&lt;/a&gt;), while the CPU using T-MAC can reach 12.6 tokens/sec with two cores, and even up to 22 tokens/sec. Considering that T-MAC&#39;s computing performance can linearly improve with the number of bits decreases (which is not observable on GPUs and NPUs based on dequantization), T-MAC can even match the NPU with a single-core CPU at 2 bits.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;NUM_THREADS&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Throughput (tokens/sec)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T-MAC (CPU)&lt;/td&gt; &#xA;   &lt;td&gt;llama-2-7b (W4)&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;12.6&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T-MAC (CPU)&lt;/td&gt; &#xA;   &lt;td&gt;llama-2-7b (W4)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;18.7&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T-MAC (CPU)&lt;/td&gt; &#xA;   &lt;td&gt;llama-2-7b (W2)&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;9.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T-MAC (CPU)&lt;/td&gt; &#xA;   &lt;td&gt;llama-2-7b (W2)&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;28.4&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NPE (NPU)&lt;/td&gt; &#xA;   &lt;td&gt;llama-2-7b (W4)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For fair comparison, we have aligned our settings with those of the NPU, including a input length of 1024 and an output length of 1024. Although Qualcomms deploy a model of 3.6GB, we deploy a slightly larger model of 3.7GB, due to our token-embed remaining un-quantized.&lt;/p&gt; &#xA; &lt;p&gt;By maximizing CPU frequency, T-MAC (CPU) can even get better results. Refer to the discussion in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/T-MAC/main/#end-2-end-speedup&#34;&gt;End-2-End speedup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Compared to CUDA GPU&lt;/h3&gt; &#xA;&lt;p&gt;T-MAC achieves comparable 2-bit mpGEMM performance compared to CUDA GPU on Jetson AGX Orin. While the CUDA GPU outperforms the CPU in executing kernels other than mpGEMM, making the end-to-end performance of T-MAC (CPU) slightly slower, T-MAC can deliver considerable savings in power and energy consumption.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Throughput (tokens/sec)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Power (W)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Energy (J/token)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama.cpp (CPU)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;7.08&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama.cpp (GPU)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;20.03&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T-MAC (CPU)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;15.62&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;10.4&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;0.66&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Throughput/power/energy comparison for Llama-2-7B (W2) on NVIDIA Jetson AGX Orin (NUM_THREADS=12 for CPU)&lt;/b&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Data sampled with &lt;a href=&#34;https://github.com/rbonghi/jetson_stats&#34;&gt;jetson-stats&lt;/a&gt; under power mode MAXN.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python (3.8 required for TVM)&lt;/li&gt; &#xA; &lt;li&gt;virtualenv&lt;/li&gt; &#xA; &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;OSX (Apple Silicon)&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;First, install &lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;zstd&lt;/code&gt; (dependency of llvm) and &lt;code&gt;libomp&lt;/code&gt; (dependency of tvm). Homebrew is recommended:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install cmake zlib libomp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;If &lt;code&gt;zstd&lt;/code&gt; is installed through homebrew, than &lt;code&gt;cmake&lt;/code&gt; should also be installed through homebrew to ensure that &lt;code&gt;zstd&lt;/code&gt; can be found by &lt;code&gt;cmake&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;Install &lt;code&gt;t_mac&lt;/code&gt; from the source (please run in a &lt;code&gt;virtualenv&lt;/code&gt;):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/microsoft/T-MAC.git&#xA;# in virtualenv&#xA;pip install . -v  # or pip install -e . -v&#xA;source build/t-mac-envs.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The command will download clang+llvm and build tvm from source. So it might take a bit of time.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Ubuntu (aarch64/x86_64)&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Install cmake&amp;gt;=3.22 from &lt;a href=&#34;https://cmake.org/download/&#34;&gt;Official Page&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Then install TVM build dependencies:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install build-essential libtinfo-dev zlib1g-dev libzstd-dev libxml2-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Install &lt;code&gt;t_mac&lt;/code&gt; from the source (please run in a &lt;code&gt;virtualenv&lt;/code&gt;):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/microsoft/T-MAC.git&#xA;# in virtualenv&#xA;pip install . -v  # or pip install -e . -v&#xA;source build/t-mac-envs.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The command will download clang+llvm and build tvm from source. So it might take a bit of time.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Windows (x86_64)&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Due to lack of stable clang+llvm prebuilt on Windows, Conda + Visual Studio is recommended to install dependencies.&lt;/p&gt; &#xA; &lt;p&gt;First, install Visual Studio 2019 and toggle on &lt;code&gt;Desk development with C++&lt;/code&gt; and &lt;code&gt;C++ Clang tools for Windows&lt;/code&gt;. Then, create conda environment within &lt;code&gt;Developer PowerShell for VS 2019&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;git clone --recursive https://github.com/microsoft/T-MAC.git&#xA;cd T-MAC&#xA;conda env create --file conda\tvm-build-environment.yaml&#xA;conda activate tvm-build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;If you are using Visual Studio 2022, replace &lt;code&gt;llvmdev =14.0.6&lt;/code&gt; with &lt;code&gt;llvmdev =17.0.6&lt;/code&gt; in the yaml file.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;After that, build TVM with:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd 3rdparty\tvm&#xA;mkdir build&#xA;cp cmake\config.cmake build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Append &lt;code&gt;set(USE_LLVM llvm-config)&lt;/code&gt; to &lt;code&gt;build\config.cmake&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd build&#xA;cmake .. -A x64&#xA;cmake --build . --config Release -- /m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Install &lt;code&gt;t_mac&lt;/code&gt; from the source:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd ..\..\..\  # back to project root directory&#xA;$env:MANUAL_BUILD = &#34;1&#34;&#xA;$env:PYTHONPATH = &#34;$pwd\3rdparty\tvm\python&#34;&#xA;pip install . -v  # or pip install -e . -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Windows (ARM64)&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;The following process could be more complicated. However, if your deployment scenerio doesn&#39;t require a native build, you can use WSL/docker and follow the Ubuntu guide.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;First, install Visual Studio 2022(/2019) and toggle on &lt;code&gt;Desk development with C++&lt;/code&gt;. Then, create conda environment within &lt;code&gt;Developer PowerShell for VS 20XX&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;git clone --recursive https://github.com/microsoft/T-MAC.git&#xA;cd T-MAC&#xA;conda env create --file conda\tvm-build-environment.yaml&#xA;conda activate tvm-build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Remember to replace &lt;code&gt;llvmdev =14.0.6&lt;/code&gt; with &lt;code&gt;llvmdev =17.0.6&lt;/code&gt; in the yaml file if you are using Visual Studio 2022 (which is recommended on ARM64 for better performance).&lt;/p&gt; &#xA; &lt;p&gt;After that, build TVM with:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd 3rdparty\tvm&#xA;mkdir build&#xA;cp cmake\config.cmake build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Append &lt;code&gt;set(USE_LLVM llvm-config)&lt;/code&gt; to &lt;code&gt;build\config.cmake&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd build&#xA;cmake .. -A x64  # Build TVM in x64, as Python and dependencies are x64&#xA;cmake --build . --config Release -- /m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;If you encounter errors like &lt;code&gt;string sub-command regex, mode replace: regex &#34;$&#34; matched an empty string.&lt;/code&gt; during running &lt;code&gt;cmake .. -A x64&lt;/code&gt; while building TVM, don&#39;t worry, and just run &lt;code&gt;cmake .. -A x64&lt;/code&gt; again. Check &lt;a href=&#34;https://github.com/llvm/llvm-project/issues/83802&#34;&gt;this issue of LLVM&lt;/a&gt; for more details.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;As clang tools in Visual Studio are in fact emulated x64 tools, please install the native arm64 tools manually.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Install CMake from &lt;a href=&#34;https://github.com/Kitware/CMake/releases/download/v3.30.1/cmake-3.30.1-windows-arm64.msi&#34;&gt;Offical Windows ARM installer&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Download Ninja from &lt;a href=&#34;https://github.com/ninja-build/ninja/releases/download/v1.12.1/ninja-winarm64.zip&#34;&gt;Release Page&lt;/a&gt; and add to Path.&lt;/li&gt; &#xA;  &lt;li&gt;Install Clang from &lt;a href=&#34;https://github.com/llvm/llvm-project/releases/download/llvmorg-17.0.6/LLVM-17.0.6-woa64.exe&#34;&gt;Release Page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Run the following commands &lt;strong&gt;outside of Developer Command Prompt/Powershell for VS&lt;/strong&gt; to ensure our native clang tools are used.&lt;/p&gt; &#xA; &lt;p&gt;Install &lt;code&gt;t_mac&lt;/code&gt; from the source:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;conda activate tvm-build&#xA;conda uninstall cmake  # To prevent potential conflict with the native ARM64 cmake&#xA;cd ..\..\..\  # back to project root directory&#xA;$env:MANUAL_BUILD = &#34;1&#34;&#xA;$env:PYTHONPATH = &#34;$pwd\3rdparty\tvm\python&#34;&#xA;pip install wmi  # To detect the native ARM64 CPU within x86_64 python&#xA;pip install . -v  # or pip install -e . -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Verification&lt;/h3&gt; &#xA;&lt;p&gt;After that, you can verify the installation through: &lt;code&gt;python -c &#34;import t_mac; print(t_mac.__version__); from tvm.contrib.clang import find_clang; print(find_clang())&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Currently, we supports end-to-end inference through llama.cpp integration.&lt;/p&gt; &#xA;&lt;p&gt;We have provided an &lt;strong&gt;all-in-one script&lt;/strong&gt;. Invoke it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install 3rdparty/llama.cpp/gguf-py&#xA;huggingface-cli download 1bitLLM/bitnet_b1_58-3B --local-dir ${model_dir}&#xA;python tools/run_pipeline.py -o ${model_dir}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have also supported models in GTPQ format from &lt;a href=&#34;https://github.com/ModelCloud/GPTQModel&#34;&gt;GPTQModel&lt;/a&gt;/&lt;a href=&#34;https://github.com/OpenGVLab/EfficientQAT&#34;&gt;EfficientQAT&lt;/a&gt;. Try it out with officially released EfficientQAT (of GPTQ format) &lt;a href=&#34;https://huggingface.co/ChenMnZ/Llama-3-8b-instruct-EfficientQAT-w2g128-GPTQ&#34;&gt;Llama-3-8b-instruct-w2-g128&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli download ChenMnZ/Llama-3-8b-instruct-EfficientQAT-w2g128-GPTQ --local-dir ${model_dir}&#xA;python tools/run_pipeline.py -o ${model_dir} -m llama-3-8b-2bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Use &lt;code&gt;-p&lt;/code&gt; or &lt;code&gt;-s&lt;/code&gt; argument to select the steps you want to run.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Use &lt;code&gt;-u&lt;/code&gt; argument to use our prebuilt kernels for ARM.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Use &lt;code&gt;-m gptq-auto&lt;/code&gt; for GPTQ models not in preset. The kernel shapes and quantization configurations will be automatically detected and validated.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;We have supported mainstream LLM models in GPTQ format (e.g., Llama-2, Llama-3, Mistral, Phi-3-mini, etc). Some models are unsupported by &lt;a href=&#34;https://github.com/kaleid-liner/llama.cpp/raw/185d96ce5087b117d6b3a48bc99f158e9daec58d/convert-hf-to-gguf-t-mac.py&#34;&gt;convert script&lt;/a&gt;. We welcome contributions from community.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;An example output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running STEP.0: Compile kernels&#xA;  Running command in /Users/user/jianyu/T-MAC/deploy:&#xA;    python compile.py -o tuned -da -nt 4 -tb -gc -gs 128 -ags 64 -t -m hf-bitnet-3b -r&#xA;Running STEP.1: Build T-MAC C++ CMakeFiles&#xA;  Running command in /Users/user/jianyu/T-MAC/build:&#xA;    cmake -DCMAKE_INSTALL_PREFIX=/Users/user/jianyu/T-MAC/install ..&#xA;Running STEP.2: Install T-MAC C++&#xA;  Running command in /Users/user/jianyu/T-MAC/build:&#xA;    cmake --build . --target install --config Release&#xA;Running STEP.3: Convert HF to GGUF&#xA;  Running command in /Users/user/jianyu/T-MAC/3rdparty/llama.cpp:&#xA;    python convert-hf-to-gguf-t-mac.py /Users/user/Downloads/test_models/hf-bitnet-3B --outtype i2 --outfile /Users/user/Downloads/test_models/hf-bitnet-3B/ggml-model.i2.gguf --kcfg /Users/user/jianyu/T-MAC/install/lib/kcfg.ini&#xA;Running STEP.4: Build llama.cpp CMakeFiles&#xA;  Running command in /Users/user/jianyu/T-MAC/3rdparty/llama.cpp/build:&#xA;    cmake .. -DLLAMA_TMAC=ON -DCMAKE_PREFIX_PATH=/Users/user/jianyu/T-MAC/install/lib/cmake/t-mac -DCMAKE_BUILD_TYPE=Release -DLLAMA_LLAMAFILE_DEFAULT=OFF -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++&#xA;Running STEP.5: Build llama.cpp&#xA;  Running command in /Users/user/jianyu/T-MAC/3rdparty/llama.cpp/build:&#xA;    cmake --build . --target main --config Release&#xA;Running STEP.6: Run inference&#xA;  Running command in /Users/user/jianyu/T-MAC/3rdparty/llama.cpp/build:&#xA;    /Users/user/jianyu/T-MAC/3rdparty/llama.cpp/build/bin/main -m /Users/user/Downloads/test_models/hf-bitnet-3B/ggml-model.i2.gguf -n 128 -t 4 -p Microsoft Corporation is an American multinational corporation and technology company headquartered in Redmond, Washington. -b 1 -ngl 0 -c 2048&#xA;Check logs/2024-07-15-17-10-11.log for inference output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Upcoming Features&lt;/h2&gt; &#xA;&lt;p&gt;We will soon:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add &lt;code&gt;I4&lt;/code&gt; format to simplify the deployment of 4-bit models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Embed T-MAC GEMM kernels into llama.cpp to accelerate prefill/prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Android cross-compilation guidance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimize for ARMv9 CPU with SME2 through LUTI4&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Techniques&lt;/h2&gt; &#xA;&lt;p&gt;LLM inference incurs significant computational cost. Low-bit quantization, a widely adopted technique, introduces the challenge of mixed-precision GEMM (mpGEMM), which is not directly supported by hardware and requires convert/dequant operations.&lt;/p&gt; &#xA;&lt;p&gt;We propose the use of a lookup table (LUT) to support mpGEMM. Our method involves the following key technniques:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Given the low precision of weights, we group one-bit weights (e.g., into groups of 4), precompute all possible partial sums, and then use a LUT to store them.&lt;/li&gt; &#xA; &lt;li&gt;We employ shift and accumulate operations to support scalable bits from 1 to 4.&lt;/li&gt; &#xA; &lt;li&gt;On a CPU, we utilize tbl/pshuf instructions for fast table lookup.&lt;/li&gt; &#xA; &lt;li&gt;We reduce the table size from $2^n$ to $2^{n-1}$, incorporating a sign bit to accelerate LUT precomputation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Our method exhibits several notable characteristics:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;T-MAC shows a linear scaling ratio of FLOPs and inference latency relative to the number of bits. This contrasts with traditional convert-based methods, which fail to achieve additional speedup when reducing from 4 bits to lower bits.&lt;/li&gt; &#xA; &lt;li&gt;T-MAC inherently supports bit-wise computation for int1/2/3/4, eliminating the need for dequantization. Furthermore, it accommodates all types of activations (e.g., fp8, fp16, int8) using fast table lookup and add instructions, bypassing the need for poorly supported fused-multiply-add instructions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please use the following BibTeX entry for citation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wei2024tmaccpurenaissancetable,&#xA;      title={T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge}, &#xA;      author={Jianyu Wei and Shijie Cao and Ting Cao and Lingxiao Ma and Lei Wang and Yanyong Zhang and Mao Yang},&#xA;      year={2024},&#xA;      eprint={2407.00088},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.DC},&#xA;      url={https://arxiv.org/abs/2407.00088}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>duckdb/pg_duckdb</title>
    <updated>2024-08-25T01:38:49Z</updated>
    <id>tag:github.com,2024-08-25:/duckdb/pg_duckdb</id>
    <link href="https://github.com/duckdb/pg_duckdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DuckDB-powered Postgres for high performance apps &amp; analytics.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pg_duckdb: Official Postgres extension for DuckDB&lt;/h1&gt; &#xA;&lt;p&gt;pg_duckdb is a Postgres extension that embeds DuckDB&#39;s columnar-vectorized analytics engine and features into Postgres. We recommend using pg_duckdb to build high performance analytics and data-intensive applications.&lt;/p&gt; &#xA;&lt;p&gt;pg_duckdb was developed in collaboration with our partners, &lt;a href=&#34;https://hydra.so&#34;&gt;Hydra&lt;/a&gt; and &lt;a href=&#34;https://motherduck.com&#34;&gt;MotherDuck&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Pre-built binaries and additional installation options are coming soon.&lt;/p&gt; &#xA;&lt;p&gt;To build pg_duckdb, you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Postgres 16 or 17&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 22.04 or MacOS&lt;/li&gt; &#xA; &lt;li&gt;Standard set of build tools for building Postgres extensions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://duckdb.org/docs/dev/building/build_instructions&#34;&gt;Build tools that are required to build DuckDB&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To build and install, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, load the pg_duckdb extension:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE EXTENSION pg_duckdb;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; Once loaded you can use DuckDB execution by running &lt;code&gt;SET duckdb.execution TO true&lt;/code&gt;. This is &lt;em&gt;opt-in&lt;/em&gt; to avoid breaking existing queries. To avoid doing that for every session, you can configure it for a certain user by doing &lt;code&gt;ALTER USER my_analytics_user SET duckdb.execution TO true&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;SELECT&lt;/code&gt; queries executed by the DuckDB engine can directly read Postgres tables.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Able to read &lt;a href=&#34;https://www.postgresql.org/docs/current/datatype.html&#34;&gt;data types&lt;/a&gt; that exist in both Postgres and DuckDB. The following data types are supported: numeric, character, binary, date/time, boolean, uuid, json, and arrays.&lt;/li&gt; &#xA;   &lt;li&gt;If DuckDB cannot support the query for any reason, execution falls back to Postgres.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Read parquet and CSV files from object storage (AWS S3, Cloudflare R2, or Google GCS).&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SELECT n FROM read_parquet(&#39;s3://bucket/file.parquet&#39;) AS (n int)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SELECT n FROM read_csv(&#39;s3://bucket/file.csv&#39;) AS (n int)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You can pass globs and arrays to these functions, just like in DuckDB&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable the DuckDB Iceberg extension using &lt;code&gt;SELECT duckdb.enable_extension(&#39;iceberg&#39;)&lt;/code&gt; and read Iceberg files with &lt;code&gt;iceberg_scan&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Write a query â€” or an entire table â€” to parquet in object storage.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;COPY (SELECT foo, bar FROM baz) TO &#39;s3://...&#39;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;COPY table TO &#39;s3://...&#39;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Read and write to Parquet format in a single query&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt; COPY (&#xA;   SELECT count(*), name&#xA;   FROM read_parquet(&#39;s3://bucket/file.parquet&#39;) AS (name text)&#xA;   GROUP BY name&#xA;   ORDER BY count DESC&#xA; ) TO &#39;s3://bucket/results.parquet&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Query and &lt;code&gt;JOIN&lt;/code&gt; data in object storage with Postgres tables, views, and materialized views.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create indexes on Postgres tables to accelerate your DuckDB queries&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install DuckDB extensions using &lt;code&gt;SELECT duckdb.install_extension(&#39;extension_name&#39;);&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Toggle DuckDB execution on/off with a setting:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SET duckdb.execution = true|false&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The best way to get started is to connect Postgres to a new or existing object storage bucket (AWS S3, Cloudflare R2, or Google GCS) with pg_duckdb. You can query data in Parquet, CSV, and Iceberg format using &lt;code&gt;read_parquet&lt;/code&gt;, &lt;code&gt;read_csv&lt;/code&gt;, and &lt;code&gt;iceberg_scan&lt;/code&gt; respectively.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add a credential to enable DuckDB&#39;s httpfs support.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;INSERT INTO duckdb.secrets&#xA;(cloud_type, cloud_id, cloud_secret, cloud_region)&#xA;VALUES (&#39;S3&#39;, &#39;access_key_id&#39;, &#39;secret_accss_key&#39;, &#39;us-east-1&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy data directly to your bucket - no ETL pipeline!&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;COPY (SELECT user_id, item_id, price, purchased_at FROM purchases)&#xA;TO &#39;s3://your-bucket/purchases.parquet;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Perform analytics on your data.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT SUM(price) AS total, item_id&#xA;FROM read_parquet(&#39;s3://your-bucket/purchases.parquet&#39;)&#xA;  AS (price float, item_id int)&#xA;GROUP BY item_id&#xA;ORDER BY total DESC&#xA;LIMIT 100;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://github.com/orgs/duckdb/projects/10&#34;&gt;project roadmap&lt;/a&gt; for upcoming planned tasks and features.&lt;/p&gt; &#xA;&lt;h3&gt;Connect with MotherDuck&lt;/h3&gt; &#xA;&lt;p&gt;pg_duckdb integration with MotherDuck will enable hybrid execution with Differential Storage.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Zero-copy snapshots and forks&lt;/li&gt; &#xA; &lt;li&gt;Time travel&lt;/li&gt; &#xA; &lt;li&gt;Data tiering&lt;/li&gt; &#xA; &lt;li&gt;Improved concurrency and cacheability&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;pg_duckdb was developed in collaboration with our partners, &lt;a href=&#34;https://hydra.so&#34;&gt;Hydra&lt;/a&gt; and &lt;a href=&#34;https://motherduck.com&#34;&gt;MotherDuck&lt;/a&gt;. We look forward to their continued contributions and leadership.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hydra.so&#34;&gt;Hydra&lt;/a&gt; is a Y Combinator-backed database company, focused on DuckDB-Powered Postgres for app developers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://motherduck.com&#34;&gt;MotherDuck&lt;/a&gt; is the cloud-based data warehouse that extends the power of DuckDB.&lt;/p&gt; &#xA;&lt;p&gt;We welcome all contributions big and small:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vote on or suggest features for our roadmap.&lt;/li&gt; &#xA; &lt;li&gt;Open a PR.&lt;/li&gt; &#xA; &lt;li&gt;Submit a feature request or bug report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please see the &lt;a href=&#34;https://github.com/orgs/duckdb/projects/10&#34;&gt;project roadmap&lt;/a&gt; for upcoming planned tasks and features.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duckdb/pg_duckdb/issues&#34;&gt;GitHub Issues&lt;/a&gt; for bugs and missing features&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.duckdb.org/&#34;&gt;Discord discussion&lt;/a&gt; with the DuckDB community&lt;/li&gt; &#xA; &lt;li&gt;See our docs for more info and limitations&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>chriskohlhoff/asio</title>
    <updated>2024-08-25T01:38:49Z</updated>
    <id>tag:github.com,2024-08-25:/chriskohlhoff/asio</id>
    <link href="https://github.com/chriskohlhoff/asio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Asio C++ Library&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>