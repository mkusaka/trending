<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-06T01:47:15Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenNMT/CTranslate2</title>
    <updated>2023-08-06T01:47:15Z</updated>
    <id>tag:github.com,2023-08-06:/OpenNMT/CTranslate2</id>
    <link href="https://github.com/OpenNMT/CTranslate2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast inference engine for Transformer models&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenNMT/CTranslate2/actions?query=workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/OpenNMT/CTranslate2/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/ctranslate2&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/ctranslate2.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opennmt.net/CTranslate2/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/OpenNMT/CTranslate2?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/OpenNMT/CTranslate2.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.opennmt.net/&#34;&gt;&lt;img src=&#34;https://img.shields.io/discourse/status?server=https%3A%2F%2Fforum.opennmt.net%2F&#34; alt=&#34;Forum&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;CTranslate2&lt;/h1&gt; &#xA;&lt;p&gt;CTranslate2 is a C++ and Python library for efficient inference with Transformer models.&lt;/p&gt; &#xA;&lt;p&gt;The project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to &lt;a href=&#34;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/#benchmarks&#34;&gt;accelerate and reduce the memory usage&lt;/a&gt; of Transformer models on CPU and GPU. The following model types are currently supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Encoder-decoder models: Transformer base/big, M2M-100, NLLB, BART, mBART, Pegasus, T5, Whisper&lt;/li&gt; &#xA; &lt;li&gt;Decoder-only models: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, CodeGen, GPTBigCode, Falcon&lt;/li&gt; &#xA; &lt;li&gt;Encoder-only models: BERT, DistilBERT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Compatible models should be first converted into an optimized model format. The library includes converters for multiple frameworks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/guides/opennmt_py.html&#34;&gt;OpenNMT-py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/guides/opennmt_tf.html&#34;&gt;OpenNMT-tf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/guides/fairseq.html&#34;&gt;Fairseq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/guides/marian.html&#34;&gt;Marian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/guides/opus_mt.html&#34;&gt;OPUS-MT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/guides/transformers.html&#34;&gt;Transformers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The project is production-oriented and comes with &lt;a href=&#34;https://opennmt.net/CTranslate2/versioning.html&#34;&gt;backward compatibility guarantees&lt;/a&gt;, but it also includes experimental features related to model compression and inference acceleration.&lt;/p&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast and efficient execution on CPU and GPU&lt;/strong&gt;&lt;br&gt;The execution &lt;a href=&#34;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/#benchmarks&#34;&gt;is significantly faster and requires less resources&lt;/a&gt; than general-purpose deep learning frameworks on supported models and tasks thanks to many advanced optimizations: layer fusion, padding removal, batch reordering, in-place operations, caching mechanism, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quantization and reduced precision&lt;/strong&gt;&lt;br&gt;The model serialization and computation support weights with &lt;a href=&#34;https://opennmt.net/CTranslate2/quantization.html&#34;&gt;reduced precision&lt;/a&gt;: 16-bit floating points (FP16), 16-bit brain floating points (BF16), 16-bit integers (INT16), and 8-bit integers (INT8).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple CPU architectures support&lt;/strong&gt;&lt;br&gt;The project supports x86-64 and AArch64/ARM64 processors and integrates multiple backends that are optimized for these platforms: &lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html&#34;&gt;Intel MKL&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN&lt;/a&gt;, &lt;a href=&#34;https://www.openblas.net/&#34;&gt;OpenBLAS&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/ruy&#34;&gt;Ruy&lt;/a&gt;, and &lt;a href=&#34;https://developer.apple.com/documentation/accelerate&#34;&gt;Apple Accelerate&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic CPU detection and code dispatch&lt;/strong&gt;&lt;br&gt;One binary can include multiple backends (e.g. Intel MKL and oneDNN) and instruction set architectures (e.g. AVX, AVX2) that are automatically selected at runtime based on the CPU information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallel and asynchronous execution&lt;/strong&gt;&lt;br&gt;Multiple batches can be processed in parallel and asynchronously using multiple GPUs or CPU cores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic memory usage&lt;/strong&gt;&lt;br&gt;The memory usage changes dynamically depending on the request size while still meeting performance requirements thanks to caching allocators on both CPU and GPU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightweight on disk&lt;/strong&gt;&lt;br&gt;Quantization can make the models 4 times smaller on disk with minimal accuracy loss.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple integration&lt;/strong&gt;&lt;br&gt;The project has few dependencies and exposes simple APIs in &lt;a href=&#34;https://opennmt.net/CTranslate2/python/overview.html&#34;&gt;Python&lt;/a&gt; and C++ to cover most integration needs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configurable and interactive decoding&lt;/strong&gt;&lt;br&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2/decoding.html&#34;&gt;Advanced decoding features&lt;/a&gt; allow autocompleting a partial sequence and returning alternatives at a specific location in the sequence.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some of these features are difficult to achieve with standard deep learning frameworks and are the motivation for this project.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and usage&lt;/h2&gt; &#xA;&lt;p&gt;CTranslate2 can be installed with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ctranslate2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Python module is used to convert models and can translate or generate text with few lines of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;translator = ctranslate2.Translator(translation_model_path)&#xA;translator.translate_batch(tokens)&#xA;&#xA;generator = ctranslate2.Generator(generation_model_path)&#xA;generator.generate_batch(start_tokens)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://opennmt.net/CTranslate2&#34;&gt;documentation&lt;/a&gt; for more information and examples.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;We translate the En-&amp;gt;De test set &lt;em&gt;newstest2014&lt;/em&gt; with multiple models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/Models-tf/#translation&#34;&gt;OpenNMT-tf WMT14&lt;/a&gt;: a base Transformer trained with OpenNMT-tf on the WMT14 dataset (4.5M lines)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/Models-py/#translation&#34;&gt;OpenNMT-py WMT14&lt;/a&gt;: a base Transformer trained with OpenNMT-py on the WMT14 dataset (4.5M lines)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/en-de#opus-2020-02-26zip&#34;&gt;OPUS-MT&lt;/a&gt;: a base Transformer trained with Marian on all OPUS data available on 2020-02-26 (81.9M lines)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The benchmark reports the number of target tokens generated per second (higher is better). The results are aggregated over multiple runs. See the &lt;a href=&#34;https://raw.githubusercontent.com/OpenNMT/CTranslate2/master/tools/benchmark&#34;&gt;benchmark scripts&lt;/a&gt; for more details and reproduce these numbers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;CPU&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Tokens per second&lt;/th&gt; &#xA;   &lt;th&gt;Max. memory&lt;/th&gt; &#xA;   &lt;th&gt;BLEU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenNMT-tf WMT14 model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenNMT-tf 2.31.0 (with TensorFlow 2.11.0)&lt;/td&gt; &#xA;   &lt;td&gt;209.2&lt;/td&gt; &#xA;   &lt;td&gt;2653MB&lt;/td&gt; &#xA;   &lt;td&gt;26.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenNMT-py WMT14 model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenNMT-py 3.0.4 (with PyTorch 1.13.1)&lt;/td&gt; &#xA;   &lt;td&gt;275.8&lt;/td&gt; &#xA;   &lt;td&gt;2012MB&lt;/td&gt; &#xA;   &lt;td&gt;26.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8&lt;/td&gt; &#xA;   &lt;td&gt;323.3&lt;/td&gt; &#xA;   &lt;td&gt;1359MB&lt;/td&gt; &#xA;   &lt;td&gt;26.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; &#xA;   &lt;td&gt;658.8&lt;/td&gt; &#xA;   &lt;td&gt;849MB&lt;/td&gt; &#xA;   &lt;td&gt;26.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int16&lt;/td&gt; &#xA;   &lt;td&gt;733.0&lt;/td&gt; &#xA;   &lt;td&gt;672MB&lt;/td&gt; &#xA;   &lt;td&gt;26.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8&lt;/td&gt; &#xA;   &lt;td&gt;860.2&lt;/td&gt; &#xA;   &lt;td&gt;529MB&lt;/td&gt; &#xA;   &lt;td&gt;26.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8 + vmap&lt;/td&gt; &#xA;   &lt;td&gt;1126.2&lt;/td&gt; &#xA;   &lt;td&gt;598MB&lt;/td&gt; &#xA;   &lt;td&gt;26.64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OPUS-MT model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers 4.26.1 (with PyTorch 1.13.1)&lt;/td&gt; &#xA;   &lt;td&gt;147.3&lt;/td&gt; &#xA;   &lt;td&gt;2332MB&lt;/td&gt; &#xA;   &lt;td&gt;27.90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Marian 1.11.0&lt;/td&gt; &#xA;   &lt;td&gt;344.5&lt;/td&gt; &#xA;   &lt;td&gt;7605MB&lt;/td&gt; &#xA;   &lt;td&gt;27.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int16&lt;/td&gt; &#xA;   &lt;td&gt;330.2&lt;/td&gt; &#xA;   &lt;td&gt;5901MB&lt;/td&gt; &#xA;   &lt;td&gt;27.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8&lt;/td&gt; &#xA;   &lt;td&gt;355.8&lt;/td&gt; &#xA;   &lt;td&gt;4763MB&lt;/td&gt; &#xA;   &lt;td&gt;27.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; &#xA;   &lt;td&gt;525.0&lt;/td&gt; &#xA;   &lt;td&gt;721MB&lt;/td&gt; &#xA;   &lt;td&gt;27.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int16&lt;/td&gt; &#xA;   &lt;td&gt;596.1&lt;/td&gt; &#xA;   &lt;td&gt;660MB&lt;/td&gt; &#xA;   &lt;td&gt;27.53&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8&lt;/td&gt; &#xA;   &lt;td&gt;696.1&lt;/td&gt; &#xA;   &lt;td&gt;516MB&lt;/td&gt; &#xA;   &lt;td&gt;27.65&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Executed with 4 threads on a &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/c5/&#34;&gt;&lt;em&gt;c5.2xlarge&lt;/em&gt;&lt;/a&gt; Amazon EC2 instance equipped with an Intel(R) Xeon(R) Platinum 8275CL CPU.&lt;/p&gt; &#xA;&lt;h4&gt;GPU&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Tokens per second&lt;/th&gt; &#xA;   &lt;th&gt;Max. GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Max. CPU memory&lt;/th&gt; &#xA;   &lt;th&gt;BLEU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenNMT-tf WMT14 model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenNMT-tf 2.31.0 (with TensorFlow 2.11.0)&lt;/td&gt; &#xA;   &lt;td&gt;1483.5&lt;/td&gt; &#xA;   &lt;td&gt;3031MB&lt;/td&gt; &#xA;   &lt;td&gt;3122MB&lt;/td&gt; &#xA;   &lt;td&gt;26.94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenNMT-py WMT14 model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenNMT-py 3.0.4 (with PyTorch 1.13.1)&lt;/td&gt; &#xA;   &lt;td&gt;1795.2&lt;/td&gt; &#xA;   &lt;td&gt;2973MB&lt;/td&gt; &#xA;   &lt;td&gt;3099MB&lt;/td&gt; &#xA;   &lt;td&gt;26.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FasterTransformer 5.3&lt;/td&gt; &#xA;   &lt;td&gt;6979.0&lt;/td&gt; &#xA;   &lt;td&gt;2402MB&lt;/td&gt; &#xA;   &lt;td&gt;1131MB&lt;/td&gt; &#xA;   &lt;td&gt;26.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- float16&lt;/td&gt; &#xA;   &lt;td&gt;8592.5&lt;/td&gt; &#xA;   &lt;td&gt;1360MB&lt;/td&gt; &#xA;   &lt;td&gt;1135MB&lt;/td&gt; &#xA;   &lt;td&gt;26.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; &#xA;   &lt;td&gt;6634.7&lt;/td&gt; &#xA;   &lt;td&gt;1261MB&lt;/td&gt; &#xA;   &lt;td&gt;953MB&lt;/td&gt; &#xA;   &lt;td&gt;26.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8&lt;/td&gt; &#xA;   &lt;td&gt;8567.2&lt;/td&gt; &#xA;   &lt;td&gt;1005MB&lt;/td&gt; &#xA;   &lt;td&gt;807MB&lt;/td&gt; &#xA;   &lt;td&gt;26.85&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- float16&lt;/td&gt; &#xA;   &lt;td&gt;10990.7&lt;/td&gt; &#xA;   &lt;td&gt;941MB&lt;/td&gt; &#xA;   &lt;td&gt;807MB&lt;/td&gt; &#xA;   &lt;td&gt;26.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8 + float16&lt;/td&gt; &#xA;   &lt;td&gt;8725.4&lt;/td&gt; &#xA;   &lt;td&gt;813MB&lt;/td&gt; &#xA;   &lt;td&gt;800MB&lt;/td&gt; &#xA;   &lt;td&gt;26.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OPUS-MT model&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformers 4.26.1 (with PyTorch 1.13.1)&lt;/td&gt; &#xA;   &lt;td&gt;1022.9&lt;/td&gt; &#xA;   &lt;td&gt;4097MB&lt;/td&gt; &#xA;   &lt;td&gt;2109MB&lt;/td&gt; &#xA;   &lt;td&gt;27.90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Marian 1.11.0&lt;/td&gt; &#xA;   &lt;td&gt;3241.0&lt;/td&gt; &#xA;   &lt;td&gt;3381MB&lt;/td&gt; &#xA;   &lt;td&gt;2156MB&lt;/td&gt; &#xA;   &lt;td&gt;27.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- float16&lt;/td&gt; &#xA;   &lt;td&gt;3962.4&lt;/td&gt; &#xA;   &lt;td&gt;3239MB&lt;/td&gt; &#xA;   &lt;td&gt;1976MB&lt;/td&gt; &#xA;   &lt;td&gt;27.94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CTranslate2 3.6.0&lt;/td&gt; &#xA;   &lt;td&gt;5876.4&lt;/td&gt; &#xA;   &lt;td&gt;1197MB&lt;/td&gt; &#xA;   &lt;td&gt;754MB&lt;/td&gt; &#xA;   &lt;td&gt;27.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8&lt;/td&gt; &#xA;   &lt;td&gt;7521.9&lt;/td&gt; &#xA;   &lt;td&gt;1005MB&lt;/td&gt; &#xA;   &lt;td&gt;792MB&lt;/td&gt; &#xA;   &lt;td&gt;27.79&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- float16&lt;/td&gt; &#xA;   &lt;td&gt;9296.7&lt;/td&gt; &#xA;   &lt;td&gt;909MB&lt;/td&gt; &#xA;   &lt;td&gt;814MB&lt;/td&gt; &#xA;   &lt;td&gt;27.90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- int8 + float16&lt;/td&gt; &#xA;   &lt;td&gt;8362.7&lt;/td&gt; &#xA;   &lt;td&gt;813MB&lt;/td&gt; &#xA;   &lt;td&gt;766MB&lt;/td&gt; &#xA;   &lt;td&gt;27.90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Executed with CUDA 11 on a &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/g5/&#34;&gt;&lt;em&gt;g5.xlarge&lt;/em&gt;&lt;/a&gt; Amazon EC2 instance equipped with a NVIDIA A10G GPU (driver version: 510.47.03).&lt;/p&gt; &#xA;&lt;h2&gt;Additional resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opennmt.net/CTranslate2&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://forum.opennmt.net&#34;&gt;Forum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitter.im/OpenNMT/CTranslate2&#34;&gt;Gitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>PixarAnimationStudios/OpenUSD</title>
    <updated>2023-08-06T01:47:15Z</updated>
    <id>tag:github.com,2023-08-06:/PixarAnimationStudios/OpenUSD</id>
    <link href="https://github.com/PixarAnimationStudios/OpenUSD" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Universal Scene Description&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Universal Scene Description&lt;/h1&gt; &#xA;&lt;p&gt;Universal Scene Description (USD) is an efficient, scalable system for authoring, reading, and streaming time-sampled scene description for interchange between graphics applications.&lt;/p&gt; &#xA;&lt;p&gt;For more details, please visit the web site &lt;a href=&#34;http://openusd.org&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;macOS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;dev&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_build/latest?definitionId=2&amp;amp;branchName=dev&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_apis/build/status/PixarAnimationStudios.USD?branchName=dev&amp;amp;jobName=Linux&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_build/latest?definitionId=2&amp;amp;branchName=dev&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_apis/build/status/PixarAnimationStudios.USD?branchName=dev&amp;amp;jobName=Windows&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_build/latest?definitionId=2&amp;amp;branchName=dev&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_apis/build/status/PixarAnimationStudios.USD?branchName=dev&amp;amp;jobName=macOS&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;release&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_build/latest?definitionId=2&amp;amp;branchName=release&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_apis/build/status/PixarAnimationStudios.USD?branchName=release&amp;amp;jobName=Linux&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_build/latest?definitionId=2&amp;amp;branchName=release&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_apis/build/status/PixarAnimationStudios.USD?branchName=release&amp;amp;jobName=Windows&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_build/latest?definitionId=2&amp;amp;branchName=release&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PixarAnimationStudios/USD/_apis/build/status/PixarAnimationStudios.USD?branchName=release&amp;amp;jobName=macOS&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Additional Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://openusd.org/docs/index.html&#34;&gt;User Documentation and Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://openusd.org/docs/api/index.html&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/BUILDING.md&#34;&gt;Advanced Build Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;Need help understanding certain concepts in USD? See &lt;a href=&#34;http://openusd.org/docs/Getting-Help-with-USD.html&#34;&gt;Getting Help with USD&lt;/a&gt; or visit our &lt;a href=&#34;https://groups.google.com/forum/#!forum/usd-interest&#34;&gt;forum&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are experiencing undocumented problems with the software, please &lt;a href=&#34;https://github.com/PixarAnimationStudios/USD/issues/new&#34;&gt;file a bug&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Platforms&lt;/h2&gt; &#xA;&lt;p&gt;USD is primarily developed on Linux platforms (CentOS 7), but is built, tested and supported on macOS and Windows.&lt;/p&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/VERSIONS.md&#34;&gt;VERSIONS.md&lt;/a&gt; for explicitly tested versions.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Required:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;C/C++ compiler&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmake.org/documentation/&#34;&gt;CMake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://boost.org&#34;&gt;Boost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.threadingbuildingblocks.org/&#34;&gt;Intel TBB&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Optional:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.org&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/VERSIONS.md&#34;&gt;3rd Party Library and Application Versions&lt;/a&gt; for version information.&lt;/p&gt; &#xA;&lt;p&gt;Additional dependencies are required for the following components. These components may be disabled at build-time. For further details see &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/BUILDING.md&#34;&gt;Advanced Build Configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imaging and USD Imaging&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Required:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixarAnimationStudios/OpenSubdiv&#34;&gt;OpenSubdiv&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Optional:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.openexr.com&#34;&gt;OpenEXR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/openimageio/home&#34;&gt;OpenImageIO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://opencolorio.org/&#34;&gt;OpenColorIO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/imageworks/OpenShadingLanguage&#34;&gt;OSL (OpenShadingLanguage)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ptex.us/&#34;&gt;Ptex&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;usdview&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Required:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://wiki.qt.io/PySide6&#34;&gt;PySide6&lt;/a&gt; or &lt;a href=&#34;http://wiki.qt.io/PySide2&#34;&gt;PySide2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.python.org/pypi/PyOpenGL/&#34;&gt;PyOpenGL&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting and Building the Code&lt;/h2&gt; &#xA;&lt;p&gt;The simplest way to build USD is to run the supplied &lt;code&gt;build_usd.py&lt;/code&gt; script. This script will download required dependencies and build and install them along with USD in a given directory.&lt;/p&gt; &#xA;&lt;p&gt;Follow the instructions below to run the script with its default behavior, which will build the USD core libraries, Imaging, and USD Imaging components. For more options and documentation, run the script with the &lt;code&gt;--help&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/BUILDING.md&#34;&gt;Advanced Build Configuration&lt;/a&gt; for examples and additional documentation for running cmake directly.&lt;/p&gt; &#xA;&lt;h4&gt;1. Install prerequisites (see &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/#dependencies&#34;&gt;Dependencies&lt;/a&gt; for required versions)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Required: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;C++ compiler: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;gcc&lt;/li&gt; &#xA;     &lt;li&gt;Xcode&lt;/li&gt; &#xA;     &lt;li&gt;Microsoft Visual Studio&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;CMake&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Optional (Can be ignored by passing &lt;code&gt;--no-python&lt;/code&gt; as an argument to &lt;code&gt;build_usd.py&lt;/code&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Python (required for &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/BUILDING.md#python&#34;&gt;bindings and tests&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;PyOpenGL (required for &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/BUILDING.md#usd-imaging&#34;&gt;usdview&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;PySide6 or PySide2 (required for &lt;a href=&#34;https://raw.githubusercontent.com/PixarAnimationStudios/OpenUSD/release/BUILDING.md#usd-imaging&#34;&gt;usdview&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Download the USD source code&lt;/h4&gt; &#xA;&lt;p&gt;You can download source code archives from &lt;a href=&#34;https://www.github.com/PixarAnimationStudios/USD&#34;&gt;GitHub&lt;/a&gt; or use &lt;code&gt;git&lt;/code&gt; to clone the repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; git clone https://github.com/PixarAnimationStudios/USD&#xA;Cloning into &#39;USD&#39;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Run the script&lt;/h4&gt; &#xA;&lt;h5&gt;Linux:&lt;/h5&gt; &#xA;&lt;p&gt;For example, the following will download, build, and install USD&#39;s dependencies, then build and install USD into &lt;code&gt;/usr/local/USD&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; python USD/build_scripts/build_usd.py /usr/local/USD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;MacOS:&lt;/h5&gt; &#xA;&lt;p&gt;In a terminal, run &lt;code&gt;xcode-select&lt;/code&gt; to ensure command line developer tools are installed. Then run the script.&lt;/p&gt; &#xA;&lt;p&gt;For example, the following will download, build, and install USD&#39;s dependencies, then build and install USD into &lt;code&gt;/opt/local/USD&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; python USD/build_scripts/build_usd.py /opt/local/USD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Windows:&lt;/h5&gt; &#xA;&lt;p&gt;Launch the &#34;x64 Native Tools Command Prompt&#34; for your version of Visual Studio and run the script in the opened shell. Make sure to use the 64-bit (x64) command prompt and not the 32-bit (x86) command prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://docs.microsoft.com/en-us/cpp/build/how-to-enable-a-64-bit-visual-cpp-toolset-on-the-command-line&#34;&gt;https://docs.microsoft.com/en-us/cpp/build/how-to-enable-a-64-bit-visual-cpp-toolset-on-the-command-line&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;For example, the following will download, build, and install USD&#39;s dependencies, then build and install USD into &lt;code&gt;C:\USD&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;C:\&amp;gt; python USD\build_scripts\build_usd.py &#34;C:\USD&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Try it out&lt;/h4&gt; &#xA;&lt;p&gt;Set the environment variables specified by the script when it finishes and launch &lt;code&gt;usdview&lt;/code&gt; with a sample asset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; usdview USD/extras/usd/tutorials/convertingLayerFormats/Sphere.usda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute to USD (and we appreciate the help!), please see the &lt;a href=&#34;http://openusd.org/docs/Contributing-to-USD.html&#34;&gt;Contributing&lt;/a&gt; page in the documentation for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gaoxiang12/slam_in_autonomous_driving</title>
    <updated>2023-08-06T01:47:15Z</updated>
    <id>tag:github.com,2023-08-06:/gaoxiang12/slam_in_autonomous_driving</id>
    <link href="https://github.com/gaoxiang12/slam_in_autonomous_driving" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《自动驾驶中的SLAM技术》对应开源代码&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;SLAM in Autonomous Driving book (SAD book)&lt;/h2&gt; &#xA;&lt;p&gt;本书向读者系统介绍了惯性导航、组合导航、激光建图、激光定位、激光惯导里程计等知识。本仓库是书籍对应的源代码仓库，可以公开使用。&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/gaoxiang12/slam_in_autonomous_driving/assets/6635511/734af25b-d866-4dcf-a155-773190ba03d8&#34; width=&#34;300&#34;&gt; &#xA;&lt;h2&gt;注意&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本书已于2023.7.10开始印刷，预计在两周内上架。届时我会更新各平台的链接信息。&lt;/li&gt; &#xA; &lt;li&gt;电子工业出版社官方：&lt;a href=&#34;https://item.jd.com/10080292102089.html&#34;&gt;https://item.jd.com/10080292102089.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;京东自营： &lt;a href=&#34;https://item.jd.com/13797963.html&#34;&gt;https://item.jd.com/13797963.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;本书的内容编排&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第1章，概述&lt;/li&gt; &#xA; &lt;li&gt;第2章，数学基础知识回顾，几何学、运动学、KF滤波器理论，矩阵李群&lt;/li&gt; &#xA; &lt;li&gt;第3章，误差状态卡尔曼滤波器，惯性导航、卫星导航、组合导航&lt;/li&gt; &#xA; &lt;li&gt;第4章，预积分，图优化，基于预积分的组合导航&lt;/li&gt; &#xA; &lt;li&gt;第5章，点云基础处理，各种最近邻结构，点云线性拟合&lt;/li&gt; &#xA; &lt;li&gt;第6章，2D激光建图，scan matching, 似然场，子地图，2D回环检测，pose graph&lt;/li&gt; &#xA; &lt;li&gt;第7章，3D激光建图，ICP，变种ICP，NDT，NDT LO, Loam-like LO，LIO松耦合&lt;/li&gt; &#xA; &lt;li&gt;第8章，紧耦合LIO，IESKF，预积分紧耦合LIO&lt;/li&gt; &#xA; &lt;li&gt;第9章，离线建图，前端，后端，批量回环检测，地图优化，切片导出&lt;/li&gt; &#xA; &lt;li&gt;第10章，融合定位，激光定位，初始化搜索，切片地图加载，EKF融合&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;本书的特点&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本书大概是您能找到的类似材料中，数学推导和代码实现最为简单的书籍。&lt;/li&gt; &#xA; &lt;li&gt;在这本书里，您会复现许多激光SLAM中的经典算法和数据结构。 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;您需要自己推导、实现一个误差状态卡尔曼滤波器(ESKF)，把IMU和GNSS的数据喂给它，看它如何推算自己的状态。&lt;/li&gt; &#xA;   &lt;li&gt;您还会用预积分系统实现一样的功能，然后对比它们的运行方式。&lt;/li&gt; &#xA;   &lt;li&gt;接下来您会实现一遍2D激光SLAM中的常见算法：扫描匹配、似然场、子地图，占据栅格，再用回环检测来构建一个更大的地图。这些都需要您自己来完成。&lt;/li&gt; &#xA;   &lt;li&gt;在激光SLAM中，您也会自己实现一遍Kd树，处理近似最近邻，然后用这个Kd树来实现ICP，点面ICP，讨论它们有什么可以改进的地方。&lt;/li&gt; &#xA;   &lt;li&gt;然后您会实现经典的NDT算法，测试它的配准性能，然后用它来搭建一个激光里程计。它比大部分现有LO快得多。&lt;/li&gt; &#xA;   &lt;li&gt;您也会实现一个点面ICP的激光里程计，它也非常快。它工作的方式类似于Loam，但更简单。&lt;/li&gt; &#xA;   &lt;li&gt;您会想要把IMU系统也放到激光里程计中。我们会实现松耦合和紧耦合的LIO系统。同样地，您需要推导一遍迭代卡尔曼滤波器和预积分图优化。&lt;/li&gt; &#xA;   &lt;li&gt;您需要把上面的系统改成离线运行的，让回环检测运行地充分一些。最后将它做成一个离线的建图系统。&lt;/li&gt; &#xA;   &lt;li&gt;最后，您可以对上述地图进行切分，然后用来做实时定位。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;本书的大部分实现都要比类似的算法库简单的多。您可以快速地理解它们的工作方式，不需要面对复杂的接口。&lt;/li&gt; &#xA; &lt;li&gt;本书会使用非常方便的并发编程。您会发现，本书的实现往往比现有算法更高效。当然这有一部分是历史原因造成的。&lt;/li&gt; &#xA; &lt;li&gt;本书每章都会配有动态演示，像这样：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gaoxiang12/slam_in_autonomous_driving/master/doc/lio_demo.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/gaoxiang12/slam_in_autonomous_driving/master/doc/2dmapping_demo.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/gaoxiang12/slam_in_autonomous_driving/master/doc/lo_demo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;希望您能喜欢本书的极简风格，发现算法的乐趣所在。&lt;/p&gt; &#xA;&lt;h2&gt;数据集&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;数据集下载链接：&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;百度云链接: &lt;a href=&#34;https://pan.baidu.com/s/1ELOcF1UTKdfiKBAaXnE8sQ?pwd=feky&#34;&gt;https://pan.baidu.com/s/1ELOcF1UTKdfiKBAaXnE8sQ?pwd=feky&lt;/a&gt; 提取码: feky&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OneDrive链接：&lt;a href=&#34;https://1drv.ms/u/s!AgNFVSzSYXMahcEZejoUwCaHRcactQ?e=YsOYy2&#34;&gt;https://1drv.ms/u/s!AgNFVSzSYXMahcEZejoUwCaHRcactQ?e=YsOYy2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;包含以下数据集。总量较大(270GB)，请视自己硬盘容量下载。&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;UrbanLoco (ULHK，3D激光，道路场景)&lt;/li&gt; &#xA;   &lt;li&gt;NCLT (3D激光，RTK，校园场景)&lt;/li&gt; &#xA;   &lt;li&gt;WXB (3D激光，园区场景)&lt;/li&gt; &#xA;   &lt;li&gt;2dmapping (2D激光，商场场景)&lt;/li&gt; &#xA;   &lt;li&gt;AVIA (大疆固态激光)&lt;/li&gt; &#xA;   &lt;li&gt;UTBM (3D激光，道路场景)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;其他的内置数据&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;第3,4章使用文本格式的IMU，RTK数据&lt;/li&gt; &#xA;   &lt;li&gt;第7章使用了一部分EPFL的数据作为配准点云来源&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;您应该将上述数据下载至./dataset/sad/目录下，这样许多默认参数可以正常工作。如果不那么做，您也可以手动指定这些文件路径。如果您硬盘容量不足，可以将其他硬盘的目录软链至此处。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;编译&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本书推荐的编译环境是Ubuntu 20.04。更老的Ubuntu版本需要适配gcc编译器，主要是C++17标准。更新的Ubuntu则需要您自己安装对应的ROS版本。&lt;/li&gt; &#xA; &lt;li&gt;在编译本书代码之前，请先安装以下库（如果您机器上没有安装的话） &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ROS Noetic: &lt;a href=&#34;http://wiki.ros.org/noetic/Installation/Ubuntu&#34;&gt;http://wiki.ros.org/noetic/Installation/Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;使用以下指令安装其余的库&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install -y ros-noetic-pcl-ros ros-noetic-velodyne-msgs libopencv-dev libgoogle-glog-dev libeigen3-dev libsuitesparse-dev libpcl-dev libyaml-cpp-dev libbtbb-dev libgmock-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pangolin: 编译安装thirdparty/pangolin.zip，或者 &lt;a href=&#34;https://github.com/stevenlovegrove/Pangolin&#34;&gt;https://github.com/stevenlovegrove/Pangolin&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;编译thirdparty/g2o，或者自行编译安装 &lt;a href=&#34;https://github.com/RainerKuemmerle/g2o&#34;&gt;https://github.com/RainerKuemmerle/g2o&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;通过cmake, make安装本repo下的&lt;code&gt;thirdparty/g2o&lt;/code&gt;库&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;之后，使用通常的cmake, make方式就可以编译本书所有内容了。例如&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;编译后各章的可执行文件位于&lt;code&gt;bin&lt;/code&gt;目录下&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;适配Ubuntu18.04&lt;/h3&gt; &#xA;&lt;p&gt;为了在Ubuntu18.04上编译运行，需要安装gcc-9，并且使用对应版本的TBB。或者在docker环境下使用。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;安装gcc-9&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo add-apt-repository ppa:ubuntu-toolchain-r/test&#xA;sudo update-alternatives --remove-all gcc&#xA;sudo update-alternatives --remove-all g++&#xA;&#xA;#命令最后的1和10是优先级，如果使用auto选择模式，系统将默认使用优先级高的&#xA;sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 1&#xA;sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 10&#xA;&#xA;sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 1&#xA;sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;检查版本&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;g++ -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;编译程序&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake .. -DBUILD_WITH_UBUNTU1804=ON&#xA;make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;在docker环境下使用&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t sad:v1 .&#xA;./docker/docker_run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;进入docker容器后&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./thirdparty/g2o&#xA;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j8&#xA;cd /sad&#xA;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;图形界面在2023年以后特定型号的笔记本端导致桌面卡死（GL硬件兼容性）：&lt;a href=&#34;https://github.com/gaoxiang12/slam_in_autonomous_driving/issues/67&#34;&gt;https://github.com/gaoxiang12/slam_in_autonomous_driving/issues/67&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;第5章test_nn编译时，gtest报gmock错误：&lt;a href=&#34;https://github.com/gaoxiang12/slam_in_autonomous_driving/issues/18&#34;&gt;https://github.com/gaoxiang12/slam_in_autonomous_driving/issues/18&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;编译器版本问题：&lt;a href=&#34;https://github.com/gaoxiang12/slam_in_autonomous_driving/issues/4&#34;&gt;https://github.com/gaoxiang12/slam_in_autonomous_driving/issues/4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODO项&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LioPreiteg在某些数据集上不收敛&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;NOTES&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[已确认] ULHK的IMU似乎和别家的不一样，已经去了gravity, iekf初期可能有问题&lt;/li&gt; &#xA; &lt;li&gt;[已确认] NCLT的IMU在转包的时候转成了Lidar系，于是Lidar与IMU之间没有旋转的外参（本来Lidar是转了90度的），现在Lidar是X左Y后Z下，原车是X前Y右Z下。本书使用的NCLT数据均基于点云系, IMU的杆臂被忽略。&lt;/li&gt; &#xA; &lt;li&gt;[已确认] NCLT的rtk fix并不是非常稳定，平均误差在米级&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>