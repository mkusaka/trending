<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-05T01:48:13Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>godotengine/godot</title>
    <updated>2023-03-05T01:48:13Z</updated>
    <id>tag:github.com,2023-03-05:/godotengine/godot</id>
    <link href="https://github.com/godotengine/godot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Godot Engine – Multi-platform 2D and 3D game engine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Godot Engine&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://godotengine.org&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/godotengine/godot/master/logo_outlined.svg?sanitize=true&#34; width=&#34;400&#34; alt=&#34;Godot Engine logo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;2D and 3D cross-platform game engine&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://godotengine.org&#34;&gt;Godot Engine&lt;/a&gt; is a feature-packed, cross-platform game engine to create 2D and 3D games from a unified interface.&lt;/strong&gt; It provides a comprehensive set of &lt;a href=&#34;https://godotengine.org/features&#34;&gt;common tools&lt;/a&gt;, so that users can focus on making games without having to reinvent the wheel. Games can be exported with one click to a number of platforms, including the major desktop platforms (Linux, macOS, Windows), mobile platforms (Android, iOS), as well as Web-based platforms and &lt;a href=&#34;https://docs.godotengine.org/en/latest/tutorials/platform/consoles.html&#34;&gt;consoles&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Free, open source and community-driven&lt;/h2&gt; &#xA;&lt;p&gt;Godot is completely free and open source under the very permissive &lt;a href=&#34;https://godotengine.org/license&#34;&gt;MIT license&lt;/a&gt;. No strings attached, no royalties, nothing. The users&#39; games are theirs, down to the last line of engine code. Godot&#39;s development is fully independent and community-driven, empowering users to help shape their engine to match their expectations. It is supported by the &lt;a href=&#34;https://sfconservancy.org/&#34;&gt;Software Freedom Conservancy&lt;/a&gt; not-for-profit.&lt;/p&gt; &#xA;&lt;p&gt;Before being open sourced in &lt;a href=&#34;https://github.com/godotengine/godot/commit/0b806ee0fc9097fa7bda7ac0109191c9c5e0a1ac&#34;&gt;February 2014&lt;/a&gt;, Godot had been developed by &lt;a href=&#34;https://github.com/reduz&#34;&gt;Juan Linietsky&lt;/a&gt; and &lt;a href=&#34;https://github.com/punto-&#34;&gt;Ariel Manzur&lt;/a&gt; (both still maintaining the project) for several years as an in-house engine, used to publish several work-for-hire titles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/godotengine/godot-design/master/screenshots/editor_tps_demo_1920x1080.jpg&#34; alt=&#34;Screenshot of a 3D scene in the Godot Engine editor&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting the engine&lt;/h2&gt; &#xA;&lt;h3&gt;Binary downloads&lt;/h3&gt; &#xA;&lt;p&gt;Official binaries for the Godot editor and the export templates can be found &lt;a href=&#34;https://godotengine.org/download&#34;&gt;on the homepage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Compiling from source&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.godotengine.org/en/latest/contributing/development/compiling&#34;&gt;See the official docs&lt;/a&gt; for compilation instructions for every supported platform.&lt;/p&gt; &#xA;&lt;h2&gt;Community and contributing&lt;/h2&gt; &#xA;&lt;p&gt;Godot is not only an engine but an ever-growing community of users and engine developers. The main community channels are listed &lt;a href=&#34;https://godotengine.org/community&#34;&gt;on the homepage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The best way to get in touch with the core engine developers is to join the &lt;a href=&#34;https://chat.godotengine.org&#34;&gt;Godot Contributors Chat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To get started contributing to the project, see the &lt;a href=&#34;https://raw.githubusercontent.com/godotengine/godot/master/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and demos&lt;/h2&gt; &#xA;&lt;p&gt;The official documentation is hosted on &lt;a href=&#34;https://docs.godotengine.org&#34;&gt;Read the Docs&lt;/a&gt;. It is maintained by the Godot community in its own &lt;a href=&#34;https://github.com/godotengine/godot-docs&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://docs.godotengine.org/en/latest/classes/&#34;&gt;class reference&lt;/a&gt; is also accessible from the Godot editor.&lt;/p&gt; &#xA;&lt;p&gt;We also maintain official demos in their own &lt;a href=&#34;https://github.com/godotengine/godot-demo-projects&#34;&gt;GitHub repository&lt;/a&gt; as well as a list of &lt;a href=&#34;https://github.com/godotengine/awesome-godot&#34;&gt;awesome Godot community resources&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are also a number of other &lt;a href=&#34;https://docs.godotengine.org/en/latest/community/tutorials.html&#34;&gt;learning resources&lt;/a&gt; provided by the community, such as text and video tutorials, demos, etc. Consult the &lt;a href=&#34;https://godotengine.org/community&#34;&gt;community channels&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.codetriage.com/godotengine/godot&#34;&gt;&lt;img src=&#34;https://www.codetriage.com/godotengine/godot/badges/users.svg?sanitize=true&#34; alt=&#34;Code Triagers Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hosted.weblate.org/engage/godot-engine/?utm_source=widget&#34;&gt;&lt;img src=&#34;https://hosted.weblate.org/widgets/godot-engine/-/godot/svg-badge.svg?sanitize=true&#34; alt=&#34;Translate on Weblate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.tickgit.com/browse?repo=github.com/godotengine/godot&#34;&gt;&lt;img src=&#34;https://badgen.net/https/api.tickgit.com/badgen/github.com/godotengine/godot&#34; alt=&#34;TODOs&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CMU-Perceptual-Computing-Lab/openpose</title>
    <updated>2023-03-05T01:48:13Z</updated>
    <id>tag:github.com,2023-03-05:/CMU-Perceptual-Computing-Lab/openpose</id>
    <link href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/Logo_main_black.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Build Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Linux&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;MacOS&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Windows&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Build Status&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34;&gt;&lt;strong&gt;OpenPose&lt;/strong&gt;&lt;/a&gt; has represented the &lt;strong&gt;first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is &lt;strong&gt;authored by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;Ginés Hidalgo&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~zhecao&#34;&gt;&lt;strong&gt;Zhe Cao&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34;&gt;&lt;strong&gt;Tomas Simon&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=sFQD3k4AAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Shih-En Wei&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://jhugestar.github.io&#34;&gt;&lt;strong&gt;Hanbyul Joo&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;http://www.cs.cmu.edu/~yaser&#34;&gt;&lt;strong&gt;Yaser Sheikh&lt;/strong&gt;&lt;/a&gt;. It is &lt;strong&gt;maintained by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;Ginés Hidalgo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;. OpenPose would not be possible without the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34;&gt;&lt;strong&gt;CMU Panoptic Studio dataset&lt;/strong&gt;&lt;/a&gt;. We would also like to thank all the people who &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/09_authors_and_contributors.md&#34;&gt;has helped OpenPose in any way&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face_hands.gif&#34; width=&#34;480&#34;&gt; &lt;br&gt; &lt;sup&gt;Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Ginés Hidalgo&lt;/a&gt; (left) and &lt;a href=&#34;https://jhugestar.github.io&#34; target=&#34;_blank&#34;&gt;Hanbyul Joo&lt;/a&gt; (right) in front of the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34; target=&#34;_blank&#34;&gt;CMU Panoptic Studio&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#related-work&#34;&gt;Related Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#quick-start-overview&#34;&gt;Quick Start Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#send-us-feedback&#34;&gt;Send Us Feedback!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Whole-body (Body, Foot, Face, and Hands) 2D Pose Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/dance_foot.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_hands.gif&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;Testing OpenPose: (Left) &lt;a href=&#34;https://www.youtube.com/watch?v=2DiQUX11YaY&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;Crazy Uptown Funk flashmob in Sydney&lt;/i&gt;&lt;/a&gt; video sequence. (Center and right) Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Ginés Hidalgo&lt;/a&gt; and &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34; target=&#34;_blank&#34;&gt;Tomas Simon&lt;/a&gt; testing face and hands&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Whole-body 3D Pose Reconstruction and Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose3d.gif&#34; width=&#34;360&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; testing the OpenPose 3D Module&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Unity Plugin&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_main.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_body_foot.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_hand_face.png&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; and &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Ginés Hidalgo&lt;/a&gt; testing the &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34; target=&#34;_blank&#34;&gt;OpenPose Unity Plugin&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Runtime Analysis&lt;/h3&gt; &#xA;&lt;p&gt;We show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose_vs_competition.png&#34; width=&#34;360&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Main Functionality&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2D real-time multi-person keypoint detection&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;15, 18 or &lt;strong&gt;25-keypoint body/foot keypoint estimation&lt;/strong&gt;, including &lt;strong&gt;6 foot keypoints&lt;/strong&gt;. &lt;strong&gt;Runtime invariant to number of detected people&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;2x21-keypoint hand keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;70-keypoint face keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/3d_reconstruction_module.md&#34;&gt;&lt;strong&gt;3D real-time single-person keypoint detection&lt;/strong&gt;&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;3D triangulation from multiple single views.&lt;/li&gt; &#xA;   &lt;li&gt;Synchronization of Flir cameras handled.&lt;/li&gt; &#xA;   &lt;li&gt;Compatible with Flir/Point Grey cameras.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/calibration_module.md&#34;&gt;&lt;strong&gt;Calibration toolbox&lt;/strong&gt;&lt;/a&gt;: Estimation of distortion, intrinsic, and extrinsic camera parameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Single-person tracking&lt;/strong&gt; for further speedup or visual smoothing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hardware compatibility&lt;/strong&gt;: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage Alternatives&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/01_demo.md&#34;&gt;&lt;strong&gt;Command-line demo&lt;/strong&gt;&lt;/a&gt; for built-in functionality.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/04_cpp_api.md/&#34;&gt;&lt;strong&gt;C++ API&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/03_python_api.md&#34;&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/a&gt; for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For further details, check the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/07_major_released_features.md&#34;&gt;major released features&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/08_release_notes.md&#34;&gt;release notes&lt;/a&gt; docs.&lt;/p&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose training code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/&#34;&gt;&lt;strong&gt;OpenPose foot dataset&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34;&gt;&lt;strong&gt;OpenPose Unity Plugin&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenPose papers published in &lt;strong&gt;IEEE TPAMI and CVPR&lt;/strong&gt;. Cite them in your publications if OpenPose helps your research! (Links and more details in the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt; section below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use OpenPose without installing or writing any code, simply &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#windows-portable-demo&#34;&gt;download and use the latest Windows portable version of OpenPose&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, you could &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source&#34;&gt;build OpenPose from source&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installation doc&lt;/a&gt; for all the alternatives.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Overview&lt;/h2&gt; &#xA;&lt;p&gt;Simply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also add any of the available flags in any order. E.g., the following example runs on a video (&lt;code&gt;--video {PATH}&lt;/code&gt;), enables face (&lt;code&gt;--face&lt;/code&gt;) and hands (&lt;code&gt;--hand&lt;/code&gt;), and saves the output keypoints on JSON files on disk (&lt;code&gt;--write_json {PATH}&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, you can also extend OpenPose&#39;s functionality from its Python and C++ APIs. After &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installing&lt;/a&gt; OpenPose, check its &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/00_index.md&#34;&gt;official doc&lt;/a&gt; for a quick overview of all the alternatives and tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;Send Us Feedback!&lt;/h2&gt; &#xA;&lt;p&gt;Our library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.&lt;/li&gt; &#xA; &lt;li&gt;Want to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/10_community_projects.md&#34;&gt;Community-based Projects&lt;/a&gt; section or even integrate it with OpenPose!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;, while the hand and face detectors also use &lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt; (the face detector was trained using the same procedure than the hand detector).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{8765346,&#xA;  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},&#xA;  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},&#xA;  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2019}&#xA;}&#xA;&#xA;@inproceedings{simon2017hand,&#xA;  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{cao2017realtime,&#xA;  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{wei2016cpm,&#xA;  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Convolutional pose machines},&#xA;  year = {2016}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Paper links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8765346&#34;&gt;IEEE TPAMI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;ArXiv&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.08050&#34;&gt;Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.00134&#34;&gt;Convolutional Pose Machines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/LICENSE&#34;&gt;license&lt;/a&gt; for further details. Interested in a commercial license? Check this &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt;. For commercial queries, use the &lt;code&gt;Contact&lt;/code&gt; section from the &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt; and also send a copy of that message to &lt;a href=&#34;mailto:yaser@cs.cmu.edu&#34;&gt;Yaser Sheikh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TranslucentTB/TranslucentTB</title>
    <updated>2023-03-05T01:48:13Z</updated>
    <id>tag:github.com,2023-03-05:/TranslucentTB/TranslucentTB</id>
    <link href="https://github.com/TranslucentTB/TranslucentTB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A lightweight utility that makes the Windows taskbar translucent/transparent.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TranslucentTB&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://liberapay.com/TranslucentTB/&#34;&gt;&lt;img src=&#34;https://img.shields.io/liberapay/patrons/TranslucentTB.svg?sanitize=true&#34; alt=&#34;Liberapay patrons&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/TranslucentTB&#34;&gt;&lt;img src=&#34;https://discordapp.com/api/guilds/304387206552879116/widget.png?style=shield&#34; alt=&#34;Join on Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/sylve0n/TranslucentTB/_build/latest?definitionId=4&amp;amp;branchName=develop&#34;&gt;&lt;img src=&#34;https://dev.azure.com/sylve0n/TranslucentTB/_apis/build/status/TranslucentTB.TranslucentTB?branchName=develop&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/translucenttb/translucenttb/overview/develop&#34;&gt;&lt;img src=&#34;https://www.codefactor.io/repository/github/translucenttb/translucenttb/badge/develop&#34; alt=&#34;CodeFactor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6440374/180880766-4380b2cf-4d9e-4d07-8986-a9b34cb6244a.png#gh-dark-mode-only&#34; alt=&#34;Microsoft Store App Awards 2022 - Community Choice Award: Open Platform (runner up)&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6440374/180880839-355c472c-0b7a-4aae-88e5-0234001cb281.png#gh-light-mode-only&#34; alt=&#34;Microsoft Store App Awards 2022 - Community Choice Award: Open Platform (runner up)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A lightweight (uses a few MB of RAM and almost no CPU) utility that makes the Windows taskbar translucent/transparent on Windows 10 and Windows 11.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Advanced &lt;strong&gt;color picker&lt;/strong&gt; supporting alpha and live preview to change the taskbar&#39;s color.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Taskbar states&lt;/strong&gt; (choose one - color can be customized on every state except Normal): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Normal&lt;/strong&gt;: Regular Windows style. (as if TranslucentTB was not running)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Opaque&lt;/strong&gt;: Tinted taskbar, without transparency.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Clear&lt;/strong&gt;: Tinted taskbar.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Blur&lt;/strong&gt;: Will make the taskbar slightly blurred. Windows 10 and Windows 11 build 22000 only.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Acrylic&lt;/strong&gt;: Will give the taskbar an appearance similar to Microsoft&#39;s Fluent Design guidelines.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt; modes (these can be used together and each of them provides a taskbar state and color you can customize): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Visible window&lt;/strong&gt;: Will change the taskbar to a different appearance if a window is currently open on the desktop.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Maximized window&lt;/strong&gt;: Will change the taskbar to a different appearance if a window is currently maximised.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Start opened&lt;/strong&gt;: Will change the taskbar appearance when the start menu is opened.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Search opened&lt;/strong&gt;: Will change the taskbar appearance when the search menu (previously Cortana) is open.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Task View opened&lt;/strong&gt;: Will change the taskbar apperance when the Task View (previously Timeline) is open.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;On Windows 10, ability to &lt;strong&gt;show or hide the Aero Peek button&lt;/strong&gt; depending on the currently active dynamic mode.&lt;/li&gt; &#xA; &lt;li&gt;On Windows 11, ability to &lt;strong&gt;show or hide the taskbar line&lt;/strong&gt; depending on the currently active dynamic mode.&lt;/li&gt; &#xA; &lt;li&gt;Compatible with &lt;a href=&#34;https://github.com/torchgm/RoundedTB&#34;&gt;RoundedTB&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Compatible with &lt;a href=&#34;https://github.com/valinet/ExplorerPatcher&#34;&gt;ExplorerPatcher&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/QbG7KQA.png&#34; alt=&#34;windows 11 acrylic&#34; width=&#34;243&#34;&gt; &lt;img src=&#34;https://i.imgur.com/zabZ52s.png&#34; alt=&#34;windows 11 clear&#34; width=&#34;243&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/M15IPJW.png&#34; alt=&#34;windows 10 acrylic&#34;&gt; &lt;img src=&#34;https://i.imgur.com/eLGTtwp.png&#34; alt=&#34;windows 10 clear&#34;&gt; &lt;img src=&#34;https://i.imgur.com/r4ZJjnL.png&#34; alt=&#34;windows 10 blur&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://apps.microsoft.com/store/detail/9PF4KZ2VN4W9&#34;&gt;&lt;img src=&#34;https://get.microsoft.com/images/en-us%20dark.svg?sanitize=true&#34; alt=&#34;Get it from Microsoft&#34; height=&#34;104&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can download the program for free from the &lt;a href=&#34;https://www.microsoft.com/store/apps/9PF4KZ2VN4W9&#34;&gt;Microsoft Store&lt;/a&gt; and take advantage of its features like background auto-updates.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can download &lt;code&gt;TranslucentTB.appinstaller&lt;/code&gt; &lt;a href=&#34;https://github.com/TranslucentTB/TranslucentTB/releases&#34;&gt;via the releases tab&lt;/a&gt; and open it to install the app.&lt;/p&gt; &#xA;&lt;p&gt;A portable version of the app is also available &lt;a href=&#34;https://github.com/TranslucentTB/TranslucentTB/releases&#34;&gt;on GitHub releases&lt;/a&gt; as &lt;code&gt;TranslucentTB.zip&lt;/code&gt;, but this version only works on Windows 11.&lt;/p&gt; &#xA;&lt;p&gt;If you want to get the latest bleeding edge build, you can grab it over at the &lt;a href=&#34;https://dev.azure.com/sylve0n/TranslucentTB/_build?definitionId=4&#34;&gt;Azure Pipelines page&lt;/a&gt;. Note that these build may not work, or include features that are partially complete. Use at your own risk.&lt;/p&gt; &#xA;&lt;h2&gt;Add to Startup&lt;/h2&gt; &#xA;&lt;p&gt;To add TranslucentTB to startup, check the &#34;Open at boot&#34; entry in the TranslucentTB tray icon&#39;s context menu. If it is grayed out, TranslucentTB startup has been disabled by your organization.&lt;/p&gt; &#xA;&lt;p&gt;Portable versions can be added to startup by creating a shortcut to the executable in &lt;code&gt;%AppData%\Microsoft\Windows\Start Menu\Programs\Startup&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Donations and contributions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://liberapay.com/TranslucentTB/&#34;&gt;We have a Liberapay!&lt;/a&gt; Don&#39;t hesitate to donate if you appreciate TranslucentTB and would like to support our work.&lt;/p&gt; &#xA;&lt;p&gt;If you want to contribute to the source code, we have &lt;a href=&#34;https://raw.githubusercontent.com/TranslucentTB/TranslucentTB/release/CONTRIBUTING.md&#34;&gt;a how-to contribute guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;Some antiviruses are over eager, so they might flag this program as malicious. IT IS NOT! Over 10M users have downloaded this program safely. The source is open, you can &lt;a href=&#34;https://raw.githubusercontent.com/TranslucentTB/TranslucentTB/release/CONTRIBUTING.md#building-from-source&#34;&gt;compile it yourself&lt;/a&gt;, and we welcome any and all security reviews.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;TranslucentTB is a team effort! It is the result of the collective efforts of many people:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ethanhs&#34;&gt;@ethanhs&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sylveon&#34;&gt;@sylveon&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MrAksel&#34;&gt;@MrAksel&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/denosawr&#34;&gt;@denosawr&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;and last but not least, all of &lt;a href=&#34;https://github.com/TranslucentTB/TranslucentTB/graphs/contributors&#34;&gt;our contributors&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/dAKirby309&#34;&gt;@dAKirby309&lt;/a&gt; for making the icon! You can find more of his stuff on &lt;a href=&#34;https://dakirby309.deviantart.com/&#34;&gt;his DeviantArt profile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This program is free (as in speech) software under the GPLv3. Please see &lt;a href=&#34;https://raw.githubusercontent.com/TranslucentTB/TranslucentTB/release/LICENSE.md&#34;&gt;the license file&lt;/a&gt; for more.&lt;/p&gt;</summary>
  </entry>
</feed>