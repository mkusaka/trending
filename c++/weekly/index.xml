<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-16T01:51:34Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ztxz16/fastllm</title>
    <updated>2023-07-16T01:51:34Z</updated>
    <id>tag:github.com,2023-07-16:/ztxz16/fastllm</id>
    <link href="https://github.com/ztxz16/fastllm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;纯c++的全平台llm加速库，支持python调用，chatglm-6B级模型单卡可达10000+token / s，支持glm, llama, moss基座，手机端流畅运行&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;fastllm&lt;/h1&gt; &#xA;&lt;h2&gt;介绍&lt;/h2&gt; &#xA;&lt;p&gt;fastllm是纯c++实现，无第三方依赖的高性能大模型推理库&lt;/p&gt; &#xA;&lt;p&gt;6~7B级模型在安卓端上也可以流畅运行&lt;/p&gt; &#xA;&lt;p&gt;部署交流QQ群： 831641348&lt;/p&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B&#34;&gt;快速开始&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/#%E6%A8%A1%E5%9E%8B%E8%8E%B7%E5%8F%96&#34;&gt;模型获取&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/#%E5%BC%80%E5%8F%91%E8%AE%A1%E5%88%92&#34;&gt;开发计划&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;h2&gt;功能概述&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🚀 纯c++实现，便于跨平台移植，可以在安卓上直接编译&lt;/li&gt; &#xA; &lt;li&gt;🚀 ARM平台支持NEON指令集加速，X86平台支持AVX指令集加速，NVIDIA平台支持CUDA加速，各个平台速度都很快就是了&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持浮点模型（FP32), 半精度模型(FP16), 量化模型(INT8, INT4) 加速&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持Batch速度优化&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持流式输出，很方便实现打字机效果&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持并发计算时动态拼Batch&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持python调用&lt;/li&gt; &#xA; &lt;li&gt;🚀 前后端分离设计，便于支持新的计算设备&lt;/li&gt; &#xA; &lt;li&gt;🚀 目前支持ChatGLM模型，各种LLAMA模型(ALPACA, VICUNA等)，BAICHUAN模型，MOSS模型&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;两行代码加速 （测试中，暂时只支持ubuntu）&lt;/h2&gt; &#xA;&lt;p&gt;使用如下命令安装fastllm_pytools包&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd fastllm&#xA;mkdir build&#xA;cd build&#xA;cmake .. -DUSE_CUDA=ON # 如果不使用GPU编译，那么使用 cmake .. -DUSE_CUDA=OFF&#xA;make -j&#xA;cd tools &amp;amp;&amp;amp; python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后只需要在原本的推理程序中加入两行即可使用fastllm加速&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 这是原来的程序，通过huggingface接口创建模型&#xA;from transformers import AutoTokenizer, AutoModel&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/chatglm2-6b&#34;, trust_remote_code = True)&#xA;model = AutoModel.from_pretrained(&#34;THUDM/chatglm2-6b&#34;, trust_remote_code = True)&#xA;&#xA;# 加入下面这两行，将huggingface模型转换成fastllm模型&#xA;from fastllm_pytools import llm&#xA;model = llm.from_hf(model, tokenizer, dtype = &#34;float16&#34;) # dtype支持 &#34;float16&#34;, &#34;int8&#34;, &#34;int4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;model支持了ChatGLM的API函数chat, stream_chat，因此ChatGLM的demo程序无需改动其他代码即可运行&lt;/p&gt; &#xA;&lt;p&gt;model还支持下列API用于生成回复&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 生成回复&#xA;print(model.response(&#34;你好&#34;))&#xA;&#xA;# 流式生成回复&#xA;for response in model.stream_response(&#34;你好&#34;):&#xA;    print(response, flush = True, end = &#34;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;转好的模型也可以导出到本地文件，之后可以直接读取，也可以使用fastllm cpp接口读取&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.save(&#34;model.flm&#34;); # 导出fastllm模型&#xA;new_model = llm.model(&#34;model.flm&#34;); # 导入fastllm模型&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注: 该功能处于测试阶段，目前仅验证了ChatGLM、ChatGLM2模型可以通过2行代码加速&lt;/p&gt; &#xA;&lt;h2&gt;推理速度&lt;/h2&gt; &#xA;&lt;p&gt;6B级int4模型单4090延迟最低约5.5ms&lt;/p&gt; &#xA;&lt;p&gt;6B级fp16模型单4090最大吞吐量超过10000 token / s&lt;/p&gt; &#xA;&lt;p&gt;6B级int4模型在骁龙865上速度大约为4~5 token / s&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/docs/benchmark.md&#34;&gt;详细测试数据点这里&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;h3&gt;编译&lt;/h3&gt; &#xA;&lt;p&gt;建议使用cmake编译，需要提前安装c++编译器，make, cmake&lt;/p&gt; &#xA;&lt;p&gt;gcc版本建议9.4以上，cmake版本建议3.23以上&lt;/p&gt; &#xA;&lt;p&gt;GPU编译需要提前安装好CUDA编译环境，建议使用尽可能新的CUDA版本&lt;/p&gt; &#xA;&lt;p&gt;使用如下命令编译&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd fastllm&#xA;mkdir build&#xA;cd build&#xA;cmake .. -DUSE_CUDA=ON # 如果不使用GPU编译，那么使用 cmake .. -DUSE_CUDA=OFF&#xA;make -j&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;编译完成后，可以使用如下命令安装简易python工具包 (暂时只支持Linux)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd tools # 这时在fastllm/build/tools目录下&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;运行demo程序&lt;/h3&gt; &#xA;&lt;p&gt;我们假设已经获取了名为&lt;code&gt;model.flm&lt;/code&gt;的模型（参照 &lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/#%E6%A8%A1%E5%9E%8B%E8%8E%B7%E5%8F%96&#34;&gt;模型获取&lt;/a&gt;，初次使用可以先下载转换好的模型)&lt;/p&gt; &#xA;&lt;p&gt;编译完成之后在build目录下可以使用下列demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 这时在fastllm/build目录下&#xA;&#xA;# 命令行聊天程序, 支持打字机效果&#xA;./main -p model.flm &#xA;&#xA;# 简易webui, 使用流式输出 + 动态batch，可多路并发访问&#xA;./webui -p model.flm --port 1234 &#xA;&#xA;# python版本的命令行聊天程序，使用了模型创建以及流式对话效果&#xA;python tools/cli_demo.py -p model.flm &#xA;&#xA;# python版本的简易webui，需要先安装streamlit-chat&#xA;streamlit run tools/web_demo.py model.flm &#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;简易python调用&lt;/h3&gt; &#xA;&lt;p&gt;编译后如果安装了简易python工具包，那么可以使用python来调用一些基本的API （如果没有安装，也可以在直接import编译生成的tools/fastllm_pytools来使用)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 模型创建&#xA;from fastllm_pytools import llm&#xA;model = llm.model(&#34;model.flm&#34;)&#xA;&#xA;# 生成回复&#xA;print(model.response(&#34;你好&#34;))&#xA;&#xA;# 流式生成回复&#xA;for response in model.stream_response(&#34;你好&#34;):&#xA;    print(response, flush = True, end = &#34;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;另外还可以设置cpu线程数等内容，详细API说明见 &lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/docs/fastllm_pytools&#34;&gt;fastllm_pytools&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;这个包不包含low level api，如果需要使用更深入的功能请参考 &lt;a href=&#34;https://raw.githubusercontent.com/ztxz16/fastllm/master/#Python%E7%BB%91%E5%AE%9A&#34;&gt;Python绑定&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python绑定&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir build-py&#xA;cd build-py&#xA;cmake .. -DPY_API=ON -DUSE_CUDA=ON （只使用CPU则使用 cmake .. -DPY_API=ON 即可）&#xA;make -j&#xA;cd -&#xA;python cli.py  -m chatglm -p chatglm-6b-int8.bin 或  &#xA;python web_api.py  -m chatglm -p chatglm-6b-int8.bin  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;上述web api可使用python web_api_client.py进行测试&lt;/p&gt; &#xA;&lt;h2&gt;Android上使用&lt;/h2&gt; &#xA;&lt;h3&gt;编译&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 在PC上编译需要下载NDK工具&#xA;# 还可以尝试使用手机端编译，在termux中可以使用cmake和gcc（不需要使用NDK）&#xA;mkdir build-android&#xA;cd build-android&#xA;export NDK=&amp;lt;your_ndk_directory&amp;gt;&#xA;# 如果手机不支持，那么去掉 &#34;-DCMAKE_CXX_FLAGS=-march=armv8.2a+dotprod&#34; （比较新的手机都是支持的）&#xA;cmake -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-23 -DCMAKE_CXX_FLAGS=-march=armv8.2a+dotprod ..&#xA;make -j&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;运行&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;在Android设备上安装termux软件&lt;/li&gt; &#xA; &lt;li&gt;在termux中执行termux-setup-storage获得读取手机文件的权限。&lt;/li&gt; &#xA; &lt;li&gt;将NDK编译出的main文件，以及模型文件存入手机，并拷贝到termux的根目录&lt;/li&gt; &#xA; &lt;li&gt;使用命令&lt;code&gt;chmod 777 main&lt;/code&gt;赋权&lt;/li&gt; &#xA; &lt;li&gt;然后可以运行main文件，参数格式参见&lt;code&gt;./main --help&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;模型获取&lt;/h2&gt; &#xA;&lt;h3&gt;模型库&lt;/h3&gt; &#xA;&lt;p&gt;可以在以下链接中下载已经转换好的模型&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/huangyuyang&#34;&gt;huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;模型导出&lt;/h3&gt; &#xA;&lt;h4&gt;ChatGLM模型导出&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 需要先安装ChatGLM-6B环境&#xA;# 如果使用自己finetune的模型需要修改chatglm_export.py文件中创建tokenizer, model的代码&#xA;# 如果使用量化模型，需要先编译好quant文件，这里假设已经存在build/quant文件&#xA;cd build&#xA;python3 tools/chatglm_export.py chatglm-6b-fp32.flm # 导出浮点模型&#xA;./quant -p chatglm-6b-fp32.flm -o chatglm-6b-fp16.flm -b 16 #导出float16模型&#xA;./quant -p chatglm-6b-fp32.flm -o chatglm-6b-int8.flm -b 8 #导出int8模型&#xA;./quant -p chatglm-6b-fp32.flm -o chatglm-6b-int4.flm -b 4 #导出int4模型&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;baichuan模型导出&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 需要先安装baichuan环境&#xA;# 默认使用的是经过sft训练的对话模型，如果使用其余模型需要修改导出文件&#xA;# 如果使用量化模型，需要先编译好quant文件，这里假设已经存在build/quant文件&#xA;cd build&#xA;python3 tools/baichuan_peft2flm.py baichuan-fp32.flm # 导出浮点模型&#xA;./quant -p baichuan-fp32.flm -o baichuan-fp16.flm -b 16 #导出float16模型&#xA;./quant -p baichuan-fp32.flm -o baichuan-int8.flm -b 8 #导出int8模型&#xA;./quant -p baichuan-fp32.flm -o baichuan-int4.flm -b 4 #导出int4模型&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MOSS模型导出&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 需要先安装MOSS环境&#xA;# 如果使用自己finetune的模型需要修改moss_export.py文件中创建tokenizer, model的代码&#xA;# 如果使用量化模型，需要先编译好quant文件，这里假设已经存在build/quant文件&#xA;cd build&#xA;python3 tools/moss_export.py moss-fp32.flm # 导出浮点模型&#xA;./quant -p moss-fp32.flm -o moss-fp16.flm -b 16 #导出float16模型&#xA;./quant -p moss-fp32.flm -o moss-int8.flm -b 8 #导出int8模型&#xA;./quant -p moss-fp32.flm -o moss-int4.flm -b 4 #导出int4模型&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLAMA系列模型导出&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 修改build/tools/alpaca2flm.py程序进行导出&#xA;# 不同llama模型使用的指令相差很大，需要参照torch2flm.py中的参数进行配置&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;开发计划&lt;/h2&gt; &#xA;&lt;p&gt;也就是俗称的画饼部分，大家如果有需要的功能可以在讨论区提出&lt;/p&gt; &#xA;&lt;h3&gt;短期计划&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持部分显存 + 部分DDR模式&lt;/li&gt; &#xA; &lt;li&gt;优化int4, int8的batch推理&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;中期计划&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持更多后端，如opencl, vulkan, 以及一些NPU加速设备&lt;/li&gt; &#xA; &lt;li&gt;支持、验证更多模型，完善模型库&lt;/li&gt; &#xA; &lt;li&gt;优化tokenizer (由于目前在python中可以直接使用原模型的tokenizer来分词，所以这项工作暂时并不急迫)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;长期计划&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持ONNX模型导入、推理&lt;/li&gt; &#xA; &lt;li&gt;支持模型微调&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>facebook/igl</title>
    <updated>2023-07-16T01:51:34Z</updated>
    <id>tag:github.com,2023-07-16:/facebook/igl</id>
    <link href="https://github.com/facebook/igl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Intermediate Graphics Library (IGL) is a cross-platform library that commands the GPU. It provides a single low-level cross-platform interface on top of various graphics APIs (e.g. OpenGL, Metal and Vulkan).&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/facebook/igl/blob/main/.github/igl-full-color-white.svg?raw=true&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://github.com/facebook/igl/blob/main/.github/igl-full-color-black.svg?raw=true&#34;&gt; &#xA;  &lt;img alt=&#34;IGL Logo&#34; src=&#34;https://raw.githubusercontent.com/facebook/igl/main/.github/igl-full-color-black.svg?sanitize=true&#34; width=&#34;500&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/facebook/igl/actions&#34;&gt;&lt;img src=&#34;https://github.com/facebook/igl/actions/workflows/c-cpp.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Intermediate Graphics Library (IGL) is a cross-platform library that commands the GPU. It encapsulates common GPU functionality with a low-level cross-platform interface. IGL is designed to support multiple backends implemented on top of various graphics APIs (e.g. OpenGL, Metal and Vulkan) with a common interface.&lt;/p&gt; &#xA;&lt;p&gt;There are a lot of good options for abstracting GPU API&#39;s; each making different trade-offs. We designed IGL around the following priorities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;em&gt;Low-level, forward-looking API.&lt;/em&gt; IGL embraces modern abstractions (command buffers, state containers, bindless, etc) and is designed to give more control than OpenGL&#39;s state machine API. As a result, IGL can have leaner backends for modern API&#39;s (e.g. Metal, Vulkan).&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Minimal overhead for C++.&lt;/em&gt; IGL supports new or existing native rendering code without overhead of language interop or the need for other language runtimes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Reach + scale in production.&lt;/em&gt; IGL has been globally battle-tested for broad device reliability (especially the long-tail of Android devices as well as Quest 2/3/Pro compatibility for OpenGL/Vulkan) &lt;em&gt;and&lt;/em&gt; performance-tuned on our apps.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Supported rendering backends&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Metal 2+&lt;/li&gt; &#xA; &lt;li&gt;OpenGL 2.x (requires &lt;a href=&#34;https://registry.khronos.org/OpenGL/extensions/ARB/ARB_framebuffer_object.txt&#34;&gt;GL_ARB_framebuffer_object&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;OpenGL 3.1+&lt;/li&gt; &#xA; &lt;li&gt;OpenGL ES 2.0+&lt;/li&gt; &#xA; &lt;li&gt;Vulkan 1.1 (requires &lt;a href=&#34;https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_KHR_buffer_device_address.html&#34;&gt;VK_KHR_buffer_device_address&lt;/a&gt; and &lt;a href=&#34;https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_EXT_descriptor_indexing.html&#34;&gt;VK_EXT_descriptor_indexing&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;WebGL 2.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported platforms&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Android&lt;/li&gt; &#xA; &lt;li&gt;iOS&lt;/li&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;macOS&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA; &lt;li&gt;WebAssembly&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;API Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Windows&lt;/th&gt; &#xA;   &lt;th&gt;Linux&lt;/th&gt; &#xA;   &lt;th&gt;macOS&lt;/th&gt; &#xA;   &lt;th&gt;iOS&lt;/th&gt; &#xA;   &lt;th&gt;Android&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vulkan 1.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt; (MoltenVK)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt; (Quest 2/3/Pro)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenGL ES 2.0 - 3.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt; (Angle)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt; (Angle)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenGL ES 3.1 - 3.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt; (Angle)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt; (Angle)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenGL 3.1 - 4.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Metal 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;✖&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Before building, run the deployment scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 deploy_content.py&#xA;python3 deploy_deps.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These scripts download external third-party dependencies. Please check [Dependencies] for the full list.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd build&#xA;cmake .. -G &#34;Visual Studio 17 2022&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install clang xorg-dev libxinerama-dev libxcursor-dev libgles2-mesa-dev libegl1-mesa-dev libglfw3-dev libglew-dev libstdc++-12-dev&#xA;cd build&#xA;cmake .. -G &#34;Unix Makefiles&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd build&#xA;cmake .. -G &#34;Xcode&#34; -DIGL_WITH_VULKAN=OFF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;iOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd build&#xA;cmake .. -G Xcode -DCMAKE_TOOLCHAIN_FILE=../third-party/deps/src/ios-cmake/ios.toolchain.cmake -DPLATFORM=SIMULATOR64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Android&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Gradle project is located within the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/igl/main/build/android/&#34;&gt;build/android&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;WebAssembly&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please install &lt;a href=&#34;https://emscripten.org/docs/getting_started/downloads.html&#34;&gt;Emscripten&lt;/a&gt; and &lt;a href=&#34;https://ninja-build.org/&#34;&gt;Ninja&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd build&#xA;emcmake cmake .. -G Ninja&#xA;cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebook/igl/main/.github/screenshot01.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebook/igl/main/.github/screenshot02.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;IGL is released under the MIT license, see &lt;a href=&#34;https://raw.githubusercontent.com/facebook/igl/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; for the full text as well as third-party library acknowledgements. SparkSL Compiler is released under the SparkSL Compiler License, see &lt;a href=&#34;https://github.com/facebook/igl/releases/download/SparkSL/SparkSL.LICENSE&#34;&gt;LICENSE&lt;/a&gt; for full text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TixiaoShan/LIO-SAM</title>
    <updated>2023-07-16T01:51:34Z</updated>
    <id>tag:github.com,2023-07-16:/TixiaoShan/LIO-SAM</id>
    <link href="https://github.com/TixiaoShan/LIO-SAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LIO-SAM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;A real-time lidar-inertial odometry package. We strongly recommend the users read this document thoroughly and test the package with the provided dataset first. A video of the demonstration of the method can be found on &lt;a href=&#34;https://www.youtube.com/watch?v=A0H8CoORZJU&#34;&gt;YouTube&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/demo.gif&#34; alt=&#34;drawing&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/device-hand-2.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/device-hand.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/device-jackal.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/device-livox-horizon.png&#34; alt=&#34;drawing&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Menu&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#system-architecture&#34;&gt;&lt;strong&gt;System architecture&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#dependency&#34;&gt;&lt;strong&gt;Package dependency&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#install&#34;&gt;&lt;strong&gt;Package install&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#prepare-lidar-data&#34;&gt;&lt;strong&gt;Prepare lidar data&lt;/strong&gt;&lt;/a&gt; (must read)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#prepare-imu-data&#34;&gt;&lt;strong&gt;Prepare IMU data&lt;/strong&gt;&lt;/a&gt; (must read)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#sample-datasets&#34;&gt;&lt;strong&gt;Sample datasets&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#run-the-package&#34;&gt;&lt;strong&gt;Run the package&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#other-notes&#34;&gt;&lt;strong&gt;Other notes&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#issues&#34;&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#paper&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#todo&#34;&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#related-package&#34;&gt;&lt;strong&gt;Related Package&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/#acknowledgement&#34;&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System architecture&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/system.png&#34; alt=&#34;drawing&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We design a system that maintains two graphs and runs up to 10x faster than real-time.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The factor graph in &#34;mapOptimization.cpp&#34; optimizes lidar odometry factor and GPS factor. This factor graph is maintained consistently throughout the whole test.&lt;/li&gt; &#xA; &lt;li&gt;The factor graph in &#34;imuPreintegration.cpp&#34; optimizes IMU and lidar odometry factor and estimates IMU bias. This factor graph is reset periodically and guarantees real-time odometry estimation at IMU frequency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependency&lt;/h2&gt; &#xA;&lt;p&gt;This is the original ROS1 implementation of LIO-SAM. For a ROS2 implementation see branch &lt;code&gt;ros2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/ROS/Installation&#34;&gt;ROS&lt;/a&gt; (tested with Kinetic and Melodic. Refer to &lt;a href=&#34;https://github.com/TixiaoShan/LIO-SAM/issues/206&#34;&gt;#206&lt;/a&gt; for Noetic) &lt;pre&gt;&lt;code&gt;sudo apt-get install -y ros-kinetic-navigation&#xA;sudo apt-get install -y ros-kinetic-robot-localization&#xA;sudo apt-get install -y ros-kinetic-robot-state-publisher&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gtsam.org/get_started/&#34;&gt;gtsam&lt;/a&gt; (Georgia Tech Smoothing and Mapping library) &lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:borglab/gtsam-release-4.0&#xA;sudo apt install libgtsam-dev libgtsam-unstable-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Use the following commands to download and compile the package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ~/catkin_ws/src&#xA;git clone https://github.com/TixiaoShan/LIO-SAM.git&#xA;cd ..&#xA;catkin_make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Docker&lt;/h2&gt; &#xA;&lt;p&gt;Build image (based on ROS1 Kinetic):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t liosam-kinetic-xenial .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you have the image, start a container as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --init -it -d \&#xA;  -v /etc/localtime:/etc/localtime:ro \&#xA;  -v /etc/timezone:/etc/timezone:ro \&#xA;  -v /tmp/.X11-unix:/tmp/.X11-unix \&#xA;  -e DISPLAY=$DISPLAY \&#xA;  liosam-kinetic-xenial \&#xA;  bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prepare lidar data&lt;/h2&gt; &#xA;&lt;p&gt;The user needs to prepare the point cloud data in the correct format for cloud deskewing, which is mainly done in &#34;imageProjection.cpp&#34;. The two requirements are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Provide point time stamp&lt;/strong&gt;. LIO-SAM uses IMU data to perform point cloud deskew. Thus, the relative point time in a scan needs to be known. The up-to-date Velodyne ROS driver should output this information directly. Here, we assume the point time channel is called &#34;time.&#34; The definition of the point type is located at the top of the &#34;imageProjection.cpp.&#34; &#34;deskewPoint()&#34; function utilizes this relative time to obtain the transformation of this point relative to the beginning of the scan. When the lidar rotates at 10Hz, the timestamp of a point should vary between 0 and 0.1 seconds. If you are using other lidar sensors, you may need to change the name of this time channel and make sure that it is the relative time in a scan.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Provide point ring number&lt;/strong&gt;. LIO-SAM uses this information to organize the point correctly in a matrix. The ring number indicates which channel of the sensor that this point belongs to. The definition of the point type is located at the top of &#34;imageProjection.cpp.&#34; The up-to-date Velodyne ROS driver should output this information directly. Again, if you are using other lidar sensors, you may need to rename this information. Note that only mechanical lidars are supported by the package currently.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prepare IMU data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;IMU requirement&lt;/strong&gt;. Like the original LOAM implementation, LIO-SAM only works with a 9-axis IMU, which gives roll, pitch, and yaw estimation. The roll and pitch estimation is mainly used to initialize the system at the correct attitude. The yaw estimation initializes the system at the right heading when using GPS data. Theoretically, an initialization procedure like VINS-Mono will enable LIO-SAM to work with a 6-axis IMU. (&lt;strong&gt;New&lt;/strong&gt;: &lt;a href=&#34;https://github.com/YJZLuckyBoy/liorf&#34;&gt;liorf&lt;/a&gt; has added support for 6-axis IMU.) The performance of the system largely depends on the quality of the IMU measurements. The higher the IMU data rate, the better the system accuracy. We use Microstrain 3DM-GX5-25, which outputs data at 500Hz. We recommend using an IMU that gives at least a 200Hz output rate. Note that the internal IMU of Ouster lidar is an 6-axis IMU.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;IMU alignment&lt;/strong&gt;. LIO-SAM transforms IMU raw data from the IMU frame to the Lidar frame, which follows the ROS REP-105 convention (x - forward, y - left, z - upward). To make the system function properly, the correct extrinsic transformation needs to be provided in &#34;params.yaml&#34; file. &lt;strong&gt;The reason why there are two extrinsics is that my IMU (Microstrain 3DM-GX5-25) acceleration and attitude have different cooridinates. Depend on your IMU manufacturer, the two extrinsics for your IMU may or may not be the same&lt;/strong&gt;. Using our setup as an example:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;we need to set the readings of x-z acceleration and gyro negative to transform the IMU data in the lidar frame, which is indicated by &#34;extrinsicRot&#34; in &#34;params.yaml.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;The transformation of attitude readings might be slightly different. IMU&#39;s attitude measurement &lt;code&gt;q_wb&lt;/code&gt; usually means the rotation of points in the IMU coordinate system to the world coordinate system (e.g. ENU). However, the algorithm requires &lt;code&gt;q_wl&lt;/code&gt;, the rotation from lidar to world. So we need a rotation from lidar to IMU &lt;code&gt;q_bl&lt;/code&gt;, where &lt;code&gt;q_wl = q_wb * q_bl&lt;/code&gt;. For convenience, the user only needs to provide &lt;code&gt;q_lb&lt;/code&gt; as &#34;extrinsicRPY&#34; in &#34;params.yaml&#34; (same as the &#34;extrinsicRot&#34; if acceleration and attitude have the same coordinate).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;IMU debug&lt;/strong&gt;. It&#39;s strongly recommended that the user uncomment the debug lines in &#34;imuHandler()&#34; of &#34;imageProjection.cpp&#34; and test the output of the transformed IMU data. The user can rotate the sensor suite to check whether the readings correspond to the sensor&#39;s movement. A YouTube video that shows the corrected IMU data can be found &lt;a href=&#34;https://youtu.be/BOUK8LYQhHs&#34;&gt;here (link to YouTube)&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/imu-transform.png&#34; alt=&#34;drawing&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/imu-debug.gif&#34; alt=&#34;drawing&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Sample datasets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download some sample datasets to test the functionality of the package. The datasets below are configured to run using the default settings:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Walking dataset:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Park dataset:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Garden dataset:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The datasets below need the parameters to be configured. In these datasets, the point cloud topic is &#34;points_raw.&#34; The IMU topic is &#34;imu_correct,&#34; which gives the IMU data in ROS REP105 standard. Because no IMU transformation is needed for this dataset, the following configurations need to be changed to run this dataset successfully:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The &#34;imuTopic&#34; parameter in &#34;config/params.yaml&#34; needs to be set to &#34;imu_correct&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;The &#34;extrinsicRot&#34; and &#34;extrinsicRPY&#34; in &#34;config/params.yaml&#34; needs to be set as identity matrices. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Rotation dataset:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Campus dataset (large):&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Campus dataset (small):&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ouster (OS1-128) dataset. No extrinsics need to be changed for this dataset if you are using the default settings. Please follow the Ouster notes below to configure the package to run with Ouster data. A video of the dataset can be found on &lt;a href=&#34;https://youtu.be/O7fKgZQzkEo&#34;&gt;YouTube&lt;/a&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Rooftop dataset:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Livox Horizon dataset. Please refer to the following notes section for paramater changes.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Livox Horizon:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;KITTI dataset. The extrinsics can be found in the Notes KITTI section below. To generate more bags using other KITTI raw data, you can use the python script provided in &#34;config/doc/kitti2bag&#34;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;2011_09_30_drive_0028:&lt;/strong&gt; [&lt;a href=&#34;https://drive.google.com/drive/folders/1gJHwfdHCRdjP7vuT556pv8atqrCJPbUq?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run the package&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run the launch file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;roslaunch lio_sam run.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Play existing bag files:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;rosbag play your-bag.bag -r 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Loop closure:&lt;/strong&gt; The loop function here gives an example of proof of concept. It is directly adapted from LeGO-LOAM loop closure. For more advanced loop closure implementation, please refer to &lt;a href=&#34;https://github.com/irapkaist/SC-LeGO-LOAM&#34;&gt;ScanContext&lt;/a&gt;. Set the &#34;loopClosureEnableFlag&#34; in &#34;params.yaml&#34; to &#34;true&#34; to test the loop closure function. In Rviz, uncheck &#34;Map (cloud)&#34; and check &#34;Map (global)&#34;. This is because the visualized map - &#34;Map (cloud)&#34; - is simply a stack of point clouds in Rviz. Their postion will not be updated after pose correction. The loop closure function here is simply adapted from LeGO-LOAM, which is an ICP-based method. Because ICP runs pretty slow, it is suggested that the playback speed is set to be &#34;-r 1&#34;. You can try the Garden dataset for testing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/loop-closure.gif&#34; alt=&#34;drawing&#34; width=&#34;350&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/loop-closure-2.gif&#34; alt=&#34;drawing&#34; width=&#34;350&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Using GPS:&lt;/strong&gt; The park dataset is provided for testing LIO-SAM with GPS data. This dataset is gathered by &lt;a href=&#34;https://robustfieldautonomylab.github.io/people.html&#34;&gt;Yewei Huang&lt;/a&gt;. To enable the GPS function, change &#34;gpsTopic&#34; in &#34;params.yaml&#34; to &#34;odometry/gps&#34;. In Rviz, uncheck &#34;Map (cloud)&#34; and check &#34;Map (global)&#34;. Also check &#34;Odom GPS&#34;, which visualizes the GPS odometry. &#34;gpsCovThreshold&#34; can be adjusted to filter bad GPS readings. &#34;poseCovThreshold&#34; can be used to adjust the frequency of adding GPS factor to the graph. For example, you will notice the trajectory is constantly corrected by GPS whey you set &#34;poseCovThreshold&#34; to 1.0. Because of the heavy iSAM optimization, it&#39;s recommended that the playback speed is &#34;-r 1&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/gps-demo.gif&#34; alt=&#34;drawing&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;KITTI:&lt;/strong&gt; Since LIO-SAM needs a high-frequency IMU for function properly, we need to use KITTI raw data for testing. One problem remains unsolved is that the intrinsics of the IMU are unknown, which has a big impact on the accuracy of LIO-SAM. Download the provided sample data and make the following changes in &#34;params.yaml&#34;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;extrinsicTrans: [-8.086759e-01, 3.195559e-01, -7.997231e-01]&lt;/li&gt; &#xA;   &lt;li&gt;extrinsicRot: [9.999976e-01, 7.553071e-04, -2.035826e-03, -7.854027e-04, 9.998898e-01, -1.482298e-02, 2.024406e-03, 1.482454e-02, 9.998881e-01]&lt;/li&gt; &#xA;   &lt;li&gt;extrinsicRPY: [9.999976e-01, 7.553071e-04, -2.035826e-03, -7.854027e-04, 9.998898e-01, -1.482298e-02, 2.024406e-03, 1.482454e-02, 9.998881e-01]&lt;/li&gt; &#xA;   &lt;li&gt;N_SCAN: 64&lt;/li&gt; &#xA;   &lt;li&gt;downsampleRate: 2 or 4&lt;/li&gt; &#xA;   &lt;li&gt;loopClosureEnableFlag: true or false&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/kitti-map.png&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/kitti-demo.gif&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ouster lidar:&lt;/strong&gt; To make LIO-SAM work with Ouster lidar, some preparations need to be done on hardware and software level. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Hardware: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Use an external IMU. LIO-SAM does not work with the internal 6-axis IMU of Ouster lidar. You need to attach a 9-axis IMU to the lidar and perform data-gathering.&lt;/li&gt; &#xA;     &lt;li&gt;Configure the driver. Change &#34;timestamp_mode&#34; in your Ouster launch file to &#34;TIME_FROM_PTP_1588&#34; so you can have ROS format timestamp for the point clouds.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Config: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Change &#34;sensor&#34; in &#34;params.yaml&#34; to &#34;ouster&#34;.&lt;/li&gt; &#xA;     &lt;li&gt;Change &#34;N_SCAN&#34; and &#34;Horizon_SCAN&#34; in &#34;params.yaml&#34; according to your lidar, i.e., N_SCAN=128, Horizon_SCAN=1024.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Gen 1 and Gen 2 Ouster: It seems that the point coordinate definition might be different in different generations. Please refer to &lt;a href=&#34;https://github.com/TixiaoShan/LIO-SAM/issues/94&#34;&gt;Issue #94&lt;/a&gt; for debugging.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/ouster-device.jpg&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/ouster-demo.gif&#34; alt=&#34;drawing&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Livox Horizon lidar:&lt;/strong&gt; Please note that solid-state lidar hasn&#39;t been extensively tested with LIO-SAM yet. An external IMU is also used here rather than the internal one. The support for such lidars is based on minimal change of the codebase from mechanical lidars. A customized &lt;a href=&#34;https://github.com/TixiaoShan/livox_ros_driver&#34;&gt;livox_ros_driver&lt;/a&gt; needs to be used to publish point cloud format that can be processed by LIO-SAM. Other SLAM solutions may offer better implementations. More studies and suggestions are welcome. Please change the following parameters to make LIO-SAM work with Livox Horizon lidar: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sensor: livox&lt;/li&gt; &#xA;   &lt;li&gt;N_SCAN: 6&lt;/li&gt; &#xA;   &lt;li&gt;Horizon_SCAN: 4000&lt;/li&gt; &#xA;   &lt;li&gt;edgeFeatureMinValidNum: 1&lt;/li&gt; &#xA;   &lt;li&gt;Use &lt;a href=&#34;https://github.com/TixiaoShan/livox_ros_driver&#34;&gt;livox_ros_driver&lt;/a&gt; for data recording&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/livox-demo.gif&#34; alt=&#34;drawing&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Service&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/lio_sam/save_map &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;save map as a PCD file. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  rosservice call [service] [resolution] [destination]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Example:&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  $ rosservice call /lio_sam/save_map 0.2 &#34;/Downloads/LOAM/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zigzag or jerking behavior&lt;/strong&gt;: if your lidar and IMU data formats are consistent with the requirement of LIO-SAM, this problem is likely caused by un-synced timestamp of lidar and IMU data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jumpping up and down&lt;/strong&gt;: if you start testing your bag file and the base_link starts to jump up and down immediately, it is likely your IMU extrinsics are wrong. For example, the gravity acceleration has negative value.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;mapOptimization crash&lt;/strong&gt;: it is usually caused by GTSAM. Please install the GTSAM specified in the README.md. More similar issues can be found &lt;a href=&#34;https://github.com/TixiaoShan/LIO-SAM/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;gps odometry unavailable&lt;/strong&gt;: it is generally caused due to unavailable transform between message frame_ids and robot frame_id (for example: transform should be available from &#34;imu_frame_id&#34; and &#34;gps_frame_id&#34; to &#34;base_link&#34; frame. Please read the Robot Localization documentation found &lt;a href=&#34;http://docs.ros.org/en/melodic/api/robot_localization/html/preparing_sensor_data.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Paper&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for citing &lt;a href=&#34;https://raw.githubusercontent.com/TixiaoShan/LIO-SAM/master/config/doc/paper.pdf&#34;&gt;LIO-SAM (IROS-2020)&lt;/a&gt; if you use any of this code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{liosam2020shan,&#xA;  title={LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping},&#xA;  author={Shan, Tixiao and Englot, Brendan and Meyers, Drew and Wang, Wei and Ratti, Carlo and Rus Daniela},&#xA;  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},&#xA;  pages={5135-5142},&#xA;  year={2020},&#xA;  organization={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Part of the code is adapted from &lt;a href=&#34;https://github.com/RobustFieldAutonomyLab/LeGO-LOAM&#34;&gt;LeGO-LOAM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{legoloam2018shan,&#xA;  title={LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain},&#xA;  author={Shan, Tixiao and Englot, Brendan},&#xA;  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},&#xA;  pages={4758-4765},&#xA;  year={2018},&#xA;  organization={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/TixiaoShan/LIO-SAM/issues/104&#34;&gt;Bug within imuPreintegration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Package&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YJZLuckyBoy/liorf&#34;&gt;liorf&lt;/a&gt; LIO-SAM with 6-axis IMU and more lidar support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chennuo0125-HIT/lidar_imu_calib&#34;&gt;Lidar-IMU calibration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gisbi-kim/SC-LIO-SAM&#34;&gt;LIO-SAM with Scan Context&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JokerJohn/LIO_SAM_6AXIS&#34;&gt;LIO-SAM with 6-axis IMU&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LIO-SAM is based on LOAM (J. Zhang and S. Singh. LOAM: Lidar Odometry and Mapping in Real-time).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>