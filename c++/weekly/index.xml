<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-29T01:48:29Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/TensorRT-LLM</title>
    <updated>2023-10-29T01:48:29Z</updated>
    <id>tag:github.com,2023-10-29:/NVIDIA/TensorRT-LLM</id>
    <link href="https://github.com/NVIDIA/TensorRT-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;TensorRT-LLM&lt;/h1&gt; &#xA; &lt;h4&gt; A TensorRT Toolbox for Large Language Models &lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-31012/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10.12-green&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/cuda-12.2-green&#34; alt=&#34;cuda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TRT-9.1-green&#34; alt=&#34;trt&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/setup.py&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-0.5.0-green&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-blue&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/architecture.md&#34;&gt;Architecture&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/performance.md&#34;&gt;Results&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;Examples&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;div align=&#34;left&#34;&gt; &#xA;  &lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#tensorrt-llm-overview&#34;&gt;TensorRT-LLM Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#support-matrix&#34;&gt;Support Matrix&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#advanced-topics&#34;&gt;Advanced Topics&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#quantization&#34;&gt;Quantization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#in-flight-batching&#34;&gt;In-flight Batching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#attention&#34;&gt;Attention&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#graph-rewriting&#34;&gt;Graph Rewriting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#benchmarking&#34;&gt;Benchmarking&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#release-notes&#34;&gt;Release Notes&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#known-issues&#34;&gt;Known issues&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;TensorRT-LLM Overview&lt;/h2&gt; &#xA;  &lt;p&gt;TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build &lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT&lt;/a&gt; engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a &lt;a href=&#34;https://github.com/triton-inference-server/tensorrtllm_backend&#34;&gt;backend&lt;/a&gt; for integration with the &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;NVIDIA Triton Inference Server&lt;/a&gt;; a production-quality system to serve LLMs. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#tensor-parallelism&#34;&gt;Tensor Parallelism&lt;/a&gt; and/or &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#pipeline-parallelism&#34;&gt;Pipeline Parallelism&lt;/a&gt;).&lt;/p&gt; &#xA;  &lt;p&gt;The Python API of TensorRT-LLM is architectured to look similar to the &lt;a href=&#34;https://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; API. It provides users with a &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/tensorrt_llm/functional.py&#34;&gt;functional&lt;/a&gt; module containing functions like &lt;code&gt;einsum&lt;/code&gt;, &lt;code&gt;softmax&lt;/code&gt;, &lt;code&gt;matmul&lt;/code&gt; or &lt;code&gt;view&lt;/code&gt;. The &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/tensorrt_llm/layers&#34;&gt;layers&lt;/a&gt; module bundles useful building blocks to assemble LLMs; like an &lt;code&gt;Attention&lt;/code&gt; block, a &lt;code&gt;MLP&lt;/code&gt; or the entire &lt;code&gt;Transformer&lt;/code&gt; layer. Model-specific components, like &lt;code&gt;GPTAttention&lt;/code&gt; or &lt;code&gt;BertAttention&lt;/code&gt;, can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/tensorrt_llm/models&#34;&gt;models&lt;/a&gt; module.&lt;/p&gt; &#xA;  &lt;p&gt;TensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs. See below for a list of supported &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#Models&#34;&gt;models&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes (see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;&lt;code&gt;examples/gpt&lt;/code&gt;&lt;/a&gt; for concrete examples). TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the &lt;a href=&#34;https://arxiv.org/abs/2211.10438&#34;&gt;SmoothQuant&lt;/a&gt; technique.&lt;/p&gt; &#xA;  &lt;p&gt;For a more detailed presentation of the software architecture and the key concepts used in TensorRT-LLM, we recommend you to read the following &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/architecture.md&#34;&gt;document&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;Installation&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;em&gt;For Windows installation, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/windows/&#34;&gt;&lt;code&gt;Windows/&lt;/code&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;TensorRT-LLM must be built from source, instructions can be found &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/installation.md&#34;&gt;here&lt;/a&gt;. An image of a Docker container with TensorRT-LLM and its Triton Inference Server Backend will be made available soon.&lt;/p&gt; &#xA;  &lt;p&gt;The remaining commands in that document must be executed from the TensorRT-LLM container.&lt;/p&gt; &#xA;  &lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;  &lt;p&gt;To create a TensorRT engine for an existing model, there are 3 steps:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download pre-trained weights,&lt;/li&gt; &#xA;   &lt;li&gt;Build a fully-optimized engine of the model,&lt;/li&gt; &#xA;   &lt;li&gt;Deploy the engine.&lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA;  &lt;p&gt;The following sections show how to use TensorRT-LLM to run the &lt;a href=&#34;https://huggingface.co/bigscience/bloom-560m&#34;&gt;BLOOM-560m&lt;/a&gt; model.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;0. In the BLOOM folder&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;Inside the Docker container, you have to install the requirements:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r examples/bloom/requirements.txt&#xA;git lfs install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;1. Download the model weights from HuggingFace&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;From the BLOOM example folder, you must download the weights of the model.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples/bloom&#xA;rm -rf ./bloom/560M&#xA;mkdir -p ./bloom/560M &amp;amp;&amp;amp; git clone https://huggingface.co/bigscience/bloom-560m ./bloom/560M&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;2. Build the engine&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Single GPU on BLOOM 560M&#xA;python build.py --model_dir ./bloom/560M/ \&#xA;                --dtype float16 \&#xA;                --use_gemm_plugin float16 \&#xA;                --use_gpt_attention_plugin float16 \&#xA;                --output_dir ./bloom/560M/trt_engines/fp16/1-gpu/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;See the BLOOM &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bloom&#34;&gt;example&lt;/a&gt; for more details and options regarding the &lt;code&gt;build.py&lt;/code&gt; script.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;3. Run&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;p&gt;The &lt;code&gt;summarize.py&lt;/code&gt; script can be used to perform the summarization of articles from the CNN Daily dataset:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python summarize.py --test_trt_llm \&#xA;                    --hf_model_location ./bloom/560M/ \&#xA;                    --data_type fp16 \&#xA;                    --engine_dir ./bloom/560M/trt_engines/fp16/1-gpu/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;More details about the script and how to run the BLOOM model can be found in the example &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bloom&#34;&gt;folder&lt;/a&gt;. Many more &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/#models&#34;&gt;models&lt;/a&gt; than BLOOM are implemented in TensorRT-LLM. They can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;  &lt;h2&gt;Support Matrix&lt;/h2&gt; &#xA;  &lt;p&gt;TensorRT-LLM optimizes the performance of a range of well-known models on NVIDIA GPUs. The following sections provide a list of supported GPU architectures as well as important features implemented in TensorRT-LLM.&lt;/p&gt; &#xA;  &lt;h3&gt;Devices&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM is rigorously tested on the following GPUs:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/h100/&#34;&gt;H100&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/l40s/&#34;&gt;L40S&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/a100/&#34;&gt;A100&lt;/a&gt;/&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/products/a30-gpu/&#34;&gt;A30&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/data-center/v100/&#34;&gt;V100&lt;/a&gt; (experimental)&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;If a GPU is not listed above, it is important to note that TensorRT-LLM is expected to work on GPUs based on the Volta, Turing, Ampere, Hopper and Ada Lovelace architectures. Certain limitations may, however, apply.&lt;/p&gt; &#xA;  &lt;h3&gt;Precision&lt;/h3&gt; &#xA;  &lt;p&gt;Various numerical precisions are supported in TensorRT-LLM. The support for some of those numerical features require specific architectures:&lt;/p&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;FP32&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;FP16&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;BF16&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;FP8&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;INT8&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;INT4&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Volta (SM70)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Turing (SM75)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Ampere (SM80, SM86)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;N&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Ada-Lovelace (SM89)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Hopper (SM90)&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Y&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;p&gt;In this release of TensorRT-LLM, the support for FP8 and quantized data types (INT8 or INT4) is not implemented for all the models. See the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/precision.md&#34;&gt;precision&lt;/a&gt; document and the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;examples&lt;/a&gt; folder for additional details.&lt;/p&gt; &#xA;  &lt;h3&gt;Key Features&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM contains examples that implement the following features.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi-head Attention(&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;MHA&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Multi-query Attention (&lt;a href=&#34;https://arxiv.org/abs/1911.02150&#34;&gt;MQA&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Group-query Attention(&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;GQA&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;In-flight Batching&lt;/li&gt; &#xA;   &lt;li&gt;Paged KV Cache for the Attention&lt;/li&gt; &#xA;   &lt;li&gt;Tensor Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;Pipeline Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;INT4/INT8 Weight-Only Quantization (W4A16 &amp;amp; W8A16)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.10438&#34;&gt;SmoothQuant&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPTQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.05433&#34;&gt;FP8&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Greedy-search&lt;/li&gt; &#xA;   &lt;li&gt;Beam-search&lt;/li&gt; &#xA;   &lt;li&gt;RoPE&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;p&gt;In this release of TensorRT-LLM, some of the features are not enabled for all the models listed in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;  &lt;h3&gt;Models&lt;/h3&gt; &#xA;  &lt;p&gt;The list of supported models is:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/baichuan&#34;&gt;Baichuan&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bert&#34;&gt;Bert&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/blip2&#34;&gt;Blip2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/bloom&#34;&gt;BLOOM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/chatglm6b&#34;&gt;ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/chatglm2-6b/&#34;&gt;ChatGLM2-6B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/falcon&#34;&gt;Falcon&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;GPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gptj&#34;&gt;GPT-J&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;GPT-Nemo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gptneox&#34;&gt;GPT-NeoX&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/llama&#34;&gt;LLaMA-v2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/mpt&#34;&gt;MPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;SantaCoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/examples/gpt&#34;&gt;StarCoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;Performance&lt;/h2&gt; &#xA;  &lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/performance.md&#34;&gt;performance&lt;/a&gt; page for performance numbers. That page contains measured numbers for four variants of popular models (GPT-J, LLAMA-7B, LLAMA-70B, Falcon-180B), measured on the H100, L40S and A100 GPU(s).&lt;/p&gt; &#xA;  &lt;h2&gt;Advanced Topics&lt;/h2&gt; &#xA;  &lt;h3&gt;Quantization&lt;/h3&gt; &#xA;  &lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/precision.md&#34;&gt;document&lt;/a&gt; describes the different quantization methods implemented in TensorRT-LLM and contains a support matrix for the different models.&lt;/p&gt; &#xA;  &lt;h3&gt;In-flight Batching&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM supports in-flight batching of requests (also known as continuous batching or iteration-level batching). It&#39;s a &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/batch_manager.md&#34;&gt;technique&lt;/a&gt; that aims at reducing wait times in queues, eliminating the need for padding requests and allowing for higher GPU utilization.&lt;/p&gt; &#xA;  &lt;h3&gt;Attention&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM implements several variants of the Attention mechanism that appears in most the Large Language Models. This &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/gpt_attention.md&#34;&gt;document&lt;/a&gt; summarizes those implementations and how they are optimized in TensorRT-LLM.&lt;/p&gt; &#xA;  &lt;h3&gt;Graph Rewriting&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM uses a declarative approach to define neural networks and contains techniques to optimize the underlying graph. For more details, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/docs/source/graph-rewriting.md&#34;&gt;doc&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;  &lt;p&gt;TensorRT-LLM provides &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/benchmarks/cpp/README.md&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/release/0.5.0/benchmarks/python/README.md&#34;&gt;Python&lt;/a&gt; tools to perform benchmarking. Note, however, that it is recommended to use the C++ version.&lt;/p&gt; &#xA;  &lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;It&#39;s recommended to add options &lt;code&gt;–shm-size=1g –ulimit memlock=-1&lt;/code&gt; to the docker or nvidia-docker run command. Otherwise you may see NCCL errors when running multiple GPU inferences. See &lt;a href=&#34;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#errors&#34;&gt;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#errors&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;When building models, memory-related issues such as&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;pre&gt;&lt;code&gt;[09/23/2023-03:13:00] [TRT] [E] 9: GPTLMHeadModel/layers/0/attention/qkv/PLUGIN_V2_Gemm_0: could not find any supported formats consistent with input/output data types&#xA;[09/23/2023-03:13:00] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (GPTLMHeadModel/layers/0/attention/qkv/PLUGIN_V2_Gemm_0: could not find any supported formats consistent with input/output data types)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;may happen. One possible solution is to reduce the amount of memory needed by reducing the maximum batch size, input and output lengths. Another option is to enable plugins, for example: &lt;code&gt;--use_gpt_attention_plugin&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;h2&gt;Release notes&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TensorRT-LLM requires TensorRT 9.1.0.4 and 23.08 containers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h3&gt;Change Log&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TensorRT-LLM v0.5.0 is the first public release.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h3&gt;Known Issues&lt;/h3&gt; &#xA;  &lt;h3&gt;Report Issues&lt;/h3&gt; &#xA;  &lt;p&gt;You can use GitHub issues to report issues with TensorRT-LLM.&lt;/p&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>Alex313031/thorium</title>
    <updated>2023-10-29T01:48:29Z</updated>
    <id>tag:github.com,2023-10-29:/Alex313031/thorium</id>
    <link href="https://github.com/Alex313031/thorium" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chromium fork named after radioactive element No. 90. Windows and MacOS/Raspi/Android/Special builds are in different repositories, links are towards the top of the README.md.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/v/tag/alex313031/thorium?label=Version%3A&#34; alt=&#34;GitHub tag (latest SemVer)&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://img.shields.io/github/license/alex313031/thorium?color=green&amp;amp;label=License%3A&#34; alt=&#34;GitHub&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://img.shields.io/github/commit-activity/w/alex313031/thorium?color=blueviolet&amp;amp;label=Commit%20Activity%3A&#34; alt=&#34;GitHub commit activity&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://img.shields.io/reddit/subreddit-subscribers/ChromiumBrowser?style=social&#34; alt=&#34;Subreddit subscribers&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Thorium&lt;/h1&gt; &#xA;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/NEW/thorium_ver_2048_grey_old.png&#34;&gt; &#xA;&lt;h2&gt;Chromium fork for linux named after &lt;a href=&#34;https://en.wikipedia.org/wiki/Thorium&#34;&gt;radioactive element No. 90&lt;/a&gt;. Windows/MacOS/RasPi/Android/Other builds see below.&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Always built with the latest stable version of &lt;a href=&#34;https://www.chromium.org/&#34;&gt;Chromium&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Intended to behave like and have the featureset of Google Chrome, with differences/patches/enhancements listed below.&lt;/li&gt; &#xA; &lt;li&gt;Includes &lt;a href=&#34;https://www.widevine.com/&#34;&gt;Widevine&lt;/a&gt;, &lt;a href=&#34;https://tools.woolyss.com/html5-audio-video-tester/&#34;&gt;All Codecs&lt;/a&gt;, Chrome Plugins, as well as thinLTO, CFlag, LDFlag, LLVM Loop, and PGO compiler optimizations. It is built with &lt;a href=&#34;https://en.wikipedia.org/wiki/SSE4&#34;&gt;SSE4&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&#34;&gt;AVX&lt;/a&gt;, and &lt;a href=&#34;https://en.wikipedia.org/wiki/AES_instruction_set&#34;&gt;AES&lt;/a&gt;, so it won&#39;t launch on CPU&#39;s below 2nd gen Core or AMD FX, but benefits from Advanced Vector EXtensions. If your CPU lacks AVX, you can use builds from &lt;a href=&#34;https://github.com/Alex313031/Thorium-Special&#34;&gt;Thorium Special&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Builds &amp;nbsp;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/winflag_animated.gif&#34; width=&#34;34&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/AVX2.png&#34; width=&#34;48&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/apple.png&#34; width=&#34;30&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/Android_Robot.svg?sanitize=true&#34; width=&#34;26&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/Raspberry_Pi_Logo.svg?sanitize=true&#34; width=&#34;24&#34;&gt; &amp;nbsp;&lt;img src=&#34;https://raw.githubusercontent.com/Alex313031/thorium-win7/main/logos/STAGING/win7/compatible-with-windows-7.png&#34; width=&#34;28&#34;&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&amp;nbsp;– Windows builds are here &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/Thorium-Win&#34;&gt;Thorium Win&lt;/a&gt; &lt;br&gt; &amp;nbsp;– AVX2 Builds for Windows and Linux &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/Thorium-AVX2&#34;&gt;Thorium AVX2&lt;/a&gt; &lt;br&gt; &amp;nbsp;– MacOS (M1 and X64) builds are located at &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/Thorium-Mac&#34;&gt;Thorium Mac&lt;/a&gt; &lt;br&gt; &amp;nbsp;– Android (arm32 &amp;amp; arm64) builds are located at &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/Thorium-Android&#34;&gt;Thorium Android&lt;/a&gt; I might also occasionally post x86 builds. &lt;br&gt; &amp;nbsp;– Raspberry Pi builds are located at &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/Thorium-Raspi&#34;&gt;Thorium Raspi&lt;/a&gt; For the Pi 3B/3B+ and Pi 4/400. &lt;br&gt; &amp;nbsp;– Special builds are located at &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/Thorium-Special&#34;&gt;Thorium Special&lt;/a&gt; You can find SSE3 builds for CPUs without AVX here. &lt;br&gt; &amp;nbsp;– Thorium Website with deb repo for auto-updating on Linux &amp;gt; &lt;a href=&#34;https://thorium.rocks/&#34;&gt;https://thorium.rocks/&lt;/a&gt; &lt;br&gt; &amp;nbsp;– &lt;strong&gt;NEW&lt;/strong&gt;: Windows 7 / 8 / 8.1 / Server 2012 builds in &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/thorium-win7&#34;&gt;Thorium Win7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;FEATURES &amp;amp; DIFFERENCES BETWEEN CHROMIUM AND THORIUM &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/NEW/bulb_light.svg#gh-dark-mode-only&#34;&gt; &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/NEW/bulb_dark.svg#gh-light-mode-only&#34;&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Various compiler flags that improve performance and target &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&#34;&gt;AVX&lt;/a&gt; CPU&#39;s (read &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/infra/PATCHES.md&#34;&gt;PATCHES.md&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Experimental &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP&#34;&gt;MPEG-DASH&lt;/a&gt; support.&lt;/li&gt; &#xA;  &lt;li&gt;HEVC/H.265 support on Linux and Windows.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://jpeg.org/jpegxl/&#34;&gt;JPEG XL&lt;/a&gt; Image File Format turned on by default.&lt;/li&gt; &#xA;  &lt;li&gt;Enable &lt;a href=&#34;https://support.google.com/chrome/answer/10538231?hl&#34;&gt;Live Caption&lt;/a&gt; (SODA) on all builds.&lt;/li&gt; &#xA;  &lt;li&gt;Experimental PDF annotation support (called &#34;Ink&#34; on ChromiumOS). # DISABLED FOR NOW BECAUSE OF CRASHES.&lt;/li&gt; &#xA;  &lt;li&gt;Patches from Debian including font rendering patch, VAAPI Patch, Intel HD support patch, native notifications patch, title bar patch, and... the VDPAU Patch!! (Rejoice Nvidia users)&lt;/li&gt; &#xA;  &lt;li&gt;VAAPI on Wayland Patch (Thanks AUR and @pierro78)&lt;/li&gt; &#xA;  &lt;li&gt;Audio Sandbox patch.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/DNS_over_HTTPS&#34;&gt;DoH&lt;/a&gt; (DNS over HTTPS) patches from Bromite.&lt;/li&gt; &#xA;  &lt;li&gt;Enable &lt;a href=&#34;https://allaboutdnt.com/&#34;&gt;Do Not Track&lt;/a&gt; by default patch from Vanadium.&lt;/li&gt; &#xA;  &lt;li&gt;Show full URLs including trailing slashes in address bar by default.&lt;/li&gt; &#xA;  &lt;li&gt;Disable &lt;a href=&#34;https://en.wikipedia.org/wiki/Federated_Learning_of_Cohorts&#34;&gt;FLOC&lt;/a&gt; patch.&lt;/li&gt; &#xA;  &lt;li&gt;Disable annoying Google API Key Infobar warning (you can still use API Keys to enable sync) from Ungoogled Chromium.&lt;/li&gt; &#xA;  &lt;li&gt;Disable annoying Default Browser Infobar warning.&lt;/li&gt; &#xA;  &lt;li&gt;Adds &lt;a href=&#34;https://duckduckgo.com/&#34;&gt;DuckDuckGo&lt;/a&gt;, &lt;a href=&#34;https://search.brave.com/&#34;&gt;Brave Search&lt;/a&gt;, &lt;a href=&#34;https://www.ecosia.org/&#34;&gt;Ecosia&lt;/a&gt;, &lt;a href=&#34;https://www.ask.com/&#34;&gt;Ask.com&lt;/a&gt;, and &lt;a href=&#34;https://yandex.com/&#34;&gt;Yandex.com&lt;/a&gt; in US and other locales, along with the normal search engines.&lt;/li&gt; &#xA;  &lt;li&gt;Always use the local NTP (New Tab Page) regardless of search engine.&lt;/li&gt; &#xA;  &lt;li&gt;Fix icons when distilling page content in &lt;a href=&#34;https://www.howtogeek.com/423643/how-to-use-google-chromes-hidden-reader-mode/&#34;&gt;Reader Mode&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Enable new Menu UI when right clicking the Reload button. (Adds &#39;Normal Reload&#39;, &#39;Hard Reload&#39;, and &#39;Clear Cache and Hard Reload&#39;)&lt;/li&gt; &#xA;  &lt;li&gt;Home button and Chrome Labs shown by Default.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://www.chromium.org/developers/design-documents/dns-prefetching/&#34;&gt;Prefetch&lt;/a&gt; settings updated to respect privacy.&lt;/li&gt; &#xA;  &lt;li&gt;Patches for &lt;a href=&#34;https://chromium.googlesource.com/chromium/src/tools/gn/+/48062805e19b4697c5fbd926dc649c78b6aaa138/README.md&#34;&gt;GN&lt;/a&gt; and &lt;a href=&#34;https://chromium.googlesource.com/chromium/src/+/HEAD/docs/design/sandbox.md&#34;&gt;chrome_sandbox&lt;/a&gt; when building.&lt;/li&gt; &#xA;  &lt;li&gt;Remove the addition of the Chrome APT sources.list during installation.&lt;/li&gt; &#xA;  &lt;li&gt;Widevine CDM Patch for Linux.&lt;/li&gt; &#xA;  &lt;li&gt;GTK auto dark mode patch&lt;/li&gt; &#xA;  &lt;li&gt;Various new flags either developed from scratch, or added from Ungoogled Chromium. See &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/infra/PATCHES.md&#34;&gt;PATCHES.md&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Enable Parallel Downloading by Default.&lt;/li&gt; &#xA;  &lt;li&gt;Inclusion of &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/pak.png&#34; width=&#34;16&#34;&gt; &lt;a href=&#34;https://github.com/Alex313031/thorium/tree/main/pak_src#readme&#34;&gt;pak&lt;/a&gt; a utility for packing and unpacking the &lt;a href=&#34;https://textslashplain.com/2022/05/03/chromium-internals-pak-files/&#34;&gt;*.pak&lt;/a&gt; files in Thorium or any other Chromium based browser.&lt;/li&gt; &#xA;  &lt;li&gt;Logo and Branding/Naming changed to the Thorium logo, Thorium name, and &#34;Alex313031&#34; being appended to &#34;The Chromium Authors&#34; in credits, etc.&lt;/li&gt; &#xA;  &lt;li&gt;.desktop file includes useful cmdline flags that enable experimental or useful features. (See &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/infra/PATCHES.md&#34;&gt;PATCHES.md&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Includes installer patches and files to include &lt;a href=&#34;https://chromedriver.chromium.org/home&#34;&gt;ChromeDriver&lt;/a&gt; and &lt;em&gt;thorium_shell&lt;/em&gt; &lt;a href=&#34;https://chromium.googlesource.com/chromium/src/+/HEAD/docs/testing/web_tests_in_content_shell.md&#34;&gt;(content_shell)&lt;/a&gt;, with a .desktop file being provided for thorium_shell (named thorium-shell.desktop and shows in desktop environments as Thorium Content Shell). These are also included in the Windows releases, but it doesn&#39;t make a shorcut, although a .png and .ico is in the install directory for you to make your own shortcut with an icon. You can also run content_shell with the command thorium-shell (custom wrapper for it, located in /usr/bin/). You can run ChromeDriver at /usr/bin/chromedriver or chromedriver.exe on Windows. Also, patches for abseil library and mini_installer when building with AVX on Windows.&lt;/li&gt; &#xA;  &lt;li&gt;Right clicking the launcher after install gives three additional desktop actions, one to open thorium-shell, another to open in Safe Mode which disables any flags one has set in chrome://flags until the next launch, and lastly to open in Dark Mode which appends the --force-dark-mode flag.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more info, read the &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/infra/PATCHES.md&#34;&gt;PATCHES.md&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Known bugs are in the &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/infra/BUGS.md&#34;&gt;BUGS.md&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;A list of Chromium command line flags can be found at &amp;gt; &lt;a href=&#34;https://peter.sh/experiments/chromium-command-line-switches&#34;&gt;https://peter.sh/experiments/chromium-command-line-switches&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/NEW/build_light.svg#gh-dark-mode-only&#34;&gt; &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/NEW/build_dark.svg#gh-light-mode-only&#34;&gt;&lt;/h2&gt; &#xA;&lt;p&gt;See &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/docs/BUILDING.md&#34;&gt;https://github.com/Alex313031/thorium/blob/main/docs/BUILDING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Debugging &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/bug.svg?sanitize=true&#34; width=&#34;28&#34;&gt;&lt;/h2&gt; &#xA;&lt;p&gt;See &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/thorium/tree/main/infra/DEBUG#readme&#34;&gt;https://github.com/Alex313031/thorium/tree/main/infra/DEBUG#readme&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&amp;nbsp;− &lt;a href=&#34;https://www.reddit.com/r/ChromiumBrowser/&#34;&gt;https://www.reddit.com/r/ChromiumBrowser/&lt;/a&gt; is a subreddit I made for Thorium and general Thorium/Chromium discussion, &lt;a href=&#34;https://thorium.rocks/&#34;&gt;https://thorium.rocks/&lt;/a&gt; is the website I made for it, and &lt;a href=&#34;https://alex313031.blogspot.com/&#34;&gt;https://alex313031.blogspot.com/&lt;/a&gt; is a blog I made relating to Thorium/ThoriumOS. &lt;br&gt; &amp;nbsp;− I also build ChromiumOS (now called ThoriumOS) with Thorium, Codecs, Widevine, linux-firmware/modules, and extra packages at &amp;gt; &lt;a href=&#34;https://github.com/Alex313031/ChromiumOS/&#34;&gt;https://github.com/Alex313031/ChromiumOS/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;− Thanks to &lt;a href=&#34;https://github.com/robrich999/&#34;&gt;https://github.com/robrich999/&lt;/a&gt; for some info and fixes that went into this project.&lt;br&gt; &amp;nbsp;− Thanks to &lt;a href=&#34;https://github.com/midzer/&#34;&gt;https://github.com/midzer/&lt;/a&gt; for support and helping with builds. &lt;br&gt; &amp;nbsp;− Also thanks to &lt;a href=&#34;https://github.com/bromite/bromite&#34;&gt;https://github.com/bromite/bromite&lt;/a&gt;, &lt;a href=&#34;https://github.com/saiarcot895/chromium-ubuntu-build&#34;&gt;https://github.com/saiarcot895/chromium-ubuntu-build&lt;/a&gt;, &lt;a href=&#34;https://github.com/Eloston/ungoogled-chromium&#34;&gt;https://github.com/Eloston/ungoogled-chromium&lt;/a&gt;, &lt;a href=&#34;https://github.com/GrapheneOS/Vanadium&#34;&gt;https://github.com/GrapheneOS/Vanadium&lt;/a&gt;, and &lt;a href=&#34;https://github.com/iridium-browser/iridium-browser&#34;&gt;https://github.com/iridium-browser/iridium-browser&lt;/a&gt; for patch code. &lt;br&gt; &amp;nbsp;− The pak_src dir, and the binaries in &lt;em&gt;pack_src/bin&lt;/em&gt; are credited to @freeer &lt;a href=&#34;https://github.com/myfreeer/chrome-pak-customizer/&#34;&gt;https://github.com/myfreeer/chrome-pak-customizer/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp; &lt;strong&gt;NOTE:&lt;/strong&gt; &lt;a href=&#34;https://github.com/Alex313031/thorium/raw/main/infra/libpepflashplayer.so&#34;&gt;&lt;em&gt;libpepflashplayer.so&lt;/em&gt;&lt;/a&gt; is included for posterity and can be used to enable Adobe Flash on older Chromium releases. ʘ‿ʘ&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Thanks for using Thorium!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/Thorium90_504.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/GitHub/GitHub-Mark-Light-32px.png#gh-dark-mode-only&#34;&gt; &lt;img src=&#34;https://github.com/Alex313031/thorium/raw/main/logos/STAGING/GitHub/GitHub-Mark-32px.png#gh-light-mode-only&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Dr-TSNG/ZygiskNext</title>
    <updated>2023-10-29T01:48:29Z</updated>
    <id>tag:github.com,2023-10-29:/Dr-TSNG/ZygiskNext</id>
    <link href="https://github.com/Dr-TSNG/ZygiskNext" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Standalone implementation of Zygisk&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zygisk Next&lt;/h1&gt; &#xA;&lt;p&gt;Standalone implementation of Zygisk, providing Zygisk API support for KernelSU and a replacement of Magisk&#39;s built-in Zygisk.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;General&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No multiple root implementation installed&lt;/li&gt; &#xA; &lt;li&gt;SELinux enforcing: Zygisk Next rely on SELinux to prevent &lt;code&gt;vold&lt;/code&gt; from aborting our fuse connection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;KernelSU&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minimal KernelSU version: 10940&lt;/li&gt; &#xA; &lt;li&gt;Minimal ksud version: 10942&lt;/li&gt; &#xA; &lt;li&gt;Kernel has full SELinux patch support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Magisk&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minimal version: 26300&lt;/li&gt; &#xA; &lt;li&gt;Built-in Zygisk turned off&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;PROCESS_ON_DENYLIST&lt;/code&gt; cannot be flagged correctly for isolated processes on Magisk DenyList currently.&lt;/p&gt; &#xA;&lt;p&gt;Zygisk Next only guarantees the same behavior of Zygisk API, but will NOT ensure Magisk&#39;s internal features.&lt;/p&gt;</summary>
  </entry>
</feed>