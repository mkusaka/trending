<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-31T01:58:42Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZhaJiHu/Cubli_Mini</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/ZhaJiHu/Cubli_Mini</id>
    <link href="https://github.com/ZhaJiHu/Cubli_Mini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cubli_Mini项目&lt;/h1&gt; &#xA;&lt;p&gt;本项目ID是来源于苏黎世联邦理工学院的Cubli，电机驱动使用的是Simple FOC。低成本、小型化、简化后作为我的首个开源项目&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;项目完全开源，任何人都可以根据本Github内容自行白嫖，当然也可以顺手点个Star&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;介绍&lt;/h3&gt; &#xA;&lt;p&gt;本项目在原版Cubli的基础上去掉主动刹车，缩小体积和一定的简化。需要手动调整到平衡点附近，才可以实现边和点平衡。Cubli_Mini是一个10x10x10CM的立方体，集电机驱动、充放电一体化。满电状态下，点平衡在无扰动下续航≥5小时。&lt;/p&gt; &#xA;&lt;p&gt;项目成本：≤800RMB&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;视频介绍：&lt;/strong&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1UR4y1c74B?spm_id_from=333.999.0.0&#34;&gt;https://www.bilibili.com/video/BV1UR4y1c74B?spm_id_from=333.999.0.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;YouTube：&lt;a href=&#34;https://youtu.be/JwiJBd6I_vY&#34;&gt;https://youtu.be/JwiJBd6I_vY&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ZhaJiHu/Cubli_Mini/raw/master/5.Doc/Pic/Cubli_Mini.JPG&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;资料更新说明（2022/5/22）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PCB BOM表添加采购链接&lt;/li&gt; &#xA; &lt;li&gt;更新&amp;lt;写给小白的白嫖教程.pdf&amp;gt;&amp;lt;代码和工程&amp;gt;，图文并茂说明如何解决头文件错误问题&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;资料更新说明（2022/5/21）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/6.Process/钣金件/，增加对应的step文件&lt;/li&gt; &#xA; &lt;li&gt;/6.Process/3D打印/，增加对应的step文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;安全说明&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;动量轮比较危险，要注意安全，手远离动量轮&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;/2.Firmware/MCU1/control/cubli_mini.h，修改VOLTAGE_LIMIT的电压可以限制飞轮的最大输出，开始使用建议降低该参数，可以从4，5V开始&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;资料更新说明（2022/5/19)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;最新发现动量轮有两个角忘了倒角了，理论上会影响大转速下的动平衡，因此对此进行修改。已经下单打样的同学也可以正常使用，因为电机KV值很低，我现在使用的也是漏倒角的动量轮。更新了/4.Model/Cubli_Mini.STEP文件。并且更新了/6.Process/钣金件/YC.JGJ.000002.4.*，文件版本号更新到.4&lt;/li&gt; &#xA; &lt;li&gt;还有说一下结构件的采购数量和采购链接都在/6.Process/BOM/结构BOM.xls&lt;/li&gt; &#xA; &lt;li&gt;物料采购的时候注意编码器模块需要3套物料&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;资料说明&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3D模型设计源文件&lt;/li&gt; &#xA; &lt;li&gt;主控模块、IMU模块、编码器模块、下载模块硬件工程&lt;/li&gt; &#xA; &lt;li&gt;主控1、主控2软件工程 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;主控模块使用两颗ESP32&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;网络调试助手 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WIFI调参使用&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;各类文档说明 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;对项目的一些说明，如调参，按键操作，控制原理等&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;打样文件 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;STL格式3D打印文件&lt;/li&gt; &#xA;   &lt;li&gt;DWG格式钣金文件&lt;/li&gt; &#xA;   &lt;li&gt;BOM表 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;结构BOM表&lt;/li&gt; &#xA;     &lt;li&gt;PCB元器件BOM表&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;结构设计&lt;/h3&gt; &#xA;&lt;p&gt;由于我只学了一周的SW，因此有些零件会漏画一些内容，如螺丝螺纹，铜柱内螺纹啥的&lt;/p&gt; &#xA;&lt;p&gt;外框等钣金件采用的是成本较低的玻纤板，在保证刚性的情况下降低成本。动量轮采用的是304不锈钢，可以在小体积下提供较大的转动惯量。辅以少量3D打印件组成。连接件为了美观使用的是阳极氧化铝柱，当然也可以自行使用成本更低的铜柱。整体结构件成本为350块以内，铝柱更换铜柱可以降低到290以内&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cubli_Mini的八个角点，是由两种不同的角组成，名字为角1和角2，各4个&lt;/li&gt; &#xA; &lt;li&gt;外框和角的连接处的结构不是45°对称，所以是有安装方向的，详情看DoC/结构和安装说明&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;电路模块&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;主控模块：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;主控模块集成了3个电机驱动电路，电池充放电路，降压稳压电路，CAN通讯电路&lt;/li&gt; &#xA; &lt;li&gt;由于单颗ESP32只有6路电机PWM，因此使用了两颗ESP32模块，使用了CAN作为MCU间的通讯方式，由于两颗ESP32都是集成在同一个主板上，因此为了节省IC成本，可以把CAN修改为IIC，UART，SPI等各类短路径通讯方式，但是需要修改主控软硬件，有想法的同学可以自行修改&lt;/li&gt; &#xA; &lt;li&gt;电机驱动电路参考Simple FOC开源电路，Simple FOC链接：&lt;a href=&#34;https://github.com/simplefoc&#34;&gt;https://github.com/simplefoc&lt;/a&gt;。电池充放电参考：&lt;a href=&#34;https://oshwhub.com/muyan2020/zi-ping-heng-di-lai-luo-san-jiao_10-10-ban-ben_copy&#34;&gt;https://oshwhub.com/qqj1228/zi-ping-heng-di-lai-luo-san-jiao_10-10-ban-ben&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMU模块：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用最为常见的MPU6050&lt;/li&gt; &#xA; &lt;li&gt;为了防止应力，对IMU所在区域挖槽，减少安装应力。但使用了3颗螺丝硬连接，并不能避免应力问题，改进方案为使用软垫片并只拧紧一个螺丝进一步减少应力&lt;/li&gt; &#xA; &lt;li&gt;由于IMU安装要求比较高，因此在贴片时，尽量保证MPU6050与PCB平行，且位于焊盘正中央&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;编码器模块：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;采用的是成本较低的AS5600，在低速情况下便宜好用&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;自动下载模块：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;为了尽量减少主控的面积，因此单独把自动下载电路摘出来，参考官方自动下载电路&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;正常情况下，自动下载模块焊接排针，主控不焊接任何排针/排母&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;固件说明&lt;/h3&gt; &#xA;&lt;p&gt;Cubli_Mini的核心内容为Simple FOC电机驱动库以及点平衡控制算法，运行在MCU1中。MCU2只是使用了Simple FOC库对电机2和电机3的控制，利用CAN反馈和接收控制命令。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MCU1固件说明：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;bsp：基于arduino库封装了一下LED，KEY，ADC，非易失储存等内容&lt;/li&gt; &#xA; &lt;li&gt;comm：公用类，目前只封装了一个时间类，获取时间差，非延时等待等简单内容&lt;/li&gt; &#xA; &lt;li&gt;control：核心库，内容为平衡算法和调参接口，调参参考Simple FOC格式。为方便使用，使用的是字符串，不使用传统的16进制协议&lt;/li&gt; &#xA; &lt;li&gt;main：main文件，驱动的初始化和FreeRTOS任务创建 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ESP32官方把WIFI，蓝牙类运行在Core0。因此在创建任务中，把WIFI任务创建在Core0中，其他则创建在Core1。只有运行于不同核心任务的数据交互我才加了互斥锁。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;目前WIFI SSID和PSW，TCP Server IP Port全部是写死在固件里面，因此要想使用WIFI调参需要修改这些内容，详情请看Doc/使用和调参说明&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;打样说明&lt;/h3&gt; &#xA;&lt;p&gt;考虑到很多同学可能是第一次进行结构打样，因此提供了打样文件&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D打印：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;文件位于：/6.Process/&lt;strong&gt;3D打印&lt;/strong&gt;，该文件为STL格式，打样数量请查看同目录下的&amp;lt;&lt;strong&gt;3D打印加工说明&lt;/strong&gt;&amp;gt;，可以在未来工厂或者嘉立创打样，首选光固化&lt;/li&gt; &#xA; &lt;li&gt;文件名称使用的是料号，料号方便管理，尾号没有.X都代表为第一版&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;钣金件打样：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;文件位于：/6.Process/&lt;strong&gt;钣金件&lt;/strong&gt;，&lt;strong&gt;每个物料的都有对应的PDF和DWG文件，PDF文件方便查看，DWG文件用于激光切割/板材加工&lt;/strong&gt;。打样数量请查看同目录下的&amp;lt;&lt;strong&gt;钣金加工说明&lt;/strong&gt;&amp;gt;，可以找淘宝给图纸直接加工。由于加工精度要求不高，个人用途下，公差全看加工厂的心情&lt;/li&gt; &#xA; &lt;li&gt;也可以直接找到对应的step文件给淘宝加工&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;PCB打样：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;从硬件工程拿着PCB文件出门左转嘉立创白嫖&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;BOM表：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;文件位于：/6.Process/&lt;strong&gt;BOM&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;结构BOM表，里面列有需要购买的&lt;strong&gt;结构物料和链接&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;PCB BOM表，里面列有需要购买的元器件物料，也可以自行找到AD工程导出&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;注意事项：&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PCB BOM表是从AD源文件导出，工程会有重复用料，需要注意&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;注意编码器需要3块，需要采购3套物料&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;DRV8313、ESP32、AS5600、IMU可以去淘宝购买，其他右转立创商城BOM表下单。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;注意贴片型铝电解电容的封装，主控板有限高区域，限高高度为8mm&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;如文件有遗漏的地方望提醒&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;写给小白的白嫖教程：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;文件位于：/Doc/写给小白的白嫖教程.pdf，手把手教你如何完成本项目的白嫖&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;控制算法&lt;/h3&gt; &#xA;&lt;p&gt;Cubli_Mini使用的是LQR来进行控制，也可以使用串级PID来进行控制。代码位于/需要修改/MCU1/control/cubli_mini&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;边平衡&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;可以参考一阶倒立摆进行建模和求解参数。由于建立模型的参数比较难准确的获得，我求解得到的参数和最佳参数有些许出入。因此本项目大多数使用玄学调参工程师来整定参数&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;点平衡：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在边平衡的基础上添加对X，Z轴的控制。采用与边平衡一样的控制方法，详情看Doc/控制原理说明。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;研究型：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;请查看原版Cubli的论文&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;调参&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;调节方法：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;可以使用UART和WIFI进行调参，WIFI需要连接到路由器，Cubli_Mini为TCP客户端&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;调节命令：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;详情命令请查看Doc/调参命令说明&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mamedev/mame</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/mamedev/mame</id>
    <link href="https://github.com/mamedev/mame" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MAME&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;MAME&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/mamedev/mame?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/mamedev/mame&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build status:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS/Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/GCC and clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Linux)/badge.svg?sanitize=true&#34; alt=&#34;CI (Linux)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows/MinGW GCC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Windows)/badge.svg?sanitize=true&#34; alt=&#34;CI (Windows)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS/clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(macOS)/badge.svg?sanitize=true&#34; alt=&#34;CI (macOS)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI Translations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Compile%20UI%20translations/badge.svg?sanitize=true&#34; alt=&#34;Compile UI translations&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Build%20documentation/badge.svg?sanitize=true&#34; alt=&#34;Build documentation&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Static analysis status for entire build (except for third-party parts of project):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scan.coverity.com/projects/mame-emulator&#34;&gt;&lt;img src=&#34;https://scan.coverity.com/projects/5727/badge.svg?flat=1&#34; alt=&#34;Coverity Scan Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is MAME?&lt;/h1&gt; &#xA;&lt;p&gt;MAME is a multi-purpose emulation framework.&lt;/p&gt; &#xA;&lt;p&gt;MAME&#39;s purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important &#34;vintage&#34; software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.&lt;/p&gt; &#xA;&lt;h1&gt;How to compile?&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re on a UNIX-like system (including Linux and macOS), it could be as easy as typing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MAME build,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=arcade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for an arcade-only build, or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=mess&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MESS build.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;http://docs.mamedev.org/initialsetup/compilingmame.html&#34;&gt;Compiling MAME&lt;/a&gt; page on our documentation site for more information, including prerequisites for macOS and popular Linux distributions.&lt;/p&gt; &#xA;&lt;p&gt;For recent versions of macOS you need to install &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; including command-line tools and &lt;a href=&#34;https://www.libsdl.org/download-2.0.php&#34;&gt;SDL 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Windows users, we provide a ready-made &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64.&lt;/p&gt; &#xA;&lt;p&gt;Visual Studio builds are also possible, but you still need &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64. In order to generate solution and project files just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or use this command to build it directly using msbuild&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019 MSBUILD=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Where can I find out more?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mamedev.org/&#34;&gt;Official MAME Development Team Site&lt;/a&gt; (includes binary downloads, wiki, forums, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mess.redump.net/&#34;&gt;Official MESS Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mametesters.org/&#34;&gt;MAME Testers&lt;/a&gt; (official bug tracker for MAME and MESS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Coding standard&lt;/h2&gt; &#xA;&lt;p&gt;MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.&lt;/p&gt; &#xA;&lt;p&gt;Some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#Allman_style&#34;&gt;Allman style&lt;/a&gt;; some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#K.26R_style&#34;&gt;K&amp;amp;R style&lt;/a&gt; -- mostly depending on who wrote the original version. &lt;strong&gt;Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source.&lt;/strong&gt; For new code, the majority tends to prefer Allman style, so if you don&#39;t care much, use that.&lt;/p&gt; &#xA;&lt;p&gt;All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD-3-Clause&lt;/a&gt; license, the &lt;a href=&#34;http://opensource.org/licenses/LGPL-2.1&#34;&gt;LGPL-2.1&lt;/a&gt;, or the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GPL-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The MAME project as a whole is made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GNU General Public License, version 2&lt;/a&gt; or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of the source files (over 90% including core files) are made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;3-clause BSD License&lt;/a&gt;, and we would encourage new contributors to make their contributions available under the terms of this license.&lt;/p&gt; &#xA;&lt;p&gt;Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the &#34;MAME&#34; name, logo, or wordmark.&lt;/p&gt; &#xA;&lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34; target=&#34;_blank&#34;&gt; &lt;img align=&#34;right&#34; src=&#34;http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (C) 1997-2021  MAMEDev and contributors&#xA;&#xA;This program is free software; you can redistribute it and/or modify it&#xA;under the terms of the GNU General Public License version 2, as provided in&#xA;docs/legal/GPL-2.0.&#xA;&#xA;This program is distributed in the hope that it will be useful, but WITHOUT&#xA;ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or&#xA;FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for&#xA;more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see COPYING for more details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mriscoc/Ender3V2S1</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/mriscoc/Ender3V2S1</id>
    <link href="https://github.com/mriscoc/Ender3V2S1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is optimized firmware for Ender3 V2/S1 3D printers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Professional Firmware for the Creality Ender 3 V2/S1 Printers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/mriscoc/Ender3V2S1.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release-date/mriscoc/Ender3V2S1.svg?sanitize=true&#34; alt=&#34;GitHub Release Date&#34;&gt; &lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/actions&#34;&gt;&lt;img src=&#34;https://github.com/mriscoc/Ender3V2S1/workflows/CI/badge.svg?branch=Ender3V2S1-Released&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Universal RET6/RCT6 Ender 3 V2/S1 Edition&lt;/h2&gt; &#xA;&lt;p&gt;Please test this firmware and let us know if it misbehaves in any way. Volunteers are standing by!&lt;br&gt; Precompiled binary files can be downloader from: &lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/releases/latest&#34;&gt;Latest Release&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img aling=&#34;left&#34; height=&#34;240&#34; src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/buildroot/share/pixmaps/Ender-3V2.jpg&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/buildroot/share/pixmaps/Ender-3S1.jpg&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for your support, I receive donations through &lt;a href=&#34;https://www.patreon.com/mriscoc&#34;&gt;Patreon&lt;/a&gt; and &lt;a href=&#34;https://www.paypal.com/paypalme/mriscoc&#34;&gt;Paypal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.paypal.com/donate?business=85SPAAR6UZEE8&amp;amp;currency_code=USD&#34;&gt;&lt;img src=&#34;https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Wiki&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/How-to-install-the-firmware&#34;&gt;How to install the firmware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/3D-BLTouch&#34;&gt;Installing a 3D/BLTouch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/Color-Themes&#34;&gt;Color themes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/Octoprint&#34;&gt;How to use with Octoprint&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/ender3v2s1firmware&#34;&gt;Telegram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/Ender3v2Firmware&#34;&gt;Reddit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/groups/ender3v2firmware&#34;&gt;Ender 3V2 Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/groups/ender3s1printer&#34;&gt;Ender 3S1 Facebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/screenshots/main.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This is a Marlin based firmware and is maintained by &lt;a href=&#34;https://github.com/mriscoc&#34;&gt;@mriscoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This work would not be possible without the supporters, helpers and betatesters at the &lt;strong&gt;Telegram&lt;/strong&gt; group.&lt;/p&gt; &#xA;&lt;p&gt;Marlin firmware is an Open Source project hosted on Github, &lt;a href=&#34;https://marlinfw.org/&#34;&gt;Marlin&lt;/a&gt; is owned and maintained by the maker community.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;THIS FIRMWARE AND ALL OTHER FILES IN THE DOWNLOAD ARE PROVIDED FREE OF CHARGE WITH NO WARRANTY OR GUARANTEE. SUPPORT IS NOT INCLUDED JUST BECAUSE YOU DOWNLOADED THE FIRMWARE. WE ARE NOT LIABLE FOR ANY DAMAGE TO YOUR PRINTER, PERSON, OR ANY OTHER PROPERTY DUE TO USE OF THIS FIRMWARE. IF YOU DO NOT AGREE TO THESE TERMS THEN DO NOT USE THE FIRMWARE.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;For the license, check the header of each file, if the license is not specified there, the project license will be used. Marlin is licensed under the GPL.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleSpeech</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/PaddlePaddle/PaddleSpeech</id>
    <link href="https://github.com/PaddlePaddle/PaddleSpeech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use Speech Toolkit including SOTA/Streaming ASR with punctuation, influential TTS with text frontend, Speaker Verification System and End-to-End Speech Simultaneous Translation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/README_cn.md&#34;&gt;简体中文&lt;/a&gt;|English)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/PaddleSpeech_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;support os&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleSpeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/paddlespeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt; Quick Start &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt; Quick Start Server &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt; Quick Start Streaming Server&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#documents&#34;&gt; Documents &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt; Models List &lt;/a&gt; | &lt;a href=&#34;https://aistudio.baidu.com/aistudio/education/group/info/25130&#34;&gt; AIStudio Courses &lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2205.12007&#34;&gt; Paper &lt;/a&gt; | &lt;a href=&#34;https://gitee.com/paddlepaddle/PaddleSpeech&#34;&gt; Gitee &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleSpeech&lt;/strong&gt; is an open-source toolkit on &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt; platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models.&lt;/p&gt; &#xA;&lt;h5&gt;Speech Recognition&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Recognition Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;I knocked at the door on the ancient side of the building.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;我认为跑步最重要的就是给我带来了身体健康。&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Speech Translation (English to Chinese)&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Translations Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;我 在 这栋 建筑 的 古老 门上 敲门。&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Text-to-Speech&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Input Text&lt;/th&gt; &#xA;    &lt;th&gt;Synthetic Audio&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Life was like a box of chocolates, you never know what you&#39;re gonna get.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;早上好，今天是2020/10/29，最低温度是-3°C。&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;季姬寂，集鸡，鸡即棘鸡。棘鸡饥叽，季姬及箕稷济鸡。鸡既济，跻姬笈，季姬忌，急咭鸡，鸡急，继圾几，季姬急，即籍箕击鸡，箕疾击几伎，伎即齑，鸡叽集几基，季姬急极屐击鸡，鸡既殛，季姬激，即记《季姬击鸡记》。&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For more synthesized audios, please refer to &lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;PaddleSpeech Text-to-Speech samples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Punctuation Restoration&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Input Text &lt;/th&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Output Text &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;今天的天气真不错啊你下午有空吗我想约你一起去吃饭&lt;/td&gt; &#xA;    &lt;td&gt;今天的天气真不错啊！你下午有空吗？我想约你一起去吃饭。&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Via the easy-to-use, efficient, flexible and scalable implementation, our vision is to empower both industrial application and academic research, including training, inference &amp;amp; testing modules, and deployment process. To be more specific, this toolkit features at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📦 &lt;strong&gt;Ease of Use&lt;/strong&gt;: low barriers to install, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt;CLI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt;Server&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt;Streaming Server&lt;/a&gt; is available to quick-start your journey.&lt;/li&gt; &#xA; &lt;li&gt;🏆 &lt;strong&gt;Align to the State-of-the-Art&lt;/strong&gt;: we provide high-speed and ultra-lightweight models, and also cutting-edge technology.&lt;/li&gt; &#xA; &lt;li&gt;🏆 &lt;strong&gt;Streaming ASR and TTS System&lt;/strong&gt;: we provide production ready streaming asr and streaming tts system.&lt;/li&gt; &#xA; &lt;li&gt;💯 &lt;strong&gt;Rule-based Chinese frontend&lt;/strong&gt;: our frontend contains Text Normalization and Grapheme-to-Phoneme (G2P, including Polyphone and Tone Sandhi). Moreover, we use self-defined linguistic rules to adapt Chinese context.&lt;/li&gt; &#xA; &lt;li&gt;📦 &lt;strong&gt;Varieties of Functions that Vitalize both Industrial and Academia&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🛎️ &lt;em&gt;Implementation of critical audio tasks&lt;/em&gt;: this toolkit contains audio functions like Automatic Speech Recognition, Text-to-Speech Synthesis, Speaker Verfication, KeyWord Spotting, Audio Classification, and Speech Translation, etc.&lt;/li&gt; &#xA;   &lt;li&gt;🔬 &lt;em&gt;Integration of mainstream models and datasets&lt;/em&gt;: the toolkit implements modules that participate in the whole pipeline of the speech tasks, and uses mainstream datasets like LibriSpeech, LJSpeech, AIShell, CSMSC, etc. See also &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt;model list&lt;/a&gt; for more details.&lt;/li&gt; &#xA;   &lt;li&gt;🧩 &lt;em&gt;Cascaded models application&lt;/em&gt;: as an extension of the typical traditional audio tasks, we combine the workflows of the aforementioned tasks with other fields like Natural language processing (NLP) and Computer Vision (CV).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recent Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;👑 2022.05.13: Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/PPASR.md&#34;&gt;PP-ASR&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/PPTTS.md&#34;&gt;PP-TTS&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/vpr/PPVPR.md&#34;&gt;PP-VPR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.05.06: &lt;code&gt;Streaming ASR&lt;/code&gt; with &lt;code&gt;Punctuation Restoration&lt;/code&gt; and &lt;code&gt;Token Timestamp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.05.06: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;, and &lt;code&gt;Punctuation Restoration&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.04.28: &lt;code&gt;Streaming Server&lt;/code&gt; is available for &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.03.28: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.03.28: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🤗 2021.12.14: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS&lt;/a&gt; Demos on Hugging Face Spaces are available!&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2021.12.10: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt;, &lt;code&gt;Speech Translation (English to Chinese)&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scan the QR code below with your Wechat, you can access to official technical exchange group and get the bonus ( more than 20GB learning materials, such as papers, codes and videos ) and the live link of the lessons. Look forward to your participation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/23690325/169763015-cbd8e28d-602c-4723-810d-dbc6da49441e.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We strongly recommend our users to install PaddleSpeech in &lt;strong&gt;Linux&lt;/strong&gt; with &lt;em&gt;python&amp;gt;=3.7&lt;/em&gt;. Up to now, &lt;strong&gt;Linux&lt;/strong&gt; supports CLI for the all our tasks, &lt;strong&gt;Mac OSX&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; only supports PaddleSpeech CLI for Audio Classification, Speech-to-Text and Text-to-Speech. To install &lt;code&gt;PaddleSpeech&lt;/code&gt;, please see &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our models with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/cli/README.md&#34;&gt;PaddleSpeech Command Line&lt;/a&gt;. Change &lt;code&gt;--input&lt;/code&gt; to test your own audio/text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech cls --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech vector --task spk --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Speech Recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech asr --lang zh --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Automatic Speech Recognition is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Translation&lt;/strong&gt; (English to Chinese) (not support for Mac and Windows now)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech st --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech tts --input &#34;你好，欢迎使用飞桨深度学习框架！&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Text to Speech is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Postprocessing&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Punctuation Restoration &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;paddlespeech text --task punc --input 今天的天气真不错啊你下午有空吗我想约你一起去吃饭&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Batch Process&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo -e &#34;1 欢迎光临。\n2 谢谢惠顾。&#34; | paddlespeech tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shell Pipeline&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ASR + Punctuation Restoration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech asr --input ./zh.wav | paddlespeech text --task punc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos&#34;&gt;demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try more functions like training and tuning, please have a look at &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Speech-to-Text Quick Start&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our speech server with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/server/README.md&#34;&gt;PaddleSpeech Server Command Line&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_server start --config_file ./paddlespeech/server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input &#34;您好，欢迎使用百度飞桨语音合成服务。&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Audio Classification Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about server command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server&#34;&gt;speech server demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartstreamingserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Streaming Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt; server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Speech Recognition Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Text to Speech Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input &#34;您好，欢迎使用百度飞桨语音合成服务。&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information please see: &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;ModelList&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech supports a series of most popular models. They are summarized in &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;released models&lt;/a&gt; and attached with available pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeechToText&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt; contains &lt;em&gt;Acoustic Model&lt;/em&gt;, &lt;em&gt;Language Model&lt;/em&gt;, and &lt;em&gt;Speech Translation&lt;/em&gt;, with the following details:&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Speech-to-Text Module Type&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Speech Recogination&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Aishell&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeech2 RNN + Conv based Models&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr0&#34;&gt;deepspeech2-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr1&#34;&gt;u2.transformer.conformer-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Librispeech&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr0&#34;&gt;deepspeech2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr1&#34;&gt;transformer.conformer.u2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr2&#34;&gt;transformer.conformer.u2-kaldi-librispeech&lt;/a&gt; &lt;/td&gt;  &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TIMIT&lt;/td&gt; &#xA;   &lt;td&gt;Unified Streaming &amp;amp; Non-streaming Two-pass&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/timit/asr1&#34;&gt; u2-timit&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment&lt;/td&gt; &#xA;   &lt;td&gt;THCHS30&lt;/td&gt; &#xA;   &lt;td&gt;MFA&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/.examples/thchs30/align0&#34;&gt;mfa-thchs30&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Language Model&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Ngram Language Model&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ngram_lm&#34;&gt;kenlm&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Speech Translation (English to Chinese)&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;TED En-Zh&lt;/td&gt; &#xA;   &lt;td&gt;Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st0&#34;&gt;transformer-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FAT + Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st1&#34;&gt;fat-st-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;TextToSpeech&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; in PaddleSpeech mainly contains three modules: &lt;em&gt;Text Frontend&lt;/em&gt;, &lt;em&gt;Acoustic Model&lt;/em&gt; and &lt;em&gt;Vocoder&lt;/em&gt;. Acoustic Model and Vocoder models are listed as follow:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Text-to-Speech Module Type &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Text Frontend &lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;   &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/tn&#34;&gt;tn&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/g2p&#34;&gt;g2p&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Acoustic Model&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts0&#34;&gt;tacotron2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts0&#34;&gt;tacotron2-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer TTS&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts1&#34;&gt;transformer-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeedySpeech&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts2&#34;&gt;speedyspeech-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts3&#34;&gt;fastspeech2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/tts3&#34;&gt;fastspeech2-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts3&#34;&gt;fastspeech2-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/tts3&#34;&gt;fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;6&#34;&gt;Vocoder&lt;/td&gt; &#xA;   &lt;td&gt;WaveFlow&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc0&#34;&gt;waveflow-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parallel WaveGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc1&#34;&gt;PWGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc1&#34;&gt;PWGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc1&#34;&gt;PWGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc1&#34;&gt;PWGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi Band MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc3&#34;&gt;Multi Band MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Style MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc4&#34;&gt;Style MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiFiGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc5&#34;&gt;HiFiGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc5&#34;&gt;HiFiGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc5&#34;&gt;HiFiGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc5&#34;&gt;HiFiGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WaveRNN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc6&#34;&gt;WaveRNN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Voice Cloning&lt;/td&gt; &#xA;   &lt;td&gt;GE2E&lt;/td&gt; &#xA;   &lt;td&gt;Librispeech, etc.&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ge2e&#34;&gt;ge2e&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc0&#34;&gt;ge2e-tacotron2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc1&#34;&gt;ge2e-fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;AudioClassification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio Classification&lt;/td&gt; &#xA;   &lt;td&gt;ESC-50&lt;/td&gt; &#xA;   &lt;td&gt;PANN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/esc50/cls0&#34;&gt;pann-esc50&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeakerVerification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;VoxCeleb12&lt;/td&gt; &#xA;   &lt;td&gt;ECAPA-TDNN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/voxceleb/sv0&#34;&gt;ecapa-tdnn-voxceleb12&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;PunctuationRestoration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Punctuation Restoration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Punctuation Restoration&lt;/td&gt; &#xA;   &lt;td&gt;IWLST2012_zh&lt;/td&gt; &#xA;   &lt;td&gt;Ernie Linear&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/iwslt2012/punc0&#34;&gt;iwslt2012-punc0&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;p&gt;Normally, &lt;a href=&#34;https://paperswithcode.com/area/speech&#34;&gt;Speech SoTA&lt;/a&gt;, &lt;a href=&#34;https://paperswithcode.com/area/audio&#34;&gt;Audio SoTA&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/area/music&#34;&gt;Music SoTA&lt;/a&gt; give you an overview of the hot academic topics in the related area. To focus on the tasks in PaddleSpeech, you will find the following guidelines are helpful to grasp the core ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/README.md&#34;&gt;Some Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Automatic Speech Recognition&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/data_preparation.md&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/ngram_lm.md&#34;&gt;Ngram LM&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/advanced_usage.md&#34;&gt;Advanced Usage&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/zh_text_frontend.md&#34;&gt;Chinese Rule Based Text Frontend&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;Test Audio Samples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Speaker Verification &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_searching/README.md&#34;&gt;Audio Searching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speaker_verification/README.md&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_tagging/README.md&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_translation/README.md&#34;&gt;Speech Translation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_server/README.md&#34;&gt;Speech Server&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;Released Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeechToText&#34;&gt;Speech-to-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#TextToSpeech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#AudioClassification&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeakerVerification&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#PunctuationRestoration&#34;&gt;Punctuation Restoration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#contribution&#34;&gt;Welcome to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Text-to-Speech module is originally called &lt;a href=&#34;https://github.com/PaddlePaddle/Parakeet&#34;&gt;Parakeet&lt;/a&gt;, and now merged with this repository. If you are interested in academic research about this task, please see &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview&#34;&gt;TTS research overview&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/raw/develop/docs/source/tts/models_introduction.md&#34;&gt;this document&lt;/a&gt; is a good guideline for the pipeline components.&lt;/p&gt; &#xA;&lt;h2&gt;⭐ Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt;: Use PaddleSpeech TTS to generate virtual human voice.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web&#34;&gt;&lt;img src=&#34;https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/demo_video.html&#34;&gt;PaddleSpeech Demo Video&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt;: Use PaddleSpeech TTS and ASR to clone voice from videos.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jerryuhoo/VTuberTalk/main/gui/gui.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;To cite PaddleSpeech for research, please use the following format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{zhang2022paddlespeech,&#xA;    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},&#xA;    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},&#xA;    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},&#xA;    year = {2022},&#xA;    publisher = {Association for Computational Linguistics},&#xA;}&#xA;&#xA;@inproceedings{zheng2021fused,&#xA;  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},&#xA;  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},&#xA;  booktitle={International Conference on Machine Learning},&#xA;  pages={12736--12746},&#xA;  year={2021},&#xA;  organization={PMLR}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;contribution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to PaddleSpeech&lt;/h2&gt; &#xA;&lt;p&gt;You are warmly welcome to submit questions in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/discussions&#34;&gt;discussions&lt;/a&gt; and bug reports in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;issues&lt;/a&gt;! Also, we highly appreciate if you are willing to contribute to this project!&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zh794390558&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3038472?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackwaterveg&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/87408988?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yt605155624&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24568452?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kuke&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3064195?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinghai-sun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7038341?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pkuyym&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5782283?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KPatr1ck&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22954146?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LittleChenCc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10339970?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/745165806&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/20623194?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Mingxue-Xu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/92848346?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chrisxu2016&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18379485?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfchener&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6771821?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luotao1&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6836917?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wanghaoshuang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7534971?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gongel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24390500?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mmglove&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/38800877?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/iclementine&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/16222986?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ZeyuChen&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1371212?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81195143?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qingqing01&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7845005?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ericxk&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/4719594?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kvinwang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6442159?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jiqiren11&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/82639260?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AshishKarel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/58069375?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chesterkuo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6285069?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tensor-tang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/21351065?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hysunflower&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/52739577?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wwhu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6081200?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lispc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/2833376?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24245709?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harisankarh&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1307053?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackiexiao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18050469?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/limpidezza&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/71760778?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/yeyupiaoling&#34;&gt;yeyupiaoling&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PPASR&#34;&gt;PPASR&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech&#34;&gt;PaddlePaddle-DeepSpeech&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle&#34;&gt;VoiceprintRecognition-PaddlePaddle&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle&#34;&gt;AudioClassification-PaddlePaddle&lt;/a&gt; for years of attention, constructive advice and great help.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/mymagicpower&#34;&gt;mymagicpower&lt;/a&gt; for the Java implementation of ASR upon &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk&#34;&gt;short&lt;/a&gt; and &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk&#34;&gt;long&lt;/a&gt; audio files.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/JiehangXie&#34;&gt;JiehangXie&lt;/a&gt;/&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt; for developing Virtual Uploader(VUP)/Virtual YouTuber(VTuber) with PaddleSpeech TTS function.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;745165806&lt;/a&gt;/&lt;a href=&#34;https://github.com/745165806/PaddleSpeechTask&#34;&gt;PaddleSpeechTask&lt;/a&gt; for contributing Punctuation Restoration model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;kslz&lt;/a&gt; for supplementary Chinese documents.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/awmmmm&#34;&gt;awmmmm&lt;/a&gt; for contributing fastspeech2 aishell3 conformer pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/phecda-xu&#34;&gt;phecda-xu&lt;/a&gt;/&lt;a href=&#34;https://github.com/phecda-xu/PaddleDubbing&#34;&gt;PaddleDubbing&lt;/a&gt; for developing a dubbing tool with GUI based on PaddleSpeech TTS model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;jerryuhoo&lt;/a&gt;/&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt; for developing a GUI tool based on PaddleSpeech TTS and code for making datasets from videos based on PaddleSpeech ASR.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Besides, PaddleSpeech depends on a lot of open source repositories. See &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/reference.md&#34;&gt;references&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/tensorflow</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/tensorflow/tensorflow</id>
    <link href="https://github.com/tensorflow/tensorflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open Source Machine Learning Framework for Everyone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/tensorflow.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.5281/zenodo.4724125&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/api-reference-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of &lt;a href=&#34;https://www.tensorflow.org/resources/tools&#34;&gt;tools&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/resources/libraries-extensions&#34;&gt;libraries&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;community&lt;/a&gt; resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google&#39;s Machine Intelligence Research organization to conduct machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow provides stable &lt;a href=&#34;https://www.tensorflow.org/api_docs/python&#34;&gt;Python&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/api_docs/cc&#34;&gt;C++&lt;/a&gt; APIs, as well as non-guaranteed backward compatible API for &lt;a href=&#34;https://www.tensorflow.org/api_docs&#34;&gt;other languages&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep up-to-date with release announcements and security updates by subscribing to &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/announce&#34;&gt;announce@tensorflow.org&lt;/a&gt;. See all the &lt;a href=&#34;https://www.tensorflow.org/community/forums&#34;&gt;mailing lists&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;TensorFlow install guide&lt;/a&gt; for the &lt;a href=&#34;https://www.tensorflow.org/install/pip&#34;&gt;pip package&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;enable GPU support&lt;/a&gt;, use a &lt;a href=&#34;https://www.tensorflow.org/install/docker&#34;&gt;Docker container&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/install/source&#34;&gt;build from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To install the current release, which includes support for &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;CUDA-enabled GPU cards&lt;/a&gt; &lt;em&gt;(Ubuntu and Windows)&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A smaller CPU-only package is also available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update TensorFlow to the latest version, add &lt;code&gt;--upgrade&lt;/code&gt; flag to the above commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Nightly binaries are available for testing using the &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly&#34;&gt;tf-nightly&lt;/a&gt; and &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly-cpu&#34;&gt;tf-nightly-cpu&lt;/a&gt; packages on PyPi.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Try your first TensorFlow program&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&#xA;&amp;gt;&amp;gt;&amp;gt; tf.add(1, 2).numpy()&#xA;3&#xA;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant(&#39;Hello, TensorFlow!&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; hello.numpy()&#xA;b&#39;Hello, TensorFlow!&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution guidelines&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want to contribute to TensorFlow, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;. This project adheres to TensorFlow&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We use &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues&#34;&gt;GitHub issues&lt;/a&gt; for tracking requests and bugs, please see &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss&#34;&gt;TensorFlow Discuss&lt;/a&gt; for general questions and discussion, and please direct specific questions to &lt;a href=&#34;https://stackoverflow.com/questions/tagged/tensorflow&#34;&gt;Stack Overflow&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The TensorFlow project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:tensorflow&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/1486&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/1486/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Continuous build status&lt;/h2&gt; &#xA;&lt;p&gt;You can find more community-supported platforms and configurations in the &lt;a href=&#34;https://github.com/tensorflow/build#community-supported-tensorflow-builds&#34;&gt;TensorFlow SIG Build community builds table&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Official Builds&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Artifacts&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux XLA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bintray.com/google/tensorflow/tensorflow/_latestVersion&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 0 and 1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 2 and 3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow MacOS CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;TensorFlow.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official&#34;&gt;TensorFlow Official Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/examples&#34;&gt;TensorFlow Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-in-practice&#34;&gt;DeepLearning.AI TensorFlow Developer Professional Certificate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-data-and-deployment&#34;&gt;TensorFlow: Data and Deployment from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/getting-started-with-tensor-flow2&#34;&gt;Getting Started with TensorFlow 2 from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-advanced-techniques&#34;&gt;TensorFlow: Advanced Techniques from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow2-deeplearning&#34;&gt;TensorFlow 2 for Deep Learning Specialization from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/introduction-tensorflow&#34;&gt;Intro to TensorFlow for A.I, M.L, and D.L from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187&#34;&gt;Intro to TensorFlow for Deep Learning from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-lite--ud190&#34;&gt;Introduction to TensorFlow Lite from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-tensorflow-gcp&#34;&gt;Machine Learning with TensorFlow on GCP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codelabs.developers.google.com/?cat=TensorFlow&#34;&gt;TensorFlow Codelabs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.tensorflow.org&#34;&gt;TensorFlow Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/resources/learn-ml&#34;&gt;Learn ML with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/tensorflow&#34;&gt;TensorFlow Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ&#34;&gt;TensorFlow YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/model_optimization/guide/roadmap&#34;&gt;TensorFlow model optimization roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/about/bib&#34;&gt;TensorFlow White Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorboard&#34;&gt;TensorBoard Visualization Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about the &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;TensorFlow community&lt;/a&gt; and how to &lt;a href=&#34;https://www.tensorflow.org/community/contribute&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PCSX2/pcsx2</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/PCSX2/pcsx2</id>
    <link href="https://github.com/PCSX2/pcsx2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PCSX2 - The Playstation 2 Emulator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PCSX2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%96%A5%EF%B8%8F%20Windows%20Builds/master?label=Windows%20Builds&#34; alt=&#34;Windows Build Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%90%A7%20Linux%20Builds/master?label=Linux%20Builds&#34; alt=&#34;Linux Build Status&#34;&gt; &lt;a href=&#34;https://www.codacy.com/gh/PCSX2/pcsx2/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=PCSX2/pcsx2&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/1f7c0d75fec74d6daa6adb084e5b4f71&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/TCz3t9k&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/309643527816609793?color=%235CA8FA&amp;amp;label=PCSX2%20Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCSX2 is a free and open-source PlayStation 2 (PS2) emulator. Its purpose is to emulate the PS2&#39;s hardware, using a combination of MIPS CPU &lt;a href=&#34;https://en.wikipedia.org/wiki/Interpreter_(computing)&#34;&gt;Interpreters&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_recompilation&#34;&gt;Recompilers&lt;/a&gt; and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Virtual_machine&#34;&gt;Virtual Machine&lt;/a&gt; which manages hardware states and PS2 system memory. This allows you to play PS2 games on your PC, with many additional features and benefits.&lt;/p&gt; &#xA;&lt;h2&gt;Project Details&lt;/h2&gt; &#xA;&lt;p&gt;The PCSX2 project has been running for more than ten years. Past versions could only run a few public domain game demos, but newer versions can run most games at full speed, including popular titles such as Final Fantasy X and Devil May Cry 3. Visit the &lt;a href=&#34;https://pcsx2.net/compat/&#34;&gt;PCSX2 compatibility list&lt;/a&gt; to check the latest compatibility status of games (with more than 2500 titles tested), or ask for help in the &lt;a href=&#34;https://forums.pcsx2.net/&#34;&gt;official forums&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The latest officially released stable version is version 1.6.0.&lt;/p&gt; &#xA;&lt;p&gt;Installers and binaries for both stable and development builds are available from &lt;a href=&#34;https://pcsx2.net/downloads/&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Minimum&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 8.1 or newer (64 bit) &lt;br&gt; - Ubuntu 18.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports SSE4.1 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 1600 &lt;br&gt; - Two physical cores, with hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D10 support &lt;br&gt; - OpenGL 3.x support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 3000 (GeForce GTX 750) &lt;br&gt; - 2 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;4 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended Single Thread Performance is based on moderately complex games. Games that pushed the PS2 hardware to its limits will struggle on CPUs at this level. Some release titles and 2D games which underutilized the PS2 hardware may run on CPUs rated as low as 1200. A quick reference for CPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:CPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;, &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-The-Most-CPU-Intensive-Games&#34;&gt;Forum&lt;/a&gt; and CPU &lt;strong&gt;light&lt;/strong&gt; games: &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-Games-that-don-t-need-a-strong-CPU-to-emulate&#34;&gt;Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommended&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 10 (64 bit) &lt;br&gt; - Ubuntu 19.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports AVX2 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 2100 &lt;br&gt; - Four physical cores, with or without hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D11 support &lt;br&gt; - OpenGL 4.6 support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 6000 (GeForce GTX 1050 Ti) &lt;br&gt; - 4 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended GPU is based on 3x Internal, ~1080p resolution requirements. Higher resolutions will require stronger cards; 6x Internal, ~4K resolution will require a &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 12000 (GeForce GTX 1070 Ti). Just like CPU requirements, this is also highly game dependent. A quick reference for GPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:GPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Technical Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need the &lt;a href=&#34;https://support.microsoft.com/en-us/help/2977003/&#34;&gt;Visual C++ 2019 x86 Redistributables&lt;/a&gt; to run PCSX2.&lt;/li&gt; &#xA; &lt;li&gt;Windows XP and Direct3D9 support was dropped after stable release 1.4.0.&lt;/li&gt; &#xA; &lt;li&gt;Windows 7 and Windows 8.0 support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;32 bit support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to update your operating system and drivers to ensure you have the best experience possible. Having a newer GPU is also recommended so you have the latest supported drivers.&lt;/li&gt; &#xA; &lt;li&gt;Because of copyright issues, and the complexity of trying to work around it, you need a BIOS dump extracted from a legitimately-owned PS2 console to use the emulator. For more information about the BIOS and how to get it from your console, visit &lt;a href=&#34;https://raw.githubusercontent.com/PCSX2/pcsx2/master/pcsx2/Docs/PCSX2_FAQ.md#question-13-where-do-i-get-a-ps2-bios&#34;&gt;this page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PCSX2 uses two CPU cores for emulation by default. A third core can be used via the MTVU speed hack, which is compatible with most games. This can be a significant speedup on CPUs with 3+ cores, but it may be a slowdown on GS-limited games (or on CPUs with fewer than 2 cores). Software renderers will then additionally use however many rendering threads it is set to and will need higher core counts to run efficiently.&lt;/li&gt; &#xA; &lt;li&gt;Requirements benchmarks are based on a statistic from the Passmark CPU bench marking software. When we say &#34;STR&#34;, we are referring to Passmark&#39;s &#34;Single Thread Rating&#34; statistic. You can look up your CPU on &lt;a href=&#34;https://cpubenchmark.net&#34;&gt;Passmark&#39;s website for CPUs&lt;/a&gt; to see how it compares to PCSX2&#39;s requirements.&lt;/li&gt; &#xA; &lt;li&gt;Vulkan requires an up-to-date GPU driver; old drivers may cause graphical problems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want more? &lt;a href=&#34;https://pcsx2.net/&#34;&gt;Check out the PCSX2 website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ethereum/solidity</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/ethereum/solidity</id>
    <link href="https://github.com/ethereum/solidity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solidity, the Smart Contract Programming Language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Solidity Contract-Oriented Programming Language&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#ethereum_solidity:gitter.im&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Matrix%20-chat-brightgreen?style=plastic&amp;amp;logo=matrix&#34; alt=&#34;Matrix Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/ethereum/solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitter%20-chat-brightgreen?style=plastic&amp;amp;logo=gitter&#34; alt=&#34;Gitter Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.soliditylang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Solidity_Forum%20-discuss-brightgreen?style=plastic&amp;amp;logo=discourse&#34; alt=&#34;Solidity&amp;nbsp;Forum&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/solidity_lang&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/solidity_lang?style=plastic&amp;amp;logo=twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/000335908?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;logo=mastodon&amp;amp;style=plastic&#34; alt=&#34;Mastodon Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can talk to us on Gitter and Matrix, tweet at us on Twitter or create a new topic in the Solidity forum. Questions, feedback, and suggestions are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Solidity is a statically typed, contract-oriented, high-level language for implementing smart contracts on the Ethereum platform.&lt;/p&gt; &#xA;&lt;p&gt;For a good overview and starting point, please check out the official &lt;a href=&#34;https://soliditylang.org&#34;&gt;Solidity Language Portal&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#build-and-install&#34;&gt;Build and Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#maintainers&#34;&gt;Maintainers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#security&#34;&gt;Security&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is a statically-typed curly-braces programming language designed for developing smart contracts that run on the Ethereum Virtual Machine. Smart contracts are programs that are executed inside a peer-to-peer network where nobody has special authority over the execution, and thus they allow to implement tokens of value, ownership, voting, and other kinds of logic.&lt;/p&gt; &#xA;&lt;p&gt;When deploying contracts, you should use the latest released version of Solidity. This is because breaking changes, as well as new features and bug fixes are introduced regularly. We currently use a 0.x version number &lt;a href=&#34;https://semver.org/#spec-item-4&#34;&gt;to indicate this fast pace of change&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build and Install&lt;/h2&gt; &#xA;&lt;p&gt;Instructions about how to build and install the Solidity compiler can be found in the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/installing-solidity.html#building-from-source&#34;&gt;Solidity documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;A &#34;Hello World&#34; program in Solidity is of even less use than in other languages, but still:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-solidity&#34;&gt;// SPDX-License-Identifier: MIT&#xA;pragma solidity &amp;gt;=0.6.0 &amp;lt;0.9.0;&#xA;&#xA;contract HelloWorld {&#xA;    function helloWorld() external pure returns (string memory) {&#xA;        return &#34;Hello, World!&#34;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get started with Solidity, you can use &lt;a href=&#34;https://remix.ethereum.org/&#34;&gt;Remix&lt;/a&gt;, which is a browser-based IDE. Here are some example contracts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#voting&#34;&gt;Voting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#blind-auction&#34;&gt;Blind Auction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#safe-remote-purchase&#34;&gt;Safe remote purchase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#micropayment-channel&#34;&gt;Micropayment Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The Solidity documentation is hosted at &lt;a href=&#34;https://docs.soliditylang.org&#34;&gt;Read the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is still under development. Contributions are always welcome! Please follow the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/contributing.html&#34;&gt;Developers Guide&lt;/a&gt; if you want to help.&lt;/p&gt; &#xA;&lt;p&gt;You can find our current feature and bug priorities for forthcoming releases in the &lt;a href=&#34;https://github.com/ethereum/solidity/projects&#34;&gt;projects section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/axic&#34;&gt;@axic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chriseth&#34;&gt;@chriseth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/LICENSE.txt&#34;&gt;GNU General Public License v3.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some third-party code has its &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/cmake/templates/license.h.in&#34;&gt;own licensing terms&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;The security policy may be &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/SECURITY.md&#34;&gt;found here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/rocksdb</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/facebook/rocksdb</id>
    <link href="https://github.com/facebook/rocksdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library that provides an embeddable, persistent key-value store for fast storage.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;RocksDB: A Persistent Key-Value Store for Flash and RAM Storage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebook/rocksdb.svg?style=svg&#34; alt=&#34;CircleCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/github/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/facebook/rocksdb.svg?branch=main&#34; alt=&#34;TravisCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/Facebook/rocksdb/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/fbgfu0so3afcno78/branch/main?svg=true&#34; alt=&#34;Appveyor Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://140-211-168-68-openstack.osuosl.org:8080/job/rocksdb&#34;&gt;&lt;img src=&#34;http://140-211-168-68-openstack.osuosl.org:8080/buildStatus/icon?job=rocksdb&amp;amp;style=plastic&#34; alt=&#34;PPC64le Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RocksDB is developed and maintained by Facebook Database Engineering Team. It is built on earlier work on &lt;a href=&#34;https://github.com/google/leveldb&#34;&gt;LevelDB&lt;/a&gt; by Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;This code is a library that forms the core building block for a fast key-value server, especially suited for storing data on flash drives. It has a Log-Structured-Merge-Database (LSM) design with flexible tradeoffs between Write-Amplification-Factor (WAF), Read-Amplification-Factor (RAF) and Space-Amplification-Factor (SAF). It has multi-threaded compactions, making it especially suitable for storing multiple terabytes of data in a single database.&lt;/p&gt; &#xA;&lt;p&gt;Start with example usage here: &lt;a href=&#34;https://github.com/facebook/rocksdb/tree/main/examples&#34;&gt;https://github.com/facebook/rocksdb/tree/main/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki&#34;&gt;github wiki&lt;/a&gt; for more explanation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in &lt;code&gt;include/&lt;/code&gt;. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Questions and discussions are welcome on the &lt;a href=&#34;https://www.facebook.com/groups/rocksdb.dev/&#34;&gt;RocksDB Developers Public&lt;/a&gt; Facebook group and &lt;a href=&#34;https://groups.google.com/g/rocksdb&#34;&gt;email list&lt;/a&gt; on Google Groups.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;RocksDB is dual-licensed under both the GPLv2 (found in the COPYING file in the root directory) and Apache 2.0 License (found in the LICENSE.Apache file in the root directory). You may select, at your option, one of the above-listed licenses.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/TensorRT</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/NVIDIA/TensorRT</id>
    <link href="https://github.com/NVIDIA/TensorRT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TensorRT is a C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TensorRT-documentation-brightgreen.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;TensorRT Open Source Software&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the Open Source Software (OSS) components of NVIDIA TensorRT. Included are the sources for TensorRT plugins and parsers (Caffe and ONNX), as well as sample applications demonstrating usage and capabilities of the TensorRT platform. These open source software components are a subset of the TensorRT General Availability (GA) release with some extensions and bug-fixes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For code contributions to TensorRT-OSS, please see our &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/main/CODING-GUIDELINES.md&#34;&gt;Coding Guidelines&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For a summary of new additions and updates shipped with TensorRT-OSS releases, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/main/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For business inquiries, please contact &lt;a href=&#34;mailto:researchinquiries@nvidia.com&#34;&gt;researchinquiries@nvidia.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For press and other inquiries, please contact Hector Marinez at &lt;a href=&#34;mailto:hmarinez@nvidia.com&#34;&gt;hmarinez@nvidia.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Build&lt;/h1&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To build the TensorRT-OSS components, you will first need the following software packages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TensorRT GA build&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/nvidia-tensorrt-download&#34;&gt;TensorRT&lt;/a&gt; v8.2.5.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;System Packages&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Recommended versions:&lt;/li&gt; &#xA;   &lt;li&gt;cuda-11.4.x + cuDNN-8.2&lt;/li&gt; &#xA;   &lt;li&gt;cuda-10.2 + cuDNN-8.2&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ftp.gnu.org/gnu/make/&#34;&gt;GNU make&lt;/a&gt; &amp;gt;= v4.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kitware/CMake/releases&#34;&gt;cmake&lt;/a&gt; &amp;gt;= v3.13&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;python&lt;/a&gt; &amp;gt;= v3.6.9&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pip/#history&#34;&gt;pip&lt;/a&gt; &amp;gt;= v19.0&lt;/li&gt; &#xA; &lt;li&gt;Essential utilities &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, &lt;a href=&#34;https://www.freedesktop.org/wiki/Software/pkg-config/&#34;&gt;pkg-config&lt;/a&gt;, &lt;a href=&#34;https://www.gnu.org/software/wget/faq.html#download&#34;&gt;wget&lt;/a&gt;, &lt;a href=&#34;https://zlib.net/&#34;&gt;zlib&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional Packages&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Containerized build&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Docker&lt;/a&gt; &amp;gt;= 19.03&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Toolchains and SDKs&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(Cross compilation for Jetson platform) &lt;a href=&#34;https://developer.nvidia.com/embedded/jetpack&#34;&gt;NVIDIA JetPack&lt;/a&gt; &amp;gt;= 4.6 (current support only for TensorRT 8.0.1)&lt;/li&gt; &#xA;   &lt;li&gt;(For Windows builds) &lt;a href=&#34;https://visualstudio.microsoft.com/vs/older-downloads/&#34;&gt;Visual Studio&lt;/a&gt; 2017 Community or Enterprise edition&lt;/li&gt; &#xA;   &lt;li&gt;(Cross compilation for QNX platform) &lt;a href=&#34;https://blackberry.qnx.com/en&#34;&gt;QNX Toolchain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;PyPI packages (for demo applications/tests)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/onnx/&#34;&gt;onnx&lt;/a&gt; 1.9.0&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/onnxruntime/&#34;&gt;onnxruntime&lt;/a&gt; 1.8.0&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/tensorflow/&#34;&gt;tensorflow-gpu&lt;/a&gt; &amp;gt;= 2.5.1&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/Pillow/&#34;&gt;Pillow&lt;/a&gt; &amp;gt;= 8.3.2&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pycuda/&#34;&gt;pycuda&lt;/a&gt; &amp;lt; 2021.1&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/numpy/&#34;&gt;numpy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pytest/&#34;&gt;pytest&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code formatting tools (for contributors)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://clang.llvm.org/docs/ClangFormat.html&#34;&gt;Clang-format&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/llvm-mirror/clang/raw/master/tools/clang-format/git-clang-format&#34;&gt;Git-clang-format&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: &lt;a href=&#34;https://github.com/onnx/onnx-tensorrt&#34;&gt;onnx-tensorrt&lt;/a&gt;, &lt;a href=&#34;http://nvlabs.github.io/cub/&#34;&gt;cub&lt;/a&gt;, and &lt;a href=&#34;https://github.com/protocolbuffers/protobuf.git&#34;&gt;protobuf&lt;/a&gt; packages are downloaded along with TensorRT OSS, and not required to be installed.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading TensorRT Build&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;h4&gt;Download TensorRT OSS&lt;/h4&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b master https://github.com/nvidia/TensorRT TensorRT&#xA;cd TensorRT&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h4&gt;(Optional - if not using TensorRT container) Specify the TensorRT GA release build&lt;/h4&gt; &lt;p&gt;If using the TensorRT OSS build container, TensorRT libraries are preinstalled under &lt;code&gt;/usr/lib/x86_64-linux-gnu&lt;/code&gt; and you may skip this step.&lt;/p&gt; &lt;p&gt;Else download and extract the TensorRT GA build from &lt;a href=&#34;https://developer.nvidia.com/nvidia-tensorrt-download&#34;&gt;NVIDIA Developer Zone&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 18.04 on x86-64 with cuda-11.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/Downloads&#xA;tar -xvzf TensorRT-8.2.5.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz&#xA;export TRT_LIBPATH=`pwd`/TensorRT-8.2.5.1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Windows on x86-64 with cuda-11.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;cd ~\Downloads&#xA;Expand-Archive .\TensorRT-8.2.5.1.Windows10.x86_64.cuda-11.4.cudnn8.2.zip&#xA;$Env:TRT_LIBPATH = &#39;$(Get-Location)\TensorRT-8.2.5.1&#39;&#xA;$Env:PATH += &#39;C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\MSBuild\15.0\Bin\&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h4&gt;(Optional - for Jetson builds only) Download the JetPack SDK&lt;/h4&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and launch the JetPack SDK manager. Login with your NVIDIA developer account.&lt;/li&gt; &#xA;   &lt;li&gt;Select the platform and target OS (example: Jetson AGX Xavier, &lt;code&gt;Linux Jetpack 4.6&lt;/code&gt;), and click Continue.&lt;/li&gt; &#xA;   &lt;li&gt;Under &lt;code&gt;Download &amp;amp; Install Options&lt;/code&gt; change the download folder and select &lt;code&gt;Download now, Install later&lt;/code&gt;. Agree to the license terms and click Continue.&lt;/li&gt; &#xA;   &lt;li&gt;Move the extracted files into the &lt;code&gt;&amp;lt;TensorRT-OSS&amp;gt;/docker/jetpack_files&lt;/code&gt; folder.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setting Up The Build Environment&lt;/h2&gt; &#xA;&lt;p&gt;For Linux platforms, we recommend that you generate a docker container for building TensorRT OSS as described below. For native builds, on Windows for example, please install the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/main/#prerequisites&#34;&gt;prerequisite&lt;/a&gt; &lt;em&gt;System Packages&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;h4&gt;Generate the TensorRT-OSS build container.&lt;/h4&gt; &lt;p&gt;The TensorRT-OSS build container can be generated using the supplied Dockerfiles and build script. The build container is configured for building TensorRT OSS out-of-the-box.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 18.04 on x86-64 with cuda-11.4.2 (default)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/ubuntu-18.04.Dockerfile --tag tensorrt-ubuntu18.04-cuda11.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: CentOS/RedHat 7 on x86-64 with cuda-10.2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/centos-7.Dockerfile --tag tensorrt-centos7-cuda10.2 --cuda 10.2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 18.04 cross-compile for Jetson (aarch64) with cuda-10.2 (JetPack SDK)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/ubuntu-cross-aarch64.Dockerfile --tag tensorrt-jetpack-cuda10.2 --cuda 10.2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 20.04 on aarch64 with cuda-11.4.2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/ubuntu-20.04-aarch64.Dockerfile --tag tensorrt-aarch64-ubuntu20.04-cuda11.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h4&gt;Launch the TensorRT-OSS build container.&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 18.04 build container&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/launch.sh --tag tensorrt-ubuntu18.04-cuda11.4 --gpus all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE:&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Use the &lt;code&gt;--tag&lt;/code&gt; corresponding to build container generated in Step 1.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/main/#prerequisites&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt; is required for GPU access (running TensorRT applications) inside the build container.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;sudo&lt;/code&gt; password for Ubuntu build containers is &#39;nvidia&#39;.&lt;/li&gt; &#xA;   &lt;li&gt;Specify port number using &lt;code&gt;--jupyter &amp;lt;port&amp;gt;&lt;/code&gt; for launching Jupyter notebooks.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building TensorRT-OSS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate Makefiles or VS project (Windows) and build.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: Linux (x86-64) build with default cuda-11.4.2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out&#xA; make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: On CentOS7, the default g++ version does not support C++14. For native builds (not using the CentOS7 build container), first install devtoolset-8 to obtain the updated g++ toolchain as follows:&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yum -y install centos-release-scl&#xA;yum-config-manager --enable rhel-server-rhscl-7-rpms&#xA;yum -y install devtoolset-8&#xA;export PATH=&#34;/opt/rh/devtoolset-8/root/bin:${PATH}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Linux (aarch64) build with default cuda-11.4.2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64-native.toolchain&#xA; make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Native build on Jetson (aarch64) with cuda-10.2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd $TRT_OSSPATH&#xA;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DTRT_PLATFORM_ID=aarch64 -DCUDA_VERSION=10.2&#xA;CC=/usr/bin/gcc make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: C compiler must be explicitly specified via &lt;code&gt;CC=&lt;/code&gt; for native &lt;code&gt;aarch64&lt;/code&gt; builds of protobuf.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 18.04 Cross-Compile for Jetson (aarch64) with cuda-10.2 (JetPack)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64.toolchain -DCUDA_VERSION=10.2 -DCUDNN_LIB=/pdk_files/cudnn/usr/lib/aarch64-linux-gnu/libcudnn.so -DCUBLAS_LIB=/usr/local/cuda-10.2/targets/aarch64-linux/lib/stubs/libcublas.so -DCUBLASLT_LIB=/usr/local/cuda-10.2/targets/aarch64-linux/lib/stubs/libcublasLt.so&#xA; make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: The latest JetPack SDK v4.6 only supports TensorRT 8.0.1.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Example: Windows (x86-64) build in Powershell&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt; cd $Env:TRT_OSSPATH&#xA; mkdir -p build ; cd build&#xA; cmake .. -DTRT_LIB_DIR=$Env:TRT_LIBPATH -DTRT_OUT_DIR=&#39;$(Get-Location)\out&#39; -DCMAKE_TOOLCHAIN_FILE=..\cmake\toolchains\cmake_x64_win.toolchain&#xA; msbuild ALL_BUILD.vcxproj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE:&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The default CUDA version used by CMake is 11.4.2. To override this, for example to 10.2, append &lt;code&gt;-DCUDA_VERSION=10.2&lt;/code&gt; to the cmake command.&lt;/li&gt; &#xA;   &lt;li&gt;If samples fail to link on CentOS7, create this symbolic link: &lt;code&gt;ln -s $TRT_OUT_DIR/libnvinfer_plugin.so $TRT_OUT_DIR/libnvinfer_plugin.so.8&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Required CMake build arguments are:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;TRT_LIB_DIR&lt;/code&gt;: Path to the TensorRT installation directory containing libraries.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;TRT_OUT_DIR&lt;/code&gt;: Output directory where generated build artifacts will be copied.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Optional CMake build arguments:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;CMAKE_BUILD_TYPE&lt;/code&gt;: Specify if binaries generated are for release or debug (contain debug symbols). Values consists of [&lt;code&gt;Release&lt;/code&gt;] | &lt;code&gt;Debug&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CUDA_VERISON&lt;/code&gt;: The version of CUDA to target, for example [&lt;code&gt;11.4.2&lt;/code&gt;].&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CUDNN_VERSION&lt;/code&gt;: The version of cuDNN to target, for example [&lt;code&gt;8.2&lt;/code&gt;].&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;PROTOBUF_VERSION&lt;/code&gt;: The version of Protobuf to use, for example [&lt;code&gt;3.0.0&lt;/code&gt;]. Note: Changing this will not configure CMake to use a system version of Protobuf, it will configure CMake to download and try building that version.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CMAKE_TOOLCHAIN_FILE&lt;/code&gt;: The path to a toolchain file for cross compilation.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;BUILD_PARSERS&lt;/code&gt;: Specify if the parsers should be built, for example [&lt;code&gt;ON&lt;/code&gt;] | &lt;code&gt;OFF&lt;/code&gt;. If turned OFF, CMake will try to find precompiled versions of the parser libraries to use in compiling samples. First in &lt;code&gt;${TRT_LIB_DIR}&lt;/code&gt;, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;BUILD_PLUGINS&lt;/code&gt;: Specify if the plugins should be built, for example [&lt;code&gt;ON&lt;/code&gt;] | &lt;code&gt;OFF&lt;/code&gt;. If turned OFF, CMake will try to find a precompiled version of the plugin library to use in compiling samples. First in &lt;code&gt;${TRT_LIB_DIR}&lt;/code&gt;, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;BUILD_SAMPLES&lt;/code&gt;: Specify if the samples should be built, for example [&lt;code&gt;ON&lt;/code&gt;] | &lt;code&gt;OFF&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GPU_ARCHS&lt;/code&gt;: GPU (SM) architectures to target. By default we generate CUDA code for all major SMs. Specific SM versions can be specified here as a quoted space-separated list to reduce compilation time and binary size. Table of compute capabilities of NVIDIA GPUs can be found &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;here&lt;/a&gt;. Examples: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;NVidia A100: &lt;code&gt;-DGPU_ARCHS=&#34;80&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Tesla T4, GeForce RTX 2080: &lt;code&gt;-DGPU_ARCHS=&#34;75&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Titan V, Tesla V100: &lt;code&gt;-DGPU_ARCHS=&#34;70&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Multiple SMs: &lt;code&gt;-DGPU_ARCHS=&#34;80 75&#34;&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;TRT_PLATFORM_ID&lt;/code&gt;: Bare-metal build (unlike containerized cross-compilation) on non Linux/x86 platforms must explicitly specify the target platform. Currently supported options: &lt;code&gt;x86_64&lt;/code&gt; (default), &lt;code&gt;aarch64&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;h2&gt;TensorRT Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT Developer Home&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html&#34;&gt;TensorRT QuickStart Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html&#34;&gt;TensorRT Developer Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html&#34;&gt;TensorRT Sample Support Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/index.html#tools&#34;&gt;TensorRT ONNX Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devtalk.nvidia.com/default/board/304/tensorrt/&#34;&gt;TensorRT Discussion Forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html&#34;&gt;TensorRT Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please refer to &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#tensorrt-8&#34;&gt;TensorRT 8.2 Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>CMU-Perceptual-Computing-Lab/openpose</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/CMU-Perceptual-Computing-Lab/openpose</id>
    <link href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/Logo_main_black.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Build Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Linux&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;MacOS&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Windows&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Build Status&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34;&gt;&lt;strong&gt;OpenPose&lt;/strong&gt;&lt;/a&gt; has represented the &lt;strong&gt;first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is &lt;strong&gt;authored by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;Ginés Hidalgo&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~zhecao&#34;&gt;&lt;strong&gt;Zhe Cao&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34;&gt;&lt;strong&gt;Tomas Simon&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=sFQD3k4AAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Shih-En Wei&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://jhugestar.github.io&#34;&gt;&lt;strong&gt;Hanbyul Joo&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;http://www.cs.cmu.edu/~yaser&#34;&gt;&lt;strong&gt;Yaser Sheikh&lt;/strong&gt;&lt;/a&gt;. It is &lt;strong&gt;maintained by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;Ginés Hidalgo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;. OpenPose would not be possible without the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34;&gt;&lt;strong&gt;CMU Panoptic Studio dataset&lt;/strong&gt;&lt;/a&gt;. We would also like to thank all the people who &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/09_authors_and_contributors.md&#34;&gt;has helped OpenPose in any way&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face_hands.gif&#34; width=&#34;480&#34;&gt; &lt;br&gt; &lt;sup&gt;Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Ginés Hidalgo&lt;/a&gt; (left) and &lt;a href=&#34;https://jhugestar.github.io&#34; target=&#34;_blank&#34;&gt;Hanbyul Joo&lt;/a&gt; (right) in front of the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34; target=&#34;_blank&#34;&gt;CMU Panoptic Studio&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#related-work&#34;&gt;Related Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#quick-start-overview&#34;&gt;Quick Start Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#send-us-feedback&#34;&gt;Send Us Feedback!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Whole-body (Body, Foot, Face, and Hands) 2D Pose Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/dance_foot.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_hands.gif&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;Testing OpenPose: (Left) &lt;a href=&#34;https://www.youtube.com/watch?v=2DiQUX11YaY&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;Crazy Uptown Funk flashmob in Sydney&lt;/i&gt;&lt;/a&gt; video sequence. (Center and right) Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Ginés Hidalgo&lt;/a&gt; and &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34; target=&#34;_blank&#34;&gt;Tomas Simon&lt;/a&gt; testing face and hands&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Whole-body 3D Pose Reconstruction and Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose3d.gif&#34; width=&#34;360&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; testing the OpenPose 3D Module&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Unity Plugin&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_main.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_body_foot.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_hand_face.png&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; and &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Ginés Hidalgo&lt;/a&gt; testing the &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34; target=&#34;_blank&#34;&gt;OpenPose Unity Plugin&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Runtime Analysis&lt;/h3&gt; &#xA;&lt;p&gt;We show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose_vs_competition.png&#34; width=&#34;360&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Main Functionality&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;2D real-time multi-person keypoint detection&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;15, 18 or &lt;strong&gt;25-keypoint body/foot keypoint estimation&lt;/strong&gt;, including &lt;strong&gt;6 foot keypoints&lt;/strong&gt;. &lt;strong&gt;Runtime invariant to number of detected people&lt;/strong&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;2x21-keypoint hand keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;70-keypoint face keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/3d_reconstruction_module.md&#34;&gt;&lt;strong&gt;3D real-time single-person keypoint detection&lt;/strong&gt;&lt;/a&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3D triangulation from multiple single views.&lt;/li&gt; &#xA;     &lt;li&gt;Synchronization of Flir cameras handled.&lt;/li&gt; &#xA;     &lt;li&gt;Compatible with Flir/Point Grey cameras.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/calibration_module.md&#34;&gt;&lt;strong&gt;Calibration toolbox&lt;/strong&gt;&lt;/a&gt;: Estimation of distortion, intrinsic, and extrinsic camera parameters.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Single-person tracking&lt;/strong&gt; for further speedup or visual smoothing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware compatibility&lt;/strong&gt;: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Usage Alternatives&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/01_demo.md&#34;&gt;&lt;strong&gt;Command-line demo&lt;/strong&gt;&lt;/a&gt; for built-in functionality.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/04_cpp_api.md/&#34;&gt;&lt;strong&gt;C++ API&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/03_python_api.md&#34;&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/a&gt; for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For further details, check the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/07_major_released_features.md&#34;&gt;major released features&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/08_release_notes.md&#34;&gt;release notes&lt;/a&gt; docs.&lt;/p&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose training code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/&#34;&gt;&lt;strong&gt;OpenPose foot dataset&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34;&gt;&lt;strong&gt;OpenPose Unity Plugin&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenPose papers published in &lt;strong&gt;IEEE TPAMI and CVPR&lt;/strong&gt;. Cite them in your publications if OpenPose helps your research! (Links and more details in the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt; section below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use OpenPose without installing or writing any code, simply &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#windows-portable-demo&#34;&gt;download and use the latest Windows portable version of OpenPose&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, you could &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source&#34;&gt;build OpenPose from source&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installation doc&lt;/a&gt; for all the alternatives.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Overview&lt;/h2&gt; &#xA;&lt;p&gt;Simply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also add any of the available flags in any order. E.g., the following example runs on a video (&lt;code&gt;--video {PATH}&lt;/code&gt;), enables face (&lt;code&gt;--face&lt;/code&gt;) and hands (&lt;code&gt;--hand&lt;/code&gt;), and saves the output keypoints on JSON files on disk (&lt;code&gt;--write_json {PATH}&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, you can also extend OpenPose&#39;s functionality from its Python and C++ APIs. After &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installing&lt;/a&gt; OpenPose, check its &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/00_index.md&#34;&gt;official doc&lt;/a&gt; for a quick overview of all the alternatives and tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;Send Us Feedback!&lt;/h2&gt; &#xA;&lt;p&gt;Our library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.&lt;/li&gt; &#xA; &lt;li&gt;Want to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/10_community_projects.md&#34;&gt;Community-based Projects&lt;/a&gt; section or even integrate it with OpenPose!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;, while the hand and face detectors also use &lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt; (the face detector was trained using the same procedure than the hand detector).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{8765346,&#xA;  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},&#xA;  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},&#xA;  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2019}&#xA;}&#xA;&#xA;@inproceedings{simon2017hand,&#xA;  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{cao2017realtime,&#xA;  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{wei2016cpm,&#xA;  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Convolutional pose machines},&#xA;  year = {2016}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Paper links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8765346&#34;&gt;IEEE TPAMI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;ArXiv&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.08050&#34;&gt;Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.00134&#34;&gt;Convolutional Pose Machines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/LICENSE&#34;&gt;license&lt;/a&gt; for further details. Interested in a commercial license? Check this &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt;. For commercial queries, use the &lt;code&gt;Contact&lt;/code&gt; section from the &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt; and also send a copy of that message to &lt;a href=&#34;mailto:yaser@cs.cmu.edu&#34;&gt;Yaser Sheikh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/MMKV</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/Tencent/MMKV</id>
    <link href="https://github.com/Tencent/MMKV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An efficient, small mobile key-value storage framework developed by WeChat. Works on Android, iOS, macOS, Windows, and POSIX.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/MMKV/raw/master/LICENSE.TXT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD_3-brightgreen.svg?style=flat&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-1.2.13-brightgreen.svg?sanitize=true&#34; alt=&#34;Release Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Platform-%20Android%20%7C%20iOS%2FmacOS%20%7C%20Win32%20%7C%20POSIX-brightgreen.svg?sanitize=true&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;中文版本请参看&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/README_CN.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MMKV is an &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;small&lt;/strong&gt;, &lt;strong&gt;easy-to-use&lt;/strong&gt; mobile key-value storage framework used in the WeChat application. It&#39;s currently available on &lt;strong&gt;Android&lt;/strong&gt;, &lt;strong&gt;iOS/macOS&lt;/strong&gt;, &lt;strong&gt;Win32&lt;/strong&gt; and &lt;strong&gt;POSIX&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for Android&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of Android to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;apply&lt;/code&gt; calls needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 50K in binary size&lt;/strong&gt;: MMKV adds about 50K per architecture on App size, and much less when zipped (APK).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via Maven&lt;/h3&gt; &#xA;&lt;p&gt;Add the following lines to &lt;code&gt;build.gradle&lt;/code&gt; on your app module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-gradle&#34;&gt;dependencies {&#xA;    implementation &#39;com.tencent:mmkv:1.2.13&#39;&#xA;    // replace &#34;1.2.13&#34; with any available version&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Starting from v1.2.8, MMKV has been &lt;strong&gt;migrated to Maven Central&lt;/strong&gt;.&lt;br&gt; For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_setup&#34;&gt;Android Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;apply&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say your &lt;code&gt;Application&lt;/code&gt; class, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;public void onCreate() {&#xA;    super.onCreate();&#xA;&#xA;    String rootDir = MMKV.initialize(this);&#xA;    System.out.println(&#34;mmkv root: &#34; + rootDir);&#xA;    //……&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;import com.tencent.mmkv.MMKV;&#xA;    &#xA;MMKV kv = MMKV.defaultMMKV();&#xA;&#xA;kv.encode(&#34;bool&#34;, true);&#xA;boolean bValue = kv.decodeBool(&#34;bool&#34;);&#xA;&#xA;kv.encode(&#34;int&#34;, Integer.MIN_VALUE);&#xA;int iValue = kv.decodeInt(&#34;int&#34;);&#xA;&#xA;kv.encode(&#34;string&#34;, &#34;Hello from mmkv&#34;);&#xA;String str = kv.decodeString(&#34;string&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_tutorial&#34;&gt;Android Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Writing random &lt;code&gt;int&lt;/code&gt; for 1000 times, we get this chart:&lt;br&gt; &lt;img src=&#34;https://github.com/Tencent/MMKV/wiki/assets/profile_android_mini.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; For more benchmark data, please refer to &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_benchmark&#34;&gt;our benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for iOS/macOS&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of iOS/macOS to achieve the best performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go, no configurations are needed. All changes are saved immediately, no &lt;code&gt;synchronize&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains encode/decode helpers and mmap logics and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Less than 30K in binary size&lt;/strong&gt;: MMKV adds less than 30K per architecture on App size, and much less when zipped (IPA).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via CocoaPods:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://guides.CocoaPods.org/using/getting-started.html&#34;&gt;CocoaPods&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Open the terminal, &lt;code&gt;cd&lt;/code&gt; to your project directory, run &lt;code&gt;pod repo update&lt;/code&gt; to make CocoaPods aware of the latest available MMKV versions;&lt;/li&gt; &#xA; &lt;li&gt;Edit your Podfile, add &lt;code&gt;pod &#39;MMKV&#39;&lt;/code&gt; to your app target;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pod install&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;.xcworkspace&lt;/code&gt; file generated by CocoaPods;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;#import &amp;lt;MMKV/MMKV.h&amp;gt;&lt;/code&gt; to your source file and we are done.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_setup&#34;&gt;iOS/macOS Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go, no configurations are needed. All changes are saved immediately, no &lt;code&gt;synchronize&lt;/code&gt; calls are needed. Setup MMKV on App startup, in your &lt;code&gt;-[MyApp application: didFinishLaunchingWithOptions:]&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-objective-c&#34;&gt;- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {&#xA;    // init MMKV in the main thread&#xA;    [MMKV initializeMMKV:nil];&#xA;&#xA;    //...&#xA;    return YES;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-objective-c&#34;&gt;MMKV *mmkv = [MMKV defaultMMKV];&#xA;    &#xA;[mmkv setBool:YES forKey:@&#34;bool&#34;];&#xA;BOOL bValue = [mmkv getBoolForKey:@&#34;bool&#34;];&#xA;    &#xA;[mmkv setInt32:-1024 forKey:@&#34;int32&#34;];&#xA;int32_t iValue = [mmkv getInt32ForKey:@&#34;int32&#34;];&#xA;    &#xA;[mmkv setString:@&#34;hello, mmkv&#34; forKey:@&#34;string&#34;];&#xA;NSString *str = [mmkv getStringForKey:@&#34;string&#34;];&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_tutorial&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Writing random &lt;code&gt;int&lt;/code&gt; for 10000 times, we get this chart:&lt;br&gt; &lt;img src=&#34;https://github.com/Tencent/MMKV/wiki/assets/profile_mini.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; For more benchmark data, please refer to &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_benchmark&#34;&gt;our benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for Win32&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of Windows to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;save&lt;/code&gt;, no &lt;code&gt;sync&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 10K in binary size&lt;/strong&gt;: MMKV adds about 10K on application size, and much less when zipped.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via Source&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Getting source code from git repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/MMKV.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;Win32/MMKV/MMKV.vcxproj&lt;/code&gt; to your solution;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;MMKV&lt;/code&gt; project to your project&#39;s dependencies;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;$(OutDir)include&lt;/code&gt; to your project&#39;s &lt;code&gt;C/C++&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Include Directories&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;$(OutDir)&lt;/code&gt; to your project&#39;s &lt;code&gt;Linker&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Library Directories&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;MMKV.lib&lt;/code&gt; to your project&#39;s &lt;code&gt;Linker&lt;/code&gt; -&amp;gt; &lt;code&gt;Input&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Dependencies&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;#include &amp;lt;MMKV/MMKV.h&amp;gt;&lt;/code&gt; to your source file and we are done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;MMKV is compiled with &lt;code&gt;MT/MTd&lt;/code&gt; runtime by default. If your project uses &lt;code&gt;MD/MDd&lt;/code&gt;, you should change MMKV&#39;s setting to match your project&#39;s (&lt;code&gt;C/C++&lt;/code&gt; -&amp;gt; &lt;code&gt;Code Generation&lt;/code&gt; -&amp;gt; &lt;code&gt;Runtime Library&lt;/code&gt;), or vice versa.&lt;/li&gt; &#xA; &lt;li&gt;MMKV is developed with Visual Studio 2017, change the &lt;code&gt;Platform Toolset&lt;/code&gt; if you use a different version of Visual Studio.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/windows_setup&#34;&gt;Win32 Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;save&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say in your &lt;code&gt;main()&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;#include &amp;lt;MMKV/MMKV.h&amp;gt;&#xA;&#xA;int main() {&#xA;    std::wstring rootDir = getYourAppDocumentDir();&#xA;    MMKV::initializeMMKV(rootDir);&#xA;    //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto mmkv = MMKV::defaultMMKV();&#xA;&#xA;mmkv-&amp;gt;set(true, &#34;bool&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;bool = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getBool(&#34;bool&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(1024, &#34;int32&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;int32 = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getInt32(&#34;int32&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(&#34;Hello, MMKV for Win32&#34;, &#34;string&#34;);&#xA;std::string result;&#xA;mmkv-&amp;gt;getString(&#34;string&#34;, result);&#xA;std::cout &amp;lt;&amp;lt; &#34;string = &#34; &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/windows_tutorial&#34;&gt;Win32 Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for POSIX&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of POSIX to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;save&lt;/code&gt;, no &lt;code&gt;sync&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 7K in binary size&lt;/strong&gt;: MMKV adds about 7K on application size, and much less when zipped.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via CMake&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Getting source code from the git repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/MMKV.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Edit your &lt;code&gt;CMakeLists.txt&lt;/code&gt;, add those lines:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;add_subdirectory(mmkv/POSIX/src mmkv)&#xA;target_link_libraries(MyApp&#xA;    mmkv)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;#include &#34;MMKV.h&#34;&lt;/code&gt; to your source file and we are done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/posix_setup&#34;&gt;POSIX Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;save&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say in your &lt;code&gt;main()&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;#include &#34;MMKV.h&#34;&#xA;&#xA;int main() {&#xA;    std::string rootDir = getYourAppDocumentDir();&#xA;    MMKV::initializeMMKV(rootDir);&#xA;    //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto mmkv = MMKV::defaultMMKV();&#xA;&#xA;mmkv-&amp;gt;set(true, &#34;bool&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;bool = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getBool(&#34;bool&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(1024, &#34;int32&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;int32 = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getInt32(&#34;int32&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(&#34;Hello, MMKV for Win32&#34;, &#34;string&#34;);&#xA;std::string result;&#xA;mmkv-&amp;gt;getString(&#34;string&#34;, result);&#xA;std::cout &amp;lt;&amp;lt; &#34;string = &#34; &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/posix_tutorial&#34;&gt;POSIX Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MMKV is published under the BSD 3-Clause license. For details check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/LICENSE.TXT&#34;&gt;LICENSE.TXT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for details of change history.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, also join our &lt;a href=&#34;https://opensource.tencent.com/contribution&#34;&gt;Tencent OpenSource Plan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To give clarity of what is expected of our members, MMKV has adopted the code of conduct defined by the Contributor Covenant, which is widely used. And we think it articulates our values well. For more, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ &amp;amp; Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; first. Should there be any questions, don&#39;t hesitate to create &lt;a href=&#34;https://github.com/Tencent/MMKV/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Personal Information Protection Rules&lt;/h2&gt; &#xA;&lt;p&gt;User privacy is taken very seriously: MMKV does not obtain, collect or upload any personal information. Please refer to the &lt;a href=&#34;https://support.weixin.qq.com/cgi-bin/mmsupportacctnodeweb-bin/pages/aY5BAtRiO1BpoHxo&#34;&gt;MMKV SDK Personal Information Protection Rules&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>changkun/modern-cpp-tutorial</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/changkun/modern-cpp-tutorial</id>
    <link href="https://github.com/changkun/modern-cpp-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;📚 Modern C++ Tutorial: C++11/14/17/20 On the Fly | https://changkun.de/modern-cpp/&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/assets/cover-2nd-en.png&#34; alt=&#34;logo&#34; height=&#34;550&#34; align=&#34;right&#34;&gt; &#xA;&lt;h1&gt;Modern C++ Tutorial: C++11/14/17/20 On the Fly&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/travis/changkun/modern-cpp-tutorial/master?style=flat-square&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-English-blue.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/README-zh-cn.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-red.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/assets/donate.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E2%82%AC-donate-ff69b4.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Purpose&lt;/h2&gt; &#xA;&lt;p&gt;The book claims to be &#34;On the Fly&#34;. Its intent is to provide a comprehensive introduction to the relevant features regarding modern C++ (before 2020s). Readers can choose interesting content according to the following table of content to learn and quickly familiarize the new features you would like to learn. Readers should be aware that not all of these features are required. Instead, it should be learned when you really need it.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, instead of coding-only, the book introduces the historical background of its technical requirements (as simple as possible), which provides great help in understanding why these features came out.&lt;/p&gt; &#xA;&lt;p&gt;In addition, the author would like to encourage readers to use modern C++ directly in their new projects and migrate their old projects to modern C++ gradually after reading the book.&lt;/p&gt; &#xA;&lt;h2&gt;Targets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This book assumes that readers are already familiar with traditional C++ (i.e. C++98 or earlier), or at least that they do not have any difficulty in reading traditional C++ code. In other words, those who have long experience in traditional C++ and people who desire to quickly understand the features of modern C++ in a short period of time are well suited to read the book.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This book introduces, to a certain extent, the dark magic of modern C++. However, these magic tricks are very limited, they are not suitable for readers who want to learn advanced C++. The purpose of this book is offering a quick start for modern C++. Of course, advanced readers can also use this book to review and examine themselves on modern C++.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Start&lt;/h2&gt; &#xA;&lt;p&gt;You can choose from the following reading methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/book/en-us/toc.md&#34;&gt;GitHub Online&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp/pdf/modern-cpp-tutorial-en-us.pdf&#34;&gt;PDF document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp/epub/modern-cpp-tutorial-en-us.epub&#34;&gt;EPUB document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp&#34;&gt;Website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code&lt;/h2&gt; &#xA;&lt;p&gt;Each chapter of this book contains a lot of code. If you encounter problems while writing your own code with the introductory features of the book, reading the source code attached to the book might be of help. You can find the book &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/code&#34;&gt;here&lt;/a&gt;. All the code is organized by chapter, the folder name is the chapter number.&lt;/p&gt; &#xA;&lt;h2&gt;Exercises&lt;/h2&gt; &#xA;&lt;p&gt;There are few exercises at the end of each chapter of the book. These are meant to test whether you have mastered the knowledge in the current chapter. You can find the possible answer to the problem &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/exercises&#34;&gt;here&lt;/a&gt;. Again, the folder name is the chapter number.&lt;/p&gt; &#xA;&lt;h2&gt;Website&lt;/h2&gt; &#xA;&lt;p&gt;The source code of the &lt;a href=&#34;https://changkun.de/modern-cpp&#34;&gt;website&lt;/a&gt; of this book can be found &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/website&#34;&gt;here&lt;/a&gt;, which is built by &lt;a href=&#34;https://hexo.io&#34;&gt;hexo&lt;/a&gt; and &lt;a href=&#34;https://vuejs.org&#34;&gt;vuejs&lt;/a&gt;. The website provides you another way of reading the book, it also adapts to mobile browsers.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in building everything locally, it is recommended using &lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Docker&lt;/a&gt;. To build, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This book was originally written in Chinese by &lt;a href=&#34;https://changkun.de&#34;&gt;Changkun Ou&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The author has limited time and language skills. If readers find any mistakes in the book or any language improvements, please feel free to open an &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/issues&#34;&gt;Issue&lt;/a&gt; or start a &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/pulls&#34;&gt;Pull request&lt;/a&gt;. For detailed guidelines and checklist, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/CONTRIBUTING.md&#34;&gt;How to contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The author is grateful to all contributors, including but not limited to &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is also supported by:&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://www.digitalocean.com/?refcode=834a3bbc951b&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=CopyPaste&#34;&gt; &lt;img src=&#34;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg?sanitize=true&#34; width=&#34;201px&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;This work was written by &lt;a href=&#34;https://changkun.de&#34;&gt;Ou Changkun&lt;/a&gt; and licensed under a &lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License&lt;/a&gt;. The code of this repository is open sourced under the &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/pytorch</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/pytorch/pytorch</id>
    <link href="https://github.com/pytorch/pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/pytorch-logo-dark.png&#34; alt=&#34;PyTorch Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt; &#xA; &lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.&lt;/p&gt; &#xA;&lt;p&gt;Our trunk health (Continuous Integration signals) can be found at &lt;a href=&#34;https://hud.pytorch.org/ci/pytorch/pytorch/master&#34;&gt;hud.pytorch.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#more-about-pytorch&#34;&gt;More About PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#a-gpu-ready-tensor-library&#34;&gt;A GPU-Ready Tensor Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#dynamic-neural-networks-tape-based-autograd&#34;&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#python-first&#34;&gt;Python First&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#imperative-experiences&#34;&gt;Imperative Experiences&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#fast-and-lean&#34;&gt;Fast and Lean&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#extensions-without-pain&#34;&gt;Extensions Without Pain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#binaries&#34;&gt;Binaries&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#nvidia-jetson-platforms&#34;&gt;NVIDIA Jetson Platforms&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#from-source&#34;&gt;From Source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-dependencies&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#get-the-pytorch-source&#34;&gt;Get the PyTorch Source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-pytorch&#34;&gt;Install PyTorch&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#adjust-build-options-optional&#34;&gt;Adjust Build Options (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#docker-image&#34;&gt;Docker Image&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#using-pre-built-images&#34;&gt;Using pre-built images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-image-yourself&#34;&gt;Building the image yourself&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-documentation&#34;&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#previous-versions&#34;&gt;Previous Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#releases-and-contributing&#34;&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34;&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a Tensor library like NumPy, with strong GPU support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html&#34;&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/jit.html&#34;&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/nn.html&#34;&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/multiprocessing.html&#34;&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/data.html&#34;&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, PyTorch is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A replacement for NumPy to use the power of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;A GPU-Ready Tensor Library&lt;/h3&gt; &#xA;&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/tensor_illustration.png&#34; alt=&#34;Tensor illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.&lt;/p&gt; &#xA;&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt; &#xA;&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt; &#xA;&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as &lt;a href=&#34;https://github.com/twitter/torch-autograd&#34;&gt;torch-autograd&lt;/a&gt;, &lt;a href=&#34;https://github.com/HIPS/autograd&#34;&gt;autograd&lt;/a&gt;, &lt;a href=&#34;https://chainer.org&#34;&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;While this technique is not unique to PyTorch, it&#39;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif&#34; alt=&#34;Dynamic graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python First&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; / &lt;a href=&#34;https://www.scipy.org/&#34;&gt;SciPy&lt;/a&gt; / &lt;a href=&#34;https://scikit-learn.org&#34;&gt;scikit-learn&lt;/a&gt; etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt; and &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;. Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt; &#xA;&lt;h3&gt;Imperative Experiences&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn&#39;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Lean&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries such as &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel MKL&lt;/a&gt; and NVIDIA (&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.&lt;/p&gt; &#xA;&lt;p&gt;Hence, PyTorch is quite fast – whether you run small or large neural networks.&lt;/p&gt; &#xA;&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We&#39;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions Without Pain&lt;/h3&gt; &#xA;&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch&#39;s Tensor API was designed to be straightforward and with minimal abstractions.&lt;/p&gt; &#xA;&lt;p&gt;You can write new neural network layers in Python using the torch API &lt;a href=&#34;https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html&#34;&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;a tutorial here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/extension-cpp&#34;&gt;an example here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;p&gt;Commands to install binaries via Conda or pip wheels are on our website: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;NVIDIA Jetson Platforms&lt;/h4&gt; &#xA;&lt;p&gt;Python wheels for NVIDIA&#39;s Jetson Nano, Jetson TX2, and Jetson AGX Xavier are provided &lt;a href=&#34;https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048&#34;&gt;here&lt;/a&gt; and the L4T container is published &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They require JetPack 4.2 and above, and &lt;a href=&#34;https://github.com/dusty-nv&#34;&gt;@dusty-nv&lt;/a&gt; and &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;@ptrblck&lt;/a&gt; are maintaining them.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;If you are installing from source, you will need Python 3.7 or later and a C++14 compiler. Also, we highly recommend installing an &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt; &#xA;&lt;p&gt;Once you have &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; installed, here are the instructions.&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with CUDA support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVIDIA CUDA&lt;/a&gt; 10.2 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;NVIDIA cuDNN&lt;/a&gt; v7 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/ax3l/9489132&#34;&gt;Compiler&lt;/a&gt; compatible with CUDA Note: You could refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf&#34;&gt;cuDNN Support Matrix&lt;/a&gt; for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardwares&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable CUDA support, export the environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are building for NVIDIA&#39;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are &lt;a href=&#34;https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/&#34;&gt;available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with ROCm support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html&#34;&gt;AMD ROCm&lt;/a&gt; 4.0 and above installation&lt;/li&gt; &#xA; &lt;li&gt;ROCm is currently supported only for Linux systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable ROCm support, export the environment variable &lt;code&gt;USE_ROCM=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Common&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# CUDA only: Add LAPACK support for the GPU if needed&#xA;conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed&#xA;conda install pkg-config libuv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed.&#xA;# Distributed package support on Windows is a prototype feature and is subject to changes.&#xA;conda install -c conda-forge libuv=1.39&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Get the PyTorch Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/pytorch/pytorch&#xA;cd pytorch&#xA;# if you are updating an existing checkout&#xA;git submodule sync&#xA;git submodule update --init --recursive --jobs 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are compiling for ROCm, you must run this command first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/amd_build/build_amd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are using &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt;, you may experience an error caused by the linker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized&#xA;collect2: error: ld returned 1 exit status&#xA;error: command &#39;g++&#39; failed with exit status 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is caused by &lt;code&gt;ld&lt;/code&gt; from Conda environment shadowing the system &lt;code&gt;ld&lt;/code&gt;. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.7.6+ and 3.8.1+.&lt;/p&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA is not supported on macOS.&lt;/p&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;p&gt;Choose Correct Visual Studio Version.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes there are regressions in new versions of Visual Studio, so it&#39;s best to use the same Visual Studio Version &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.circleci/scripts/vs_install.ps1&#34;&gt;16.8.5&lt;/a&gt; as Pytorch CI&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from &lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;https://visualstudio.microsoft.com/visual-cpp-build-tools/&lt;/a&gt;. The build tools &lt;em&gt;do not&lt;/em&gt; come with Visual Studio Code by default.&lt;/p&gt; &#xA;&lt;p&gt;If you want to build legacy python code, please refer to &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md#building-on-legacy-code-and-cuda&#34;&gt;Building on legacy code and CUDA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build with CPU&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s fairly easy to build with CPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;conda activate&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#39;ll need to manually download the library and set up the building environment by tweaking &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;LIB&lt;/code&gt;. The instruction &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/notes/windows.rst#building-from-source&#34;&gt;here&lt;/a&gt; is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.&lt;/p&gt; &#xA;&lt;p&gt;Build with CUDA&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm&#34;&gt;NVTX&lt;/a&gt; is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called &#34;Nsight Compute&#34;. To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. &lt;br&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.&lt;/p&gt; &#xA;&lt;p&gt;Additional libraries such as &lt;a href=&#34;https://developer.nvidia.com/magma&#34;&gt;Magma&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN, a.k.a MKLDNN or DNNL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/mozilla/sccache&#34;&gt;Sccache&lt;/a&gt; are often needed. Please refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/tree/master/.jenkins/pytorch/win-test-helpers/installation-helpers&#34;&gt;installation-helper&lt;/a&gt; to install them.&lt;/p&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat&#34;&gt;build_pytorch.bat&lt;/a&gt; script for some other environment variables configurations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmd&#xA;&#xA;:: Set the environment variables after you have downloaded and upzipped the mkl package,&#xA;:: else CMake would throw an error as `Could NOT find OpenMP`.&#xA;set CMAKE_INCLUDE_PATH={Your directory}\mkl\include&#xA;set LIB={Your directory}\mkl\lib;%LIB%&#xA;&#xA;:: Read the content in the previous section carefully before you proceed.&#xA;:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&#xA;:: &#34;Visual Studio 2019 Developer Command Prompt&#34; will be run automatically.&#xA;:: Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&#xA;set CMAKE_GENERATOR_TOOLSET_VERSION=14.27&#xA;set DISTUTILS_USE_SDK=1&#xA;for /f &#34;usebackq tokens=*&#34; %i in (`&#34;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&#34; -version [15^,17^) -products * -latest -property installationPath`) do call &#34;%i\VC\Auxiliary\Build\vcvarsall.bat&#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%&#xA;&#xA;:: [Optional] If you want to override the CUDA host compiler&#xA;set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe&#xA;&#xA;python setup.py install&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Adjust Build Options (Optional)&lt;/h5&gt; &#xA;&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.&lt;/p&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;h4&gt;Using pre-built images&lt;/h4&gt; &#xA;&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building the image yourself&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;# images are tagged as docker.io/${your_docker_username}/pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the Documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build documentation in various formats, you will need &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the &lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt; &#xA;&lt;p&gt;If you get a katex error run &lt;code&gt;npm install katex&lt;/code&gt;. If it persists, try &lt;code&gt;npm install -g katex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Previous Versions&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found on &lt;a href=&#34;https://pytorch.org/previous-versions&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Three-pointers to get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;Examples: easy to understand PyTorch code across all domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/&#34;&gt;The API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/GLOSSARY.md&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;PyTorch Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;Intro to Deep Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34;&gt;Intro to Machine Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-networks-with-pytorch&#34;&gt;Deep Neural Networks with PyTorch from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PyTorch&#34;&gt;PyTorch Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/&#34;&gt;PyTorch Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw&#34;&gt;PyTorch YouTube&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: Discuss implementations, research, etc. &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt; &#xA; &lt;li&gt;Slack: The &lt;a href=&#34;https://pytorch.slack.com/&#34;&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href=&#34;https://goo.gl/forms/PP1AGvNHpSaJP8to1&#34;&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: &lt;a href=&#34;https://eepurl.com/cbG0rv&#34;&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Page: Important announcements about PyTorch. &lt;a href=&#34;https://www.facebook.com/pytorch&#34;&gt;https://www.facebook.com/pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For brand guidelines, please visit our website at &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a 90-day release cycle (major releases). Please let us know if you encounter a bug by &lt;a href=&#34;https://github.com/pytorch/pytorch/issues&#34;&gt;filing an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about making a contribution to Pytorch, please see our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch is currently maintained by &lt;a href=&#34;https://apaszke.github.io/&#34;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&#34;https://github.com/colesbury&#34;&gt;Sam Gross&lt;/a&gt;, &lt;a href=&#34;http://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt; and &lt;a href=&#34;https://github.com/gchanan&#34;&gt;Gregory Chanan&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project is unrelated to &lt;a href=&#34;https://github.com/hughperkins/pytorch&#34;&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/leveldb</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/google/leveldb</id>
    <link href="https://github.com/google/leveldb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;ci&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Authors: Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keys and values are arbitrary byte arrays.&lt;/li&gt; &#xA; &lt;li&gt;Data is stored sorted by key.&lt;/li&gt; &#xA; &lt;li&gt;Callers can provide a custom comparison function to override the sort order.&lt;/li&gt; &#xA; &lt;li&gt;The basic operations are &lt;code&gt;Put(key,value)&lt;/code&gt;, &lt;code&gt;Get(key)&lt;/code&gt;, &lt;code&gt;Delete(key)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiple changes can be made in one atomic batch.&lt;/li&gt; &#xA; &lt;li&gt;Users can create a transient snapshot to get a consistent view of data.&lt;/li&gt; &#xA; &lt;li&gt;Forward and backward iteration is supported over the data.&lt;/li&gt; &#xA; &lt;li&gt;Data is automatically compressed using the &lt;a href=&#34;https://google.github.io/snappy/&#34;&gt;Snappy compression library&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/raw/main/doc/index.md&#34;&gt;LevelDB library documentation&lt;/a&gt; is online and bundled with the source code.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is not a SQL database. It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.&lt;/li&gt; &#xA; &lt;li&gt;Only a single process (possibly multi-threaded) can access a particular database at a time.&lt;/li&gt; &#xA; &lt;li&gt;There is no client-server support builtin to the library. An application that needs such support will have to wrap their own server around the library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting the Source&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/google/leveldb.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;This project supports &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; out of the box.&lt;/p&gt; &#xA;&lt;h3&gt;Build for POSIX&lt;/h3&gt; &#xA;&lt;p&gt;Quick start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake -DCMAKE_BUILD_TYPE=Release .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building for Windows&lt;/h3&gt; &#xA;&lt;p&gt;First generate the Visual Studio 2017 project/solution files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -G &#34;Visual Studio 15&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default default will build for x86. For 64-bit run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmake -G &#34;Visual Studio 15 Win64&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile the Windows solution from the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;devenv /build Debug leveldb.sln&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or open leveldb.sln in Visual Studio and build from within.&lt;/p&gt; &#xA;&lt;p&gt;Please see the CMake documentation and &lt;code&gt;CMakeLists.txt&lt;/code&gt; for more advanced usage.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing to the leveldb Project&lt;/h1&gt; &#xA;&lt;p&gt;The leveldb project welcomes contributions. leveldb&#39;s primary goal is to be a reliable and fast key/value store. Changes that are in line with the features/limitations outlined above, and meet the requirements below, will be considered.&lt;/p&gt; &#xA;&lt;p&gt;Contribution requirements:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tested platforms only&lt;/strong&gt;. We &lt;em&gt;generally&lt;/em&gt; will only accept changes for platforms that are compiled and tested. This means POSIX (for Linux and macOS) or Windows. Very small changes will sometimes be accepted, but consider that more of an exception than the rule.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stable API&lt;/strong&gt;. We strive very hard to maintain a stable API. Changes that require changes for projects using leveldb &lt;em&gt;might&lt;/em&gt; be rejected without sufficient benefit to the project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tests&lt;/strong&gt;: All changes must be accompanied by a new (or changed) test, or a sufficient explanation as to why a new (or changed) test is not required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Style&lt;/strong&gt;: This project conforms to the &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google C++ Style Guide&lt;/a&gt;. To ensure your changes are properly formatted please run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clang-format -i --style=file &amp;lt;file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We are unlikely to accept contributions to the build configuration files, such as &lt;code&gt;CMakeLists.txt&lt;/code&gt;. We are focused on maintaining a build configuration that allows us to test that the project works in a few supported configurations inside Google. We are not currently interested in supporting other requirements, such as different operating systems, compilers, or build systems.&lt;/p&gt; &#xA;&lt;h2&gt;Submitting a Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;Before any pull request will be accepted the author must first sign a Contributor License Agreement (CLA) at &lt;a href=&#34;https://cla.developers.google.com/&#34;&gt;https://cla.developers.google.com/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to keep the commit timeline linear &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits&#34;&gt;squash&lt;/a&gt; your changes down to a single commit and &lt;a href=&#34;https://git-scm.com/docs/git-rebase&#34;&gt;rebase&lt;/a&gt; on google/leveldb/main. This keeps the commit timeline linear and more easily sync&#39;ed with the internal repository at Google. More information at GitHub&#39;s &lt;a href=&#34;https://help.github.com/articles/about-git-rebase/&#34;&gt;About Git rebase&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Here is a performance report (with explanations) from the run of the included db_bench program. The results are somewhat noisy, but should be enough to get a ballpark performance estimate.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use a database with a million entries. Each entry has a 16 byte key, and a 100 byte value. Values used by the benchmark compress to about half their original size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LevelDB:    version 1.1&#xA;Date:       Sun May  1 12:11:26 2011&#xA;CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz&#xA;CPUCache:   4096 KB&#xA;Keys:       16 bytes each&#xA;Values:     100 bytes each (50 bytes after compression)&#xA;Entries:    1000000&#xA;Raw Size:   110.6 MB (estimated)&#xA;File Size:  62.9 MB (estimated)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Write performance&lt;/h2&gt; &#xA;&lt;p&gt;The &#34;fill&#34; benchmarks create a brand new database, in either sequential, or random order. The &#34;fillsync&#34; benchmark flushes data from the operating system to the disk after every operation; the other write operations leave the data sitting in the operating system buffer cache for a while. The &#34;overwrite&#34; benchmark does random writes that update existing keys in the database.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fillseq      :       1.765 micros/op;   62.7 MB/s&#xA;fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)&#xA;fillrandom   :       2.460 micros/op;   45.0 MB/s&#xA;overwrite    :       2.380 micros/op;   46.5 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each &#34;op&#34; above corresponds to a write of a single key/value pair. I.e., a random write benchmark goes at approximately 400,000 writes per second.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;fillsync&#34; operation costs much less (0.3 millisecond) than a disk seek (typically 10 milliseconds). We suspect that this is because the hard disk itself is buffering the update in its memory and responding before the data has been written to the platter. This may or may not be safe based on whether or not the hard disk has enough power to save its memory in the event of a power failure.&lt;/p&gt; &#xA;&lt;h2&gt;Read performance&lt;/h2&gt; &#xA;&lt;p&gt;We list the performance of reading sequentially in both the forward and reverse direction, and also the performance of a random lookup. Note that the database created by the benchmark is quite small. Therefore the report characterizes the performance of leveldb when the working set fits in memory. The cost of reading a piece of data that is not present in the operating system buffer cache will be dominated by the one or two disk seeks needed to fetch the data from disk. Write performance will be mostly unaffected by whether or not the working set fits in memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)&#xA;readseq     :  0.476 micros/op;  232.3 MB/s&#xA;readreverse :  0.724 micros/op;  152.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LevelDB compacts its underlying storage data in the background to improve read performance. The results listed above were done immediately after a lot of random writes. The results after compactions (which are usually triggered automatically) are better.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)&#xA;readseq     :  0.423 micros/op;  261.8 MB/s&#xA;readreverse :  0.663 micros/op;  166.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the high cost of reads comes from repeated decompression of blocks read from disk. If we supply enough cache to the leveldb so it can hold the uncompressed blocks in memory, the read performance improves again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)&#xA;readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository contents&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/index.md&#34;&gt;doc/index.md&lt;/a&gt; for more explanation. See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/impl.md&#34;&gt;doc/impl.md&lt;/a&gt; for a brief overview of the implementation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in include/leveldb/*.h. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Guide to header files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/db.h&lt;/strong&gt;: Main interface to the DB: Start here.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/options.h&lt;/strong&gt;: Control over the behavior of an entire database, and also control over the behavior of individual reads and writes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/comparator.h&lt;/strong&gt;: Abstraction for user-specified comparison function. If you want just bytewise comparison of keys, you can use the default comparator, but clients can write their own comparator implementations if they want custom ordering (e.g. to handle different character encodings, etc.).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/iterator.h&lt;/strong&gt;: Interface for iterating over data. You can get an iterator from a DB object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/write_batch.h&lt;/strong&gt;: Interface for atomically applying multiple updates to a database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/slice.h&lt;/strong&gt;: A simple module for maintaining a pointer and a length into some other byte array.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/status.h&lt;/strong&gt;: Status is returned from many of the public interfaces and is used to report success and various kinds of errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/env.h&lt;/strong&gt;: Abstraction of the OS environment. A posix implementation of this interface is in util/env_posix.cc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/table.h, include/leveldb/table_builder.h&lt;/strong&gt;: Lower-level modules that most clients probably won&#39;t use directly.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/onnxruntime</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/microsoft/onnxruntime</id>
    <link href="https://github.com/microsoft/onnxruntime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/images/ONNX_Runtime_logo_dark.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime is a cross-platform inference and training machine-learning accelerator&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime inference&lt;/strong&gt; can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing&#34;&gt;Learn more →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime training&lt;/strong&gt; can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-training&#34;&gt;Learn more →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;General Information&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai&#34;&gt;onnxruntime.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage documention and tutorials&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai/docs&#34;&gt;onnxruntime.ai/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Companion sample repositories&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ONNX Runtime Inferencing: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-inference-examples&#34;&gt;microsoft/onnxruntime-inference-examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ONNX Runtime Training: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-training-examples&#34;&gt;microsoft/onnxruntime-training-examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build Pipeline Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;EPs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20x64%20NoContribops%20CI%20Pipeline?label=Linux+CPU+x64+No+Contrib+Ops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=78&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/centos7_cpu?label=Linux+CentOS7&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=140&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-distributed?label=Distributed+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20NUPHAR%20CI%20Pipeline?label=Linux+NUPHAR&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=55&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline?label=Linux+OpenVINO&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mac&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=65&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20NoContribops%20CI%20Pipeline?label=MacOS+NoContribops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Android&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;iOS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WebAssembly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20WebAssembly%20CI%20Pipeline?label=WASM&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data/Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Windows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/Privacy.md&#34;&gt;privacy statement&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and Feedback&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For feature requests or bug reports, please file a &lt;a href=&#34;https://github.com/Microsoft/onnxruntime/issues&#34;&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general discussion or questions, please use &lt;a href=&#34;https://github.com/microsoft/onnxruntime/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SerenityOS/serenity</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/SerenityOS/serenity</id>
    <link href="https://github.com/SerenityOS/serenity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Serenity Operating System 🐞&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SerenityOS&lt;/h1&gt; &#xA;&lt;p&gt;Graphical Unix-like operating system for x86 computers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SerenityOS/serenity/actions?query=workflow%3A%22Build%2C%20lint%2C%20and%20test%22&#34;&gt;&lt;img src=&#34;https://github.com/SerenityOS/serenity/workflows/Build,%20lint,%20and%20test/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_build/latest?definitionId=1&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_apis/build/status/CI?branchName=master&#34; alt=&#34;Azure DevOps Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:serenity&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/serenity.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=SerenityOS_serenity&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=SerenityOS_serenity&amp;amp;metric=ncloc&#34; alt=&#34;Sonar Cube Static Analysis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/830522505605283862.svg?logo=discord&amp;amp;logoColor=white&amp;amp;logoWidth=20&amp;amp;labelColor=7289DA&amp;amp;label=Discord&amp;amp;color=17cf48&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is a love letter to &#39;90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.&lt;/p&gt; &#xA;&lt;p&gt;Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix. This is a system by us, for us, based on the things we like.&lt;/p&gt; &#xA;&lt;p&gt;You can watch videos of the system being developed on YouTube:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/andreaskling&#34;&gt;Andreas Kling&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/linusgroh&#34;&gt;Linus Groh&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;FAQ&lt;/strong&gt;: &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshot&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Meta/Screenshots/screenshot-b36968c.png&#34; alt=&#34;Screenshot as of b36968c.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modern x86 32-bit and 64-bit kernel with pre-emptive multi-threading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Applications/Browser/&#34;&gt;Browser&lt;/a&gt; with JavaScript, WebAssembly, and more (check the spec compliance for &lt;a href=&#34;https://libjs.dev/test262/&#34;&gt;JS&lt;/a&gt;, &lt;a href=&#34;https://css.tobyase.de/&#34;&gt;CSS&lt;/a&gt;, and &lt;a href=&#34;https://libjs.dev/wasm/&#34;&gt;WASM&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Security features (hardware protections, limited userland capabilities, W^X memory, &lt;code&gt;pledge&lt;/code&gt; &amp;amp; &lt;code&gt;unveil&lt;/code&gt;, (K)ASLR, OOM-resistance, web-content isolation, state-of-the-art TLS algorithms, ...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Services/&#34;&gt;System services&lt;/a&gt; (WindowServer, LoginServer, AudioServer, WebServer, RequestServer, CrashServer, ...) and modern IPC&lt;/li&gt; &#xA; &lt;li&gt;Good POSIX compatibility (&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/LibC/&#34;&gt;LibC&lt;/a&gt;, Shell, syscalls, signals, pseudoterminals, filesystem notifications, standard Unix &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Utilities/&#34;&gt;utilities&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;POSIX-like virtual file systems (/proc, /dev, /sys, /tmp, ...) and ext2 file system&lt;/li&gt; &#xA; &lt;li&gt;Network stack and applications with support for IPv4, TCP, UDP; DNS, HTTP, Gemini, IMAP, NTP&lt;/li&gt; &#xA; &lt;li&gt;Profiling, debugging and other development tools (Kernel-supported profiling, detailed program analysis with software emulation in UserspaceEmulator, CrashReporter, interactive GUI playground, HexEditor, HackStudio IDE for C++ and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/&#34;&gt;Libraries&lt;/a&gt; for everything from cryptography to OpenGL, audio, JavaScript, GUI, playing chess, ...&lt;/li&gt; &#xA; &lt;li&gt;Support for many common and uncommon file formats (PNG, JPEG, GIF, MP3, WAV, FLAC, ZIP, TAR, PDF, QOI, Gemini, ...)&lt;/li&gt; &#xA; &lt;li&gt;Unified style and design philosophy, flexible theming system, &lt;a href=&#34;https://fonts.serenityos.net/font-family&#34;&gt;custom (bitmap and vector) fonts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Games/&#34;&gt;Games&lt;/a&gt; (Solitaire, Minesweeper, 2048, chess, Conway&#39;s Game of Life, ...) and &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Demos/&#34;&gt;demos&lt;/a&gt; (CatDog, Starfield, Eyes, mandelbrot set, WidgetGallery, ...)&lt;/li&gt; &#xA; &lt;li&gt;Every-day GUI programs and utilities (Spreadsheet with JavaScript, TextEditor, Terminal, PixelPaint, various multimedia viewers and players, Mail, Assistant, Calculator, ...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... and all of the above are right in this repository, no extra dependencies, built from-scratch by us :^)&lt;/p&gt; &#xA;&lt;p&gt;Additionally, there are &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Ports/AvailablePorts.md&#34;&gt;over two hundred ports of popular open-source software&lt;/a&gt;, including games, compilers, Unix tools, multimedia apps and more.&lt;/p&gt; &#xA;&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; &#xA;&lt;p&gt;Man pages are available online at &lt;a href=&#34;https://man.serenityos.org&#34;&gt;man.serenityos.org&lt;/a&gt;. These pages are generated from the Markdown source files in &lt;a href=&#34;https://github.com/SerenityOS/serenity/tree/master/Base/usr/share/man&#34;&gt;&lt;code&gt;Base/usr/share/man&lt;/code&gt;&lt;/a&gt; and updated automatically.&lt;/p&gt; &#xA;&lt;p&gt;When running SerenityOS you can use &lt;code&gt;man&lt;/code&gt; for the terminal interface, or &lt;code&gt;help&lt;/code&gt; for the GUI.&lt;/p&gt; &#xA;&lt;p&gt;Code-related documentation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Documentation/&#34;&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;How do I build and run this?&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/BuildInstructions.md&#34;&gt;SerenityOS build instructions&lt;/a&gt;. Serenity runs on Linux, macOS (aarch64 might be a challenge), Windows (with WSL2) and many other *Nixes with hardware or software virtualization.&lt;/p&gt; &#xA;&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; &#xA;&lt;p&gt;Join our Discord server: &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;SerenityOS Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before opening an issue, please see the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/CONTRIBUTING.md#issue-policy&#34;&gt;issue policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A general guide for contributing can be found in &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andreas Kling&lt;/strong&gt; - &lt;a href=&#34;https://twitter.com/awesomekling&#34;&gt;awesomekling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robin Burchell&lt;/strong&gt; - &lt;a href=&#34;https://github.com/rburchell&#34;&gt;rburchell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conrad Pankoff&lt;/strong&gt; - &lt;a href=&#34;https://github.com/deoxxa&#34;&gt;deoxxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sergey Bugaev&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bugaevc&#34;&gt;bugaevc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Liav A&lt;/strong&gt; - &lt;a href=&#34;https://github.com/supercomputer7&#34;&gt;supercomputer7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linus Groh&lt;/strong&gt; - &lt;a href=&#34;https://github.com/linusg&#34;&gt;linusg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ali Mohammad Pur&lt;/strong&gt; - &lt;a href=&#34;https://github.com/alimpfard&#34;&gt;alimpfard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shannon Booth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/shannonbooth&#34;&gt;shannonbooth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hüseyin ASLITÜRK&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asliturk&#34;&gt;asliturk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Matthew Olsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mattco98&#34;&gt;mattco98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nico Weber&lt;/strong&gt; - &lt;a href=&#34;https://github.com/nico&#34;&gt;nico&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brian Gianforcaro&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bgianfo&#34;&gt;bgianfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ben Wiederhake&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BenWiederhake&#34;&gt;BenWiederhake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tom&lt;/strong&gt; - &lt;a href=&#34;https://github.com/tomuta&#34;&gt;tomuta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Paul Scharnofske&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asynts&#34;&gt;asynts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Itamar Shenhar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/itamar8910&#34;&gt;itamar8910&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Luke Wilde&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Lubrsi&#34;&gt;Lubrsi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brendan Coles&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bcoles&#34;&gt;bcoles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andrew Kaster&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ADKaster&#34;&gt;ADKaster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;thankyouverycool&lt;/strong&gt; - &lt;a href=&#34;https://github.com/thankyouverycool&#34;&gt;thankyouverycool&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Idan Horowitz&lt;/strong&gt; - &lt;a href=&#34;https://github.com/IdanHo&#34;&gt;IdanHo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gunnar Beutner&lt;/strong&gt; - &lt;a href=&#34;https://github.com/gunnarbeutner&#34;&gt;gunnarbeutner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Flynn&lt;/strong&gt; - &lt;a href=&#34;https://github.com/trflynn89&#34;&gt;trflynn89&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jean-Baptiste Boric&lt;/strong&gt; - &lt;a href=&#34;https://github.com/boricj&#34;&gt;boricj&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stephan Unverwerth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sunverwerth&#34;&gt;sunverwerth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Wipfli&lt;/strong&gt; - &lt;a href=&#34;https://github.com/MaxWipfli&#34;&gt;MaxWipfli&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Daniel Bertalan&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BertalanD&#34;&gt;BertalanD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jelle Raaijmakers&lt;/strong&gt; - &lt;a href=&#34;https://github.com/GMTA&#34;&gt;GMTA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sam Atkins&lt;/strong&gt; - &lt;a href=&#34;https://github.com/AtkinsSJ&#34;&gt;AtkinsSJ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tobias Christiansen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/TobyAsE&#34;&gt;TobyAsE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lenny Maiorani&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ldm5180&#34;&gt;ldm5180&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sin-ack&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sin-ack&#34;&gt;sin-ack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jesse Buhagiar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Quaker762&#34;&gt;Quaker762&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Peter Elliott&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Petelliott&#34;&gt;Petelliott&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Karol Kosek&lt;/strong&gt; - &lt;a href=&#34;https://github.com/krkk&#34;&gt;krkk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mustafa Quraish&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mustafaquraish&#34;&gt;mustafaquraish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;David Tuin&lt;/strong&gt; - &lt;a href=&#34;https://github.com/davidot&#34;&gt;davidot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leon Albrecht&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Hendiadyoin1&#34;&gt;Hendiadyoin1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Schumacher&lt;/strong&gt; - &lt;a href=&#34;https://github.com/timschumi&#34;&gt;timschumi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Marcus Nilsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/metmo&#34;&gt;metmo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gegga Thor&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Xexxa&#34;&gt;Xexxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;kleines Filmröllchen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kleinesfilmroellchen&#34;&gt;kleinesfilmroellchen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kenneth Myhra&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kennethmyhra&#34;&gt;kennethmyhra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maciej&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sppmacd&#34;&gt;sppmacd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sahan Fernando&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ccapitalK&#34;&gt;ccapitalK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And many more! &lt;a href=&#34;https://github.com/SerenityOS/serenity/graphs/contributors&#34;&gt;See here&lt;/a&gt; for a full contributor list. The people listed above have landed more than 100 commits in the project. :^)&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is licensed under a 2-clause BSD license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>electron/electron</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/electron/electron</id>
    <link href="https://github.com/electron/electron" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://electronjs.org&#34;&gt;&lt;img src=&#34;https://electronjs.org/images/electron-logo.svg?sanitize=true&#34; alt=&#34;Electron Logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/electron/electron/tree/main&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/electron/electron/tree/main.svg?style=shield&#34; alt=&#34;CircleCI Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/electron-bot/electron-ljo26/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/4lggi9dpjc1qob7k/branch/main?svg=true&#34; alt=&#34;AppVeyor Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/APGC3k5yaH&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/745037351163527189?color=%237289DA&amp;amp;label=chat&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Electron Discord Invite&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;📝&lt;/span&gt; Available Translations: 🇨🇳 🇧🇷 🇪🇸 🇯🇵 🇷🇺 🇫🇷 🇺🇸 🇩🇪. View these docs in other languages at &lt;a href=&#34;https://github.com/electron/i18n/tree/master/content/&#34;&gt;electron/i18n&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Electron framework lets you write cross-platform desktop applications using JavaScript, HTML and CSS. It is based on &lt;a href=&#34;https://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; and &lt;a href=&#34;https://www.chromium.org&#34;&gt;Chromium&lt;/a&gt; and is used by the &lt;a href=&#34;https://github.com/atom/atom&#34;&gt;Atom editor&lt;/a&gt; and many other &lt;a href=&#34;https://electronjs.org/apps&#34;&gt;apps&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/electronjs&#34;&gt;@ElectronJS&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; &#xA;&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&#34;https://github.com/electron/electron/tree/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&#34;mailto:coc@electronjs.org&#34;&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href=&#34;https://docs.npmjs.com/&#34;&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;. The preferred method is to install Electron as a development dependency in your app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install electron --save-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more installation options and troubleshooting tips, see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/installation.md&#34;&gt;installation&lt;/a&gt;. For info on how to manage Electron versions in your apps, see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/electron-versioning.md&#34;&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Platform support&lt;/h2&gt; &#xA;&lt;p&gt;Each Electron release provides binaries for macOS, Windows, and Linux.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS (El Capitan and up): Electron provides 64-bit Intel and ARM binaries for macOS. Apple Silicon support was added in Electron 11.&lt;/li&gt; &#xA; &lt;li&gt;Windows (Windows 7 and up): Electron provides &lt;code&gt;ia32&lt;/code&gt; (&lt;code&gt;x86&lt;/code&gt;), &lt;code&gt;x64&lt;/code&gt; (&lt;code&gt;amd64&lt;/code&gt;), and &lt;code&gt;arm64&lt;/code&gt; binaries for Windows. Windows on ARM support was added in Electron 5.0.8.&lt;/li&gt; &#xA; &lt;li&gt;Linux: The prebuilt binaries of Electron are built on Ubuntu 20.04. They have also been verified to work on: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ubuntu 14.04 and newer&lt;/li&gt; &#xA;   &lt;li&gt;Fedora 24 and newer&lt;/li&gt; &#xA;   &lt;li&gt;Debian 8 and newer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start &amp;amp; Electron Fiddle&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://github.com/electron/fiddle&#34;&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt; to build, run, and package small Electron experiments, to see code examples for all of Electron&#39;s APIs, and to try out different versions of Electron. It&#39;s designed to make the start of your journey with Electron easier.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, clone and run the &lt;a href=&#34;https://github.com/electron/electron-quick-start&#34;&gt;electron/electron-quick-start&lt;/a&gt; repository to see a minimal Electron app in action:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/electron/electron-quick-start&#xA;cd electron-quick-start&#xA;npm install&#xA;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources for learning Electron&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://electronjs.org/docs&#34;&gt;electronjs.org/docs&lt;/a&gt; - All of Electron&#39;s documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/fiddle&#34;&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/electron-quick-start&#34;&gt;electron/electron-quick-start&lt;/a&gt; - A very basic starter Electron app&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://electronjs.org/community#boilerplates&#34;&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Programmatic usage&lt;/h2&gt; &#xA;&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the binary. Use this to spawn Electron from Node scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const electron = require(&#39;electron&#39;)&#xA;const proc = require(&#39;child_process&#39;)&#xA;&#xA;// will print something similar to /Users/maf/.../Electron&#xA;console.log(electron)&#xA;&#xA;// spawn Electron&#xA;const child = proc.spawn(electron)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mirrors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://npmmirror.com/mirrors/electron/&#34;&gt;China&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.electronjs.org/docs/latest/tutorial/installation#mirror&#34;&gt;Advanced Installation Instructions&lt;/a&gt; to learn how to use a custom mirror.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation translations&lt;/h2&gt; &#xA;&lt;p&gt;We crowdsource translations for our documentation via &lt;a href=&#34;https://crowdin.com/project/electron&#34;&gt;Crowdin&lt;/a&gt;. We currently accept translations for Chinese (Simplified), French, German, Japanese, Portuguese, Russian, and Spanish.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we&#39;re looking for and how to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps, and more can be found on the &lt;a href=&#34;https://www.electronjs.org/community&#34;&gt;Community page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/electron/electron/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;When using Electron logos, make sure to follow &lt;a href=&#34;https://openjsf.org/wp-content/uploads/sites/84/2021/01/OpenJS-Foundation-Trademark-Policy-2021-01-12.docx.pdf&#34;&gt;OpenJS Foundation Trademark Policy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Project-OSRM/osrm-backend</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/Project-OSRM/osrm-backend</id>
    <link href="https://github.com/Project-OSRM/osrm-backend" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source Routing Machine - C++ backend&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Open Source Routing Machine&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Linux / macOS&lt;/th&gt; &#xA;   &lt;th&gt;Windows&lt;/th&gt; &#xA;   &lt;th&gt;Code Coverage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend/actions/workflows/osrm-backend.yml&#34;&gt;&lt;img src=&#34;https://github.com/Project-OSRM/osrm-backend/actions/workflows/osrm-backend.yml/badge.svg?sanitize=true&#34; alt=&#34;osrm-backend CI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ci.appveyor.com/project/DennisOSRM/osrm-backend&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/4iuo3s9gxprmcjjh&#34; alt=&#34;AppVeyor&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://codecov.io/gh/Project-OSRM/osrm-backend&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/Project-OSRM/osrm-backend/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;High performance routing engine written in C++14 designed to run on OpenStreetMap data.&lt;/p&gt; &#xA;&lt;p&gt;The following services are available via HTTP API, C++ library interface and NodeJs wrapper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nearest - Snaps coordinates to the street network and returns the nearest matches&lt;/li&gt; &#xA; &lt;li&gt;Route - Finds the fastest route between coordinates&lt;/li&gt; &#xA; &lt;li&gt;Table - Computes the duration or distances of the fastest route between all pairs of supplied coordinates&lt;/li&gt; &#xA; &lt;li&gt;Match - Snaps noisy GPS traces to the road network in the most plausible way&lt;/li&gt; &#xA; &lt;li&gt;Trip - Solves the Traveling Salesman Problem using a greedy heuristic&lt;/li&gt; &#xA; &lt;li&gt;Tile - Generates Mapbox Vector Tiles with internal routing metadata&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To quickly try OSRM use our &lt;a href=&#34;http://map.project-osrm.org&#34;&gt;demo server&lt;/a&gt; which comes with both the backend and a frontend on top.&lt;/p&gt; &#xA;&lt;p&gt;For a quick introduction about how the road network is represented in OpenStreetMap and how to map specific road network features have a look at &lt;a href=&#34;https://www.mapbox.com/mapping/mapping-for-navigation/&#34;&gt;this guide about mapping for navigation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Related &lt;a href=&#34;https://github.com/Project-OSRM&#34;&gt;Project-OSRM&lt;/a&gt; repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Project-OSRM/osrm-frontend&#34;&gt;osrm-frontend&lt;/a&gt; - User-facing frontend with map. The demo server runs this on top of the backend&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Project-OSRM/osrm-text-instructions&#34;&gt;osrm-text-instructions&lt;/a&gt; - Text instructions from OSRM route response&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/osrm/osrm-backend/&#34;&gt;osrm-backend-docker&lt;/a&gt; - Ready to use Docker images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Full documentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://project-osrm.org&#34;&gt;Hosted documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/http.md&#34;&gt;osrm-routed HTTP API documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/libosrm.md&#34;&gt;libosrm API documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;IRC: &lt;code&gt;irc.oftc.net&lt;/code&gt;, channel: &lt;code&gt;#osrm&lt;/code&gt; (&lt;a href=&#34;https://webchat.oftc.net&#34;&gt;Webchat&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mailinglist: &lt;code&gt;https://lists.openstreetmap.org/listinfo/osrm-talk&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The easiest and quickest way to setup your own routing engine is to use Docker images we provide.&lt;/p&gt; &#xA;&lt;p&gt;There are two pre-processing pipelines available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Contraction Hierarchies (CH)&lt;/li&gt; &#xA; &lt;li&gt;Multi-Level Dijkstra (MLD)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;we recommend using MLD by default except for special use-cases such as very large distance matrices where CH is still a better fit for the time being. In the following we explain the MLD pipeline. If you want to use the CH pipeline instead replace &lt;code&gt;osrm-partition&lt;/code&gt; and &lt;code&gt;osrm-customize&lt;/code&gt; with a single &lt;code&gt;osrm-contract&lt;/code&gt; and change the algorithm option for &lt;code&gt;osrm-routed&lt;/code&gt; to &lt;code&gt;--algorithm ch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;p&gt;We base our Docker images (&lt;a href=&#34;https://hub.docker.com/r/osrm/osrm-backend/&#34;&gt;backend&lt;/a&gt;, &lt;a href=&#34;https://hub.docker.com/r/osrm/osrm-frontend/&#34;&gt;frontend&lt;/a&gt;) on Debian and make sure they are as lightweight as possible.&lt;/p&gt; &#xA;&lt;p&gt;Download OpenStreetMap extracts for example from &lt;a href=&#34;http://download.geofabrik.de/&#34;&gt;Geofabrik&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget http://download.geofabrik.de/europe/germany/berlin-latest.osm.pbf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pre-process the extract with the car profile and start a routing engine HTTP server on port 5000&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -t -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-extract -p /opt/car.lua /data/berlin-latest.osm.pbf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The flag &lt;code&gt;-v &#34;${PWD}:/data&#34;&lt;/code&gt; creates the directory &lt;code&gt;/data&lt;/code&gt; inside the docker container and makes the current working directory &lt;code&gt;&#34;${PWD}&#34;&lt;/code&gt; available there. The file &lt;code&gt;/data/berlin-latest.osm.pbf&lt;/code&gt; inside the container is referring to &lt;code&gt;&#34;${PWD}/berlin-latest.osm.pbf&#34;&lt;/code&gt; on the host.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -t -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-partition /data/berlin-latest.osrm&#xA;docker run -t -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-customize /data/berlin-latest.osrm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;berlin-latest.osrm&lt;/code&gt; has a different file extension.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -t -i -p 5000:5000 -v &#34;${PWD}:/data&#34; osrm/osrm-backend osrm-routed --algorithm mld /data/berlin-latest.osrm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make requests against the HTTP server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl &#34;http://127.0.0.1:5000/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally start a user-friendly frontend on port 9966, and open it up in your browser&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 9966:9966 osrm/osrm-frontend&#xA;xdg-open &#39;http://127.0.0.1:9966&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In case Docker complains about not being able to connect to the Docker daemon make sure you are in the &lt;code&gt;docker&lt;/code&gt; group.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo usermod -aG docker $USER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After adding yourself to the &lt;code&gt;docker&lt;/code&gt; group make sure to log out and back in again with your terminal.&lt;/p&gt; &#xA;&lt;p&gt;We support the following images on Docker Cloud:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;master&lt;/code&gt; compiled with release flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest-assertions&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;master&lt;/code&gt; compiled with with release flag, assertions enabled and debug symbols&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;latest-debug&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;master&lt;/code&gt; compiled with debug flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;tag&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;specific tag compiled with release flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;tag&amp;gt;-debug&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;specific tag compiled with debug flag&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Building from Source&lt;/h3&gt; &#xA;&lt;p&gt;The following targets Ubuntu 16.04. For instructions how to build on different distributions, macOS or Windows see our &lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend/wiki&#34;&gt;Wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install build-essential git cmake pkg-config \&#xA;libbz2-dev libxml2-dev libzip-dev libboost-all-dev \&#xA;lua5.2 liblua5.2-dev libtbb-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Compile and install OSRM binaries&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build&#xA;cd build&#xA;cmake ..&#xA;cmake --build .&#xA;sudo cmake --build . --target install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Request Against the Demo Server&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/Project-OSRM/osrm-backend/wiki/Demo-server&#34;&gt;API usage policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Simple query with instructions and alternatives on Berlin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl &#34;https://router.project-osrm.org/route/v1/driving/13.388860,52.517037;13.385983,52.496891?steps=true&amp;amp;alternatives=true&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the Node.js Bindings&lt;/h3&gt; &#xA;&lt;p&gt;The Node.js bindings provide read-only access to the routing engine. We provide API documentation and examples &lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/nodejs/api.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will need a modern &lt;code&gt;libstdc++&lt;/code&gt; toolchain (&lt;code&gt;&amp;gt;= GLIBCXX_3.4.20&lt;/code&gt;) for binary compatibility if you want to use the pre-built binaries. For older Ubuntu systems you can upgrade your standard library for example with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:ubuntu-toolchain-r/test&#xA;sudo apt-get update -y&#xA;sudo apt-get install -y libstdc++-5-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can install the Node.js bindings via &lt;code&gt;npm install osrm&lt;/code&gt; or from this repository either via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will check and use pre-built binaries if they&#39;re available for this release and your Node version, or via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install --build-from-source&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to always force building the Node.js bindings from source.&lt;/p&gt; &#xA;&lt;p&gt;For usage details have a look &lt;a href=&#34;https://raw.githubusercontent.com/Project-OSRM/osrm-backend/master/docs/nodejs/api.md&#34;&gt;these API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;An exemplary implementation by a 3rd party with Docker and Node.js can be found &lt;a href=&#34;https://github.com/door2door-io/osrm-express-server-demo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;References in publications&lt;/h2&gt; &#xA;&lt;p&gt;When using the code in a (scientific) publication, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{luxen-vetter-2011,&#xA; author = {Luxen, Dennis and Vetter, Christian},&#xA; title = {Real-time routing with OpenStreetMap data},&#xA; booktitle = {Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},&#xA; series = {GIS &#39;11},&#xA; year = {2011},&#xA; isbn = {978-1-4503-1031-4},&#xA; location = {Chicago, Illinois},&#xA; pages = {513--516},&#xA; numpages = {4},&#xA; url = {http://doi.acm.org/10.1145/2093973.2094062},&#xA; doi = {10.1145/2093973.2094062},&#xA; acmid = {2094062},&#xA; publisher = {ACM},&#xA; address = {New York, NY, USA},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/serving</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/tensorflow/serving</id>
    <link href="https://github.com/tensorflow/serving" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A flexible, high-performance serving system for machine learning models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Serving&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu.svg?sanitize=true&#34; alt=&#34;Ubuntu Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu-tf-head.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu-tf-head.svg?sanitize=true&#34; alt=&#34;Ubuntu Build Status at TF HEAD&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/docker-cpu-nightly.svg?sanitize=true&#34; alt=&#34;Docker CPU Nightly Build Status&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/docker-gpu-nightly.svg?sanitize=true&#34; alt=&#34;Docker GPU Nightly Build Status&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It deals with the &lt;em&gt;inference&lt;/em&gt; aspect of machine learning, taking models after &lt;em&gt;training&lt;/em&gt; and managing their lifetimes, providing clients with versioned access via a high-performance, reference-counted lookup table. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.&lt;/p&gt; &#xA;&lt;p&gt;To note a few features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can serve multiple models, or multiple versions of the same model simultaneously&lt;/li&gt; &#xA; &lt;li&gt;Exposes both gRPC as well as HTTP inference endpoints&lt;/li&gt; &#xA; &lt;li&gt;Allows deployment of new model versions without changing any client code&lt;/li&gt; &#xA; &lt;li&gt;Supports canarying new versions and A/B testing experimental models&lt;/li&gt; &#xA; &lt;li&gt;Adds minimal latency to inference time due to efficient, low-overhead implementation&lt;/li&gt; &#xA; &lt;li&gt;Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls&lt;/li&gt; &#xA; &lt;li&gt;Supports many &lt;em&gt;servables&lt;/em&gt;: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Serve a Tensorflow model in 60 seconds&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download the TensorFlow Serving Docker image and repo&#xA;docker pull tensorflow/serving&#xA;&#xA;git clone https://github.com/tensorflow/serving&#xA;# Location of demo models&#xA;TESTDATA=&#34;$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata&#34;&#xA;&#xA;# Start TensorFlow Serving container and open the REST API port&#xA;docker run -t --rm -p 8501:8501 \&#xA;    -v &#34;$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two&#34; \&#xA;    -e MODEL_NAME=half_plus_two \&#xA;    tensorflow/serving &amp;amp;&#xA;&#xA;# Query the model using the predict API&#xA;curl -d &#39;{&#34;instances&#34;: [1.0, 2.0, 5.0]}&#39; \&#xA;    -X POST http://localhost:8501/v1/models/half_plus_two:predict&#xA;&#xA;# Returns =&amp;gt; { &#34;predictions&#34;: [2.5, 3.0, 4.5] }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;End-to-End Training &amp;amp; Serving Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the official Tensorflow documentations site for &lt;a href=&#34;https://www.tensorflow.org/tfx/tutorials/serving/rest_simple&#34;&gt;a complete tutorial to train and serve a Tensorflow Model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Set up&lt;/h3&gt; &#xA;&lt;p&gt;The easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/docker.md&#34;&gt;Install Tensorflow Serving using Docker&lt;/a&gt; &lt;em&gt;(Recommended)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/setup.md&#34;&gt;Install Tensorflow Serving without Docker&lt;/a&gt; &lt;em&gt;(Not Recommended)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/building_with_docker.md&#34;&gt;Build Tensorflow Serving from Source with Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/serving_kubernetes.md&#34;&gt;Deploy Tensorflow Serving on Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Use&lt;/h3&gt; &#xA;&lt;h4&gt;Export your Tensorflow model&lt;/h4&gt; &#xA;&lt;p&gt;In order to serve a Tensorflow model, simply export a SavedModel from your Tensorflow program. &lt;a href=&#34;https://github.com/tensorflow/tensorflow/raw/master/tensorflow/python/saved_model/README.md&#34;&gt;SavedModel&lt;/a&gt; is a language-neutral, recoverable, hermetic serialization format that enables higher-level systems and tools to produce, consume, and transform TensorFlow models.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://www.tensorflow.org/guide/saved_model#save_and_restore_models&#34;&gt;Tensorflow documentation&lt;/a&gt; for detailed instructions on how to export SavedModels.&lt;/p&gt; &#xA;&lt;h4&gt;Configure and Use Tensorflow Serving&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/serving_basic.md&#34;&gt;Follow a tutorial on Serving Tensorflow models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/serving_config.md&#34;&gt;Configure Tensorflow Serving to make it fit your serving use case&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/performance.md&#34;&gt;Performance Guide&lt;/a&gt; and learn how to &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/tensorboard.md&#34;&gt;use TensorBoard to profile and optimize inference requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/api_rest.md&#34;&gt;REST API Guide&lt;/a&gt; or &lt;a href=&#34;https://github.com/tensorflow/serving/tree/master/tensorflow_serving/apis&#34;&gt;gRPC API definition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/saved_model_warmup.md&#34;&gt;Use SavedModel Warmup if initial inference requests are slow due to lazy initialization of graph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/signature_defs.md&#34;&gt;If encountering issues regarding model signatures, please read the SignatureDef documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If using a model with custom ops, &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/custom_op.md&#34;&gt;learn how to serve models with custom ops&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extend&lt;/h3&gt; &#xA;&lt;p&gt;Tensorflow Serving&#39;s architecture is highly modular. You can use some parts individually (e.g. batch scheduling) and/or extend it to serve new use cases.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/building_with_docker.md&#34;&gt;Ensure you are familiar with building Tensorflow Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/architecture.md&#34;&gt;Learn about Tensorflow Serving&#39;s architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tfx/serving/api_docs/cc/&#34;&gt;Explore the Tensorflow Serving C++ API reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/custom_servable.md&#34;&gt;Create a new type of Servable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/custom_source.md&#34;&gt;Create a custom Source of Servable versions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you&#39;d like to contribute to TensorFlow Serving, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;For more information&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the official &lt;a href=&#34;http://tensorflow.org&#34;&gt;TensorFlow website&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>protocolbuffers/protobuf</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/protocolbuffers/protobuf</id>
    <link href="https://github.com/protocolbuffers/protobuf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2008 Google Inc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf&#39;s documentation on the Google Developers site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Compiler Installation&lt;/h2&gt; &#xA;&lt;p&gt;The protocol compiler is written in C++. If you are using C++, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; &#xA;&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf/releases&#34;&gt;https://github.com/protocolbuffers/protobuf/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the maven repo here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&#34;&gt;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; &#xA;&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&#34;&gt;src&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&#34;&gt;java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&#34;&gt;objectivec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C#&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&#34;&gt;csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&#34;&gt;ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf-go&#34;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&#34;&gt;php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dart-lang/protobuf&#34;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The best way to learn how to use protobuf is to follow the tutorials in our developer guide:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;https://developers.google.com/protocol-buffers/docs/tutorials&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The complete documentation for Protocol Buffers is available via the web at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openvinotoolkit/openvino</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/openvinotoolkit/openvino</id>
    <link href="https://github.com/openvinotoolkit/openvino" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenVINO™ Toolkit repository&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/docs/img/openvino-logo-purple-black.png&#34; width=&#34;400px&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/releases/tag/2022.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-2022.1-green.svg?sanitize=true&#34; alt=&#34;Stable release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Apache License Version 2.0&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/checks-status/openvinotoolkit/openvino/master?label=GitHub%20checks&#34; alt=&#34;GitHub branch checks state&#34;&gt; &lt;img src=&#34;https://img.shields.io/azure-devops/build/openvinoci/b2bab62f-ab2f-4871-a538-86ea1be7d20f/13?label=Public%20CI&#34; alt=&#34;Azure DevOps builds (branch)&#34;&gt; &lt;a href=&#34;https://badge.fury.io/py/openvino&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/openvino.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/openvino&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/openvino&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#what-is-openvino-toolkit&#34;&gt;What is OpenVINO?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#supported-hardware-matrix&#34;&gt;Supported Hardware matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#products-which-use-openvino&#34;&gt;Products which use OpenVINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#system-requirements&#34;&gt;System requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#how-to-build&#34;&gt;How to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#how-to-contribute&#34;&gt;How to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#get-a-support&#34;&gt;Get a support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#see-also&#34;&gt;See also&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is OpenVINO toolkit?&lt;/h2&gt; &#xA;&lt;p&gt;OpenVINO™ is an open-source toolkit for optimizing and deploying AI inference.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Boost deep learning performance in computer vision, automatic speech recognition, natural language processing and other common tasks&lt;/li&gt; &#xA; &lt;li&gt;Use models trained with popular frameworks like TensorFlow, PyTorch and more&lt;/li&gt; &#xA; &lt;li&gt;Reduce resource demands and efficiently deploy on a range of Intel® platforms from edge to cloud&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This open-source version includes several components: namely &lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html&#34;&gt;Model Optimizer&lt;/a&gt;, &lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html&#34;&gt;OpenVINO™ Runtime&lt;/a&gt;, &lt;a href=&#34;https://docs.openvino.ai/latest/pot_introduction.html&#34;&gt;Post-Training Optimization Tool&lt;/a&gt;, as well as CPU, GPU, MYRIAD, multi device and heterogeneous plugins to accelerate deep learning inferencing on Intel® CPUs and Intel® Processor Graphics. It supports pre-trained models from the &lt;a href=&#34;https://github.com/openvinotoolkit/open_model_zoo&#34;&gt;Open Model Zoo&lt;/a&gt;, along with 100+ open source and public models in popular formats such as TensorFlow, ONNX, PaddlePaddle, MXNet, Caffe, Kaldi.&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html&#34;&gt;OpenVINO™ Runtime&lt;/a&gt; - is a set of C++ libraries with C and Python bindings providing a common API to deliver inference solutions on the platform of your choice. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/core&#34;&gt;core&lt;/a&gt; - provides the base API for model representation and modification.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/inference&#34;&gt;inference&lt;/a&gt; - provides an API to infer models on device.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/common/transformations&#34;&gt;transformations&lt;/a&gt; - contains the set of common transformations which are used in OpenVINO plugins.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/common/low_precision_transformations&#34;&gt;low precision transformations&lt;/a&gt; - contains the set of transformations which are used in low precision models&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings&#34;&gt;bindings&lt;/a&gt; - contains all awailable OpenVINO bindings which are maintained by OpenVINO team. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings/c&#34;&gt;c&lt;/a&gt; - provides C API for OpenVINO™ Runtime&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings/python&#34;&gt;python&lt;/a&gt; - Python API for OpenVINO™ Runtime&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins&#34;&gt;Plugins&lt;/a&gt; - contains OpenVINO plugins which are maintained in open-source by OpenVINO team. For more information please taje a look to the &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#supported-hardware-matrix&#34;&gt;list of supported devices&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/frontends&#34;&gt;Frontends&lt;/a&gt; - contains available OpenVINO frontends which allow to read model from native framework format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html&#34;&gt;Model Optimizer&lt;/a&gt; - is a cross-platform command-line tool that facilitates the transition between training and deployment environments, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/pot_introduction.html&#34;&gt;Post-Training Optimization Tool&lt;/a&gt; - is designed to accelerate the inference of deep learning models by applying special methods without model retraining or fine-tuning, for example, post-training 8-bit quantization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/samples&#34;&gt;Samples&lt;/a&gt; - applications on C, C++ and Python languages which shows basic use cases of OpenVINO usages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Hardware matrix&lt;/h2&gt; &#xA;&lt;p&gt;The OpenVINO™ Runtime can infer models on different hardware devices. This section provides the list of supported devices.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;ShortDescription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_CPU.html#doxid-openvino-docs-o-v-u-g-supported-plugins-c-p-u&#34;&gt;Intel CPU&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_cpu&#34;&gt;openvino_intel_cpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Xeon with Intel® Advanced Vector Extensions 2 (Intel® AVX2), Intel® Advanced Vector Extensions 512 (Intel® AVX-512), and AVX512_BF16, Intel Core Processors with Intel AVX2, Intel Atom Processors with Intel® Streaming SIMD Extensions (Intel® SSE)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_ARM_CPU.html&#34;&gt;ARM CPU&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_contrib/tree/master/modules/arm_plugin&#34;&gt;openvino_arm_cpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Raspberry Pi™ 4 Model B, Apple® Mac mini with M1 chip, NVIDIA® Jetson Nano™, Android™ devices &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_GPU.html#doxid-openvino-docs-o-v-u-g-supported-plugins-g-p-u&#34;&gt;Intel GPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_gpu&#34;&gt;openvino_intel_gpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Processor Graphics, including Intel HD Graphics and Intel Iris Graphics&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_GNA.html#doxid-openvino-docs-o-v-u-g-supported-plugins-g-n-a&#34;&gt;Intel GNA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_gna&#34;&gt;openvino_intel_gna_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Speech Enabling Developer Kit, Amazon Alexa* Premium Far-Field Developer Kit, Intel Pentium Silver J5005 Processor, Intel Pentium Silver N5000 Processor, Intel Celeron J4005 Processor, Intel Celeron J4105 Processor, Intel Celeron Processor N4100, Intel Celeron Processor N4000, Intel Core i3-8121U Processor, Intel Core i7-1065G7 Processor, Intel Core i7-1060G7 Processor, Intel Core i5-1035G4 Processor, Intel Core i5-1035G7 Processor, Intel Core i5-1035G1 Processor, Intel Core i5-1030G7 Processor, Intel Core i5-1030G4 Processor, Intel Core i3-1005G1 Processor, Intel Core i3-1000G1 Processor, Intel Core i3-1000G4 Processor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_IE_DG_supported_plugins_VPU.html#doxid-openvino-docs-i-e-d-g-supported-plugins-v-p-u&#34;&gt;Myriad plugin&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_myriad&#34;&gt;openvino_intel_myriad_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel® Neural Compute Stick 2 powered by the Intel® Movidius™ Myriad™ X&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Also OpenVINO™ Toolkit contains several plugins which should simplify to load model on several hardware devices:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;ShortDescription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_IE_DG_supported_plugins_AUTO.html#doxid-openvino-docs-i-e-d-g-supported-plugins-a-u-t-o&#34;&gt;Auto&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto&#34;&gt;openvino_auto_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto plugin enables selecting Intel device for inference automatically&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Automatic_Batching.html&#34;&gt;Auto Batch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto_batch&#34;&gt;openvino_auto_batch_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto batch plugin performs on-the-fly automatic batching (i.e. grouping inference requests together) to improve device utilization, with no programming effort from the user&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Hetero_execution.html#doxid-openvino-docs-o-v-u-g-hetero-execution&#34;&gt;Hetero&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/hetero&#34;&gt;openvino_hetero_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Heterogeneous execution enables automatic inference splitting between several devices&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Running_on_multiple_devices.html#doxid-openvino-docs-o-v-u-g-running-on-multiple-devices&#34;&gt;Multi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto&#34;&gt;openvino_auto_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi plugin enables simultaneous inference of the same model on several devices in parallel&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenVINO™ Toolkit is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt;. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;User documentation&lt;/h3&gt; &#xA;&lt;p&gt;The latest documentation for OpenVINO™ Toolkit is availabe &lt;a href=&#34;https://docs.openvino.ai/&#34;&gt;here&lt;/a&gt;. This documentation contains detailed information about all OpenVINO components and provides all important information which could be needed if you create an application which is based on binary OpenVINO distribution or own OpenVINO version without source code modification.&lt;/p&gt; &#xA;&lt;h3&gt;Developer documentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#todo-add&#34;&gt;Developer documentation&lt;/a&gt; contains information about architectural decisions which are applied inside the OpenVINO components. This documentation has all necessary information which could be needed in order to contribute to OpenVINO.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;The list of OpenVINO tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_notebooks&#34;&gt;Jupiter notebooks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Products which use OpenVINO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opencv.org/&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onnxruntime.ai/&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/build/ovtfoverview.html&#34;&gt;OpenVINO™ Integration with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/TNN/tree/master&#34;&gt;TNN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;The full information about system requirements depends on platform and available in section &lt;code&gt;System requirement&lt;/code&gt; on dedicated pages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_linux.html&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_windows.html&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_macos.html&#34;&gt;macOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_raspbian.html&#34;&gt;Raspbian&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;p&gt;Please take a look to &lt;a href=&#34;https://github.com/openvinotoolkit/openvino/wiki#how-to-build&#34;&gt;OpenVINO Wiki&lt;/a&gt; to get more information about OpenVINO build process.&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for details. Thank you!&lt;/p&gt; &#xA;&lt;h2&gt;Get a support&lt;/h2&gt; &#xA;&lt;p&gt;Please report questions, issues and suggestions using:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/issues&#34;&gt;GitHub* Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://stackoverflow.com/questions/tagged/openvino&#34;&gt;&lt;code&gt;openvino&lt;/code&gt;&lt;/a&gt; tag on StackOverflow*&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/forums/computer-vision&#34;&gt;Forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;See also&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/wiki&#34;&gt;OpenVINO Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storage.openvinotoolkit.org/&#34;&gt;OpenVINO Storage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Additional OpenVINO™ toolkit modules: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_contrib&#34;&gt;openvino_contrib&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html&#34;&gt;Intel® Distribution of OpenVINO™ toolkit Product Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/articles/OpenVINO-RelNotes&#34;&gt;Intel® Distribution of OpenVINO™ toolkit Release Notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/nncf&#34;&gt;Neural Network Compression Framework (NNCF)&lt;/a&gt; - a suite of advanced algorithms for model inference optimization including quantization, filter pruning, binarization and sparsity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/training_extensions&#34;&gt;OpenVINO™ Training Extensions (OTE)&lt;/a&gt; - convenient environment to train Deep Learning models and convert them using OpenVINO for optimized inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/model_server&#34;&gt;OpenVINO™ Model Server (OVMS)&lt;/a&gt; - a scalable, high-performance solution for serving deep learning models optimized for Intel architectures&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/workbench_docs_Workbench_DG_Introduction.html&#34;&gt;DL Workbench&lt;/a&gt; - An alternative, web-based version of OpenVINO designed to make production of pretrained deep learning models significantly easier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/cvat&#34;&gt;Computer Vision Annotation Tool (CVAT)&lt;/a&gt; - an online, interactive video and image annotation tool for computer vision purposes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/datumaro&#34;&gt;Dataset Management Framework (Datumaro)&lt;/a&gt; - a framework and CLI tool to build, transform, and analyze datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;* Other names and brands may be claimed as the property of others.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>keith2018/SoftGLRender</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/keith2018/SoftGLRender</id>
    <link href="https://github.com/keith2018/SoftGLRender" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tiny C++ Software Renderer/Rasterizer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SoftGLRender&lt;/h1&gt; &#xA;&lt;p&gt;Tiny C++ Software Renderer/Rasterizer, it implements the main GPU rendering pipeline, 3D models (GLTF) are loaded by &lt;a href=&#34;https://github.com/assimp/assimp&#34;&gt;assimp&lt;/a&gt;, and using &lt;a href=&#34;https://github.com/g-truc/glm&#34;&gt;GLM&lt;/a&gt; as math library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/helmet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wireframe&lt;/li&gt; &#xA; &lt;li&gt;View Frustum culling&lt;/li&gt; &#xA; &lt;li&gt;Back-Front culling&lt;/li&gt; &#xA; &lt;li&gt;Orbit Camera Controller&lt;/li&gt; &#xA; &lt;li&gt;Perspective Correct Interpolation&lt;/li&gt; &#xA; &lt;li&gt;Reversed Z&lt;/li&gt; &#xA; &lt;li&gt;Early Z&lt;/li&gt; &#xA; &lt;li&gt;Tangent Space Normal Mapping&lt;/li&gt; &#xA; &lt;li&gt;Basic Lighting&lt;/li&gt; &#xA; &lt;li&gt;Blinn-Phong shading&lt;/li&gt; &#xA; &lt;li&gt;PBR &amp;amp; IBL shading&lt;/li&gt; &#xA; &lt;li&gt;Skybox CubeMap &amp;amp; Equirectangular&lt;/li&gt; &#xA; &lt;li&gt;Texture mipmaps&lt;/li&gt; &#xA; &lt;li&gt;Texture tiling and swizzling (linear, tiled, morton)&lt;/li&gt; &#xA; &lt;li&gt;Texture filtering and wrapping&lt;/li&gt; &#xA; &lt;li&gt;Shader varying partial derivative &lt;code&gt;dFdx&lt;/code&gt; &lt;code&gt;dFdy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpha mask &amp;amp; blend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Texture Filtering&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NEAREST&lt;/li&gt; &#xA; &lt;li&gt;LINEAR&lt;/li&gt; &#xA; &lt;li&gt;NEAREST_MIPMAP_NEAREST&lt;/li&gt; &#xA; &lt;li&gt;LINEAR_MIPMAP_NEAREST&lt;/li&gt; &#xA; &lt;li&gt;NEAREST_MIPMAP_LINEAR&lt;/li&gt; &#xA; &lt;li&gt;LINEAR_MIPMAP_LINEAR&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Texture Wrapping&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;REPEAT&lt;/li&gt; &#xA; &lt;li&gt;MIRRORED_REPEAT&lt;/li&gt; &#xA; &lt;li&gt;CLAMP_TO_EDGE&lt;/li&gt; &#xA; &lt;li&gt;CLAMP_TO_BORDER&lt;/li&gt; &#xA; &lt;li&gt;CLAMP_TO_ZERO&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Texture Fetch&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lod&lt;/li&gt; &#xA; &lt;li&gt;Bias&lt;/li&gt; &#xA; &lt;li&gt;Offset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Anti Aliasing&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SSAA&lt;/li&gt; &#xA; &lt;li&gt;FXAA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; MSAA\TAA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Shadow Map&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Showcase&lt;/h2&gt; &#xA;&lt;h3&gt;Render Textured&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;BoomBox (PBR) &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/boombox.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Robot &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/robot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;DamagedHelmet (PBR) &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/helmet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GlassTable &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/glasstable.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;AfricanHead &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/africanhead.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Brickwall &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/brickwall.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Cube &lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/cube.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Render Wireframe&lt;/h3&gt; &#xA;&lt;p&gt;Check &#34;show clip&#34; to show the triangles created by frustum clip&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/screenshot/clip.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/g-truc/glm&#34;&gt;GLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dropbox/json11&#34;&gt;json11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nothings/stb&#34;&gt;stb_image&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/assimp/assimp&#34;&gt;assimp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ocornut/imgui&#34;&gt;imgui&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir build&#xA;cd build&#xA;cmake ..&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd bin&#xA;./SoftGLRender&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is licensed under the MIT License (see &lt;a href=&#34;https://raw.githubusercontent.com/keith2018/SoftGLRender/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/folly</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/facebook/folly</id>
    <link href="https://github.com/facebook/folly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source C++ library developed and used at Facebook.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Folly: Facebook Open-source Library&lt;/h1&gt; &#xA;&lt;a href=&#34;https://opensource.facebook.com/support-ukraine&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine - Help Provide Humanitarian Aid to Ukraine.&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;What is &lt;code&gt;folly&lt;/code&gt;?&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebook/folly/main/static/logo.svg?sanitize=true&#34; alt=&#34;Logo Folly&#34; width=&#34;15%&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;Folly (acronymed loosely after Facebook Open Source Library) is a library of C++14 components designed with practicality and efficiency in mind. &lt;strong&gt;Folly contains a variety of core library components used extensively at Facebook&lt;/strong&gt;. In particular, it&#39;s often a dependency of Facebook&#39;s other open source C++ efforts and place where those projects can share code.&lt;/p&gt; &#xA;&lt;p&gt;It complements (as opposed to competing against) offerings such as Boost and of course &lt;code&gt;std&lt;/code&gt;. In fact, we embark on defining our own component only when something we need is either not available, or does not meet the needed performance profile. We endeavor to remove things from folly if or when &lt;code&gt;std&lt;/code&gt; or Boost obsoletes them.&lt;/p&gt; &#xA;&lt;p&gt;Performance concerns permeate much of Folly, sometimes leading to designs that are more idiosyncratic than they would otherwise be (see e.g. &lt;code&gt;PackedSyncPtr.h&lt;/code&gt;, &lt;code&gt;SmallLocks.h&lt;/code&gt;). Good performance at large scale is a unifying theme in all of Folly.&lt;/p&gt; &#xA;&lt;h2&gt;Check it out in the intro video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Wr_IfOICYSs&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/Wr_IfOICYSs/0.jpg&#34; alt=&#34;Explain Like I’m 5: Folly&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Logical Design&lt;/h1&gt; &#xA;&lt;p&gt;Folly is a collection of relatively independent components, some as simple as a few symbols. There is no restriction on internal dependencies, meaning that a given folly module may use any other folly components.&lt;/p&gt; &#xA;&lt;p&gt;All symbols are defined in the top-level namespace &lt;code&gt;folly&lt;/code&gt;, except of course macros. Macro names are ALL_UPPERCASE and should be prefixed with &lt;code&gt;FOLLY_&lt;/code&gt;. Namespace &lt;code&gt;folly&lt;/code&gt; defines other internal namespaces such as &lt;code&gt;internal&lt;/code&gt; or &lt;code&gt;detail&lt;/code&gt;. User code should not depend on symbols in those namespaces.&lt;/p&gt; &#xA;&lt;p&gt;Folly has an &lt;code&gt;experimental&lt;/code&gt; directory as well. This designation connotes primarily that we feel the API may change heavily over time. This code, typically, is still in heavy use and is well tested.&lt;/p&gt; &#xA;&lt;h1&gt;Physical Design&lt;/h1&gt; &#xA;&lt;p&gt;At the top level Folly uses the classic &#34;stuttering&#34; scheme &lt;code&gt;folly/folly&lt;/code&gt; used by Boost and others. The first directory serves as an installation root of the library (with possible versioning a la &lt;code&gt;folly-1.0/&lt;/code&gt;), and the second is to distinguish the library when including files, e.g. &lt;code&gt;#include &amp;lt;folly/FBString.h&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The directory structure is flat (mimicking the namespace structure), i.e. we don&#39;t have an elaborate directory hierarchy (it is possible this will change in future versions). The subdirectory &lt;code&gt;experimental&lt;/code&gt; contains files that are used inside folly and possibly at Facebook but not considered stable enough for client use. Your code should not use files in &lt;code&gt;folly/experimental&lt;/code&gt; lest it may break when you update Folly.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;folly/folly/test&lt;/code&gt; subdirectory includes the unittests for all components, usually named &lt;code&gt;ComponentXyzTest.cpp&lt;/code&gt; for each &lt;code&gt;ComponentXyz.*&lt;/code&gt;. The &lt;code&gt;folly/folly/docs&lt;/code&gt; directory contains documentation.&lt;/p&gt; &#xA;&lt;h1&gt;What&#39;s in it?&lt;/h1&gt; &#xA;&lt;p&gt;Because of folly&#39;s fairly flat structure, the best way to see what&#39;s in it is to look at the headers in &lt;a href=&#34;https://github.com/facebook/folly/tree/main/folly&#34;&gt;top level &lt;code&gt;folly/&lt;/code&gt; directory&lt;/a&gt;. You can also check the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/folly/main/folly/docs&#34;&gt;&lt;code&gt;docs&lt;/code&gt; folder&lt;/a&gt; for documentation, starting with the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/folly/main/folly/docs/Overview.md&#34;&gt;overview&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Folly is published on GitHub at &lt;a href=&#34;https://github.com/facebook/folly&#34;&gt;https://github.com/facebook/folly&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Build Notes&lt;/h1&gt; &#xA;&lt;p&gt;Because folly does not provide any ABI compatibility guarantees from commit to commit, we generally recommend building folly as a static library.&lt;/p&gt; &#xA;&lt;p&gt;folly supports gcc (5.1+), clang, or MSVC. It should run on Linux (x86-32, x86-64, and ARM), iOS, macOS, and Windows (x86-64). The CMake build is only tested on some of these platforms; at a minimum, we aim to support macOS and Linux (on the latest Ubuntu LTS release or newer.)&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;getdeps.py&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This script is used by many of Meta&#39;s OSS tools. It will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s written in python so you&#39;ll need python3.6 or later on your PATH. It works on Linux, macOS and Windows.&lt;/p&gt; &#xA;&lt;p&gt;The settings for folly&#39;s cmake build are held in its getdeps manifest &lt;code&gt;build/fbcode_builder/manifests/folly&lt;/code&gt;, which you can edit locally if desired.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;If on Linux or MacOS (with homebrew installed) you can install system dependencies to save building them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo&#xA;git clone https://github.com/facebook/folly&#xA;# Install dependencies&#xA;cd folly&#xA;sudo ./build/fbcode_builder/getdeps.py install-system-deps --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to see the packages before installing them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build/fbcode_builder/getdeps.py install-system-deps --dry-run --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On other platforms or if on Linux and without system dependencies &lt;code&gt;getdeps.py&lt;/code&gt; will mostly download and build them for you during the build step.&lt;/p&gt; &#xA;&lt;p&gt;Some of the dependencies &lt;code&gt;getdeps.py&lt;/code&gt; uses and installs are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a version of boost compiled with C++14 support.&lt;/li&gt; &#xA; &lt;li&gt;googletest is required to build and run folly&#39;s tests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;This script will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; currently requires python 3.6+ to be on your path.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; will invoke cmake etc&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo&#xA;git clone https://github.com/facebook/folly&#xA;cd folly&#xA;# Build, using system dependencies if available&#xA;python3 ./build/fbcode_builder/getdeps.py --allow-system-packages build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It puts output in its scratch area:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;installed/folly/lib/libfolly.a&lt;/code&gt;: Library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also specify a &lt;code&gt;--scratch-path&lt;/code&gt; argument to control the location of the scratch directory used for the build. You can find the default scratch install location from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-inst-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are also &lt;code&gt;--install-dir&lt;/code&gt; and &lt;code&gt;--install-prefix&lt;/code&gt; arguments to provide some more fine-grained control of the installation directories. However, given that folly provides no compatibility guarantees between commits we generally recommend building and installing the libraries to a temporary location, and then pointing your project&#39;s build at this temporary location, rather than installing folly in the traditional system installation directories. e.g., if you are building with CMake you can use the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; variable to allow CMake to find folly in this temporary installation directory when building your project.&lt;/p&gt; &#xA;&lt;p&gt;If you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run tests&lt;/h3&gt; &#xA;&lt;p&gt;By default &lt;code&gt;getdeps.py&lt;/code&gt; will build the tests for folly. To run them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd folly&#xA;python3 ./build/fbcode_builder/getdeps.py --allow-system-packages test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;build.sh&lt;/code&gt;/&lt;code&gt;build.bat&lt;/code&gt; wrapper&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;build.sh&lt;/code&gt; can be used on Linux and MacOS, on Windows use the &lt;code&gt;build.bat&lt;/code&gt; script instead. Its a wrapper around &lt;code&gt;getdeps.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build with cmake directly&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t want to let getdeps invoke cmake for you then by default, building the tests is disabled as part of the CMake &lt;code&gt;all&lt;/code&gt; target. To build the tests, specify &lt;code&gt;-DBUILD_TESTS=ON&lt;/code&gt; to CMake at configure time.&lt;/p&gt; &#xA;&lt;p&gt;NB if you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate on a &lt;code&gt;getdeps.py&lt;/code&gt; build, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch-path build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Running tests with ctests also works if you cd to the build dir, e.g. &lt;code&gt; &lt;/code&gt;(cd $(python3 ./build/fbcode_builder/getdeps.py show-build-dir) &amp;amp;&amp;amp; ctest)`&lt;/p&gt; &#xA;&lt;h3&gt;Finding dependencies in non-default locations&lt;/h3&gt; &#xA;&lt;p&gt;If you have boost, gtest, or other dependencies installed in a non-default location, you can use the &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;CMAKE_LIBRARY_PATH&lt;/code&gt; variables to make CMAKE look also look for header files and libraries in non-standard locations. For example, to also search the directories &lt;code&gt;/alt/include/path1&lt;/code&gt; and &lt;code&gt;/alt/include/path2&lt;/code&gt; for header files and the directories &lt;code&gt;/alt/lib/path1&lt;/code&gt; and &lt;code&gt;/alt/lib/path2&lt;/code&gt; for libraries, you can invoke &lt;code&gt;cmake&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake \&#xA;  -DCMAKE_INCLUDE_PATH=/alt/include/path1:/alt/include/path2 \&#xA;  -DCMAKE_LIBRARY_PATH=/alt/lib/path1:/alt/lib/path2 ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ubuntu LTS, CentOS Stream, Fedora&lt;/h2&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;getdeps.py&lt;/code&gt; approach above. We test in CI on Ubuntu LTS, and occasionally on other distros.&lt;/p&gt; &#xA;&lt;p&gt;If you find the set of system packages is not quite right for your chosen distro, you can specify distro version specific overrides in the dependency manifests (e.g. &lt;a href=&#34;https://github.com/facebook/folly/raw/main/build/fbcode_builder/manifests/boost&#34;&gt;https://github.com/facebook/folly/blob/main/build/fbcode_builder/manifests/boost&lt;/a&gt; ). You could probably make it work on most recent Ubuntu/Debian or Fedora/Redhat derived distributions.&lt;/p&gt; &#xA;&lt;p&gt;At time of writing (Dec 2021) there is a build break on GCC 11.x based systems in lang_badge_test. If you don&#39;t need badge functionality you can work around by commenting it out from CMakeLists.txt (unfortunately fbthrift does need it)&lt;/p&gt; &#xA;&lt;h2&gt;Windows (Vcpkg)&lt;/h2&gt; &#xA;&lt;p&gt;Note that many tests are disabled for folly Windows builds, you can see them in the log from the cmake configure step, or by looking for WINDOWS_DISABLED in &lt;code&gt;CMakeLists.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;That said, &lt;code&gt;getdeps.py&lt;/code&gt; builds work on Windows and are tested in CI.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer, you can try Vcpkg. folly is available in &lt;a href=&#34;https://github.com/Microsoft/vcpkg#vcpkg&#34;&gt;Vcpkg&lt;/a&gt; and releases may be built via &lt;code&gt;vcpkg install folly:x64-windows&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also use &lt;code&gt;vcpkg install folly:x64-windows --head&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;macOS&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; builds work on macOS and are tested in CI, however if you prefer, you can try one of the macOS package managers&lt;/p&gt; &#xA;&lt;h3&gt;Homebrew&lt;/h3&gt; &#xA;&lt;p&gt;folly is available as a Formula and releases may be built via &lt;code&gt;brew install folly&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also use &lt;code&gt;folly/build/bootstrap-osx-homebrew.sh&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  ./folly/build/bootstrap-osx-homebrew.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a build directory &lt;code&gt;_build&lt;/code&gt; in the top-level.&lt;/p&gt; &#xA;&lt;h3&gt;MacPorts&lt;/h3&gt; &#xA;&lt;p&gt;Install the required packages from MacPorts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  sudo port install \&#xA;    boost \&#xA;    cmake \&#xA;    gflags \&#xA;    git \&#xA;    google-glog \&#xA;    libevent \&#xA;    libtool \&#xA;    lz4 \&#xA;    lzma \&#xA;    openssl \&#xA;    snappy \&#xA;    xz \&#xA;    zlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and install double-conversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/google/double-conversion.git&#xA;  cd double-conversion&#xA;  cmake -DBUILD_SHARED_LIBS=ON .&#xA;  make&#xA;  sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and install folly with the parameters listed below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/facebook/folly.git&#xA;  cd folly&#xA;  mkdir _build&#xA;  cd _build&#xA;  cmake ..&#xA;  make&#xA;  sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>sam-astro/Z-Sharp</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/sam-astro/Z-Sharp</id>
    <link href="https://github.com/sam-astro/Z-Sharp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Custom programming interpreter for ZSharp (Z#), a language I made up.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sam-astro/Z-Sharp/master/ExtraResources/ZS-Gem-Icon-Small.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sam-astro/Z-Sharp/master/ExtraResources/ZS-Logo-Light-Small.png&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Z-Sharp is no longer in development! This project was never meant to go beyond the scope of a simple thing I could make pong in, yet people continue to ask for features and fixes, and I continue to oblige. So sadly, even though this was a cool project in which I learned a lot, it will be ending now. I will eventually make some docs and standards for the syntax, and will still leave this repository open. This way anybody can make their own interpreter or compiler for it. I will also still accept pull requests for any changes to this repository.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Z-Sharp is a custom programming language I made because I don&#39;t like c++ very much (Z-Sharp&#39;s interpreter is written in c++ though). Z-Sharp scripts have the file extension .ZS. The base syntax and formatting I would say is quite similar to C# or Python, but differs as task complexity increases. It also has support for graphics using SDL2.&lt;/p&gt; &#xA;&lt;p&gt;Before using Z#: There is &lt;em&gt;&lt;strong&gt;no documentation&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;strings&lt;/strong&gt;&lt;/em&gt; barely work, &lt;em&gt;&lt;strong&gt;performance&lt;/strong&gt;&lt;/em&gt; isn&#39;t great, the syntax is &lt;em&gt;&lt;strong&gt;very specific&lt;/strong&gt;&lt;/em&gt;, and most errors just cause it to &lt;em&gt;&lt;strong&gt;crash without warning&lt;/strong&gt;&lt;/em&gt;. I am just a &lt;em&gt;single developer&lt;/em&gt; working on this during my free time; between school, other projects, and YouTube. Z-Sharp will most likely never be finished, since it was really supposed to end when the video was published about it. If you are trying to use a common programming language feature, ask yourself this: &lt;em&gt;&lt;strong&gt;Is this feature required to play pong?&lt;/strong&gt;&lt;/em&gt; If not, then most likely that feature &lt;em&gt;&lt;strong&gt;has not been implemented yet&lt;/strong&gt;&lt;/em&gt;. I initially only made the language so I could create pong and make a video about it, so it really is the &lt;em&gt;&lt;strong&gt;bare minimum&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and getting started:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stevenrafft.github.io/ZSharpDocs/#/README&#34;&gt;The docs and tutorial&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Downloading or installing is very simple, here is how depending on your version and operating system:&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to &lt;a href=&#34;https://github.com/sam-astro/Z-Sharp/releases&#34;&gt;the most recent release&lt;/a&gt; and download &lt;code&gt;ZSharp-Win-Installer.zip&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Unzip &lt;code&gt;ZSharp-Win-Installer.zip&lt;/code&gt; and open the unzipped folder.&lt;/li&gt; &#xA; &lt;li&gt;Inside is a single file titled &lt;code&gt;ZSharp-Setup.exe&lt;/code&gt;. Run it, and follow the setup instructions.&lt;/li&gt; &#xA; &lt;li&gt;If it fails to run, make sure the &lt;code&gt;MS Visual Runtime and MSVC C++ Redistribute&lt;/code&gt; are installed. You can download them &lt;a href=&#34;https://docs.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist&#34;&gt;here from Microsoft&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Now that it is installed, there are a few ways to use it: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(recommended) Any ZSharp file that ends with .ZS will automatically be associated with the interpreter. Just double-click it, and the interpreter will run.&lt;/li&gt; &#xA;   &lt;li&gt;Drag and drop any .ZS script directly onto the executable.&lt;/li&gt; &#xA;   &lt;li&gt;Use command line, providing path to interpreter and then to script like so: &lt;code&gt;&amp;gt; ./ZSharp.exe ./Pong-Example-Project/script.zs&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Feel free to use and edit the &lt;code&gt;Pong-Example-Project&lt;/code&gt;. It is a single script called &lt;code&gt;script.zs&lt;/code&gt;, and you can open it with any of the methods above. It is also located on the releases page.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you don&#39;t want to install ZSharp on your device, or you want easier acces to the executable and .DLLs, another version is provided called &lt;code&gt;ZS_Win_Base_Raw.zip&lt;/code&gt;. This just contains all of the files the installer puts on your computer.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install requirements:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You need the package &lt;em&gt;&lt;strong&gt;libsdl2-dev&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For Debian based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo apt install libsdl2-dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Arch based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo pacman -S sdl2&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You also need the package &lt;em&gt;&lt;strong&gt;libsdl2-image-dev&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For Debian based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo apt install libsdl2-image-dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Arch based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo pacman -S sdl2_image&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You also need the package &lt;em&gt;&lt;strong&gt;libsdl2-ttf-dev&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For Debian based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo apt install libsdl2-ttf-dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Arch based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo pacman -S sdl2_ttf&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to &lt;a href=&#34;https://github.com/sam-astro/Z-Sharp/releases&#34;&gt;the most recent release&lt;/a&gt; and download &lt;code&gt;ZSharp-Linux.zip&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Unzip &lt;code&gt;ZSharp-Linux.zip&lt;/code&gt; and open the unzipped folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You will see some files. The Z# interpreter is &lt;code&gt;ZSharp&lt;/code&gt;. Any time you want to execute a script, this is the program that will be used. You can use it like so:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use terminal, providing path to executable and then to script like so: &lt;code&gt;$ ./ZSharp ./Pong-Example-Project/script.zs&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Feel free to use and edit the included &lt;code&gt;Pong-Example-Project&lt;/code&gt;. It is a single script called &lt;code&gt;script.zs&lt;/code&gt;, and you can open it with any of the methods above.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Here is some example code:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;// Comments are indicated by two forward slashes&#xA;// They can only be on their own line&#xA;//    int j = 4 // &amp;lt;- This is invalid comment placement&#xA;&#xA;// All programs start with a main function&#xA;func Main()&#xA;{&#xA;    int i = 0&#xA;    string s = &#34;r&#34;&#xA;    &#xA;    i += 2&#xA;    i -= 1&#xA;    i /= 3&#xA;    i *= 2&#xA;    &#xA;    while i &amp;lt; 10&#xA;    {&#xA;        i += 1&#xA;    }&#xA;    &#xA;    if s == &#34;r&#34;&#xA;    {&#xA;        Printl(s + &#34; is r&#34;)&#xA;    }&#xA;    &#xA;    int functionNumber = ExampleFunction(&#34;A&#34;, s)&#xA;    ExampleFunction(1, 3)&#xA;    &#xA;    GlobalFunction()&#xA;}&#xA;&#xA;// Declare new function with &#39;func&#39;, then it&#39;s name, and the names of any input variables.&#xA;// The input variables don&#39;t need type, as those are automatic. Also, they don&#39;t need to&#xA;/// be assigned at all on execute and can be left blank&#xA;func ExampleFunction(inputA, inputB)&#xA;{&#xA;    Printl(&#34;In A is: &#34; + inputA)&#xA;    Printl(&#34;In B is: &#34; + inputB)&#xA;    &#xA;    // Return a value to the valling location&#xA;    return 4&#xA;}&#xA;&#xA;func GlobalFunction()&#xA;{&#xA;    // Create variables that can be accessed from anywhere (ex. in Main or ExampleFunction) with the &#39;global&#39; keyword before type&#xA;    global int x = 12&#xA;    global string y = &#34;Y String&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is how to use graphics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;func Main()&#xA;{&#xA;    int screenWidth = 500&#xA;    int screenHeight = 500&#xA;    ZS.Graphics.Init(&#34;Title of window&#34;, screenWidth, screenHeight)&#xA;    // After graphics are initialized, the main function will not finish.&#xA;    // Instead, Start() will be called a single time, then Update() every frame after that.&#xA;}&#xA;&#xA;// Runs once at start of graphics initialization&#xA;func Start()&#xA;{&#xA;    // Vec2 are initialized using function &#39;NVec2(x, y)&#39;&#xA;    Vec2 position = NVec2(250, 250)&#xA;    Vec2 scale = NVec2(20, 20)&#xA;    float rotation = 0&#xA;&#xA;    // Sprite object, stores (and loads from file) the texture, location, scale, and rotation&#xA;    global Sprite exampleSprite = ZS.Graphics.Sprite(&#34;./square.png&#34;, position, scale, rotation)&#xA;}&#xA;&#xA;// Executes each frame&#xA;func Update(deltaTime)&#xA;{&#xA;    // Draws the image created in Start(). This is usually at the end of update.&#xA;    ZS.Graphics.Draw(exampleSprite)   &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, ZSharp is &lt;em&gt;&lt;strong&gt;VERY&lt;/strong&gt;&lt;/em&gt; strict with formatting, and can throw an error if you forget to put a space somewhere. Also, speaking of errors, if your code has any it will show in the console. Errors are colored red, and warnings are colored yellow. A line number will also usually be provided. This is &lt;em&gt;&lt;strong&gt;Not&lt;/strong&gt;&lt;/em&gt; the line relative to the &lt;em&gt;documents&lt;/em&gt; beginning, but rather the &lt;em&gt;functions&lt;/em&gt; beginning. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ERROR: line 5 in function Main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is the 5th line &lt;em&gt;inside of Main&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;func Main()&#xA;{&#xA;   // line 1&#xA;   // line 2&#xA;   // line 3&#xA;   // line 4&#xA;   int g = &#34;s&#34;&#xA;   // ^ above line is the error, since it is line 5&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I am planning to change how error reporting works to report the document line number as well, but this is how it is for now.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MarlinFirmware/Marlin</title>
    <updated>2022-05-31T01:58:42Z</updated>
    <id>tag:github.com,2022-05-31:/MarlinFirmware/Marlin</id>
    <link href="https://github.com/MarlinFirmware/Marlin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Marlin is an optimized firmware for RepRap 3D printers based on the Arduino platform. | Many commercial 3D printers come with Marlin installed. Check with your vendor if you need source code for your specific machine.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MarlinFirmware/Marlin/bugfix-2.0.x/buildroot/share/pixmaps/logo/marlin-outrun-nf-500.png&#34; height=&#34;250&#34; alt=&#34;MarlinFirmware&#39;s logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Marlin 3D Printer Firmware&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MarlinFirmware/Marlin/bugfix-2.0.x/LICENSE&#34;&gt;&lt;img alt=&#34;GPL-V3.0 License&#34; src=&#34;https://img.shields.io/github/license/marlinfirmware/marlin.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/graphs/contributors&#34;&gt;&lt;img alt=&#34;Contributors&#34; src=&#34;https://img.shields.io/github/contributors/marlinfirmware/marlin.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/releases&#34;&gt;&lt;img alt=&#34;Last Release Date&#34; src=&#34;https://img.shields.io/github/release-date/MarlinFirmware/Marlin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/actions&#34;&gt;&lt;img alt=&#34;CI Status&#34; src=&#34;https://github.com/MarlinFirmware/Marlin/actions/workflows/test-builds.yml/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/thinkyhead&#34;&gt;&lt;img alt=&#34;GitHub Sponsors&#34; src=&#34;https://img.shields.io/github/sponsors/thinkyhead?color=db61a2&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/MarlinFirmware&#34;&gt;&lt;img alt=&#34;Follow MarlinFirmware on Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/MarlinFirmware?style=social&amp;amp;logo=twitter&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Additional documentation can be found at the &lt;a href=&#34;https://marlinfw.org/&#34;&gt;Marlin Home Page&lt;/a&gt;. Please test this firmware and let us know if it misbehaves in any way. Volunteers are standing by!&lt;/p&gt; &#xA;&lt;h2&gt;Marlin 2.0 Bugfix Branch&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Not for production use. Use with caution!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Marlin 2.0 takes this popular RepRap firmware to the next level by adding support for much faster 32-bit and ARM-based boards while improving support for 8-bit AVR boards. Read about Marlin&#39;s decision to use a &#34;Hardware Abstraction Layer&#34; below.&lt;/p&gt; &#xA;&lt;p&gt;This branch is for patches to the latest 2.0.x release version. Periodically this branch will form the basis for the next minor 2.0.x release.&lt;/p&gt; &#xA;&lt;p&gt;Download earlier versions of Marlin on the &lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/releases&#34;&gt;Releases page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example Configurations&lt;/h2&gt; &#xA;&lt;p&gt;Before building Marlin you&#39;ll need to configure it for your specific hardware. Your vendor should have already provided source code with configurations for the installed firmware, but if you ever decide to upgrade you&#39;ll need updated configuration files. Marlin users have contributed dozens of tested example configurations to get you started. Visit the &lt;a href=&#34;https://github.com/MarlinFirmware/Configurations&#34;&gt;MarlinFirmware/Configurations&lt;/a&gt; repository to find the right configuration for your hardware.&lt;/p&gt; &#xA;&lt;h2&gt;Building Marlin 2.0&lt;/h2&gt; &#xA;&lt;p&gt;To build Marlin 2.0 you&#39;ll need &lt;a href=&#34;https://www.arduino.cc/en/main/software&#34;&gt;Arduino IDE 1.8.8 or newer&lt;/a&gt; or &lt;a href=&#34;https://docs.platformio.org/en/latest/ide.html#platformio-ide&#34;&gt;PlatformIO&lt;/a&gt;. We&#39;ve posted detailed instructions on &lt;a href=&#34;https://marlinfw.org/docs/basics/install_arduino.html&#34;&gt;Building Marlin with Arduino&lt;/a&gt; and &lt;a href=&#34;https://marlinfw.org/docs/basics/install_rearm.html&#34;&gt;Building Marlin with PlatformIO for ReArm&lt;/a&gt; (which applies well to other 32-bit boards).&lt;/p&gt; &#xA;&lt;h2&gt;Hardware Abstraction Layer (HAL)&lt;/h2&gt; &#xA;&lt;p&gt;Marlin 2.0 introduces a layer of abstraction so that all the existing high-level code can be built for 32-bit platforms while still retaining full 8-bit AVR compatibility. Retaining AVR compatibility and a single code-base is important to us, because we want to make sure that features and patches get as much testing and attention as possible, and that all platforms always benefit from the latest improvements.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Platforms&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Platform&lt;/th&gt; &#xA;   &lt;th&gt;MCU&lt;/th&gt; &#xA;   &lt;th&gt;Example Boards&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.arduino.cc/&#34;&gt;Arduino AVR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ATmega&lt;/td&gt; &#xA;   &lt;td&gt;RAMPS, Melzi, RAMBo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.microchip.com/en-us/product/AT90USB1286&#34;&gt;Teensy++ 2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AT90USB1286&lt;/td&gt; &#xA;   &lt;td&gt;Printrboard&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.arduino.cc/en/Guide/ArduinoDue&#34;&gt;Arduino Due&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SAM3X8E&lt;/td&gt; &#xA;   &lt;td&gt;RAMPS-FD, RADDS, RAMPS4DUE&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/espressif/arduino-esp32&#34;&gt;ESP32&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ESP32&lt;/td&gt; &#xA;   &lt;td&gt;FYSETC E4, E4d@BOX, MRR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nxp.com/products/processors-and-microcontrollers/arm-microcontrollers/general-purpose-mcus/lpc1700-cortex-m3/512-kb-flash-64-kb-sram-ethernet-usb-lqfp100-package:LPC1768FBD100&#34;&gt;LPC1768&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M3&lt;/td&gt; &#xA;   &lt;td&gt;MKS SBASE, Re-ARM, Selena Compact&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nxp.com/products/processors-and-microcontrollers/arm-microcontrollers/general-purpose-mcus/lpc1700-cortex-m3/512-kb-flash-64-kb-sram-ethernet-usb-lqfp100-package:LPC1769FBD100&#34;&gt;LPC1769&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M3&lt;/td&gt; &#xA;   &lt;td&gt;Smoothieboard, Azteeg X5 mini, TH3D EZBoard&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.st.com/en/microcontrollers-microprocessors/stm32f103.html&#34;&gt;STM32F103&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M3&lt;/td&gt; &#xA;   &lt;td&gt;Malyan M200, GTM32 Pro, MKS Robin, BTT SKR Mini&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.st.com/en/microcontrollers-microprocessors/stm32f401.html&#34;&gt;STM32F401&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M4&lt;/td&gt; &#xA;   &lt;td&gt;ARMED, Rumba32, SKR Pro, Lerdge, FYSETC S6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.st.com/en/microcontrollers-microprocessors/stm32f7x6.html&#34;&gt;STM32F7x6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M7&lt;/td&gt; &#xA;   &lt;td&gt;The Borg, RemRam V1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.adafruit.com/product/4064&#34;&gt;SAMD51P20A&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M4&lt;/td&gt; &#xA;   &lt;td&gt;Adafruit Grand Central M4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pjrc.com/store/teensy35.html&#34;&gt;Teensy 3.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pjrc.com/store/teensy36.html&#34;&gt;Teensy 3.6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pjrc.com/store/teensy40.html&#34;&gt;Teensy 4.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pjrc.com/store/teensy41.html&#34;&gt;Teensy 4.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ARM® Cortex-M7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux Native&lt;/td&gt; &#xA;   &lt;td&gt;x86/ARM/etc.&lt;/td&gt; &#xA;   &lt;td&gt;Raspberry Pi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Submitting Patches&lt;/h2&gt; &#xA;&lt;p&gt;Proposed patches should be submitted as a Pull Request against the (&lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/tree/bugfix-2.0.x&#34;&gt;bugfix-2.0.x&lt;/a&gt;) branch.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This branch is for fixing bugs and integrating any new features for the duration of the Marlin 2.0.x life-cycle.&lt;/li&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://marlinfw.org/docs/development/coding_standards.html&#34;&gt;Coding Standards&lt;/a&gt; to gain points with the maintainers.&lt;/li&gt; &#xA; &lt;li&gt;Please submit Feature Requests and Bug Reports to the &lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/issues/new/choose&#34;&gt;Issue Queue&lt;/a&gt;. Support resources are also listed there.&lt;/li&gt; &#xA; &lt;li&gt;Whenever you add new features, be sure to add tests to &lt;code&gt;buildroot/tests&lt;/code&gt; and then run your tests locally, if possible. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It&#39;s optional: Running all the tests on Windows might take a long time, and they will run anyway on GitHub.&lt;/li&gt; &#xA;   &lt;li&gt;If you&#39;re running the tests on Linux (or on WSL with the code on a Linux volume) the speed is much faster.&lt;/li&gt; &#xA;   &lt;li&gt;You can use &lt;code&gt;make tests-all-local&lt;/code&gt; or &lt;code&gt;make tests-single-local TEST_TARGET=...&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;If you prefer Docker you can use &lt;code&gt;make tests-all-local-docker&lt;/code&gt; or &lt;code&gt;make tests-all-local-docker TEST_TARGET=...&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Marlin Support&lt;/h2&gt; &#xA;&lt;p&gt;The Issue Queue is reserved for Bug Reports and Feature Requests. To get help with configuration and troubleshooting, please use the following resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://marlinfw.org&#34;&gt;Marlin Documentation&lt;/a&gt; - Official Marlin documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/n5NJ59y&#34;&gt;Marlin Discord&lt;/a&gt; - Discuss issues with Marlin users and developers&lt;/li&gt; &#xA; &lt;li&gt;Facebook Group &lt;a href=&#34;https://www.facebook.com/groups/1049718498464482/&#34;&gt;&#34;Marlin Firmware&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;RepRap.org &lt;a href=&#34;https://forums.reprap.org/list.php?415&#34;&gt;Marlin Forum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Group &lt;a href=&#34;https://www.facebook.com/groups/3Dtechtalk/&#34;&gt;&#34;Marlin Firmware for 3D Printers&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/results?search_query=marlin+configuration&#34;&gt;Marlin Configuration&lt;/a&gt; on YouTube&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Marlin is constantly improving thanks to a huge number of contributors from all over the world bringing their specialties and talents. Huge thanks are due to &lt;a href=&#34;https://github.com/MarlinFirmware/Marlin/graphs/contributors&#34;&gt;all the contributors&lt;/a&gt; who regularly patch up bugs, help direct traffic, and basically keep Marlin from falling apart. Marlin&#39;s continued existence would not be possible without them.&lt;/p&gt; &#xA;&lt;h2&gt;Administration&lt;/h2&gt; &#xA;&lt;p&gt;Regular users can open and close their own issues, but only the administrators can do project-related things like add labels, merge changes, set milestones, and kick trolls. The current Marlin admin team consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scott Lahteine [&lt;a href=&#34;https://github.com/thinkyhead&#34;&gt;@thinkyhead&lt;/a&gt;] - USA - Project Maintainer &amp;nbsp; &lt;a href=&#34;https://www.thinkyhead.com/donate-to-marlin&#34;&gt;💸 Donate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Roxanne Neufeld [&lt;a href=&#34;https://github.com/Roxy-3D&#34;&gt;@Roxy-3D&lt;/a&gt;] - USA&lt;/li&gt; &#xA; &lt;li&gt;Keith Bennett [&lt;a href=&#34;https://github.com/thisiskeithb&#34;&gt;@thisiskeithb&lt;/a&gt;] - USA &amp;nbsp; &lt;a href=&#34;https://github.com/sponsors/thisiskeithb&#34;&gt;💸 Donate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Peter Ellens [&lt;a href=&#34;https://github.com/ellensp&#34;&gt;@ellensp&lt;/a&gt;] - New Zealand&lt;/li&gt; &#xA; &lt;li&gt;Victor Oliveira [&lt;a href=&#34;https://github.com/rhapsodyv&#34;&gt;@rhapsodyv&lt;/a&gt;] - Brazil&lt;/li&gt; &#xA; &lt;li&gt;Chris Pepper [&lt;a href=&#34;https://github.com/p3p&#34;&gt;@p3p&lt;/a&gt;] - UK&lt;/li&gt; &#xA; &lt;li&gt;Jason Smith [&lt;a href=&#34;https://github.com/sjasonsmith&#34;&gt;@sjasonsmith&lt;/a&gt;] - USA&lt;/li&gt; &#xA; &lt;li&gt;Luu Lac [&lt;a href=&#34;https://github.com/shitcreek&#34;&gt;@shitcreek&lt;/a&gt;] - USA&lt;/li&gt; &#xA; &lt;li&gt;Bob Kuhn [&lt;a href=&#34;https://github.com/Bob-the-Kuhn&#34;&gt;@Bob-the-Kuhn&lt;/a&gt;] - USA&lt;/li&gt; &#xA; &lt;li&gt;Erik van der Zalm [&lt;a href=&#34;https://github.com/ErikZalm&#34;&gt;@ErikZalm&lt;/a&gt;] - Netherlands &amp;nbsp; &lt;a href=&#34;https://flattr.com/submit/auto?user_id=ErikZalm&amp;amp;url=https://github.com/MarlinFirmware/Marlin&amp;amp;title=Marlin&amp;amp;language=&amp;amp;tags=github&amp;amp;category=software&#34;&gt;💸 Donate&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Marlin is published under the &lt;a href=&#34;https://raw.githubusercontent.com/MarlinFirmware/Marlin/bugfix-2.0.x/LICENSE&#34;&gt;GPL license&lt;/a&gt; because we believe in open development. The GPL comes with both rights and obligations. Whether you use Marlin firmware as the driver for your open or closed-source product, you must keep Marlin open, and you must provide your compatible Marlin source code to end users upon request. The most straightforward way to comply with the Marlin license is to make a fork of Marlin on Github, perform your modifications, and direct users to your modified fork.&lt;/p&gt; &#xA;&lt;p&gt;While we can&#39;t prevent the use of this code in products (3D printers, CNC, etc.) that are closed source or crippled by a patent, we would prefer that you choose another firmware or, better yet, make your own.&lt;/p&gt;</summary>
  </entry>
</feed>