<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T01:59:58Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>protocolbuffers/protobuf</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/protocolbuffers/protobuf</id>
    <link href="https://github.com/protocolbuffers/protobuf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2008 Google Inc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf&#39;s documentation on the Google Developers site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Compiler Installation&lt;/h2&gt; &#xA;&lt;p&gt;The protocol compiler is written in C++. If you are using C++, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; &#xA;&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf/releases&#34;&gt;https://github.com/protocolbuffers/protobuf/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the maven repo here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&#34;&gt;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; &#xA;&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&#34;&gt;src&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&#34;&gt;java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&#34;&gt;objectivec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C#&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&#34;&gt;csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&#34;&gt;ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf-go&#34;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&#34;&gt;php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dart-lang/protobuf&#34;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The best way to learn how to use protobuf is to follow the tutorials in our developer guide:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;https://developers.google.com/protocol-buffers/docs/tutorials&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The complete documentation for Protocol Buffers is available via the web at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PolyMC/PolyMC</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/PolyMC/PolyMC</id>
    <link href="https://github.com/PolyMC/PolyMC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A custom launcher for Minecraft that allows you to easily manage multiple installations of Minecraft at once (Fork of MultiMC)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PolyMC/PolyMC/develop/program_info/polymc-header-black.svg#gh-light-mode-only&#34; alt=&#34;PolyMC logo&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PolyMC/PolyMC/develop/program_info/polymc-header.svg#gh-dark-mode-only&#34; alt=&#34;PolyMC logo&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;PolyMC is a custom launcher for Minecraft that focuses on predictability, long term stability and simplicity.&lt;/p&gt; &#xA;&lt;p&gt;This is a &lt;strong&gt;fork&lt;/strong&gt; of the MultiMC Launcher and not endorsed by MultiMC. If you want to read about why this fork was created, check out &lt;a href=&#34;https://polymc.org/wiki/overview/faq/&#34;&gt;our FAQ page&lt;/a&gt;. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All downloads and instructions for PolyMC can be found &lt;a href=&#34;https://polymc.org/download/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Last build status: &lt;a href=&#34;https://github.com/PolyMC/PolyMC/actions&#34;&gt;https://github.com/PolyMC/PolyMC/actions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development Builds&lt;/h2&gt; &#xA;&lt;p&gt;There are per-commit development builds available &lt;a href=&#34;https://github.com/PolyMC/PolyMC/actions&#34;&gt;here&lt;/a&gt;. These have debug information in the binaries, so their file sizes are relatively larger. Portable builds are provided for AppImage on Linux, Windows, and macOS.&lt;/p&gt; &#xA;&lt;p&gt;For Debian and Arch, you can use these packages for the latest development versions:&lt;br&gt; &lt;a href=&#34;https://aur.archlinux.org/packages/polymc-git/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/aur-polymc--git-blue&#34; alt=&#34;polymc-git&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mpr.makedeb.org/packages/polymc-git&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mpr-polymc--git-orange&#34; alt=&#34;polymc-git&#34;&gt;&lt;/a&gt;&lt;br&gt; For flatpak, you can use &lt;a href=&#34;https://discourse.flathub.org/t/how-to-use-flathub-beta/2111&#34;&gt;flathub-beta&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Help &amp;amp; Support&lt;/h1&gt; &#xA;&lt;p&gt;Feel free to create an issue if you need help. However, you might find it easier to ask in the Discord server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/xq7fxrgtMP&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/923671181020766230?label=PolyMC%20Discord&#34; alt=&#34;PolyMC Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For people who don&#39;t want to use Discord, we have a Matrix Space which is bridged to the Discord server:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#polymc:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc:matrix.org?label=PolyMC%20space&#34; alt=&#34;PolyMC Space&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If there are any issues with the space or you are using a client that does not support the feature here are the individual rooms:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#polymc-development:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-development:matrix.org?label=PolyMC%20Development&#34; alt=&#34;Development&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-discussion:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-discussion:matrix.org?label=PolyMC%20Discussion&#34; alt=&#34;Discussion&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-github:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-github:matrix.org?label=PolyMC%20Github&#34; alt=&#34;Github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-maintainers:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-maintainers:matrix.org?label=PolyMC%20Maintainers&#34; alt=&#34;Maintainers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-news:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-news:matrix.org?label=PolyMC%20News&#34; alt=&#34;News&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-offtopic:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-offtopic:matrix.org?label=PolyMC%20Offtopic&#34; alt=&#34;Offtopic&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-support:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-support:matrix.org?label=PolyMC%20Support&#34; alt=&#34;Support&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://matrix.to/#/#polymc-voice:matrix.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/matrix/polymc-voice:matrix.org?label=PolyMC%20Voice&#34; alt=&#34;Voice&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;we also have a subreddit you can post your issues and suggestions on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.reddit.com/r/PolyMCLauncher/&#34;&gt;r/PolyMCLauncher&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;If you want to contribute to PolyMC you might find it useful to join our Discord Server or Matrix Space.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;If you want to build PolyMC yourself, check &lt;a href=&#34;https://polymc.org/wiki/development/build-instructions/&#34;&gt;Build Instructions&lt;/a&gt; for build instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Code formatting&lt;/h2&gt; &#xA;&lt;p&gt;Just follow the existing formatting.&lt;/p&gt; &#xA;&lt;p&gt;In general, in order of importance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure your IDE is not messing up line endings or whitespace and avoid using linters.&lt;/li&gt; &#xA; &lt;li&gt;Prefer readability over dogma.&lt;/li&gt; &#xA; &lt;li&gt;Keep to the existing formatting.&lt;/li&gt; &#xA; &lt;li&gt;Indent with 4 space unless it&#39;s in a submodule.&lt;/li&gt; &#xA; &lt;li&gt;Keep lists (of arguments, parameters, initializers...) as lists, not paragraphs. It should either read from top to bottom, or left to right. Not both.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;p&gt;The translation effort for PolyMC is hosted on &lt;a href=&#34;https://hosted.weblate.org/projects/polymc/polymc/&#34;&gt;Weblate&lt;/a&gt; and information about translating PolyMC is available at &lt;a href=&#34;https://github.com/PolyMC/Translations&#34;&gt;https://github.com/PolyMC/Translations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download information&lt;/h2&gt; &#xA;&lt;p&gt;To modify download information or change packaging information send a pull request or issue to the website &lt;a href=&#34;https://github.com/PolyMC/polymc.github.io/raw/master/src/download.md&#34;&gt;Here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Forking/Redistributing/Custom builds policy&lt;/h2&gt; &#xA;&lt;p&gt;Do whatever you want, we don&#39;t care. Just follow the license. If you have any questions about this feel free to ask in an issue.&lt;/p&gt; &#xA;&lt;p&gt;All launcher code is available under the GPL-3.0-only license.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/PolyMC/polymc.github.io&#34;&gt;Source for the website&lt;/a&gt; is hosted under the AGPL-3.0-or-later License.&lt;/p&gt; &#xA;&lt;p&gt;The logo and related assets are under the CC BY-SA 4.0 license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>WasmEdge/WasmEdge</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/WasmEdge/WasmEdge</id>
    <link href="https://github.com/WasmEdge/WasmEdge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. It powers serverless apps, embedded functions, microservices, smart contracts, and IoT devices.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;right&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/README-zh.md&#34;&gt;中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/README-zh-TW.md&#34;&gt;正體中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/wasmedge-runtime-logo.png&#34; alt=&#34;WasmEdge Logo&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime. It is &lt;a href=&#34;https://ieeexplore.ieee.org/document/9214403&#34;&gt;the fastest Wasm VM&lt;/a&gt; today. WasmEdge is an official sandbox project hosted by the &lt;a href=&#34;https://www.cncf.io/&#34;&gt;CNCF&lt;/a&gt;. Its &lt;a href=&#34;https://wasmedge.org/book/en/intro/use.html&#34;&gt;use cases&lt;/a&gt; include modern web application architectures (Isomorphic &amp;amp; Jamstack applications), microservices on the edge cloud, serverless SaaS APIs, embedded functions, smart contracts, and smart devices.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/WasmEdge/WasmEdge/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt; &lt;a href=&#34;https://codecov.io/gh/WasmEdge/WasmEdge&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/WasmEdge/WasmEdge/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge/actions/workflows/codeql-analysis.yml&#34;&gt;&lt;img src=&#34;https://github.com/WasmEdge/WasmEdge/actions/workflows/codeql-analysis.yml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/5059&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/5059/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Quick start guides&lt;/h1&gt; &#xA;&lt;p&gt;🚀 &lt;a href=&#34;https://wasmedge.org/book/en/start/install.html&#34;&gt;Install&lt;/a&gt; WasmEdge &lt;br&gt; 🤖 &lt;a href=&#34;https://wasmedge.org/book/en/extend/build.html&#34;&gt;Build&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/CONTRIBUTING.md&#34;&gt;contribute to&lt;/a&gt; WasmEdge &lt;br&gt; ⌨️ &lt;a href=&#34;https://wasmedge.org/book/en/index.html#webassembly-examples&#34;&gt;Run&lt;/a&gt; a standalone Wasm program or a &lt;a href=&#34;https://wasmedge.org/book/en/dev/js.html&#34;&gt;JavaScript program&lt;/a&gt; from CLI or &lt;a href=&#34;https://wasmedge.org/book/en/start/docker.html&#34;&gt;Docker&lt;/a&gt; &lt;br&gt; 🔌 Embed a Wasm function in your &lt;a href=&#34;https://wasmedge.org/book/en/embed/node.html&#34;&gt;Node.js&lt;/a&gt;, &lt;a href=&#34;https://wasmedge.org/book/en/embed/go.html&#34;&gt;Go&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/bindings/rust/&#34;&gt;Rust&lt;/a&gt;, or &lt;a href=&#34;https://wasmedge.org/book/en/embed/c.html&#34;&gt;C&lt;/a&gt; app &lt;br&gt; 🛠 Manage and orchestrate Wasm runtimes using &lt;a href=&#34;https://wasmedge.org/book/en/kubernetes.html&#34;&gt;Kubernetes&lt;/a&gt;, &lt;a href=&#34;https://wasmedge.org/book/en/frameworks/app/yomo.html&#34;&gt;data streaming frameworks&lt;/a&gt;, and &lt;a href=&#34;https://medium.com/ethereum-on-steroids/running-ethereum-smart-contracts-in-a-substrate-blockchain-56fbc27fc95a&#34;&gt;blockchains&lt;/a&gt; &lt;br&gt; 📚 &lt;strong&gt;&lt;a href=&#34;https://wasmedge.org/book/en/&#34;&gt;Check out our official documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;The WasmEdge Runtime provides a well-defined execution sandbox for its contained WebAssembly bytecode program. The runtime offers isolation and protection for operating system resources (e.g., file system, sockets, environment variables, processes) and memory space. The most important use case for WasmEdge is to safely execute user-defined or community-contributed code as plug-ins in a software product (e.g., SaaS, software-defined vehicles, edge nodes, or even blockchain nodes). It enables third-party developers, vendors, suppliers, and community members to extend and customize the software product. &lt;strong&gt;&lt;a href=&#34;https://wasmedge.org/book/en/intro/use.html&#34;&gt;Learn more here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07115&#34;&gt;A Lightweight Design for High-performance Serverless Computing&lt;/a&gt;, published on IEEE Software, Jan 2021. &lt;a href=&#34;https://arxiv.org/abs/2010.07115&#34;&gt;https://arxiv.org/abs/2010.07115&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.infoq.com/articles/arm-vs-x86-cloud-performance/&#34;&gt;Performance Analysis for Arm vs. x86 CPUs in the Cloud&lt;/a&gt;, published on infoQ.com, Jan 2021. &lt;a href=&#34;https://www.infoq.com/articles/arm-vs-x86-cloud-performance/&#34;&gt;https://www.infoq.com/articles/arm-vs-x86-cloud-performance/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.suborbital.dev/suborbital-wasmedge&#34;&gt;WasmEdge is the fastest WebAssembly Runtime in Suborbital Reactr test suite&lt;/a&gt;, Dec 2021&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;WasmEdge can run standard WebAssembly bytecode programs compiled from C/C++, Rust, Swift, AssemblyScript, or Kotlin source code. It &lt;a href=&#34;https://wasmedge.org/book/en/dev/js.html&#34;&gt;runs JavaScript&lt;/a&gt;, including 3rd party ES6, CJS, and NPM modules, in a secure, fast, lightweight, portable, and containerized sandbox. It also supports mixing of those languages (e.g., to &lt;a href=&#34;https://wasmedge.org/book/en/dev/js/rust.html&#34;&gt;use Rust to implement a JavaScript API&lt;/a&gt;), the &lt;a href=&#34;https://wasmedge.org/book/en/dev/js/fetch.html&#34;&gt;Fetch&lt;/a&gt; API, and &lt;a href=&#34;https://wasmedge.org/book/en/dev/js/ssr.html&#34;&gt;Server-side Rendering (SSR)&lt;/a&gt; functions on edge servers.&lt;/p&gt; &#xA;&lt;p&gt;WasmEdge supports all standard WebAssembly features and many proposed extensions. It also supports a number of extensions tailored for cloud-native and edge computing uses (e.g., the &lt;a href=&#34;https://wasmedge.org/book/en/dev/rust/networking.html&#34;&gt;WasmEdge network sockets&lt;/a&gt;, and the &lt;a href=&#34;https://wasmedge.org/book/en/dev/rust/tensorflow.html&#34;&gt;WasmEdge Tensorflow extension&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learn more about &lt;a href=&#34;https://wasmedge.org/book/en/intro/features.html&#34;&gt;technical highlights&lt;/a&gt; of WasmEdge.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Integrations and management&lt;/h2&gt; &#xA;&lt;p&gt;WasmEdge and its contained wasm program can be started from the &lt;a href=&#34;https://wasmedge.org/book/en/index.html&#34;&gt;CLI&lt;/a&gt; as a new process, or from a existing process. If started from an existing process (e.g., from a running &lt;a href=&#34;https://wasmedge.org/book/en/embed/node.html&#34;&gt;Node.js&lt;/a&gt; or &lt;a href=&#34;https://wasmedge.org/book/en/embed/go.html&#34;&gt;Go&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/bindings/rust/wasmedge-rs&#34;&gt;Rust&lt;/a&gt; program), WasmEdge will simply run inside the process as a function. Currently, WasmEdge is not yet thread-safe. In order to use WasmEdge in your own application or cloud-native frameworks, please refer to the guides below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/book/en/embed.html&#34;&gt;Embed WasmEdge into a host application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/book/en/kubernetes.html&#34;&gt;Orchestrate and manage WasmEdge instances using container tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/book/en/frameworks/mesh/dapr.html&#34;&gt;Run a WasmEdge app as a Dapr microservice&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/book/en/frameworks/app/reactr.html&#34;&gt;Use Reactr to embed and extend WasmEdge functions in SaaS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to the WasmEdge project, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; document for details. If you are looking for ideas, checkout our &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;&#34;help wanted&#34; issues&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to open a GitHub issue on a related project or to join the following channels:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mailing list: Send an email to &lt;a href=&#34;https://groups.google.com/g/wasmedge/&#34;&gt;WasmEdge@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord: Join the &lt;a href=&#34;https://discord.gg/h4KDyB8XTt&#34;&gt;WasmEdge Discord server&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Slack: Join the #WasmEdge channel on the &lt;a href=&#34;https://slack.cncf.io/&#34;&gt;CNCF Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter: Follow @realwasmedge on &lt;a href=&#34;https://twitter.com/realwasmedge&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community Meeting&lt;/h2&gt; &#xA;&lt;p&gt;We host a monthly community meeting to showcase new features, demo new use cases, and a Q&amp;amp;A part. Everyone is welcome!&lt;/p&gt; &#xA;&lt;p&gt;Time: The first Tuesday of each month at 11PM Hong Kong Time/ 7AM PST.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/document/d/1iFlVl7R97Lze4RDykzElJGDjjWYDlkI8Rhf8g4dQ5Rk/edit#&#34;&gt;Public meeting agenda/notes&lt;/a&gt; | &lt;a href=&#34;https://us06web.zoom.us/j/88282362606?pwd=UFhOdzlVKyswdW43c21BKy9DdkdyUT09&#34;&gt;Zoom link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/rocksdb</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/facebook/rocksdb</id>
    <link href="https://github.com/facebook/rocksdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library that provides an embeddable, persistent key-value store for fast storage.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;RocksDB: A Persistent Key-Value Store for Flash and RAM Storage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebook/rocksdb.svg?style=svg&#34; alt=&#34;CircleCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/github/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/facebook/rocksdb.svg?branch=main&#34; alt=&#34;TravisCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/Facebook/rocksdb/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/fbgfu0so3afcno78/branch/main?svg=true&#34; alt=&#34;Appveyor Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://140-211-168-68-openstack.osuosl.org:8080/job/rocksdb&#34;&gt;&lt;img src=&#34;http://140-211-168-68-openstack.osuosl.org:8080/buildStatus/icon?job=rocksdb&amp;amp;style=plastic&#34; alt=&#34;PPC64le Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RocksDB is developed and maintained by Facebook Database Engineering Team. It is built on earlier work on &lt;a href=&#34;https://github.com/google/leveldb&#34;&gt;LevelDB&lt;/a&gt; by Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;This code is a library that forms the core building block for a fast key-value server, especially suited for storing data on flash drives. It has a Log-Structured-Merge-Database (LSM) design with flexible tradeoffs between Write-Amplification-Factor (WAF), Read-Amplification-Factor (RAF) and Space-Amplification-Factor (SAF). It has multi-threaded compactions, making it especially suitable for storing multiple terabytes of data in a single database.&lt;/p&gt; &#xA;&lt;p&gt;Start with example usage here: &lt;a href=&#34;https://github.com/facebook/rocksdb/tree/main/examples&#34;&gt;https://github.com/facebook/rocksdb/tree/main/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki&#34;&gt;github wiki&lt;/a&gt; for more explanation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in &lt;code&gt;include/&lt;/code&gt;. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Questions and discussions are welcome on the &lt;a href=&#34;https://www.facebook.com/groups/rocksdb.dev/&#34;&gt;RocksDB Developers Public&lt;/a&gt; Facebook group and &lt;a href=&#34;https://groups.google.com/g/rocksdb&#34;&gt;email list&lt;/a&gt; on Google Groups.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;RocksDB is dual-licensed under both the GPLv2 (found in the COPYING file in the root directory) and Apache 2.0 License (found in the LICENSE.Apache file in the root directory). You may select, at your option, one of the above-listed licenses.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MaaAssistantArknights/MaaAssistantArknights</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/MaaAssistantArknights/MaaAssistantArknights</id>
    <link href="https://github.com/MaaAssistantArknights/MaaAssistantArknights" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《明日方舟》小助手，自动刷图、智能基建换班，全日常一键长草！&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;LOGO&#34; src=&#34;https://user-images.githubusercontent.com/18511905/148931479-23aef436-2fc1-4c1e-84c9-bae17be710a5.png&#34; width=&#34;360&#34; height=&#34;270/&#34;&gt; &#xA; &lt;h1&gt;MaaAssistantArknights&lt;/h1&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;img alt=&#34;C++&#34; src=&#34;https://img.shields.io/badge/c++-17-%2300599C?logo=cplusplus&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;img alt=&#34;platform&#34; src=&#34;https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-blueviolet&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/github/license/MaaAssistantArknights/MaaAssistantArknights&#34;&gt; &#xA;  &lt;img alt=&#34;commit&#34; src=&#34;https://img.shields.io/github/commit-activity/m/MaaAssistantArknights/MaaAssistantArknights?color=%23ff69b4&#34;&gt; &#xA;  &lt;img alt=&#34;stars&#34; src=&#34;https://img.shields.io/github/stars/MaaAssistantArknights/MaaAssistantArknights?style=social&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;MAA 的意思是 MAA Assistant Arknights&lt;/p&gt; &#xA; &lt;p&gt;一款明日方舟游戏小助手&lt;/p&gt; &#xA; &lt;p&gt;基于图像识别技术，一键完成全部日常任务！&lt;/p&gt; &#xA; &lt;p&gt;绝赞更新中 ✿✿ヽ(°▽°)ノ✿&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;亮点功能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;刷理智，掉落识别及上传 &lt;a href=&#34;https://penguin-stats.cn/&#34;&gt;企鹅物流&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;智能基建换班，自动计算干员效率，单设施内最优解&lt;/li&gt; &#xA; &lt;li&gt;自动公招，可选使用加急许可，一次全部刷完&lt;/li&gt; &#xA; &lt;li&gt;访问好友、收取信用及购物、领取日常奖励等。一键全日常自动长草！&lt;/li&gt; &#xA; &lt;li&gt;肉鸽全自动刷源石锭和蜡烛，自动识别干员及练度&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;新功能！导入作业 JSON 文件，自动抄作业！&lt;/strong&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV14u411673q/&#34;&gt;视频演示&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;话不多说，看图！&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18511905/148376809-a80537b7-5e97-4978-959e-afada28c03c3.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/18511905/152695664-382dc0cd-de6c-4012-890f-456f697e8724.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;下载地址&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MaaAssistantArknights/MaaAssistantArknights/releases/latest&#34;&gt;稳定版&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/MaaAssistantArknights/MaaAssistantArknights/releases&#34;&gt;测试版&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;使用说明&lt;/h2&gt; &#xA;&lt;h3&gt;基本说明&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;请根据 &lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E6%A8%A1%E6%8B%9F%E5%99%A8%E6%94%AF%E6%8C%81.md&#34;&gt;模拟器支持情况&lt;/a&gt;，进行对应的操作。&lt;/li&gt; &#xA; &lt;li&gt;解压压缩包，到 &lt;strong&gt;没有中文或特殊符号&lt;/strong&gt; 的文件夹路径。&lt;/li&gt; &#xA; &lt;li&gt;第一次运行软件，&lt;strong&gt;请使用管理员权限&lt;/strong&gt; 打开 &lt;code&gt;MeoAsstGui.exe&lt;/code&gt;。运行过一次后，后续不再需要管理员权限。&lt;/li&gt; &#xA; &lt;li&gt;开始运行后，除 &lt;code&gt;自动关机&lt;/code&gt; 外，所有设置均不可再修改。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;目前仅对 &lt;code&gt;16:9&lt;/code&gt; 分辨率支持较好，最低支持 &lt;code&gt;1280 * 720&lt;/code&gt;，更高不限。非 &lt;code&gt;16:9&lt;/code&gt; 分辨率可能会有奇奇怪怪的问题，正在进一步适配中_(:з」∠)_&lt;/p&gt; &#xA;&lt;p&gt;更多使用说明请参考 &lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D.md&#34;&gt;详细介绍&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;软件一打开就闪退&lt;/li&gt; &#xA; &lt;li&gt;连接错误/捕获模拟器窗口错误&lt;/li&gt; &#xA; &lt;li&gt;识别错误/任务开始后一直不动、没有反应&lt;/li&gt; &#xA; &lt;li&gt;如何连接自定义端口&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;请参考 &lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.md&#34;&gt;常见问题&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;主要关联项目&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;全新 GUI: &lt;a href=&#34;https://github.com/MaaAssistantArknights/MaaAsstElectronUI&#34;&gt;MaaAsstElectronUI&lt;/a&gt; （正在开发中，欢迎加入！）&lt;/li&gt; &#xA; &lt;li&gt;更新服务器: &lt;a href=&#34;https://github.com/MaaAssistantArknights/MaaDownloadServer&#34;&gt;MaaDownloadServer&lt;/a&gt; （正在开发中，欢迎加入！）&lt;/li&gt; &#xA; &lt;li&gt;自动战斗作业服务器: &lt;a href=&#34;https://github.com/MaaAssistantArknights/MaaCopilotServer&#34;&gt;MaaCopilotServer&lt;/a&gt; （正在开发中，欢迎加入！）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;h3&gt;开源库&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;图像识别库：&lt;a href=&#34;https://github.com/opencv/opencv.git&#34;&gt;opencv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;文字识别库：&lt;a href=&#34;https://github.com/DayBreak-u/chineseocr_lite.git&#34;&gt;chineseocr_lite&lt;/a&gt;&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;文字识别库：&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;del&gt;关卡掉落识别：&lt;a href=&#34;https://github.com/penguin-statistics/recognizer&#34;&gt;企鹅物流识别&lt;/a&gt;&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;地图格子识别：&lt;a href=&#34;https://github.com/yuanyan3060/Arknights-Tile-Pos&#34;&gt;Arknights-Tile-Pos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C++ JSON库：&lt;a href=&#34;https://github.com/MistEO/meojson.git&#34;&gt;meojson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C++ 运算符解析器：&lt;a href=&#34;https://github.com/kimwalisch/calculator&#34;&gt;calculator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C++ base64编解码：&lt;a href=&#34;https://github.com/ReneNyffenegger/cpp-base64&#34;&gt;cpp-base64&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C++ 解压压缩库：&lt;a href=&#34;https://github.com/madler/zlib&#34;&gt;zlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C++ Gzip封装：&lt;a href=&#34;https://github.com/mapbox/gzip-hpp&#34;&gt;gzip-hpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WPF MVVW框架：&lt;a href=&#34;https://github.com/canton7/Stylet&#34;&gt;Stylet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WPF控件库：&lt;a href=&#34;https://github.com/HandyOrg/HandyControl&#34;&gt;HandyControl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C# JSON库: &lt;a href=&#34;https://github.com/JamesNK/Newtonsoft.Json&#34;&gt;Newtonsoft.Json&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;下载器：&lt;a href=&#34;https://github.com/aria2/aria2&#34;&gt;aria2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;数据源&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;del&gt;公开招募数据：&lt;a href=&#34;https://www.bigfun.cn/tools/aktools/hr&#34;&gt;明日方舟工具箱&lt;/a&gt;&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;干员及基建数据：&lt;a href=&#34;http://prts.wiki/&#34;&gt;PRTS明日方舟中文WIKI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;关卡数据：&lt;a href=&#34;https://penguin-stats.cn/&#34;&gt;企鹅物流数据统计&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;材料数据：&lt;a href=&#34;https://github.com/yuanyan3060/Arknights-Bot-Resource&#34;&gt;明日方舟bot常用素材&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;贡献/参与者&lt;/h3&gt; &#xA;&lt;p&gt;感谢所有参与到开发/测试中的朋友们，是大家的帮助让 MAA 越来越好！ (*´▽｀)ノノ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MaaAssistantArknights/MaaAssistantArknights/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contributors-img.web.app/image?repo=MaaAssistantArknights/MaaAssistantArknights&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;开发相关&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;直接使用 Visual Studio 2019 &lt;del&gt;或更高版本&lt;/del&gt; 打开 &lt;code&gt;MeoAssistantArknights.sln&lt;/code&gt; 即可，所有环境都是配置好的&lt;/p&gt; &#xA;&lt;h3&gt;Linux | macOS&lt;/h3&gt; &#xA;&lt;p&gt;请参考 &lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/Linux%E7%BC%96%E8%AF%91%E6%95%99%E7%A8%8B.md&#34;&gt;Linux 编译教程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/include/AsstCaller.h&#34;&gt;C 接口&lt;/a&gt;：&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/tools/TestCaller/main.cpp&#34;&gt;集成示例&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/Python/asst.py&#34;&gt;Python 接口&lt;/a&gt;：&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/Python/sample.py&#34;&gt;集成示例&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/Golang/maa/&#34;&gt;Golang 接口&lt;/a&gt;：&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/Golang/cli.go&#34;&gt;集成示例&lt;/a&gt;（已停止维护 orz）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/dart/&#34;&gt;Dart 接口&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/Java/Maaj&#34;&gt;Java 接口&lt;/a&gt;：&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/src/Java/Maaj/src/main/java/com/iguigui/maaj/MaaJavaSample.java&#34;&gt;集成示例&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E9%9B%86%E6%88%90%E6%96%87%E6%A1%A3.md&#34;&gt;集成文档&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E5%9B%9E%E8%B0%83%E6%B6%88%E6%81%AF%E5%8D%8F%E8%AE%AE.md&#34;&gt;回调消息协议&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E4%BB%BB%E5%8A%A1%E6%B5%81%E7%A8%8B%E5%8D%8F%E8%AE%AE.md&#34;&gt;任务流程协议&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E6%88%98%E6%96%97%E6%B5%81%E7%A8%8B%E5%8D%8F%E8%AE%AE.md&#34;&gt;自动抄作业协议&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;对连 Git 都不熟悉的超级萌新，请看&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MaaAssistantArknights/MaaAssistantArknights/master/docs/%E5%86%99%E7%BB%99%E8%90%8C%E6%96%B0%E7%9A%84%E5%8F%91%E7%94%B5%E5%85%A8%E6%B5%81%E7%A8%8B.md&#34;&gt;写给萌新的发电全流程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;声明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本软件 logo 并非使用 AGPL 3.0 协议开源，画师 &lt;a href=&#34;https://weibo.com/u/3251357314&#34;&gt;耗毛&lt;/a&gt; 及软件全体开发者保留所有权利。不得以 AGPL 3.0 协议已授权为由在未经授权的情况下使用本软件 logo, 不得在未经授权的情况下将本软件 logo 用于任何商业用途。&lt;/li&gt; &#xA; &lt;li&gt;本软件开源、免费，仅供学习交流使用。若您遇到商家使用本软件进行代练并收费，可能是设备与时间等费用，产生的问题及后果与本软件无关。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;广告&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://live.bilibili.com/2808861&#34;&gt;B 站直播间&lt;/a&gt;：每晚直播敲代码，近期很长一段时间应该都是在写本软件~&lt;br&gt; &lt;a href=&#34;https://jq.qq.com/?_wv=1027&amp;amp;k=ypbzXcA2&#34;&gt;技术交流 &amp;amp; 吹水群&lt;/a&gt;：内卷地狱！（明日方舟弱相关）&lt;br&gt; &lt;a href=&#34;https://jq.qq.com/?_wv=1027&amp;amp;k=1giyMpPb&#34;&gt;自动战斗 JSON 作业分享群&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://jq.qq.com/?_wv=1027&amp;amp;k=JM9oCk3C&#34;&gt;开发者群&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果觉得软件对你有帮助，帮忙点个 Star 吧！~（网页最上方右上角的小星星），这就是对我们最大的支持了！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mriscoc/Ender3V2S1</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/mriscoc/Ender3V2S1</id>
    <link href="https://github.com/mriscoc/Ender3V2S1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is optimized firmware for Ender3 V2/S1 3D printers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Professional Firmware for the Creality Ender 3 V2/S1 Printers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/mriscoc/Ender3V2S1.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release-date/mriscoc/Ender3V2S1.svg?sanitize=true&#34; alt=&#34;GitHub Release Date&#34;&gt; &lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/actions&#34;&gt;&lt;img src=&#34;https://github.com/mriscoc/Ender3V2S1/workflows/CI/badge.svg?branch=Ender3V2S1-Released&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Universal RET6/RCT6 Ender 3 V2/S1 Edition&lt;/h2&gt; &#xA;&lt;p&gt;Please test this firmware and let us know if it misbehaves in any way. Volunteers are standing by!&lt;br&gt; Precompiled binary files can be downloader from: &lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/releases/latest&#34;&gt;Latest Release&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img aling=&#34;left&#34; height=&#34;240&#34; src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/buildroot/share/pixmaps/Ender-3V2.jpg&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/buildroot/share/pixmaps/Ender-3S1.jpg&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for your support, I receive donations through &lt;a href=&#34;https://www.patreon.com/mriscoc&#34;&gt;Patreon&lt;/a&gt; and &lt;a href=&#34;https://www.paypal.com/paypalme/mriscoc&#34;&gt;Paypal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.paypal.com/donate?business=85SPAAR6UZEE8&amp;amp;currency_code=USD&#34;&gt;&lt;img src=&#34;https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Wiki&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/How-to-install-the-firmware&#34;&gt;How to install the firmware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/3D-BLTouch&#34;&gt;Installing a 3D/BLTouch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/Color-Themes&#34;&gt;Color themes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/Octoprint&#34;&gt;How to use with Octoprint&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/ender3v2s1firmware&#34;&gt;Telegram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/Ender3v2Firmware&#34;&gt;Reddit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/groups/ender3v2firmware&#34;&gt;Ender 3V2 Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/groups/ender3s1printer&#34;&gt;Ender 3S1 Facebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/screenshots/main.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This is a Marlin based firmware and is maintained by &lt;a href=&#34;https://github.com/mriscoc&#34;&gt;@mriscoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This work would not be possible without the supporters, helpers and betatesters at the &lt;strong&gt;Telegram&lt;/strong&gt; group.&lt;/p&gt; &#xA;&lt;p&gt;Marlin firmware is an Open Source project hosted on Github, &lt;a href=&#34;https://marlinfw.org/&#34;&gt;Marlin&lt;/a&gt; is owned and maintained by the maker community.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;THIS FIRMWARE AND ALL OTHER FILES IN THE DOWNLOAD ARE PROVIDED FREE OF CHARGE WITH NO WARRANTY OR GUARANTEE. SUPPORT IS NOT INCLUDED JUST BECAUSE YOU DOWNLOADED THE FIRMWARE. WE ARE NOT LIABLE FOR ANY DAMAGE TO YOUR PRINTER, PERSON, OR ANY OTHER PROPERTY DUE TO USE OF THIS FIRMWARE. IF YOU DO NOT AGREE TO THESE TERMS THEN DO NOT USE THE FIRMWARE.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;For the license, check the header of each file, if the license is not specified there, the project license will be used. Marlin is licensed under the GPL.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SerenityOS/serenity</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/SerenityOS/serenity</id>
    <link href="https://github.com/SerenityOS/serenity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Serenity Operating System 🐞&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SerenityOS&lt;/h1&gt; &#xA;&lt;p&gt;Graphical Unix-like operating system for x86 computers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SerenityOS/serenity/actions?query=workflow%3A%22Build%2C%20lint%2C%20and%20test%22&#34;&gt;&lt;img src=&#34;https://github.com/SerenityOS/serenity/workflows/Build,%20lint,%20and%20test/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_build/latest?definitionId=1&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_apis/build/status/CI?branchName=master&#34; alt=&#34;Azure DevOps Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:serenity&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/serenity.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=SerenityOS_serenity&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=SerenityOS_serenity&amp;amp;metric=ncloc&#34; alt=&#34;Sonar Cube Static Analysis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/830522505605283862.svg?logo=discord&amp;amp;logoColor=white&amp;amp;logoWidth=20&amp;amp;labelColor=7289DA&amp;amp;label=Discord&amp;amp;color=17cf48&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is a love letter to &#39;90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.&lt;/p&gt; &#xA;&lt;p&gt;Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix. This is a system by us, for us, based on the things we like.&lt;/p&gt; &#xA;&lt;p&gt;You can watch videos of the system being developed on YouTube:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/andreaskling&#34;&gt;Andreas Kling&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/linusgroh&#34;&gt;Linus Groh&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;FAQ&lt;/strong&gt;: &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshot&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Meta/Screenshots/screenshot-b36968c.png&#34; alt=&#34;Screenshot as of b36968c.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modern x86 32-bit and 64-bit kernel with pre-emptive multi-threading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Applications/Browser/&#34;&gt;Browser&lt;/a&gt; with JavaScript, WebAssembly, and more (check the spec compliance for &lt;a href=&#34;https://libjs.dev/test262/&#34;&gt;JS&lt;/a&gt;, &lt;a href=&#34;https://css.tobyase.de/&#34;&gt;CSS&lt;/a&gt;, and &lt;a href=&#34;https://libjs.dev/wasm/&#34;&gt;WASM&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Security features (hardware protections, limited userland capabilities, W^X memory, &lt;code&gt;pledge&lt;/code&gt; &amp;amp; &lt;code&gt;unveil&lt;/code&gt;, (K)ASLR, OOM-resistance, web-content isolation, state-of-the-art TLS algorithms, ...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Services/&#34;&gt;System services&lt;/a&gt; (WindowServer, LoginServer, AudioServer, WebServer, RequestServer, CrashServer, ...) and modern IPC&lt;/li&gt; &#xA; &lt;li&gt;Good POSIX compatibility (&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/LibC/&#34;&gt;LibC&lt;/a&gt;, Shell, syscalls, signals, pseudoterminals, filesystem notifications, standard Unix &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Utilities/&#34;&gt;utilities&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;POSIX-like virtual file systems (/proc, /dev, /sys, /tmp, ...) and ext2 file system&lt;/li&gt; &#xA; &lt;li&gt;Network stack and applications with support for IPv4, TCP, UDP; DNS, HTTP, Gemini, IMAP, NTP&lt;/li&gt; &#xA; &lt;li&gt;Profiling, debugging and other development tools (Kernel-supported profiling, detailed program analysis with software emulation in UserspaceEmulator, CrashReporter, interactive GUI playground, HexEditor, HackStudio IDE for C++ and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/&#34;&gt;Libraries&lt;/a&gt; for everything from cryptography to OpenGL, audio, JavaScript, GUI, playing chess, ...&lt;/li&gt; &#xA; &lt;li&gt;Support for many common and uncommon file formats (PNG, JPEG, GIF, MP3, WAV, FLAC, ZIP, TAR, PDF, QOI, Gemini, ...)&lt;/li&gt; &#xA; &lt;li&gt;Unified style and design philosophy, flexible theming system, &lt;a href=&#34;https://fonts.serenityos.net/font-family&#34;&gt;custom (bitmap and vector) fonts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Games/&#34;&gt;Games&lt;/a&gt; (Solitaire, Minesweeper, 2048, chess, Conway&#39;s Game of Life, ...) and &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Demos/&#34;&gt;demos&lt;/a&gt; (CatDog, Starfield, Eyes, mandelbrot set, WidgetGallery, ...)&lt;/li&gt; &#xA; &lt;li&gt;Every-day GUI programs and utilities (Spreadsheet with JavaScript, TextEditor, Terminal, PixelPaint, various multimedia viewers and players, Mail, Assistant, Calculator, ...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... and all of the above are right in this repository, no extra dependencies, built from-scratch by us :^)&lt;/p&gt; &#xA;&lt;p&gt;Additionally, there are &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Ports/AvailablePorts.md&#34;&gt;over two hundred ports of popular open-source software&lt;/a&gt;, including games, compilers, Unix tools, multimedia apps and more.&lt;/p&gt; &#xA;&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; &#xA;&lt;p&gt;Man pages are available online at &lt;a href=&#34;https://man.serenityos.org&#34;&gt;man.serenityos.org&lt;/a&gt;. These pages are generated from the Markdown source files in &lt;a href=&#34;https://github.com/SerenityOS/serenity/tree/master/Base/usr/share/man&#34;&gt;&lt;code&gt;Base/usr/share/man&lt;/code&gt;&lt;/a&gt; and updated automatically.&lt;/p&gt; &#xA;&lt;p&gt;When running SerenityOS you can use &lt;code&gt;man&lt;/code&gt; for the terminal interface, or &lt;code&gt;help&lt;/code&gt; for the GUI.&lt;/p&gt; &#xA;&lt;p&gt;Code-related documentation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Documentation/&#34;&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;How do I build and run this?&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/BuildInstructions.md&#34;&gt;SerenityOS build instructions&lt;/a&gt;. Serenity runs on Linux, macOS (aarch64 might be a challenge), Windows (with WSL2) and many other *Nixes with hardware or software virtualization.&lt;/p&gt; &#xA;&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; &#xA;&lt;p&gt;Join our Discord server: &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;SerenityOS Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before opening an issue, please see the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/CONTRIBUTING.md#issue-policy&#34;&gt;issue policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A general guide for contributing can be found in &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andreas Kling&lt;/strong&gt; - &lt;a href=&#34;https://twitter.com/awesomekling&#34;&gt;awesomekling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robin Burchell&lt;/strong&gt; - &lt;a href=&#34;https://github.com/rburchell&#34;&gt;rburchell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conrad Pankoff&lt;/strong&gt; - &lt;a href=&#34;https://github.com/deoxxa&#34;&gt;deoxxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sergey Bugaev&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bugaevc&#34;&gt;bugaevc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Liav A&lt;/strong&gt; - &lt;a href=&#34;https://github.com/supercomputer7&#34;&gt;supercomputer7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linus Groh&lt;/strong&gt; - &lt;a href=&#34;https://github.com/linusg&#34;&gt;linusg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ali Mohammad Pur&lt;/strong&gt; - &lt;a href=&#34;https://github.com/alimpfard&#34;&gt;alimpfard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shannon Booth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/shannonbooth&#34;&gt;shannonbooth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hüseyin ASLITÜRK&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asliturk&#34;&gt;asliturk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Matthew Olsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mattco98&#34;&gt;mattco98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nico Weber&lt;/strong&gt; - &lt;a href=&#34;https://github.com/nico&#34;&gt;nico&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brian Gianforcaro&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bgianfo&#34;&gt;bgianfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ben Wiederhake&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BenWiederhake&#34;&gt;BenWiederhake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tom&lt;/strong&gt; - &lt;a href=&#34;https://github.com/tomuta&#34;&gt;tomuta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Paul Scharnofske&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asynts&#34;&gt;asynts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Itamar Shenhar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/itamar8910&#34;&gt;itamar8910&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Luke Wilde&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Lubrsi&#34;&gt;Lubrsi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brendan Coles&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bcoles&#34;&gt;bcoles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andrew Kaster&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ADKaster&#34;&gt;ADKaster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;thankyouverycool&lt;/strong&gt; - &lt;a href=&#34;https://github.com/thankyouverycool&#34;&gt;thankyouverycool&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Idan Horowitz&lt;/strong&gt; - &lt;a href=&#34;https://github.com/IdanHo&#34;&gt;IdanHo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gunnar Beutner&lt;/strong&gt; - &lt;a href=&#34;https://github.com/gunnarbeutner&#34;&gt;gunnarbeutner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Flynn&lt;/strong&gt; - &lt;a href=&#34;https://github.com/trflynn89&#34;&gt;trflynn89&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jean-Baptiste Boric&lt;/strong&gt; - &lt;a href=&#34;https://github.com/boricj&#34;&gt;boricj&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stephan Unverwerth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sunverwerth&#34;&gt;sunverwerth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Wipfli&lt;/strong&gt; - &lt;a href=&#34;https://github.com/MaxWipfli&#34;&gt;MaxWipfli&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Daniel Bertalan&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BertalanD&#34;&gt;BertalanD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jelle Raaijmakers&lt;/strong&gt; - &lt;a href=&#34;https://github.com/GMTA&#34;&gt;GMTA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sam Atkins&lt;/strong&gt; - &lt;a href=&#34;https://github.com/AtkinsSJ&#34;&gt;AtkinsSJ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tobias Christiansen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/TobyAsE&#34;&gt;TobyAsE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lenny Maiorani&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ldm5180&#34;&gt;ldm5180&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sin-ack&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sin-ack&#34;&gt;sin-ack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jesse Buhagiar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Quaker762&#34;&gt;Quaker762&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Peter Elliott&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Petelliott&#34;&gt;Petelliott&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Karol Kosek&lt;/strong&gt; - &lt;a href=&#34;https://github.com/krkk&#34;&gt;krkk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mustafa Quraish&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mustafaquraish&#34;&gt;mustafaquraish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;David Tuin&lt;/strong&gt; - &lt;a href=&#34;https://github.com/davidot&#34;&gt;davidot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leon Albrecht&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Hendiadyoin1&#34;&gt;Hendiadyoin1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Schumacher&lt;/strong&gt; - &lt;a href=&#34;https://github.com/timschumi&#34;&gt;timschumi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Marcus Nilsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/metmo&#34;&gt;metmo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gegga Thor&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Xexxa&#34;&gt;Xexxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;kleines Filmröllchen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kleinesfilmroellchen&#34;&gt;kleinesfilmroellchen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kenneth Myhra&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kennethmyhra&#34;&gt;kennethmyhra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maciej&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sppmacd&#34;&gt;sppmacd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sahan Fernando&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ccapitalK&#34;&gt;ccapitalK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And many more! &lt;a href=&#34;https://github.com/SerenityOS/serenity/graphs/contributors&#34;&gt;See here&lt;/a&gt; for a full contributor list. The people listed above have landed more than 100 commits in the project. :^)&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is licensed under a 2-clause BSD license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/tensorflow</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/tensorflow/tensorflow</id>
    <link href="https://github.com/tensorflow/tensorflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open Source Machine Learning Framework for Everyone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/tensorflow.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.5281/zenodo.4724125&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/api-reference-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of &lt;a href=&#34;https://www.tensorflow.org/resources/tools&#34;&gt;tools&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/resources/libraries-extensions&#34;&gt;libraries&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;community&lt;/a&gt; resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google&#39;s Machine Intelligence Research organization to conduct machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow provides stable &lt;a href=&#34;https://www.tensorflow.org/api_docs/python&#34;&gt;Python&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/api_docs/cc&#34;&gt;C++&lt;/a&gt; APIs, as well as non-guaranteed backward compatible API for &lt;a href=&#34;https://www.tensorflow.org/api_docs&#34;&gt;other languages&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep up-to-date with release announcements and security updates by subscribing to &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/announce&#34;&gt;announce@tensorflow.org&lt;/a&gt;. See all the &lt;a href=&#34;https://www.tensorflow.org/community/forums&#34;&gt;mailing lists&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;TensorFlow install guide&lt;/a&gt; for the &lt;a href=&#34;https://www.tensorflow.org/install/pip&#34;&gt;pip package&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;enable GPU support&lt;/a&gt;, use a &lt;a href=&#34;https://www.tensorflow.org/install/docker&#34;&gt;Docker container&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/install/source&#34;&gt;build from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To install the current release, which includes support for &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;CUDA-enabled GPU cards&lt;/a&gt; &lt;em&gt;(Ubuntu and Windows)&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A smaller CPU-only package is also available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update TensorFlow to the latest version, add &lt;code&gt;--upgrade&lt;/code&gt; flag to the above commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Nightly binaries are available for testing using the &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly&#34;&gt;tf-nightly&lt;/a&gt; and &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly-cpu&#34;&gt;tf-nightly-cpu&lt;/a&gt; packages on PyPi.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Try your first TensorFlow program&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&#xA;&amp;gt;&amp;gt;&amp;gt; tf.add(1, 2).numpy()&#xA;3&#xA;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant(&#39;Hello, TensorFlow!&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; hello.numpy()&#xA;b&#39;Hello, TensorFlow!&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution guidelines&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want to contribute to TensorFlow, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;. This project adheres to TensorFlow&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We use &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues&#34;&gt;GitHub issues&lt;/a&gt; for tracking requests and bugs, please see &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss&#34;&gt;TensorFlow Discuss&lt;/a&gt; for general questions and discussion, and please direct specific questions to &lt;a href=&#34;https://stackoverflow.com/questions/tagged/tensorflow&#34;&gt;Stack Overflow&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The TensorFlow project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:tensorflow&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/1486&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/1486/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Continuous build status&lt;/h2&gt; &#xA;&lt;p&gt;You can find more community-supported platforms and configurations in the &lt;a href=&#34;https://github.com/tensorflow/build#community-supported-tensorflow-builds&#34;&gt;TensorFlow SIG Build community builds table&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Official Builds&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Artifacts&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux XLA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bintray.com/google/tensorflow/tensorflow/_latestVersion&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 0 and 1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 2 and 3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow MacOS CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;TensorFlow.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official&#34;&gt;TensorFlow Official Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/examples&#34;&gt;TensorFlow Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-in-practice&#34;&gt;DeepLearning.AI TensorFlow Developer Professional Certificate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-data-and-deployment&#34;&gt;TensorFlow: Data and Deployment from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/getting-started-with-tensor-flow2&#34;&gt;Getting Started with TensorFlow 2 from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-advanced-techniques&#34;&gt;TensorFlow: Advanced Techniques from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow2-deeplearning&#34;&gt;TensorFlow 2 for Deep Learning Specialization from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/introduction-tensorflow&#34;&gt;Intro to TensorFlow for A.I, M.L, and D.L from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187&#34;&gt;Intro to TensorFlow for Deep Learning from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-lite--ud190&#34;&gt;Introduction to TensorFlow Lite from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-tensorflow-gcp&#34;&gt;Machine Learning with TensorFlow on GCP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codelabs.developers.google.com/?cat=TensorFlow&#34;&gt;TensorFlow Codelabs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.tensorflow.org&#34;&gt;TensorFlow Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/resources/learn-ml&#34;&gt;Learn ML with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/tensorflow&#34;&gt;TensorFlow Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ&#34;&gt;TensorFlow YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/model_optimization/guide/roadmap&#34;&gt;TensorFlow model optimization roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/about/bib&#34;&gt;TensorFlow White Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorboard&#34;&gt;TensorBoard Visualization Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about the &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;TensorFlow community&lt;/a&gt; and how to &lt;a href=&#34;https://www.tensorflow.org/community/contribute&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>changkun/modern-cpp-tutorial</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/changkun/modern-cpp-tutorial</id>
    <link href="https://github.com/changkun/modern-cpp-tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;📚 Modern C++ Tutorial: C++11/14/17/20 On the Fly | https://changkun.de/modern-cpp/&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/assets/cover-2nd-en.png&#34; alt=&#34;logo&#34; height=&#34;550&#34; align=&#34;right&#34;&gt; &#xA;&lt;h1&gt;Modern C++ Tutorial: C++11/14/17/20 On the Fly&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/travis/changkun/modern-cpp-tutorial/master?style=flat-square&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-English-blue.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/README-zh-cn.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-red.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/assets/donate.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E2%82%AC-donate-ff69b4.svg?style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Purpose&lt;/h2&gt; &#xA;&lt;p&gt;The book claims to be &#34;On the Fly&#34;. Its intent is to provide a comprehensive introduction to the relevant features regarding modern C++ (before 2020s). Readers can choose interesting content according to the following table of content to learn and quickly familiarize the new features you would like to learn. Readers should be aware that not all of these features are required. Instead, it should be learned when you really need it.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, instead of coding-only, the book introduces the historical background of its technical requirements (as simple as possible), which provides great help in understanding why these features came out.&lt;/p&gt; &#xA;&lt;p&gt;In addition, the author would like to encourage readers to use modern C++ directly in their new projects and migrate their old projects to modern C++ gradually after reading the book.&lt;/p&gt; &#xA;&lt;h2&gt;Targets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This book assumes that readers are already familiar with traditional C++ (i.e. C++98 or earlier), or at least that they do not have any difficulty in reading traditional C++ code. In other words, those who have long experience in traditional C++ and people who desire to quickly understand the features of modern C++ in a short period of time are well suited to read the book.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This book introduces, to a certain extent, the dark magic of modern C++. However, these magic tricks are very limited, they are not suitable for readers who want to learn advanced C++. The purpose of this book is offering a quick start for modern C++. Of course, advanced readers can also use this book to review and examine themselves on modern C++.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Start&lt;/h2&gt; &#xA;&lt;p&gt;You can choose from the following reading methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/book/en-us/toc.md&#34;&gt;GitHub Online&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp/pdf/modern-cpp-tutorial-en-us.pdf&#34;&gt;PDF document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp/epub/modern-cpp-tutorial-en-us.epub&#34;&gt;EPUB document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://changkun.de/modern-cpp&#34;&gt;Website&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code&lt;/h2&gt; &#xA;&lt;p&gt;Each chapter of this book contains a lot of code. If you encounter problems while writing your own code with the introductory features of the book, reading the source code attached to the book might be of help. You can find the book &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/code&#34;&gt;here&lt;/a&gt;. All the code is organized by chapter, the folder name is the chapter number.&lt;/p&gt; &#xA;&lt;h2&gt;Exercises&lt;/h2&gt; &#xA;&lt;p&gt;There are few exercises at the end of each chapter of the book. These are meant to test whether you have mastered the knowledge in the current chapter. You can find the possible answer to the problem &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/exercises&#34;&gt;here&lt;/a&gt;. Again, the folder name is the chapter number.&lt;/p&gt; &#xA;&lt;h2&gt;Website&lt;/h2&gt; &#xA;&lt;p&gt;The source code of the &lt;a href=&#34;https://changkun.de/modern-cpp&#34;&gt;website&lt;/a&gt; of this book can be found &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/website&#34;&gt;here&lt;/a&gt;, which is built by &lt;a href=&#34;https://hexo.io&#34;&gt;hexo&lt;/a&gt; and &lt;a href=&#34;https://vuejs.org&#34;&gt;vuejs&lt;/a&gt;. The website provides you another way of reading the book, it also adapts to mobile browsers.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in building everything locally, it is recommended using &lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Docker&lt;/a&gt;. To build, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This book was originally written in Chinese by &lt;a href=&#34;https://changkun.de&#34;&gt;Changkun Ou&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The author has limited time and language skills. If readers find any mistakes in the book or any language improvements, please feel free to open an &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/issues&#34;&gt;Issue&lt;/a&gt; or start a &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/pulls&#34;&gt;Pull request&lt;/a&gt;. For detailed guidelines and checklist, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/CONTRIBUTING.md&#34;&gt;How to contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The author is grateful to all contributors, including but not limited to &lt;a href=&#34;https://github.com/changkun/modern-cpp-tutorial/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is also supported by:&lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://www.digitalocean.com/?refcode=834a3bbc951b&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=CopyPaste&#34;&gt; &lt;img src=&#34;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg?sanitize=true&#34; width=&#34;201px&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;This work was written by &lt;a href=&#34;https://changkun.de&#34;&gt;Ou Changkun&lt;/a&gt; and licensed under a &lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License&lt;/a&gt;. The code of this repository is open sourced under the &lt;a href=&#34;https://raw.githubusercontent.com/changkun/modern-cpp-tutorial/master/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/leveldb</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/google/leveldb</id>
    <link href="https://github.com/google/leveldb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;ci&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Authors: Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keys and values are arbitrary byte arrays.&lt;/li&gt; &#xA; &lt;li&gt;Data is stored sorted by key.&lt;/li&gt; &#xA; &lt;li&gt;Callers can provide a custom comparison function to override the sort order.&lt;/li&gt; &#xA; &lt;li&gt;The basic operations are &lt;code&gt;Put(key,value)&lt;/code&gt;, &lt;code&gt;Get(key)&lt;/code&gt;, &lt;code&gt;Delete(key)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiple changes can be made in one atomic batch.&lt;/li&gt; &#xA; &lt;li&gt;Users can create a transient snapshot to get a consistent view of data.&lt;/li&gt; &#xA; &lt;li&gt;Forward and backward iteration is supported over the data.&lt;/li&gt; &#xA; &lt;li&gt;Data is automatically compressed using the &lt;a href=&#34;https://google.github.io/snappy/&#34;&gt;Snappy compression library&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/raw/main/doc/index.md&#34;&gt;LevelDB library documentation&lt;/a&gt; is online and bundled with the source code.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is not a SQL database. It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.&lt;/li&gt; &#xA; &lt;li&gt;Only a single process (possibly multi-threaded) can access a particular database at a time.&lt;/li&gt; &#xA; &lt;li&gt;There is no client-server support builtin to the library. An application that needs such support will have to wrap their own server around the library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting the Source&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/google/leveldb.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;This project supports &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; out of the box.&lt;/p&gt; &#xA;&lt;h3&gt;Build for POSIX&lt;/h3&gt; &#xA;&lt;p&gt;Quick start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake -DCMAKE_BUILD_TYPE=Release .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building for Windows&lt;/h3&gt; &#xA;&lt;p&gt;First generate the Visual Studio 2017 project/solution files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -G &#34;Visual Studio 15&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default default will build for x86. For 64-bit run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmake -G &#34;Visual Studio 15 Win64&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile the Windows solution from the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;devenv /build Debug leveldb.sln&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or open leveldb.sln in Visual Studio and build from within.&lt;/p&gt; &#xA;&lt;p&gt;Please see the CMake documentation and &lt;code&gt;CMakeLists.txt&lt;/code&gt; for more advanced usage.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing to the leveldb Project&lt;/h1&gt; &#xA;&lt;p&gt;The leveldb project welcomes contributions. leveldb&#39;s primary goal is to be a reliable and fast key/value store. Changes that are in line with the features/limitations outlined above, and meet the requirements below, will be considered.&lt;/p&gt; &#xA;&lt;p&gt;Contribution requirements:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tested platforms only&lt;/strong&gt;. We &lt;em&gt;generally&lt;/em&gt; will only accept changes for platforms that are compiled and tested. This means POSIX (for Linux and macOS) or Windows. Very small changes will sometimes be accepted, but consider that more of an exception than the rule.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stable API&lt;/strong&gt;. We strive very hard to maintain a stable API. Changes that require changes for projects using leveldb &lt;em&gt;might&lt;/em&gt; be rejected without sufficient benefit to the project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tests&lt;/strong&gt;: All changes must be accompanied by a new (or changed) test, or a sufficient explanation as to why a new (or changed) test is not required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Style&lt;/strong&gt;: This project conforms to the &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google C++ Style Guide&lt;/a&gt;. To ensure your changes are properly formatted please run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clang-format -i --style=file &amp;lt;file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We are unlikely to accept contributions to the build configuration files, such as &lt;code&gt;CMakeLists.txt&lt;/code&gt;. We are focused on maintaining a build configuration that allows us to test that the project works in a few supported configurations inside Google. We are not currently interested in supporting other requirements, such as different operating systems, compilers, or build systems.&lt;/p&gt; &#xA;&lt;h2&gt;Submitting a Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;Before any pull request will be accepted the author must first sign a Contributor License Agreement (CLA) at &lt;a href=&#34;https://cla.developers.google.com/&#34;&gt;https://cla.developers.google.com/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to keep the commit timeline linear &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits&#34;&gt;squash&lt;/a&gt; your changes down to a single commit and &lt;a href=&#34;https://git-scm.com/docs/git-rebase&#34;&gt;rebase&lt;/a&gt; on google/leveldb/main. This keeps the commit timeline linear and more easily sync&#39;ed with the internal repository at Google. More information at GitHub&#39;s &lt;a href=&#34;https://help.github.com/articles/about-git-rebase/&#34;&gt;About Git rebase&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Here is a performance report (with explanations) from the run of the included db_bench program. The results are somewhat noisy, but should be enough to get a ballpark performance estimate.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use a database with a million entries. Each entry has a 16 byte key, and a 100 byte value. Values used by the benchmark compress to about half their original size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LevelDB:    version 1.1&#xA;Date:       Sun May  1 12:11:26 2011&#xA;CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz&#xA;CPUCache:   4096 KB&#xA;Keys:       16 bytes each&#xA;Values:     100 bytes each (50 bytes after compression)&#xA;Entries:    1000000&#xA;Raw Size:   110.6 MB (estimated)&#xA;File Size:  62.9 MB (estimated)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Write performance&lt;/h2&gt; &#xA;&lt;p&gt;The &#34;fill&#34; benchmarks create a brand new database, in either sequential, or random order. The &#34;fillsync&#34; benchmark flushes data from the operating system to the disk after every operation; the other write operations leave the data sitting in the operating system buffer cache for a while. The &#34;overwrite&#34; benchmark does random writes that update existing keys in the database.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fillseq      :       1.765 micros/op;   62.7 MB/s&#xA;fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)&#xA;fillrandom   :       2.460 micros/op;   45.0 MB/s&#xA;overwrite    :       2.380 micros/op;   46.5 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each &#34;op&#34; above corresponds to a write of a single key/value pair. I.e., a random write benchmark goes at approximately 400,000 writes per second.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;fillsync&#34; operation costs much less (0.3 millisecond) than a disk seek (typically 10 milliseconds). We suspect that this is because the hard disk itself is buffering the update in its memory and responding before the data has been written to the platter. This may or may not be safe based on whether or not the hard disk has enough power to save its memory in the event of a power failure.&lt;/p&gt; &#xA;&lt;h2&gt;Read performance&lt;/h2&gt; &#xA;&lt;p&gt;We list the performance of reading sequentially in both the forward and reverse direction, and also the performance of a random lookup. Note that the database created by the benchmark is quite small. Therefore the report characterizes the performance of leveldb when the working set fits in memory. The cost of reading a piece of data that is not present in the operating system buffer cache will be dominated by the one or two disk seeks needed to fetch the data from disk. Write performance will be mostly unaffected by whether or not the working set fits in memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)&#xA;readseq     :  0.476 micros/op;  232.3 MB/s&#xA;readreverse :  0.724 micros/op;  152.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LevelDB compacts its underlying storage data in the background to improve read performance. The results listed above were done immediately after a lot of random writes. The results after compactions (which are usually triggered automatically) are better.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)&#xA;readseq     :  0.423 micros/op;  261.8 MB/s&#xA;readreverse :  0.663 micros/op;  166.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the high cost of reads comes from repeated decompression of blocks read from disk. If we supply enough cache to the leveldb so it can hold the uncompressed blocks in memory, the read performance improves again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)&#xA;readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository contents&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/index.md&#34;&gt;doc/index.md&lt;/a&gt; for more explanation. See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/impl.md&#34;&gt;doc/impl.md&lt;/a&gt; for a brief overview of the implementation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in include/leveldb/*.h. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Guide to header files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/db.h&lt;/strong&gt;: Main interface to the DB: Start here.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/options.h&lt;/strong&gt;: Control over the behavior of an entire database, and also control over the behavior of individual reads and writes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/comparator.h&lt;/strong&gt;: Abstraction for user-specified comparison function. If you want just bytewise comparison of keys, you can use the default comparator, but clients can write their own comparator implementations if they want custom ordering (e.g. to handle different character encodings, etc.).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/iterator.h&lt;/strong&gt;: Interface for iterating over data. You can get an iterator from a DB object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/write_batch.h&lt;/strong&gt;: Interface for atomically applying multiple updates to a database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/slice.h&lt;/strong&gt;: A simple module for maintaining a pointer and a length into some other byte array.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/status.h&lt;/strong&gt;: Status is returned from many of the public interfaces and is used to report success and various kinds of errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/env.h&lt;/strong&gt;: Abstraction of the OS environment. A posix implementation of this interface is in util/env_posix.cc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/table.h, include/leveldb/table_builder.h&lt;/strong&gt;: Lower-level modules that most clients probably won&#39;t use directly.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>feiyangqingyun/QWidgetDemo</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/feiyangqingyun/QWidgetDemo</id>
    <link href="https://github.com/feiyangqingyun/QWidgetDemo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qt编写的一些开源的demo，预计会有100多个，一直持续更新完善，代码简洁易懂注释详细，每个都是独立项目，非常适合初学者，代码随意传播使用，拒绝打赏和捐赠，欢迎留言评论！&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;0 前言说明﻿﻿&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;下载说明：由于可执行文件比较大，如有需要请到网盘下载。&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;网店地址：&lt;a href=&#34;https://shop244026315.taobao.com/&#34;&gt;https://shop244026315.taobao.com/&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;联系方式：QQ（517216493）微信（feiyangqingyun）推荐加微信。&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;以下项目已经全部支持Qt4/5/6所有版本以及后续版本&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;监控作品体验：&lt;a href=&#34;https://pan.baidu.com/s/1d7TH_GEYl5nOecuNlWJJ7g&#34;&gt;https://pan.baidu.com/s/1d7TH_GEYl5nOecuNlWJJ7g&lt;/a&gt; 提取码：01jf&lt;/li&gt; &#xA; &lt;li&gt;其他作品体验：&lt;a href=&#34;https://pan.baidu.com/s/1ZxG-oyUKe286LPMPxOrO2A&#34;&gt;https://pan.baidu.com/s/1ZxG-oyUKe286LPMPxOrO2A&lt;/a&gt; 提取码：o05q&lt;/li&gt; &#xA; &lt;li&gt;监控系统在线文档：&lt;a href=&#34;https://feiyangqingyun.gitee.io/QWidgetDemo/video_system/&#34;&gt;https://feiyangqingyun.gitee.io/QWidgetDemo/video_system/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;大屏系统在线文档：&lt;a href=&#34;https://feiyangqingyun.gitee.io/QWidgetDemo/bigscreen/&#34;&gt;https://feiyangqingyun.gitee.io/QWidgetDemo/bigscreen/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;物联网系统在线文档：&lt;a href=&#34;https://feiyangqingyun.gitee.io/QWidgetDemo/iotsystem/&#34;&gt;https://feiyangqingyun.gitee.io/QWidgetDemo/iotsystem/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;1 特别说明&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;可以选择打开QWidgetDemo.pro一次性编译所有的，也可以到目录下打开pro编译。&lt;/li&gt; &#xA; &lt;li&gt;如果发现有些子项目没有加载请打开对应目录下的.pro仔细看里面的注释。&lt;/li&gt; &#xA; &lt;li&gt;编译好的可执行文件在源码同级目录下的bin目录。&lt;/li&gt; &#xA; &lt;li&gt;亲测Qt4.6到Qt6.2所有版本，亲测win、linux、mac、uos等系统。&lt;/li&gt; &#xA; &lt;li&gt;有少部分项目不支持部分Qt版本比如Qt6，会自动跳过。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;建议用git命令行下载，压缩包下载后重新解压文件可能会有问题，不知为何。&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;强烈推荐各位搭配Qt开发经验一起学习 &lt;a href=&#34;https://gitee.com/feiyangqingyun/qtkaifajingyan&#34;&gt;https://gitee.com/feiyangqingyun/qtkaifajingyan&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;各位有需要购买Qt商业版可以联系Qt中国商务人员微信 zzwdkxx ，支持Qt。&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2 目录说明&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;高质量项目加粗显示&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;目录&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;相关&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;battery&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;电池电量&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;devicebutton&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;设备按钮&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;devicesizetable&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;磁盘容量&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;imageswitch&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图片开关&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ipaddress&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;IP地址输入框&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;lightbutton&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;高亮按钮&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;navbutton&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;导航按钮&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;savelog&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;日志重定向输出&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;saveruntime&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;运行时间记录&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;smoothcurve&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;平滑曲线&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;control&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;控件相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zhtopy&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;汉字转拼音&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;comtool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;串口调试助手&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;nettool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;网络调试助手&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;netserver&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;网络中转服务器&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;base64helper&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;图片文字转base&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;countcode&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;代码行数统计工具&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;emailtool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;邮件发送工具&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;moneytool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;存款利息计算器&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;pngtool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图片警告去除工具&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;keytool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;秘钥生成器&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;keydemo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;秘钥测试程序&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;livetool&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;程序启动器&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;tool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;工具相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;livedemo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;程序启动示例&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频播放&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;videobox&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;视频监控布局&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频播放&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;videopanel&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;视频监控面板&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频播放&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;videowidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;视频监控控件&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频播放&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;playffmpeg&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;视频播放ffmpeg&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频播放&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;playvlc&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;视频播放vlc&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;video&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;视频播放&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;plaympv&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;视频播放mpv&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;colorwidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;颜色拾取器&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;framelesswidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;通用无边框窗体&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;gifwidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GIF录屏&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;lunarcalendarwidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;农历控件&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;maskwidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;通用遮罩层&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;movewidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;通用控件移动&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;widget&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;窗体相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;screenwidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;屏幕截图&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;flatui&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;扁平化风格&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;styledemo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;三套风格样式&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;uidemo01&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;界面美化基础示例&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;uidemo08&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;界面美化入门示例&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;uidemo09&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;九宫格主界面&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;uidemo10&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;扁平化主界面&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ui&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;界面美化&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;iconhelper&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;超级图形字体&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bgdemo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;异形窗体&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;dbpage&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;通用数据库翻页&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;echartgauge&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;echart图表js交互&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;lineeditnext&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;文本框回车自动跳转&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;mouseline&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;鼠标十字线&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ntpclient&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NTP校时&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;trayicon&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;通用托盘效果&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;multobj2slot&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;多对象共用槽&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;other&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;其他相关&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;drawrect&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;随机大量矩形&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;third&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;第三方类&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;designer&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;QtDesigner设计师(Qt4)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;third&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;第三方类&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;hotkey&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;全局热键1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;third&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;第三方类&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;shortcut&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;全局热键2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;third&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;第三方类&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;miniblink&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;浏览器miniblink内核&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;third&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;第三方类&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;qwtdemo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;无插件qwt示例&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;third&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;第三方类&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;qcustomplotdemo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;精美图表qcustomplot示例&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;netfriend&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;网友提供&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;astackwidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;动态StackWidget&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;netfriend&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;网友提供&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;imagecropper&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;头像设置工具&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;netfriend&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;网友提供&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;imageviewwindow&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;图片3D效果切换&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;netfriend&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;网友提供&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;slidepuzzlewidget&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;滑块图片验证码&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;3 效果图&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 控件相关-control&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;电池电量-battery &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/battery.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;设备按钮-devicebutton &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/devicebutton.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;磁盘容量-devicesizetable &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/devicesizetable.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;图片开关-imageswitch &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/imageswitch.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;IP地址输入框-ipaddress &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/ipaddress.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;高亮按钮-lightbutton &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/lightbutton.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;导航按钮-navbutton &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/navbutton.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;日志重定向输出-savelog &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/savelog.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;运行时间记录-saveruntime &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/saveruntime.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;平滑曲线-smoothcurve &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/smoothcurve.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;汉字转拼音-zhtopy &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/control/0snap/zhtopy.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.2 工具相关-tool&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;串口调试助手-comtool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/comtool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;网络调试助手-nettool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/nettool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;网络中转服务器-netserver &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/netserver.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/netserver2.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;图片文字转base-base64helper &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/base64helper.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;代码行数统计-countcode &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/countcode.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;邮件发送工具-emailtool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/emailtool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;存款利息计算器-moneytool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/moneytool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;图片警告去除工具-pngtool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/pngtool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;秘钥生成器-keytool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/keytool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;秘钥测试程序-keydemo &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/keydemo.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;程序启动器-livetool &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/livetool.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;程序启动示例-livedemo &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/tool/0snap/livedemo.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.3 视频播放-video&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;视频监控布局-videobox &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/video/0snap/videobox.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;视频监控面板-videopanel &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/video/0snap/videopanel.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;视频监控控件-videowidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/video/0snap/videowidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;视频播放ffmpeg-playffmpeg &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/video/0snap/playffmpeg.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;视频播放vlc-playvlc &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/video/0snap/playvlc.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;视频播放mpv-plaympv &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/video/0snap/plaympv.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.4 窗体相关-widget&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;颜色拾取器-colorwidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/colorwidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;通用无边框窗体-framelesswidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/framelesswidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;GIF录屏-gifwidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/gifwidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;农历控件-lunarcalendarwidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/lunarcalendarwidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;通用遮罩层-maskwidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/maskwidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;通用控件移动-movewidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/movewidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;屏幕截图-screenwidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/widget/0snap/screenwidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.5 界面美化-ui&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;扁平化风格-flatui &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/flatui.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;三套风格样式-styledemo &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/styledemo.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;界面美化基础示例-uidemo01 &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/uidemo01.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;界面美化入门示例-uidemo08 &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/uidemo08.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;九宫格主界面-uidemo09 &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/uidemo09.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;扁平化主界面-uidemo10 &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/uidemo10.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;超级图形字体-iconhelper &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/iconhelper1.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/ui/0snap/iconhelper2.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.6 其他相关-other&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;异形窗体-bgdemo &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/bgdemo.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;通用数据库翻页-dbpage &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/dbpage.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;echart图表js交互-echartgauge &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/echartgauge.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;文本框回车自动跳转-lineeditnext &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/lineeditnext.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;鼠标十字线-mouseline &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/mouseline.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;NTP校时-ntpclient &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/ntpclient.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;通用托盘效果-trayicon &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/trayicon.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;多对象共用槽-multobj2slot &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/multobj2slot.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;随机大量矩形-drawrect &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/other/0snap/drawrect.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.7 第三方类-third&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;QtDesigner设计师(Qt4)-designer &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/designer.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;全局热键1-hotkey &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/hotkey.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;全局热键2-shortcut &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/shortcut.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;浏览器miniblink内核-miniblink &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/miniblink.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;无插件qwt示例-qwtdemo &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qwtdemo.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;精美图表qcustomplot示例-qcustomplotdemo &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo1.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo2.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo3.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo4.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo5.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo6.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/third/0snap/qcustomplotdemo7.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3.8 网友提供-netfriend&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;动态StackWidget-astackwidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/netfriend/0snap/astackwidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;头像设置工具-imagecropper &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/netfriend/0snap/imagecropper.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;图片3D效果切换-imageviewwindow &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/netfriend/0snap/imageviewwindow.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;滑块图片验证码-slidepuzzlewidget &lt;img src=&#34;https://raw.githubusercontent.com/feiyangqingyun/QWidgetDemo/master/netfriend/0snap/sliderpuzzlewidget.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;4 学习群&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Qt技术交流群1 46679801(已满员)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Qt技术交流群2 573199610(未满员)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Qt高级学习群 951393302(未满员，推荐此群)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Qt交流大会群 853086607(已满员)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>PCSX2/pcsx2</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/PCSX2/pcsx2</id>
    <link href="https://github.com/PCSX2/pcsx2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PCSX2 - The Playstation 2 Emulator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PCSX2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%96%A5%EF%B8%8F%20Windows%20Builds/master?label=Windows%20Builds&#34; alt=&#34;Windows Build Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%90%A7%20Linux%20Builds/master?label=Linux%20Builds&#34; alt=&#34;Linux Build Status&#34;&gt; &lt;a href=&#34;https://www.codacy.com/gh/PCSX2/pcsx2/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=PCSX2/pcsx2&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/1f7c0d75fec74d6daa6adb084e5b4f71&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/TCz3t9k&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/309643527816609793?color=%235CA8FA&amp;amp;label=PCSX2%20Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCSX2 is a free and open-source PlayStation 2 (PS2) emulator. Its purpose is to emulate the PS2&#39;s hardware, using a combination of MIPS CPU &lt;a href=&#34;https://en.wikipedia.org/wiki/Interpreter_(computing)&#34;&gt;Interpreters&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_recompilation&#34;&gt;Recompilers&lt;/a&gt; and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Virtual_machine&#34;&gt;Virtual Machine&lt;/a&gt; which manages hardware states and PS2 system memory. This allows you to play PS2 games on your PC, with many additional features and benefits.&lt;/p&gt; &#xA;&lt;h2&gt;Project Details&lt;/h2&gt; &#xA;&lt;p&gt;The PCSX2 project has been running for more than ten years. Past versions could only run a few public domain game demos, but newer versions can run most games at full speed, including popular titles such as Final Fantasy X and Devil May Cry 3. Visit the &lt;a href=&#34;https://pcsx2.net/compat/&#34;&gt;PCSX2 compatibility list&lt;/a&gt; to check the latest compatibility status of games (with more than 2500 titles tested), or ask for help in the &lt;a href=&#34;https://forums.pcsx2.net/&#34;&gt;official forums&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The latest officially released stable version is version 1.6.0.&lt;/p&gt; &#xA;&lt;p&gt;Installers and binaries for both stable and development builds are available from &lt;a href=&#34;https://pcsx2.net/downloads/&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Minimum&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 8.1 or newer (64 bit) &lt;br&gt; - Ubuntu 18.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports SSE4.1 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 1600 &lt;br&gt; - Two physical cores, with hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D10 support &lt;br&gt; - OpenGL 3.x support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 3000 (GeForce GTX 750) &lt;br&gt; - 2 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;4 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended Single Thread Performance is based on moderately complex games. Games that pushed the PS2 hardware to its limits will struggle on CPUs at this level. Some release titles and 2D games which underutilized the PS2 hardware may run on CPUs rated as low as 1200. A quick reference for CPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:CPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;, &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-The-Most-CPU-Intensive-Games&#34;&gt;Forum&lt;/a&gt; and CPU &lt;strong&gt;light&lt;/strong&gt; games: &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-Games-that-don-t-need-a-strong-CPU-to-emulate&#34;&gt;Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommended&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 10 (64 bit) &lt;br&gt; - Ubuntu 19.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports AVX2 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 2100 &lt;br&gt; - Four physical cores, with or without hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D11 support &lt;br&gt; - OpenGL 4.6 support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 6000 (GeForce GTX 1050 Ti) &lt;br&gt; - 4 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended GPU is based on 3x Internal, ~1080p resolution requirements. Higher resolutions will require stronger cards; 6x Internal, ~4K resolution will require a &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 12000 (GeForce GTX 1070 Ti). Just like CPU requirements, this is also highly game dependent. A quick reference for GPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:GPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Technical Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need the &lt;a href=&#34;https://support.microsoft.com/en-us/help/2977003/&#34;&gt;Visual C++ 2019 x86 Redistributables&lt;/a&gt; to run PCSX2.&lt;/li&gt; &#xA; &lt;li&gt;Windows XP and Direct3D9 support was dropped after stable release 1.4.0.&lt;/li&gt; &#xA; &lt;li&gt;Windows 7 and Windows 8.0 support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;32 bit support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to update your operating system and drivers to ensure you have the best experience possible. Having a newer GPU is also recommended so you have the latest supported drivers.&lt;/li&gt; &#xA; &lt;li&gt;Because of copyright issues, and the complexity of trying to work around it, you need a BIOS dump extracted from a legitimately-owned PS2 console to use the emulator. For more information about the BIOS and how to get it from your console, visit &lt;a href=&#34;https://raw.githubusercontent.com/PCSX2/pcsx2/master/pcsx2/Docs/PCSX2_FAQ.md#question-13-where-do-i-get-a-ps2-bios&#34;&gt;this page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PCSX2 uses two CPU cores for emulation by default. A third core can be used via the MTVU speed hack, which is compatible with most games. This can be a significant speedup on CPUs with 3+ cores, but it may be a slowdown on GS-limited games (or on CPUs with fewer than 2 cores). Software renderers will then additionally use however many rendering threads it is set to and will need higher core counts to run efficiently.&lt;/li&gt; &#xA; &lt;li&gt;Requirements benchmarks are based on a statistic from the Passmark CPU bench marking software. When we say &#34;STR&#34;, we are referring to Passmark&#39;s &#34;Single Thread Rating&#34; statistic. You can look up your CPU on &lt;a href=&#34;https://cpubenchmark.net&#34;&gt;Passmark&#39;s website for CPUs&lt;/a&gt; to see how it compares to PCSX2&#39;s requirements.&lt;/li&gt; &#xA; &lt;li&gt;Vulkan requires an up-to-date GPU driver; old drivers may cause graphical problems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want more? &lt;a href=&#34;https://pcsx2.net/&#34;&gt;Check out the PCSX2 website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleSpeech</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/PaddlePaddle/PaddleSpeech</id>
    <link href="https://github.com/PaddlePaddle/PaddleSpeech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use Speech Toolkit including SOTA/Streaming ASR with punctuation, influential TTS with text frontend, Speaker Verification System and End-to-End Speech Simultaneous Translation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/README_cn.md&#34;&gt;简体中文&lt;/a&gt;|English)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/PaddleSpeech_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;support os&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleSpeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/paddlespeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt; Quick Start &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt; Quick Start Server &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt; Quick Start Streaming Server&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#documents&#34;&gt; Documents &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt; Models List &lt;/a&gt; | &lt;a href=&#34;https://aistudio.baidu.com/aistudio/education/group/info/25130&#34;&gt; AIStudio Courses &lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2205.12007&#34;&gt; Paper &lt;/a&gt; | &lt;a href=&#34;https://gitee.com/paddlepaddle/PaddleSpeech&#34;&gt; Gitee &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleSpeech&lt;/strong&gt; is an open-source toolkit on &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt; platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models.&lt;/p&gt; &#xA;&lt;h5&gt;Speech Recognition&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Recognition Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;I knocked at the door on the ancient side of the building.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;我认为跑步最重要的就是给我带来了身体健康。&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Speech Translation (English to Chinese)&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Translations Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;我 在 这栋 建筑 的 古老 门上 敲门。&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Text-to-Speech&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Input Text&lt;/th&gt; &#xA;    &lt;th&gt;Synthetic Audio&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Life was like a box of chocolates, you never know what you&#39;re gonna get.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;早上好，今天是2020/10/29，最低温度是-3°C。&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;季姬寂，集鸡，鸡即棘鸡。棘鸡饥叽，季姬及箕稷济鸡。鸡既济，跻姬笈，季姬忌，急咭鸡，鸡急，继圾几，季姬急，即籍箕击鸡，箕疾击几伎，伎即齑，鸡叽集几基，季姬急极屐击鸡，鸡既殛，季姬激，即记《季姬击鸡记》。&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For more synthesized audios, please refer to &lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;PaddleSpeech Text-to-Speech samples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Punctuation Restoration&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Input Text &lt;/th&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Output Text &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;今天的天气真不错啊你下午有空吗我想约你一起去吃饭&lt;/td&gt; &#xA;    &lt;td&gt;今天的天气真不错啊！你下午有空吗？我想约你一起去吃饭。&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Via the easy-to-use, efficient, flexible and scalable implementation, our vision is to empower both industrial application and academic research, including training, inference &amp;amp; testing modules, and deployment process. To be more specific, this toolkit features at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📦 &lt;strong&gt;Ease of Use&lt;/strong&gt;: low barriers to install, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt;CLI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt;Server&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt;Streaming Server&lt;/a&gt; is available to quick-start your journey.&lt;/li&gt; &#xA; &lt;li&gt;🏆 &lt;strong&gt;Align to the State-of-the-Art&lt;/strong&gt;: we provide high-speed and ultra-lightweight models, and also cutting-edge technology.&lt;/li&gt; &#xA; &lt;li&gt;🏆 &lt;strong&gt;Streaming ASR and TTS System&lt;/strong&gt;: we provide production ready streaming asr and streaming tts system.&lt;/li&gt; &#xA; &lt;li&gt;💯 &lt;strong&gt;Rule-based Chinese frontend&lt;/strong&gt;: our frontend contains Text Normalization and Grapheme-to-Phoneme (G2P, including Polyphone and Tone Sandhi). Moreover, we use self-defined linguistic rules to adapt Chinese context.&lt;/li&gt; &#xA; &lt;li&gt;📦 &lt;strong&gt;Varieties of Functions that Vitalize both Industrial and Academia&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;🛎️ &lt;em&gt;Implementation of critical audio tasks&lt;/em&gt;: this toolkit contains audio functions like Automatic Speech Recognition, Text-to-Speech Synthesis, Speaker Verfication, KeyWord Spotting, Audio Classification, and Speech Translation, etc.&lt;/li&gt; &#xA;   &lt;li&gt;🔬 &lt;em&gt;Integration of mainstream models and datasets&lt;/em&gt;: the toolkit implements modules that participate in the whole pipeline of the speech tasks, and uses mainstream datasets like LibriSpeech, LJSpeech, AIShell, CSMSC, etc. See also &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt;model list&lt;/a&gt; for more details.&lt;/li&gt; &#xA;   &lt;li&gt;🧩 &lt;em&gt;Cascaded models application&lt;/em&gt;: as an extension of the typical traditional audio tasks, we combine the workflows of the aforementioned tasks with other fields like Natural language processing (NLP) and Computer Vision (CV).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recent Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;👑 2022.05.13: Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/PPASR.md&#34;&gt;PP-ASR&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/PPTTS.md&#34;&gt;PP-TTS&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/vpr/PPVPR.md&#34;&gt;PP-VPR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.05.06: &lt;code&gt;Streaming ASR&lt;/code&gt; with &lt;code&gt;Punctuation Restoration&lt;/code&gt; and &lt;code&gt;Token Timestamp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.05.06: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;, and &lt;code&gt;Punctuation Restoration&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.04.28: &lt;code&gt;Streaming Server&lt;/code&gt; is available for &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.03.28: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2022.03.28: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🤗 2021.12.14: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS&lt;/a&gt; Demos on Hugging Face Spaces are available!&lt;/li&gt; &#xA; &lt;li&gt;👏🏻 2021.12.10: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt;, &lt;code&gt;Speech Translation (English to Chinese)&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scan the QR code below with your Wechat, you can access to official technical exchange group and get the bonus ( more than 20GB learning materials, such as papers, codes and videos ) and the live link of the lessons. Look forward to your participation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/23690325/169763015-cbd8e28d-602c-4723-810d-dbc6da49441e.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We strongly recommend our users to install PaddleSpeech in &lt;strong&gt;Linux&lt;/strong&gt; with &lt;em&gt;python&amp;gt;=3.7&lt;/em&gt;. Up to now, &lt;strong&gt;Linux&lt;/strong&gt; supports CLI for the all our tasks, &lt;strong&gt;Mac OSX&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; only supports PaddleSpeech CLI for Audio Classification, Speech-to-Text and Text-to-Speech. To install &lt;code&gt;PaddleSpeech&lt;/code&gt;, please see &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our models with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/cli/README.md&#34;&gt;PaddleSpeech Command Line&lt;/a&gt;. Change &lt;code&gt;--input&lt;/code&gt; to test your own audio/text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech cls --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech vector --task spk --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Speech Recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech asr --lang zh --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Automatic Speech Recognition is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Translation&lt;/strong&gt; (English to Chinese) (not support for Mac and Windows now)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech st --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech tts --input &#34;你好，欢迎使用飞桨深度学习框架！&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Text to Speech is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Postprocessing&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Punctuation Restoration &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;paddlespeech text --task punc --input 今天的天气真不错啊你下午有空吗我想约你一起去吃饭&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Batch Process&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo -e &#34;1 欢迎光临。\n2 谢谢惠顾。&#34; | paddlespeech tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shell Pipeline&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ASR + Punctuation Restoration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech asr --input ./zh.wav | paddlespeech text --task punc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos&#34;&gt;demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try more functions like training and tuning, please have a look at &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Speech-to-Text Quick Start&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our speech server with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/server/README.md&#34;&gt;PaddleSpeech Server Command Line&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_server start --config_file ./paddlespeech/server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input &#34;您好，欢迎使用百度飞桨语音合成服务。&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Audio Classification Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about server command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server&#34;&gt;speech server demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartstreamingserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Streaming Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt; server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Speech Recognition Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Text to Speech Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input &#34;您好，欢迎使用百度飞桨语音合成服务。&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information please see: &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;ModelList&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech supports a series of most popular models. They are summarized in &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;released models&lt;/a&gt; and attached with available pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeechToText&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt; contains &lt;em&gt;Acoustic Model&lt;/em&gt;, &lt;em&gt;Language Model&lt;/em&gt;, and &lt;em&gt;Speech Translation&lt;/em&gt;, with the following details:&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Speech-to-Text Module Type&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Speech Recogination&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Aishell&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeech2 RNN + Conv based Models&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr0&#34;&gt;deepspeech2-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr1&#34;&gt;u2.transformer.conformer-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Librispeech&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr0&#34;&gt;deepspeech2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr1&#34;&gt;transformer.conformer.u2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr2&#34;&gt;transformer.conformer.u2-kaldi-librispeech&lt;/a&gt; &lt;/td&gt;  &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TIMIT&lt;/td&gt; &#xA;   &lt;td&gt;Unified Streaming &amp;amp; Non-streaming Two-pass&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/timit/asr1&#34;&gt; u2-timit&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment&lt;/td&gt; &#xA;   &lt;td&gt;THCHS30&lt;/td&gt; &#xA;   &lt;td&gt;MFA&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/.examples/thchs30/align0&#34;&gt;mfa-thchs30&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Language Model&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Ngram Language Model&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ngram_lm&#34;&gt;kenlm&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Speech Translation (English to Chinese)&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;TED En-Zh&lt;/td&gt; &#xA;   &lt;td&gt;Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st0&#34;&gt;transformer-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FAT + Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st1&#34;&gt;fat-st-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;TextToSpeech&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; in PaddleSpeech mainly contains three modules: &lt;em&gt;Text Frontend&lt;/em&gt;, &lt;em&gt;Acoustic Model&lt;/em&gt; and &lt;em&gt;Vocoder&lt;/em&gt;. Acoustic Model and Vocoder models are listed as follow:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Text-to-Speech Module Type &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Text Frontend &lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;   &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/tn&#34;&gt;tn&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/g2p&#34;&gt;g2p&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Acoustic Model&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts0&#34;&gt;tacotron2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts0&#34;&gt;tacotron2-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer TTS&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts1&#34;&gt;transformer-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeedySpeech&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts2&#34;&gt;speedyspeech-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts3&#34;&gt;fastspeech2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/tts3&#34;&gt;fastspeech2-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts3&#34;&gt;fastspeech2-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/tts3&#34;&gt;fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;6&#34;&gt;Vocoder&lt;/td&gt; &#xA;   &lt;td&gt;WaveFlow&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc0&#34;&gt;waveflow-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parallel WaveGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc1&#34;&gt;PWGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc1&#34;&gt;PWGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc1&#34;&gt;PWGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc1&#34;&gt;PWGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi Band MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc3&#34;&gt;Multi Band MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Style MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc4&#34;&gt;Style MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiFiGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc5&#34;&gt;HiFiGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc5&#34;&gt;HiFiGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc5&#34;&gt;HiFiGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc5&#34;&gt;HiFiGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WaveRNN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc6&#34;&gt;WaveRNN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Voice Cloning&lt;/td&gt; &#xA;   &lt;td&gt;GE2E&lt;/td&gt; &#xA;   &lt;td&gt;Librispeech, etc.&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ge2e&#34;&gt;ge2e&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc0&#34;&gt;ge2e-tacotron2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc1&#34;&gt;ge2e-fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;AudioClassification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio Classification&lt;/td&gt; &#xA;   &lt;td&gt;ESC-50&lt;/td&gt; &#xA;   &lt;td&gt;PANN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/esc50/cls0&#34;&gt;pann-esc50&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeakerVerification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;VoxCeleb12&lt;/td&gt; &#xA;   &lt;td&gt;ECAPA-TDNN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/voxceleb/sv0&#34;&gt;ecapa-tdnn-voxceleb12&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;PunctuationRestoration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Punctuation Restoration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Punctuation Restoration&lt;/td&gt; &#xA;   &lt;td&gt;IWLST2012_zh&lt;/td&gt; &#xA;   &lt;td&gt;Ernie Linear&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/iwslt2012/punc0&#34;&gt;iwslt2012-punc0&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;p&gt;Normally, &lt;a href=&#34;https://paperswithcode.com/area/speech&#34;&gt;Speech SoTA&lt;/a&gt;, &lt;a href=&#34;https://paperswithcode.com/area/audio&#34;&gt;Audio SoTA&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/area/music&#34;&gt;Music SoTA&lt;/a&gt; give you an overview of the hot academic topics in the related area. To focus on the tasks in PaddleSpeech, you will find the following guidelines are helpful to grasp the core ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/README.md&#34;&gt;Some Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Automatic Speech Recognition&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/data_preparation.md&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/ngram_lm.md&#34;&gt;Ngram LM&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/advanced_usage.md&#34;&gt;Advanced Usage&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/zh_text_frontend.md&#34;&gt;Chinese Rule Based Text Frontend&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;Test Audio Samples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Speaker Verification &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_searching/README.md&#34;&gt;Audio Searching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speaker_verification/README.md&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_tagging/README.md&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_translation/README.md&#34;&gt;Speech Translation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_server/README.md&#34;&gt;Speech Server&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;Released Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeechToText&#34;&gt;Speech-to-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#TextToSpeech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#AudioClassification&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeakerVerification&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#PunctuationRestoration&#34;&gt;Punctuation Restoration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#contribution&#34;&gt;Welcome to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Text-to-Speech module is originally called &lt;a href=&#34;https://github.com/PaddlePaddle/Parakeet&#34;&gt;Parakeet&lt;/a&gt;, and now merged with this repository. If you are interested in academic research about this task, please see &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview&#34;&gt;TTS research overview&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/raw/develop/docs/source/tts/models_introduction.md&#34;&gt;this document&lt;/a&gt; is a good guideline for the pipeline components.&lt;/p&gt; &#xA;&lt;h2&gt;⭐ Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt;: Use PaddleSpeech TTS to generate virtual human voice.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web&#34;&gt;&lt;img src=&#34;https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/demo_video.html&#34;&gt;PaddleSpeech Demo Video&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt;: Use PaddleSpeech TTS and ASR to clone voice from videos.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jerryuhoo/VTuberTalk/main/gui/gui.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;To cite PaddleSpeech for research, please use the following format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{zhang2022paddlespeech,&#xA;    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},&#xA;    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},&#xA;    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},&#xA;    year = {2022},&#xA;    publisher = {Association for Computational Linguistics},&#xA;}&#xA;&#xA;@inproceedings{zheng2021fused,&#xA;  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},&#xA;  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},&#xA;  booktitle={International Conference on Machine Learning},&#xA;  pages={12736--12746},&#xA;  year={2021},&#xA;  organization={PMLR}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;contribution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to PaddleSpeech&lt;/h2&gt; &#xA;&lt;p&gt;You are warmly welcome to submit questions in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/discussions&#34;&gt;discussions&lt;/a&gt; and bug reports in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;issues&lt;/a&gt;! Also, we highly appreciate if you are willing to contribute to this project!&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zh794390558&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3038472?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackwaterveg&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/87408988?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yt605155624&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24568452?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kuke&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3064195?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinghai-sun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7038341?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pkuyym&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5782283?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KPatr1ck&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22954146?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LittleChenCc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10339970?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/745165806&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/20623194?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Mingxue-Xu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/92848346?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chrisxu2016&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18379485?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfchener&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6771821?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luotao1&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6836917?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wanghaoshuang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7534971?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gongel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24390500?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mmglove&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/38800877?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/iclementine&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/16222986?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ZeyuChen&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1371212?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81195143?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qingqing01&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7845005?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ericxk&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/4719594?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kvinwang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6442159?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jiqiren11&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/82639260?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AshishKarel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/58069375?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chesterkuo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6285069?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tensor-tang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/21351065?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hysunflower&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/52739577?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wwhu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6081200?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lispc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/2833376?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24245709?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harisankarh&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1307053?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackiexiao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18050469?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/limpidezza&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/71760778?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/yeyupiaoling&#34;&gt;yeyupiaoling&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PPASR&#34;&gt;PPASR&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech&#34;&gt;PaddlePaddle-DeepSpeech&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle&#34;&gt;VoiceprintRecognition-PaddlePaddle&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle&#34;&gt;AudioClassification-PaddlePaddle&lt;/a&gt; for years of attention, constructive advice and great help.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/mymagicpower&#34;&gt;mymagicpower&lt;/a&gt; for the Java implementation of ASR upon &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk&#34;&gt;short&lt;/a&gt; and &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk&#34;&gt;long&lt;/a&gt; audio files.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/JiehangXie&#34;&gt;JiehangXie&lt;/a&gt;/&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt; for developing Virtual Uploader(VUP)/Virtual YouTuber(VTuber) with PaddleSpeech TTS function.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;745165806&lt;/a&gt;/&lt;a href=&#34;https://github.com/745165806/PaddleSpeechTask&#34;&gt;PaddleSpeechTask&lt;/a&gt; for contributing Punctuation Restoration model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;kslz&lt;/a&gt; for supplementary Chinese documents.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/awmmmm&#34;&gt;awmmmm&lt;/a&gt; for contributing fastspeech2 aishell3 conformer pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/phecda-xu&#34;&gt;phecda-xu&lt;/a&gt;/&lt;a href=&#34;https://github.com/phecda-xu/PaddleDubbing&#34;&gt;PaddleDubbing&lt;/a&gt; for developing a dubbing tool with GUI based on PaddleSpeech TTS model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;jerryuhoo&lt;/a&gt;/&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt; for developing a GUI tool based on PaddleSpeech TTS and code for making datasets from videos based on PaddleSpeech ASR.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Besides, PaddleSpeech depends on a lot of open source repositories. See &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/reference.md&#34;&gt;references&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mamedev/mame</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/mamedev/mame</id>
    <link href="https://github.com/mamedev/mame" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MAME&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;MAME&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/mamedev/mame?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/mamedev/mame&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build status:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS/Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/GCC and clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Linux)/badge.svg?sanitize=true&#34; alt=&#34;CI (Linux)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows/MinGW GCC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Windows)/badge.svg?sanitize=true&#34; alt=&#34;CI (Windows)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS/clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(macOS)/badge.svg?sanitize=true&#34; alt=&#34;CI (macOS)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI Translations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Compile%20UI%20translations/badge.svg?sanitize=true&#34; alt=&#34;Compile UI translations&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Build%20documentation/badge.svg?sanitize=true&#34; alt=&#34;Build documentation&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Static analysis status for entire build (except for third-party parts of project):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scan.coverity.com/projects/mame-emulator&#34;&gt;&lt;img src=&#34;https://scan.coverity.com/projects/5727/badge.svg?flat=1&#34; alt=&#34;Coverity Scan Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is MAME?&lt;/h1&gt; &#xA;&lt;p&gt;MAME is a multi-purpose emulation framework.&lt;/p&gt; &#xA;&lt;p&gt;MAME&#39;s purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important &#34;vintage&#34; software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.&lt;/p&gt; &#xA;&lt;h1&gt;How to compile?&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re on a UNIX-like system (including Linux and macOS), it could be as easy as typing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MAME build,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=arcade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for an arcade-only build, or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=mess&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MESS build.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;http://docs.mamedev.org/initialsetup/compilingmame.html&#34;&gt;Compiling MAME&lt;/a&gt; page on our documentation site for more information, including prerequisites for macOS and popular Linux distributions.&lt;/p&gt; &#xA;&lt;p&gt;For recent versions of macOS you need to install &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; including command-line tools and &lt;a href=&#34;https://www.libsdl.org/download-2.0.php&#34;&gt;SDL 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Windows users, we provide a ready-made &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64.&lt;/p&gt; &#xA;&lt;p&gt;Visual Studio builds are also possible, but you still need &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64. In order to generate solution and project files just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or use this command to build it directly using msbuild&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019 MSBUILD=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Where can I find out more?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mamedev.org/&#34;&gt;Official MAME Development Team Site&lt;/a&gt; (includes binary downloads, wiki, forums, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mess.redump.net/&#34;&gt;Official MESS Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mametesters.org/&#34;&gt;MAME Testers&lt;/a&gt; (official bug tracker for MAME and MESS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Coding standard&lt;/h2&gt; &#xA;&lt;p&gt;MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.&lt;/p&gt; &#xA;&lt;p&gt;Some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#Allman_style&#34;&gt;Allman style&lt;/a&gt;; some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#K.26R_style&#34;&gt;K&amp;amp;R style&lt;/a&gt; -- mostly depending on who wrote the original version. &lt;strong&gt;Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source.&lt;/strong&gt; For new code, the majority tends to prefer Allman style, so if you don&#39;t care much, use that.&lt;/p&gt; &#xA;&lt;p&gt;All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD-3-Clause&lt;/a&gt; license, the &lt;a href=&#34;http://opensource.org/licenses/LGPL-2.1&#34;&gt;LGPL-2.1&lt;/a&gt;, or the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GPL-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The MAME project as a whole is made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GNU General Public License, version 2&lt;/a&gt; or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of the source files (over 90% including core files) are made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;3-clause BSD License&lt;/a&gt;, and we would encourage new contributors to make their contributions available under the terms of this license.&lt;/p&gt; &#xA;&lt;p&gt;Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the &#34;MAME&#34; name, logo, or wordmark.&lt;/p&gt; &#xA;&lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34; target=&#34;_blank&#34;&gt; &lt;img align=&#34;right&#34; src=&#34;http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (C) 1997-2021  MAMEDev and contributors&#xA;&#xA;This program is free software; you can redistribute it and/or modify it&#xA;under the terms of the GNU General Public License version 2, as provided in&#xA;docs/legal/GPL-2.0.&#xA;&#xA;This program is distributed in the hope that it will be useful, but WITHOUT&#xA;ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or&#xA;FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for&#xA;more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see COPYING for more details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Akebi-Group/Akebi-GC</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/Akebi-Group/Akebi-GC</id>
    <link href="https://github.com/Akebi-Group/Akebi-GC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The great software for some game that exploiting anime girls (and boys).&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Akebi GC&lt;/h1&gt; The great software for some game that exploiting anime girls (and boys). &#xA;&lt;hr&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h3&gt;Building from source&lt;/h3&gt; &#xA;&lt;p&gt;It is reccomended to use &lt;a href=&#34;https://visualstudio.microsoft.com/&#34;&gt;Visual Studio 2022.&lt;/a&gt; As well as setting up &lt;strong&gt;&lt;code&gt;cheat-library&lt;/code&gt;&lt;/strong&gt; as startup project. &lt;strong&gt;The following is a recommended procedure, but others may be used.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone repository with &lt;code&gt;git clone --recurse-submodules https://github.com/Akebi-Group/Akebi-GC.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open &lt;code&gt;Akebi-GC/akebi-gc.sln&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build solution &lt;code&gt;akebi-gc.sln&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Release&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Head over to the releases page&lt;/li&gt; &#xA; &lt;li&gt;Download the latest binaries&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;(1-2 are optional if you didn&#39;t build from source)&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;code&gt;/bin&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open Compiled version (debug, release)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Insure that &lt;code&gt;CLibrary.dll&lt;/code&gt; is in the same folder that &lt;code&gt;injector.exe&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;injector.exe&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Features&lt;/h1&gt; &#xA;&lt;h4&gt;General&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Protection Bypass&lt;/li&gt; &#xA; &lt;li&gt;In-Game GUI&lt;/li&gt; &#xA; &lt;li&gt;Hotkeys&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Player&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;God Mode&lt;/li&gt; &#xA; &lt;li&gt;Unlimited Stamina&lt;/li&gt; &#xA; &lt;li&gt;Dumb Enemies (Enemies don&#39;t attack)&lt;/li&gt; &#xA; &lt;li&gt;Player&lt;/li&gt; &#xA; &lt;li&gt;Multiply Attacks&lt;/li&gt; &#xA; &lt;li&gt;No Cooldown Skill/Ultimate&lt;/li&gt; &#xA; &lt;li&gt;No Cooldown Sprint&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;World&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto Loot&lt;/li&gt; &#xA; &lt;li&gt;Auto Talk&lt;/li&gt; &#xA; &lt;li&gt;Killaura&lt;/li&gt; &#xA; &lt;li&gt;Auto Tree Farm&lt;/li&gt; &#xA; &lt;li&gt;Mob Vacuum&lt;/li&gt; &#xA; &lt;li&gt;Auto Fish&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Teleport&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chest/Oculi Teleport (Teleports to nearest)&lt;/li&gt; &#xA; &lt;li&gt;Map Teleport (Teleport to mark on map)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Visuals&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ESP&lt;/li&gt; &#xA; &lt;li&gt;Interactive Map&lt;/li&gt; &#xA; &lt;li&gt;Elemental Sight&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Debugging&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Entity List&lt;/li&gt; &#xA; &lt;li&gt;Position Info&lt;/li&gt; &#xA; &lt;li&gt;FPS Graph&lt;/li&gt; &#xA; &lt;li&gt;Packet Sniffer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Demo&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Map Teleportation&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/map-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Noclip&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/noclip-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TP to Oculi&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/oculi-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TP to Chests&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/chest-teleport-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Rapid Fire&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/rapid-fire-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Auto Talk&lt;/summary&gt; &#xA; &lt;img src=&#34;https://github.com/CallowBlack/gif-demos/raw/main/genshin-cheat/auto-talk-demo.gif&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cutscene Skipping&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create database for chests, oculi, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Adding a feature&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the Project&lt;/li&gt; &#xA; &lt;li&gt;Create your Feature Branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Commit your Changes (&lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the Branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Open a Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Suggestions&lt;/h2&gt; &#xA;&lt;p&gt;Open an issue with the title of the suggesstion you want to make. In the description, make sure it is descriptive enough so our devs can understand what you want and how you want it.&lt;/p&gt; &#xA;&lt;h2&gt;Bugs&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the short explanation for bug reporting, as well as the bug report template.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Find a bug and write down what happened, as well as your first thoughts on what you think caused it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Try to reproduce the bug. For this you need to understand what actually happened, leading up to the bug and when the actual bug happened. To make sure you get all this information correctly taking various forms of documentations, such as video, screenshots etc is essential. These steps makes it a lot easier to try and figure out what actually happened. Try to replicate the scenario where the bug appeared, as close to the original as possible. What we would recommend for this step is using the bug reporting template which can be found on page 2 and simply adding the information you have / find in there.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;can it be reproduced? Yes or no. If yes: Explain in as much detail as possible what happens when the bug occurs and why it occurs. Try and explain it as cleanly and as concise as possible to make sure that the coders don’t have to read an essay to understand what could be a simple bug with a simple fix. For this, remember that information is very subjective so it is much better to over communicate than to risk confusion. If no: Try to provide as much information about the bug as possible, so that the testers will be able to replicate the scenario in which the bug occurred more easily so we can try to reproduce the bug.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Tell us which version you are using. Otherwise we would be getting bug reports on the same issue, that has been infact fixed in the latest commits. copy the SHA / Version Number of the latest commit when you built the mod. For example: &lt;code&gt;bd17a00ec388f3b93624280cde9e1c66e740edf9&lt;/code&gt; / Release 0.7&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Notes: Please remember to always record your testing sessions on your local hard drive and then upload them unlisted to youtube to conserve memory space on your computer and to give us easy access to your replays. This is to ensure that the optimal amount of documentation is available for the bug testers and coders to use as a guideline for either replicating scenarios, reproducing bugs or fixing them.&lt;/p&gt; &#xA;&lt;p&gt;TL:DR Record all your stuff while playing the mod and report any bugs to the issues section of this repository.&lt;/p&gt; &#xA;&lt;h3&gt;Bug reporting template&lt;/h3&gt; &#xA;&lt;p&gt;Title: e.g. “Instantly kill enemy with Shackles“ Description: “Game crashed if x, y, z“&lt;/p&gt; &#xA;&lt;p&gt;-- Footer -- Date Occured: 5 / 3 / 2022 Is it reproducible: Yes / Occasionally / No Latest Commit used: &lt;code&gt;bd17a00ec388f3b93624280cde9e1c66e740edf9&lt;/code&gt; Release Version: 0.7&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/MMKV</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/Tencent/MMKV</id>
    <link href="https://github.com/Tencent/MMKV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An efficient, small mobile key-value storage framework developed by WeChat. Works on Android, iOS, macOS, Windows, and POSIX.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/MMKV/raw/master/LICENSE.TXT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD_3-brightgreen.svg?style=flat&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/release-1.2.13-brightgreen.svg?sanitize=true&#34; alt=&#34;Release Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Platform-%20Android%20%7C%20iOS%2FmacOS%20%7C%20Win32%20%7C%20POSIX-brightgreen.svg?sanitize=true&#34; alt=&#34;Platform&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;中文版本请参看&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/README_CN.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MMKV is an &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;small&lt;/strong&gt;, &lt;strong&gt;easy-to-use&lt;/strong&gt; mobile key-value storage framework used in the WeChat application. It&#39;s currently available on &lt;strong&gt;Android&lt;/strong&gt;, &lt;strong&gt;iOS/macOS&lt;/strong&gt;, &lt;strong&gt;Win32&lt;/strong&gt; and &lt;strong&gt;POSIX&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for Android&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of Android to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;apply&lt;/code&gt; calls needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 50K in binary size&lt;/strong&gt;: MMKV adds about 50K per architecture on App size, and much less when zipped (APK).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via Maven&lt;/h3&gt; &#xA;&lt;p&gt;Add the following lines to &lt;code&gt;build.gradle&lt;/code&gt; on your app module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-gradle&#34;&gt;dependencies {&#xA;    implementation &#39;com.tencent:mmkv:1.2.13&#39;&#xA;    // replace &#34;1.2.13&#34; with any available version&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Starting from v1.2.8, MMKV has been &lt;strong&gt;migrated to Maven Central&lt;/strong&gt;.&lt;br&gt; For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_setup&#34;&gt;Android Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;apply&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say your &lt;code&gt;Application&lt;/code&gt; class, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;public void onCreate() {&#xA;    super.onCreate();&#xA;&#xA;    String rootDir = MMKV.initialize(this);&#xA;    System.out.println(&#34;mmkv root: &#34; + rootDir);&#xA;    //……&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Java&#34;&gt;import com.tencent.mmkv.MMKV;&#xA;    &#xA;MMKV kv = MMKV.defaultMMKV();&#xA;&#xA;kv.encode(&#34;bool&#34;, true);&#xA;boolean bValue = kv.decodeBool(&#34;bool&#34;);&#xA;&#xA;kv.encode(&#34;int&#34;, Integer.MIN_VALUE);&#xA;int iValue = kv.decodeInt(&#34;int&#34;);&#xA;&#xA;kv.encode(&#34;string&#34;, &#34;Hello from mmkv&#34;);&#xA;String str = kv.decodeString(&#34;string&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_tutorial&#34;&gt;Android Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Writing random &lt;code&gt;int&lt;/code&gt; for 1000 times, we get this chart:&lt;br&gt; &lt;img src=&#34;https://github.com/Tencent/MMKV/wiki/assets/profile_android_mini.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; For more benchmark data, please refer to &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/android_benchmark&#34;&gt;our benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for iOS/macOS&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of iOS/macOS to achieve the best performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go, no configurations are needed. All changes are saved immediately, no &lt;code&gt;synchronize&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains encode/decode helpers and mmap logics and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Less than 30K in binary size&lt;/strong&gt;: MMKV adds less than 30K per architecture on App size, and much less when zipped (IPA).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via CocoaPods:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://guides.CocoaPods.org/using/getting-started.html&#34;&gt;CocoaPods&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Open the terminal, &lt;code&gt;cd&lt;/code&gt; to your project directory, run &lt;code&gt;pod repo update&lt;/code&gt; to make CocoaPods aware of the latest available MMKV versions;&lt;/li&gt; &#xA; &lt;li&gt;Edit your Podfile, add &lt;code&gt;pod &#39;MMKV&#39;&lt;/code&gt; to your app target;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pod install&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;.xcworkspace&lt;/code&gt; file generated by CocoaPods;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;#import &amp;lt;MMKV/MMKV.h&amp;gt;&lt;/code&gt; to your source file and we are done.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_setup&#34;&gt;iOS/macOS Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go, no configurations are needed. All changes are saved immediately, no &lt;code&gt;synchronize&lt;/code&gt; calls are needed. Setup MMKV on App startup, in your &lt;code&gt;-[MyApp application: didFinishLaunchingWithOptions:]&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-objective-c&#34;&gt;- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {&#xA;    // init MMKV in the main thread&#xA;    [MMKV initializeMMKV:nil];&#xA;&#xA;    //...&#xA;    return YES;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-objective-c&#34;&gt;MMKV *mmkv = [MMKV defaultMMKV];&#xA;    &#xA;[mmkv setBool:YES forKey:@&#34;bool&#34;];&#xA;BOOL bValue = [mmkv getBoolForKey:@&#34;bool&#34;];&#xA;    &#xA;[mmkv setInt32:-1024 forKey:@&#34;int32&#34;];&#xA;int32_t iValue = [mmkv getInt32ForKey:@&#34;int32&#34;];&#xA;    &#xA;[mmkv setString:@&#34;hello, mmkv&#34; forKey:@&#34;string&#34;];&#xA;NSString *str = [mmkv getStringForKey:@&#34;string&#34;];&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_tutorial&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Writing random &lt;code&gt;int&lt;/code&gt; for 10000 times, we get this chart:&lt;br&gt; &lt;img src=&#34;https://github.com/Tencent/MMKV/wiki/assets/profile_mini.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; For more benchmark data, please refer to &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/iOS_benchmark&#34;&gt;our benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for Win32&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of Windows to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;save&lt;/code&gt;, no &lt;code&gt;sync&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 10K in binary size&lt;/strong&gt;: MMKV adds about 10K on application size, and much less when zipped.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via Source&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Getting source code from git repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/MMKV.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;Win32/MMKV/MMKV.vcxproj&lt;/code&gt; to your solution;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;MMKV&lt;/code&gt; project to your project&#39;s dependencies;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;$(OutDir)include&lt;/code&gt; to your project&#39;s &lt;code&gt;C/C++&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Include Directories&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;$(OutDir)&lt;/code&gt; to your project&#39;s &lt;code&gt;Linker&lt;/code&gt; -&amp;gt; &lt;code&gt;General&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Library Directories&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;MMKV.lib&lt;/code&gt; to your project&#39;s &lt;code&gt;Linker&lt;/code&gt; -&amp;gt; &lt;code&gt;Input&lt;/code&gt; -&amp;gt; &lt;code&gt;Additional Dependencies&lt;/code&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;#include &amp;lt;MMKV/MMKV.h&amp;gt;&lt;/code&gt; to your source file and we are done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;MMKV is compiled with &lt;code&gt;MT/MTd&lt;/code&gt; runtime by default. If your project uses &lt;code&gt;MD/MDd&lt;/code&gt;, you should change MMKV&#39;s setting to match your project&#39;s (&lt;code&gt;C/C++&lt;/code&gt; -&amp;gt; &lt;code&gt;Code Generation&lt;/code&gt; -&amp;gt; &lt;code&gt;Runtime Library&lt;/code&gt;), or vice versa.&lt;/li&gt; &#xA; &lt;li&gt;MMKV is developed with Visual Studio 2017, change the &lt;code&gt;Platform Toolset&lt;/code&gt; if you use a different version of Visual Studio.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/windows_setup&#34;&gt;Win32 Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;save&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say in your &lt;code&gt;main()&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;#include &amp;lt;MMKV/MMKV.h&amp;gt;&#xA;&#xA;int main() {&#xA;    std::wstring rootDir = getYourAppDocumentDir();&#xA;    MMKV::initializeMMKV(rootDir);&#xA;    //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto mmkv = MMKV::defaultMMKV();&#xA;&#xA;mmkv-&amp;gt;set(true, &#34;bool&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;bool = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getBool(&#34;bool&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(1024, &#34;int32&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;int32 = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getInt32(&#34;int32&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(&#34;Hello, MMKV for Win32&#34;, &#34;string&#34;);&#xA;std::string result;&#xA;mmkv-&amp;gt;getString(&#34;string&#34;, result);&#xA;std::cout &amp;lt;&amp;lt; &#34;string = &#34; &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/windows_tutorial&#34;&gt;Win32 Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;MMKV for POSIX&lt;/h1&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;. MMKV uses mmap to keep memory synced with files, and protobuf to encode/decode values, making the most of POSIX to achieve the best performance.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Multi-Process concurrency&lt;/strong&gt;: MMKV supports concurrent read-read and read-write access between processes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy-to-use&lt;/strong&gt;. You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;save&lt;/code&gt;, no &lt;code&gt;sync&lt;/code&gt; calls are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Small&lt;/strong&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A handful of files&lt;/strong&gt;: MMKV contains process locks, encode/decode helpers and mmap logics, and nothing more. It&#39;s really tidy.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;About 7K in binary size&lt;/strong&gt;: MMKV adds about 7K on application size, and much less when zipped.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation Via CMake&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Getting source code from the git repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/MMKV.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Edit your &lt;code&gt;CMakeLists.txt&lt;/code&gt;, add those lines:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;add_subdirectory(mmkv/POSIX/src mmkv)&#xA;target_link_libraries(MyApp&#xA;    mmkv)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;#include &#34;MMKV.h&#34;&lt;/code&gt; to your source file and we are done.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For other installation options, see &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/posix_setup&#34;&gt;POSIX Setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;You can use MMKV as you go. All changes are saved immediately, no &lt;code&gt;sync&lt;/code&gt;, no &lt;code&gt;save&lt;/code&gt; calls needed.&lt;br&gt; Setup MMKV on App startup, say in your &lt;code&gt;main()&lt;/code&gt;, add these lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;#include &#34;MMKV.h&#34;&#xA;&#xA;int main() {&#xA;    std::string rootDir = getYourAppDocumentDir();&#xA;    MMKV::initializeMMKV(rootDir);&#xA;    //...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV has a global instance, that can be used directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto mmkv = MMKV::defaultMMKV();&#xA;&#xA;mmkv-&amp;gt;set(true, &#34;bool&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;bool = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getBool(&#34;bool&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(1024, &#34;int32&#34;);&#xA;std::cout &amp;lt;&amp;lt; &#34;int32 = &#34; &amp;lt;&amp;lt; mmkv-&amp;gt;getInt32(&#34;int32&#34;) &amp;lt;&amp;lt; std::endl;&#xA;&#xA;mmkv-&amp;gt;set(&#34;Hello, MMKV for Win32&#34;, &#34;string&#34;);&#xA;std::string result;&#xA;mmkv-&amp;gt;getString(&#34;string&#34;, result);&#xA;std::cout &amp;lt;&amp;lt; &#34;string = &#34; &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMKV also supports &lt;strong&gt;Multi-Process Access&lt;/strong&gt;. Full tutorials can be found here &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/posix_tutorial&#34;&gt;POSIX Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MMKV is published under the BSD 3-Clause license. For details check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/LICENSE.TXT&#34;&gt;LICENSE.TXT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for details of change history.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, also join our &lt;a href=&#34;https://opensource.tencent.com/contribution&#34;&gt;Tencent OpenSource Plan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To give clarity of what is expected of our members, MMKV has adopted the code of conduct defined by the Contributor Covenant, which is widely used. And we think it articulates our values well. For more, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/MMKV/master/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ &amp;amp; Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/Tencent/MMKV/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; first. Should there be any questions, don&#39;t hesitate to create &lt;a href=&#34;https://github.com/Tencent/MMKV/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Personal Information Protection Rules&lt;/h2&gt; &#xA;&lt;p&gt;User privacy is taken very seriously: MMKV does not obtain, collect or upload any personal information. Please refer to the &lt;a href=&#34;https://support.weixin.qq.com/cgi-bin/mmsupportacctnodeweb-bin/pages/aY5BAtRiO1BpoHxo&#34;&gt;MMKV SDK Personal Information Protection Rules&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MultiMC/Launcher</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/MultiMC/Launcher</id>
    <link href="https://github.com/MultiMC/Launcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A custom launcher for Minecraft that allows you to easily manage multiple installations of Minecraft at once&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://avatars2.githubusercontent.com/u/5411890&#34; alt=&#34;MultiMC logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;MultiMC&lt;/h1&gt; &#xA;&lt;p&gt;MultiMC is a custom launcher for Minecraft that focuses on predictability, long term stability and simplicity.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute, talk to us on &lt;a href=&#34;https://discord.gg/multimc&#34;&gt;Discord&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;p&gt;While blindly submitting PRs is definitely possible, they&#39;re not necessarily going to get accepted.&lt;/p&gt; &#xA;&lt;p&gt;We aren&#39;t looking for flashy features, but expanding upon the existing feature set without distruption or endangering future viability of the project is OK.&lt;/p&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;If you want to build the launcher yourself, check &lt;a href=&#34;https://raw.githubusercontent.com/MultiMC/Launcher/develop/BUILD.md&#34;&gt;BUILD.md&lt;/a&gt; for build instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Code formatting&lt;/h3&gt; &#xA;&lt;p&gt;Just follow the existing formatting.&lt;/p&gt; &#xA;&lt;p&gt;In general, in order of importance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure your IDE is not messing up line endings or whitespace and avoid using linters.&lt;/li&gt; &#xA; &lt;li&gt;Prefer readability over dogma.&lt;/li&gt; &#xA; &lt;li&gt;Keep to the existing formatting.&lt;/li&gt; &#xA; &lt;li&gt;Indent with 4 space unless it&#39;s in a submodule.&lt;/li&gt; &#xA; &lt;li&gt;Keep lists (of arguments, parameters, initializers...) as lists, not paragraphs. It should either read from top to bottom, or left to right. Not both.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;p&gt;Translations can be done &lt;a href=&#34;https://translate.multimc.org&#34;&gt;on crowdin&lt;/a&gt;. Please avoid making direct pull requests to the translations repository.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright © 2013-2022 MultiMC Contributors&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this program except in compliance with the License. You may obtain a copy of the License at &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;h2&gt;Forking/Redistributing/Custom builds policy&lt;/h2&gt; &#xA;&lt;p&gt;We keep Launcher open source because we think it&#39;s important to be able to see the source code for a project like this, and we do so using the Apache license.&lt;/p&gt; &#xA;&lt;p&gt;The license gives you access to the source MultiMC is build from, but:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Not the name, logo and other branding.&lt;/li&gt; &#xA; &lt;li&gt;Not the API tokens required to talk to services the launcher depends on.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Because of the nature of the agreements required to interact with the Microsoft identity platform, it&#39;s impossible for us to continue allowing everyone to build the code as &#39;MultiMC&#39;. The source code has been debranded and now builds as &lt;code&gt;DevLauncher&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;p&gt;You must provide your own branding if you want to distribute your own builds.&lt;/p&gt; &#xA;&lt;p&gt;You will also have to register your own app on Azure to be able to handle Microsoft account logins.&lt;/p&gt; &#xA;&lt;p&gt;If you decide to fork the project, a mention of its origins in the About dialog and the license is acceptable. However, it should be abundantly clear that the project is a fork &lt;em&gt;without&lt;/em&gt; implying that you have our blessing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/folly</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/facebook/folly</id>
    <link href="https://github.com/facebook/folly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source C++ library developed and used at Facebook.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Folly: Facebook Open-source Library&lt;/h1&gt; &#xA;&lt;a href=&#34;https://opensource.facebook.com/support-ukraine&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine - Help Provide Humanitarian Aid to Ukraine.&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;What is &lt;code&gt;folly&lt;/code&gt;?&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebook/folly/main/static/logo.svg?sanitize=true&#34; alt=&#34;Logo Folly&#34; width=&#34;15%&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;Folly (acronymed loosely after Facebook Open Source Library) is a library of C++14 components designed with practicality and efficiency in mind. &lt;strong&gt;Folly contains a variety of core library components used extensively at Facebook&lt;/strong&gt;. In particular, it&#39;s often a dependency of Facebook&#39;s other open source C++ efforts and place where those projects can share code.&lt;/p&gt; &#xA;&lt;p&gt;It complements (as opposed to competing against) offerings such as Boost and of course &lt;code&gt;std&lt;/code&gt;. In fact, we embark on defining our own component only when something we need is either not available, or does not meet the needed performance profile. We endeavor to remove things from folly if or when &lt;code&gt;std&lt;/code&gt; or Boost obsoletes them.&lt;/p&gt; &#xA;&lt;p&gt;Performance concerns permeate much of Folly, sometimes leading to designs that are more idiosyncratic than they would otherwise be (see e.g. &lt;code&gt;PackedSyncPtr.h&lt;/code&gt;, &lt;code&gt;SmallLocks.h&lt;/code&gt;). Good performance at large scale is a unifying theme in all of Folly.&lt;/p&gt; &#xA;&lt;h2&gt;Check it out in the intro video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Wr_IfOICYSs&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/Wr_IfOICYSs/0.jpg&#34; alt=&#34;Explain Like I’m 5: Folly&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Logical Design&lt;/h1&gt; &#xA;&lt;p&gt;Folly is a collection of relatively independent components, some as simple as a few symbols. There is no restriction on internal dependencies, meaning that a given folly module may use any other folly components.&lt;/p&gt; &#xA;&lt;p&gt;All symbols are defined in the top-level namespace &lt;code&gt;folly&lt;/code&gt;, except of course macros. Macro names are ALL_UPPERCASE and should be prefixed with &lt;code&gt;FOLLY_&lt;/code&gt;. Namespace &lt;code&gt;folly&lt;/code&gt; defines other internal namespaces such as &lt;code&gt;internal&lt;/code&gt; or &lt;code&gt;detail&lt;/code&gt;. User code should not depend on symbols in those namespaces.&lt;/p&gt; &#xA;&lt;p&gt;Folly has an &lt;code&gt;experimental&lt;/code&gt; directory as well. This designation connotes primarily that we feel the API may change heavily over time. This code, typically, is still in heavy use and is well tested.&lt;/p&gt; &#xA;&lt;h1&gt;Physical Design&lt;/h1&gt; &#xA;&lt;p&gt;At the top level Folly uses the classic &#34;stuttering&#34; scheme &lt;code&gt;folly/folly&lt;/code&gt; used by Boost and others. The first directory serves as an installation root of the library (with possible versioning a la &lt;code&gt;folly-1.0/&lt;/code&gt;), and the second is to distinguish the library when including files, e.g. &lt;code&gt;#include &amp;lt;folly/FBString.h&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The directory structure is flat (mimicking the namespace structure), i.e. we don&#39;t have an elaborate directory hierarchy (it is possible this will change in future versions). The subdirectory &lt;code&gt;experimental&lt;/code&gt; contains files that are used inside folly and possibly at Facebook but not considered stable enough for client use. Your code should not use files in &lt;code&gt;folly/experimental&lt;/code&gt; lest it may break when you update Folly.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;folly/folly/test&lt;/code&gt; subdirectory includes the unittests for all components, usually named &lt;code&gt;ComponentXyzTest.cpp&lt;/code&gt; for each &lt;code&gt;ComponentXyz.*&lt;/code&gt;. The &lt;code&gt;folly/folly/docs&lt;/code&gt; directory contains documentation.&lt;/p&gt; &#xA;&lt;h1&gt;What&#39;s in it?&lt;/h1&gt; &#xA;&lt;p&gt;Because of folly&#39;s fairly flat structure, the best way to see what&#39;s in it is to look at the headers in &lt;a href=&#34;https://github.com/facebook/folly/tree/main/folly&#34;&gt;top level &lt;code&gt;folly/&lt;/code&gt; directory&lt;/a&gt;. You can also check the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/folly/main/folly/docs&#34;&gt;&lt;code&gt;docs&lt;/code&gt; folder&lt;/a&gt; for documentation, starting with the &lt;a href=&#34;https://raw.githubusercontent.com/facebook/folly/main/folly/docs/Overview.md&#34;&gt;overview&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Folly is published on GitHub at &lt;a href=&#34;https://github.com/facebook/folly&#34;&gt;https://github.com/facebook/folly&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Build Notes&lt;/h1&gt; &#xA;&lt;p&gt;Because folly does not provide any ABI compatibility guarantees from commit to commit, we generally recommend building folly as a static library.&lt;/p&gt; &#xA;&lt;p&gt;folly supports gcc (5.1+), clang, or MSVC. It should run on Linux (x86-32, x86-64, and ARM), iOS, macOS, and Windows (x86-64). The CMake build is only tested on some of these platforms; at a minimum, we aim to support macOS and Linux (on the latest Ubuntu LTS release or newer.)&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;getdeps.py&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This script is used by many of Meta&#39;s OSS tools. It will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s written in python so you&#39;ll need python3.6 or later on your PATH. It works on Linux, macOS and Windows.&lt;/p&gt; &#xA;&lt;p&gt;The settings for folly&#39;s cmake build are held in its getdeps manifest &lt;code&gt;build/fbcode_builder/manifests/folly&lt;/code&gt;, which you can edit locally if desired.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;If on Linux or MacOS (with homebrew installed) you can install system dependencies to save building them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo&#xA;git clone https://github.com/facebook/folly&#xA;# Install dependencies&#xA;cd folly&#xA;sudo ./build/fbcode_builder/getdeps.py install-system-deps --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to see the packages before installing them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build/fbcode_builder/getdeps.py install-system-deps --dry-run --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On other platforms or if on Linux and without system dependencies &lt;code&gt;getdeps.py&lt;/code&gt; will mostly download and build them for you during the build step.&lt;/p&gt; &#xA;&lt;p&gt;Some of the dependencies &lt;code&gt;getdeps.py&lt;/code&gt; uses and installs are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a version of boost compiled with C++14 support.&lt;/li&gt; &#xA; &lt;li&gt;googletest is required to build and run folly&#39;s tests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;This script will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; currently requires python 3.6+ to be on your path.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; will invoke cmake etc&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo&#xA;git clone https://github.com/facebook/folly&#xA;cd folly&#xA;# Build, using system dependencies if available&#xA;python3 ./build/fbcode_builder/getdeps.py --allow-system-packages build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It puts output in its scratch area:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;installed/folly/lib/libfolly.a&lt;/code&gt;: Library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also specify a &lt;code&gt;--scratch-path&lt;/code&gt; argument to control the location of the scratch directory used for the build. You can find the default scratch install location from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-inst-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are also &lt;code&gt;--install-dir&lt;/code&gt; and &lt;code&gt;--install-prefix&lt;/code&gt; arguments to provide some more fine-grained control of the installation directories. However, given that folly provides no compatibility guarantees between commits we generally recommend building and installing the libraries to a temporary location, and then pointing your project&#39;s build at this temporary location, rather than installing folly in the traditional system installation directories. e.g., if you are building with CMake you can use the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; variable to allow CMake to find folly in this temporary installation directory when building your project.&lt;/p&gt; &#xA;&lt;p&gt;If you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run tests&lt;/h3&gt; &#xA;&lt;p&gt;By default &lt;code&gt;getdeps.py&lt;/code&gt; will build the tests for folly. To run them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd folly&#xA;python3 ./build/fbcode_builder/getdeps.py --allow-system-packages test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;build.sh&lt;/code&gt;/&lt;code&gt;build.bat&lt;/code&gt; wrapper&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;build.sh&lt;/code&gt; can be used on Linux and MacOS, on Windows use the &lt;code&gt;build.bat&lt;/code&gt; script instead. Its a wrapper around &lt;code&gt;getdeps.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build with cmake directly&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t want to let getdeps invoke cmake for you then by default, building the tests is disabled as part of the CMake &lt;code&gt;all&lt;/code&gt; target. To build the tests, specify &lt;code&gt;-DBUILD_TESTS=ON&lt;/code&gt; to CMake at configure time.&lt;/p&gt; &#xA;&lt;p&gt;NB if you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate on a &lt;code&gt;getdeps.py&lt;/code&gt; build, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch-path build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Running tests with ctests also works if you cd to the build dir, e.g. &lt;code&gt; &lt;/code&gt;(cd $(python3 ./build/fbcode_builder/getdeps.py show-build-dir) &amp;amp;&amp;amp; ctest)`&lt;/p&gt; &#xA;&lt;h3&gt;Finding dependencies in non-default locations&lt;/h3&gt; &#xA;&lt;p&gt;If you have boost, gtest, or other dependencies installed in a non-default location, you can use the &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;CMAKE_LIBRARY_PATH&lt;/code&gt; variables to make CMAKE look also look for header files and libraries in non-standard locations. For example, to also search the directories &lt;code&gt;/alt/include/path1&lt;/code&gt; and &lt;code&gt;/alt/include/path2&lt;/code&gt; for header files and the directories &lt;code&gt;/alt/lib/path1&lt;/code&gt; and &lt;code&gt;/alt/lib/path2&lt;/code&gt; for libraries, you can invoke &lt;code&gt;cmake&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake \&#xA;  -DCMAKE_INCLUDE_PATH=/alt/include/path1:/alt/include/path2 \&#xA;  -DCMAKE_LIBRARY_PATH=/alt/lib/path1:/alt/lib/path2 ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ubuntu LTS, CentOS Stream, Fedora&lt;/h2&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;getdeps.py&lt;/code&gt; approach above. We test in CI on Ubuntu LTS, and occasionally on other distros.&lt;/p&gt; &#xA;&lt;p&gt;If you find the set of system packages is not quite right for your chosen distro, you can specify distro version specific overrides in the dependency manifests (e.g. &lt;a href=&#34;https://github.com/facebook/folly/raw/main/build/fbcode_builder/manifests/boost&#34;&gt;https://github.com/facebook/folly/blob/main/build/fbcode_builder/manifests/boost&lt;/a&gt; ). You could probably make it work on most recent Ubuntu/Debian or Fedora/Redhat derived distributions.&lt;/p&gt; &#xA;&lt;p&gt;At time of writing (Dec 2021) there is a build break on GCC 11.x based systems in lang_badge_test. If you don&#39;t need badge functionality you can work around by commenting it out from CMakeLists.txt (unfortunately fbthrift does need it)&lt;/p&gt; &#xA;&lt;h2&gt;Windows (Vcpkg)&lt;/h2&gt; &#xA;&lt;p&gt;Note that many tests are disabled for folly Windows builds, you can see them in the log from the cmake configure step, or by looking for WINDOWS_DISABLED in &lt;code&gt;CMakeLists.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;That said, &lt;code&gt;getdeps.py&lt;/code&gt; builds work on Windows and are tested in CI.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer, you can try Vcpkg. folly is available in &lt;a href=&#34;https://github.com/Microsoft/vcpkg#vcpkg&#34;&gt;Vcpkg&lt;/a&gt; and releases may be built via &lt;code&gt;vcpkg install folly:x64-windows&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also use &lt;code&gt;vcpkg install folly:x64-windows --head&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;macOS&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; builds work on macOS and are tested in CI, however if you prefer, you can try one of the macOS package managers&lt;/p&gt; &#xA;&lt;h3&gt;Homebrew&lt;/h3&gt; &#xA;&lt;p&gt;folly is available as a Formula and releases may be built via &lt;code&gt;brew install folly&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also use &lt;code&gt;folly/build/bootstrap-osx-homebrew.sh&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  ./folly/build/bootstrap-osx-homebrew.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a build directory &lt;code&gt;_build&lt;/code&gt; in the top-level.&lt;/p&gt; &#xA;&lt;h3&gt;MacPorts&lt;/h3&gt; &#xA;&lt;p&gt;Install the required packages from MacPorts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  sudo port install \&#xA;    boost \&#xA;    cmake \&#xA;    gflags \&#xA;    git \&#xA;    google-glog \&#xA;    libevent \&#xA;    libtool \&#xA;    lz4 \&#xA;    lzma \&#xA;    openssl \&#xA;    snappy \&#xA;    xz \&#xA;    zlib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and install double-conversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/google/double-conversion.git&#xA;  cd double-conversion&#xA;  cmake -DBUILD_SHARED_LIBS=ON .&#xA;  make&#xA;  sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and install folly with the parameters listed below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/facebook/folly.git&#xA;  cd folly&#xA;  mkdir _build&#xA;  cd _build&#xA;  cmake ..&#xA;  make&#xA;  sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ethereum/solidity</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/ethereum/solidity</id>
    <link href="https://github.com/ethereum/solidity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solidity, the Smart Contract Programming Language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Solidity Contract-Oriented Programming Language&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#ethereum_solidity:gitter.im&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Matrix%20-chat-brightgreen?style=plastic&amp;amp;logo=matrix&#34; alt=&#34;Matrix Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/ethereum/solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitter%20-chat-brightgreen?style=plastic&amp;amp;logo=gitter&#34; alt=&#34;Gitter Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.soliditylang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Solidity_Forum%20-discuss-brightgreen?style=plastic&amp;amp;logo=discourse&#34; alt=&#34;Solidity&amp;nbsp;Forum&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/solidity_lang&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/solidity_lang?style=plastic&amp;amp;logo=twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/000335908?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;logo=mastodon&amp;amp;style=plastic&#34; alt=&#34;Mastodon Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can talk to us on Gitter and Matrix, tweet at us on Twitter or create a new topic in the Solidity forum. Questions, feedback, and suggestions are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Solidity is a statically typed, contract-oriented, high-level language for implementing smart contracts on the Ethereum platform.&lt;/p&gt; &#xA;&lt;p&gt;For a good overview and starting point, please check out the official &lt;a href=&#34;https://soliditylang.org&#34;&gt;Solidity Language Portal&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#build-and-install&#34;&gt;Build and Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#maintainers&#34;&gt;Maintainers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#security&#34;&gt;Security&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is a statically-typed curly-braces programming language designed for developing smart contracts that run on the Ethereum Virtual Machine. Smart contracts are programs that are executed inside a peer-to-peer network where nobody has special authority over the execution, and thus they allow to implement tokens of value, ownership, voting, and other kinds of logic.&lt;/p&gt; &#xA;&lt;p&gt;When deploying contracts, you should use the latest released version of Solidity. This is because breaking changes, as well as new features and bug fixes are introduced regularly. We currently use a 0.x version number &lt;a href=&#34;https://semver.org/#spec-item-4&#34;&gt;to indicate this fast pace of change&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build and Install&lt;/h2&gt; &#xA;&lt;p&gt;Instructions about how to build and install the Solidity compiler can be found in the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/installing-solidity.html#building-from-source&#34;&gt;Solidity documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;A &#34;Hello World&#34; program in Solidity is of even less use than in other languages, but still:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-solidity&#34;&gt;// SPDX-License-Identifier: MIT&#xA;pragma solidity &amp;gt;=0.6.0 &amp;lt;0.9.0;&#xA;&#xA;contract HelloWorld {&#xA;    function helloWorld() external pure returns (string memory) {&#xA;        return &#34;Hello, World!&#34;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get started with Solidity, you can use &lt;a href=&#34;https://remix.ethereum.org/&#34;&gt;Remix&lt;/a&gt;, which is a browser-based IDE. Here are some example contracts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#voting&#34;&gt;Voting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#blind-auction&#34;&gt;Blind Auction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#safe-remote-purchase&#34;&gt;Safe remote purchase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#micropayment-channel&#34;&gt;Micropayment Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The Solidity documentation is hosted at &lt;a href=&#34;https://docs.soliditylang.org&#34;&gt;Read the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is still under development. Contributions are always welcome! Please follow the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/contributing.html&#34;&gt;Developers Guide&lt;/a&gt; if you want to help.&lt;/p&gt; &#xA;&lt;p&gt;You can find our current feature and bug priorities for forthcoming releases in the &lt;a href=&#34;https://github.com/ethereum/solidity/projects&#34;&gt;projects section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/axic&#34;&gt;@axic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chriseth&#34;&gt;@chriseth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/LICENSE.txt&#34;&gt;GNU General Public License v3.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some third-party code has its &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/cmake/templates/license.h.in&#34;&gt;own licensing terms&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;The security policy may be &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/SECURITY.md&#34;&gt;found here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>iovisor/bpftrace</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/iovisor/bpftrace</id>
    <link href="https://github.com/iovisor/bpftrace" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-level tracing language for Linux eBPF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bpftrace&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/iovisor/bpftrace/actions?query=workflow%3ACI+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/iovisor/bpftrace/workflows/CI/badge.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://webchat.oftc.net/?channels=bpftrace&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/IRC-bpftrace-blue.svg?sanitize=true&#34; alt=&#34;IRC#bpftrace&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/iovisor/bpftrace/alerts/&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/alerts/g/iovisor/bpftrace.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Total alerts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;bpftrace is a high-level tracing language for Linux enhanced Berkeley Packet Filter (eBPF) available in recent Linux kernels (4.x). bpftrace uses LLVM as a backend to compile scripts to BPF-bytecode and makes use of &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;BCC&lt;/a&gt; for interacting with the Linux BPF system, as well as existing Linux tracing capabilities: kernel dynamic tracing (kprobes), user-level dynamic tracing (uprobes), and tracepoints. The bpftrace language is inspired by awk and C, and predecessor tracers such as DTrace and SystemTap. bpftrace was created by &lt;a href=&#34;https://github.com/ajor&#34;&gt;Alastair Robertson&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about bpftrace, see the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/man/adoc/bpftrace.adoc&#34;&gt;Manual&lt;/a&gt; the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/reference_guide.md&#34;&gt;Reference Guide&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/tutorial_one_liners.md&#34;&gt;One-Liner Tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;One-Liners&lt;/h2&gt; &#xA;&lt;p&gt;The following one-liners demonstrate different capabilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Files opened by process&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_enter_open { printf(&#34;%s %s\n&#34;, comm, str(args-&amp;gt;filename)); }&#39;&#xA;&#xA;# Syscall count by program&#xA;bpftrace -e &#39;tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }&#39;&#xA;&#xA;# Read bytes by process:&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_exit_read /args-&amp;gt;ret/ { @[comm] = sum(args-&amp;gt;ret); }&#39;&#xA;&#xA;# Read size distribution by process:&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_exit_read { @[comm] = hist(args-&amp;gt;ret); }&#39;&#xA;&#xA;# Show per-second syscall rates:&#xA;bpftrace -e &#39;tracepoint:raw_syscalls:sys_enter { @ = count(); } interval:s:1 { print(@); clear(@); }&#39;&#xA;&#xA;# Trace disk size by process&#xA;bpftrace -e &#39;tracepoint:block:block_rq_issue { printf(&#34;%d %s %d\n&#34;, pid, comm, args-&amp;gt;bytes); }&#39;&#xA;&#xA;# Count page faults by process&#xA;bpftrace -e &#39;software:faults:1 { @[comm] = count(); }&#39;&#xA;&#xA;# Count LLC cache misses by process name and PID (uses PMCs):&#xA;bpftrace -e &#39;hardware:cache-misses:1000000 { @[comm, pid] = count(); }&#39;&#xA;&#xA;# Profile user-level stacks at 99 Hertz, for PID 189:&#xA;bpftrace -e &#39;profile:hz:99 /pid == 189/ { @[ustack] = count(); }&#39;&#xA;&#xA;# Files opened, for processes in the root cgroup-v2&#xA;bpftrace -e &#39;tracepoint:syscalls:sys_enter_openat /cgroup == cgroupid(&#34;/sys/fs/cgroup/unified/mycg&#34;)/ { printf(&#34;%s\n&#34;, str(args-&amp;gt;filename)); }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More powerful scripts can easily be constructed. See &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools&#34;&gt;Tools&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;For build and install instructions, see &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;p&gt;bpftrace contains various tools, which also serve as examples of programming in the bpftrace language.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bashreadline.bt&#34;&gt;bashreadline.bt&lt;/a&gt;: Print entered bash commands system wide. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bashreadline_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biolatency.bt&#34;&gt;biolatency.bt&lt;/a&gt;: Block I/O latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biolatency_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biosnoop.bt&#34;&gt;biosnoop.bt&lt;/a&gt;: Block I/O tracing tool, showing per I/O latency. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biosnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biostacks.bt&#34;&gt;biostacks.bt&lt;/a&gt;: Show disk I/O latency with initialization stacks. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/biostacks_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bitesize.bt&#34;&gt;bitesize.bt&lt;/a&gt;: Show disk I/O size as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/bitesize_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/capable.bt&#34;&gt;capable.bt&lt;/a&gt;: Trace security capability checks. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/capable_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/cpuwalk.bt&#34;&gt;cpuwalk.bt&lt;/a&gt;: Sample which CPUs are executing processes. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/cpuwalk_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/dcsnoop.bt&#34;&gt;dcsnoop.bt&lt;/a&gt;: Trace directory entry cache (dcache) lookups. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/dcsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/execsnoop.bt&#34;&gt;execsnoop.bt&lt;/a&gt;: Trace new processes via exec() syscalls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/execsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/gethostlatency.bt&#34;&gt;gethostlatency.bt&lt;/a&gt;: Show latency for getaddrinfo/gethostbyname[2] calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/gethostlatency_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/killsnoop.bt&#34;&gt;killsnoop.bt&lt;/a&gt;: Trace signals issued by the kill() syscall. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/killsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/loads.bt&#34;&gt;loads.bt&lt;/a&gt;: Print load averages. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/loads_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/mdflush.bt&#34;&gt;mdflush.bt&lt;/a&gt;: Trace md flush events. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/mdflush_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/naptime.bt&#34;&gt;naptime.bt&lt;/a&gt;: Show voluntary sleep calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/naptime_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/opensnoop.bt&#34;&gt;opensnoop.bt&lt;/a&gt;: Trace open() syscalls showing filenames. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/opensnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/oomkill.bt&#34;&gt;oomkill.bt&lt;/a&gt;: Trace OOM killer. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/oomkill_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/pidpersec.bt&#34;&gt;pidpersec.bt&lt;/a&gt;: Count new processes (via fork). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/pidpersec_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlat.bt&#34;&gt;runqlat.bt&lt;/a&gt;: CPU scheduler run queue latency as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlat_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlen.bt&#34;&gt;runqlen.bt&lt;/a&gt;: CPU scheduler run queue length as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/runqlen_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/setuids.bt&#34;&gt;setuids.bt&lt;/a&gt;: Trace the setuid syscalls: privilege escalation. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/setuids_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/statsnoop.bt&#34;&gt;statsnoop.bt&lt;/a&gt;: Trace stat() syscalls for general debugging. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/statsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/swapin.bt&#34;&gt;swapin.bt&lt;/a&gt;: Show swapins by process. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/swapin_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syncsnoop.bt&#34;&gt;syncsnoop.bt&lt;/a&gt;: Trace sync() variety of syscalls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syncsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syscount.bt&#34;&gt;syscount.bt&lt;/a&gt;: Count system calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/syscount_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpaccept.bt&#34;&gt;tcpaccept.bt&lt;/a&gt;: Trace TCP passive connections (accept()). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpaccept_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpconnect.bt&#34;&gt;tcpconnect.bt&lt;/a&gt;: Trace TCP active connections (connect()). &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpconnect_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpdrop.bt&#34;&gt;tcpdrop.bt&lt;/a&gt;: Trace kernel-based TCP packet drops with details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpdrop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcplife.bt&#34;&gt;tcplife.bt&lt;/a&gt;: Trace TCP session lifespans with connection details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcplife_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpretrans.bt&#34;&gt;tcpretrans.bt&lt;/a&gt;: Trace TCP retransmits. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpretrans_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpsynbl.bt&#34;&gt;tcpsynbl.bt&lt;/a&gt;: Show TCP SYN backlog as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/tcpsynbl_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/threadsnoop.bt&#34;&gt;threadsnoop.bt&lt;/a&gt;: List new thread creation. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/threadsnoop_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfscount.bt&#34;&gt;vfscount.bt&lt;/a&gt;: Count VFS calls. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfscount_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfsstat.bt&#34;&gt;vfsstat.bt&lt;/a&gt;: Count some VFS calls, with per-second summaries. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/vfsstat_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/writeback.bt&#34;&gt;writeback.bt&lt;/a&gt;: Trace file system writeback events with details. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/writeback_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;tools/&lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/xfsdist.bt&#34;&gt;xfsdist.bt&lt;/a&gt;: Summarize XFS operation latency distribution as a histogram. &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/tools/xfsdist_example.txt&#34;&gt;Examples&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more eBPF observability tools, see &lt;a href=&#34;https://github.com/iovisor/bcc#tools&#34;&gt;bcc tools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Probe types&lt;/h2&gt; &#xA;&lt;center&gt;&#xA; &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/images/bpftrace_probes_2018.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/images/bpftrace_probes_2018.png&#34; border=&#34;0&#34; width=&#34;700&#34;&gt;&lt;/a&gt;&#xA;&lt;/center&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/docs/reference_guide.md&#34;&gt;Reference Guide&lt;/a&gt; for more detail.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;For additional help / discussion, please use our &lt;a href=&#34;https://github.com/iovisor/bpftrace/discussions&#34;&gt;discussions&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Have ideas for new bpftrace tools? &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/CONTRIBUTING-TOOLS.md&#34;&gt;CONTRIBUTING-TOOLS.md&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bugs reports and feature requests: &lt;a href=&#34;https://github.com/iovisor/bpftrace/issues&#34;&gt;Issue Tracker&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;bpftrace development IRC: #bpftrace at irc.oftc.net&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;For build &amp;amp; test directly in docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For build in docker then test directly on host&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./build-static.sh&#xA;$ ./build-static/src/bpftrace&#xA;$ ./build-static/tests/bpftrace_test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vagrant&lt;/h3&gt; &#xA;&lt;p&gt;For development and testing a &lt;a href=&#34;https://raw.githubusercontent.com/iovisor/bpftrace/master/Vagrantfile&#34;&gt;Vagrantfile&lt;/a&gt; is available.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you have the &lt;code&gt;vbguest&lt;/code&gt; plugin installed, it is required to correctly install the shared file system driver on the ubuntu boxes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant plugin install vagrant-vbguest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start VM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ vagrant status&#xA;$ vagrant up $YOUR_CHOICE&#xA;$ vagrant ssh $YOUR_CHOICE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2019 Alastair Robertson&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zaphoyd/websocketpp</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/zaphoyd/websocketpp</id>
    <link href="https://github.com/zaphoyd/websocketpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ websocket client/server library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WebSocket++ (0.8.2)&lt;/h1&gt; &#xA;&lt;p&gt;WebSocket++ is a header only C++ library that implements RFC6455 The WebSocket Protocol. It allows integrating WebSocket client and server functionality into C++ programs. It uses interchangeable network transport modules including one based on raw char buffers, one based on C++ iostreams, and one based on Asio (either via Boost or standalone). End users can write additional transport policies to support other networking or event libraries as needed.&lt;/p&gt; &#xA;&lt;h1&gt;Major Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full support for RFC6455&lt;/li&gt; &#xA; &lt;li&gt;Partial support for Hixie 76 / Hybi 00, 07-17 draft specs (server only)&lt;/li&gt; &#xA; &lt;li&gt;Message/event based interface&lt;/li&gt; &#xA; &lt;li&gt;Supports secure WebSockets (TLS), IPv6, and explicit proxies.&lt;/li&gt; &#xA; &lt;li&gt;Flexible dependency management (C++11 Standard Library or Boost)&lt;/li&gt; &#xA; &lt;li&gt;Interchangeable network transport modules (raw, iostream, Asio, or custom)&lt;/li&gt; &#xA; &lt;li&gt;Portable/cross platform (Posix/Windows, 32/64bit, Intel/ARM/PPC)&lt;/li&gt; &#xA; &lt;li&gt;Thread-safe&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Get Involved&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/zaphoyd/websocketpp&#34;&gt;&lt;img src=&#34;https://travis-ci.org/zaphoyd/websocketpp.png&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Project Website&lt;/strong&gt; &lt;a href=&#34;http://www.zaphoyd.com/websocketpp/&#34;&gt;http://www.zaphoyd.com/websocketpp/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User Manual&lt;/strong&gt; &lt;a href=&#34;http://docs.websocketpp.org/&#34;&gt;http://docs.websocketpp.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GitHub Repository&lt;/strong&gt; &lt;a href=&#34;https://github.com/zaphoyd/websocketpp/&#34;&gt;https://github.com/zaphoyd/websocketpp/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;GitHub pull requests should be submitted to the &lt;code&gt;develop&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Announcements Mailing List&lt;/strong&gt; &lt;a href=&#34;http://groups.google.com/group/websocketpp-announcements/&#34;&gt;http://groups.google.com/group/websocketpp-announcements/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IRC Channel&lt;/strong&gt; #websocketpp (freenode)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discussion / Development / Support Mailing List / Forum&lt;/strong&gt; &lt;a href=&#34;http://groups.google.com/group/websocketpp/&#34;&gt;http://groups.google.com/group/websocketpp/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Author&lt;/h1&gt; &#xA;&lt;p&gt;Peter Thorson - &lt;a href=&#34;mailto:websocketpp@zaphoyd.com&#34;&gt;websocketpp@zaphoyd.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wenet-e2e/wenet</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/wenet-e2e/wenet</id>
    <link href="https://github.com/wenet-e2e/wenet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Production First and Production Ready End-to-End Speech Recognition Toolkit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WeNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/raw/main/README_CN.md&#34;&gt;&lt;strong&gt;中文版&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7%7C3.8-brightgreen&#34; alt=&#34;Python-Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/discussions&#34;&gt;&lt;strong&gt;Discussions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/papers.html&#34;&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/server/x86&#34;&gt;&lt;strong&gt;Runtime (x86)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet&#34;&gt;&lt;strong&gt;Runtime (android)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/pretrained_models.md&#34;&gt;&lt;strong&gt;Pretrained Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We&lt;/strong&gt; share neural &lt;strong&gt;Net&lt;/strong&gt; together.&lt;/p&gt; &#xA;&lt;p&gt;The main motivation of WeNet is to close the gap between research and production end-to-end (E2E) speech recognition models, to reduce the effort of productionizing E2E models, and to explore better E2E models for production.&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Production first and production ready&lt;/strong&gt;: The core design principle of WeNet. WeNet provides full stack solutions for speech recognition.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Unified solution for streaming and non-streaming ASR&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/pdf/2012.05481.pdf&#34;&gt;U2 framework&lt;/a&gt;--develop, train, and deploy only once.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Runtime solution&lt;/em&gt;: built-in server &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/server/x86&#34;&gt;x86&lt;/a&gt; and on-device &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet&#34;&gt;android&lt;/a&gt; runtime solution.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Model exporting solution&lt;/em&gt;: built-in solution to export model to LibTorch/ONNX for inference.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;LM solution&lt;/em&gt;: built-in production-level &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/lm.md&#34;&gt;LM solution&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Other production solutions&lt;/em&gt;: built-in contextual biasing, time stamp, endpoint, and n-best solutions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Accurate&lt;/strong&gt;: WeNet achieves SOTA results on a lot of public speech datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Light weight&lt;/strong&gt;: WeNet is easy to install, easy to use, well designed, and well documented.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;code&gt;examples/$dataset/s0/README.md&lt;/code&gt; for benchmark on different speech datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Installation(Python Only)&lt;/h2&gt; &#xA;&lt;p&gt;If you just want to use WeNet as a python package for speech recognition application, just install it by &lt;code&gt;pip&lt;/code&gt;, please note python 3.6+ is required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install wenet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And please see &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/runtime/binding/python/README.md&#34;&gt;doc&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;h2&gt;Installation(Training and Developing)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/wenet-e2e/wenet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Conda: please see &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create Conda env:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n wenet python=3.8&#xA;conda activate wenet&#xA;pip install -r requirements.txt&#xA;conda install pytorch=1.10.0 torchvision torchaudio=0.10.0 cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optionally, if you want to use x86 runtime or language model(LM), you have to build the runtime as follows. Otherwise, you can just ignore this step.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# runtime build requires cmake 3.14 or above&#xA;cd runtime/server/x86&#xA;mkdir build &amp;amp;&amp;amp; cd build &amp;amp;&amp;amp; cmake .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; &#xA;&lt;p&gt;Please visit &lt;a href=&#34;https://github.com/wenet-e2e/wenet/discussions&#34;&gt;Discussions&lt;/a&gt; for further discussion.&lt;/p&gt; &#xA;&lt;p&gt;For Chinese users, you can aslo scan the QR code on the left to follow our offical account of WeNet. We created a WeChat group for better discussion and quicker response. Please scan the personal QR code on the right, and the guy is responsible for inviting you to the chat group.&lt;/p&gt; &#xA;&lt;p&gt;If you can not access the QR image, please access it on &lt;a href=&#34;https://gitee.com/robin1001/qr/tree/master&#34;&gt;gitee&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/wenet.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/binbin.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Or you can directly discuss on &lt;a href=&#34;https://github.com/wenet-e2e/wenet/issues&#34;&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.chumenwenwen.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/chumenwenwen.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://lxie.npu-aslp.org&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/colleges/nwpu.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://www.aishelltech.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/aishelltech.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://www.ximalaya.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/ximalaya.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.jd.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/jd.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://horizon.ai&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/hobot.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://thuhcsi.github.io&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/colleges/thu.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nvidia.com/en-us&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/nvidia.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPnet&lt;/a&gt; for transformer based modeling.&lt;/li&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;Kaldi&lt;/a&gt; for WFST based decoding for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred &lt;a href=&#34;https://github.com/srvk/eesen&#34;&gt;EESEN&lt;/a&gt; for building TLG based graph for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred to &lt;a href=&#34;https://github.com/ZhengkunTian/OpenTransformer/&#34;&gt;OpenTransformer&lt;/a&gt; for python batch inference of e2e models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{yao2021wenet,&#xA;  title={WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit},&#xA;  author={Yao, Zhuoyuan and Wu, Di and Wang, Xiong and Zhang, Binbin and Yu, Fan and Yang, Chao and Peng, Zhendong and Chen, Xiaoyu and Xie, Lei and Lei, Xin},&#xA;  booktitle={Proc. Interspeech},&#xA;  year={2021},&#xA;  address={Brno, Czech Republic }&#xA;  organization={IEEE}&#xA;}&#xA;&#xA;@article{zhang2022wenet,&#xA;  title={WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit},&#xA;  author={Zhang, Binbin and Wu, Di and Peng, Zhendong and Song, Xingchen and Yao, Zhuoyuan and Lv, Hang and Xie, Lei and Yang, Chao and Pan, Fuping and Niu, Jianwei},&#xA;  journal={arXiv preprint arXiv:2203.15455},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>applenob/Cpp_Primer_Practice</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/applenob/Cpp_Primer_Practice</id>
    <link href="https://github.com/applenob/Cpp_Primer_Practice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;搞定C++👊。C++ Primer 中文版第5版学习仓库，包括笔记和课后练习答案。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cpp Primer 学习&lt;/h1&gt; &#xA;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;p&gt;《C++ Primer 中文版（第 5 版）》学习仓库，包括&lt;strong&gt;笔记&lt;/strong&gt;和&lt;strong&gt;课后练习答案&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;环境&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;system: ubuntu 16.04&lt;/li&gt; &#xA; &lt;li&gt;IDE: VS Code&lt;/li&gt; &#xA; &lt;li&gt;compiler: g++&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://book.douban.com/subject/25708312/&#34;&gt;豆瓣链接&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第1章 : 开始 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch01.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch01.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;第 I 部分 : C++基础 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;第2章 : 变量和基本类型 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch02.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch02.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第3章 : 字符串、向量和数组 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch03.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch03.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第4章 : 表达式 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch04.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch04.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第5章 : 语句 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch05.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch05.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第6章 : 函数 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch06.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch06.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第7章 : 类 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch07.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch07.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第 II 部分 : C++标准库 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;第8章 : IO库 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch08.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch08.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第9章 : 顺序容器 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch09.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch09.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第10章 : 泛型算法 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch10.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch10.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第11章 : 关联容器 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch11.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch11.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第12章 : 动态内存 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch12.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch12.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第 III 部分 : 类设计者的工具 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;第13章 : 拷贝控制 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch13.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch13.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第14章 : 重载与类型转换 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch14.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch14.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第15章 : 面向对象程序设计 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch15.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch15.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第16章 : 模版与泛型编程 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch16.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch16.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;第 IV 部分 : 高级主题 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;第17章 : 标准库与特殊设施 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch17.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch17.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第18章 : 用于大型程序的工具 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch18.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch18.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;第19章 : 特殊工具与技术 &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/notes/ch19.md&#34;&gt;笔记&lt;/a&gt; &lt;a href=&#34;https://github.com/applenob/Cpp_Primer_Practice/tree/master/excersize/ch19.md&#34;&gt;练习&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;参考&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mooophy/Cpp-Primer&#34;&gt;C++ Primer 5 Answers(C++11/14)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huangmingchuan/Cpp_Primer_Answers&#34;&gt;《C++ Primer》第五版中文版习题答案&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;参与贡献&lt;/h2&gt; &#xA;&lt;p&gt;本仓库由多位小伙伴一起参与编写，欢迎大家对本仓库进行补充，一起帮大家更好地理解这本“大部头”。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/pytorch</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/pytorch/pytorch</id>
    <link href="https://github.com/pytorch/pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/pytorch-logo-dark.png&#34; alt=&#34;PyTorch Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt; &#xA; &lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.&lt;/p&gt; &#xA;&lt;p&gt;Our trunk health (Continuous Integration signals) can be found at &lt;a href=&#34;https://hud.pytorch.org/ci/pytorch/pytorch/master&#34;&gt;hud.pytorch.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#more-about-pytorch&#34;&gt;More About PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#a-gpu-ready-tensor-library&#34;&gt;A GPU-Ready Tensor Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#dynamic-neural-networks-tape-based-autograd&#34;&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#python-first&#34;&gt;Python First&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#imperative-experiences&#34;&gt;Imperative Experiences&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#fast-and-lean&#34;&gt;Fast and Lean&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#extensions-without-pain&#34;&gt;Extensions Without Pain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#binaries&#34;&gt;Binaries&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#nvidia-jetson-platforms&#34;&gt;NVIDIA Jetson Platforms&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#from-source&#34;&gt;From Source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-dependencies&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#get-the-pytorch-source&#34;&gt;Get the PyTorch Source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-pytorch&#34;&gt;Install PyTorch&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#adjust-build-options-optional&#34;&gt;Adjust Build Options (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#docker-image&#34;&gt;Docker Image&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#using-pre-built-images&#34;&gt;Using pre-built images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-image-yourself&#34;&gt;Building the image yourself&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-documentation&#34;&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#previous-versions&#34;&gt;Previous Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#releases-and-contributing&#34;&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34;&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a Tensor library like NumPy, with strong GPU support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html&#34;&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/jit.html&#34;&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/nn.html&#34;&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/multiprocessing.html&#34;&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/data.html&#34;&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, PyTorch is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A replacement for NumPy to use the power of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;A GPU-Ready Tensor Library&lt;/h3&gt; &#xA;&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/tensor_illustration.png&#34; alt=&#34;Tensor illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.&lt;/p&gt; &#xA;&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt; &#xA;&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt; &#xA;&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as &lt;a href=&#34;https://github.com/twitter/torch-autograd&#34;&gt;torch-autograd&lt;/a&gt;, &lt;a href=&#34;https://github.com/HIPS/autograd&#34;&gt;autograd&lt;/a&gt;, &lt;a href=&#34;https://chainer.org&#34;&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;While this technique is not unique to PyTorch, it&#39;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif&#34; alt=&#34;Dynamic graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python First&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; / &lt;a href=&#34;https://www.scipy.org/&#34;&gt;SciPy&lt;/a&gt; / &lt;a href=&#34;https://scikit-learn.org&#34;&gt;scikit-learn&lt;/a&gt; etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt; and &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;. Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt; &#xA;&lt;h3&gt;Imperative Experiences&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn&#39;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Lean&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries such as &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel MKL&lt;/a&gt; and NVIDIA (&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.&lt;/p&gt; &#xA;&lt;p&gt;Hence, PyTorch is quite fast – whether you run small or large neural networks.&lt;/p&gt; &#xA;&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We&#39;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions Without Pain&lt;/h3&gt; &#xA;&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch&#39;s Tensor API was designed to be straightforward and with minimal abstractions.&lt;/p&gt; &#xA;&lt;p&gt;You can write new neural network layers in Python using the torch API &lt;a href=&#34;https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html&#34;&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;a tutorial here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/extension-cpp&#34;&gt;an example here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;p&gt;Commands to install binaries via Conda or pip wheels are on our website: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;NVIDIA Jetson Platforms&lt;/h4&gt; &#xA;&lt;p&gt;Python wheels for NVIDIA&#39;s Jetson Nano, Jetson TX2, and Jetson AGX Xavier are provided &lt;a href=&#34;https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048&#34;&gt;here&lt;/a&gt; and the L4T container is published &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They require JetPack 4.2 and above, and &lt;a href=&#34;https://github.com/dusty-nv&#34;&gt;@dusty-nv&lt;/a&gt; and &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;@ptrblck&lt;/a&gt; are maintaining them.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;If you are installing from source, you will need Python 3.7 or later and a C++14 compiler. Also, we highly recommend installing an &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt; &#xA;&lt;p&gt;Once you have &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; installed, here are the instructions.&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with CUDA support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVIDIA CUDA&lt;/a&gt; 10.2 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;NVIDIA cuDNN&lt;/a&gt; v7 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/ax3l/9489132&#34;&gt;Compiler&lt;/a&gt; compatible with CUDA Note: You could refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf&#34;&gt;cuDNN Support Matrix&lt;/a&gt; for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardwares&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable CUDA support, export the environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are building for NVIDIA&#39;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are &lt;a href=&#34;https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/&#34;&gt;available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with ROCm support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html&#34;&gt;AMD ROCm&lt;/a&gt; 4.0 and above installation&lt;/li&gt; &#xA; &lt;li&gt;ROCm is currently supported only for Linux systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable ROCm support, export the environment variable &lt;code&gt;USE_ROCM=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Common&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# CUDA only: Add LAPACK support for the GPU if needed&#xA;conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed&#xA;conda install pkg-config libuv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed.&#xA;# Distributed package support on Windows is a prototype feature and is subject to changes.&#xA;conda install -c conda-forge libuv=1.39&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Get the PyTorch Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/pytorch/pytorch&#xA;cd pytorch&#xA;# if you are updating an existing checkout&#xA;git submodule sync&#xA;git submodule update --init --recursive --jobs 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are compiling for ROCm, you must run this command first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/amd_build/build_amd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are using &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt;, you may experience an error caused by the linker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized&#xA;collect2: error: ld returned 1 exit status&#xA;error: command &#39;g++&#39; failed with exit status 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is caused by &lt;code&gt;ld&lt;/code&gt; from Conda environment shadowing the system &lt;code&gt;ld&lt;/code&gt;. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.7.6+ and 3.8.1+.&lt;/p&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA is not supported on macOS.&lt;/p&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;p&gt;Choose Correct Visual Studio Version.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes there are regressions in new versions of Visual Studio, so it&#39;s best to use the same Visual Studio Version &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.circleci/scripts/vs_install.ps1&#34;&gt;16.8.5&lt;/a&gt; as Pytorch CI&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from &lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;https://visualstudio.microsoft.com/visual-cpp-build-tools/&lt;/a&gt;. The build tools &lt;em&gt;do not&lt;/em&gt; come with Visual Studio Code by default.&lt;/p&gt; &#xA;&lt;p&gt;If you want to build legacy python code, please refer to &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md#building-on-legacy-code-and-cuda&#34;&gt;Building on legacy code and CUDA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build with CPU&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s fairly easy to build with CPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;conda activate&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#39;ll need to manually download the library and set up the building environment by tweaking &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;LIB&lt;/code&gt;. The instruction &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/notes/windows.rst#building-from-source&#34;&gt;here&lt;/a&gt; is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.&lt;/p&gt; &#xA;&lt;p&gt;Build with CUDA&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm&#34;&gt;NVTX&lt;/a&gt; is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called &#34;Nsight Compute&#34;. To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. &lt;br&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.&lt;/p&gt; &#xA;&lt;p&gt;Additional libraries such as &lt;a href=&#34;https://developer.nvidia.com/magma&#34;&gt;Magma&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN, a.k.a MKLDNN or DNNL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/mozilla/sccache&#34;&gt;Sccache&lt;/a&gt; are often needed. Please refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/tree/master/.jenkins/pytorch/win-test-helpers/installation-helpers&#34;&gt;installation-helper&lt;/a&gt; to install them.&lt;/p&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat&#34;&gt;build_pytorch.bat&lt;/a&gt; script for some other environment variables configurations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmd&#xA;&#xA;:: Set the environment variables after you have downloaded and upzipped the mkl package,&#xA;:: else CMake would throw an error as `Could NOT find OpenMP`.&#xA;set CMAKE_INCLUDE_PATH={Your directory}\mkl\include&#xA;set LIB={Your directory}\mkl\lib;%LIB%&#xA;&#xA;:: Read the content in the previous section carefully before you proceed.&#xA;:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&#xA;:: &#34;Visual Studio 2019 Developer Command Prompt&#34; will be run automatically.&#xA;:: Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&#xA;set CMAKE_GENERATOR_TOOLSET_VERSION=14.27&#xA;set DISTUTILS_USE_SDK=1&#xA;for /f &#34;usebackq tokens=*&#34; %i in (`&#34;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&#34; -version [15^,17^) -products * -latest -property installationPath`) do call &#34;%i\VC\Auxiliary\Build\vcvarsall.bat&#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%&#xA;&#xA;:: [Optional] If you want to override the CUDA host compiler&#xA;set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe&#xA;&#xA;python setup.py install&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Adjust Build Options (Optional)&lt;/h5&gt; &#xA;&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.&lt;/p&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;h4&gt;Using pre-built images&lt;/h4&gt; &#xA;&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building the image yourself&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;# images are tagged as docker.io/${your_docker_username}/pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the Documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build documentation in various formats, you will need &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the &lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt; &#xA;&lt;p&gt;If you get a katex error run &lt;code&gt;npm install katex&lt;/code&gt;. If it persists, try &lt;code&gt;npm install -g katex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Previous Versions&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found on &lt;a href=&#34;https://pytorch.org/previous-versions&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Three-pointers to get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;Examples: easy to understand PyTorch code across all domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/&#34;&gt;The API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/GLOSSARY.md&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;PyTorch Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;Intro to Deep Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34;&gt;Intro to Machine Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-networks-with-pytorch&#34;&gt;Deep Neural Networks with PyTorch from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PyTorch&#34;&gt;PyTorch Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/&#34;&gt;PyTorch Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw&#34;&gt;PyTorch YouTube&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: Discuss implementations, research, etc. &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt; &#xA; &lt;li&gt;Slack: The &lt;a href=&#34;https://pytorch.slack.com/&#34;&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href=&#34;https://goo.gl/forms/PP1AGvNHpSaJP8to1&#34;&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: &lt;a href=&#34;https://eepurl.com/cbG0rv&#34;&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Page: Important announcements about PyTorch. &lt;a href=&#34;https://www.facebook.com/pytorch&#34;&gt;https://www.facebook.com/pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For brand guidelines, please visit our website at &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a 90-day release cycle (major releases). Please let us know if you encounter a bug by &lt;a href=&#34;https://github.com/pytorch/pytorch/issues&#34;&gt;filing an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about making a contribution to Pytorch, please see our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch is currently maintained by &lt;a href=&#34;https://apaszke.github.io/&#34;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&#34;https://github.com/colesbury&#34;&gt;Sam Gross&lt;/a&gt;, &lt;a href=&#34;http://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt; and &lt;a href=&#34;https://github.com/gchanan&#34;&gt;Gregory Chanan&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project is unrelated to &lt;a href=&#34;https://github.com/hughperkins/pytorch&#34;&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DefTruth/lite.ai.toolkit</title>
    <updated>2022-05-29T01:59:58Z</updated>
    <id>tag:github.com,2022-05-29:/DefTruth/lite.ai.toolkit</id>
    <link href="https://github.com/DefTruth/lite.ai.toolkit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🛠 A lite C++ toolkit of awesome AI models with ONNXRuntime, NCNN, MNN and TNN. YOLOX, YOLOP, MODNet, YOLOR, NanoDet, YOLOX, SCRFD, YOLOX . MNN, NCNN, TNN, ONNXRuntime, CPU/GPU.&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;lite.ai.toolkit-Introduction&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/logo-v3.png&#34; alt=&#34;logo-v3&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/mac|linux|win-pass-brightgreen.svg&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/device-GPU/CPU-yellow.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/license-GPLv3-blue.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/onnxruntime-1.10.0-turquoise.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/mnn-1.2.0-hotpink.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/ncnn-1.0.21-orange.svg?sanitize=true&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/tnn-0.3.0-blue.svg?sanitize=true&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;🛠&lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt;: A lite C++ toolkit of awesome AI models, such as &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-object-detection&#34;&gt;Object Detection&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-face-detection&#34;&gt;Face Detection&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-face-recognition&#34;&gt;Face Recognition&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-segmentation&#34;&gt;Segmentation&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-matting&#34;&gt;Matting&lt;/a&gt;, etc. See &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Model-Zoo&#34;&gt;Model Zoo&lt;/a&gt; and &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;ONNX Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md&#34;&gt;MNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md&#34;&gt;TNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md&#34;&gt;NCNN Hub&lt;/a&gt;. [❤️ Star 🌟👆🏻 this repo to support me if it does any helps to you, thanks ~ ]&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_yolov5_1.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/efficientdet_d0.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/street.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_ultraface.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_deeplabv3_resnet101.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_mosaic.jpg&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/teslai.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/tesla.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3i.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/yolop1.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/yolop1.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/yolop2.png&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/yolop2.gif&#34; height=&#34;90px&#34; width=&#34;90px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/README.zh.md&#34;&gt;中文文档&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-MacOS&#34;&gt;MacOS&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Linux&#34;&gt;Linux&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Windows&#34;&gt;Windows&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Core Features 👏👋&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Core-Features&#34;&gt;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simply and User friendly.&lt;/strong&gt; Simply and Consistent syntax like &lt;strong&gt;lite::cv::Type::Class&lt;/strong&gt;, see &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minimum Dependencies.&lt;/strong&gt; Only &lt;strong&gt;OpenCV&lt;/strong&gt; and &lt;strong&gt;ONNXRuntime&lt;/strong&gt; are required by default, see &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Lite.AI.ToolKit&#34;&gt;build&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lots of Algorithm Modules.&lt;/strong&gt; Contains almost &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;300+&lt;/a&gt;&lt;/strong&gt; C++ re-implementations and &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;500+&lt;/a&gt;&lt;/strong&gt; weights.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations 🎉🎉&lt;/h2&gt; &#xA;&lt;p&gt;Consider to cite it as follows if you use &lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; in your projects.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{lite.ai.toolkit2021,&#xA;  title={lite.ai.toolkit: A lite C++ toolkit of awesome AI models.},&#xA;  url={https://github.com/DefTruth/lite.ai.toolkit},&#xA;  note={Open-source software available at https://github.com/DefTruth/lite.ai.toolkit},&#xA;  author={Yan Jun},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;About Training 🤓👀&lt;/h2&gt; &#xA;&lt;p&gt;A high level Training and Evaluating Toolkit for Face Landmarks Detection is available at &lt;a href=&#34;https://github.com/DefTruth/torchlm&#34;&gt;torchlm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloads &amp;amp; RoadMap ✅&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-RoadMap&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/lite.ai.toolkit-roadmap-v0.1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some prebuilt lite.ai.toolkit libs for MacOS(x64) and Linux(x64) are available, you can download the libs from the release links. Further, prebuilt libs for Windows(x64) and Android will be coming soon ~ Please, see &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/48&#34;&gt;issues#48&lt;/a&gt; for more details of the prebuilt plan and refer to &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases&#34;&gt;releases&lt;/a&gt; for more available prebuilt libs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&#34;&gt;lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&#34;&gt;lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&#34;&gt;lite0.1.1-osx10.15.x-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&#34;&gt;lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.8.1.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&#34;&gt;lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.9.0.zip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/releases/download/v0.1.1/lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&#34;&gt;lite0.1.1-ubuntu18.04-ocv4.5.2-ffmpeg4.2.2-onnxruntime1.10.0.zip&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In Linux, in order to link the prebuilt libs, you need to export &lt;code&gt;lite.ai.toolkit/lib&lt;/code&gt; to LD_LIBRARY_PATH first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export LD_LIBRARY_PATH=YOUR-PATH-TO/lite.ai.toolkit/lib:$LD_LIBRARY_PATH&#xA;export LIBRARY_PATH=YOUR-PATH-TO/lite.ai.toolkit/lib:$LIBRARY_PATH  # (may need)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Setup 👀&lt;/h2&gt; &#xA;&lt;p&gt;To quickly setup &lt;code&gt;lite.ai.toolkit&lt;/code&gt;, you can follow the &lt;code&gt;CMakeLists.txt&lt;/code&gt; listed as belows. 👇👀&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)&#xA;include_directories(${LITE_AI_DIR}/include)&#xA;link_directories(${LITE_AI_DIR}/lib})&#xA;set(TOOLKIT_LIBS lite.ai.toolkit onnxruntime)&#xA;set(OpenCV_LIBS opencv_core opencv_imgcodecs opencv_imgproc opencv_video opencv_videoio)&#xA;&#xA;add_executable(lite_yolov5 examples/test_lite_yolov5.cpp)&#xA;target_link_libraries(lite_yolov5 ${TOOLKIT_LIBS} ${OpenCV_LIBS})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contents 📖💡&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Core-Features&#34;&gt;Core Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Quick-Start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-RoadMap&#34;&gt;RoadMap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Important-Updates&#34;&gt;Important Updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Supported-Models-Matrix&#34;&gt;Supported Models Matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Build-Lite.AI.ToolKit&#34;&gt;Build Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Model-Zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-References&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Contribute&#34;&gt;Contribute&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. Quick Start 🌟🌟&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Quick-Start&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example0: Object Detection using &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/yolov5s.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_yolov5_1.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_yolov5_1.jpg&#34;;&#xA;&#xA;  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); &#xA;  std::vector&amp;lt;lite::types::Boxf&amp;gt; detected_boxes;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  yolov5-&amp;gt;detect(img_bgr, detected_boxes);&#xA;  &#xA;  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);&#xA;  cv::imwrite(save_img_path, img_bgr);  &#xA;  &#xA;  delete yolov5;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;&lt;div align=&#39;center&#39;&gt;&#xA;  &lt;img src=&#34;docs/resources/scrfd-mgmatting-nanodetplus.jpg&#34; height=&#34;250px&#34; width=&#34;750px&#34; &gt;&#xA;&lt;/div&gt;   &#xA;----&gt; &#xA;&lt;h2&gt;2. Important Updates 🆕&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Important-Updates&#34;&gt;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click here to see details of Important Updates! &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;C++&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Awesome&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2022/04/03】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_modnet.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2011.11961.pdf&#34;&gt;AAAI 2022&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ZHKKKe/MODNet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;matting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2022/03/23】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNtet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet98.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.03771&#34;&gt;CVPR 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/jhb86253817/PIPNet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;face::align&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2022/01/19】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLO5Face&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolo5face.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.12931&#34;&gt;arXiv 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/deepcam-cn/yolov5-face.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;face::detect&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2022/01/07】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;SCRFD&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_scrfd.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.04714&#34;&gt;CVPR 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/deepinsight/insightface.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;face::detect&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/12/27】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDetPlus&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet_plus.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/449912627&#34;&gt;blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/12/08】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;MGMatting&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mg_matting.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.06722&#34;&gt;CVPR 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yucornetto/MGMatting.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;matting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/11/11】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases/tag/v6.0&#34;&gt;YoloV5_V_6_0&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zenodo.org/record/5563715#.YbXffH1Bzfs&#34;&gt;doi&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases/tag/v6.0&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ultralytics/yolov5.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/10/26】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/tag/0.1.1rc0&#34;&gt;YoloX_V_0_1_1&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.08430&#34;&gt;arXiv 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Megvii-BaseDetection/YOLOX.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/10/02】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDet&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/306530300&#34;&gt;blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RangiLyu/nanodet.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/09/20】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_rvm.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11515&#34;&gt;WACV 2022&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PeterL1n/RobustVideoMatting.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;matting&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;【2021/09/02】&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;YOLOP&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolop.cpp&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.11250&#34;&gt;arXiv 2021&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;code&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hustvl/YOLOP.svg?style=social&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;detection&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;3. Supported Models Matrix&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Supported-Models-Matrix&#34;&gt;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/ = not supported now.&lt;/li&gt; &#xA; &lt;li&gt;✅ = known work and official supported now.&lt;/li&gt; &#xA; &lt;li&gt;✔️ = known work, but unofficial supported now.&lt;/li&gt; &#xA; &lt;li&gt;❔ = in my plan, but not coming soon, maybe a few months later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ONNXRuntime&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;NCNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TNN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MacOS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Android&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YoloV5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/yolov3&#34;&gt;YoloV3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;236M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov3.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/tiny-yolov3&#34;&gt;TinyYoloV3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tiny_yolov3.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/argusswift/YOLOv4-pytorch&#34;&gt;YoloV4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;176M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov4.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/ssd&#34;&gt;SSD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ssd.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/object_detection_segmentation/ssd-mobilenetv1&#34;&gt;SSDMobileNetV1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ssd_mobilenetv1.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YoloX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolox.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bubbliiiing/yolov4-tiny-pytorch&#34;&gt;TinyYoloV4VOC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tiny_yolov4_voc.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bubbliiiing/yolov4-tiny-pytorch&#34;&gt;TinyYoloV4COCO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tiny_yolov4_coco.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;YoloR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolor.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;ScaledYoloV4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;270M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_scaled_yolov4.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch&#34;&gt;EfficientDet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientdet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch&#34;&gt;EfficientDetD7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;220M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientdet_d7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch&#34;&gt;EfficientDetD8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;322M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientdet_d8.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;YOLOP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolop.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDetPlus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet_plus.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;NanoDetEffi...&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet_efficientnet_lite.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YoloX_V_0_1_1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolox_v0.1.1.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YoloV5_V_6_0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_v6.0.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch&#34;&gt;GlintArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_glint_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch&#34;&gt;GlintCosFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_glint_cosface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc&#34;&gt;GlintPartialFC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;170M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_glint_partial_fc.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/timesler/facenet-pytorch&#34;&gt;FaceNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_facenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZhaoJ9014/face.evoLVe.PyTorch&#34;&gt;FocalArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;166M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_focal_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZhaoJ9014/face.evoLVe.PyTorch&#34;&gt;FocalAsiaArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;166M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_focal_asia_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Tencent/TFace/tree/master/tasks/distfc&#34;&gt;TencentCurricularFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;249M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tencent_curricular_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Tencent/TFace/tree/master/tasks/cifp&#34;&gt;TencentCifpFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;130M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_tencent_cifp_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/louis-she/center-loss.pytorch&#34;&gt;CenterLossFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;280M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_center_loss_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/clcarwin/sphereface_pytorch&#34;&gt;SphereFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_sphere_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/penincillin/DREAM&#34;&gt;PoseRobustFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pose_robust_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/penincillin/DREAM&#34;&gt;NaivePoseRobustFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_naive_pose_robust_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Xiaoccer/MobileFaceNet_Pytorch&#34;&gt;MobileFaceNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobile_facenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch&#34;&gt;CavaGhostArcFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_cava_ghost_arcface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch&#34;&gt;CavaCombinedFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;250M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_cava_combined_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/grib0ed0v/face_recognition.pytorch&#34;&gt;MobileSEFocalFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;faceid&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilese_focal_face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_rvm.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;MGMatting&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;113M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mg_matting.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_modnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZHKKKe/MODNet&#34;&gt;MODNetDyn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_modnet_dyn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/BackgroundMattingV2&#34;&gt;BackgroundMattingV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_backgroundmattingv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PeterL1n/BackgroundMattingV2&#34;&gt;BackgroundMattingV2Dyn&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;matting&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_backgroundmattingv2_dyn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34;&gt;UltraFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ultraface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/biubug6/Pytorch_Retinaface&#34;&gt;RetinaFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.6M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_retinaface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/zisianw/FaceBoxes.PyTorch&#34;&gt;FaceBoxes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_faceboxes.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/FaceBoxesV2&#34;&gt;FaceBoxesV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_faceboxesv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;SCRFD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_scrfd.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLO5Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolo5face.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Hsintao/pfld_106_face_landmarks&#34;&gt;PFLD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pfld.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/polarisZhao/PFLD-pytorch&#34;&gt;PFLD98&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pfld98.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;MobileNetV268&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilenetv2_68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;MobileNetV2SE68&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilenetv2_se_68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;PFLD68&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pfld68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Single430/FaceLandmark1000&#34;&gt;FaceLandmark1000&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_face_landmarks_1000.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet98&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet98.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet68&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet68.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet29&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet29.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/jhb86253817/PIPNet&#34;&gt;PIPNet19&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_pipnet19.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/omasaht/headpose-fsanet-pytorch&#34;&gt;FSANet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::pose&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fsanet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;AgeGoogleNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_age_googlenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;GenderGoogleNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_gender_googlenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/body_analysis/emotion_ferplus&#34;&gt;EmotionFerPlus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_emotion_ferplus.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;VGG16Age&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;514M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_vgg16_age.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/body_analysis/age_gender&#34;&gt;VGG16Gender&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_vgg16_gender.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/oukohou/SSR_Net_Pytorch&#34;&gt;SSRNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;190K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ssrnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;EfficientEmotion7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficient_emotion7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;EfficientEmotion8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficient_emotion8.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;MobileEmotion7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobile_emotion7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;ReXNetEmotion7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::attr&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_rexnet_emotion7.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/classification/efficientnet-lite4&#34;&gt;EfficientNetLite4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_efficientnet_lite4.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/classification/shufflenet&#34;&gt;ShuffleNetV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_shufflenetv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_densenet/&#34;&gt;DenseNet121&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_densenet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_ghostnet/&#34;&gt;GhostNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ghostnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_hardnet//&#34;&gt;HdrDNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_hardnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_ibnnet/&#34;&gt;IBNNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_ibnnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_mobilenet_v2/&#34;&gt;MobileNetV2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_mobilenetv2.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_resnet/&#34;&gt;ResNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_resnet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_resnext/&#34;&gt;ResNeXt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;95M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_resnext.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&#34;&gt;DeepLabV3ResNet101&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;232M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_deeplabv3_resnet101.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_fcn_resnet101/&#34;&gt;FCNResNet101&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;207M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;segmentation&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fcn_resnet101.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/onnx/models/raw/master/vision/style_transfer/fast_neural_style&#34;&gt;FastStyleTransfer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;style&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_fast_style_transfer.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/richzhang/colorization&#34;&gt;Colorizer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;123M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;colorization&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_colorizer.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/niazwazir/SUB_PIXEL_CNN&#34;&gt;SubPixelCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;234K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;resolution&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_subpixel_cnn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/niazwazir/SUB_PIXEL_CNN&#34;&gt;SubPixelCNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;234K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;resolution&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_subpixel_cnn.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/quarrying/quarrying-insect-id&#34;&gt;InsectDet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_insectdet.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/quarrying/quarrying-insect-id&#34;&gt;InsectID&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_insectid.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/quarrying/quarrying-plant-id&#34;&gt;PlantID&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;classification&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_plantid.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/deepcam-cn/yolov5-face&#34;&gt;YOLOv5BlazeFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::detect&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_blazeface.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases/tag/v6.1&#34;&gt;YoloV5_V_6_1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;detection&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_yolov5_v6.1.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google/mediapipe&#34;&gt;FaceMesh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align3d&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_facemesh.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google/mediapipe&#34;&gt;IrisLandmarks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.6M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;face::align3d&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_iris_landmarks.cpp&#34;&gt;demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✔️&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;4. Build Docs.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-MacOS&#34;&gt;&lt;/div&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-Lite.AI.ToolKit&#34;&gt;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS: Build the shared lib of &lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; for &lt;strong&gt;MacOS&lt;/strong&gt; from sources. Note that Lite.Ai.ToolKit uses &lt;strong&gt;onnxruntime&lt;/strong&gt; as default backend, for the reason that onnxruntime supports the most of onnx&#39;s operators.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    git clone --depth=1 https://github.com/DefTruth/lite.ai.toolkit.git  # latest&#xA;    cd lite.ai.toolkit &amp;amp;&amp;amp; sh ./build.sh  # On MacOS, you can use the built OpenCV, ONNXRuntime, MNN, NCNN and TNN libs in this repo.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-Linux&#34;&gt;&lt;/div&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Build-Windows&#34;&gt;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;💡 Linux and Windows. &lt;/summary&gt; &#xA; &lt;h3&gt;Linux and Windows.&lt;/h3&gt; &#xA; &lt;p&gt;⚠️ &lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; is not directly support Linux and Windows now. For Linux and Windows, you need to build or download(if have official builts) the shared libs of &lt;strong&gt;OpenCV&lt;/strong&gt;、&lt;strong&gt;ONNXRuntime&lt;/strong&gt; and any other Engines(like MNN, NCNN, TNN) firstly, then put the headers into the specific directories or just let these directories unchange(use the headers offer by this repo, the header file of the dependent library of this project is directly copied from the corresponding official library). However, the dynamic libraries under different operating systems need to be recompiled or downloaded. MacOS users can directly use the dynamic libraries of each dependent library provided by this project:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/opencv2&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-opencv/include/opencv4/opencv2 lite.ai.toolkit/opencv2&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/onnxruntime&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-onnxruntime/include/onnxruntime lite.ai.toolkit/onnxruntime&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/MNN&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-MNN/include/MNN lite.ai.toolkit/MNN&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/ncnn&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-ncnn/include/ncnn lite.ai.toolkit/ncnn&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/tnn&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp -r you-path-to-downloaded-or-built-TNN/include/tnn lite.ai.toolkit/tnn&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;and put the libs into &lt;strong&gt;lite.ai.toolkit/lib/(linux|windows)&lt;/strong&gt; directory. Please reference the build-docs&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; for &lt;strong&gt;third_party&lt;/strong&gt;.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;lite.ai.toolkit/lib/(linux|windows)&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cp you-path-to-downloaded-or-built-opencv/lib/*opencv* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-onnxruntime/lib/*onnxruntime* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-MNN/lib/*MNN* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-ncnn/lib/*ncnn* lite.ai.toolkit/lib/(linux|windows)/&#xA;  cp you-path-to-downloaded-or-built-TNN/lib/*TNN* lite.ai.toolkit/lib/(linux|windows)/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note, your also need to install ffmpeg(&amp;lt;=4.2.2) in Linux to support the opencv videoio module. See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/6&#34;&gt;issue#203&lt;/a&gt;. In MacOS, ffmpeg4.2.2 was been package into lite.ai.toolkit, thus, no installation need in OSX. In Windows, ffmpeg was been package into opencv dll prebuilt by the team of opencv. Please make sure -DWITH_FFMPEG=ON and check the configuration info when building opencv.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;first, build ffmpeg(&amp;lt;=4.2.2) from source.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone --depth=1 https://git.ffmpeg.org/ffmpeg.git -b n4.2.2&#xA;cd ffmpeg&#xA;./configure --enable-shared --disable-x86asm --prefix=/usr/local/opt/ffmpeg --disable-static&#xA;make -j8&#xA;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;then, build opencv with -DWITH_FFMPEG=ON, just like&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash&#xA;&#xA;mkdir build&#xA;cd build&#xA;&#xA;cmake .. \&#xA;  -D CMAKE_BUILD_TYPE=Release \&#xA;  -D CMAKE_INSTALL_PREFIX=your-path-to-custom-dir \&#xA;  -D BUILD_TESTS=OFF \&#xA;  -D BUILD_PERF_TESTS=OFF \&#xA;  -D BUILD_opencv_python3=OFF \&#xA;  -D BUILD_opencv_python2=OFF \&#xA;  -D BUILD_SHARED_LIBS=ON \&#xA;  -D BUILD_opencv_apps=OFF \&#xA;  -D WITH_FFMPEG=ON &#xA;  &#xA;make -j8&#xA;make install&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;after built opencv, you can follow the steps to build lite.ai.toolkit.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Windows: You can reference to &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/6&#34;&gt;issue#6&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Linux: The Docs and Docker image for Linux will be coming soon ~ &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/2&#34;&gt;issue#2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Happy News !!! : 🚀 You can download the latest &lt;strong&gt;ONNXRuntime&lt;/strong&gt; official built libs of Windows, Linux, MacOS and Arm !!! Both CPU and GPU versions are available. No more attentions needed pay to build it from source. Download the official built libs from &lt;a href=&#34;https://github.com/microsoft/onnxruntime/releases&#34;&gt;v1.8.1&lt;/a&gt;. I have used version 1.7.0 for Lite.Ai.ToolKit now, you can download it from &lt;a href=&#34;https://github.com/microsoft/onnxruntime/releases/tag/v1.7.0&#34;&gt;v1.7.0&lt;/a&gt;, but version 1.8.1 should also work, I guess ~ 🙃🤪🍀. For &lt;strong&gt;OpenCV&lt;/strong&gt;, try to build from source(Linux) or down load the official built(Windows) from &lt;a href=&#34;https://github.com/opencv/opencv/releases&#34;&gt;OpenCV 4.5.3&lt;/a&gt;. Then put the includes and libs into specific directory of Lite.Ai.ToolKit.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;GPU Compatibility for Windows: See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/10&#34;&gt;issue#10&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;GPU Compatibility for Linux: See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/97&#34;&gt;issue#97&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;🔑️ How to link Lite.Ai.ToolKit?&lt;/summary&gt; * To link Lite.Ai.ToolKit, you can follow the CMakeLists.txt listed belows. &#xA; &lt;pre&gt;&lt;code class=&#34;language-cmake&#34;&gt;cmake_minimum_required(VERSION 3.10)&#xA;project(lite.ai.toolkit.demo)&#xA;&#xA;set(CMAKE_CXX_STANDARD 11)&#xA;&#xA;# setting up lite.ai.toolkit&#xA;set(LITE_AI_DIR ${CMAKE_SOURCE_DIR}/lite.ai.toolkit)&#xA;set(LITE_AI_INCLUDE_DIR ${LITE_AI_DIR}/include)&#xA;set(LITE_AI_LIBRARY_DIR ${LITE_AI_DIR}/lib)&#xA;include_directories(${LITE_AI_INCLUDE_DIR})&#xA;link_directories(${LITE_AI_LIBRARY_DIR})&#xA;&#xA;set(OpenCV_LIBS&#xA;        opencv_highgui&#xA;        opencv_core&#xA;        opencv_imgcodecs&#xA;        opencv_imgproc&#xA;        opencv_video&#xA;        opencv_videoio&#xA;        )&#xA;# add your executable&#xA;set(EXECUTABLE_OUTPUT_PATH ${CMAKE_SOURCE_DIR}/examples/build)&#xA;&#xA;add_executable(lite_rvm examples/test_lite_rvm.cpp)&#xA;target_link_libraries(lite_rvm&#xA;        lite.ai.toolkit&#xA;        onnxruntime&#xA;        MNN  # need, if built lite.ai.toolkit with ENABLE_MNN=ON,  default OFF&#xA;        ncnn # need, if built lite.ai.toolkit with ENABLE_NCNN=ON, default OFF &#xA;        TNN  # need, if built lite.ai.toolkit with ENABLE_TNN=ON,  default OFF &#xA;        ${OpenCV_LIBS})  # link lite.ai.toolkit &amp;amp; other libs.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ./build/lite.ai.toolkit/lib &amp;amp;&amp;amp; otool -L liblite.ai.toolkit.0.0.1.dylib &#xA;liblite.ai.toolkit.0.0.1.dylib:&#xA;        @rpath/liblite.ai.toolkit.0.0.1.dylib (compatibility version 0.0.1, current version 0.0.1)&#xA;        @rpath/libopencv_highgui.4.5.dylib (compatibility version 4.5.0, current version 4.5.2)&#xA;        @rpath/libonnxruntime.1.7.0.dylib (compatibility version 0.0.0, current version 1.7.0)&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ../ &amp;amp;&amp;amp; tree .&#xA;├── bin&#xA;├── include&#xA;│&amp;nbsp;&amp;nbsp; ├── lite&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── backend.h&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; ├── config.h&#xA;│&amp;nbsp;&amp;nbsp; │&amp;nbsp;&amp;nbsp; └── lite.h&#xA;│&amp;nbsp;&amp;nbsp; └── ort&#xA;└── lib&#xA;    └── liblite.ai.toolkit.0.0.1.dylib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Run the built examples:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ./build/lite.ai.toolkit/bin &amp;amp;&amp;amp; ls -lh | grep lite&#xA;-rwxr-xr-x  1 root  staff   301K Jun 26 23:10 liblite.ai.toolkit.0.0.1.dylib&#xA;...&#xA;-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov4&#xA;-rwxr-xr-x  1 root  staff   196K Jun 26 23:10 lite_yolov5&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./lite_yolov5&#xA;LITEORT_DEBUG LogId: ../../../hub/onnx/cv/yolov5s.onnx&#xA;=============== Input-Dims ==============&#xA;...&#xA;detected num_anchors: 25200&#xA;generate_bboxes num: 66&#xA;Default Version Detected Boxes Num: 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To link &lt;code&gt;lite.ai.toolkit&lt;/code&gt; shared lib. You need to make sure that &lt;code&gt;OpenCV&lt;/code&gt; and &lt;code&gt;onnxruntime&lt;/code&gt; are linked correctly. A minimum example to show you how to link the shared lib of Lite.AI.ToolKit correctly for your own project can be found at &lt;a href=&#34;https://github.com/DefTruth/RobustVideoMatting-ncnn-mnn-tnn-onnxruntime/raw/main/CMakeLists.txt&#34;&gt;CMakeLists.txt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;5. Model Zoo.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Model-Zoo&#34;&gt;&lt;/div&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-2&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lite.Ai.ToolKit&lt;/strong&gt; contains &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;80+&lt;/a&gt;&lt;/strong&gt; AI models with &lt;strong&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;500+&lt;/a&gt;&lt;/strong&gt; frozen pretrained files now. Most of the files are converted by myself. You can use it through &lt;strong&gt;lite::cv::Type::Class&lt;/strong&gt; syntax, such as &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-object-detection&#34;&gt;lite::cv::detection::YoloV5&lt;/a&gt;&lt;/strong&gt;. More details can be found at &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;Examples for Lite.Ai.ToolKit&lt;/a&gt;. Note, for Google Drive, I can not upload all the *.onnx files because of the storage limitation (15G).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;File&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Baidu Drive&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Google Drive&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Docker Hub&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hub (Docs)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ONNX&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1elUGcx7CZkkjEoYhTMwTRQ&#34;&gt;Baidu Drive&lt;/a&gt; code: 8gin&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1p6uBcxGeyS1exc-T61vL8YRhwjYL4iD2?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-onnx-hub/tags&#34;&gt;ONNX Docker v0.1.22.01.08 (28G), v0.1.22.02.02 (400M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;ONNX Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1KyO-bCYUv6qPq2M8BH_Okg&#34;&gt;Baidu Drive&lt;/a&gt; code: 9v63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-mnn-hub/tags&#34;&gt;MNN Docker v0.1.22.01.08 (11G), v0.1.22.02.02 (213M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md&#34;&gt;MNN Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NCNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1hlnqyNsFbMseGFWscgVhgQ&#34;&gt;Baidu Drive&lt;/a&gt; code: sc7f&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-ncnn-hub/tags&#34;&gt;NCNN Docker v0.1.22.01.08 (9G), v0.1.22.02.02 (197M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md&#34;&gt;NCNN Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1lvM2YKyUbEc5HKVtqITpcw&#34;&gt;Baidu Drive&lt;/a&gt; code: 6o6k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❔&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://hub.docker.com/r/qyjdefdocker/lite.ai.toolkit-tnn-hub/tags&#34;&gt;TNN Docker v0.1.22.01.08 (11G), v0.1.22.02.02 (217M)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md&#34;&gt;TNN Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.01.08  # (28G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08   # (11G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.01.08  # (9G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.01.08   # (11G)&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.02.02  # (400M) + YOLO5Face&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.02.02   # (213M) + YOLO5Face&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.02.02  # (197M) + YOLO5Face&#xA;  docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.02.02   # (217M) + YOLO5Face&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; ❇️ Lite.Ai.ToolKit modules.&lt;/summary&gt; &#xA; &lt;h3&gt;Namespace and Lite.Ai.ToolKit modules.&lt;/h3&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Namespace&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Details&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::detection&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Object Detection. one-stage and anchor-free detectors, YoloV5, YoloV4, SSD, etc. ✅&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::classification&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Image Classification. DensNet, ShuffleNet, ResNet, IBNNet, GhostNet, etc. ✅&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::faceid&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Recognition. ArcFace, CosFace, CurricularFace, etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Analysis. &lt;em&gt;detect&lt;/em&gt;, &lt;em&gt;align&lt;/em&gt;, &lt;em&gt;pose&lt;/em&gt;, &lt;em&gt;attr&lt;/em&gt;, etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::detect&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Detection. UltraFace, RetinaFace, FaceBoxes, PyramidBox, etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::align&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Alignment. PFLD(106), FaceLandmark1000(1000 landmarks), PRNet, etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::align3d&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;3D Face Alignment. FaceMesh(468 3D landmarks), IrisLandmark(71+5 3D landmarks), etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::pose&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Head Pose Estimation. FSANet, etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::face::attr&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face Attributes. Emotion, Age, Gender. EmotionFerPlus, VGG16Age, etc. ❇️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::segmentation&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Object Segmentation. Such as FCN, DeepLabV3, etc. ❇️ ️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::style&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Style Transfer. Contains neural style transfer now, such as FastStyleTransfer. ⚠️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::matting&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Image Matting. Object and Human matting. ❇️ ️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::colorization&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Colorization. Make Gray image become RGB. ⚠️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;em&gt;lite::cv::resolution&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Super Resolution. ⚠️&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h3&gt;Lite.Ai.ToolKit&#39;s Classes and Pretrained Files.&lt;/h3&gt; &#xA; &lt;p&gt;Correspondence between the classes in &lt;strong&gt;Lite.AI.ToolKit&lt;/strong&gt; and pretrained model files can be found at &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;lite.ai.toolkit.hub.onnx.md&lt;/a&gt;. For examples, the pretrained model files for &lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt; and &lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt; are listed as follows.&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Class&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Pretrained ONNX Files&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rename or Converted From (Repo)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5l.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (🔥🔥💥↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;188Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5m.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (🔥🔥💥↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;85Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5s.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (🔥🔥💥↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;29Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloV5&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolov5x.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (🔥🔥💥↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;351Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_x.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥!!↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;378Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_l.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥!!↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;207Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_m.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥!!↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;97Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_s.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥!!↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;34Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_tiny.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥!!↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;19Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;em&gt;lite::cv::detection::YoloX&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;yolox_nano.onnx&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥!!↑)&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.5Mb&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;It means that you can load the the any one &lt;code&gt;yolov5*.onnx&lt;/code&gt; and &lt;code&gt;yolox_*.onnx&lt;/code&gt; according to your application through the same Lite.AI.ToolKit&#39;s classes, such as &lt;em&gt;YoloV5&lt;/em&gt;, &lt;em&gt;YoloX&lt;/em&gt;, etc.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5x.onnx&#34;);  // for server&#xA;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5l.onnx&#34;); &#xA;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5m.onnx&#34;);  &#xA;auto *yolov5 = new lite::cv::detection::YoloV5(&#34;yolov5s.onnx&#34;);  // for mobile device &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_x.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_l.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_m.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_s.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_tiny.onnx&#34;);  &#xA;auto *yolox = new lite::cv::detection::YoloX(&#34;yolox_nano.onnx&#34;);  // 3.5Mb only !&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; 🔑️ How to download Model Zoo from Docker Hub?&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Firstly, pull the image from docker hub. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker pull qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08 # (11G)&#xA;docker pull qyjdefdocker/lite.ai.toolkit-ncnn-hub:v0.1.22.01.08 # (9G)&#xA;docker pull qyjdefdocker/lite.ai.toolkit-tnn-hub:v0.1.22.01.08 # (11G)&#xA;docker pull qyjdefdocker/lite.ai.toolkit-onnx-hub:v0.1.22.01.08 # (28G)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Secondly, run the container with local &lt;code&gt;share&lt;/code&gt; dir using &lt;code&gt;docker run -idt xxx&lt;/code&gt;. A minimum example will show you as follows. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;make a &lt;code&gt;share&lt;/code&gt; dir in your local device.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir share # any name is ok.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;write &lt;code&gt;run_mnn_docker_hub.sh&lt;/code&gt; script like:&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/bash  &#xA;PORT1=6072&#xA;PORT2=6084&#xA;SERVICE_DIR=/Users/xxx/Desktop/your-path-to/share&#xA;CONRAINER_DIR=/home/hub/share&#xA;CONRAINER_NAME=mnn_docker_hub_d&#xA;&#xA;docker run -idt -p ${PORT2}:${PORT1} -v ${SERVICE_DIR}:${CONRAINER_DIR} --shm-size=16gb --name ${CONRAINER_NAME} qyjdefdocker/lite.ai.toolkit-mnn-hub:v0.1.22.01.08&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Finally, copy the model weights from &lt;code&gt;/home/hub/mnn/cv&lt;/code&gt; to your local &lt;code&gt;share&lt;/code&gt; dir. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# activate mnn docker.&#xA;sh ./run_mnn_docker_hub.sh&#xA;docker exec -it mnn_docker_hub_d /bin/bash&#xA;# copy the models to the share dir.&#xA;cd /home/hub &#xA;cp -rf mnn/cv share/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Model Hubs&lt;/h3&gt; &#xA;&lt;p&gt;The pretrained and converted ONNX files provide by lite.ai.toolkit are listed as follows. Also, see &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Model-Zoo&#34;&gt;Model Zoo&lt;/a&gt; and &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.onnx.md&#34;&gt;ONNX Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.mnn.md&#34;&gt;MNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.tnn.md&#34;&gt;TNN Hub&lt;/a&gt;, &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/docs/hub/lite.ai.toolkit.hub.ncnn.md&#34;&gt;NCNN Hub&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;6. Examples.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Examples-for-Lite.AI.ToolKit&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;More examples can be found at &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/tree/main/examples/lite/cv&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-object-detection&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example0: Object Detection using &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/yolov5s.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_yolov5_1.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_yolov5_1.jpg&#34;;&#xA;&#xA;  auto *yolov5 = new lite::cv::detection::YoloV5(onnx_path); &#xA;  std::vector&amp;lt;lite::types::Boxf&amp;gt; detected_boxes;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  yolov5-&amp;gt;detect(img_bgr, detected_boxes);&#xA;  &#xA;  lite::utils::draw_boxes_inplace(img_bgr, detected_boxes);&#xA;  cv::imwrite(save_img_path, img_bgr);  &#xA;  &#xA;  delete yolov5;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_yolov5_1.jpg&#34; height=&#34;256px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_yolov5_2.jpg&#34; height=&#34;256px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Or you can use Newest 🔥🔥 ! YOLO series&#39;s detector &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; or &lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;YoloR&lt;/a&gt;. They got the similar results.&lt;/p&gt; &#xA;&lt;p&gt;More classes for general object detection (80 classes, COCO).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *detector = new lite::cv::detection::YoloX(onnx_path);  // Newest YOLO detector !!! 2021-07&#xA;auto *detector = new lite::cv::detection::YoloV4(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloV3(onnx_path); &#xA;auto *detector = new lite::cv::detection::TinyYoloV3(onnx_path); &#xA;auto *detector = new lite::cv::detection::SSD(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloV5(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloR(onnx_path);  // Newest YOLO detector !!! 2021-05&#xA;auto *detector = new lite::cv::detection::TinyYoloV4VOC(onnx_path); &#xA;auto *detector = new lite::cv::detection::TinyYoloV4COCO(onnx_path); &#xA;auto *detector = new lite::cv::detection::ScaledYoloV4(onnx_path); &#xA;auto *detector = new lite::cv::detection::EfficientDet(onnx_path); &#xA;auto *detector = new lite::cv::detection::EfficientDetD7(onnx_path); &#xA;auto *detector = new lite::cv::detection::EfficientDetD8(onnx_path); &#xA;auto *detector = new lite::cv::detection::YOLOP(onnx_path);&#xA;auto *detector = new lite::cv::detection::NanoDet(onnx_path); // Super fast and tiny!&#xA;auto *detector = new lite::cv::detection::NanoDetPlus(onnx_path); // Super fast and tiny! 2021/12/25&#xA;auto *detector = new lite::cv::detection::NanoDetEfficientNetLite(onnx_path); // Super fast and tiny!&#xA;auto *detector = new lite::cv::detection::YoloV5_V_6_0(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloV5_V_6_1(onnx_path); &#xA;auto *detector = new lite::cv::detection::YoloX_V_0_1_1(onnx_path);  // Newest YOLO detector !!! 2021-07&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-matting&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example1: Video Matting using &lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting2021🔥🔥🔥&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/rvm_mobilenetv3_fp32.onnx&#34;;&#xA;  std::string video_path = &#34;../../../examples/lite/resources/test_lite_rvm_0.mp4&#34;;&#xA;  std::string output_path = &#34;../../../logs/test_lite_rvm_0.mp4&#34;;&#xA;  std::string background_path = &#34;../../../examples/lite/resources/test_lite_matting_bgr.jpg&#34;;&#xA;  &#xA;  auto *rvm = new lite::cv::matting::RobustVideoMatting(onnx_path, 16); // 16 threads&#xA;  std::vector&amp;lt;lite::types::MattingContent&amp;gt; contents;&#xA;  &#xA;  // 1. video matting.&#xA;  cv::Mat background = cv::imread(background_path);&#xA;  rvm-&amp;gt;detect_video(video_path, output_path, contents, false, 0.4f,&#xA;                    20, true, true, background);&#xA;  &#xA;  delete rvm;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/interviewi.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/interview.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3i.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/dance3.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/teslai.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/tesla.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/b5i.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/b5.gif&#34; height=&#34;80px&#34; width=&#34;150px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for matting (image matting, video matting, trimap/mask-free, trimap/mask-based)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *matting = new lite::cv::matting::RobustVideoMatting:(onnx_path);  //  WACV 2022.&#xA;auto *matting = new lite::cv::matting::MGMatting(onnx_path); // CVPR 2021&#xA;auto *matting = new lite::cv::matting::MODNet(onnx_path); // AAAI 2022&#xA;auto *matting = new lite::cv::matting::MODNetDyn(onnx_path); // AAAI 2022 Dynamic Shape Inference.&#xA;auto *matting = new lite::cv::matting::BackgroundMattingV2(onnx_path); // CVPR 2020 &#xA;auto *matting = new lite::cv::matting::BackgroundMattingV2Dyn(onnx_path); // CVPR 2020 Dynamic Shape Inference.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-alignment&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example2: 1000 Facial Landmarks Detection using &lt;a href=&#34;https://github.com/Single430/FaceLandmark1000&#34;&gt;FaceLandmarks1000&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/FaceLandmark1000.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_face_landmarks_0.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_face_landmarks_1000.jpg&#34;;&#xA;    &#xA;  auto *face_landmarks_1000 = new lite::cv::face::align::FaceLandmark1000(onnx_path);&#xA;&#xA;  lite::types::Landmarks landmarks;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  face_landmarks_1000-&amp;gt;detect(img_bgr, landmarks);&#xA;  lite::utils::draw_landmarks_inplace(img_bgr, landmarks);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;  &#xA;  delete face_landmarks_1000;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_face_landmarks_1000_0.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face alignment (68 points, 98 points, 106 points, 1000 points)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *align = new lite::cv::face::align::PFLD(onnx_path);  // 106 landmarks, 1.0Mb only!&#xA;auto *align = new lite::cv::face::align::PFLD98(onnx_path);  // 98 landmarks, 4.8Mb only!&#xA;auto *align = new lite::cv::face::align::PFLD68(onnx_path);  // 68 landmarks, 2.8Mb only!&#xA;auto *align = new lite::cv::face::align::MobileNetV268(onnx_path);  // 68 landmarks, 9.4Mb only!&#xA;auto *align = new lite::cv::face::align::MobileNetV2SE68(onnx_path);  // 68 landmarks, 11Mb only!&#xA;auto *align = new lite::cv::face::align::FaceLandmark1000(onnx_path);  // 1000 landmarks, 2.0Mb only!&#xA;auto *align = new lite::cv::face::align::PIPNet98(onnx_path);  // 98 landmarks, CVPR2021!&#xA;auto *align = new lite::cv::face::align::PIPNet68(onnx_path);  // 68 landmarks, CVPR2021!&#xA;auto *align = new lite::cv::face::align::PIPNet29(onnx_path);  // 29 landmarks, CVPR2021!&#xA;auto *align = new lite::cv::face::align::PIPNet19(onnx_path);  // 19 landmarks, CVPR2021!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Example3: FaceMesh(3D Landmarks) Detection using &lt;a href=&#34;https://github.com/google/mediapipe/raw/master/mediapipe/modules/face_landmark/&#34;&gt;FaceMesh&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/facemesh_face_landmark.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_face_align.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_facemesh.jpg&#34;;&#xA;&#xA;  auto *facemesh = new lite::cv::face::align3d::FaceMesh(onnx_path);&#xA;&#xA;  float confidence = 0.f;&#xA;  lite::types::Landmarks3D landmarks3d;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  facemesh-&amp;gt;detect(img_bgr, landmarks3d, confidence);&#xA;&#xA;  lite::utils::draw_facemesh_inplace(img_bgr, landmarks3d);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;  &#xA;  delete facemesh;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/facemesh0.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/facemesh1.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/facemesh2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for 3d face landmarks detection (x, y, z axis, 468 3D face landmarks, 71+5 3D iris landmarks)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *align = new lite::cv::face::align3d::FaceMesh(onnx_path);  // 468 3D landmarks, 2.4Mb only! from mediapipe!&#xA;auto *align = new lite::cv::face::align3d::IrisLandmarks(onnx_path);  // 71+5 3D iris landmarks, 2.6Mb only! from mediapipe!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-colorization&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example4: Colorization using &lt;a href=&#34;https://github.com/richzhang/colorization&#34;&gt;colorization&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/eccv16-colorizer.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_colorizer_1.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_eccv16_colorizer_1.jpg&#34;;&#xA;  &#xA;  auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);&#xA;  &#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  lite::types::ColorizeContent colorize_content;&#xA;  colorizer-&amp;gt;detect(img_bgr, colorize_content);&#xA;  &#xA;  if (colorize_content.flag) cv::imwrite(save_img_path, colorize_content.mat);&#xA;  delete colorizer;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_colorizer_1.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_colorizer_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_colorizer_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_siggraph17_colorizer_1.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_siggraph17_colorizer_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_siggraph17_colorizer_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for colorization (gray to rgb)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *colorizer = new lite::cv::colorization::Colorizer(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-recognition&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example5: Face Recognition using &lt;a href=&#34;https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch&#34;&gt;ArcFace&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/ms1mv3_arcface_r100.onnx&#34;;&#xA;  std::string test_img_path0 = &#34;../../../examples/lite/resources/test_lite_faceid_0.png&#34;;&#xA;  std::string test_img_path1 = &#34;../../../examples/lite/resources/test_lite_faceid_1.png&#34;;&#xA;  std::string test_img_path2 = &#34;../../../examples/lite/resources/test_lite_faceid_2.png&#34;;&#xA;&#xA;  auto *glint_arcface = new lite::cv::faceid::GlintArcFace(onnx_path);&#xA;&#xA;  lite::types::FaceContent face_content0, face_content1, face_content2;&#xA;  cv::Mat img_bgr0 = cv::imread(test_img_path0);&#xA;  cv::Mat img_bgr1 = cv::imread(test_img_path1);&#xA;  cv::Mat img_bgr2 = cv::imread(test_img_path2);&#xA;  glint_arcface-&amp;gt;detect(img_bgr0, face_content0);&#xA;  glint_arcface-&amp;gt;detect(img_bgr1, face_content1);&#xA;  glint_arcface-&amp;gt;detect(img_bgr2, face_content2);&#xA;&#xA;  if (face_content0.flag &amp;amp;&amp;amp; face_content1.flag &amp;amp;&amp;amp; face_content2.flag)&#xA;  {&#xA;    float sim01 = lite::utils::math::cosine_similarity&amp;lt;float&amp;gt;(&#xA;        face_content0.embedding, face_content1.embedding);&#xA;    float sim02 = lite::utils::math::cosine_similarity&amp;lt;float&amp;gt;(&#xA;        face_content0.embedding, face_content2.embedding);&#xA;    std::cout &amp;lt;&amp;lt; &#34;Detected Sim01: &#34; &amp;lt;&amp;lt; sim  &amp;lt;&amp;lt; &#34; Sim02: &#34; &amp;lt;&amp;lt; sim02 &amp;lt;&amp;lt; std::endl;&#xA;  }&#xA;&#xA;  delete glint_arcface;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_arcface_resnet_0.png&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_arcface_resnet_1.png&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_arcface_resnet_2.png&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Detected Sim01: 0.721159 Sim02: -0.0626267&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;More classes for face recognition (face id vector extract)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *recognition = new lite::cv::faceid::GlintCosFace(onnx_path);  // DeepGlint(insightface)&#xA;auto *recognition = new lite::cv::faceid::GlintArcFace(onnx_path);  // DeepGlint(insightface)&#xA;auto *recognition = new lite::cv::faceid::GlintPartialFC(onnx_path); // DeepGlint(insightface)&#xA;auto *recognition = new lite::cv::faceid::FaceNet(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::FocalArcFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::FocalAsiaArcFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::TencentCurricularFace(onnx_path); // Tencent(TFace)&#xA;auto *recognition = new lite::cv::faceid::TencentCifpFace(onnx_path); // Tencent(TFace)&#xA;auto *recognition = new lite::cv::faceid::CenterLossFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::SphereFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::PoseRobustFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::NaivePoseRobustFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::MobileFaceNet(onnx_path); // 3.8Mb only !&#xA;auto *recognition = new lite::cv::faceid::CavaGhostArcFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::CavaCombinedFace(onnx_path);&#xA;auto *recognition = new lite::cv::faceid::MobileSEFocalFace(onnx_path); // 4.5Mb only !&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-detection&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example6: Face Detection using &lt;a href=&#34;https://github.com/deepinsight/insightface/raw/master/detection/scrfd/&#34;&gt;SCRFD 2021&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/scrfd_2.5g_bnkps_shape640x640.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_face_detector.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_scrfd.jpg&#34;;&#xA;  &#xA;  auto *scrfd = new lite::cv::face::detect::SCRFD(onnx_path);&#xA;  &#xA;  std::vector&amp;lt;lite::types::BoxfWithLandmarks&amp;gt; detected_boxes;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  scrfd-&amp;gt;detect(img_bgr, detected_boxes);&#xA;  &#xA;  lite::utils::draw_boxes_with_landmarks_inplace(img_bgr, detected_boxes);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;  &#xA;  std::cout &amp;lt;&amp;lt; &#34;Default Version Done! Detected Face Num: &#34; &amp;lt;&amp;lt; detected_boxes.size() &amp;lt;&amp;lt; std::endl;&#xA;  &#xA;  delete scrfd;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/docs/resources/scrfd_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face detection (super fast face detection)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *detector = new lite::face::detect::UltraFace(onnx_path);  // 1.1Mb only !&#xA;auto *detector = new lite::face::detect::FaceBoxes(onnx_path);  // 3.8Mb only ! &#xA;auto *detector = new lite::face::detect::FaceBoxesv2(onnx_path);  // 4.0Mb only ! &#xA;auto *detector = new lite::face::detect::RetinaFace(onnx_path);  // 1.6Mb only ! CVPR2020&#xA;auto *detector = new lite::face::detect::SCRFD(onnx_path);  // 2.5Mb only ! CVPR2021, Super fast and accurate!!&#xA;auto *detector = new lite::face::detect::YOLO5Face(onnx_path);  // 2021, Super fast and accurate!!&#xA;auto *detector = new lite::face::detect::YOLOv5BlazeFace(onnx_path);  // 2021, Super fast and accurate!!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-segmentation&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example7: Segmentation using &lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&#34;&gt;DeepLabV3ResNet101&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/deeplabv3_resnet101_coco.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_deeplabv3_resnet101.png&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_deeplabv3_resnet101.jpg&#34;;&#xA;&#xA;  auto *deeplabv3_resnet101 = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path, 16); // 16 threads&#xA;&#xA;  lite::types::SegmentContent content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  deeplabv3_resnet101-&amp;gt;detect(img_bgr, content);&#xA;&#xA;  if (content.flag)&#xA;  {&#xA;    cv::Mat out_img;&#xA;    cv::addWeighted(img_bgr, 0.2, content.color_mat, 0.8, 0., out_img);&#xA;    cv::imwrite(save_img_path, out_img);&#xA;    if (!content.names_map.empty())&#xA;    {&#xA;      for (auto it = content.names_map.begin(); it != content.names_map.end(); ++it)&#xA;      {&#xA;        std::cout &amp;lt;&amp;lt; it-&amp;gt;first &amp;lt;&amp;lt; &#34; Name: &#34; &amp;lt;&amp;lt; it-&amp;gt;second &amp;lt;&amp;lt; std::endl;&#xA;      }&#xA;    }&#xA;  }&#xA;  delete deeplabv3_resnet101;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_deeplabv3_resnet101.png&#34; height=&#34;256px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_deeplabv3_resnet101.jpg&#34; height=&#34;256px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for segmentation (human segmentation, instance segmentation)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *segment = new lite::cv::segmentation::FCNResNet101(onnx_path);&#xA;auto *segment = new lite::cv::segmentation::DeepLabV3ResNet101(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-face-attributes-analysis&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example8: Age Estimation using &lt;a href=&#34;https://github.com/oukohou/SSR_Net_Pytorch&#34;&gt;SSRNet&lt;/a&gt; . Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/ssrnet.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_ssrnet.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_ssrnet.jpg&#34;;&#xA;&#xA;  auto *ssrnet = new lite::cv::face::attr::SSRNet(onnx_path);&#xA;&#xA;  lite::types::Age age;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  ssrnet-&amp;gt;detect(img_bgr, age);&#xA;  lite::utils::draw_age_inplace(img_bgr, age);&#xA;  cv::imwrite(save_img_path, img_bgr);&#xA;  std::cout &amp;lt;&amp;lt; &#34;Default Version Done! Detected SSRNet Age: &#34; &amp;lt;&amp;lt; age.age &amp;lt;&amp;lt; std::endl;&#xA;&#xA;  delete ssrnet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_ssrnet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_gender_googlenet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_emotion_ferplus.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for face attributes analysis (age, gender, emotion)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *attribute = new lite::cv::face::attr::AgeGoogleNet(onnx_path);  &#xA;auto *attribute = new lite::cv::face::attr::GenderGoogleNet(onnx_path); &#xA;auto *attribute = new lite::cv::face::attr::EmotionFerPlus(onnx_path);&#xA;auto *attribute = new lite::cv::face::attr::VGG16Age(onnx_path);&#xA;auto *attribute = new lite::cv::face::attr::VGG16Gender(onnx_path);&#xA;auto *attribute = new lite::cv::face::attr::EfficientEmotion7(onnx_path); // 7 emotions, 15Mb only!&#xA;auto *attribute = new lite::cv::face::attr::EfficientEmotion8(onnx_path); // 8 emotions, 15Mb only!&#xA;auto *attribute = new lite::cv::face::attr::MobileEmotion7(onnx_path); // 7 emotions, 13Mb only!&#xA;auto *attribute = new lite::cv::face::attr::ReXNetEmotion7(onnx_path); // 7 emotions&#xA;auto *attribute = new lite::cv::face::attr::SSRNet(onnx_path); // age estimation, 190kb only!!!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-image-classification&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example9: 1000 Classes Classification using &lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_densenet/&#34;&gt;DenseNet&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/densenet121.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_densenet.jpg&#34;;&#xA;&#xA;  auto *densenet = new lite::cv::classification::DenseNet(onnx_path);&#xA;&#xA;  lite::types::ImageNetContent content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  densenet-&amp;gt;detect(img_bgr, content);&#xA;  if (content.flag)&#xA;  {&#xA;    const unsigned int top_k = content.scores.size();&#xA;    if (top_k &amp;gt; 0)&#xA;    {&#xA;      for (unsigned int i = 0; i &amp;lt; top_k; ++i)&#xA;        std::cout &amp;lt;&amp;lt; i + 1&#xA;                  &amp;lt;&amp;lt; &#34;: &#34; &amp;lt;&amp;lt; content.labels.at(i)&#xA;                  &amp;lt;&amp;lt; &#34;: &#34; &amp;lt;&amp;lt; content.texts.at(i)&#xA;                  &amp;lt;&amp;lt; &#34;: &#34; &amp;lt;&amp;lt; content.scores.at(i)&#xA;                  &amp;lt;&amp;lt; std::endl;&#xA;    }&#xA;  }&#xA;  delete densenet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_densenet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_densenet.png&#34; height=&#34;224px&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for image classification (1000 classes)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *classifier = new lite::cv::classification::EfficientNetLite4(onnx_path);  &#xA;auto *classifier = new lite::cv::classification::ShuffleNetV2(onnx_path); // 8.7Mb only!&#xA;auto *classifier = new lite::cv::classification::GhostNet(onnx_path);&#xA;auto *classifier = new lite::cv::classification::HdrDNet(onnx_path);&#xA;auto *classifier = new lite::cv::classification::IBNNet(onnx_path);&#xA;auto *classifier = new lite::cv::classification::MobileNetV2(onnx_path); // 13Mb only!&#xA;auto *classifier = new lite::cv::classification::ResNet(onnx_path); &#xA;auto *classifier = new lite::cv::classification::ResNeXt(onnx_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-head-pose-estimation&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example10: Head Pose Estimation using &lt;a href=&#34;https://github.com/omasaht/headpose-fsanet-pytorch&#34;&gt;FSANet&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/fsanet-var.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_fsanet.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_fsanet.jpg&#34;;&#xA;&#xA;  auto *fsanet = new lite::cv::face::pose::FSANet(onnx_path);&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  lite::types::EulerAngles euler_angles;&#xA;  fsanet-&amp;gt;detect(img_bgr, euler_angles);&#xA;  &#xA;  if (euler_angles.flag)&#xA;  {&#xA;    lite::utils::draw_axis_inplace(img_bgr, euler_angles);&#xA;    cv::imwrite(save_img_path, img_bgr);&#xA;    std::cout &amp;lt;&amp;lt; &#34;yaw:&#34; &amp;lt;&amp;lt; euler_angles.yaw &amp;lt;&amp;lt; &#34; pitch:&#34; &amp;lt;&amp;lt; euler_angles.pitch &amp;lt;&amp;lt; &#34; row:&#34; &amp;lt;&amp;lt; euler_angles.roll &amp;lt;&amp;lt; std::endl;&#xA;  }&#xA;  delete fsanet;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet_2.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fsanet_3.jpg&#34; height=&#34;224px&#34; width=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for head pose estimation (euler angle, yaw, pitch, roll)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *pose = new lite::cv::face::pose::FSANet(onnx_path); // 1.2Mb only!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-style-transfer&#34;&gt;&lt;/div&gt; &#xA;&lt;h4&gt;Example11: Style Transfer using &lt;a href=&#34;https://github.com/onnx/models/tree/master/vision/style_transfer/fast_neural_style&#34;&gt;FastStyleTransfer&lt;/a&gt;. Download model from Model-Zoo&lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &#34;lite/lite.h&#34;&#xA;&#xA;static void test_default()&#xA;{&#xA;  std::string onnx_path = &#34;../../../hub/onnx/cv/style-candy-8.onnx&#34;;&#xA;  std::string test_img_path = &#34;../../../examples/lite/resources/test_lite_fast_style_transfer.jpg&#34;;&#xA;  std::string save_img_path = &#34;../../../logs/test_lite_fast_style_transfer_candy.jpg&#34;;&#xA;  &#xA;  auto *fast_style_transfer = new lite::cv::style::FastStyleTransfer(onnx_path);&#xA; &#xA;  lite::types::StyleContent style_content;&#xA;  cv::Mat img_bgr = cv::imread(test_img_path);&#xA;  fast_style_transfer-&amp;gt;detect(img_bgr, style_content);&#xA;&#xA;  if (style_content.flag) cv::imwrite(save_img_path, style_content.mat);&#xA;  delete fast_style_transfer;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output is:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/examples/lite/resources/test_lite_fast_style_transfer.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_candy.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_mosaic.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_pointilism.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_rain_princes.jpg&#34; height=&#34;224px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/logs/test_lite_fast_style_transfer_udnie.jpg&#34; height=&#34;224px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More classes for style transfer (neural style transfer, others)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;auto *transfer = new lite::cv::style::FastStyleTransfer(onnx_path); // 6.4Mb only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;7. License.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-License&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;The code of &lt;a href=&#34;https://raw.githubusercontent.com/DefTruth/lite.ai.toolkit/main/#lite.ai.toolkit-Introduction&#34;&gt;Lite.Ai.ToolKit&lt;/a&gt; is released under the GPL-3.0 License.&lt;/p&gt; &#xA;&lt;h2&gt;8. References.&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-References&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;Many thanks to these following projects. All the Lite.AI.ToolKit&#39;s models are sourced from these repos.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34;&gt;RobustVideoMatting&lt;/a&gt; (🔥🔥🔥new!!↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RangiLyu/nanodet&#34;&gt;nanodet&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; (🔥🔥🔥new!!↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hustvl/YOLOP&#34;&gt;YOLOP&lt;/a&gt; (🔥🔥new!!↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;YOLOR&lt;/a&gt; (🔥🔥new!!↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;ScaledYOLOv4&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;yolov5&lt;/a&gt; (🔥🔥💥↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/TFace&#34;&gt;TFace&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/argusswift/YOLOv4-pytorch&#34;&gt;YOLOv4-pytorch&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34;&gt;Ultra-Light-Fast-Generic-Face-Detector-1MB&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Expand for More References.&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/omasaht/headpose-fsanet-pytorch&#34;&gt;headpose-fsanet-pytorch&lt;/a&gt; (🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Hsintao/pfld_106_face_landmarks&#34;&gt;pfld_106_face_landmarks&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/models&#34;&gt;onnx-models&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/oukohou/SSR_Net_Pytorch&#34;&gt;SSR_Net_Pytorch&lt;/a&gt; (🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/richzhang/colorization&#34;&gt;colorization&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/niazwazir/SUB_PIXEL_CNN&#34;&gt;SUB_PIXEL_CNN&lt;/a&gt; (🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/vision&#34;&gt;torchvision&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/timesler/facenet-pytorch&#34;&gt;facenet-pytorch&lt;/a&gt; (🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ZhaoJ9014/face.evoLVe.PyTorch&#34;&gt;face.evoLVe.PyTorch&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/louis-she/center-loss.pytorch&#34;&gt;center-loss.pytorch&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/clcarwin/sphereface_pytorch&#34;&gt;sphereface_pytorch&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/penincillin/DREAM&#34;&gt;DREAM&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Xiaoccer/MobileFaceNet_Pytorch&#34;&gt;MobileFaceNet_Pytorch&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch&#34;&gt;cavaface.pytorch&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/HuangYG123/CurricularFace&#34;&gt;CurricularFace&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/HSE-asavchenko/face-emotion-recognition&#34;&gt;face-emotion-recognition&lt;/a&gt; (🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/grib0ed0v/face_recognition.pytorch&#34;&gt;face_recognition.pytorch&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/polarisZhao/PFLD-pytorch&#34;&gt;PFLD-pytorch&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/cunjian/pytorch_face_landmark&#34;&gt;pytorch_face_landmark&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Single430/FaceLandmark1000&#34;&gt;FaceLandmark1000&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/biubug6/Pytorch_Retinaface&#34;&gt;Pytorch_Retinaface&lt;/a&gt; (🔥🔥🔥↑)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/zisianw/FaceBoxes.PyTorch&#34;&gt;FaceBoxes&lt;/a&gt; (🔥🔥↑)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;9. Compilation Options.&lt;/h2&gt; &#xA;&lt;p&gt;In addition, &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;NCNN&lt;/a&gt; and &lt;a href=&#34;https://github.com/Tencent/TNN&#34;&gt;TNN&lt;/a&gt; support for some models will be added in the future, but due to operator compatibility and some other reasons, it is impossible to ensure that all models supported by &lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNXRuntime C++&lt;/a&gt; can run through &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;NCNN&lt;/a&gt; and &lt;a href=&#34;https://github.com/Tencent/TNN&#34;&gt;TNN&lt;/a&gt;. So, if you want to use all the models supported by this repo and don&#39;t care about the performance gap of &lt;em&gt;1~2ms&lt;/em&gt;, just let &lt;a href=&#34;https://github.com/microsoft/onnxruntime&#34;&gt;ONNXRuntime&lt;/a&gt; as default inference engine for this repo. However, you can follow the steps below if you want to build with &lt;a href=&#34;https://github.com/alibaba/MNN&#34;&gt;MNN&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;NCNN&lt;/a&gt; or &lt;a href=&#34;https://github.com/Tencent/TNN&#34;&gt;TNN&lt;/a&gt; support.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;change the &lt;code&gt;build.sh&lt;/code&gt; with &lt;code&gt;DENABLE_MNN=ON&lt;/code&gt;,&lt;code&gt;DENABLE_NCNN=ON&lt;/code&gt; or &lt;code&gt;DENABLE_TNN=ON&lt;/code&gt;, such as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd build &amp;amp;&amp;amp; cmake \&#xA;  -DCMAKE_BUILD_TYPE=MinSizeRel \&#xA;  -DINCLUDE_OPENCV=ON \   # Whether to package OpenCV into lite.ai.toolkit, default ON; otherwise, you need to setup OpenCV yourself.&#xA;  -DENABLE_MNN=ON \       # Whether to build with MNN,  default OFF, only some models are supported now.&#xA;  -DENABLE_NCNN=OFF \     # Whether to build with NCNN, default OFF, only some models are supported now.&#xA;  -DENABLE_TNN=OFF \      # Whether to build with TNN,  default OFF, only some models are supported now.&#xA;  .. &amp;amp;&amp;amp; make -j8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;use the MNN, NCNN or TNN version interface, see &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/examples/lite/cv/test_lite_nanodet.cpp&#34;&gt;demo&lt;/a&gt;, such as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-C++&#34;&gt;auto *nanodet = new lite::mnn::cv::detection::NanoDet(mnn_path);&#xA;auto *nanodet = new lite::tnn::cv::detection::NanoDet(proto_path, model_path);&#xA;auto *nanodet = new lite::ncnn::cv::detection::NanoDet(param_path, bin_path);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;10. Contribute&lt;/h2&gt; &#xA;&lt;div id=&#34;lite.ai.toolkit-Contribute&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;How to add your own models and become a contributor? See &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/191&#34;&gt;CONTRIBUTING.zh.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;11. Many Thanks !!! 🤗🎉🎉&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/issues/207&#34;&gt;Windows10 VS2019 CUDA 11.1 Build Docs&lt;/a&gt; (&lt;a href=&#34;https://github.com/zhanghongyong123456&#34;&gt;@zhanghongyong123456&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/docs/build/Linux.zh.md&#34;&gt;Linux Build Docs&lt;/a&gt; (&lt;a href=&#34;https://github.com/lee1221ee&#34;&gt;@lee1221ee&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/pull/105&#34;&gt;Some Windows10 Bugs Fixed&lt;/a&gt; (&lt;a href=&#34;https://github.com/ysc3839&#34;&gt;@ysc3839&lt;/a&gt;, &lt;a href=&#34;https://github.com/AvenSun&#34;&gt;@AvenSun&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>