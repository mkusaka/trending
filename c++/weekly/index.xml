<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-16T01:39:22Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google/highway</title>
    <updated>2024-06-16T01:39:22Z</updated>
    <id>tag:github.com,2024-06-16:/google/highway</id>
    <link href="https://github.com/google/highway" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Performance-portable, length-agnostic SIMD with runtime dispatch&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Efficient and performance-portable vector software&lt;/h1&gt; &#xA;&lt;p&gt;Highway is a C++ library that provides portable SIMD/vector intrinsics.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://google.github.io/highway/en/master/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.&lt;/p&gt; &#xA;&lt;h2&gt;Why&lt;/h2&gt; &#xA;&lt;p&gt;We are passionate about high-performance software. We see major untapped potential in CPUs (servers, mobile, desktops). Highway is for engineers who want to reliably and economically push the boundaries of what is possible in software.&lt;/p&gt; &#xA;&lt;h2&gt;How&lt;/h2&gt; &#xA;&lt;p&gt;CPUs provide SIMD/vector instructions that apply the same operation to multiple data items. This can reduce energy usage e.g. &lt;em&gt;fivefold&lt;/em&gt; because fewer instructions are executed. We also often see &lt;em&gt;5-10x&lt;/em&gt; speedups.&lt;/p&gt; &#xA;&lt;p&gt;Highway makes SIMD/vector programming practical and workable according to these guiding principles:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Does what you expect&lt;/strong&gt;: Highway is a C++ library with carefully-chosen functions that map well to CPU instructions without extensive compiler transformations. The resulting code is more predictable and robust to code changes/compiler updates than autovectorization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Works on widely-used platforms&lt;/strong&gt;: Highway supports five architectures; the same application code can target various instruction sets, including those with &#39;scalable&#39; vectors (size unknown at compile time). Highway only requires C++11 and supports four families of compilers. If you would like to use Highway on other platforms, please raise an issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flexible to deploy&lt;/strong&gt;: Applications using Highway can run on heterogeneous clouds or client devices, choosing the best available instruction set at runtime. Alternatively, developers may choose to target a single instruction set without any runtime overhead. In both cases, the application code is the same except for swapping &lt;code&gt;HWY_STATIC_DISPATCH&lt;/code&gt; with &lt;code&gt;HWY_DYNAMIC_DISPATCH&lt;/code&gt; plus one line of code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Suitable for a variety of domains&lt;/strong&gt;: Highway provides an extensive set of operations, used for image processing (floating-point), compression, video analysis, linear algebra, cryptography, sorting and random generation. We recognise that new use-cases may require additional ops and are happy to add them where it makes sense (e.g. no performance cliffs on some architectures). If you would like to discuss, please file an issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rewards data-parallel design&lt;/strong&gt;: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures. However, the biggest gains are unlocked by designing algorithms and data structures for scalable vectors. Helpful techniques include batching, structure-of-array layouts, and aligned/padded allocations.&lt;/p&gt; &#xA;&lt;p&gt;We recommend these resources for getting started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://const.me/articles/simd/simd.pdf&#34;&gt;SIMD for C++ Developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.algorithmica.org/hpc/&#34;&gt;Algorithms for Modern Hardware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://agner.org/optimize/optimizing_cpp.pdf&#34;&gt;Optimizing software in C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/&#34;&gt;Improving performance with SIMD intrinsics in three use cases&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Online demos using Compiler Explorer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gcc.godbolt.org/z/KM3ben7ET&#34;&gt;multiple targets with dynamic dispatch&lt;/a&gt; (more complicated, but flexible and uses best available SIMD)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gcc.godbolt.org/z/rGnjMevKG&#34;&gt;single target using -m flags&lt;/a&gt; (simpler, but requires/only uses the instruction set enabled by compiler flags)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We observe that Highway is referenced in the following open source projects, found via sourcegraph.com. Most are GitHub repositories. If you would like to add your project or link to it directly, feel free to raise an issue or contact us via the below email.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf / Waterfox)&lt;/li&gt; &#xA; &lt;li&gt;Cryptography: google/distributed_point_functions&lt;/li&gt; &#xA; &lt;li&gt;Data structures: bkille/BitLib&lt;/li&gt; &#xA; &lt;li&gt;Image codecs: eustas/2im, &lt;a href=&#34;https://github.com/GrokImageCompression/grok&#34;&gt;Grok JPEG 2000&lt;/a&gt;, &lt;a href=&#34;https://github.com/libjxl/libjxl&#34;&gt;JPEG XL&lt;/a&gt;, OpenHTJ2K, &lt;a href=&#34;https://github.com/osamu620/JPEGenc&#34;&gt;JPEGenc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite, &lt;a href=&#34;https://github.com/libvips/libvips&#34;&gt;libvips&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image viewers: AlienCowEatCake/ImageViewer, mirillis/jpegxl-wic, &lt;a href=&#34;https://bitbucket.org/kfj/pv/&#34;&gt;Lux panorama/image viewer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Information retrieval: &lt;a href=&#34;https://github.com/iresearch-toolkit/iresearch/raw/e7638e7a4b99136ca41f82be6edccf01351a7223/core/utils/simd_utils.hpp&#34;&gt;iresearch database index&lt;/a&gt;, michaeljclark/zvec&lt;/li&gt; &#xA; &lt;li&gt;Machine learning: Tensorflow, Numpy, zpye/SimpleInfer&lt;/li&gt; &#xA; &lt;li&gt;Voxels: rools/voxl&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mnm-team.org/pub/Fopras/rock23/&#34;&gt;Evaluation of C++ SIMD Libraries&lt;/a&gt;: &#34;Highway excelled with a strong performance across multiple SIMD extensions [..]. Thus, Highway may currently be the most suitable SIMD library for many software projects.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kfjahnke/zimt&#34;&gt;zimt&lt;/a&gt;: C++11 template library to process n-dimensional arrays with multi-threaded SIMD code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/highway/tree/master/hwy/contrib/sort&#34;&gt;vectorized Quicksort&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2205.05982&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;d like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories: alpinelinux, conan-io, conda-forge, DragonFlyBSD, freebsd, ghostbsd, microsoft/vcpkg, MidnightBSD, MSYS2, NetBSD, openSUSE, opnsense, Xilinx/Vitis_Libraries. See also the list at &lt;a href=&#34;https://repology.org/project/highway-simd-library/versions&#34;&gt;https://repology.org/project/highway-simd-library/versions&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Current status&lt;/h2&gt; &#xA;&lt;h3&gt;Targets&lt;/h3&gt; &#xA;&lt;p&gt;Highway supports 24 targets, listed in alphabetical order of platform:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any: &lt;code&gt;EMU128&lt;/code&gt;, &lt;code&gt;SCALAR&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Armv7+: &lt;code&gt;NEON_WITHOUT_AES&lt;/code&gt;, &lt;code&gt;NEON&lt;/code&gt;, &lt;code&gt;NEON_BF16&lt;/code&gt;, &lt;code&gt;SVE&lt;/code&gt;, &lt;code&gt;SVE2&lt;/code&gt;, &lt;code&gt;SVE_256&lt;/code&gt;, &lt;code&gt;SVE2_128&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;IBM Z: &lt;code&gt;Z14&lt;/code&gt;, &lt;code&gt;Z15&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;POWER: &lt;code&gt;PPC8&lt;/code&gt; (v2.07), &lt;code&gt;PPC9&lt;/code&gt; (v3.0), &lt;code&gt;PPC10&lt;/code&gt; (v3.1B, not yet supported due to compiler bugs, see #1207; also requires QEMU 7.2);&lt;/li&gt; &#xA; &lt;li&gt;RISC-V: &lt;code&gt;RVV&lt;/code&gt; (1.0);&lt;/li&gt; &#xA; &lt;li&gt;WebAssembly: &lt;code&gt;WASM&lt;/code&gt;, &lt;code&gt;WASM_EMU256&lt;/code&gt; (a 2x unrolled version of wasm128, enabled if &lt;code&gt;HWY_WANT_WASM2&lt;/code&gt; is defined. This will remain supported until it is potentially superseded by a future version of WASM.);&lt;/li&gt; &#xA; &lt;li&gt;x86: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SSE2&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SSSE3&lt;/code&gt; (~Intel Core)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SSE4&lt;/code&gt; (~Nehalem, also includes AES + CLMUL).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX2&lt;/code&gt; (~Haswell, also includes BMI2 + F16 + FMA)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3&lt;/code&gt; (~Skylake, AVX-512F/BW/CD/DQ/VL)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3_DL&lt;/code&gt; (~Icelake, includes BitAlg + CLMUL + GFNI + VAES + VBMI + VBMI2 + VNNI + VPOPCNT; requires opt-in by defining &lt;code&gt;HWY_WANT_AVX3_DL&lt;/code&gt; unless compiling for static dispatch),&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3_ZEN4&lt;/code&gt; (like AVX3_DL but optimized for AMD Zen4; requires opt-in by defining &lt;code&gt;HWY_WANT_AVX3_ZEN4&lt;/code&gt; if compiling for static dispatch, but enabled by default for runtime dispatch),&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3_SPR&lt;/code&gt; (~Sapphire Rapids, includes AVX-512FP16)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our policy is that unless otherwise specified, targets will remain supported as long as they can be (cross-)compiled with currently supported Clang or GCC, and tested using QEMU. If the target can be compiled with LLVM trunk and tested using our version of QEMU without extra flags, then it is eligible for inclusion in our continuous testing infrastructure. Otherwise, the target will be manually tested before releases with selected versions/configurations of Clang and GCC.&lt;/p&gt; &#xA;&lt;p&gt;SVE was initially tested using farm_sve (see acknowledgments).&lt;/p&gt; &#xA;&lt;h3&gt;Versioning&lt;/h3&gt; &#xA;&lt;p&gt;Highway releases aim to follow the semver.org system (MAJOR.MINOR.PATCH), incrementing MINOR after backward-compatible additions and PATCH after backward-compatible fixes. We recommend using releases (rather than the Git tip) because they are tested more extensively, see below.&lt;/p&gt; &#xA;&lt;p&gt;The current version 1.0 signals an increased focus on backwards compatibility. Applications using documented functionality will remain compatible with future updates that have the same major version number.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;Continuous integration tests build with a recent version of Clang (running on native x86, or QEMU for RISC-V and Arm) and MSVC 2019 (v19.28, running on native x86).&lt;/p&gt; &#xA;&lt;p&gt;Before releases, we also test on x86 with Clang and GCC, and Armv7/8 via GCC cross-compile. See the &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/release_testing_process.md&#34;&gt;testing process&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;Related modules&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;contrib&lt;/code&gt; directory contains SIMD-related utilities: an image class with aligned rows, a math library (16 functions already implemented, mostly trigonometry), and functions for computing dot products and sorting.&lt;/p&gt; &#xA;&lt;h3&gt;Other libraries&lt;/h3&gt; &#xA;&lt;p&gt;If you only require x86 support, you may also use Agner Fog&#39;s &lt;a href=&#34;https://github.com/vectorclass&#34;&gt;VCL vector class library&lt;/a&gt;. It includes many functions including a complete math library.&lt;/p&gt; &#xA;&lt;p&gt;If you have existing code using x86/NEON intrinsics, you may be interested in &lt;a href=&#34;https://github.com/simd-everywhere/simde&#34;&gt;SIMDe&lt;/a&gt;, which emulates those intrinsics using other platforms&#39; intrinsics or autovectorization.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This project uses CMake to generate and build. In a Debian-based system you can install it via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install cmake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Highway&#39;s unit tests use &lt;a href=&#34;https://github.com/google/googletest&#34;&gt;googletest&lt;/a&gt;. By default, Highway&#39;s CMake downloads this dependency at configuration time. You can avoid this by setting the &lt;code&gt;HWY_SYSTEM_GTEST&lt;/code&gt; CMake variable to ON and installing gtest separately:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install libgtest-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can define &lt;code&gt;HWY_TEST_STANDALONE=1&lt;/code&gt; and remove all occurrences of &lt;code&gt;gtest_main&lt;/code&gt; in each BUILD file, then tests avoid the dependency on GUnit.&lt;/p&gt; &#xA;&lt;p&gt;Running cross-compiled tests requires support from the OS, which on Debian is provided by the &lt;code&gt;qemu-user-binfmt&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;make -j &amp;amp;&amp;amp; make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can run &lt;code&gt;run_tests.sh&lt;/code&gt; (&lt;code&gt;run_tests.bat&lt;/code&gt; on Windows).&lt;/p&gt; &#xA;&lt;p&gt;Bazel is also supported for building, but it is not as widely used/tested.&lt;/p&gt; &#xA;&lt;p&gt;When building for Armv7, a limitation of current compilers requires you to add &lt;code&gt;-DHWY_CMAKE_ARM7:BOOL=ON&lt;/code&gt; to the CMake command line; see #834 and #1032. We understand that work is underway to remove this limitation.&lt;/p&gt; &#xA;&lt;p&gt;Building on 32-bit x86 is not officially supported, and AVX2/3 are disabled by default there. Note that johnplatts has successfully built and run the Highway tests on 32-bit x86, including AVX2/3, on GCC 7/8 and Clang 8/11/12. On Ubuntu 22.04, Clang 11 and 12, but not later versions, require extra compiler flags &lt;code&gt;-m32 -isystem /usr/i686-linux-gnu/include&lt;/code&gt;. Clang 10 and earlier require the above plus &lt;code&gt;-isystem /usr/i686-linux-gnu/include/c++/12/i686-linux-gnu&lt;/code&gt;. See #1279.&lt;/p&gt; &#xA;&lt;h2&gt;Building highway - Using vcpkg&lt;/h2&gt; &#xA;&lt;p&gt;highway is now available in &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vcpkg install highway&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The highway port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;benchmark&lt;/code&gt; inside examples/ as a starting point.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&#34;&gt;quick-reference page&lt;/a&gt; briefly lists all operations and their parameters, and the &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/instruction_matrix.pdf&#34;&gt;instruction_matrix&lt;/a&gt; indicates the number of instructions per operation.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/faq.md&#34;&gt;FAQ&lt;/a&gt; answers questions about portability, API design and where to find more information.&lt;/p&gt; &#xA;&lt;p&gt;We recommend using full SIMD vectors whenever possible for maximum performance portability. To obtain them, pass a &lt;code&gt;ScalableTag&amp;lt;float&amp;gt;&lt;/code&gt; (or equivalently &lt;code&gt;HWY_FULL(float)&lt;/code&gt;) tag to functions such as &lt;code&gt;Zero/Set/Load&lt;/code&gt;. There are two alternatives for use-cases requiring an upper bound on the lanes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For up to &lt;code&gt;N&lt;/code&gt; lanes, specify &lt;code&gt;CappedTag&amp;lt;T, N&amp;gt;&lt;/code&gt; or the equivalent &lt;code&gt;HWY_CAPPED(T, N)&lt;/code&gt;. The actual number of lanes will be &lt;code&gt;N&lt;/code&gt; rounded down to the nearest power of two, such as 4 if &lt;code&gt;N&lt;/code&gt; is 5, or 8 if &lt;code&gt;N&lt;/code&gt; is 8. This is useful for data structures such as a narrow matrix. A loop is still required because vectors may actually have fewer than &lt;code&gt;N&lt;/code&gt; lanes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For exactly a power of two &lt;code&gt;N&lt;/code&gt; lanes, specify &lt;code&gt;FixedTag&amp;lt;T, N&amp;gt;&lt;/code&gt;. The largest supported &lt;code&gt;N&lt;/code&gt; depends on the target, but is guaranteed to be at least &lt;code&gt;16/sizeof(T)&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Due to ADL restrictions, user code calling Highway ops must either:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reside inside &lt;code&gt;namespace hwy { namespace HWY_NAMESPACE {&lt;/code&gt;; or&lt;/li&gt; &#xA; &lt;li&gt;prefix each op with an alias such as &lt;code&gt;namespace hn = hwy::HWY_NAMESPACE; hn::Add()&lt;/code&gt;; or&lt;/li&gt; &#xA; &lt;li&gt;add using-declarations for each op used: &lt;code&gt;using hwy::HWY_NAMESPACE::Add;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, each function that calls Highway ops (such as &lt;code&gt;Load&lt;/code&gt;) must either be prefixed with &lt;code&gt;HWY_ATTR&lt;/code&gt;, OR reside between &lt;code&gt;HWY_BEFORE_NAMESPACE()&lt;/code&gt; and &lt;code&gt;HWY_AFTER_NAMESPACE()&lt;/code&gt;. Lambda functions currently require &lt;code&gt;HWY_ATTR&lt;/code&gt; before their opening brace.&lt;/p&gt; &#xA;&lt;p&gt;Do not use namespace-scope nor &lt;code&gt;static&lt;/code&gt; initializers for SIMD vectors because this can cause SIGILL when using runtime dispatch and the compiler chooses an initializer compiled for a target not supported by the current CPU. Instead, constants initialized via &lt;code&gt;Set&lt;/code&gt; should generally be local (const) variables.&lt;/p&gt; &#xA;&lt;p&gt;The entry points into code using Highway differ slightly depending on whether they use static or dynamic dispatch. In both cases, we recommend that the top-level function receives one or more pointers to arrays, rather than target-specific vector types.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For static dispatch, &lt;code&gt;HWY_TARGET&lt;/code&gt; will be the best available target among &lt;code&gt;HWY_BASELINE_TARGETS&lt;/code&gt;, i.e. those allowed for use by the compiler (see &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&#34;&gt;quick-reference&lt;/a&gt;). Functions inside &lt;code&gt;HWY_NAMESPACE&lt;/code&gt; can be called using &lt;code&gt;HWY_STATIC_DISPATCH(func)(args)&lt;/code&gt; within the same module they are defined in. You can call the function from other modules by wrapping it in a regular function and declaring the regular function in a header.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For dynamic dispatch, a table of function pointers is generated via the &lt;code&gt;HWY_EXPORT&lt;/code&gt; macro that is used by &lt;code&gt;HWY_DYNAMIC_DISPATCH(func)(args)&lt;/code&gt; to call the best function pointer for the current CPU&#39;s supported targets. A module is automatically compiled for each target in &lt;code&gt;HWY_TARGETS&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&#34;&gt;quick-reference&lt;/a&gt;) if &lt;code&gt;HWY_TARGET_INCLUDE&lt;/code&gt; is defined and &lt;code&gt;foreach_target.h&lt;/code&gt; is included. Note that the first invocation of &lt;code&gt;HWY_DYNAMIC_DISPATCH&lt;/code&gt;, or each call to the pointer returned by the first invocation of &lt;code&gt;HWY_DYNAMIC_POINTER&lt;/code&gt;, involves some CPU detection overhead. You can prevent this by calling the following before any invocation of &lt;code&gt;HWY_DYNAMIC_*&lt;/code&gt;: &lt;code&gt;hwy::GetChosenTarget().Update(hwy::SupportedTargets());&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When using dynamic dispatch, &lt;code&gt;foreach_target.h&lt;/code&gt; is included from translation units (.cc files), not headers. Headers containing vector code shared between several translation units require a special include guard, for example the following taken from &lt;code&gt;examples/skeleton-inl.h&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#if defined(HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_) == defined(HWY_TARGET_TOGGLE)&#xA;#ifdef HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_&#xA;#undef HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_&#xA;#else&#xA;#define HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_&#xA;#endif&#xA;&#xA;#include &#34;hwy/highway.h&#34;&#xA;// Your vector code&#xA;#endif&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By convention, we name such headers &lt;code&gt;-inl.h&lt;/code&gt; because their contents (often function templates) are usually inlined.&lt;/p&gt; &#xA;&lt;h2&gt;Compiler flags&lt;/h2&gt; &#xA;&lt;p&gt;Applications should be compiled with optimizations enabled. Without inlining SIMD code may slow down by factors of 10 to 100. For clang and GCC, &lt;code&gt;-O2&lt;/code&gt; is generally sufficient.&lt;/p&gt; &#xA;&lt;p&gt;For MSVC, we recommend compiling with &lt;code&gt;/Gv&lt;/code&gt; to allow non-inlined functions to pass vector arguments in registers. If intending to use the AVX2 target together with half-width vectors (e.g. for &lt;code&gt;PromoteTo&lt;/code&gt;), it is also important to compile with &lt;code&gt;/arch:AVX2&lt;/code&gt;. This seems to be the only way to reliably generate VEX-encoded SSE instructions on MSVC. Sometimes MSVC generates VEX-encoded SSE instructions, if they are mixed with AVX, but not always, see &lt;a href=&#34;https://developercommunity.visualstudio.com/t/10618264&#34;&gt;DevCom-10618264&lt;/a&gt;. Otherwise, mixing VEX-encoded AVX2 instructions and non-VEX SSE may cause severe performance degradation. Unfortunately, with &lt;code&gt;/arch:AVX2&lt;/code&gt; option, the resulting binary will then require AVX2. Note that no such flag is needed for clang and GCC because they support target-specific attributes, which we use to ensure proper VEX code generation for AVX2 targets.&lt;/p&gt; &#xA;&lt;h2&gt;Strip-mining loops&lt;/h2&gt; &#xA;&lt;p&gt;When vectorizing a loop, an important question is whether and how to deal with a number of iterations (&#39;trip count&#39;, denoted &lt;code&gt;count&lt;/code&gt;) that does not evenly divide the vector size &lt;code&gt;N = Lanes(d)&lt;/code&gt;. For example, it may be necessary to avoid writing past the end of an array.&lt;/p&gt; &#xA;&lt;p&gt;In this section, let &lt;code&gt;T&lt;/code&gt; denote the element type and &lt;code&gt;d = ScalableTag&amp;lt;T&amp;gt;&lt;/code&gt;. Assume the loop body is given as a function &lt;code&gt;template&amp;lt;bool partial, class D&amp;gt; void LoopBody(D d, size_t index, size_t max_n)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&#34;Strip-mining&#34; is a technique for vectorizing a loop by transforming it into an outer loop and inner loop, such that the number of iterations in the inner loop matches the vector width. Then, the inner loop is replaced with vector operations.&lt;/p&gt; &#xA;&lt;p&gt;Highway offers several strategies for loop vectorization:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure all inputs/outputs are padded. Then the (outer) loop is simply&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i += N) LoopBody&amp;lt;false&amp;gt;(d, i, 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, the template parameter and second function argument are not needed.&lt;/p&gt; &lt;p&gt;This is the preferred option, unless &lt;code&gt;N&lt;/code&gt; is in the thousands and vector operations are pipelined with long latencies. This was the case for supercomputers in the 90s, but nowadays ALUs are cheap and we see most implementations split vectors into 1, 2 or 4 parts, so there is little cost to processing entire vectors even if we do not need all their lanes. Indeed this avoids the (potentially large) cost of predication or partial loads/stores on older targets, and does not duplicate code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Process whole vectors and include previously processed elements in the last vector:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i += N) LoopBody&amp;lt;false&amp;gt;(d, HWY_MIN(i, count - N), 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the second preferred option provided that &lt;code&gt;count &amp;gt;= N&lt;/code&gt; and &lt;code&gt;LoopBody&lt;/code&gt; is idempotent. Some elements might be processed twice, but a single code path and full vectorization is usually worth it. Even if &lt;code&gt;count &amp;lt; N&lt;/code&gt;, it usually makes sense to pad inputs/outputs up to &lt;code&gt;N&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the &lt;code&gt;Transform*&lt;/code&gt; functions in hwy/contrib/algo/transform-inl.h. This takes care of the loop and remainder handling and you simply define a generic lambda function (C++14) or functor which receives the current vector from the input/output array, plus optionally vectors from up to two extra input arrays, and returns the value to write to the input/output array.&lt;/p&gt; &lt;p&gt;Here is an example implementing the BLAS function SAXPY (&lt;code&gt;alpha * x + y&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Transform1(d, x, n, y, [](auto d, const auto v, const auto v1) HWY_ATTR {&#xA;  return MulAdd(Set(d, alpha), v, v1);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Process whole vectors as above, followed by a scalar loop:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;size_t i = 0;&#xA;for (; i + N &amp;lt;= count; i += N) LoopBody&amp;lt;false&amp;gt;(d, i, 0);&#xA;for (; i &amp;lt; count; ++i) LoopBody&amp;lt;false&amp;gt;(CappedTag&amp;lt;T, 1&amp;gt;(), i, 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The template parameter and second function arguments are again not needed.&lt;/p&gt; &lt;p&gt;This avoids duplicating code, and is reasonable if &lt;code&gt;count&lt;/code&gt; is large. If &lt;code&gt;count&lt;/code&gt; is small, the second loop may be slower than the next option.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Process whole vectors as above, followed by a single call to a modified &lt;code&gt;LoopBody&lt;/code&gt; with masking:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;size_t i = 0;&#xA;for (; i + N &amp;lt;= count; i += N) {&#xA;  LoopBody&amp;lt;false&amp;gt;(d, i, 0);&#xA;}&#xA;if (i &amp;lt; count) {&#xA;  LoopBody&amp;lt;true&amp;gt;(d, i, count - i);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the template parameter and third function argument can be used inside &lt;code&gt;LoopBody&lt;/code&gt; to non-atomically &#39;blend&#39; the first &lt;code&gt;num_remaining&lt;/code&gt; lanes of &lt;code&gt;v&lt;/code&gt; with the previous contents of memory at subsequent locations: &lt;code&gt;BlendedStore(v, FirstN(d, num_remaining), d, pointer);&lt;/code&gt;. Similarly, &lt;code&gt;MaskedLoad(FirstN(d, num_remaining), d, pointer)&lt;/code&gt; loads the first &lt;code&gt;num_remaining&lt;/code&gt; elements and returns zero in other lanes.&lt;/p&gt; &lt;p&gt;This is a good default when it is infeasible to ensure vectors are padded, but is only safe &lt;code&gt;#if !HWY_MEM_OPS_MIGHT_FAULT&lt;/code&gt;! In contrast to the scalar loop, only a single final iteration is needed. The increased code size from two loop bodies is expected to be worthwhile because it avoids the cost of masking in all but the final iteration.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Additional resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/highway_intro.pdf&#34;&gt;Highway introduction (slides)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/instruction_matrix.pdf&#34;&gt;Overview of instructions per operation on different architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/design_philosophy.md&#34;&gt;Design philosophy and comparison&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/impl_details.md&#34;&gt;Implementation details&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We have used &lt;a href=&#34;https://gitlab.inria.fr/bramas/farm-sve&#34;&gt;farm-sve&lt;/a&gt; by Berenger Bramas; it has proved useful for checking the SVE port on an x86 development machine.&lt;/p&gt; &#xA;&lt;p&gt;This is not an officially supported Google product. Contact: &lt;a href=&#34;mailto:janwas@google.com&#34;&gt;janwas@google.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/TensorRT</title>
    <updated>2024-06-16T01:39:22Z</updated>
    <id>tag:github.com,2024-06-16:/NVIDIA/TensorRT</id>
    <link href="https://github.com/NVIDIA/TensorRT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TensorRT-documentation-brightgreen.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;TensorRT Open Source Software&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the Open Source Software (OSS) components of NVIDIA TensorRT. It includes the sources for TensorRT plugins and ONNX parser, as well as sample applications demonstrating usage and capabilities of the TensorRT platform. These open source software components are a subset of the TensorRT General Availability (GA) release with some extensions and bug-fixes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For code contributions to TensorRT-OSS, please see our &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/release/10.0/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/release/10.0/CODING-GUIDELINES.md&#34;&gt;Coding Guidelines&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For a summary of new additions and updates shipped with TensorRT-OSS releases, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/release/10.0/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For business inquiries, please contact &lt;a href=&#34;mailto:researchinquiries@nvidia.com&#34;&gt;researchinquiries@nvidia.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For press and other inquiries, please contact Hector Marinez at &lt;a href=&#34;mailto:hmarinez@nvidia.com&#34;&gt;hmarinez@nvidia.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Need enterprise support? NVIDIA global support is available for TensorRT with the &lt;a href=&#34;https://www.nvidia.com/en-us/data-center/products/ai-enterprise/&#34;&gt;NVIDIA AI Enterprise software suite&lt;/a&gt;. Check out &lt;a href=&#34;https://www.nvidia.com/en-us/launchpad/ai/ai-enterprise/&#34;&gt;NVIDIA LaunchPad&lt;/a&gt; for free access to a set of hands-on labs with TensorRT hosted on NVIDIA infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://www.nvidia.com/en-us/deep-learning-ai/triton-tensorrt-newsletter/&#34;&gt;TensorRT and Triton community&lt;/a&gt; and stay current on the latest product updates, bug fixes, content, best practices, and more.&lt;/p&gt; &#xA;&lt;h1&gt;Prebuilt TensorRT Python Package&lt;/h1&gt; &#xA;&lt;p&gt;We provide the TensorRT Python package for an easy installation. &lt;br&gt; To install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install tensorrt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can skip the &lt;strong&gt;Build&lt;/strong&gt; section to enjoy TensorRT with Python.&lt;/p&gt; &#xA;&lt;h1&gt;Build&lt;/h1&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To build the TensorRT-OSS components, you will first need the following software packages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TensorRT GA build&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TensorRT v10.0.1.6 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Available from direct download links listed below&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;System Packages&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Recommended versions:&lt;/li&gt; &#xA;   &lt;li&gt;cuda-12.2.0 + cuDNN-8.9&lt;/li&gt; &#xA;   &lt;li&gt;cuda-11.8.0 + cuDNN-8.9&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ftp.gnu.org/gnu/make/&#34;&gt;GNU make&lt;/a&gt; &amp;gt;= v4.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kitware/CMake/releases&#34;&gt;cmake&lt;/a&gt; &amp;gt;= v3.13&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;python&lt;/a&gt; &amp;gt;= v3.8, &amp;lt;= v3.10.x&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pip/#history&#34;&gt;pip&lt;/a&gt; &amp;gt;= v19.0&lt;/li&gt; &#xA; &lt;li&gt;Essential utilities &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, &lt;a href=&#34;https://www.freedesktop.org/wiki/Software/pkg-config/&#34;&gt;pkg-config&lt;/a&gt;, &lt;a href=&#34;https://www.gnu.org/software/wget/faq.html#download&#34;&gt;wget&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional Packages&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Containerized build&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;Docker&lt;/a&gt; &amp;gt;= 19.03&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;PyPI packages (for demo applications/tests)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/onnx/&#34;&gt;onnx&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/onnxruntime/&#34;&gt;onnxruntime&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/tensorflow/&#34;&gt;tensorflow-gpu&lt;/a&gt; &amp;gt;= 2.5.1&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/Pillow/&#34;&gt;Pillow&lt;/a&gt; &amp;gt;= 9.0.1&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pycuda/&#34;&gt;pycuda&lt;/a&gt; &amp;lt; 2021.1&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/numpy/&#34;&gt;numpy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pytest/&#34;&gt;pytest&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Code formatting tools (for contributors)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://clang.llvm.org/docs/ClangFormat.html&#34;&gt;Clang-format&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/llvm-mirror/clang/raw/master/tools/clang-format/git-clang-format&#34;&gt;Git-clang-format&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: &lt;a href=&#34;https://github.com/onnx/onnx-tensorrt&#34;&gt;onnx-tensorrt&lt;/a&gt;, &lt;a href=&#34;http://nvlabs.github.io/cub/&#34;&gt;cub&lt;/a&gt;, and &lt;a href=&#34;https://github.com/protocolbuffers/protobuf.git&#34;&gt;protobuf&lt;/a&gt; packages are downloaded along with TensorRT OSS, and not required to be installed.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading TensorRT Build&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;h4&gt;Download TensorRT OSS&lt;/h4&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b main https://github.com/nvidia/TensorRT TensorRT&#xA;cd TensorRT&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h4&gt;(Optional - if not using TensorRT container) Specify the TensorRT GA release build path&lt;/h4&gt; &lt;p&gt;If using the TensorRT OSS build container, TensorRT libraries are preinstalled under &lt;code&gt;/usr/lib/x86_64-linux-gnu&lt;/code&gt; and you may skip this step.&lt;/p&gt; &lt;p&gt;Else download and extract the TensorRT GA build from &lt;a href=&#34;https://developer.nvidia.com&#34;&gt;NVIDIA Developer Zone&lt;/a&gt; with the direct links below:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.0.1/tars/TensorRT-10.0.1.6.Linux.x86_64-gnu.cuda-11.8.tar.gz&#34;&gt;TensorRT 10.0.1.6 for CUDA 11.8, Linux x86_64&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.0.1/tars/TensorRT-10.0.1.6.Linux.x86_64-gnu.cuda-12.4.tar.gz&#34;&gt;TensorRT 10.0.1.6 for CUDA 12.4, Linux x86_64&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.0.1/zip/TensorRT-10.0.1.6.Windows10.win10.cuda-11.8.zip&#34;&gt;TensorRT 10.0.1.6 for CUDA 11.8, Windows x86_64&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.0.1/zip/TensorRT-10.0.1.6.Windows10.win10.cuda-12.4.zip&#34;&gt;TensorRT 10.0.1.6 for CUDA 12.4, Windows x86_64&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 20.04 on x86-64 with cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/Downloads&#xA;tar -xvzf TensorRT-10.0.1.6.Linux.x86_64-gnu.cuda-12.4.tar.gz&#xA;export TRT_LIBPATH=`pwd`/TensorRT-10.0.1.6&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Windows on x86-64 with cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Expand-Archive -Path TensorRT-10.0.1.6.Windows10.win10.cuda-12.4.zip&#xA;$env:TRT_LIBPATH=&#34;$pwd\TensorRT-10.0.1.6\lib&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setting Up The Build Environment&lt;/h2&gt; &#xA;&lt;p&gt;For Linux platforms, we recommend that you generate a docker container for building TensorRT OSS as described below. For native builds, please install the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/release/10.0/#prerequisites&#34;&gt;prerequisite&lt;/a&gt; &lt;em&gt;System Packages&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;h4&gt;Generate the TensorRT-OSS build container.&lt;/h4&gt; &lt;p&gt;The TensorRT-OSS build container can be generated using the supplied Dockerfiles and build scripts. The build containers are configured for building TensorRT OSS out-of-the-box.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 20.04 on x86-64 with cuda-12.4 (default)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/ubuntu-20.04.Dockerfile --tag tensorrt-ubuntu20.04-cuda12.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Rockylinux8 on x86-64 with cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/rockylinux8.Dockerfile --tag tensorrt-rockylinux8-cuda12.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 22.04 cross-compile for Jetson (aarch64) with cuda-12.4 (JetPack SDK)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/ubuntu-cross-aarch64.Dockerfile --tag tensorrt-jetpack-cuda12.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 22.04 on aarch64 with cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/build.sh --file docker/ubuntu-22.04-aarch64.Dockerfile --tag tensorrt-aarch64-ubuntu22.04-cuda12.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h4&gt;Launch the TensorRT-OSS build container.&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 20.04 build container&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./docker/launch.sh --tag tensorrt-ubuntu20.04-cuda12.4 --gpus all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: &lt;br&gt; 1. Use the &lt;code&gt;--tag&lt;/code&gt; corresponding to build container generated in Step 1. &lt;br&gt; 2. &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/TensorRT/release/10.0/#prerequisites&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt; is required for GPU access (running TensorRT applications) inside the build container. &lt;br&gt; 3. &lt;code&gt;sudo&lt;/code&gt; password for Ubuntu build containers is &#39;nvidia&#39;. &lt;br&gt; 4. Specify port number using &lt;code&gt;--jupyter &amp;lt;port&amp;gt;&lt;/code&gt; for launching Jupyter notebooks.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building TensorRT-OSS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate Makefiles and build.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example: Linux (x86-64) build with default cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out&#xA; make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Linux (aarch64) build with default cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64-native.toolchain&#xA; make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Example: Native build on Jetson (aarch64) with cuda-12.4&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DTRT_PLATFORM_ID=aarch64 -DCUDA_VERSION=12.4&#xA;CC=/usr/bin/gcc make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: C compiler must be explicitly specified via CC= for native aarch64 builds of protobuf.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Example: Ubuntu 22.04 Cross-Compile for Jetson (aarch64) with cuda-12.4 (JetPack)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build &amp;amp;&amp;amp; cd build&#xA; cmake .. -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64.toolchain -DCUDA_VERSION=12.4 -DCUDNN_LIB=/pdk_files/cudnn/usr/lib/aarch64-linux-gnu/libcudnn.so -DCUBLAS_LIB=/usr/local/cuda-12.4/targets/aarch64-linux/lib/stubs/libcublas.so -DCUBLASLT_LIB=/usr/local/cuda-12.4/targets/aarch64-linux/lib/stubs/libcublasLt.so -DTRT_LIB_DIR=/pdk_files/tensorrt/lib&#xA; make -j$(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;**Example: Native builds on Windows (x86) with cuda-12.4**&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt; cd $TRT_OSSPATH&#xA; mkdir -p build&#xA; cd -p build&#xA; cmake .. -DTRT_LIB_DIR=&#34;$env:TRT_LIBPATH&#34; -DCUDNN_ROOT_DIR=&#34;$env:CUDNN_PATH&#34; -DTRT_OUT_DIR=&#34;$pwd\\out&#34;&#xA; msbuild TensorRT.sln /property:Configuration=Release -m:$env:NUMBER_OF_PROCESSORS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;NOTE: &lt;br&gt; 1. The default CUDA version used by CMake is 12.4.0. To override this, for example to 11.8, append &lt;code&gt;-DCUDA_VERSION=11.8&lt;/code&gt; to the cmake command.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Required CMake build arguments are:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;TRT_LIB_DIR&lt;/code&gt;: Path to the TensorRT installation directory containing libraries.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;TRT_OUT_DIR&lt;/code&gt;: Output directory where generated build artifacts will be copied.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Optional CMake build arguments:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;CMAKE_BUILD_TYPE&lt;/code&gt;: Specify if binaries generated are for release or debug (contain debug symbols). Values consists of [&lt;code&gt;Release&lt;/code&gt;] | &lt;code&gt;Debug&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CUDA_VERSION&lt;/code&gt;: The version of CUDA to target, for example [&lt;code&gt;11.7.1&lt;/code&gt;].&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CUDNN_VERSION&lt;/code&gt;: The version of cuDNN to target, for example [&lt;code&gt;8.6&lt;/code&gt;].&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;PROTOBUF_VERSION&lt;/code&gt;: The version of Protobuf to use, for example [&lt;code&gt;3.0.0&lt;/code&gt;]. Note: Changing this will not configure CMake to use a system version of Protobuf, it will configure CMake to download and try building that version.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;CMAKE_TOOLCHAIN_FILE&lt;/code&gt;: The path to a toolchain file for cross compilation.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;BUILD_PARSERS&lt;/code&gt;: Specify if the parsers should be built, for example [&lt;code&gt;ON&lt;/code&gt;] | &lt;code&gt;OFF&lt;/code&gt;. If turned OFF, CMake will try to find precompiled versions of the parser libraries to use in compiling samples. First in &lt;code&gt;${TRT_LIB_DIR}&lt;/code&gt;, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;BUILD_PLUGINS&lt;/code&gt;: Specify if the plugins should be built, for example [&lt;code&gt;ON&lt;/code&gt;] | &lt;code&gt;OFF&lt;/code&gt;. If turned OFF, CMake will try to find a precompiled version of the plugin library to use in compiling samples. First in &lt;code&gt;${TRT_LIB_DIR}&lt;/code&gt;, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;BUILD_SAMPLES&lt;/code&gt;: Specify if the samples should be built, for example [&lt;code&gt;ON&lt;/code&gt;] | &lt;code&gt;OFF&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;GPU_ARCHS&lt;/code&gt;: GPU (SM) architectures to target. By default we generate CUDA code for all major SMs. Specific SM versions can be specified here as a quoted space-separated list to reduce compilation time and binary size. Table of compute capabilities of NVIDIA GPUs can be found &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;here&lt;/a&gt;. Examples: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;NVidia A100: &lt;code&gt;-DGPU_ARCHS=&#34;80&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Tesla T4, GeForce RTX 2080: &lt;code&gt;-DGPU_ARCHS=&#34;75&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Titan V, Tesla V100: &lt;code&gt;-DGPU_ARCHS=&#34;70&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Multiple SMs: &lt;code&gt;-DGPU_ARCHS=&#34;80 75&#34;&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;TRT_PLATFORM_ID&lt;/code&gt;: Bare-metal build (unlike containerized cross-compilation). Currently supported options: &lt;code&gt;x86_64&lt;/code&gt; (default).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;h2&gt;TensorRT Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT Developer Home&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html&#34;&gt;TensorRT QuickStart Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html&#34;&gt;TensorRT Developer Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html&#34;&gt;TensorRT Sample Support Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/index.html#tools&#34;&gt;TensorRT ONNX Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devtalk.nvidia.com/default/board/304/tensorrt/&#34;&gt;TensorRT Discussion Forums&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html&#34;&gt;TensorRT Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please refer to &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/release-notes&#34;&gt;TensorRT Release Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>moonlight-stream/moonlight-qt</title>
    <updated>2024-06-16T01:39:22Z</updated>
    <id>tag:github.com,2024-06-16:/moonlight-stream/moonlight-qt</id>
    <link href="https://github.com/moonlight-stream/moonlight-qt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GameStream client for PCs (Windows, Mac, Linux, and Steam Link)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Moonlight PC&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://moonlight-stream.org&#34;&gt;Moonlight PC&lt;/a&gt; is an open source PC client for NVIDIA GameStream and &lt;a href=&#34;https://github.com/LizardByte/Sunshine&#34;&gt;Sunshine&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Moonlight also has mobile versions for &lt;a href=&#34;https://github.com/moonlight-stream/moonlight-android&#34;&gt;Android&lt;/a&gt; and &lt;a href=&#34;https://github.com/moonlight-stream/moonlight-ios&#34;&gt;iOS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can follow development on our &lt;a href=&#34;https://moonlight-stream.org/discord&#34;&gt;Discord server&lt;/a&gt; and help translate Moonlight into your language on &lt;a href=&#34;https://hosted.weblate.org/projects/moonlight/moonlight-qt/&#34;&gt;Weblate&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ci.appveyor.com/project/cgutman/moonlight-qt/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/glj5cxqwy2w3bglv/branch/master?svg=true&#34; alt=&#34;AppVeyor Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/moonlight-stream/moonlight-qt/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/moonlight-stream/moonlight-qt/total&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hosted.weblate.org/projects/moonlight/moonlight-qt/&#34;&gt;&lt;img src=&#34;https://hosted.weblate.org/widgets/moonlight/-/moonlight-qt/svg-badge.svg?sanitize=true&#34; alt=&#34;Translation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hardware accelerated video decoding on Windows, Mac, and Linux&lt;/li&gt; &#xA; &lt;li&gt;H.264, HEVC, and AV1 codec support (AV1 requires Sunshine and a supported host GPU)&lt;/li&gt; &#xA; &lt;li&gt;HDR streaming support&lt;/li&gt; &#xA; &lt;li&gt;7.1 surround sound audio support&lt;/li&gt; &#xA; &lt;li&gt;10-point multitouch support (Sunshine only)&lt;/li&gt; &#xA; &lt;li&gt;Gamepad support with force feedback and motion controls for up to 16 players&lt;/li&gt; &#xA; &lt;li&gt;Support for both pointer capture (for games) and direct mouse control (for remote desktop)&lt;/li&gt; &#xA; &lt;li&gt;Support for passing system-wide keyboard shortcuts like Alt+Tab to the host&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloads&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/moonlight-stream/moonlight-qt/releases&#34;&gt;Windows, macOS, and Steam Link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://snapcraft.io/moonlight&#34;&gt;Snap (for Ubuntu-based Linux distros)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flathub.org/apps/details/com.moonlight_stream.Moonlight&#34;&gt;Flatpak (for other Linux distros)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/moonlight-stream/moonlight-qt/releases&#34;&gt;AppImage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/moonlight-stream/moonlight-docs/wiki/Installing-Moonlight-Qt-on-Raspberry-Pi-4&#34;&gt;Raspberry Pi 4 and 5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/moonlight-stream/moonlight-docs/wiki/Installing-Moonlight-Qt-on-ARM%E2%80%90based-Single-Board-Computers&#34;&gt;Generic ARM 32-bit and 64-bit Debian packages&lt;/a&gt; (not for Raspberry Pi)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/moonlight-stream/moonlight-docs/wiki/Installing-Moonlight-Qt-on-RISC%E2%80%90V-Single-Board-Computers&#34;&gt;Experimental RISC-V Debian packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/moonlight-stream/moonlight-docs/wiki/Installing-Moonlight-Qt-on-Linux4Tegra-(L4T)-Ubuntu&#34;&gt;NVIDIA Jetson and Nintendo Switch (Ubuntu L4T)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Special Thanks&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cloudsmith.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OSS%20hosting%20by-cloudsmith-blue?logo=cloudsmith&amp;amp;style=flat-square&#34; alt=&#34;Hosted By: Cloudsmith&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hosting for Moonlight&#39;s Debian and L4T package repositories is graciously provided for free by &lt;a href=&#34;https://cloudsmith.com&#34;&gt;Cloudsmith&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;h3&gt;Windows Build Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Qt 5.15 SDK or later. Qt 6 is also supported for x64 and ARM64 builds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt; (Community edition is fine)&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;strong&gt;MSVC&lt;/strong&gt; option during Qt installation. MinGW is not supported.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.7-zip.org/&#34;&gt;7-Zip&lt;/a&gt; (only if building installers for non-development PCs)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;macOS Build Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Qt 6.4 SDK or later&lt;/li&gt; &#xA; &lt;li&gt;Xcode 13 or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sindresorhus/create-dmg&#34;&gt;create-dmg&lt;/a&gt; (only if building DMGs for use on non-development Macs)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Linux/Unix Build Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Qt 6 is recommended, but Qt 5.9 or later is also supported (replace &lt;code&gt;qmake6&lt;/code&gt; with &lt;code&gt;qmake&lt;/code&gt; when using Qt 5).&lt;/li&gt; &#xA; &lt;li&gt;GCC or Clang&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg 4.0 or later&lt;/li&gt; &#xA; &lt;li&gt;Install the required packages: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Debian/Ubuntu: &lt;code&gt;libegl1-mesa-dev libgl1-mesa-dev libopus-dev libsdl2-dev libsdl2-ttf-dev libssl-dev libavcodec-dev libavformat-dev libva-dev libvdpau-dev libxkbcommon-dev wayland-protocols libdrm-dev qtbase6-dev qt6-declarative-dev libqt6svg6-dev qml6-module-qtquick-controls qml6-module-qtquick-templates qml6-module-qtquick-layouts qml6-module-qtqml-workerscript qml6-module-qtquick-window qml6-module-qtquick&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;RedHat/Fedora (RPM Fusion repo required): &lt;code&gt;openssl-devel SDL2-devel SDL2_ttf-devel ffmpeg-devel libva-devel libvdpau-devel opus-devel pulseaudio-libs-devel alsa-lib-devel libdrm-devel qt6-qtsvg-devel qt6-qtdeclarative-devel&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Building the Vulkan HDR renderer requires a &lt;code&gt;libplacebo-dev&lt;/code&gt;/&lt;code&gt;libplacebo-devel&lt;/code&gt; version of at least v338.0 and FFmpeg 6.1 or later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Steam Link Build Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ValveSoftware/steamlink-sdk&#34;&gt;Steam Link SDK&lt;/a&gt; cloned on your build system&lt;/li&gt; &#xA; &lt;li&gt;STEAMLINK_SDK_PATH environment variable set to the Steam Link SDK path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build Setup Steps&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the latest Qt SDK (and optionally, the Qt Creator IDE) from &lt;a href=&#34;https://www.qt.io/download&#34;&gt;https://www.qt.io/download&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can install Qt via Homebrew on macOS, but you will need to use &lt;code&gt;brew install qt --with-debug&lt;/code&gt; to be able to create debug builds of Moonlight.&lt;/li&gt; &#xA;   &lt;li&gt;You may also use your Linux distro&#39;s package manager for the Qt SDK as long as the packages are Qt 5.9 or later.&lt;/li&gt; &#xA;   &lt;li&gt;This step is not required for building on Steam Link, because the Steam Link SDK includes Qt 5.14.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;git submodule update --init --recursive&lt;/code&gt; from within &lt;code&gt;moonlight-qt/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open the project in Qt Creator or build from qmake on the command line. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To build a binary for use on non-development machines, use the scripts in the &lt;code&gt;scripts&lt;/code&gt; folder. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;For Windows builds, use &lt;code&gt;scripts\build-arch.bat&lt;/code&gt; and &lt;code&gt;scripts\generate-bundle.bat&lt;/code&gt;. Execute these scripts from the root of the repository within a Qt command prompt. Ensure 7-Zip binary directory is on your &lt;code&gt;%PATH%&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;For macOS builds, use &lt;code&gt;scripts/generate-dmg.sh&lt;/code&gt;. Execute this script from the root of the repository and ensure Qt&#39;s &lt;code&gt;bin&lt;/code&gt; folder is in your &lt;code&gt;$PATH&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;For Steam Link builds, run &lt;code&gt;scripts/build-steamlink-app.sh&lt;/code&gt; from the root of the repository.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;To build from the command line for development use on macOS or Linux, run &lt;code&gt;qmake6 moonlight-qt.pro&lt;/code&gt; then &lt;code&gt;make debug&lt;/code&gt; or &lt;code&gt;make release&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;To create an embedded build for a single-purpose device, use &lt;code&gt;qmake6 &#34;CONFIG+=embedded&#34; moonlight-qt.pro&lt;/code&gt; and build normally. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;This build will lack windowed mode, Discord/Help links, and other features that don&#39;t make sense on an embedded device.&lt;/li&gt; &#xA;     &lt;li&gt;For platforms with poor GPU performance, add &lt;code&gt;&#34;CONFIG+=gpuslow&#34;&lt;/code&gt; to prefer direct KMSDRM rendering over GL/Vulkan renderers. Direct KMSDRM rendering can use dedicated YUV/RGB conversion and scaling hardware rather than slower GPU shaders for these operations.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork us&lt;/li&gt; &#xA; &lt;li&gt;Write code&lt;/li&gt; &#xA; &lt;li&gt;Send Pull Requests&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://moonlight-stream.org&#34;&gt;website&lt;/a&gt; for project links and information.&lt;/p&gt;</summary>
  </entry>
</feed>