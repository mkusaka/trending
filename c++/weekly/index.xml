<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-10T01:51:18Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>intel/intel-extension-for-transformers</title>
    <updated>2023-12-10T01:51:18Z</updated>
    <id>tag:github.com,2023-12-10:/intel/intel-extension-for-transformers</id>
    <link href="https://github.com/intel/intel-extension-for-transformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ö° Build your chatbot within minutes on your favorite device; offer SOTA compression techniques for LLMs; run LLMs efficiently on Intel Platforms‚ö°&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Intel¬Æ Extension for Transformers&lt;/h1&gt; &#xA; &lt;h3&gt;An Innovative Transformer-based Toolkit to Accelerate GenAI/LLM Everywhere&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/Wxk3J3ZJkU&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/Wxk3J3ZJkU?compact=true&amp;amp;style=flat-square&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/intel/intel-extension-for-transformers&#34; alt=&#34;Release Notes&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/architecture.md&#34;&gt;üè≠Architecture&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat&#34;&gt;üí¨NeuralChat&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;üòÉInference&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/examples.md&#34;&gt;üíªExamples&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html&#34;&gt;üìñDocumentations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üöÄLatest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/12] Supported &lt;strong&gt;QLoRA on CPUs&lt;/strong&gt; to make fine-tuning on client CPU possible. Check out the &lt;a href=&#34;https://medium.com/@NeuralCompressor/creating-your-own-llms-on-your-laptop-a08cc4f7c91b&#34;&gt;blog&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Demonstrated up to &lt;strong&gt;3x LLM inference speedup&lt;/strong&gt; using &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/blog/assisted-generation&#34;&gt;Assisted Generation&lt;/a&gt;&lt;/strong&gt; (also called Speculative Decoding) from Hugging Face with Intel optimizations! Check out &lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/raw/main/examples/huggingface/pytorch/text-generation/assisted_generation/README.md&#34;&gt;more details&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Supported &lt;strong&gt;LLM QLoRA on CPU&lt;/strong&gt;, first time enabling LLM fine-tuning on client CPUs (see &lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/raw/main/docs/qloracpu.md&#34;&gt;more details&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Refreshed &lt;strong&gt;top-1 7B-sized LLM&lt;/strong&gt; by releasing &lt;a href=&#34;https://huggingface.co/Intel/neural-chat-7b-v3-1&#34;&gt;&lt;strong&gt;NeuralChat-v3-1&lt;/strong&gt;&lt;/a&gt;. Check out the &lt;a href=&#34;https://www.youtube.com/watch?v=bWhZ1u_1rlc&#34;&gt;nice video&lt;/a&gt; published by &lt;a href=&#34;https://www.youtube.com/@intheworldofai&#34;&gt;WorldofAI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Released &lt;a href=&#34;https://huggingface.co/Intel/neural-chat-7b-v3&#34;&gt;&lt;strong&gt;NeuralChat-v3&lt;/strong&gt;&lt;/a&gt;, new &lt;strong&gt;top-1 7B-sized LLM&lt;/strong&gt; available on Hugging Face. The model is fine-tuned on Habana Gaudi2 with supervised fine-tuning and direct preference optimization. Check out the &lt;a href=&#34;https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3&#34;&gt;blog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Published a &lt;strong&gt;4-bit chatbot demo&lt;/strong&gt; (based on NeuralChat) available on &lt;a href=&#34;https://huggingface.co/spaces/Intel/NeuralChat-ICX-INT4&#34;&gt;Intel Hugging Face Space&lt;/a&gt;. Welcome to have a try! To setup the demo locally, please follow the &lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/raw/main/intel_extension_for_transformers/neural_chat/docs/notebooks/setup_text_chatbot_service_on_spr.ipynb&#34;&gt;instructions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Released &lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/raw/main/intel_extension_for_transformers/llm/runtime/graph/docs/infinite_inference.md&#34;&gt;&lt;strong&gt;Fast, accurate, and infinite LLM inference&lt;/strong&gt;&lt;/a&gt; with improved &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;StreamingLLM&lt;/a&gt; on Intel CPUs!&lt;/li&gt; &#xA; &lt;li&gt;[2023/11] Our paper &lt;a href=&#34;https://arxiv.org/abs/2311.00502&#34;&gt;Efficient LLM Inference on CPUs&lt;/a&gt; has been accepted by &lt;strong&gt;NeurIPS&#39;23&lt;/strong&gt; on Efficient Natural Language and Speech Processing. Thanks to all the collaborators!&lt;/li&gt; &#xA; &lt;li&gt;[2023/10] LLM runtime, an Intel-optimized &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; compatible runtime, demonstrates &lt;strong&gt;up to 15x performance gain in 1st token generation and 1.5x in other token generation&lt;/strong&gt; over the default &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/10] LLM runtime now supports LLM inference with &lt;strong&gt;infinite-length inputs up to 4 million tokens&lt;/strong&gt;, inspired from &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;StreamingLLM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/09] NeuralChat has been showcased in &lt;a href=&#34;https://www.youtube.com/watch?v=RbKRELWP9y8&amp;amp;t=2954s&#34;&gt;&lt;strong&gt;Intel Innovation‚Äô23 Keynote&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next-23&#34;&gt;Google Cloud Next&#39;23&lt;/a&gt; to demonstrate GenAI/LLM capabilities on Intel Xeon Scalable Processors.&lt;/li&gt; &#xA; &lt;li&gt;[2023/08] NeuralChat supports &lt;strong&gt;custom chatbot development and deployment within minutes&lt;/strong&gt; on broad Intel HWs such as Xeon Scalable Processors, Gaudi2,&amp;nbsp;Xeon CPU Max Series,&amp;nbsp;Data Center GPU Max Series, Arc Series, and Core Processors. Check out &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&#34;&gt;Notebooks&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07] LLM runtime extends Hugging Face Transformers API to provide seamless low precision inference for popular LLMs, supporting low precision data types such as INT3/INT4/FP4/NF4/INT5/INT8/FP8.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;h2&gt;üèÉInstallation&lt;/h2&gt; &#xA; &lt;h3&gt;Quick Install from Pypi&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install intel-extension-for-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;For more installation methods, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/installation.md&#34;&gt;Installation Page&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h2&gt;üåüIntroduction&lt;/h2&gt; &#xA; &lt;p&gt;Intel¬Æ Extension for Transformers is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular, effective on 4th Intel Xeon Scalable processor&amp;nbsp;Sapphire Rapids (codenamed &lt;a href=&#34;https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html&#34;&gt;Sapphire Rapids&lt;/a&gt;). The toolkit provides the below key features and examples:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Seamless user experience of model compressions on Transformer-based models by extending &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face transformers&lt;/a&gt;&amp;nbsp;APIs and leveraging &lt;a href=&#34;https://github.com/intel/neural-compressor&#34;&gt;Intel¬Æ Neural Compressor&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Advanced software optimizations and unique compression-aware runtime (released with NeurIPS 2022&#39;s paper &lt;a href=&#34;https://arxiv.org/abs/2211.07715&#34;&gt;Fast Distilbert on CPUs&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2210.17114&#34;&gt;QuaLA-MiniLM: a Quantized Length Adaptive MiniLM&lt;/a&gt;, and NeurIPS 2021&#39;s paper &lt;a href=&#34;https://arxiv.org/abs/2111.05754&#34;&gt;Prune Once for All: Sparse Pre-Trained Language Models&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Optimized Transformer-based model packages such as &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/text-generation/deployment&#34;&gt;GPT-J-6B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/language-modeling/quantization#2-validated-model-list&#34;&gt;GPT-NEOX&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/language-modeling/inference#BLOOM-176B&#34;&gt;BLOOM-176B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/summarization/quantization#2-validated-model-list&#34;&gt;T5&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/summarization/quantization#2-validated-model-list&#34;&gt;Flan-T5&lt;/a&gt;, and end-to-end workflows such as &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb&#34;&gt;SetFit-based text classification&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/workflows/dlsa&#34;&gt;document level sentiment analysis (DLSA)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat&#34;&gt;NeuralChat&lt;/a&gt;, a customizable chatbot framework to create your own chatbot within minutes by leveraging a rich set of plugins &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md&#34;&gt;Knowledge Retrieval&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/audio/README.md&#34;&gt;Speech Interaction&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/caching/README.md&#34;&gt;Query Caching&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/security/README.md&#34;&gt;Security Guardrail&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;Inference&lt;/a&gt; of Large Language Model (LLM) in pure C/C++ with weight-only quantization kernels, supporting &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/gptneox&#34;&gt;GPT-NEOX&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/llama&#34;&gt;LLAMA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/mpt&#34;&gt;MPT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/falcon&#34;&gt;FALCON&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/bloom&#34;&gt;BLOOM-7B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/opt&#34;&gt;OPT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/chatglm&#34;&gt;ChatGLM2-6B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/gptj&#34;&gt;GPT-J-6B&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/models/gptneox&#34;&gt;Dolly-v2-3B&lt;/a&gt;. Support AMX, VNNI, AVX512F and AVX2 instruction set.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;üå±Getting Started&lt;/h2&gt; &#xA; &lt;p&gt;Below is the sample code to enable the chatbot. See more &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Chatbot&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# pip install intel-extension-for-transformers&#xA;from intel_extension_for_transformers.neural_chat import build_chatbot&#xA;chatbot = build_chatbot()&#xA;response = chatbot.predict(&#34;Tell me about Intel Xeon Scalable Processors.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Below is the sample code to enable weight-only INT4/INT8 inference. See more &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;INT4 Inference&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, TextStreamer&#xA;from intel_extension_for_transformers.transformers import AutoModelForCausalLM&#xA;model_name = &#34;Intel/neural-chat-7b-v1-1&#34;     # Hugging Face model_id or local model&#xA;prompt = &#34;Once upon a time, there existed a little girl,&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&#xA;inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids&#xA;streamer = TextStreamer(tokenizer)&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)&#xA;outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;INT8 Inference&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, TextStreamer&#xA;from intel_extension_for_transformers.transformers import AutoModelForCausalLM&#xA;model_name = &#34;Intel/neural-chat-7b-v1-1&#34;     # Hugging Face model_id or local model&#xA;prompt = &#34;Once upon a time, there existed a little girl,&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&#xA;inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids&#xA;streamer = TextStreamer(tokenizer)&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)&#xA;outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h2&gt;üéØValidated Models&lt;/h2&gt; &#xA; &lt;p&gt;You can access the latest int4 performance and accuracy at &lt;a href=&#34;https://medium.com/@NeuralCompressor/llm-performance-of-intel-extension-for-transformers-f7d061556176&#34;&gt;int4 blog&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Additionally, we are preparing to introduce Baichuan, Mistral, and other models into &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;LLM Runtime (Intel Optimized llamacpp)&lt;/a&gt;. For comprehensive accuracy and performance data, though not the most up-to-date, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/release_data.md&#34;&gt;Release data&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h2&gt;üìñDocumentation&lt;/h2&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;OVERVIEW&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat&#34;&gt;NeuralChat&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph&#34;&gt;LLM Runtime&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;NEURALCHAT&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_spr.ipynb&#34;&gt;Chatbot on Intel CPU&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_xpu.ipynb&#34;&gt;Chatbot on Intel GPU&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_habana_gaudi.ipynb&#34;&gt;Chatbot on Gaudi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/examples/deployment/talkingbot/pc/build_talkingbot_on_pc.ipynb&#34;&gt;Chatbot on Client&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;4&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&#34;&gt;More Notebooks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;LLM RUNTIME&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/README.md&#34;&gt;LLM Runtime&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/README.md#2-run-llm-with-python-api&#34;&gt;Streaming LLM&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/core/README.md&#34;&gt;Low Precision Kernels&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph/docs/tensor_parallelism.md&#34;&gt;Tensor Parallelism&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;LLM COMPRESSION&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/smoothquant.md&#34;&gt;SmoothQuant (INT8)&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/weightonlyquant.md&#34;&gt;Weight-only Quantization (INT4/FP4/NF4/INT8)&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/qloracpu.md&#34;&gt;QLoRA on CPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;GENERAL COMPRESSION&lt;/th&gt; &#xA;   &lt;/tr&gt;&#xA;   &lt;tr&gt; &#xA;   &lt;/tr&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/quantization.md&#34;&gt;Quantization&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/pruning.md&#34;&gt;Pruning&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/distillation.md&#34;&gt;Distillation&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/text-classification/orchestrate_optimizations/README.md&#34;&gt;Orchestration&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/language-modeling/nas/README.md&#34;&gt;Neural Architecture Search&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/export.md&#34;&gt;Export&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/metrics.md&#34;&gt;Metrics&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/objectives.md&#34;&gt;Objectives&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/pipeline.md&#34;&gt;Pipeline&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/examples/huggingface/pytorch/question-answering/dynamic/README.md&#34;&gt;Length Adaptive&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/examples.md#early-exit&#34;&gt;Early Exit&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/data_augmentation.md&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th colspan=&#34;8&#34; align=&#34;center&#34;&gt;TUTORIALS &amp;amp; RESULTS&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/tutorials/README.md&#34;&gt;Tutorials&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/graph#supported-models&#34;&gt;LLM List&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/examples.md&#34;&gt;General Model List&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.md&#34;&gt;Model Performance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h2&gt;üôåDemo&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;LLM Infinite Inference (up to 4M tokens)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/assets/109187816/1698dcda-c9ec-4f44-b159-f4e9d67ab15b&#34;&gt;https://github.com/intel/intel-extension-for-transformers/assets/109187816/1698dcda-c9ec-4f44-b159-f4e9d67ab15b&lt;/a&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;LLM QLoRA on Client CPU&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/assets/88082706/9d9bdb7e-65db-47bb-bbed-d23b151e8b31&#34;&gt;https://github.com/intel/intel-extension-for-transformers/assets/88082706/9d9bdb7e-65db-47bb-bbed-d23b151e8b31&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h2&gt;üìÉSelected Publications/Events&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;NeurIPS&#39;2023 on Efficient Natural Language and Speech Processing: &lt;a href=&#34;https://arxiv.org/abs/2311.00502&#34;&gt;Efficient LLM Inference on CPUs&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;NeurIPS&#39;2023 on Diffusion Models: &lt;a href=&#34;https://arxiv.org/pdf/2311.16133.pdf&#34;&gt;Effective Quantization for Diffusion Models on CPUs&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on datalearner: &lt;a href=&#34;https://www.datalearner.com/blog/1051701014024122&#34;&gt;Analysis of the top ten popular open source LLM of HuggingFace in the fourth week of November 2023 - the explosion of multi-modal large models and small-scale models&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on zaker: &lt;a href=&#34;https://app.myzaker.com/news/article.php?pk=656857148e9f0961d70ac3d3&#34;&gt;With this toolkit, the inference performance of large models can be accelerated by 40 times&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on geeky-gadgets: [New Intel Neural-Chat 7B LLM tops Hugging Face leaderboard beating original Mistral 7B] (&lt;a href=&#34;https://www.geeky-gadgets.com/intel-neural-chat-7b-llm/&#34;&gt;https://www.geeky-gadgets.com/intel-neural-chat-7b-llm/&lt;/a&gt;) (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Huggingface: &lt;a href=&#34;https://huggingface.co/blog/Andyrasika/neural-chat-intel&#34;&gt;Intel Neural-Chat 7b: Fine-Tuning on Gaudi2 for Top LLM Performance&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Video on YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=7_urstS-noU&#34;&gt;Neural Chat 7B v3-1 Installation on Windows - Step by Step&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Video on YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=bWhZ1u_1rlc&#34;&gt;Intel&#39;s Neural-Chat 7b: Most Powerful 7B Model! Beats GPT-4!?&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on marktechpost: &lt;a href=&#34;https://www.marktechpost.com/2023/11/09/intel-researchers-propose-a-new-artificial-intelligence-approach-to-deploy-llms-on-cpus-more-efficiently&#34;&gt;Intel Researchers Propose a New Artificial Intelligence Approach to Deploy LLMs on CPUs More Efficiently&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on VMware: &lt;a href=&#34;https://core.vmware.com/resource/ai-without-gpus-technical-brief-vmware-private-ai-intel#section6&#34;&gt;AI without GPUs: A Technical Brief for VMware Private AI with Intel&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;News releases on VMware: &lt;a href=&#34;https://news.vmware.com/releases/vmware-explore-2023-barcelona-intel-private-ai&#34;&gt;VMware Collaborates with Intel to Unlock Private AI Everywhere&lt;/a&gt; (Nov 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Video on YouTube: &lt;a href=&#34;https://www.youtube.com/watch?v=KWT6yKfu4n0&#34;&gt;Build Your Own ChatBot with Neural Chat | Intel Software&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/@NeuralCompressor/high-performance-low-bit-layer-wise-weight-only-quantization-on-a-laptop-712580899396&#34;&gt;Layer-wise Low-bit Weight Only Quantization on a Laptop&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/@NeuralCompressor/llm-performance-of-intel-extension-for-transformers-f7d061556176&#34;&gt;Intel-Optimized Llama.CPP in Intel Extension for Transformers&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA;  &lt;li&gt;Blog published on Medium: &lt;a href=&#34;https://medium.com/intel-analytics-software/reduce-large-language-model-carbon-footprint-with-intel-neural-compressor-and-intel-extension-for-dfadec3af76a&#34;&gt;Reduce the Carbon Footprint of Large Language Models&lt;/a&gt; (Oct 2023)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;View &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/publication.md&#34;&gt;Full Publication List&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;h2&gt;Additional Content&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/release.md&#34;&gt;Release Information&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/contributions.md&#34;&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/legal.md&#34;&gt;Legal Information&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/SECURITY.md&#34;&gt;Security Policy&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/LICENSE&#34;&gt;Apache License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Excellent open-source projects: &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;, &lt;a href=&#34;https://github.com/IntelLabs/fastRAG&#34;&gt;fastRAG&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt;, &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;gptq&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evauation-harness&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;peft&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;trl&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;streamingllm&lt;/a&gt; and many others.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Thanks to all the &lt;a href=&#34;https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/docs/contributors.md&#34;&gt;contributors&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;üíÅCollaborations&lt;/h2&gt; &#xA; &lt;p&gt;Welcome to raise any interesting ideas on model compression techniques and LLM-based chatbot development! Feel free to reach &lt;a href=&#34;mailto:itrex.maintainers@intel.com&#34;&gt;us&lt;/a&gt;, and we look forward to our collaborations on Intel Extension for Transformers!&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>SpiderLabs/ModSecurity</title>
    <updated>2023-12-10T01:51:18Z</updated>
    <id>tag:github.com,2023-12-10:/SpiderLabs/ModSecurity</id>
    <link href="https://github.com/SpiderLabs/ModSecurity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ModSecurity is an open source, cross platform web application firewall (WAF) engine for Apache, IIS and Nginx that is developed by Trustwave&#39;s SpiderLabs. It has a robust event-based programming language which provides protection from a range of attacks against web applications and allows for HTTP traffic monitoring, logging and real-time analys‚Ä¶&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/SpiderLabs/ModSecurity/raw/v3/master/others/modsec.png&#34; width=&#34;50%&#34;&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/SpiderLabs/ModSecurity/workflows/Quality%20Assurance/badge.svg?sanitize=true&#34; alt=&#34;Quality Assurance&#34;&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=USHvY32Uy62L&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=USHvY32Uy62L&amp;amp;metric=alert_status&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=USHvY32Uy62L&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=USHvY32Uy62L&amp;amp;metric=sqale_rating&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=USHvY32Uy62L&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=USHvY32Uy62L&amp;amp;metric=reliability_rating&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=USHvY32Uy62L&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=USHvY32Uy62L&amp;amp;metric=security_rating&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=USHvY32Uy62L&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=USHvY32Uy62L&amp;amp;metric=vulnerabilities&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Libmodsecurity is one component of the ModSecurity v3 project. The library codebase serves as an interface to ModSecurity Connectors taking in web traffic and applying traditional ModSecurity processing. In general, it provides the capability to load/interpret rules written in the ModSecurity SecRules format and apply them to HTTP content provided by your application via Connectors.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for ModSecurity for Apache (aka ModSecurity v2.x), it is still under maintenance and available: &lt;a href=&#34;https://github.com/SpiderLabs/ModSecurity/tree/v2/master&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What is the difference between this project and the old ModSecurity (v2.x.x)?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All Apache dependencies have been removed&lt;/li&gt; &#xA; &lt;li&gt;Higher performance&lt;/li&gt; &#xA; &lt;li&gt;New features&lt;/li&gt; &#xA; &lt;li&gt;New architecture&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Libmodsecurity is a complete rewrite of the ModSecurity platform. When it was first devised the ModSecurity project started as just an Apache module. Over time the project has been extended, due to popular demand, to support other platforms including (but not limited to) Nginx and IIS. In order to provide for the growing demand for additional platform support, it has became necessary to remove the Apache dependencies underlying this project, making it more platform independent.&lt;/p&gt; &#xA;&lt;p&gt;As a result of this goal we have rearchitected Libmodsecurity such that it is no longer dependent on the Apache web server (both at compilation and during runtime). One side effect of this is that across all platforms users can expect increased performance. Additionally, we have taken this opportunity to lay the groundwork for some new features that users have been long seeking. For example we are looking to natively support auditlogs in the JSON format, along with a host of other functionality in future versions.&lt;/p&gt; &#xA;&lt;h3&gt;It is no longer just a module.&lt;/h3&gt; &#xA;&lt;p&gt;The &#39;ModSecurity&#39; branch no longer contains the traditional module logic (for Nginx, Apache, and IIS) that has traditionally been packaged all together. Instead, this branch only contains the library portion (libmodsecurity) for this project. This library is consumed by what we have termed &#39;Connectors&#39; these connectors will interface with your webserver and provide the library with a common format that it understands. Each of these connectors is maintained as a separate GitHub project. For instance, the Nginx connector is supplied by the ModSecurity-nginx project (&lt;a href=&#34;https://github.com/SpiderLabs/ModSecurity-nginx&#34;&gt;https://github.com/SpiderLabs/ModSecurity-nginx&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Keeping these connectors separated allows each project to have different release cycles, issues and development trees. Additionally, it means that when you install ModSecurity v3 you only get exactly what you need, no extras you won&#39;t be using.&lt;/p&gt; &#xA;&lt;h1&gt;Compilation&lt;/h1&gt; &#xA;&lt;p&gt;Before starting the compilation process, make sure that you have all the dependencies in place. Read the subsection ‚ÄúDependencies‚Äù for further information.&lt;/p&gt; &#xA;&lt;p&gt;After the compilation make sure that there are no issues on your build/platform. We strongly recommend the utilization of the unit tests and regression tests. These test utilities are located under the subfolder ‚Äòtests‚Äô.&lt;/p&gt; &#xA;&lt;p&gt;As a dynamic library, don‚Äôt forget that libmodsecurity must be installed to a location (folder) where you OS will be looking for dynamic libraries.&lt;/p&gt; &#xA;&lt;h3&gt;Unix (Linux, MacOS, FreeBSD, ‚Ä¶)&lt;/h3&gt; &#xA;&lt;p&gt;On unix the project uses autotools to help the compilation process.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ./build.sh&#xA;$ ./configure&#xA;$ make&#xA;$ sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Details on distribution specific builds can be found in our Wiki: &lt;a href=&#34;https://github.com/SpiderLabs/ModSecurity/wiki/Compilation-recipes&#34;&gt;Compilation Recipes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Windows build is not ready yet.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;This library is written in C++ using the C++11 standards. It also uses Flex and Yacc to produce the ‚ÄúSec Rules Language‚Äù parser. Other, mandatory dependencies include YAJL, as ModSecurity uses JSON for producing logs and its testing framework, libpcre (not yet mandatory) for processing regular expressions in SecRules, and libXML2 (not yet mandatory) which is used for parsing XML requests.&lt;/p&gt; &#xA;&lt;p&gt;All others dependencies are related to operators specified within SecRules or configuration directives and may not be required for compilation. A short list of such dependencies is as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;libinjection is needed for the operator @detectXSS and @detectSQL&lt;/li&gt; &#xA; &lt;li&gt;curl is needed for the directive SecRemoteRules.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If those libraries are missing ModSecurity will be compiled without the support for the operator @detectXSS and the configuration directive SecRemoteRules.&lt;/p&gt; &#xA;&lt;h1&gt;Library documentation&lt;/h1&gt; &#xA;&lt;p&gt;The library documentation is written within the code in Doxygen format. To generate this documentation, please use the doxygen utility with the provided configuration file, ‚Äúdoxygen.cfg‚Äù, located with the &#34;doc/&#34; subfolder. This will generate HTML formatted documentation including usage examples.&lt;/p&gt; &#xA;&lt;h1&gt;Library utilization&lt;/h1&gt; &#xA;&lt;p&gt;The library provides a C++ and C interface. Some resources are currently only available via the C++ interface, for instance, the capability to create custom logging mechanism (see the regression test to check for how those logging mechanism works). The objective is to have both APIs (C, C++) providing the same functionality, if you find an aspect of the API that is missing via a particular interface, please open an issue.&lt;/p&gt; &#xA;&lt;p&gt;Inside the subfolder examples, there are simple examples on how to use the API. Below some are illustrated:&lt;/p&gt; &#xA;&lt;h3&gt;Simple example using C++&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;using ModSecurity::ModSecurity;&#xA;using ModSecurity::Rules;&#xA;using ModSecurity::Transaction;&#xA;&#xA;ModSecurity *modsec;&#xA;ModSecurity::Rules *rules;&#xA;&#xA;modsec = new ModSecurity();&#xA;&#xA;rules = new Rules();&#xA;&#xA;rules-&amp;gt;loadFromUri(rules_file);&#xA;&#xA;Transaction *modsecTransaction = new Transaction(modsec, rules);&#xA;&#xA;modsecTransaction-&amp;gt;processConnection(&#34;127.0.0.1&#34;);&#xA;if (modsecTransaction-&amp;gt;intervention()) {&#xA;   std::cout &amp;lt;&amp;lt; &#34;There is an intervention&#34; &amp;lt;&amp;lt; std::endl;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Simple example using C&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &#34;modsecurity/modsecurity.h&#34;&#xA;#include &#34;modsecurity/transaction.h&#34;&#xA;&#xA;&#xA;char main_rule_uri[] = &#34;basic_rules.conf&#34;;&#xA;&#xA;int main (int argc, char **argv)&#xA;{&#xA;    ModSecurity *modsec = NULL;&#xA;    Transaction *transaction = NULL;&#xA;    Rules *rules = NULL;&#xA;&#xA;    modsec = msc_init();&#xA;&#xA;    rules = msc_create_rules_set();&#xA;    msc_rules_add_file(rules, main_rule_uri);&#xA;&#xA;    transaction = msc_new_transaction(modsec, rules);&#xA;&#xA;    msc_process_connection(transaction, &#34;127.0.0.1&#34;);&#xA;    msc_process_uri(transaction, &#34;http://www.modsecurity.org/test?key1=value1&amp;amp;key2=value2&amp;amp;key3=value3&amp;amp;test=args&amp;amp;test=test&#34;);&#xA;    msc_process_request_headers(transaction);&#xA;    msc_process_request_body(transaction);&#xA;    msc_process_response_headers(transaction);&#xA;    msc_process_response_body(transaction);&#xA;&#xA;    return 0;&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;You are more than welcome to contribute to this project and look forward to growing the community around this new version of ModSecurity. Areas of interest include: New functionalities, fixes, bug report, support for beginning users, or anything that you are willing to help with.&lt;/p&gt; &#xA;&lt;h2&gt;Providing patches&lt;/h2&gt; &#xA;&lt;p&gt;We prefer to have your patch within the GitHub infrastructure to facilitate our review work, and our Q.A. integration. GitHub provides excellent documentation on how to perform ‚ÄúPull Requests‚Äù, more information available here: &lt;a href=&#34;https://help.github.com/articles/using-pull-requests/&#34;&gt;https://help.github.com/articles/using-pull-requests/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please respect the coding style. Pull requests can include various commits, so provide one fix or one piece of functionality per commit. Please do not change anything outside the scope of your target work (e.g. coding style in a function that you have passed by). For further information about the coding style used in this project, please check: &lt;a href=&#34;https://www.chromium.org/blink/coding-style&#34;&gt;https://www.chromium.org/blink/coding-style&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Provides explanative commit messages. Your first line should give the highlights of your patch, 3rd and on give a more detailed explanation/technical details about your patch. Patch explanation is valuable during the review process.&lt;/p&gt; &#xA;&lt;h3&gt;Don‚Äôt know where to start?&lt;/h3&gt; &#xA;&lt;p&gt;Within our code there are various items marked as TODO or FIXME that may need your attention. Check the list of items by performing a grep:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd /path/to/modsecurity-nginx&#xA;$ egrep -Rin &#34;TODO|FIXME&#34; -R *&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A TODO list is also available as part of the Doxygen documentation.&lt;/p&gt; &#xA;&lt;h3&gt;Testing your patch&lt;/h3&gt; &#xA;&lt;p&gt;Along with the manual testing, we strongly recommend you to use the our regression tests and unit tests. If you have implemented an operator, don‚Äôt forget to create unit tests for it. If you implement anything else, it is encouraged that you develop complimentary regression tests for it.&lt;/p&gt; &#xA;&lt;p&gt;The regression test and unit test utilities are native and do not demand any external tool or script, although you need to fetch the test cases from other repositories, as they are shared with other versions of ModSecurity, those others repositories git submodules. To fetch the submodules repository and run the utilities, follow the commands listed below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd /path/to/your/ModSecurity&#xA;$ git submodule foreach git pull&#xA;$ cd test&#xA;$ ./regression-tests&#xA;$ ./unit-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Debugging&lt;/h3&gt; &#xA;&lt;p&gt;Before start the debugging process, make sure of where your bug is. The problem could be on your connector or in libmodsecurity. In order to identify where the bug is, it is recommended that you develop a regression test that mimics the scenario where the bug is happening. If the bug is reproducible with the regression-test utility, then it will be far simpler to debug and ensure that it never occurs again. On Linux it is recommended that anyone undertaking debugging utilize gdb and/or valgrind as needed.&lt;/p&gt; &#xA;&lt;p&gt;During the configuration/compilation time, you may want to disable the compiler optimization making your ‚Äúback traces‚Äù populated with readable data. Use the CFLAGS to disable the compilation optimization parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ export CFLAGS=&#34;-g -O0&#34;&#xA;$ ./build.sh&#xA;$ ./configure&#xA;$ make&#xA;$ sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reporting Issues&lt;/h2&gt; &#xA;&lt;p&gt;If you are facing a configuration issue or something is not working as you expected to be, please use the ModSecurity user‚Äôs mailing list. Issues on GitHub are also welcomed, but we prefer to have user ask questions on the mailing list first so that you can reach an entire community. Also don‚Äôt forget to look for existing issues before open a new one.&lt;/p&gt; &#xA;&lt;p&gt;If you are going to open a new issue on GitHub, don‚Äôt forget to tell us the version of your libmodsecurity and the version of a specific connector if there is one.&lt;/p&gt; &#xA;&lt;h3&gt;Security issue&lt;/h3&gt; &#xA;&lt;p&gt;Please do not make public any security issue. Contact us at: &lt;a href=&#34;mailto:security@modsecurity.org&#34;&gt;security@modsecurity.org&lt;/a&gt; reporting the issue. Once the problem is fixed your credit will be given.&lt;/p&gt; &#xA;&lt;h2&gt;Feature request&lt;/h2&gt; &#xA;&lt;p&gt;We are open to discussing any new feature request with the community via the mailing lists. You can alternativly, feel free to open GitHub issues requesting new features. Before opening a new issue, please check if there is one already opened on the same topic.&lt;/p&gt; &#xA;&lt;h2&gt;Bindings&lt;/h2&gt; &#xA;&lt;p&gt;The libModSecurity design allows the integration with bindings. There is an effort to avoid breaking API [binary] compatibility to make an easy integration with possible bindings. Currently, there are two notable projects maintained by the community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python - &lt;a href=&#34;https://github.com/actions-security/pymodsecurity&#34;&gt;https://github.com/actions-security/pymodsecurity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Varnish - &lt;a href=&#34;https://github.com/xdecock/vmod-modsecurity&#34;&gt;https://github.com/xdecock/vmod-modsecurity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;Having our packages in distros on time is a desire that we have, so let us know if there is anything we can do to facilitate your work as a packager.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsor Note&lt;/h2&gt; &#xA;&lt;p&gt;Development of ModSecurity is sponsored by Trustwave. Sponsorship will end July 1, 2024. Additional information can be found here &lt;a href=&#34;https://www.trustwave.com/en-us/resources/security-resources/software-updates/end-of-sale-and-trustwave-support-for-modsecurity-web-application-firewall/&#34;&gt;https://www.trustwave.com/en-us/resources/security-resources/software-updates/end-of-sale-and-trustwave-support-for-modsecurity-web-application-firewall/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chaitin/SafeLine</title>
    <updated>2023-12-10T01:51:18Z</updated>
    <id>tag:github.com,2023-12-10:/chaitin/SafeLine</id>
    <link href="https://github.com/chaitin/SafeLine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‰∏ÄÊ¨æË∂≥Â§üÁÆÄÂçï„ÄÅË∂≥Â§üÂ•ΩÁî®„ÄÅË∂≥Â§üÂº∫ÁöÑÂÖçË¥π WAF„ÄÇÂü∫‰∫é‰∏öÁïåÈ¢ÜÂÖàÁöÑËØ≠‰πâÂºïÊìéÊ£ÄÊµãÊäÄÊúØÔºå‰Ωú‰∏∫ÂèçÂêë‰ª£ÁêÜÊé•ÂÖ•Ôºå‰øùÊä§‰Ω†ÁöÑÁΩëÁ´ô‰∏çÂèóÈªëÂÆ¢ÊîªÂáª„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://waf-ce.chaitin.cn/images/403.svg?sanitize=true&#34; width=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Èõ∑Ê±† - ÂπøÂèóÂ•ΩËØÑÁöÑÁ§æÂå∫ WAF&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/SafeLine-BEST_WAF-blue&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/chaitin/safeline.svg?color=blue&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release-date/chaitin/safeline.svg?color=blue&amp;amp;label=update&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/v/chaitin/safeline-mgt-api?color=blue&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/chaitin/safeline?style=social&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://waf-ce.chaitin.cn/&#34;&gt;ÂÆòÊñπÁΩëÁ´ô&lt;/a&gt; | &lt;a href=&#34;https://demo.waf-ce.chaitin.cn:9443/dashboard&#34;&gt;Âú®Á∫ø Demo&lt;/a&gt; | &lt;a href=&#34;https://waf-ce.chaitin.cn/posts/guide_introduction&#34;&gt;ÊäÄÊúØÊñáÊ°£&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/chaitin/SafeLine/main/README_EN.md&#34;&gt;For English&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;‰∏ÄÊ¨æË∂≥Â§üÁÆÄÂçï„ÄÅË∂≥Â§üÂ•ΩÁî®„ÄÅË∂≥Â§üÂº∫ÁöÑÂÖçË¥π WAF„ÄÇÂü∫‰∫é‰∏öÁïåÈ¢ÜÂÖàÁöÑËØ≠‰πâÂºïÊìéÊ£ÄÊµãÊäÄÊúØÔºå‰Ωú‰∏∫ÂèçÂêë‰ª£ÁêÜÊé•ÂÖ•Ôºå‰øùÊä§‰Ω†ÁöÑÁΩëÁ´ô‰∏çÂèóÈªëÂÆ¢ÊîªÂáª„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Ê†∏ÂøÉÊ£ÄÊµãËÉΩÂäõÁî±Êô∫ËÉΩËØ≠‰πâÂàÜÊûêÁÆóÊ≥ïÈ©±Âä®Ôºå‰∏ì‰∏∫Á§æÂå∫ËÄåÁîüÔºå‰∏çËÆ©ÈªëÂÆ¢Ë∂äÈõ∑Ê±†ÂçäÊ≠•„ÄÇ&lt;/p&gt; &#xA;&lt;img src=&#34;https://waf-ce.chaitin.cn/images/album/0.png&#34;&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;Áõ∏ÂÖ≥Ê∫êÁ†Å‰ªìÂ∫ì&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/chaitin/yanshi&#34;&gt;ËØ≠‰πâÂàÜÊûêËá™Âä®Êú∫ÂºïÊìé&lt;/a&gt; | &lt;a href=&#34;https://github.com/chaitin/safeline-open-platform&#34;&gt;ÊµÅÈáèÂàÜÊûêÊèí‰ª∂&lt;/a&gt; | &lt;a href=&#34;https://github.com/chaitin/lua-resty-t1k&#34;&gt;T1K ÂçèËÆÆ&lt;/a&gt; | &lt;a href=&#34;https://github.com/chaitin/blazehttp&#34;&gt;ÊµãËØïÂ∑•ÂÖ∑&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Áõ∏ÂÖ≥ÁâπÊÄß&lt;/h2&gt; &#xA;&lt;h4&gt;‰æøÊç∑ÊÄß&lt;/h4&gt; &#xA;&lt;p&gt;ÈááÁî®ÂÆπÂô®ÂåñÈÉ®ÁΩ≤Ôºå‰∏ÄÊù°ÂëΩ‰ª§Âç≥ÂèØÂÆåÊàêÂÆâË£ÖÔºå0 ÊàêÊú¨‰∏äÊâã„ÄÇÂÆâÂÖ®ÈÖçÁΩÆÂºÄÁÆ±Âç≥Áî®ÔºåÊó†ÈúÄ‰∫∫Â∑•Áª¥Êä§ÔºåÂèØÂÆûÁé∞ÂÆâÂÖ®Ë∫∫Âπ≥ÂºèÁÆ°ÁêÜ„ÄÇ&lt;/p&gt; &#xA;&lt;h4&gt;ÂÆâÂÖ®ÊÄß&lt;/h4&gt; &#xA;&lt;p&gt;È¶ñÂàõ‰∏öÂÜÖÈ¢ÜÂÖàÁöÑÊô∫ËÉΩËØ≠‰πâÂàÜÊûêÁÆóÊ≥ïÔºåÁ≤æÂáÜÊ£ÄÊµã„ÄÅ‰ΩéËØØÊä•„ÄÅÈöæÁªïËøá„ÄÇËØ≠‰πâÂàÜÊûêÁÆóÊ≥ïÊó†ËßÑÂàôÔºåÈù¢ÂØπÊú™Áü•ÁâπÂæÅÁöÑ 0day ÊîªÂáª‰∏çÂÜçÊâãË∂≥Êó†Êé™„ÄÇ&lt;/p&gt; &#xA;&lt;h4&gt;È´òÊÄßËÉΩ&lt;/h4&gt; &#xA;&lt;p&gt;Êó†ËßÑÂàôÂºïÊìéÔºåÁ∫øÊÄßÂÆâÂÖ®Ê£ÄÊµãÁÆóÊ≥ïÔºåÂπ≥ÂùáËØ∑Ê±ÇÊ£ÄÊµãÂª∂ËøüÂú® 1 ÊØ´ÁßíÁ∫ßÂà´„ÄÇÂπ∂ÂèëËÉΩÂäõÂº∫ÔºåÂçïÊ†∏ËΩªÊùæÊ£ÄÊµã 2000+ TPSÔºåÂè™Ë¶ÅÁ°¨‰ª∂Ë∂≥Â§üÂº∫ÔºåÂèØÊîØÊíëÁöÑÊµÅÈáèËßÑÊ®°Êó†‰∏äÈôê„ÄÇ&lt;/p&gt; &#xA;&lt;h4&gt;È´òÂèØÁî®&lt;/h4&gt; &#xA;&lt;p&gt;ÊµÅÈáèÂ§ÑÁêÜÂºïÊìéÂü∫‰∫é Nginx ÂºÄÂèëÔºåÊÄßËÉΩ‰∏éÁ®≥ÂÆöÊÄßÂùáÂèØÂæóÂà∞‰øùÈöú„ÄÇÂÜÖÁΩÆÂÆåÂñÑÁöÑÂÅ•Â∫∑Ê£ÄÊü•Êú∫Âà∂ÔºåÊúçÂä°ÂèØÁî®ÊÄßÈ´òËææ 99.99%„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ ÂÆâË£Ö&lt;/h2&gt; &#xA;&lt;h3&gt;ÈÖçÁΩÆÈúÄÊ±Ç&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Êìç‰ΩúÁ≥ªÁªüÔºöLinux&lt;/li&gt; &#xA; &lt;li&gt;Êåá‰ª§Êû∂ÊûÑÔºöx86_64&lt;/li&gt; &#xA; &lt;li&gt;ËΩØ‰ª∂‰æùËµñÔºöDocker 20.10.6 ÁâàÊú¨‰ª•‰∏ä&lt;/li&gt; &#xA; &lt;li&gt;ËΩØ‰ª∂‰æùËµñÔºöDocker Compose 2.0.0 ÁâàÊú¨‰ª•‰∏ä&lt;/li&gt; &#xA; &lt;li&gt;ÊúÄÂ∞èÂåñÁéØÂ¢ÉÔºö1 Ê†∏ CPU / 1 GB ÂÜÖÂ≠ò / 10 GB Á£ÅÁõò&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‰∏ÄÈîÆÂÆâË£Ö&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash -c &#34;$(curl -fsSLk https://waf-ce.chaitin.cn/release/latest/setup.sh)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Êõ¥Â§öÂÆâË£ÖÊñπÂºèËØ∑ÂèÇËÄÉ &lt;a href=&#34;https://waf-ce.chaitin.cn/posts/guide_install&#34;&gt;ÂÆâË£ÖÈõ∑Ê±†&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üïπÔ∏è Âø´ÈÄü‰ΩøÁî®&lt;/h2&gt; &#xA;&lt;h3&gt;ÁôªÂΩï&lt;/h3&gt; &#xA;&lt;p&gt;ÊµèËßàÂô®ÊâìÂºÄÂêéÂè∞ÁÆ°ÁêÜÈ°µÈù¢ &lt;code&gt;https://&amp;lt;waf-ip&amp;gt;:9443&lt;/code&gt;„ÄÇÊ†πÊçÆÁïåÈù¢ÊèêÁ§∫Ôºå‰ΩøÁî® &lt;strong&gt;ÊîØÊåÅ TOTP ÁöÑËÆ§ËØÅËΩØ‰ª∂&lt;/strong&gt; Êâ´Êèè‰∫åÁª¥Á†ÅÔºåÁÑ∂ÂêéËæìÂÖ•Âä®ÊÄÅÂè£‰ª§ÁôªÂΩïÔºö&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://waf-ce.chaitin.cn/images/gif/login.gif&#34; alt=&#34;login.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ÈÖçÁΩÆÈò≤Êä§Á´ôÁÇπ&lt;/h3&gt; &#xA;&lt;p&gt;Èõ∑Ê±†‰ª•ÂèçÂêë‰ª£ÁêÜÊñπÂºèÊé•ÂÖ•Ôºå‰ºòÂÖà‰∫éÁΩëÁ´ôÊúçÂä°Âô®Êé•Êî∂ÊµÅÈáèÔºåÂØπÊµÅÈáè‰∏≠ÁöÑÊîªÂáªË°å‰∏∫ËøõË°åÊ£ÄÊµãÂíåÊ∏ÖÊ¥óÔºåÂ∞ÜÊ∏ÖÊ¥óËøáÂêéÁöÑÊµÅÈáèËΩ¨ÂèëÁªôÁΩëÁ´ôÊúçÂä°Âô®„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://waf-ce.chaitin.cn/images/gif/config_site.gif&#34; alt=&#34;config.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;font color=&#34;grey&#34;&gt;üí° TIPS: Ê∑ªÂä†ÂêéÔºåÊâßË°å &lt;code&gt;curl -H &#34;Host: &amp;lt;ÂüüÂêç&amp;gt;&#34; http://&amp;lt;WAF IP&amp;gt;:&amp;lt;Á´ØÂè£&amp;gt;&lt;/code&gt; Â∫îËÉΩËé∑ÂèñÂà∞‰∏öÂä°ÁΩëÁ´ôÁöÑÂìçÂ∫î„ÄÇ&lt;/font&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ÊµãËØïÊïàÊûú&lt;/h3&gt; &#xA;&lt;p&gt;‰ΩøÁî®‰ª•‰∏ãÊñπÂºèÂ∞ùËØïÊ®°ÊãüÈªëÂÆ¢ÊîªÂáªÔºåÁúãÁúãÈõ∑Ê±†ÁöÑÈò≤Êä§ÊïàÊûúÂ¶Ç‰Ωï&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ÊµèËßàÂô®ËÆøÈóÆ &lt;code&gt;http://&amp;lt;IPÊàñÂüüÂêç&amp;gt;:&amp;lt;Á´ØÂè£&amp;gt;/?id=1%20AND%201=1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÊµèËßàÂô®ËÆøÈóÆ &lt;code&gt;http://&amp;lt;IPÊàñÂüüÂêç&amp;gt;:&amp;lt;Á´ØÂè£&amp;gt;/?a=&amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://waf-ce.chaitin.cn/images/gif/detect_log.gif&#34; alt=&#34;log.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Â¶ÇÊûú‰Ω†ÈúÄË¶ÅËøõË°åÊ∑±Â∫¶ÊµãËØïÔºåËØ∑ÂèÇËÄÉ &lt;a href=&#34;https://waf-ce.chaitin.cn/posts/guide_test&#34;&gt;ÊµãËØïÈò≤Êä§ÊïàÊûú&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;FAQ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://waf-ce.chaitin.cn/posts/faq_install&#34;&gt;ÂÆâË£ÖÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://waf-ce.chaitin.cn/posts/faq_login&#34;&gt;ÁôªÂΩïÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://waf-ce.chaitin.cn/posts/faq_access&#34;&gt;ÁΩëÁ´ôÊó†Ê≥ïËÆøÈóÆ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://waf-ce.chaitin.cn/posts/faq_config&#34;&gt;ÈÖçÁΩÆÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://waf-ce.chaitin.cn/posts/faq_other&#34;&gt;ÂÖ∂‰ªñÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üèòÔ∏è ËÅîÁ≥ªÊàë‰ª¨&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ÂèØ‰ª•ÈÄöËøá GitHub Issue Áõ¥Êé•ËøõË°å Bug ÂèçÈ¶àÂíåÂäüËÉΩÂª∫ËÆÆ&lt;/li&gt; &#xA; &lt;li&gt;ÂèØ‰ª•Êâ´Êèè‰∏ãÊñπ‰∫åÁª¥Á†ÅÂä†ÂÖ•Èõ∑Ê±†Á§æÂå∫ÁâàÁî®Êà∑ËÆ®ËÆ∫Áæ§&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://waf-ce.chaitin.cn/images/wechat-230825.png&#34; width=&#34;30%&#34;&gt; &#xA;&lt;h2&gt;Star History &lt;a name=&#34;star-history&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/chaitin/safeline/stargazers&#34;&gt; &lt;img width=&#34;500&#34; alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=chaitin/safeline&amp;amp;type=Date&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>