<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-20T01:37:58Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zackriya-Solutions/meeting-minutes</title>
    <updated>2025-04-20T01:37:58Z</updated>
    <id>tag:github.com,2025-04-20:/Zackriya-Solutions/meeting-minutes</id>
    <link href="https://github.com/Zackriya-Solutions/meeting-minutes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A free and open source, self hosted Ai based live meeting note taker and minutes summary generator that can completely run in your Local device (Mac OS and windows OS Support added. Working on adding linux support soon) https://meetily.zackriya.com/&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; style=&#34;border-bottom: none&#34;&gt; &#xA; &lt;h1&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zackriya-Solutions/meeting-minutes/main/docs/6.png&#34; width=&#34;400&#34; style=&#34;border-radius: 10px;&#34;&gt; &lt;br&gt; Meetily - AI-Powered Meeting Assistant &lt;/h1&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Pre_Release-v0.0.3-brightgreen&#34; alt=&#34;Pre-Release&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Stars-1000+-red&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Supported_OS-macOS,_Windows-yellow&#34; alt=&#34;Supported OS&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;h3&gt; &lt;br&gt; Open source Ai Assistant for taking meeting notes &lt;/h3&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://meetily.zackriya.com&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://in.linkedin.com/company/zackriya-solutions&#34;&gt;&lt;b&gt;Author&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/crRymMQBFH&#34;&gt;&lt;b&gt;Discord Channel&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;An AI-Powered Meeting Assistant that captures live meeting audio, transcribes it in real-time, and generates summaries while ensuring user privacy. Perfect for teams who want to focus on discussions while automatically capturing and organizing meeting content without the need for external servers or complex infrastructure.&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zackriya-Solutions/meeting-minutes/main/docs/demo_small.gif&#34; width=&#34;650&#34; alt=&#34;Meetily Demo&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://youtu.be/5k_Q5Wlahuk&#34;&gt;View full Demo Video&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;An AI-powered meeting assistant that captures live meeting audio, transcribes it in real-time, and generates summaries while ensuring user privacy. Perfect for teams who want to focus on discussions while automatically capturing and organizing meeting content.&lt;/p&gt; &#xA;&lt;h3&gt;Why?&lt;/h3&gt; &#xA;&lt;p&gt;While there are many meeting transcription tools available, this solution stands out by offering:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Privacy First&lt;/strong&gt;: All processing happens locally on your device&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cost Effective&lt;/strong&gt;: Uses open-source AI models instead of expensive APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Works offline, supports multiple meeting platforms&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Self-host and modify for your specific needs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intelligent&lt;/strong&gt;: Built-in knowledge graph for semantic search across meetings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;✅ Modern, responsive UI with real-time updates&lt;/p&gt; &#xA;&lt;p&gt;✅ Real-time audio capture (microphone + system audio)&lt;/p&gt; &#xA;&lt;p&gt;✅ Live transcription using Whisper.cpp&lt;/p&gt; &#xA;&lt;p&gt;🚧 Speaker diarization&lt;/p&gt; &#xA;&lt;p&gt;✅ Local processing for privacy&lt;/p&gt; &#xA;&lt;p&gt;✅ Packaged the app for macOS and Windows&lt;/p&gt; &#xA;&lt;p&gt;🚧 Export to Markdown/PDF&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: We have a Rust-based implementation that explores better performance and native integration. It currently implements:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;✅ Real-time audio capture from both microphone and system audio&lt;/li&gt; &#xA;  &lt;li&gt;✅ Live transcription using locally-running Whisper&lt;/li&gt; &#xA;  &lt;li&gt;✅ Speaker diarization&lt;/li&gt; &#xA;  &lt;li&gt;✅ Rich text editor for notes&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We are currently working on:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;✅ Export to Markdown/PDF&lt;/li&gt; &#xA;  &lt;li&gt;✅ Export to HTML&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Release 0.0.3&lt;/h2&gt; &#xA;&lt;p&gt;A new release is available!&lt;/p&gt; &#xA;&lt;p&gt;Please check out the release &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases/tag/v0.0.3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What&#39;s New&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows Support&lt;/strong&gt;: Fixed audio capture issues on Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improved Error Handling&lt;/strong&gt;: Better error handling and logging for audio devices&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced Device Detection&lt;/strong&gt;: More robust audio device detection across platforms&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Windows Installers&lt;/strong&gt;: Added both .exe and .msi installers for Windows&lt;/li&gt; &#xA; &lt;li&gt;Transcription quality is improved&lt;/li&gt; &#xA; &lt;li&gt;Bug fixes and improvements for frontend&lt;/li&gt; &#xA; &lt;li&gt;Better backend app build process&lt;/li&gt; &#xA; &lt;li&gt;Improved documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What would be next?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Database connection to save meeting minutes&lt;/li&gt; &#xA; &lt;li&gt;Improve summarization quality for smaller LLM models&lt;/li&gt; &#xA; &lt;li&gt;Add download options for meeting transcriptions&lt;/li&gt; &#xA; &lt;li&gt;Add download option for summary&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Smaller LLMs can hallucinate, making summarization quality poor; Please use model above 32B parameter size&lt;/li&gt; &#xA; &lt;li&gt;Backend build process requires CMake, C++ compiler, etc. Making it harder to build&lt;/li&gt; &#xA; &lt;li&gt;Backend build process requires Python 3.10 or newer&lt;/li&gt; &#xA; &lt;li&gt;Frontend build process requires Node.js&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLM Integration&lt;/h2&gt; &#xA;&lt;p&gt;The backend supports multiple LLM providers through a unified interface. Current implementations include:&lt;/p&gt; &#xA;&lt;h3&gt;Supported Providers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt; (Claude models)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq&lt;/strong&gt; (Llama3.2 90 B)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; (Local models that supports function calling)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Create &lt;code&gt;.env&lt;/code&gt; file with your API keys:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;# Required for Anthropic&#xA;ANTHROPIC_API_KEY=your_key_here  &#xA;&#xA;# Required for Groq &#xA;GROQ_API_KEY=your_key_here&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;System Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zackriya-Solutions/meeting-minutes/main/docs/HighLevel.jpg&#34; alt=&#34;High Level Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Core Components&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audio Capture Service&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Real-time microphone/system audio capture&lt;/li&gt; &#xA;   &lt;li&gt;Audio preprocessing pipeline&lt;/li&gt; &#xA;   &lt;li&gt;Built with Rust (experimental) and Python&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transcription Engine&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Whisper.cpp for local transcription&lt;/li&gt; &#xA;   &lt;li&gt;Supports multiple model sizes (tiny-&amp;gt;large)&lt;/li&gt; &#xA;   &lt;li&gt;GPU-accelerated processing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Orchestrator&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unified interface for multiple providers&lt;/li&gt; &#xA;   &lt;li&gt;Automatic fallback handling&lt;/li&gt; &#xA;   &lt;li&gt;Chunk processing with overlap&lt;/li&gt; &#xA;   &lt;li&gt;Model configuration:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Services&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ChromaDB&lt;/strong&gt;: Vector store for transcript embeddings&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;SQLite&lt;/strong&gt;: Process tracking and metadata storage&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;API Layer&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FastAPI endpoints: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;POST /upload&lt;/li&gt; &#xA;     &lt;li&gt;POST /process&lt;/li&gt; &#xA;     &lt;li&gt;GET /summary/{id}&lt;/li&gt; &#xA;     &lt;li&gt;DELETE /summary/{id}&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Deployment Architecture&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Tauri app + Next.js (packaged executables)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python FastAPI: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Transcript workers&lt;/li&gt; &#xA;   &lt;li&gt;LLM inference&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.js 18+&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10+&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg&lt;/li&gt; &#xA; &lt;li&gt;Rust 1.65+ (for experimental features)&lt;/li&gt; &#xA; &lt;li&gt;Cmake 3.22+ (for building the frontend)&lt;/li&gt; &#xA; &lt;li&gt;For Windows: Visual Studio Build Tools with C++ development workload&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;1. Frontend Setup&lt;/h3&gt; &#xA;&lt;h4&gt;Run packaged version&lt;/h4&gt; &#xA;&lt;p&gt;Go to the &lt;a href=&#34;https://github.com/Zackriya-Solutions/meeting-minutes/releases&#34;&gt;releases page&lt;/a&gt; and download the latest version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For Windows:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download either the &lt;code&gt;.exe&lt;/code&gt; installer or &lt;code&gt;.msi&lt;/code&gt; package&lt;/li&gt; &#xA; &lt;li&gt;Once the installer is downloaded, double-click the executable file to run it&lt;/li&gt; &#xA; &lt;li&gt;Windows will ask if you want to run untrusted apps, click &#34;More info&#34; and choose &#34;Run anyway&#34;&lt;/li&gt; &#xA; &lt;li&gt;Follow the installation wizard to complete the setup&lt;/li&gt; &#xA; &lt;li&gt;The application will be installed and available on your desktop&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the &lt;code&gt;dmg_darwin_arch64.zip&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Extract the file&lt;/li&gt; &#xA; &lt;li&gt;Double-click the &lt;code&gt;.dmg&lt;/code&gt; file inside the extracted folder&lt;/li&gt; &#xA; &lt;li&gt;Drag the application to your Applications folder&lt;/li&gt; &#xA; &lt;li&gt;Execute the following command in terminal to remove the quarantine attribute: &lt;pre&gt;&lt;code&gt;xattr -c /Applications/meeting-minutes-frontend.app&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Provide necessary permissions for audio capture and microphone access.&lt;/p&gt; &#xA;&lt;h4&gt;Dev run&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Navigate to frontend directory&#xA;cd frontend&#xA;&#xA;# Give execute permissions to clean_build.sh&#xA;chmod +x clean_build.sh&#xA;&#xA;# run clean_build.sh&#xA;./clean_build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Backend Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/Zackriya-Solutions/meeting-minutes.git&#xA;cd meeting-minutes/backend&#xA;&#xA;# Create and activate virtual environment&#xA;# On macOS/Linux:&#xA;python -m venv venv&#xA;source venv/bin/activate&#xA;&#xA;# On Windows:&#xA;python -m venv venv&#xA;.\venv\Scripts\activate&#xA;&#xA;# Install dependencies&#xA;pip install -r requirements.txt&#xA;&#xA;# Add environment file with API keys&#xA;# On macOS/Linux:&#xA;echo -e &#34;ANTHROPIC_API_KEY=your_api_key\nGROQ_API_KEY=your_api_key&#34; | tee .env&#xA;&#xA;# On Windows (PowerShell):&#xA;&#34;ANTHROPIC_API_KEY=your_api_key`nGROQ_API_KEY=your_api_key&#34; | Out-File -FilePath .env -Encoding utf8&#xA;&#xA;# Configure environment variables for Groq&#xA;# On macOS/Linux:&#xA;export GROQ_API_KEY=your_groq_api_key&#xA;&#xA;# On Windows (PowerShell):&#xA;$env:GROQ_API_KEY=&#34;your_groq_api_key&#34;&#xA;&#xA;# Build dependencies&#xA;# On macOS/Linux:&#xA;chmod +x build_whisper.sh&#xA;./build_whisper.sh&#xA;&#xA;# On Windows:&#xA;.\build_whisper.bat&#xA;&#xA;# Start backend servers&#xA;# On macOS/Linux:&#xA;./clean_start_backend.sh&#xA;&#xA;# On Windows:&#xA;.\start_with_output.ps1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development Guidelines&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the established project structure&lt;/li&gt; &#xA; &lt;li&gt;Write tests for new features&lt;/li&gt; &#xA; &lt;li&gt;Document API changes&lt;/li&gt; &#xA; &lt;li&gt;Use type hints in Python code&lt;/li&gt; &#xA; &lt;li&gt;Follow ESLint configuration for JavaScript/TypeScript&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository&lt;/li&gt; &#xA; &lt;li&gt;Create a feature branch&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License - Feel free to use this project for your own purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Introducing Subscription&lt;/h2&gt; &#xA;&lt;p&gt;We are planning to add a subscription option so that you don&#39;t have to run the backend on your own server. This will help you scale better and run the service 24/7. This is based on a few requests we received. If you are interested, please fill out the form &lt;a href=&#34;http://zackriya.com/aimeeting/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Last updated: March 3, 2025&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Zackriya-Solutions/meeting-minutes&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Zackriya-Solutions/meeting-minutes&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ggml-org/llama.cpp</title>
    <updated>2025-04-20T01:37:58Z</updated>
    <id>tag:github.com,2025-04-20:/ggml-org/llama.cpp</id>
    <link href="https://github.com/ggml-org/llama.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM inference in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png&#34; alt=&#34;llama&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml&#34;&gt;&lt;img src=&#34;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg?sanitize=true&#34; alt=&#34;Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/users/ggerganov/projects/7&#34;&gt;Roadmap&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/3471&#34;&gt;Project status&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/205&#34;&gt;Manifesto&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggml-org/ggml&#34;&gt;ggml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inference of Meta&#39;s &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt; model (and others) in pure C/C++&lt;/p&gt; &#xA;&lt;h2&gt;Recent API changes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/issues/9289&#34;&gt;Changelog for &lt;code&gt;libllama&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/issues/9291&#34;&gt;Changelog for &lt;code&gt;llama-server&lt;/code&gt; REST API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hot topics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to use &lt;a href=&#34;https://developer.apple.com/documentation/metal/mtlresidencyset?language=objc&#34;&gt;MTLResidencySet&lt;/a&gt; to keep the GPU memory active?&lt;/strong&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/11427&#34;&gt;https://github.com/ggml-org/llama.cpp/pull/11427&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VS Code extension for FIM completions:&lt;/strong&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.vscode&#34;&gt;https://github.com/ggml-org/llama.vscode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Universal &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/function-calling.md&#34;&gt;tool call support&lt;/a&gt; in &lt;code&gt;llama-server&lt;/code&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/9639&#34;&gt;https://github.com/ggml-org/llama.cpp/pull/9639&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vim/Neovim plugin for FIM completions: &lt;a href=&#34;https://github.com/ggml-org/llama.vim&#34;&gt;https://github.com/ggml-org/llama.vim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Introducing GGUF-my-LoRA &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/10123&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face Inference Endpoints now support GGUF out of the box! &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9669&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face GGUF editor: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9268&#34;&gt;discussion&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/CISCai/gguf-editor&#34;&gt;tool&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without any dependencies&lt;/li&gt; &#xA; &lt;li&gt;Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks&lt;/li&gt; &#xA; &lt;li&gt;AVX, AVX2, AVX512 and AMX support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use&lt;/li&gt; &#xA; &lt;li&gt;Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads MTT GPUs via MUSA)&lt;/li&gt; &#xA; &lt;li&gt;Vulkan and SYCL backend support&lt;/li&gt; &#xA; &lt;li&gt;CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;llama.cpp&lt;/code&gt; project is the main playground for developing new features for the &lt;a href=&#34;https://github.com/ggml-org/ggml&#34;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Models&lt;/summary&gt; &#xA; &lt;p&gt;Typically finetunes of the base models below are supported as well.&lt;/p&gt; &#xA; &lt;p&gt;Instructions for adding support for new models: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/HOWTO-add-model.md&#34;&gt;HOWTO-add-model.md&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Text-only&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA 🦙&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA 2 🦙🦙&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA 3 🦙🦙🦙&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;Mistral 7B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=mistral-ai/Mixtral&#34;&gt;Mixtral MoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;DBRX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=tiiuae/falcon&#34;&gt;Falcon&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese LLaMA / Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese LLaMA-2 / Alpaca-2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/bofenghuang/vigogne&#34;&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/5423&#34;&gt;BERT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;Koala&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=baichuan-inc/Baichuan&#34;&gt;Baichuan 1 &amp;amp; 2&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/hiyouga/baichuan-7b-sft&#34;&gt;derivations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=BAAI/Aquila&#34;&gt;Aquila 1 &amp;amp; 2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3187&#34;&gt;Starcoder models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/smallcloudai/Refact-1_6B-fim&#34;&gt;Refact&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3417&#34;&gt;MPT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3553&#34;&gt;Bloom&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=01-ai/Yi&#34;&gt;Yi models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/stabilityai&#34;&gt;StableLM models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=deepseek-ai/deepseek&#34;&gt;Deepseek models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Qwen/Qwen&#34;&gt;Qwen models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3557&#34;&gt;PLaMo-13B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=microsoft/phi&#34;&gt;Phi models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/11003&#34;&gt;PhiMoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/gpt2&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/5118&#34;&gt;Orion 14B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=internlm2&#34;&gt;InternLM2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/WisdomShell/codeshell&#34;&gt;CodeShell&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Gemma&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/state-spaces/mamba&#34;&gt;Mamba&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/keyfan/grok-1-hf&#34;&gt;Grok-1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=xverse&#34;&gt;Xverse&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=CohereForAI/c4ai-command-r&#34;&gt;Command-R models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=sea-lion&#34;&gt;SEA-LION&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/GritLM/GritLM-7B&#34;&gt;GritLM-7B&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/GritLM/GritLM-8x7B&#34;&gt;GritLM-8x7B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://allenai.org/olmo&#34;&gt;OLMo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://allenai.org/olmo&#34;&gt;OLMo 2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/allenai/OLMoE-1B-7B-0924&#34;&gt;OLMoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330&#34;&gt;Granite models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; + &lt;a href=&#34;https://github.com/EleutherAI/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520&#34;&gt;Snowflake-Arctic MoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Smaug&#34;&gt;Smaug&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/LumiOpen/Poro-34B&#34;&gt;Poro 34B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/1bitLLM&#34;&gt;Bitnet b1.58 models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=flan-t5&#34;&gt;Flan T5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca&#34;&gt;Open Elm models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/THUDM/chatglm3-6b&#34;&gt;ChatGLM3-6b&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/THUDM/glm-4-9b&#34;&gt;ChatGLM4-9b&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/THUDM/glm-edge-1.5b-chat&#34;&gt;GLMEdge-1.5b&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/THUDM/glm-edge-4b-chat&#34;&gt;GLMEdge-4b&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&#34;&gt;GLM-4-0414&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966&#34;&gt;SmolLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct&#34;&gt;EXAONE-3.0-7.8B-Instruct&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a&#34;&gt;FalconMamba Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/inceptionai/jais-13b-chat&#34;&gt;Jais&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a&#34;&gt;Bielik-11B-v2.3&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM&#34;&gt;RWKV-6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1&#34;&gt;QRWKV-6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct&#34;&gt;GigaChat-20B-A3B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/trillionlabs/Trillion-7B-preview&#34;&gt;Trillion-7B-preview&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&#34;&gt;Ling models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Multimodal&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e&#34;&gt;LLaVA 1.5 models&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2&#34;&gt;LLaVA 1.6 models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=SkunkworksAI/Bakllava&#34;&gt;BakLLaVA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/NousResearch/Obsidian-3B-V0.5&#34;&gt;Obsidian&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Lin-Chen/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=mobileVLM&#34;&gt;MobileVLM 1.7B/3B models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Yi-VL&#34;&gt;Yi-VL&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=MiniCPM&#34;&gt;Mini CPM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/vikhyatk/moondream2&#34;&gt;Moondream&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/BAAI-DCAI/Bunny&#34;&gt;Bunny&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=glm-edge&#34;&gt;GLM-EDGE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&#34;&gt;Qwen2-VL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Bindings&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python: &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Go: &lt;a href=&#34;https://github.com/go-skynet/go-llama.cpp&#34;&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Node.js: &lt;a href=&#34;https://github.com/withcatai/node-llama-cpp&#34;&gt;withcatai/node-llama-cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;JS/TS (llama.cpp server client): &lt;a href=&#34;https://modelfusion.dev/integration/model-provider/llamacpp&#34;&gt;lgrammel/modelfusion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;JS/TS (Programmable Prompt Engine CLI): &lt;a href=&#34;https://github.com/offline-ai/cli&#34;&gt;offline-ai/cli&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;JavaScript/Wasm (works in browser): &lt;a href=&#34;https://github.com/tangledgroup/llama-cpp-wasm&#34;&gt;tangledgroup/llama-cpp-wasm&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Typescript/Wasm (nicer API, available on npm): &lt;a href=&#34;https://github.com/ngxson/wllama&#34;&gt;ngxson/wllama&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ruby: &lt;a href=&#34;https://github.com/yoshoku/llama_cpp.rb&#34;&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (more features): &lt;a href=&#34;https://github.com/edgenai/llama_cpp-rs&#34;&gt;edgenai/llama_cpp-rs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (nicer API): &lt;a href=&#34;https://github.com/mdrokz/rust-llama.cpp&#34;&gt;mdrokz/rust-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (more direct bindings): &lt;a href=&#34;https://github.com/utilityai/llama-cpp-rs&#34;&gt;utilityai/llama-cpp-rs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (automated build from crates.io): &lt;a href=&#34;https://github.com/ShelbyJenkins/llm_client&#34;&gt;ShelbyJenkins/llm_client&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;C#/.NET: &lt;a href=&#34;https://github.com/SciSharp/LLamaSharp&#34;&gt;SciSharp/LLamaSharp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;C#/VB.NET (more features - community license): &lt;a href=&#34;https://docs.lm-kit.com/lm-kit-net/index.html&#34;&gt;LM-Kit.NET&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Scala 3: &lt;a href=&#34;https://github.com/donderom/llm4s&#34;&gt;donderom/llm4s&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Clojure: &lt;a href=&#34;https://github.com/phronmophobic/llama.clj&#34;&gt;phronmophobic/llama.clj&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;React Native: &lt;a href=&#34;https://github.com/mybigday/llama.rn&#34;&gt;mybigday/llama.rn&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Java: &lt;a href=&#34;https://github.com/kherud/java-llama.cpp&#34;&gt;kherud/java-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zig: &lt;a href=&#34;https://github.com/Deins/llama.cpp.zig&#34;&gt;deins/llama.cpp.zig&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Flutter/Dart: &lt;a href=&#34;https://github.com/netdur/llama_cpp_dart&#34;&gt;netdur/llama_cpp_dart&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Flutter: &lt;a href=&#34;https://github.com/xuegao-tzx/Fllama&#34;&gt;xuegao-tzx/Fllama&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;PHP (API bindings and features built on top of llama.cpp): &lt;a href=&#34;https://github.com/distantmagic/resonance&#34;&gt;distantmagic/resonance&lt;/a&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/6326&#34;&gt;(more info)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Guile Scheme: &lt;a href=&#34;https://savannah.nongnu.org/projects/guile-llama-cpp&#34;&gt;guile_llama_cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Swift &lt;a href=&#34;https://github.com/srgtuszy/llama-cpp-swift&#34;&gt;srgtuszy/llama-cpp-swift&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Swift &lt;a href=&#34;https://github.com/ShenghaiWang/SwiftLlama&#34;&gt;ShenghaiWang/SwiftLlama&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Delphi &lt;a href=&#34;https://github.com/Embarcadero/llama-cpp-delphi&#34;&gt;Embarcadero/llama-cpp-delphi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;UIs&lt;/summary&gt; &#xA; &lt;p&gt;&lt;em&gt;(to have a project listed here, it should clearly state that it depends on &lt;code&gt;llama.cpp&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&#34;&gt;AI Sublime Text plugin&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/cztomsik/ava&#34;&gt;cztomsik/ava&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/alexpinel/Dot&#34;&gt;Dot&lt;/a&gt; (GPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ylsdamxssjxxdd/eva&#34;&gt;eva&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/iohub/coLLaMA&#34;&gt;iohub/collama&lt;/a&gt; (Apache-2.0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/janhq/jan&#34;&gt;janhq/jan&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/johnbean393/Sidekick&#34;&gt;johnbean393/Sidekick&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/zhouwg/kantv?tab=readme-ov-file&#34;&gt;KanTV&lt;/a&gt; (Apache-2.0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/firatkiral/kodibot&#34;&gt;KodiBot&lt;/a&gt; (GPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.vim&#34;&gt;llama.vim&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/abgulati/LARS&#34;&gt;LARS&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/vietanhdev/llama-assistant&#34;&gt;Llama Assistant&lt;/a&gt; (GPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/guinmoon/LLMFarm?tab=readme-ov-file&#34;&gt;LLMFarm&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/undreamai/LLMUnity&#34;&gt;LLMUnity&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LMStudio&lt;/a&gt; (proprietary)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/mudler/LocalAI&#34;&gt;LocalAI&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/LostRuins/koboldcpp&#34;&gt;LostRuins/koboldcpp&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://mindmac.app&#34;&gt;MindMac&lt;/a&gt; (proprietary)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/MindWorkAI/AI-Studio&#34;&gt;MindWorkAI/AI-Studio&lt;/a&gt; (FSL-1.1-MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Mobile-Artificial-Intelligence/maid&#34;&gt;Mobile-Artificial-Intelligence/maid&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;Mozilla-Ocho/llamafile&lt;/a&gt; (Apache-2.0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/nat/openplayground&#34;&gt;nat/openplayground&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;nomic-ai/gpt4all&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama/ollama&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga/text-generation-webui&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/a-ghorbani/pocketpal-ai&#34;&gt;PocketPal AI&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/psugihara/FreeChat&#34;&gt;psugihara/FreeChat&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ptsochantaris/emeltal&#34;&gt;ptsochantaris/emeltal&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/pythops/tenere&#34;&gt;pythops/tenere&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/containers/ramalama&#34;&gt;ramalama&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/semperai/amica&#34;&gt;semperai/amica&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/withcatai/catai&#34;&gt;withcatai/catai&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/blackhole89/autopen&#34;&gt;Autopen&lt;/a&gt; (GPL)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tools&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/akx/ggify&#34;&gt;akx/ggify&lt;/a&gt; – download PyTorch models from HuggingFace Hub and convert them to GGML&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/akx/ollama-dl&#34;&gt;akx/ollama-dl&lt;/a&gt; – download models from the Ollama library to be used directly with llama.cpp&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/crashr/gppm&#34;&gt;crashr/gppm&lt;/a&gt; – launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser&#34;&gt;gpustack/gguf-parser&lt;/a&gt; - review/check the GGUF file and estimate the memory usage&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902&#34;&gt;Styled Lines&lt;/a&gt; (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Infrastructure&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/distantmagic/paddler&#34;&gt;Paddler&lt;/a&gt; - Stateful load balancer custom-tailored for llama.cpp&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/gpustack/gpustack&#34;&gt;GPUStack&lt;/a&gt; - Manage GPU clusters for running LLMs&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/onicai/llama_cpp_canister&#34;&gt;llama_cpp_canister&lt;/a&gt; - llama.cpp as a smart contract on the Internet Computer, using WebAssembly&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/mostlygeek/llama-swap&#34;&gt;llama-swap&lt;/a&gt; - transparent proxy that adds automatic model switching with llama-server&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/kalavai-net/kalavai-client&#34;&gt;Kalavai&lt;/a&gt; - Crowdsource end to end LLM deployment at any scale&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/InftyAI/llmaz&#34;&gt;llmaz&lt;/a&gt; - ☸️ Easy, advanced inference platform for large language models on Kubernetes.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Games&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/MorganRO8/Lucys_Labyrinth&#34;&gt;Lucy&#39;s Labyrinth&lt;/a&gt; - A simple maze game where agents controlled by an AI model will try to trick you.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Supported backends&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Backend&lt;/th&gt; &#xA;   &lt;th&gt;Target devices&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#metal-build&#34;&gt;Metal&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apple Silicon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#blas-build&#34;&gt;BLAS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/BLIS.md&#34;&gt;BLIS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/SYCL.md&#34;&gt;SYCL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel and Nvidia GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#musa&#34;&gt;MUSA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moore Threads MTT GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cuda&#34;&gt;CUDA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Nvidia GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#hip&#34;&gt;HIP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AMD GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#vulkan&#34;&gt;Vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cann&#34;&gt;CANN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ascend NPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md&#34;&gt;OpenCL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adreno GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/tree/master/examples/rpc&#34;&gt;RPC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Building the project&lt;/h2&gt; &#xA;&lt;p&gt;The main product of this project is the &lt;code&gt;llama&lt;/code&gt; library. Its C-style interface can be found in &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/include/llama.h&#34;&gt;include/llama.h&lt;/a&gt;. The project also includes many example programs and tools using the &lt;code&gt;llama&lt;/code&gt; library. The examples range from simple, minimal code snippets to sophisticated sub-projects such as an OpenAI-compatible HTTP server. Possible methods for obtaining the binaries:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repository and build locally, see &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&#34;&gt;how to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On MacOS or Linux, install &lt;code&gt;llama.cpp&lt;/code&gt; via &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/install.md&#34;&gt;brew, flox or nix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use a Docker image, see &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&#34;&gt;documentation for Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download pre-built binaries from &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/releases&#34;&gt;releases&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Obtaining and quantizing models&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://huggingface.co&#34;&gt;Hugging Face&lt;/a&gt; platform hosts a &lt;a href=&#34;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&#34;&gt;number of LLMs&lt;/a&gt; compatible with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&#34;&gt;Trending&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?sort=trending&amp;amp;search=llama+gguf&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can either manually download the GGUF file or directly use any &lt;code&gt;llama.cpp&lt;/code&gt;-compatible models from &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; or other model hosting sites, such as &lt;a href=&#34;https://modelscope.cn/&#34;&gt;ModelScope&lt;/a&gt;, by using this CLI argument: &lt;code&gt;-hf &amp;lt;user&amp;gt;/&amp;lt;model&amp;gt;[:quant]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default, the CLI would download from Hugging Face, you can switch to other options with the environment variable &lt;code&gt;MODEL_ENDPOINT&lt;/code&gt;. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. &lt;code&gt;MODEL_ENDPOINT=https://www.modelscope.cn/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After downloading a model, use the CLI tools to run it locally - see below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires the model to be stored in the &lt;a href=&#34;https://github.com/ggml-org/ggml/raw/master/docs/gguf.md&#34;&gt;GGUF&lt;/a&gt; file format. Models in other data formats can be converted to GGUF using the &lt;code&gt;convert_*.py&lt;/code&gt; Python scripts in this repo.&lt;/p&gt; &#xA;&lt;p&gt;The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://huggingface.co/spaces/ggml-org/gguf-my-repo&#34;&gt;GGUF-my-repo space&lt;/a&gt; to convert to GGUF format and quantize model weights to smaller sizes&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://huggingface.co/spaces/ggml-org/gguf-my-lora&#34;&gt;GGUF-my-LoRA space&lt;/a&gt; to convert LoRA adapters to GGUF format (more info: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/10123&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://huggingface.co/spaces/CISCai/gguf-editor&#34;&gt;GGUF-editor space&lt;/a&gt; to edit GGUF meta data in the browser (more info: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9268&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/9268&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://ui.endpoints.huggingface.co/&#34;&gt;Inference Endpoints&lt;/a&gt; to directly host &lt;code&gt;llama.cpp&lt;/code&gt; in the cloud (more info: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9669&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about model quantization, &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/quantize/README.md&#34;&gt;read this documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/main&#34;&gt;&lt;code&gt;llama-cli&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A CLI tool for accessing and experimenting with most of &lt;code&gt;llama.cpp&lt;/code&gt;&#39;s functionality.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Run in conversation mode&lt;/summary&gt; &#xA;   &lt;p&gt;Models with a built-in chat template will automatically activate conversation mode. If this doesn&#39;t occur, you can manually enable it by adding &lt;code&gt;-cnv&lt;/code&gt; and specifying a suitable chat template with &lt;code&gt;--chat-template NAME&lt;/code&gt;&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-cli -m model.gguf&#xA;&#xA;# &amp;gt; hi, who are you?&#xA;# Hi there! I&#39;m your helpful assistant! I&#39;m an AI-powered chatbot designed to assist and provide information to users like you. I&#39;m here to help answer your questions, provide guidance, and offer support on a wide range of topics. I&#39;m a friendly and knowledgeable AI, and I&#39;m always happy to help with anything you need. What&#39;s on your mind, and how can I assist you today?&#xA;#&#xA;# &amp;gt; what is 1+1?&#xA;# Easy peasy! The answer to 1+1 is... 2!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Run in conversation mode with custom chat template&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use the &#34;chatml&#34; template (use -h to see the list of supported templates)&#xA;llama-cli -m model.gguf -cnv --chat-template chatml&#xA;&#xA;# use a custom template&#xA;llama-cli -m model.gguf -cnv --in-prefix &#39;User: &#39; --reverse-prompt &#39;User:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Run simple text completion&lt;/summary&gt; &#xA;   &lt;p&gt;To disable conversation mode explicitly, use &lt;code&gt;-no-cnv&lt;/code&gt;&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-cli -m model.gguf -p &#34;I believe the meaning of life is&#34; -n 128 -no-cnv&#xA;&#xA;# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don&#39;t align with societal expectations. I think that&#39;s what I love about yoga – it&#39;s not just a physical practice, but a spiritual one too. It&#39;s about connecting with yourself, listening to your inner voice, and honoring your own unique journey.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Constrain the output with a custom grammar&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p &#39;Request: schedule a call at 8pm; Command:&#39;&#xA;&#xA;# {&#34;appointmentTime&#34;: &#34;8pm&#34;, &#34;appointmentDetails&#34;: &#34;schedule a a call&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/&#34;&gt;grammars/&lt;/a&gt; folder contains a handful of sample grammars. To write your own, check out the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&#34;&gt;GBNF Guide&lt;/a&gt;.&lt;/p&gt; &#xA;   &lt;p&gt;For authoring more complex JSON grammars, check out &lt;a href=&#34;https://grammar.intrinsiclabs.ai/&#34;&gt;https://grammar.intrinsiclabs.ai/&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/server&#34;&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A lightweight, &lt;a href=&#34;https://github.com/openai/openai-openapi&#34;&gt;OpenAI API&lt;/a&gt; compatible, HTTP server for serving LLMs.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Start a local HTTP server with default configuration on port 8080&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-server -m model.gguf --port 8080&#xA;&#xA;# Basic web UI can be accessed via browser: http://localhost:8080&#xA;# Chat completion endpoint: http://localhost:8080/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Support multiple-users and parallel decoding&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# up to 4 concurrent requests, each with 4096 max context&#xA;llama-server -m model.gguf -c 16384 -np 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Enable speculative decoding&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# the draft.gguf model should be a small variant of the target model.gguf&#xA;llama-server -m model.gguf -md draft.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Serve an embedding model&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use the /embedding endpoint&#xA;llama-server -m model.gguf --embedding --pooling cls -ub 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Serve a reranking model&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use the /reranking endpoint&#xA;llama-server -m model.gguf --reranking&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Constrain all outputs with a grammar&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# custom grammar&#xA;llama-server -m model.gguf --grammar-file grammar.gbnf&#xA;&#xA;# JSON&#xA;llama-server -m model.gguf --grammar-file grammars/json.gbnf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/perplexity&#34;&gt;&lt;code&gt;llama-perplexity&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A tool for measuring the perplexity &lt;a href=&#34;%5Bhttps://huggingface.co/docs/transformers/perplexity%5D(https://huggingface.co/docs/transformers/perplexity)&#34;&gt;^1&lt;/a&gt; (and other quality metrics) of a model over a given text.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Measure the perplexity over a text file&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-perplexity -m model.gguf -f file.txt&#xA;&#xA;# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...&#xA;# Final estimate: PPL = 5.4007 +/- 0.67339&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Measure KL divergence&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# TODO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/llama-bench&#34;&gt;&lt;code&gt;llama-bench&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Benchmark the performance of the inference for various parameters.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Run default benchmark&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-bench -m model.gguf&#xA;&#xA;# Output:&#xA;# | model               |       size |     params | backend    | threads |          test |                  t/s |&#xA;# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |&#xA;# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ± 20.55 |&#xA;# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ± 0.81 |&#xA;#&#xA;# build: 3e0ba0e60 (4229)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/run&#34;&gt;&lt;code&gt;llama-run&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A comprehensive example for running &lt;code&gt;llama.cpp&lt;/code&gt; models. Useful for inferencing. Used with RamaLama &lt;a href=&#34;%5BRamaLama%5D(https://github.com/containers/ramalama)&#34;&gt;^3&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Run a model with a specific prompt (by default it&#39;s pulled from Ollama registry)&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-run granite-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/simple&#34;&gt;&lt;code&gt;llama-simple&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A minimal example for implementing apps with &lt;code&gt;llama.cpp&lt;/code&gt;. Useful for developers.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Basic text completion&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-simple -m model.gguf&#xA;&#xA;# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called &#34;The Art of&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Contributors can open PRs&lt;/li&gt; &#xA; &lt;li&gt;Collaborators can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; &#xA; &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; &#xA; &lt;li&gt;Any help with managing issues, PRs and projects is very appreciated!&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&#34;&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information&lt;/li&gt; &#xA; &lt;li&gt;Make sure to read this: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/205&#34;&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A bit of backstory for those who are interested: &lt;a href=&#34;https://changelog.com/podcast/532&#34;&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/main/README.md&#34;&gt;main (cli)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/server/README.md&#34;&gt;server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&#34;&gt;GBNF grammars&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Development documentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&#34;&gt;How to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&#34;&gt;Running on Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/android.md&#34;&gt;Build on Android&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/token_generation_performance_tips.md&#34;&gt;Performance troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks&#34;&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Seminal papers and background on the models&lt;/h4&gt; &#xA;&lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLaMA: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-3 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-3.5 / InstructGPT / ChatGPT: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openai.com/research/instruction-following&#34;&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;XCFramework&lt;/h2&gt; &#xA;&lt;p&gt;The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;// swift-tools-version: 5.10&#xA;// The swift-tools-version declares the minimum version of Swift required to build this package.&#xA;&#xA;import PackageDescription&#xA;&#xA;let package = Package(&#xA;    name: &#34;MyLlamaPackage&#34;,&#xA;    targets: [&#xA;        .executableTarget(&#xA;            name: &#34;MyLlamaPackage&#34;,&#xA;            dependencies: [&#xA;                &#34;LlamaFramework&#34;&#xA;            ]),&#xA;        .binaryTarget(&#xA;            name: &#34;LlamaFramework&#34;,&#xA;            url: &#34;https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip&#34;,&#xA;            checksum: &#34;c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab&#34;&#xA;        )&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above example is using an intermediate build &lt;code&gt;b5046&lt;/code&gt; of the library. This can be modified to use a different version by changing the URL and checksum.&lt;/p&gt; &#xA;&lt;h2&gt;Completions&lt;/h2&gt; &#xA;&lt;p&gt;Command-line completion is available for some environments.&lt;/p&gt; &#xA;&lt;h4&gt;Bash Completion&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ build/bin/llama-cli --completion-bash &amp;gt; ~/.llama-completion.bash&#xA;$ source ~/.llama-completion.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally this can be added to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; to load it automatically. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ echo &#34;source ~/.llama-completion.bash&#34; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>TigerVNC/tigervnc</title>
    <updated>2025-04-20T01:37:58Z</updated>
    <id>tag:github.com,2025-04-20:/TigerVNC/tigervnc</id>
    <link href="https://github.com/TigerVNC/tigervnc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High performance, multi-platform VNC client and server&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;About TigerVNC&lt;/h1&gt; &#xA;&lt;p&gt;Virtual Network Computing (VNC) is a remote display system which allows you to view and interact with a virtual desktop environment that is running on another computer on the network. Using VNC, you can run graphical applications on a remote machine and send only the display from these applications to your local machine. VNC is platform-independent and supports a wide variety of operating systems and architectures as both servers and clients.&lt;/p&gt; &#xA;&lt;p&gt;TigerVNC is a high-speed version of VNC based on the RealVNC 4 and X.org code bases. TigerVNC started as a next-generation development effort for TightVNC on Unix and Linux platforms, but it split from its parent project in early 2009 so that TightVNC could focus on Windows platforms. TigerVNC supports a variant of Tight encoding that is greatly accelerated by the use of the libjpeg-turbo JPEG codec.&lt;/p&gt; &#xA;&lt;h1&gt;Legal&lt;/h1&gt; &#xA;&lt;p&gt;Incomplete and generally out of date copyright list::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    Copyright (C) 1999 AT&amp;amp;T Laboratories Cambridge&#xA;    Copyright (C) 2002-2005 RealVNC Ltd.&#xA;    Copyright (C) 2000-2006 TightVNC Group&#xA;    Copyright (C) 2005-2006 Martin Koegler&#xA;    Copyright (C) 2005-2006 Sun Microsystems, Inc.&#xA;    Copyright (C) 2006 OCCAM Financial Technology&#xA;    Copyright (C) 2000-2008 Constantin Kaplinsky&#xA;    Copyright (C) 2004-2017 Peter Astrand for Cendio AB&#xA;    Copyright (C) 2010 Antoine Martin&#xA;    Copyright (C) 2010 m-privacy GmbH&#xA;    Copyright (C) 2009-2011 D. R. Commander&#xA;    Copyright (C) 2009-2011 Pierre Ossman for Cendio AB&#xA;    Copyright (C) 2004, 2009-2011 Red Hat, Inc.&#xA;    Copyright (C) 2009-2025 TigerVNC team&#xA;    All Rights Reserved.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This software is distributed under the GNU General Public Licence as published by the Free Software Foundation. See the file LICENCE.TXT for the conditions under which this software is made available. TigerVNC also contains code from other sources. See the Acknowledgements section below, and the individual source files, for details of the conditions under which they are made available.&lt;/p&gt; &#xA;&lt;h1&gt;All Platforms&lt;/h1&gt; &#xA;&lt;p&gt;All versions of TigerVNC contain the following programs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vncviewer - the cross-platform TigerVNC Viewer, written using FLTK. vncviewer connects to a VNC server and allows you to interact with the remote desktop being displayed by the VNC server. The VNC server can be running on a Windows or a Unix/Linux machine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Windows-specific&lt;/h1&gt; &#xA;&lt;p&gt;The Windows version of TigerVNC contains the following programs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;winvnc - the TigerVNC server for Windows. winvnc allows a Windows desktop to be accessed remotely using a VNC viewer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;WARNING: winvnc is currently unmaintained and and may not function correctly.&lt;/p&gt; &#xA;&lt;p&gt;winvnc may not work if the Fast user switching or Remote desktop features are in use.&lt;/p&gt; &#xA;&lt;h1&gt;Unix/Linux-specific (not Mac)&lt;/h1&gt; &#xA;&lt;p&gt;The Unix/Linux version of TigerVNC contains the following programs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Xvnc - the TigerVNC server for Unix. Xvnc is both a VNC server and an X server with a &#34;virtual&#34; framebuffer. You should normally use the vncserver service to start Xvnc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;vncpasswd - a program which allows you to change the VNC password used to access your VNC server sessions (assuming that VNC authentication is being used.) This command must be run to set a password before using VNC authentication with any of the servers or services.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;vncconfig - a program which is used to configure and control a running instance of Xvnc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;x0vncserver - an inefficient VNC server which continuously polls any X display, allowing it to be controlled via VNC. It is intended mainly as a demonstration of a simple VNC server.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It also contains the following systemd service:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;mailto:vncserver@.service&#34;&gt;vncserver@.service&lt;/a&gt; - a service to start a user session with Xvnc and one of the desktop environments available on the system.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ACKNOWLEDGEMENTS&lt;/h1&gt; &#xA;&lt;p&gt;This distribution contains public domain DES software by Richard Outerbridge. This is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (c) 1988,1989,1990,1991,1992 by Richard Outerbridge.&#xA;(GEnie : OUTER; CIS : [71755,204]) Graven Imagery, 1992.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This distribution contains software from the X Window System. This is:&lt;/p&gt; &#xA;&lt;p&gt;Copyright 1987, 1988, 1998 The Open Group&lt;/p&gt; &#xA;&lt;p&gt;Permission to use, copy, modify, distribute, and sell this software and its documentation for any purpose is hereby granted without fee, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation.&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE OPEN GROUP BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt; &#xA;&lt;p&gt;Except as contained in this notice, the name of The Open Group shall not be used in advertising or otherwise to promote the sale, use or other dealings in this Software without prior written authorization from The Open Group.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 1987, 1988 by Digital Equipment Corporation, Maynard, Massachusetts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;                     All Rights Reserved&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Digital not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission.&lt;/p&gt; &#xA;&lt;p&gt;DIGITAL DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT SHALL DIGITAL BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.&lt;/p&gt;</summary>
  </entry>
</feed>