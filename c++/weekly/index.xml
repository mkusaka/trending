<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-10T01:59:17Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>notepad-plus-plus/notepad-plus-plus</title>
    <updated>2022-07-10T01:59:17Z</updated>
    <id>tag:github.com,2022-07-10:/notepad-plus-plus/notepad-plus-plus</id>
    <link href="https://github.com/notepad-plus-plus/notepad-plus-plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notepad++ official repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;What is Notepad++ ?&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/notepad-plus-plus/notepad-plus-plus.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://ci.appveyor.com/project/donho/notepad-plus-plus&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/github/notepad-plus-plus/notepad-plus-plus?branch=master&amp;amp;svg=true&#34; alt=&#34;Appveyor build status&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://community.notepad-plus-plus.org/&#34;&gt;&lt;img src=&#34;https://notepad-plus-plus.org/assets/images/NppCommunityBadge.svg?sanitize=true&#34; alt=&#34;Join the disscussions at https://community.notepad-plus-plus.org/&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Notepad++ is a free (free as in both &#34;free speech&#34; and &#34;free beer&#34;) source code editor and Notepad replacement that supports several programming languages and natural languages. Running in the MS Windows environment, its use is governed by &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/LICENSE&#34;&gt;GPL License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://notepad-plus-plus.org/&#34;&gt;Notepad++ official site&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Notepad++ Release Key&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Since the release of version 7.6.5 Notepad++ is signed using GPG with the following key:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Signer:&lt;/strong&gt; Notepad++&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;E-mail:&lt;/strong&gt; &lt;a href=&#34;mailto:don.h@free.fr&#34;&gt;don.h@free.fr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key ID:&lt;/strong&gt; 0x8D84F46E&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key fingerprint:&lt;/strong&gt; 14BC E436 2749 B2B5 1F8C 7122 6C42 9F1D 8D84 F46E&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Key type:&lt;/strong&gt; RSA 4096/4096&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Created:&lt;/strong&gt; 2019-03-11&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Expires:&lt;/strong&gt; 2024-03-11&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/notepad-plus-plus/notepad-plus-plus/raw/master/nppGpgPub.asc&#34;&gt;https://github.com/notepad-plus-plus/notepad-plus-plus/blob/master/nppGpgPub.asc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported OS&lt;/h2&gt; &#xA;&lt;p&gt;All the Windows systems still supported by Microsoft are supported by Notepad++. However, not all Notepad++ users can or want to use the newest system. Here is the &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/SUPPORTED_SYSTEM.md&#34;&gt;Supported systems information&lt;/a&gt; you may need in case you are one of them.&lt;/p&gt; &#xA;&lt;h2&gt;Build Notepad++&lt;/h2&gt; &#xA;&lt;p&gt;Please follow &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/BUILD.md&#34;&gt;build guide&lt;/a&gt; to build Notepad++ from source.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome. Be mindful of our &lt;a href=&#34;https://raw.githubusercontent.com/notepad-plus-plus/notepad-plus-plus/master/CONTRIBUTING.md&#34;&gt;Contribution Rules&lt;/a&gt; to increase the likelihood of your contribution getting accepted.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/notepad-plus-plus/notepad-plus-plus/graphs/contributors&#34;&gt;Notepad++ Contributors&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/FasterTransformer</title>
    <updated>2022-07-10T01:59:17Z</updated>
    <id>tag:github.com,2022-07-10:/NVIDIA/FasterTransformer</id>
    <link href="https://github.com/NVIDIA/FasterTransformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Transformer related optimization, including BERT, GPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FasterTransformer&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.&lt;/p&gt; &#xA;&lt;h2&gt;Table Of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#fastertransformer&#34;&gt;FasterTransformer&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#table-of-contents&#34;&gt;Table Of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#model-overview&#34;&gt;Model overview&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#support-matrix&#34;&gt;Support matrix&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#advanced&#34;&gt;Advanced&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#performance&#34;&gt;Performance&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performance&#34;&gt;BERT base performance&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performances-of-fastertransformer-new-features&#34;&gt;BERT base performances of FasterTransformer new features&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performance-on-tensorflow&#34;&gt;BERT base performance on TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#bert-base-performance-on-pytorch&#34;&gt;BERT base performance on PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#decoding-and-decoder-performance&#34;&gt;Decoding and Decoder performance&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#decoder-and-decoding-end-to-end-translation-performance-on-tensorflow&#34;&gt;Decoder and Decoding end-to-end translation performance on TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#decoder-and-decoding-end-to-end-translation-performance-on-pytorch&#34;&gt;Decoder and Decoding end-to-end translation performance on PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#gpt-performance&#34;&gt;GPT performance&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#release-notes&#34;&gt;Release notes&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#changelog&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/#known-issues&#34;&gt;Known issues&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model overview&lt;/h2&gt; &#xA;&lt;p&gt;In NLP, encoder and decoder are two important components, with the transformer layer becoming a popular architecture for both components. FasterTransformer implements a highly optimized transformer layer for both the encoder and decoder for inference. On Volta, Turing and Ampere GPUs, the computing power of Tensor Cores are used automatically when the precision of the data and weights are FP16.&lt;/p&gt; &#xA;&lt;p&gt;FasterTransformer is built on top of CUDA, cuBLAS, cuBLASLt and C++. We provide at least one API of the following frameworks: TensorFlow, PyTorch and Triton backend. Users can integrate FasterTransformer into these frameworks directly. For supporting frameworks, we also provide example codes to demonstrate how to use, and show the performance on these frameworks.&lt;/p&gt; &#xA;&lt;h3&gt;Support matrix&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;FP16&lt;/th&gt; &#xA;   &lt;th&gt;INT8 (after Turing)&lt;/th&gt; &#xA;   &lt;th&gt;Sparsity (after Ampere)&lt;/th&gt; &#xA;   &lt;th&gt;Tensor parallel&lt;/th&gt; &#xA;   &lt;th&gt;Pipeline parallel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XLNet&lt;/td&gt; &#xA;   &lt;td&gt;C++&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Encoder&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Encoder&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoder&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoder&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoding&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Longformer&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;Triton backend&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swin Transformer&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swin Transformer&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT&lt;/td&gt; &#xA;   &lt;td&gt;TensorRT&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note that the FasterTransformer supports the models above on C++ because all source codes are built on C++.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details of specific models are put in &lt;code&gt;xxx_guide.md&lt;/code&gt; of &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs&#34;&gt;&lt;code&gt;docs/&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;xxx&lt;/code&gt; means the model name. Some common questions and the respective answers are put in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/QAList.md&#34;&gt;&lt;code&gt;docs/QAList.md&lt;/code&gt;&lt;/a&gt;. Note that the model of Encoder and BERT are similar and we put the explanation into &lt;code&gt;bert_guide.md&lt;/code&gt; together.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced&lt;/h2&gt; &#xA;&lt;p&gt;The following code lists the directory structure of FasterTransformer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/src/fastertransformer: source code of FasterTransformer&#xA;    |--/models: Implementation of different models, like BERT, GPT.&#xA;    |--/layers: Implementation of layer modeuls, like attention layer, ffn layer.&#xA;    |--/kernels: CUDA kernels for different models/layers and operations, like addBiasResiual.&#xA;    |--/tensorrt_plugin: encapluate FasterTransformer into TensorRT plugin.&#xA;    |--/tf_op: custom Tensorflow OP implementation&#xA;    |--/th_op: custom PyTorch OP implementation&#xA;    |--/triton_backend: custom triton backend implementation&#xA;    |--/utils: Contains common cuda utils, like cublasMMWrapper, memory_utils&#xA;/examples: C++, tensorflow and pytorch interface examples&#xA;    |--/cpp: C++ interface examples&#xA;    |--/pytorch: PyTorch OP examples&#xA;    |--/tensorflow: TensorFlow OP examples&#xA;    |--tensorrt: TensorRT examples&#xA;/docs: Documents to explain the details of implementation of different models, and show the benchmark&#xA;/benchmark: Contains the scripts to run the benchmarks of different models&#xA;/tests: Unit tests&#xA;/templates: Documents to explain how to add a new model/example into FasterTransformer repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that many folders contains many sub-folders to split different models. Quantization tools are move to &lt;code&gt;examples&lt;/code&gt;, like &lt;code&gt;examples/tensorflow/bert/bert-quantization/&lt;/code&gt; and &lt;code&gt;examples/pytorch/bert/bert-quantization-sparsity/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Hardware settings:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8xA100-80GBs (with mclk 1593MHz, pclk 1410MHz) with AMD EPYC 7742 64-Core Processor&lt;/li&gt; &#xA; &lt;li&gt;T4 (with mclk 5000MHz, pclk 1590MHz) with Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In order to run the following benchmark, we need to install the unix computing tool &#34;bc&#34; by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get install bc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;BERT base performance&lt;/h3&gt; &#xA;&lt;p&gt;The FP16 results of TensorFlow were obtained by running the &lt;code&gt;benchmarks/bert/tf_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The INT8 results of TensorFlow were obtained by running the &lt;code&gt;benchmarks/bert/tf_int8_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The FP16 results of PyTorch were obtained by running the &lt;code&gt;benchmarks/bert/pyt_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The INT8 results of PyTorch were obtained by running the &lt;code&gt;benchmarks/bert/pyt_int8_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More benchmarks are put in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/bert_guide.md#bert-performance&#34;&gt;&lt;code&gt;docs/bert_guide.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;BERT base performances of FasterTransformer new features&lt;/h4&gt; &#xA;&lt;p&gt;The following figure compares the performances of different features of FasterTransformer and FasterTransformer under FP16 on T4.&lt;/p&gt; &#xA;&lt;p&gt;For large batch size and sequence length, both EFF-FT and FT-INT8-v2 bring about 2x speedup. Using Effective FasterTransformer and int8v2 at the same time can bring about 3.5x speedup compared to FasterTransformer FP16 for large case.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/FT_Encoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h4&gt;BERT base performance on TensorFlow&lt;/h4&gt; &#xA;&lt;p&gt;The following figure compares the performances of different features of FasterTransformer and TensorFlow XLA under FP16 on T4.&lt;/p&gt; &#xA;&lt;p&gt;For small batch size and sequence length, using FasterTransformer can bring about 3x speedup.&lt;/p&gt; &#xA;&lt;p&gt;For large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/TF_Encoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h4&gt;BERT base performance on PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;The following figure compares the performances of different features of FasterTransformer and PyTorch TorchScript under FP16 on T4.&lt;/p&gt; &#xA;&lt;p&gt;For small batch size and sequence length, using FasterTransformer CustomExt can bring about 4x ~ 6x speedup.&lt;/p&gt; &#xA;&lt;p&gt;For large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/Py_Encoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Decoding and Decoder performance&lt;/h3&gt; &#xA;&lt;p&gt;The results of TensorFlow were obtained by running the &lt;code&gt;benchmarks/decoding/tf_decoding_beamsearch_benchmark.sh&lt;/code&gt; and &lt;code&gt;benchmarks/decoding/tf_decoding_sampling_benchmark.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The results of PyTorch were obtained by running the &lt;code&gt;benchmarks/decoding/pyt_decoding_beamsearch_benchmark.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the experiments of decoding, we updated the following parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;head_num = 8&lt;/li&gt; &#xA; &lt;li&gt;size_per_head = 64&lt;/li&gt; &#xA; &lt;li&gt;num_layers = 6 for both encoder and decoder&lt;/li&gt; &#xA; &lt;li&gt;vocabulary_size = 32001 for TensorFlow sample codes, 31538 for PyTorch sample codes&lt;/li&gt; &#xA; &lt;li&gt;memory_hidden_dim = 512&lt;/li&gt; &#xA; &lt;li&gt;max sequenc elength = 128&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More benchmarks are put in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/decoder_guide.md#decoding-performance&#34;&gt;&lt;code&gt;docs/decoder_guide.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Decoder and Decoding end-to-end translation performance on TensorFlow&lt;/h4&gt; &#xA;&lt;p&gt;The following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to TensorFlow under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to TensorFlow, FT-Decoder provides 1.5x ~ 3x speedup; while FT-Decoding provides 4x ~ 18x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/TF_Decoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Decoder and Decoding end-to-end translation performance on PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;The following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to PyTorch under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to PyTorch, FT-Decoder provides 1.2x ~ 3x speedup; while FT-Decoding provides 3.8x ~ 13x speedup.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/Py_Decoder_T4.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;GPT performance&lt;/h3&gt; &#xA;&lt;p&gt;The following figure compares the performances of Megatron and FasterTransformer under FP16 on A100.&lt;/p&gt; &#xA;&lt;p&gt;In the experiments of decoding, we updated the following parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;head_num = 96&lt;/li&gt; &#xA; &lt;li&gt;size_per_head = 128&lt;/li&gt; &#xA; &lt;li&gt;num_layers = 48 for GPT-89B model, 96 for GPT-175B model&lt;/li&gt; &#xA; &lt;li&gt;data_type = FP16&lt;/li&gt; &#xA; &lt;li&gt;vocab_size = 51200&lt;/li&gt; &#xA; &lt;li&gt;top_p = 0.9&lt;/li&gt; &#xA; &lt;li&gt;tensor parallel size = 8&lt;/li&gt; &#xA; &lt;li&gt;input sequence length = 512&lt;/li&gt; &#xA; &lt;li&gt;ouptut sequence length = 32&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/docs/images/FT_GPT_A100.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Release notes&lt;/h2&gt; &#xA;&lt;h3&gt;Changelog&lt;/h3&gt; &#xA;&lt;p&gt;April 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the default accumulation type of all gemm to FP32.&lt;/li&gt; &#xA; &lt;li&gt;Support bfloat16 inference in GPT model.&lt;/li&gt; &#xA; &lt;li&gt;Support Nemo Megatron T5 and Megatron-LM T5 model.&lt;/li&gt; &#xA; &lt;li&gt;Support ViT.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;March 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support &lt;code&gt;stop_ids&lt;/code&gt; and &lt;code&gt;ban_bad_ids&lt;/code&gt; in GPT-J.&lt;/li&gt; &#xA; &lt;li&gt;Support dynamice &lt;code&gt;start_id&lt;/code&gt; and &lt;code&gt;end_id&lt;/code&gt; in GPT-J, GPT, T5 and Decoding.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Febuary 2022&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support Swin Transformer.&lt;/li&gt; &#xA; &lt;li&gt;Optimize the k/v cache update of beam search by in-direction buffer.&lt;/li&gt; &#xA; &lt;li&gt;Support runtime input for GPT-J, T5 and GPT.&lt;/li&gt; &#xA; &lt;li&gt;Support soft prompt in GPT and GPT-J.&lt;/li&gt; &#xA; &lt;li&gt;Support custom all reduce kernel. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Limitation: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Only support tensor parallel size = 8 on DGX-A100.&lt;/li&gt; &#xA;     &lt;li&gt;Only support CUDA with cudaMallocAsync.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;December 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add TensorRT plugin of T5 model.&lt;/li&gt; &#xA; &lt;li&gt;Change some hyper-parameters of GPT model to runtime query.&lt;/li&gt; &#xA; &lt;li&gt;Optimize the memory allocator under C++ code.&lt;/li&gt; &#xA; &lt;li&gt;Fix bug of CUB including when using CUDA 11.5 or newer version.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;November 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Update the FasterTransformer 5.0 beta&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add GPT-3 INT8 weight only qauntization for batch size &amp;lt;= 2.&lt;/li&gt; &#xA; &lt;li&gt;Support multi-node multi-gpu support on T5.&lt;/li&gt; &#xA; &lt;li&gt;Enhance the multi-node multi-gpu supporting in GPT-3.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;August 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 5.0 beta&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Refactor the repo and codes&lt;/li&gt; &#xA;   &lt;li&gt;And special thanks to NAVER Corp. for contributing a lot to this version, as listed below. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Bugs fix &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Fix error that occurs when batch_size is less than max_batch_size for gpt pytorch wrapper.&lt;/li&gt; &#xA;       &lt;li&gt;Fix memory leak that occurs every forward because of reused allocator.&lt;/li&gt; &#xA;       &lt;li&gt;Fix race condition that occurs in repetition penalty kernel.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Enhancement &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Add random seed setting.&lt;/li&gt; &#xA;       &lt;li&gt;Fix GEMM buffer overflow on FP16 of GPT.&lt;/li&gt; &#xA;       &lt;li&gt;Change to invalidate finished buffer for every completion.&lt;/li&gt; &#xA;       &lt;li&gt;Introduce stop_before for early stop.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Support Longformer.&lt;/li&gt; &#xA;   &lt;li&gt;Rename &lt;code&gt;layer_para&lt;/code&gt; to &lt;code&gt;pipeline_para&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Optimize the sorting of top p sampling.&lt;/li&gt; &#xA;   &lt;li&gt;Support sparsity for Ampere GPUs on BERT.&lt;/li&gt; &#xA;   &lt;li&gt;Support &lt;code&gt;size_per_head&lt;/code&gt; 96, 160, 192, 224, 256 for GPT model.&lt;/li&gt; &#xA;   &lt;li&gt;Support multi-node inference for GPT Triton backend.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;June 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support XLNet&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;April 2021&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 4.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support multi-gpus and multi-nodes inference for GPT model on C++ and PyTorch.&lt;/li&gt; &#xA;   &lt;li&gt;Support single node, multi-gpus inference for GPT model on triton.&lt;/li&gt; &#xA;   &lt;li&gt;Add the int8 fused multi-head attention kernel for bert.&lt;/li&gt; &#xA;   &lt;li&gt;Add the FP16 fused multi-head attention kernel of V100 for bert.&lt;/li&gt; &#xA;   &lt;li&gt;Optimize the kernel of decoder.&lt;/li&gt; &#xA;   &lt;li&gt;Move to independent repo.&lt;/li&gt; &#xA;   &lt;li&gt;Eager mode PyTorch extension is deprecated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Dec 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 3.1&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optimize the decoding by adding the finisehd mask to prevent useless computing.&lt;/li&gt; &#xA;   &lt;li&gt;Support opennmt encoder.&lt;/li&gt; &#xA;   &lt;li&gt;Remove the TensorRT plugin supporting.&lt;/li&gt; &#xA;   &lt;li&gt;TorchScript custom op is deprecated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Nov 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimize the INT8 inference.&lt;/li&gt; &#xA; &lt;li&gt;Support PyTorch INT8 inference.&lt;/li&gt; &#xA; &lt;li&gt;Provide PyTorch INT8 quantiztion tools.&lt;/li&gt; &#xA; &lt;li&gt;Integrate the fused multi-head attention kernel of TensorRT into FasterTransformer.&lt;/li&gt; &#xA; &lt;li&gt;Add unit test of SQuAD.&lt;/li&gt; &#xA; &lt;li&gt;Update the missed NGC checkpoints.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sep 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support GPT2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 3.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support INT8 quantization of encoder of cpp and TensorFlow op.&lt;/li&gt; &#xA;   &lt;li&gt;Add bert-tf-quantization tool.&lt;/li&gt; &#xA;   &lt;li&gt;Fix the issue that Cmake 15 or Cmake 16 fail to build this project.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Aug 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix the bug of trt plugin.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;June 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 2.1&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add Effective FasterTransformer based on the idea of &lt;a href=&#34;https://github.com/bytedance/effective_transformer&#34;&gt;Effective Transformer&lt;/a&gt; idea.&lt;/li&gt; &#xA;   &lt;li&gt;Optimize the beam search kernels.&lt;/li&gt; &#xA;   &lt;li&gt;Add PyTorch op supporting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;May 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix the bug that seq_len of encoder must be larger than 3.&lt;/li&gt; &#xA; &lt;li&gt;Add the position_encoding of decoding as the input of FasterTransformer decoding. This is convenient to use different types of position encoding. FasterTransformer does not compute the position encoding value, but only lookup the table.&lt;/li&gt; &#xA; &lt;li&gt;Modifying the method of loading model in &lt;code&gt;translate_sample.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;April 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rename &lt;code&gt;decoding_opennmt.h&lt;/code&gt; to &lt;code&gt;decoding_beamsearch.h&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add DiverseSiblingsSearch for decoding.&lt;/li&gt; &#xA; &lt;li&gt;Add sampling into Decoding &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The implementation is in the &lt;code&gt;decoding_sampling.h&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Add top_k sampling, top_p sampling for decoding.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Refactor the tensorflow custom op codes. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Merge &lt;code&gt;bert_transformer_op.h&lt;/code&gt;, &lt;code&gt;bert_transformer_op.cu.cc&lt;/code&gt; into &lt;code&gt;bert_transformer_op.cc&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Merge &lt;code&gt;decoder.h&lt;/code&gt;, &lt;code&gt;decoder.cu.cc&lt;/code&gt; into &lt;code&gt;decoder.cc&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Merge &lt;code&gt;decoding_beamsearch.h&lt;/code&gt;, &lt;code&gt;decoding_beamsearch.cu.cc&lt;/code&gt; into &lt;code&gt;decoding_beamsearch.cc&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix the bugs of finalize function decoding.py.&lt;/li&gt; &#xA; &lt;li&gt;Fix the bug of tf DiverseSiblingSearch.&lt;/li&gt; &#xA; &lt;li&gt;Add BLEU scorer &lt;code&gt;bleu_score.py&lt;/code&gt; into &lt;code&gt;utils&lt;/code&gt;. Note that the BLEU score requires python3.&lt;/li&gt; &#xA; &lt;li&gt;Fuse QKV Gemm of encoder and masked_multi_head_attention of decoder.&lt;/li&gt; &#xA; &lt;li&gt;Add dynamic batch size and dynamic sequence length features into all ops.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;March 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add feature in FasterTransformer 2.0 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;translate_sample.py&lt;/code&gt; to demonstrate how to translate a sentence by restoring the pretrained model of OpenNMT-tf.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix bugs of Fastertransformer 2.0 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fix the bug of maximum sequence length of decoder cannot be larger than 128.&lt;/li&gt; &#xA;   &lt;li&gt;Fix the bug that decoding does not check finish or not after each step.&lt;/li&gt; &#xA;   &lt;li&gt;Fix the bug of decoder about max_seq_len.&lt;/li&gt; &#xA;   &lt;li&gt;Modify the decoding model structure to fit the OpenNMT-tf decoding model. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Add a layer normalization layer after decoder.&lt;/li&gt; &#xA;     &lt;li&gt;Add a normalization for inputs of decoder&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Febuary 2020&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 2.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Provide a highly optimized OpenNMT-tf based decoder and decoding, including C++ API and TensorFlow op.&lt;/li&gt; &#xA;   &lt;li&gt;Refine the sample codes of encoder.&lt;/li&gt; &#xA;   &lt;li&gt;Add dynamic batch size feature into encoder op.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;July 2019&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Release the FasterTransformer 1.0&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Provide a highly optimized bert equivalent transformer layer, including C++ API, TensorFlow op and TensorRT plugin.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Undefined symbol errors when import the extension &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Please &lt;code&gt;import torch&lt;/code&gt; first. If this has been done, it is due to the incompatible C++ ABI. You may need to check the PyTorch used during compilation and execution are the same, or you need to check how your PyTorch is compiled, or the version of your GCC, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Results of TensorFlow and OP would be different in decoding. This problem is caused by the accumulated log probability, and we do not avoid this problem.&lt;/li&gt; &#xA; &lt;li&gt;If encounter some problem in the custom environment, try to use the gcc/g++ 4.8 to build the project of TensorFlow op, especially for TensorFlow 1.14.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>WerWolv/ImHex</title>
    <updated>2022-07-10T01:59:17Z</updated>
    <id>tag:github.com,2022-07-10:/WerWolv/ImHex</id>
    <link href="https://github.com/WerWolv/ImHex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üîç A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://imhex.werwolv.net&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;a href=&#34;https://imhex.werwolv.net&#34;&gt;&lt;span&gt;üîç&lt;/span&gt; ImHex&lt;/a&gt;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a title=&#34;&#39;Build&#39; workflow Status&#34; href=&#34;https://github.com/WerWolv/ImHex/actions?query=workflow%3ABuild&#34;&gt;&lt;img alt=&#34;&#39;Build&#39; workflow Status&#34; src=&#34;https://img.shields.io/github/workflow/status/WerWolv/ImHex/Build?longCache=true&amp;amp;style=for-the-badge&amp;amp;label=Build&amp;amp;logoColor=fff&amp;amp;logo=GitHub%20Actions&#34;&gt;&lt;/a&gt; &lt;a title=&#34;Discord Server&#34; href=&#34;https://discord.gg/X63jZ36xBY&#34;&gt;&lt;img alt=&#34;Discord Server&#34; src=&#34;https://img.shields.io/discord/789833418631675954?label=Discord&amp;amp;logo=Discord&amp;amp;style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;a title=&#34;Total Downloads&#34; href=&#34;https://github.com/WerWolv/ImHex/releases/latest&#34;&gt;&lt;img alt=&#34;Total Downloads&#34; src=&#34;https://img.shields.io/github/downloads/WerWolv/ImHex/total?longCache=true&amp;amp;style=for-the-badge&amp;amp;label=Downloads&amp;amp;logoColor=fff&amp;amp;logo=GitHub&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Supporting&lt;/h2&gt; &#xA;&lt;p&gt;If you like my work, please consider supporting me on GitHub Sponsors, Patreon or PayPal. Thanks a lot!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/sponsors/WerWolv&#34;&gt;&lt;img src=&#34;https://werwolv.net/assets/github_banner.png&#34; alt=&#34;GitHub donate button&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/werwolv&#34;&gt;&lt;img src=&#34;https://c5.patreon.com/external/logo/become_a_patron_button.png&#34; alt=&#34;Patreon donate button&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://werwolv.net/donate&#34;&gt;&lt;img src=&#34;https://werwolv.net/assets/paypal_banner.png&#34; alt=&#34;PayPal donate button&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10835354/139717326-8044769d-527b-4d88-8adf-2d4ecafdca1f.png&#34; alt=&#34;Hex editor, patterns and data information&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/10835354/139717323-1f8c9d52-f7eb-4f43-9f11-097ac728ed6c.png&#34; alt=&#34;Bookmarks, disassembler and data processor&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Featureful hex view &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Byte patching&lt;/li&gt; &#xA;   &lt;li&gt;Patch management&lt;/li&gt; &#xA;   &lt;li&gt;Copy bytes as feature &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Bytes&lt;/li&gt; &#xA;     &lt;li&gt;Hex string&lt;/li&gt; &#xA;     &lt;li&gt;C, C++, C#, Rust, Python, Java &amp;amp; JavaScript array&lt;/li&gt; &#xA;     &lt;li&gt;ASCII-Art hex view&lt;/li&gt; &#xA;     &lt;li&gt;HTML self-contained div&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;String and hex search&lt;/li&gt; &#xA;   &lt;li&gt;Colorful highlighting&lt;/li&gt; &#xA;   &lt;li&gt;Goto from start, end and current cursor position&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Custom C++-like pattern language for parsing highlighting a file&#39;s content &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Automatic loading based on MIME type&lt;/li&gt; &#xA;   &lt;li&gt;arrays, pointers, structs, unions, enums, bitfields, namespaces, little and big endian support, conditionals and much more!&lt;/li&gt; &#xA;   &lt;li&gt;Useful error messages, syntax highlighting and error marking&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data importing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Base64 files&lt;/li&gt; &#xA;   &lt;li&gt;IPS and IPS32 patches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data exporting &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;IPS and IPS32 patches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data inspector allowing interpretation of data as many different types (little and big endian)&lt;/li&gt; &#xA; &lt;li&gt;Huge file support with fast and efficient loading&lt;/li&gt; &#xA; &lt;li&gt;String search &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Copying of strings&lt;/li&gt; &#xA;   &lt;li&gt;Copying of demangled strings&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;File hashing support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CRC16 and CRC32 with custom initial values and polynomials&lt;/li&gt; &#xA;   &lt;li&gt;MD4, MD5&lt;/li&gt; &#xA;   &lt;li&gt;SHA-1, SHA-224, SHA-256, SHA-384, SHA-512&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Disassembler supporting many architectures (frontend for Capstone) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ARM32 (ARM, Thumb, Cortex-M, AArch32)&lt;/li&gt; &#xA;   &lt;li&gt;ARM64&lt;/li&gt; &#xA;   &lt;li&gt;MIPS (MIPS32, MIPS64, MIPS32R6, Micro)&lt;/li&gt; &#xA;   &lt;li&gt;x86 (16-bit, 32-bit, 64-bit)&lt;/li&gt; &#xA;   &lt;li&gt;PowerPC (32-bit, 64-bit)&lt;/li&gt; &#xA;   &lt;li&gt;SPARC&lt;/li&gt; &#xA;   &lt;li&gt;IBM SystemZ&lt;/li&gt; &#xA;   &lt;li&gt;xCORE&lt;/li&gt; &#xA;   &lt;li&gt;M68K&lt;/li&gt; &#xA;   &lt;li&gt;TMS320C64X&lt;/li&gt; &#xA;   &lt;li&gt;M680X&lt;/li&gt; &#xA;   &lt;li&gt;Ethereum&lt;/li&gt; &#xA;   &lt;li&gt;RISC-V&lt;/li&gt; &#xA;   &lt;li&gt;WebAssembly&lt;/li&gt; &#xA;   &lt;li&gt;MOS565XX&lt;/li&gt; &#xA;   &lt;li&gt;Berkeley Packet Filter&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Bookmarks &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Region highlighting&lt;/li&gt; &#xA;   &lt;li&gt;Comments&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data Analyzer &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;File magic-based file parser and MIME type database&lt;/li&gt; &#xA;   &lt;li&gt;Byte distribution graph&lt;/li&gt; &#xA;   &lt;li&gt;Entropy graph&lt;/li&gt; &#xA;   &lt;li&gt;Highest and average entropy&lt;/li&gt; &#xA;   &lt;li&gt;Encrypted / Compressed file detection&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Built-in Content Store &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Download all files found in the database directly from within ImHex&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Yara Rules support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Quickly scan a file for vulnerabilities with official yara rules&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Helpful tools &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Itanium and MSVC demangler&lt;/li&gt; &#xA;   &lt;li&gt;ASCII table&lt;/li&gt; &#xA;   &lt;li&gt;Regex replacer&lt;/li&gt; &#xA;   &lt;li&gt;Mathematical expression evaluator (Calculator)&lt;/li&gt; &#xA;   &lt;li&gt;Hexadecimal Color picker&lt;/li&gt; &#xA;   &lt;li&gt;Base converter&lt;/li&gt; &#xA;   &lt;li&gt;UNIX Permissions calculator&lt;/li&gt; &#xA;   &lt;li&gt;Anonfiles File upload tool&lt;/li&gt; &#xA;   &lt;li&gt;Wikipedia term definition finder&lt;/li&gt; &#xA;   &lt;li&gt;File utilities &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;File splitter&lt;/li&gt; &#xA;     &lt;li&gt;File combiner&lt;/li&gt; &#xA;     &lt;li&gt;File shredder&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Built-in cheat sheet for pattern language and Math evaluator&lt;/li&gt; &#xA; &lt;li&gt;Doesn&#39;t burn out your retinas when used in late-night sessions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pattern Language&lt;/h2&gt; &#xA;&lt;p&gt;The custom C-like Pattern Language developed and used by ImHex is easy to read, understand and learn. A guide with all features of the language can be found &lt;a href=&#34;http://imhex.werwolv.net/docs&#34;&gt;on the docs page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Database&lt;/h2&gt; &#xA;&lt;p&gt;For format patterns, includable libraries magic and constant files, check out the &lt;a href=&#34;https://github.com/WerWolv/ImHex-Patterns&#34;&gt;ImHex-Patterns&lt;/a&gt; repository. Feel free to PR your own files there as well!&lt;/p&gt; &#xA;&lt;h2&gt;Plugin development&lt;/h2&gt; &#xA;&lt;p&gt;To develop plugins for ImHex, use one of the following two templates projects to get started. You then have access to the entirety of libimhex as well as the ImHex API and the Content Registry to interact with ImHex or to add new content.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WerWolv/ImHex-Cpp-Plugin-Template&#34;&gt;C++ Plugin Template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WerWolv/ImHex-Rust-Plugin-Template&#34;&gt;Rust Plugin Template&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Nightly builds&lt;/h2&gt; &#xA;&lt;p&gt;Nightlies are available via GitHub Actions &lt;a href=&#34;https://github.com/WerWolv/ImHex/actions?query=workflow%3ABuild&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows ‚Ä¢ &lt;strong&gt;x86_64&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Windows%20Installer.zip&#34;&gt;Installer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Windows%20Portable%20ZIP.zip&#34;&gt;Portable&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MacOS ‚Ä¢ &lt;strong&gt;x86_64&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/macOS%20DMG.zip&#34;&gt;DMG&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Linux ‚Ä¢ &lt;strong&gt;x86_64&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Linux%20DEB.zip&#34;&gt;DEB&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/Linux%20AppImage.zip&#34;&gt;AppImage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nightly.link/WerWolv/ImHex/workflows/build/master/ArchLinux%20.pkg.tar.zst.zip&#34;&gt;Arch Package&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compiling&lt;/h2&gt; &#xA;&lt;p&gt;To compile ImHex, a C++20 compiler is required. Releases are all mainly built using GCC, however on macOS, clang is also required to compile some ObjC code.&lt;/p&gt; &#xA;&lt;p&gt;Many dependencies are bundled into the repository using submodules so make sure to clone it using the &lt;code&gt;--recurse-submodules&lt;/code&gt; option. All dependencies that aren&#39;t bundled, can be installed using the dependency installer scripts found in the &lt;code&gt;/dist&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Thog&#34;&gt;Mary&lt;/a&gt; for her immense help porting ImHex to MacOS and help during development&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Roblabla&#34;&gt;Roblabla&lt;/a&gt; for adding MSI Installer support to ImHex&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jam1garner&#34;&gt;jam1garner&lt;/a&gt; and &lt;a href=&#34;https://github.com/raytwo&#34;&gt;raytwo&lt;/a&gt; for their help with adding Rust support to plugins&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mailaender&#34;&gt;Mailaender&lt;/a&gt; for getting ImHex onto Flathub&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iTrooz&#34;&gt;iTrooz&lt;/a&gt; for many improvements related to release packaging and the GitHub Action runners.&lt;/li&gt; &#xA; &lt;li&gt;Everybody else who has reported issues on Discord or GitHub that I had great conversations with :)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks a lot to ocornut for their amazing &lt;a href=&#34;https://github.com/ocornut/imgui&#34;&gt;Dear ImGui&lt;/a&gt; which is used for building the entire interface &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Thanks to ocornut as well for their hex editor view used as base for this project.&lt;/li&gt; &#xA;   &lt;li&gt;Thanks to BalazsJako for their incredible &lt;a href=&#34;https://github.com/BalazsJako/ImGuiColorTextEdit&#34;&gt;ImGuiColorTextEdit&lt;/a&gt; used for the pattern language syntax highlighting&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Thanks to nlohmann for their &lt;a href=&#34;https://github.com/nlohmann/json&#34;&gt;json&lt;/a&gt; library used for project files&lt;/li&gt; &#xA; &lt;li&gt;Thanks to aquynh for &lt;a href=&#34;https://github.com/aquynh/capstone&#34;&gt;capstone&lt;/a&gt; which is the base of the disassembly window&lt;/li&gt; &#xA; &lt;li&gt;Thanks to vitaut for their &lt;a href=&#34;https://github.com/fmtlib/fmt&#34;&gt;libfmt&lt;/a&gt; library which makes formatting and logging so much better&lt;/li&gt; &#xA; &lt;li&gt;Thanks to rxi for &lt;a href=&#34;https://github.com/rxi/microtar&#34;&gt;microtar&lt;/a&gt; used for extracting downloaded store assets&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>