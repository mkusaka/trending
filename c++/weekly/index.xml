<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-24T01:42:56Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Mozilla-Ocho/llamafile</title>
    <updated>2024-03-24T01:42:56Z</updated>
    <id>tag:github.com,2024-03-24:/Mozilla-Ocho/llamafile</id>
    <link href="https://github.com/Mozilla-Ocho/llamafile" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Distribute and run LLMs with a single file.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llamafile&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/llamafile-640x640.png&#34; width=&#34;320&#34; height=&#34;320&#34; alt=&#34;[line drawing of llama animal head in front of slightly open manilla folder filled with files]&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;llamafile lets you distribute and run LLMs with a single file. (&lt;a href=&#34;https://hacks.mozilla.org/2023/11/introducing-llamafile/&#34;&gt;announcement blog post&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/teDuGYVTB2&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/teDuGYVTB2&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our goal is to make open LLMs much more accessible to both developers and end users. We&#39;re doing that by combining &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; with &lt;a href=&#34;https://github.com/jart/cosmopolitan&#34;&gt;Cosmopolitan Libc&lt;/a&gt; into one framework that collapses all the complexity of LLMs down to a single-file executable (called a &#34;llamafile&#34;) that runs locally on most computers, with no installation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/teDuGYVTB2&#34;&gt;Join us on our Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to try it for yourself is to download our example llamafile for the &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt; model (license: &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;, &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI&lt;/a&gt;). LLaVA is a new LLM that can do more than just chat; you can also upload images and ask it questions about them. With llamafile, this all happens locally; no data ever leaves your computer.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true&#34;&gt;llava-v1.5-7b-q4.llamafile&lt;/a&gt; (3.97 GB).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open your computer&#39;s terminal.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you&#39;re using macOS, Linux, or BSD, you&#39;ll need to grant permission for your computer to execute this new file. (You only need to do this once.)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;chmod +x llava-v1.5-7b-q4.llamafile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;If you&#39;re on Windows, rename the file by adding &#34;.exe&#34; on the end.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the llamafile. e.g.:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./llava-v1.5-7b-q4.llamafile -ngl 9999&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Your browser should open automatically and display a chat interface. (If it doesn&#39;t, just open your browser and point it at &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When you&#39;re done chatting, return to your terminal and hit &lt;code&gt;Control-C&lt;/code&gt; to shut down llamafile.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Having trouble? See the &#34;Gotchas&#34; section below.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;JSON API Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;When llamafile is started, in addition to hosting a web UI chat server at &lt;a href=&#34;http://127.0.0.1:8080/&#34;&gt;http://127.0.0.1:8080/&lt;/a&gt;, an &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI API&lt;/a&gt; compatible chat completions endpoint is provided too. It&#39;s designed to support the most common OpenAI API use cases, in a way that runs entirely locally. We&#39;ve also extended it to include llama.cpp specific features (e.g. mirostat) that may also be used. For further details on what fields and endpoints are available, refer to both the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;OpenAI documentation&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llama.cpp/server/README.md#api-endpoints&#34;&gt;llamafile server README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Curl API Client Example&lt;/summary&gt; &#xA; &lt;p&gt;The simplest way to get started using the API is to copy and paste the following curl command into your terminal.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl http://localhost:8080/v1/chat/completions \&#xA;-H &#34;Content-Type: application/json&#34; \&#xA;-H &#34;Authorization: Bearer no-key&#34; \&#xA;-d &#39;{&#xA;  &#34;model&#34;: &#34;LLaMA_CPP&#34;,&#xA;  &#34;messages&#34;: [&#xA;      {&#xA;          &#34;role&#34;: &#34;system&#34;,&#xA;          &#34;content&#34;: &#34;You are LLAMAfile, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.&#34;&#xA;      },&#xA;      {&#xA;          &#34;role&#34;: &#34;user&#34;,&#xA;          &#34;content&#34;: &#34;Write a limerick about python exceptions&#34;&#xA;      }&#xA;    ]&#xA;}&#39; | python3 -c &#39;&#xA;import json&#xA;import sys&#xA;json.dump(json.load(sys.stdin), sys.stdout, indent=2)&#xA;print()&#xA;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The response that&#39;s printed should look like the following:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;   &#34;choices&#34; : [&#xA;      {&#xA;         &#34;finish_reason&#34; : &#34;stop&#34;,&#xA;         &#34;index&#34; : 0,&#xA;         &#34;message&#34; : {&#xA;            &#34;content&#34; : &#34;There once was a programmer named Mike\nWho wrote code that would often choke\nHe used try and except\nTo handle each step\nAnd his program ran without any hike.&#34;,&#xA;            &#34;role&#34; : &#34;assistant&#34;&#xA;         }&#xA;      }&#xA;   ],&#xA;   &#34;created&#34; : 1704199256,&#xA;   &#34;id&#34; : &#34;chatcmpl-Dt16ugf3vF8btUZj9psG7To5tc4murBU&#34;,&#xA;   &#34;model&#34; : &#34;LLaMA_CPP&#34;,&#xA;   &#34;object&#34; : &#34;chat.completion&#34;,&#xA;   &#34;usage&#34; : {&#xA;      &#34;completion_tokens&#34; : 38,&#xA;      &#34;prompt_tokens&#34; : 78,&#xA;      &#34;total_tokens&#34; : 116&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Python API Client example&lt;/summary&gt; &#xA; &lt;p&gt;If you&#39;ve already developed your software using the &lt;a href=&#34;https://pypi.org/project/openai/&#34;&gt;&lt;code&gt;openai&lt;/code&gt; Python package&lt;/a&gt; (that&#39;s published by OpenAI) then you should be able to port your app to talk to llamafile instead, by making a few changes to &lt;code&gt;base_url&lt;/code&gt; and &lt;code&gt;api_key&lt;/code&gt;. This example assumes you&#39;ve run &lt;code&gt;pip3 install openai&lt;/code&gt; to install OpenAI&#39;s client software, which is required by this example. Their package is just a simple Python wrapper around the OpenAI API interface, which can be implemented by any server.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python3&#xA;from openai import OpenAI&#xA;client = OpenAI(&#xA;    base_url=&#34;http://localhost:8080/v1&#34;, # &#34;http://&amp;lt;Your api-server IP&amp;gt;:port&#34;&#xA;    api_key = &#34;sk-no-key-required&#34;&#xA;)&#xA;completion = client.chat.completions.create(&#xA;    model=&#34;LLaMA_CPP&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Write a limerick about python exceptions&#34;}&#xA;    ]&#xA;)&#xA;print(completion.choices[0].message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The above code will return a Python object like this:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ChatCompletionMessage(content=&#39;There once was a programmer named Mike\nWho wrote code that would often strike\nAn error would occur\nAnd he\&#39;d shout &#34;Oh no!&#34;\nBut Python\&#39;s exceptions made it all right.&#39;, role=&#39;assistant&#39;, function_call=None, tool_calls=None)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Other example llamafiles&lt;/h2&gt; &#xA;&lt;p&gt;We also provide example llamafiles for other models, so you can easily try out llamafile with different kinds of LLMs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;llamafile&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA 1.5&lt;/td&gt; &#xA;   &lt;td&gt;3.97 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true&#34;&gt;llava-v1.5-7b-q4.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral-7B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;5.15 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://choosealicense.com/licenses/apache-2.0/&#34;&gt;Apache 2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.llamafile?download=true&#34;&gt;mistral-7b-instruct-v0.2.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;30.03 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://choosealicense.com/licenses/apache-2.0/&#34;&gt;Apache 2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/Mixtral-8x7B-Instruct-v0.1-llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile?download=true&#34;&gt;mixtral-8x7b-instruct-v0.1.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python-34B&lt;/td&gt; &#xA;   &lt;td&gt;22.23 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/WizardCoder-Python-34B-V1.0-llamafile/resolve/main/wizardcoder-python-34b-v1.0.Q5_K_M.llamafile?download=true&#34;&gt;wizardcoder-python-34b-v1.0.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python-13B&lt;/td&gt; &#xA;   &lt;td&gt;7.33 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;LLaMA 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b.llamafile?download=true&#34;&gt;wizardcoder-python-13b.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TinyLlama-1.1B&lt;/td&gt; &#xA;   &lt;td&gt;0.76 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://choosealicense.com/licenses/apache-2.0/&#34;&gt;Apache 2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile?download=true&#34;&gt;TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rocket-3B&lt;/td&gt; &#xA;   &lt;td&gt;1.89 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/deed.en&#34;&gt;cc-by-sa-4.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/rocket-3B-llamafile/resolve/main/rocket-3b.Q5_K_M.llamafile?download=true&#34;&gt;rocket-3b.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-2&lt;/td&gt; &#xA;   &lt;td&gt;1.96 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/jartine/phi-2-llamafile/resolve/main/phi-2.Q5_K_M.llamafile?download=true&#34;&gt;phi-2.Q5_K_M.llamafile&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here is an example for the Mistral command-line llamafile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./mistral-7b-instruct-v0.2.Q5_K_M.llamafile -ngl 9999 --temp 0.7 -p &#39;[INST]Write a story about llamas[/INST]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is an example for WizardCoder-Python command-line llamafile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./wizardcoder-python-13b.llamafile -ngl 9999 --temp 0 -e -r &#39;```\n&#39; -p &#39;```c\nvoid *memcpy_sse2(char *dst, const char *src, size_t size) {\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here&#39;s an example for the LLaVA command-line llamafile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./llava-v1.5-7b-q4.llamafile -ngl 9999 --temp 0.2 --image lemurs.jpg -e -p &#39;### User: What do you see?\n### Assistant:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As before, macOS, Linux, and BSD users will need to use the &#34;chmod&#34; command to grant execution permissions to the file before running these llamafiles for the first time.&lt;/p&gt; &#xA;&lt;p&gt;Unfortunately, Windows users cannot make use of many of these example llamafiles because Windows has a maximum executable file size of 4GB, and all of these examples exceed that size. (The LLaVA llamafile works on Windows because it is 30MB shy of the size limit.) But don&#39;t lose heart: llamafile allows you to use external weights; this is described later in this document.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Having trouble? See the &#34;Gotchas&#34; section below.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How llamafile works&lt;/h2&gt; &#xA;&lt;p&gt;A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open LLM, as well as everything needed to actually run that model on your computer. There&#39;s nothing to install or configure (with a few caveats, discussed in subsequent sections of this document).&lt;/p&gt; &#xA;&lt;p&gt;This is all accomplished by combining llama.cpp with Cosmopolitan Libc, which provides some useful capabilities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;llamafiles can run on multiple CPU microarchitectures. We added runtime dispatching to llama.cpp that lets new Intel systems use modern CPU features without trading away support for older computers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;llamafiles can run on multiple CPU architectures. We do that by concatenating AMD64 and ARM64 builds with a shell script that launches the appropriate one. Our file format is compatible with WIN32 and most UNIX shells. It&#39;s also able to be easily converted (by either you or your users) to the platform-native format, whenever required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;llamafiles can run on six OSes (macOS, Windows, Linux, FreeBSD, OpenBSD, and NetBSD). If you make your own llama files, you&#39;ll only need to build your code once, using a Linux-style toolchain. The GCC-based compiler we provide is itself an Actually Portable Executable, so you can build your software for all six OSes from the comfort of whichever one you prefer most for development.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The weights for an LLM can be embedded within the llamafile. We added support for PKZIP to the GGML library. This lets uncompressed weights be mapped directly into memory, similar to a self-extracting archive. It enables quantized weights distributed online to be prefixed with a compatible version of the llama.cpp software, thereby ensuring its originally observed behaviors can be reproduced indefinitely.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, with the tools included in this project you can create your &lt;em&gt;own&lt;/em&gt; llamafiles, using any compatible model weights you want. You can then distribute these llamafiles to other people, who can easily make use of them regardless of what kind of computer they have.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Using llamafile with external weights&lt;/h2&gt; &#xA;&lt;p&gt;Even though our example llamafiles have the weights built-in, you don&#39;t &lt;em&gt;have&lt;/em&gt; to use llamafile that way. Instead, you can download &lt;em&gt;just&lt;/em&gt; the llamafile software (without any weights included) from our releases page. You can then use it alongside any external weights you may have on hand. External weights are particularly useful for Windows users because they enable you to work around Windows&#39; 4GB executable file size limit.&lt;/p&gt; &#xA;&lt;p&gt;For Windows users, here&#39;s an example for the Mistral LLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -L -o llamafile.exe https://github.com/Mozilla-Ocho/llamafile/releases/download/0.6/llamafile-0.6&#xA;curl -L -o mistral.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf&#xA;./llamafile.exe -m mistral.gguf -ngl 9999&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows users may need to change &lt;code&gt;./llamafile.exe&lt;/code&gt; to &lt;code&gt;.\llamafile.exe&lt;/code&gt; when running the above command.&lt;/p&gt; &#xA;&lt;h2&gt;Gotchas&lt;/h2&gt; &#xA;&lt;p&gt;On macOS with Apple Silicon you need to have Xcode Command Line Tools installed for llamafile to be able to bootstrap itself.&lt;/p&gt; &#xA;&lt;p&gt;If you use zsh and have trouble running llamafile, try saying &lt;code&gt;sh -c ./llamafile&lt;/code&gt;. This is due to a bug that was fixed in zsh 5.9+. The same is the case for Python &lt;code&gt;subprocess&lt;/code&gt;, old versions of Fish, etc.&lt;/p&gt; &#xA;&lt;p&gt;On some Linux systems, you might get errors relating to &lt;code&gt;run-detectors&lt;/code&gt; or WINE. This is due to &lt;code&gt;binfmt_misc&lt;/code&gt; registrations. You can fix that by adding an additional registration for the APE file format llamafile uses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf&#xA;sudo chmod +x /usr/bin/ape&#xA;sudo sh -c &#34;echo &#39;:APE:M::MZqFpD::/usr/bin/ape:&#39; &amp;gt;/proc/sys/fs/binfmt_misc/register&#34;&#xA;sudo sh -c &#34;echo &#39;:APE-jart:M::jartsr::/usr/bin/ape:&#39; &amp;gt;/proc/sys/fs/binfmt_misc/register&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As mentioned above, on Windows you may need to rename your llamafile by adding &lt;code&gt;.exe&lt;/code&gt; to the filename.&lt;/p&gt; &#xA;&lt;p&gt;Also as mentioned above, Windows also has a maximum file size limit of 4GB for executables. The LLaVA server executable above is just 30MB shy of that limit, so it&#39;ll work on Windows, but with larger models like WizardCoder 13B, you need to store the weights in a separate file. An example is provided above; see &#34;Using llamafile with external weights.&#34;&lt;/p&gt; &#xA;&lt;p&gt;On WSL, it&#39;s recommended that the WIN32 interop feature be disabled:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo sh -c &#34;echo -1 &amp;gt; /proc/sys/fs/binfmt_misc/WSLInterop&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Raspberry Pi, if you get &#34;mmap error 12&#34; then it means your kernel is configured with fewer than 48 bits of address space. You need to upgrade to RPI 5. You can still use RPI 4 if you either (1) rebuild your kernel, or (2) get your SDcard OS image directly from Ubuntu (don&#39;t use RPI OS).&lt;/p&gt; &#xA;&lt;p&gt;On any platform, if your llamafile process is immediately killed, check if you have CrowdStrike and then ask to be whitelisted.&lt;/p&gt; &#xA;&lt;h2&gt;Supported OSes&lt;/h2&gt; &#xA;&lt;p&gt;llamafile supports the following operating systems, which require a minimum stock install:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux 2.6.18+ (i.e. every distro since RHEL5 c. 2007)&lt;/li&gt; &#xA; &lt;li&gt;Darwin (macOS) 23.1.0+ [1] (GPU is only supported on ARM64)&lt;/li&gt; &#xA; &lt;li&gt;Windows 8+ (AMD64 only)&lt;/li&gt; &#xA; &lt;li&gt;FreeBSD 13+&lt;/li&gt; &#xA; &lt;li&gt;NetBSD 9.2+ (AMD64 only)&lt;/li&gt; &#xA; &lt;li&gt;OpenBSD 7+ (AMD64 only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;On Windows, llamafile runs as a native portable executable. On UNIX systems, llamafile extracts a small loader program named &lt;code&gt;ape&lt;/code&gt; to &lt;code&gt;$TMPDIR/.llamafile&lt;/code&gt; or &lt;code&gt;~/.ape-1.9&lt;/code&gt; which is used to map your model into memory.&lt;/p&gt; &#xA;&lt;p&gt;[1] Darwin kernel versions 15.6+ &lt;em&gt;should&lt;/em&gt; be supported, but we currently have no way of testing that.&lt;/p&gt; &#xA;&lt;h2&gt;Supported CPUs&lt;/h2&gt; &#xA;&lt;p&gt;llamafile supports the following CPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AMD64&lt;/strong&gt; microprocessors must have AVX. Otherwise llamafile will print an error and refuse to run. This means that if you have an Intel CPU, it needs to be Intel Sandybridge or newer (circa 2011+), and if you have an AMD CPU, then it needs to be Bulldozer or newer (circa 2011+). Support for AVX2, FMA, F16C, and VNNI are conditionally enabled at runtime if you have a newer CPU. There&#39;s no support for AVX512 runtime dispatching yet.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ARM64&lt;/strong&gt; microprocessors must have ARMv8a+. This means everything from Apple Silicon to 64-bit Raspberry Pis will work, provided your weights fit into memory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GPU support&lt;/h2&gt; &#xA;&lt;p&gt;llamafile supports the following kinds of GPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple Metal&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;AMD&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;GPU on MacOS ARM64 is supported by compiling a small module using the Xcode Command Line Tools, which need to be installed. This is a one time cost that happens the first time you run your llamafile. The DSO built by llamafile is stored in &lt;code&gt;$TMPDIR/.llamafile&lt;/code&gt; or &lt;code&gt;$HOME/.llamafile&lt;/code&gt;. Offloading to GPU is enabled by default when a Metal GPU is present. This can be disabled by passing &lt;code&gt;-ngl 0&lt;/code&gt; or &lt;code&gt;--gpu disable&lt;/code&gt; to force llamafile to perform CPU inference.&lt;/p&gt; &#xA;&lt;p&gt;Owners of NVIDIA and AMD graphics cards need to pass the &lt;code&gt;-ngl 999&lt;/code&gt; flag to enable maximum offloading. If multiple GPUs are present then the work will be divided evenly among them by default, so you can load larger models. Multiple GPU support may be broken on AMD Radeon systems. If that happens to you, then use &lt;code&gt;export HIP_VISIBLE_DEVICES=0&lt;/code&gt; which forces llamafile to only use the first GPU.&lt;/p&gt; &#xA;&lt;p&gt;Windows users are encouraged to use our release binaries, because they contain prebuilt DLLs for both NVIDIA and AMD graphics cards, which only depend on the graphics driver being installed. If llamafile detects that NVIDIA&#39;s CUDA SDK or AMD&#39;s ROCm HIP SDK are installed, then llamafile will try to build a faster DLL that uses cuBLAS or rocBLAS. In order for llamafile to successfully build a cuBLAS module, it needs to be run on the x64 MSVC command prompt. You can use CUDA via WSL by enabling &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&#34;&gt;Nvidia CUDA on WSL&lt;/a&gt; and running your llamafiles inside of WSL. Using WSL has the added benefit of letting you run llamafiles greater than 4GB on Windows.&lt;/p&gt; &#xA;&lt;p&gt;On Linux, NVIDIA users will need to install the CUDA SDK (ideally using the shell script installer) and ROCm users need to install the HIP SDK. They&#39;re detected by looking to see if &lt;code&gt;nvcc&lt;/code&gt; or &lt;code&gt;hipcc&lt;/code&gt; are on the PATH.&lt;/p&gt; &#xA;&lt;p&gt;If you have both an AMD GPU &lt;em&gt;and&lt;/em&gt; an NVIDIA GPU in your machine, then you may need to qualify which one you want used, by passing either &lt;code&gt;--gpu amd&lt;/code&gt; or &lt;code&gt;--gpu nvidia&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the event that GPU support couldn&#39;t be compiled and dynamically linked on the fly for any reason, llamafile will fall back to CPU inference.&lt;/p&gt; &#xA;&lt;h2&gt;Source installation&lt;/h2&gt; &#xA;&lt;p&gt;Developing on llamafile requires a modern version of the GNU &lt;code&gt;make&lt;/code&gt; command (called &lt;code&gt;gmake&lt;/code&gt; on some systems), &lt;code&gt;sha256sum&lt;/code&gt; (otherwise &lt;code&gt;cc&lt;/code&gt; will be used to build it), &lt;code&gt;wget&lt;/code&gt; (or &lt;code&gt;curl&lt;/code&gt;), and &lt;code&gt;unzip&lt;/code&gt; available at &lt;a href=&#34;https://cosmo.zip/pub/cosmos/bin/&#34;&gt;https://cosmo.zip/pub/cosmos/bin/&lt;/a&gt;. Windows users need &lt;a href=&#34;https://justine.lol/cosmo3/&#34;&gt;cosmos bash&lt;/a&gt; shell too.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make -j8&#xA;sudo make install PREFIX=/usr/local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to generate code for a libc function using the llama.cpp command line interface, utilizing WizardCoder-Python-13B weights:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;llamafile -ngl 9999 \&#xA;  -m wizardcoder-python-13b-v1.0.Q8_0.gguf \&#xA;  --temp 0 -r &#39;}\n&#39; -r &#39;```\n&#39; \&#xA;  -e -p &#39;```c\nvoid *memcpy(void *dst, const void *src, size_t size) {\n&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a similar example that instead utilizes Mistral-7B-Instruct weights for prose composition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;llamafile -ngl 9999 \&#xA;  -m mistral-7b-instruct-v0.1.Q4_K_M.gguf \&#xA;  -p &#39;[INST]Write a story about llamas[/INST]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example of how llamafile can be used as an interactive chatbot that lets you query knowledge contained in training data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;llamafile -m llama-65b-Q5_K.gguf -p &#39;&#xA;The following is a conversation between a Researcher and their helpful AI assistant Digital Athena which is a large language model trained on the sum of human knowledge.&#xA;Researcher: Good morning.&#xA;Digital Athena: How can I help you today?&#xA;Researcher:&#39; --interactive --color --batch_size 1024 --ctx_size 4096 \&#xA;--keep -1 --temp 0 --mirostat 2 --in-prefix &#39; &#39; --interactive-first \&#xA;--in-suffix &#39;Digital Athena:&#39; --reverse-prompt &#39;Researcher:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example of how you can use llamafile to summarize HTML URLs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;(&#xA;  echo &#39;[INST]Summarize the following text:&#39;&#xA;  links -codepage utf-8 \&#xA;        -force-html \&#xA;        -width 500 \&#xA;        -dump https://www.poetryfoundation.org/poems/48860/the-raven |&#xA;    sed &#39;s/   */ /g&#39;&#xA;  echo &#39;[/INST]&#39;&#xA;) | llamafile -ngl 9999 \&#xA;      -m mistral-7b-instruct-v0.2.Q5_K_M.gguf \&#xA;      -f /dev/stdin \&#xA;      -c 0 \&#xA;      --temp 0 \&#xA;      -n 500 \&#xA;      --no-display-prompt 2&amp;gt;/dev/null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s how you can use llamafile to describe a jpg/png/gif/bmp image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;llamafile -ngl 9999 --temp 0 \&#xA;  --image ~/Pictures/lemurs.jpg \&#xA;  -m llava-v1.5-7b-Q4_K.gguf \&#xA;  --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \&#xA;  -e -p &#39;### User: What do you see?\n### Assistant: &#39; \&#xA;  --no-display-prompt 2&amp;gt;/dev/null&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s possible to use BNF grammar to enforce the output is predictable and safe to use in your shell script. The simplest grammar would be &lt;code&gt;--grammar &#39;root ::= &#34;yes&#34; | &#34;no&#34;&#39;&lt;/code&gt; to force the LLM to only print to standard output either &lt;code&gt;&#34;yes\n&#34;&lt;/code&gt; or &lt;code&gt;&#34;no\n&#34;&lt;/code&gt;. Another example is if you wanted to write a script to rename all your image files, you could say:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;llamafile -ngl 9999 --temp 0 \&#xA;    --image lemurs.jpg \&#xA;    -m llava-v1.5-7b-Q4_K.gguf \&#xA;    --mmproj llava-v1.5-7b-mmproj-Q4_0.gguf \&#xA;    --grammar &#39;root ::= [a-z]+ (&#34; &#34; [a-z]+)+&#39; \&#xA;    -e -p &#39;### User: What do you see?\n### Assistant: &#39; \&#xA;    --no-display-prompt 2&amp;gt;/dev/null |&#xA;  sed -e&#39;s/ /_/g&#39; -e&#39;s/$/.jpg/&#39;&#xA;a_baby_monkey_on_the_back_of_a_mother.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to run llama.cpp&#39;s built-in HTTP server. This example uses LLaVA v1.5-7B, a multimodal LLM that works with llama.cpp&#39;s recently-added support for image inputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;llamafile -ngl 9999 \&#xA;  -m llava-v1.5-7b-Q8_0.gguf \&#xA;  --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \&#xA;  --host 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command will launch a browser tab on your personal computer to display a web interface. It lets you chat with your LLM and upload images to it.&lt;/p&gt; &#xA;&lt;h2&gt;Creating llamafiles&lt;/h2&gt; &#xA;&lt;p&gt;If you want to be able to just say:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./llava.llamafile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and have it run the web server without having to specify arguments, then you can embed both the weights and a special &lt;code&gt;.args&lt;/code&gt; inside, which specifies the default arguments. First, let&#39;s create a file named &lt;code&gt;.args&lt;/code&gt; which has this content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;-m&#xA;llava-v1.5-7b-Q8_0.gguf&#xA;--mmproj&#xA;llava-v1.5-7b-mmproj-Q8_0.gguf&#xA;--host&#xA;0.0.0.0&#xA;-ngl&#xA;9999&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As we can see above, there&#39;s one argument per line. The &lt;code&gt;...&lt;/code&gt; argument optionally specifies where any additional CLI arguments passed by the user are to be inserted. Next, we&#39;ll add both the weights and the argument file to the executable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cp /usr/local/bin/llamafile llava.llamafile&#xA;&#xA;zipalign -j0 \&#xA;  llava.llamafile \&#xA;  llava-v1.5-7b-Q8_0.gguf \&#xA;  llava-v1.5-7b-mmproj-Q8_0.gguf \&#xA;  .args&#xA;&#xA;./llava.llamafile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Congratulations. You&#39;ve just made your own LLM executable that&#39;s easy to share with your friends.&lt;/p&gt; &#xA;&lt;h2&gt;Distribution&lt;/h2&gt; &#xA;&lt;p&gt;One good way to share a llamafile with your friends is by posting it on Hugging Face. If you do that, then it&#39;s recommended that you mention in your Hugging Face commit message what git revision or released version of llamafile you used when building your llamafile. That way everyone online will be able verify the provenance of its executable content. If you&#39;ve made changes to the llama.cpp or cosmopolitan source code, then the Apache 2.0 license requires you to explain what changed. One way you can do that is by embedding a notice in your llamafile using &lt;code&gt;zipalign&lt;/code&gt; that describes the changes, and mention it in your Hugging Face commit.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a man page for each of the llamafile programs installed when you run &lt;code&gt;sudo make install&lt;/code&gt;. The command manuals are also typeset as PDF files that you can download from our GitHub releases page. Lastly, most commands will display that information when passing the &lt;code&gt;--help&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;h2&gt;Technical details&lt;/h2&gt; &#xA;&lt;p&gt;Here is a succinct overview of the tricks we used to create the fattest executable format ever. The long story short is llamafile is a shell script that launches itself and runs inference on embedded weights in milliseconds without needing to be copied or installed. What makes that possible is mmap(). Both the llama.cpp executable and the weights are concatenated onto the shell script. A tiny loader program is then extracted by the shell script, which maps the executable into memory. The llama.cpp executable then opens the shell script again as a file, and calls mmap() again to pull the weights into memory and make them directly accessible to both the CPU and GPU.&lt;/p&gt; &#xA;&lt;h3&gt;ZIP weights embedding&lt;/h3&gt; &#xA;&lt;p&gt;The trick to embedding weights inside llama.cpp executables is to ensure the local file is aligned on a page size boundary. That way, assuming the zip file is uncompressed, once it&#39;s mmap()&#39;d into memory we can pass pointers directly to GPUs like Apple Metal, which require that data be page size aligned. Since no existing ZIP archiving tool has an alignment flag, we had to write about &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/zipalign.c&#34;&gt;500 lines of code&lt;/a&gt; to insert the ZIP files ourselves. However, once there, every existing ZIP program should be able to read them, provided they support ZIP64. This makes the weights much more easily accessible than they otherwise would have been, had we invented our own file format for concatenated files.&lt;/p&gt; &#xA;&lt;h3&gt;Microarchitectural portability&lt;/h3&gt; &#xA;&lt;p&gt;On Intel and AMD microprocessors, llama.cpp spends most of its time in the matmul quants, which are usually written thrice for SSSE3, AVX, and AVX2. llamafile pulls each of these functions out into a separate file that can be &lt;code&gt;#include&lt;/code&gt;ed multiple times, with varying &lt;code&gt;__attribute__((__target__(&#34;arch&#34;)))&lt;/code&gt; function attributes. Then, a wrapper function is added which uses Cosmopolitan&#39;s &lt;code&gt;X86_HAVE(FOO)&lt;/code&gt; feature to runtime dispatch to the appropriate implementation.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture portability&lt;/h3&gt; &#xA;&lt;p&gt;llamafile solves architecture portability by building llama.cpp twice: once for AMD64 and again for ARM64. It then wraps them with a shell script which has an MZ prefix. On Windows, it&#39;ll run as a native binary. On Linux, it&#39;ll extract a small 8kb executable called &lt;a href=&#34;https://github.com/jart/cosmopolitan/raw/master/ape/loader.c&#34;&gt;APE Loader&lt;/a&gt; to &lt;code&gt;${TMPDIR:-${HOME:-.}}/.ape&lt;/code&gt; that&#39;ll map the binary portions of the shell script into memory. It&#39;s possible to avoid this process by running the &lt;a href=&#34;https://github.com/jart/cosmopolitan/raw/master/tool/build/assimilate.c&#34;&gt;&lt;code&gt;assimilate&lt;/code&gt;&lt;/a&gt; program that comes included with the &lt;code&gt;cosmocc&lt;/code&gt; compiler. What the &lt;code&gt;assimilate&lt;/code&gt; program does is turn the shell script executable into the host platform&#39;s native executable format. This guarantees a fallback path exists for traditional release processes when it&#39;s needed.&lt;/p&gt; &#xA;&lt;h3&gt;GPU support&lt;/h3&gt; &#xA;&lt;p&gt;Cosmopolitan Libc uses static linking, since that&#39;s the only way to get the same executable to run on six OSes. This presents a challenge for llama.cpp, because it&#39;s not possible to statically link GPU support. The way we solve that is by checking if a compiler is installed on the host system. For Apple, that would be Xcode, and for other platforms, that would be &lt;code&gt;nvcc&lt;/code&gt;. llama.cpp has a single file implementation of each GPU module, named &lt;code&gt;ggml-metal.m&lt;/code&gt; (Objective C) and &lt;code&gt;ggml-cuda.cu&lt;/code&gt; (Nvidia C). llamafile embeds those source files within the zip archive and asks the platform compiler to build them at runtime, targeting the native GPU microarchitecture. If it works, then it&#39;s linked with platform C library dlopen() implementation. See &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/cuda.c&#34;&gt;llamafile/cuda.c&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Mozilla-Ocho/llamafile/main/llamafile/metal.c&#34;&gt;llamafile/metal.c&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to use the platform-specific dlopen() function, we need to ask the platform-specific compiler to build a small executable that exposes these interfaces. On ELF platforms, Cosmopolitan Libc maps this helper executable into memory along with the platform&#39;s ELF interpreter. The platform C library then takes care of linking all the GPU libraries, and then runs the helper program which longjmp()&#39;s back into Cosmopolitan. The executable program is now in a weird hybrid state where two separate C libraries exist which have different ABIs. For example, thread local storage works differently on each operating system, and programs will crash if the TLS register doesn&#39;t point to the appropriate memory. The way Cosmopolitan Libc solves that on AMD is by using SSE to recompile the executable at runtime to change &lt;code&gt;%fs&lt;/code&gt; register accesses into &lt;code&gt;%gs&lt;/code&gt; which takes a millisecond. On ARM, Cosmo uses the &lt;code&gt;x28&lt;/code&gt; register for TLS which can be made safe by passing the &lt;code&gt;-ffixed-x28&lt;/code&gt; flag when compiling GPU modules. Lastly, llamafile uses the &lt;code&gt;__ms_abi__&lt;/code&gt; attribute so that function pointers passed between the application and GPU modules conform to the Windows calling convention. Amazingly enough, every compiler we tested, including nvcc on Linux and even Objective-C on MacOS, all support compiling WIN32 style functions, thus ensuring your llamafile will be able to talk to Windows drivers, when it&#39;s run on Windows, without needing to be recompiled as a separate file for Windows. See &lt;a href=&#34;https://github.com/jart/cosmopolitan/raw/master/libc/dlopen/dlopen.c&#34;&gt;cosmopolitan/dlopen.c&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;h2&gt;A note about models&lt;/h2&gt; &#xA;&lt;p&gt;The example llamafiles provided above should not be interpreted as endorsements or recommendations of specific models, licenses, or data sets on the part of Mozilla.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;llamafile adds pledge() and SECCOMP sandboxing to llama.cpp. This is enabled by default. It can be turned off by passing the &lt;code&gt;--unsecure&lt;/code&gt; flag. Sandboxing is currently only supported on Linux and OpenBSD on systems without GPUs; on other platforms it&#39;ll simply log a warning.&lt;/p&gt; &#xA;&lt;p&gt;Our approach to security has these benefits:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;After it starts up, your HTTP server isn&#39;t able to access the filesystem at all. This is good, since it means if someone discovers a bug in the llama.cpp server, then it&#39;s much less likely they&#39;ll be able to access sensitive information on your machine or make changes to its configuration. On Linux, we&#39;re able to sandbox things even further; the only networking related system call the HTTP server will allowed to use after starting up, is accept(). That further limits an attacker&#39;s ability to exfiltrate information, in the event that your HTTP server is compromised.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The main CLI command won&#39;t be able to access the network at all. This is enforced by the operating system kernel. It also won&#39;t be able to write to the file system. This keeps your computer safe in the event that a bug is ever discovered in the the GGUF file format that lets an attacker craft malicious weights files and post them online. The only exception to this rule is if you pass the &lt;code&gt;--prompt-cache&lt;/code&gt; flag without also specifying &lt;code&gt;--prompt-cache-ro&lt;/code&gt;. In that case, security currently needs to be weakened to allow &lt;code&gt;cpath&lt;/code&gt; and &lt;code&gt;wpath&lt;/code&gt; access, but network access will remain forbidden.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Therefore your llamafile is able to protect itself against the outside world, but that doesn&#39;t mean you&#39;re protected from llamafile. Sandboxing is self-imposed. If you obtained your llamafile from an untrusted source then its author could have simply modified it to not do that. In that case, you can run the untrusted llamafile inside another sandbox, such as a virtual machine, to make sure it behaves how you expect.&lt;/p&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;While the llamafile project is Apache 2.0-licensed, our changes to llama.cpp are licensed under MIT (just like the llama.cpp project itself) so as to remain compatible and upstreamable in the future, should that be desired.&lt;/p&gt; &#xA;&lt;p&gt;The llamafile logo on this page was generated with the assistance of DALL·E 3.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Mozilla-Ocho/llamafile&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Mozilla-Ocho/llamafile&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hsutter/cppfront</title>
    <updated>2024-03-24T01:42:56Z</updated>
    <id>tag:github.com,2024-03-24:/hsutter/cppfront</id>
    <link href="https://github.com/hsutter/cppfront" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A personal experimental C++ Syntax 2 -&gt; Syntax 1 compiler&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cppfront&lt;/h1&gt; &#xA;&lt;p&gt;Copyright (c) Herb Sutter • See &lt;a href=&#34;https://raw.githubusercontent.com/hsutter/cppfront/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hsutter.github.io/cppfront/welcome/hello-world/&#34;&gt;&lt;img width=&#34;410&#34; src=&#34;https://github.com/hsutter/cppfront/assets/1801526/1e160e93-f966-4aee-8377-9f9f3982a95f&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hsutter/cppfront/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hsutter/cppfront/actions/workflows/build-cppfront.yaml&#34;&gt;&lt;img src=&#34;https://github.com/hsutter/cppfront/actions/workflows/build-cppfront.yaml/badge.svg?sanitize=true&#34; alt=&#34;Build (clang, gcc, vs)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cppfront is a compiler from an experimental C++ &#39;syntax 2&#39; (Cpp2) to today&#39;s &#39;syntax 1&#39; (Cpp1), to prove out some concepts, share some ideas, and prototype features that can also be proposed for evolving today&#39;s C++.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation: &lt;a href=&#34;https://hsutter.github.io/cppfront/&#34;&gt;available here&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;What&#39;s different about this project?&lt;/h2&gt; &#xA;&lt;p&gt;In short, it aims to help evolve C++ itself, not to be a &#34;C++ successor.&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What it isn&#39;t.&lt;/strong&gt; Cpp2 is not a successor or alternate language with its own divergent or incompatible ecosystem. For example, it does not have its own nonstandard incompatible modules/concepts/etc. that compete with the Standard C++ features; it does not replace your Standard C++ compiler or other tools; and it does not require any changes to your Standard C++ compiler or standard library or other libraries or tools to keep fully using all of them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What it is.&lt;/strong&gt; Cpp2 aims to be another &#34;skin&#34; for C++ itself, just a simpler and safer way to write ordinary C++ types/functions/objects, and a faster way to experiment with proposals for future new Standard C++ features in a simpler compiler and syntax flavor. It seamlessly uses Standard C++ modules and concepts requirements and other features, and it works with all existing C++20 or higher compilers and libraries and tools right out of the box with no changes required to use them all seamlessly and directly with zero overhead.&lt;/p&gt; &#xA;&lt;p&gt;For more, see &lt;a href=&#34;https://hsutter.github.io/cppfront/#what-is-cpp2&#34;&gt;What is Cpp2?&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Scores of people have given valuable feedback and many are listed below, but I especially want to thank Joe Duffy, Anders Hejlsberg, Bjarne Stroustrup, Andrew Sutton, Tim Sweeney, and Mads Torgersen for their insights and valuable feedback on this work over the years — especially when they disagreed with me. I&#39;d also like to thank Dave Abrahams, Andrei Alexandrescu, Walter Bright, Lee Howes, Chris McKinsey, Scott Meyers, Gor Nishanov, Andrew Pardoe, Sean Parent, Jared Parsons, David Sankel, Nathan Sidwell, JC van Winkel, and Ville Voutilainen for broad feedback on the design.&lt;/p&gt; &#xA;&lt;p&gt;Many more people are listed below for their help with specific parts of the design and those proposals/prototypes. I apologize for the names I have forgotten.&lt;/p&gt; &#xA;&lt;h2&gt;Further information&lt;/h2&gt; &#xA;&lt;p&gt;To learn more, please see:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hsutter.github.io/cppfront/&#34;&gt;&lt;strong&gt;👀 The documentation 👀&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ELeZAKCN4tY&#34;&gt;My CppCon 2022 talk, &#34;Can C++ be 10x simpler and safer ...?&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8U3hl8XMm8c&#34;&gt;My CppCon 2023 talk, &#34;Cooperative C++ Evolution: Toward a TypeScript for C++&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hsutter/cppfront/wiki&#34;&gt;This repo&#39;s wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The list of papers and talks below&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Papers and talks derived from this work (presented in current syntax as contributions toward ISO C++&#39;s evolution itself)&lt;/h3&gt; &#xA;&lt;p&gt;Here are the ISO C++ papers and CppCon conference talks I&#39;ve given since 2015 that have been derived from this work, in the order that I brought each piece forward. Most of the details in the materials below are still current with only incremental updates, apart from the specific syntax of course.&lt;/p&gt; &#xA;&lt;h4&gt;2015: Lifetime safety&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/hEx5DNLWGgA&#34;&gt;&lt;strong&gt;CppCon 2015&lt;/strong&gt;: &#34;Writing good C++14... &lt;em&gt;by default&lt;/em&gt;&#34;&lt;/a&gt; particularly &lt;a href=&#34;https://youtu.be/hEx5DNLWGgA?t=1757&#34;&gt;from 29:00 onward&lt;/a&gt; shows the Lifetime analysis with live demos in a Visual Studio prototype.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/80BZxujhY38&#34;&gt;&lt;strong&gt;CppCon 2018&lt;/strong&gt;: &#34;Thoughts on a more powerful &lt;em&gt;and&lt;/em&gt; simpler C++ (#5 of N)&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://youtu.be/80BZxujhY38?t=1097&#34;&gt;The section starting at 18:00&lt;/a&gt; is an update on the Lifetime status with live demos in a Clang prototype.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://youtu.be/80BZxujhY38?t=5307&#34;&gt;The final part starting at 1:28:00&lt;/a&gt; shows the Lifetime and Metaclasses proposals working hand-in-hand. This is one of the few places before cppfront where the same compiler has contained prototypes of multiple &#39;syntax 2&#39;-derived features so I could show how they build on each other when used together.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/isocpp/CppCoreGuidelines/raw/master/docs/Lifetime.pdf&#34;&gt;&lt;strong&gt;C++ Core Guidelines&lt;/strong&gt;: Lifetime safety profile&lt;/a&gt; is this static analysis adopted by the C++ Core Guidelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wg21.link/p1179&#34;&gt;&lt;strong&gt;P1179&lt;/strong&gt;: Lifetime Safety: Preventing common dangling&lt;/a&gt; is the same analysis in the WG 21 paper list.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is not yet implemented in cppfront. Implementations are shipping in Visual Studio and in CLion, and initial parts have been upstreamed in Clang. I want to especially thank Matthias Gehre, Gabor Horvath, Neil MacIntosh, and Kyle Reed for their help in implementing the Lifetime static analysis design in Visual Studio and a Clang fork. Thanks also to the following for their input and feedback on the specification: Andrei Alexandrescu, Steve Carroll, Pavel Curtis, Gabriel Dos Reis, Joe Duffy, Daniel Frampton, Anna Gringauze, Chris Hawblitzel, Nicolai Josuttis, Ellie Kornstaedt, Aaron Lahman, Ryan McDougall, Nathan Myers, Gor Nishanov, Andrew Pardoe, Jared Parsons, Dave Sielaff, Richard Smith, Jim Springfield, and Bjarne Stroustrup.&lt;/p&gt; &#xA;&lt;h4&gt;2016: Garbage-collected memory arena&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JfmTagWcqoE&#34;&gt;&lt;strong&gt;CppCon 2016&lt;/strong&gt;: &#34;Leak-freedom in C++... &lt;em&gt;by default&lt;/em&gt;&#34;&lt;/a&gt; particularly &lt;a href=&#34;https://youtu.be/JfmTagWcqoE?t=3558&#34;&gt;from 59:00 onward&lt;/a&gt; where I show the strawman prototype I wrote of a tracing garbage-collection memory arena.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hsutter/gcpp&#34;&gt;&lt;strong&gt;Github.com/hsutter/gcpp&lt;/strong&gt;: &#34;gcpp: Deferred and unordered destruction&#34;&lt;/a&gt; is the GitHub prototype I wrote.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is not yet implemented in cppfront. I welcome a real GC expert to collaborate with on bringing this forward to become a &#34;real&#34; usable tracing GC memory arena that C++ code can opt into, with real C++ zero-overhead costing (don&#39;t pay anything if you don&#39;t do a &lt;code&gt;gc.new&lt;/code&gt;, and if you do use it then have the costs be proportional to the number of &lt;code&gt;gc.new&lt;/code&gt; allocations).&lt;/p&gt; &#xA;&lt;h3&gt;2017: Spaceship operator for comparisons, &lt;code&gt;&amp;lt;=&amp;gt;&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4AfRAVcThyA&#34;&gt;&lt;strong&gt;CppCon 2017 (just the intro, first 6 minutes)&lt;/strong&gt;: &#34;Meta: Thoughts on generative C++&#34;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wg21.link/p0515&#34;&gt;&lt;strong&gt;P0515&lt;/strong&gt;: Consistent comparison&lt;/a&gt; is the proposal in today&#39;s syntax that I proposed, and was adopted, for C++20.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is part of ISO C++20 and C++23, except only for chained comparisons which is implemented in cppfront. Thank you again to everyone who helped land this in the Standard in C++20 and improve it in C++23, including especially Walter Brown, Lawrence Crowl, Cameron DaCamara, Gabriel Dos Reis, Jens Maurer, Barry Revzin, Richard Smith, and David Stone.&lt;/p&gt; &#xA;&lt;h4&gt;2017: Reflection, generation, and metaclasses&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6nsyX37nsRs&#34;&gt;&lt;strong&gt;ACCU 2017&lt;/strong&gt;: &#34;Thoughts on metaclasses&#34;&lt;/a&gt; is the first talk I gave about this.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4AfRAVcThyA&#34;&gt;&lt;strong&gt;CppCon 2017&lt;/strong&gt;: &#34;Meta: Thoughts on generative C++&#34;&lt;/a&gt; from after the intro, &lt;a href=&#34;https://youtu.be/4AfRAVcThyA?t=393&#34;&gt;from 6:00 onward&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/80BZxujhY38&#34;&gt;&lt;strong&gt;CppCon 2018&lt;/strong&gt;: &#34;Thoughts on a more powerful &lt;em&gt;and&lt;/em&gt; simpler C++ (&#34;Simplifying C++&#34; #5 of N)&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://youtu.be/80BZxujhY38?t=1097&#34;&gt;The section starting at 51:00&lt;/a&gt; is an update on the Metaclasses status with live demos in a Clang prototype.&lt;/li&gt; &#xA;   &lt;li&gt;(repeating the Lifetime section bullet above) &lt;a href=&#34;https://youtu.be/80BZxujhY38?t=5307&#34;&gt;The final part starting at 1:28&lt;/a&gt; shows the Lifetime and Metaclasses proposals working hand-in-hand. This is one of the few places before cppfront where the same compiler has contained prototypes of multiple &#39;syntax 2&#39;-derived features so I could show how they build on each other when used together.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wg21.link/p0707&#34;&gt;&lt;strong&gt;P0707&lt;/strong&gt;: Metaclass functions: Generative C++&lt;/a&gt; is the paper I brought to the ISO C++ committee.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is mostly implemented in cppfront, except for the ability to write your own metafunctions (that&#39;s coming). Thanks again to Andrew Sutton and his colleagues Wyatt Childers and Jennifer Yao for their help in implementing the Clang-based prototypes of this proposal, and everyone else who contributed feedback on the design including Louis Brandy, Chandler Carruth, Casey Carter, Matúš Chochlík, Lawrence Crowl, Pavel Curtis, Louis Dionne, Gabriel Dos Reis, Joe Duffy, Kenny Kerr, Nicolai Josuttis, Aaron Lahman, Scott Meyers, Axel Naumann, Gor Nishanov, Stephan T. Lavavej, Andrew Pardoe, Sean Parent, Jared Parsons, David Sankel, Richard Smith, Jeff Snyder, Mike Spertus, Mads Torgersen, Daveed Vandevoorde, Tony Van Eerd, JC van Winkel, Ville Voutilainen, and Titus Winters, and many more WG 21 / SG 7 participants.&lt;/p&gt; &#xA;&lt;h4&gt;2018: Updates to Lifetime and Metaclasses (see above)&lt;/h4&gt; &#xA;&lt;h4&gt;2019: Zero-overhead deterministic exceptions: Throwing values&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=os7cqJ5qlzo&#34;&gt;&lt;strong&gt;ACCU 2019&lt;/strong&gt;: &#34;De-fragmenting C++: Making exceptions more affordable and usable&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ARYP83yNAWk&#34;&gt;&lt;strong&gt;CppCon 2019&lt;/strong&gt;: &#34;De-fragmenting C++: Making exceptions and RTTI more affordable and usable (&#34;Simplifying C++&#34; #6 of N)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wg21.link/p0709&#34;&gt;&lt;strong&gt;P0709&lt;/strong&gt;: Zero-overhead deterministic exceptions: Throwing values&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is not yet implemented in cppfront.&lt;/p&gt; &#xA;&lt;h4&gt;2020: Parameter passing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ACCU autumn 2019&lt;/strong&gt;: &#34;Quantifying accidental complexity: An empirical look at teaching and using C++&#34; was my first public talk about this, but a &#34;beta&#34; version that was not recorded; you can find the description &lt;a href=&#34;https://accu.org/conf-previous/2019_autumn/sessions/#XQuantifyingAccidentalComplexityAnEmpiricalLookatTeachingandUsingC&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6lurOCdaj0Y&#34;&gt;&lt;strong&gt;CppCon 2020&lt;/strong&gt;: &#34;Quantifying accidental complexity: An empirical look at teaching and using C++&#34;&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The first half of the talk is about how to be rigorous and actually measure that we&#39;re making improvements, including to measure the percentage of today&#39;s C++ guidance that is about parameter passing and initialization.&lt;/li&gt; &#xA;   &lt;li&gt;The second half of the talk is about &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;inout&lt;/code&gt;, &lt;code&gt;out&lt;/code&gt;, &lt;code&gt;move&lt;/code&gt;, and &lt;code&gt;forward&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hsutter/708/raw/main/708.pdf&#34;&gt;&lt;strong&gt;d0708&lt;/strong&gt;: &#34;Parameter passing -&amp;gt; guaranteed unified initialization and value setting&lt;/a&gt; goes into additional detail I didn&#39;t have time for in the talk.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hsutter/708&#34;&gt;&lt;strong&gt;Github.com/hsutter/708&lt;/strong&gt;&lt;/a&gt; is a repo with the paper and demo examples as used in the talk.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wg21.link/p2064&#34;&gt;&lt;strong&gt;P2064&lt;/strong&gt;: &#34;Assumptions&#34;&lt;/a&gt; is also related to this &#39;syntax 2&#39; work, because this work includes a contracts design, and assumptions ought to be separate from that. This paper was making the technical argument why assumptions and assertions (contracts) are different things.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is implemented in cppfront, including the unified &lt;code&gt;operator=&lt;/code&gt; for user-defined types. Thanks to Andrew Sutton for an initial Clang-based implementation.&lt;/p&gt; &#xA;&lt;h4&gt;2020: &#34;Bridge to NewThingia&#34;&lt;/h4&gt; &#xA;&lt;p&gt;In 2020 I also started socializing the ideas of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;How do you answer &#34;why is your thing different when others that look like it have all failed&#34;?&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;What does it take to be adoptable, including to enable incremental adoption?&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I had specifically in mind a major C++ evolution&#39;s success when many attempts to make C or C++ safer have failed, and the importance of seamless compatibility. The talk was &#34;Bridge to NewThingia,&#34; presented at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://herbsutter.com/2020/06/14/talk-video-available-bridge-to-newthingia-devaroundthesun/&#34;&gt;&lt;strong&gt;DevAroundTheSun&lt;/strong&gt;: &#34;Bridge to Newthingia&#34;&lt;/a&gt;, an initial 26-minute version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BF3qw1ObUyo&#34;&gt;&lt;strong&gt;C++ on Sea&lt;/strong&gt;: &#34;Bridge to NewThingia&#34;&lt;/a&gt; which especially &lt;a href=&#34;https://youtu.be/BF3qw1ObUyo?t=2883&#34;&gt;at the end starting near 48:00&lt;/a&gt; had a slide that directly tackled the &#34;C++ major evolution&#34; scenario, and laid out what I think it would take to have credible answers to the key questions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2021: &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;as&lt;/code&gt;, and pattern matching&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=raB_289NxBk&#34;&gt;&lt;strong&gt;CppCon 2021&lt;/strong&gt;: &#34;Extending and simplifying C++: Thoughts on pattern matching using &lt;code&gt;is&lt;/code&gt; and &lt;code&gt;as&lt;/code&gt;&#34;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wg21.link/p2392&#34;&gt;&lt;strong&gt;P2392&lt;/strong&gt;: Pattern matching using &lt;code&gt;is&lt;/code&gt; and &lt;code&gt;as&lt;/code&gt;&lt;/a&gt; is the ISO C++ committee paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is mostly implemented in cppfront. There is support for &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;as&lt;/code&gt;, and basic &lt;code&gt;inspect&lt;/code&gt; expressions.&lt;/p&gt; &#xA;&lt;h4&gt;2022: CppCon 2022 talk and cppfront&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ELeZAKCN4tY&#34;&gt;&lt;strong&gt;CppCon 2022: &#34;Can C++ be 10x simpler and safer ...?&#34;&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;This repo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Epilog: 2016 roadmap diagram&lt;/h1&gt; &#xA;&lt;p&gt;Finally, here is a roadmap diagram I made in 2016 that is still recognizably a roadmap of Cpp2&#39;s design approach, although a few additions like &lt;code&gt;&amp;lt;=&amp;gt;&lt;/code&gt; came later. I think this is important to show design decisions are related and support each other, so that they are not a gaggle of point fixes but a coordinated refactoring of C++ into a smaller number of regular and combinable features. As Bjarne Stroustrup put it in the &lt;em&gt;ACM History of Programming Languages III&lt;/em&gt; (among other places):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;&lt;strong&gt;10% the size of C++&lt;/strong&gt; in definition and similar in front-end compiler size. ... &lt;strong&gt;Most of the simplification would come from generalization.&lt;/strong&gt;&#34; (B. Stroustrup, ACM HOPL-III, 2007; emphasis added)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1801526/189503047-0b0a4f0f-c5e7-42b2-a17d-37d80bef3970.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;I haven&#39;t updated this roadmap diagram since 2016, but it shows many of the talks and papers that have come since then from this work, and it&#39;s still a pretty up-to-date roadmap of the major parts of Cpp2. As of spring 2023, cppfront implements most of this roadmap.&lt;/p&gt; &#xA;&lt;p&gt;I hope you enjoy reading about this personal experiment, and I hope that it might at least start a conversation about what could be possible &lt;em&gt;&lt;strong&gt;within C++&lt;/strong&gt;&lt;/em&gt;&#39;s own evolution to make C++ 10x simpler, safer, and more toolable.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fxsound2/fxsound-app</title>
    <updated>2024-03-24T01:42:56Z</updated>
    <id>tag:github.com,2024-03-24:/fxsound2/fxsound-app</id>
    <link href="https://github.com/fxsound2/fxsound-app" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FxSound application and DSP source code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FxSound&lt;/h1&gt; &#xA;&lt;p&gt;FxSound is a digital audio program built for Windows PC&#39;s. The background processing, built on a high-fidelity audio engine, acts as a sort of digital soundcard for your system. This means that your signals will have the clean passthrough when FxSound is active. There are active effects for shaping and boosting your sound&#39;s volume, timbre, and equalization included on top of this clean processing, allowing you to customize and enhance your sound.&lt;/p&gt; &#xA;&lt;h2&gt;General Information&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://www.fxsound.com&#34;&gt;https://www.fxsound.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Installer: &lt;a href=&#34;https://download.fxsound.com/fxsoundlatest&#34;&gt;https://download.fxsound.com/fxsoundlatest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source code: &lt;a href=&#34;https://github.com/fxsound2/fxsound-app&#34;&gt;https://github.com/fxsound2/fxsound-app&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Issue tracker: &lt;a href=&#34;https://github.com/fxsound2/fxsound-app/issues&#34;&gt;https://github.com/fxsound2/fxsound-app/issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Forum: &lt;a href=&#34;https://forum.fxsound.com&#34;&gt;https://forum.fxsound.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.paypal.com/donate/?hosted_button_id=JVNQGYXCQ2GPG&#34;&gt;Donate to FxSound&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and install the &lt;a href=&#34;https://download.fxsound.com/fxsoundlatest&#34;&gt;latest version of FxSound&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://visualstudio.microsoft.com/vs&#34;&gt;Visual Studio 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://developer.microsoft.com/en-us/windows/downloads/windows-sdk&#34;&gt;Windows SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://github.com/juce-framework/JUCE/releases/tag/6.1.6&#34;&gt;JUCE framework version 6.1.6&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;FxSound application requires FxSound Audio Enhancer virtual audio driver. So, to run FxSound application built from source, we need to install FxSound which installs the audio driver. While building with JUCE 7.x.x version we ran into an issue that the application CPU utilisation goes high when the display is off. So, we are building FxSound with JUCE 6.1.6.&lt;/p&gt; &#xA;&lt;h3&gt;Build FxSound from Visual Studio&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;a href=&#34;https://github.com/fxsound2/fxsound-app/raw/main/fxsound/Project/FxSound.sln&#34;&gt;fxsound/Project/FxSound.sln&lt;/a&gt; in Visual Studio&lt;/li&gt; &#xA; &lt;li&gt;Build the required configuration and platform and run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build after exporting the project form Projucer&lt;/h3&gt; &#xA;&lt;p&gt;FxSound application has three components.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;FxSound GUI application which uses JUCE framework&lt;/li&gt; &#xA; &lt;li&gt;Audiopassthru module which is used by the application to interact with the audio devices&lt;/li&gt; &#xA; &lt;li&gt;DfxDsp module which is the DSP for processing audio&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Due to the some limitations with Projucer, after exporting the Visual Studio solution from Projucer, few changes have to be made in the solution to build FxSound.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Since the audiopassthru and DfxDsp dependency projects cannot be added to the solution when FxSound.sln is exported, open fxsound/Project/FxSound.sln in Visual Studio and add the existing projects audiopassthru/audiopassthru.vcxproj, dsp/DfxDsp.vcxproj.&lt;/li&gt; &#xA; &lt;li&gt;From FxSound_App project, add reference to audiopassthru and DfxDsp.&lt;/li&gt; &#xA; &lt;li&gt;By default, only the x64 platform configuration is created in the exported FxSound_App project. To build 32 bit, add a 32 bit Win32 configuration from the Visual Studio Configuration Manager as a new platform, then choose x64 in the &#34;Copy settings from:&#34; option.&lt;/li&gt; &#xA; &lt;li&gt;If you run FxSound from Visual Studio, to let the application to use the presets, set the Working Directory to &lt;code&gt;$(SolutionDir)..\..\bin\$(PlatformTarget)&lt;/code&gt; in FxSound_App Project-&amp;gt;Properties-&amp;gt;Debugging setting.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;We welcome anyone who wants to contribute to this project. For more details on how to contribute, follow &lt;a href=&#34;https://raw.githubusercontent.com/fxsound2/fxsound-app/main/CONTRIBUTING.md&#34;&gt;this contributing guideline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fxsound2/fxsound-app/raw/main/LICENSE&#34;&gt;GPL v3.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>