<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-26T01:51:16Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>opencv/opencv</title>
    <updated>2023-02-26T01:51:16Z</updated>
    <id>tag:github.com,2023-02-26:/opencv/opencv</id>
    <link href="https://github.com/opencv/opencv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source Computer Vision Library&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;OpenCV: Open Source Computer Vision Library&lt;/h2&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Homepage: &lt;a href=&#34;https://opencv.org&#34;&gt;https://opencv.org&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Courses: &lt;a href=&#34;https://opencv.org/courses&#34;&gt;https://opencv.org/courses&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Docs: &lt;a href=&#34;https://docs.opencv.org/4.x/&#34;&gt;https://docs.opencv.org/4.x/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Q&amp;amp;A forum: &lt;a href=&#34;https://forum.opencv.org&#34;&gt;https://forum.opencv.org&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;previous forum (read only): &lt;a href=&#34;http://answers.opencv.org&#34;&gt;http://answers.opencv.org&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Issue tracking: &lt;a href=&#34;https://github.com/opencv/opencv/issues&#34;&gt;https://github.com/opencv/opencv/issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Additional OpenCV functionality: &lt;a href=&#34;https://github.com/opencv/opencv_contrib&#34;&gt;https://github.com/opencv/opencv_contrib&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://github.com/opencv/opencv/wiki/How_to_contribute&#34;&gt;contribution guidelines&lt;/a&gt; before starting work on a pull request.&lt;/p&gt; &#xA;&lt;h4&gt;Summary of the guidelines:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;One pull request per issue;&lt;/li&gt; &#xA; &lt;li&gt;Choose the right base branch;&lt;/li&gt; &#xA; &lt;li&gt;Include tests and documentation;&lt;/li&gt; &#xA; &lt;li&gt;Clean up &#34;oops&#34; commits before submitting;&lt;/li&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://github.com/opencv/opencv/wiki/Coding_Style_Guide&#34;&gt;coding style guide&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>dusty-nv/jetson-inference</title>
    <updated>2023-02-26T01:51:16Z</updated>
    <id>tag:github.com,2023-02-26:/dusty-nv/jetson-inference</id>
    <link href="https://github.com/dusty-nv/jetson-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-header.jpg&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h1&gt;Deploying Deep Learning&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to our instructional guide for inference and realtime &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#api-reference&#34;&gt;DNN vision&lt;/a&gt; library for NVIDIA &lt;strong&gt;&lt;a href=&#34;http://www.nvidia.com/object/embedded-systems.html&#34;&gt;Jetson Nano/TX1/TX2/Xavier NX/AGX Xavier/AGX Orin&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repo uses NVIDIA &lt;strong&gt;&lt;a href=&#34;https://developer.nvidia.com/tensorrt&#34;&gt;TensorRT&lt;/a&gt;&lt;/strong&gt; for efficiently deploying neural networks onto the embedded Jetson platform, improving performance and power efficiency using graph optimizations, kernel fusion, and FP16/INT8 precision.&lt;/p&gt; &#xA;&lt;p&gt;Vision primitives, such as &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console-2.md&#34;&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt; for image recognition, &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console-2.md&#34;&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt; for object detection, &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console-2.md&#34;&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt; for semantic segmentation, and &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/posenet.md&#34;&gt;&lt;code&gt;poseNet&lt;/code&gt;&lt;/a&gt; for pose estimation inherit from the shared &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/c/tensorNet.h&#34;&gt;&lt;code&gt;tensorNet&lt;/code&gt;&lt;/a&gt; object. Examples are provided for streaming from live camera feed and processing images. See the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#api-reference&#34;&gt;API Reference&lt;/a&gt;&lt;/strong&gt; section for detailed reference documentation of the C++ and Python libraries.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/dev/docs/images/deep-vision-primitives.jpg&#34;&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#hello-ai-world&#34;&gt;Hello AI World&lt;/a&gt; tutorial for running inference and transfer learning onboard your Jetson, including collecting your own datasets and training your own models. It covers image classification, object detection, semantic segmentation, pose estimation, and mono depth.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#hello-ai-world&#34;&gt;Hello AI World&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#video-walkthroughs&#34;&gt;Video Walkthroughs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#api-reference&#34;&gt;API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#code-examples&#34;&gt;Code Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#pre-trained-models&#34;&gt;Pre-Trained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#recommended-system-requirements&#34;&gt;System Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/CHANGELOG.md&#34;&gt;Change Log&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&amp;gt; &amp;nbsp; JetPack 5.0 is now supported, along with &lt;a href=&#34;https://developer.nvidia.com/embedded/jetson-agx-orin-developer-kit&#34;&gt;Jetson AGX Orin&lt;/a&gt;. &lt;br&gt; &amp;gt; &amp;nbsp; Try the new &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/posenet.md&#34;&gt;Pose Estimation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/depthnet.md&#34;&gt;Mono Depth&lt;/a&gt; tutorials! &lt;br&gt; &amp;gt; &amp;nbsp; See the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/CHANGELOG.md&#34;&gt;Change Log&lt;/a&gt; for the latest updates and new features. &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Hello AI World&lt;/h2&gt; &#xA;&lt;p&gt;Hello AI World can be run completely onboard your Jetson, including inferencing with TensorRT and transfer learning with PyTorch. The inference portion of Hello AI World - which includes coding your own image classification and object detection applications for Python or C++, and live camera demos - can be run on your Jetson in roughly two hours or less, while transfer learning is best left to leave running overnight.&lt;/p&gt; &#xA;&lt;h4&gt;System Setup&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/jetpack-setup-2.md&#34;&gt;Setting up Jetson with JetPack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-docker.md&#34;&gt;Running the Docker Container&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo-2.md&#34;&gt;Building the Project from Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console-2.md&#34;&gt;Classifying Images with ImageNet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console-2.md&#34;&gt;Using the ImageNet Program on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-python-2.md&#34;&gt;Coding Your Own Image Recognition Program (Python)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-2.md&#34;&gt;Coding Your Own Image Recognition Program (C++)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-camera-2.md&#34;&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console-2.md&#34;&gt;Locating Objects with DetectNet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console-2.md#detecting-objects-from-the-command-line&#34;&gt;Detecting Objects from Images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-camera-2.md&#34;&gt;Running the Live Camera Detection Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-example-2.md&#34;&gt;Coding Your Own Object Detection Program&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console-2.md&#34;&gt;Semantic Segmentation with SegNet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console-2.md#segmenting-images-from-the-command-line&#34;&gt;Segmenting Images from the Command Line&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-camera-2.md&#34;&gt;Running the Live Camera Segmentation Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/posenet.md&#34;&gt;Pose Estimation with PoseNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/depthnet.md&#34;&gt;Monocular Depth with DepthNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-transfer-learning.md&#34;&gt;Transfer Learning with PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Classification/Recognition (ResNet-18) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-cat-dog.md&#34;&gt;Re-training on the Cat/Dog Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-plants.md&#34;&gt;Re-training on the PlantCLEF Dataset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-collect.md&#34;&gt;Collecting your own Classification Datasets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Object Detection (SSD-Mobilenet) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-ssd.md&#34;&gt;Re-training SSD-Mobilenet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/pytorch-collect-detection.md&#34;&gt;Collecting your own Detection Datasets&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Appendix&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-streaming.md&#34;&gt;Camera Streaming and Multimedia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-image.md&#34;&gt;Image Manipulation with CUDA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dusty-nv/ros_deep_learning&#34;&gt;Deep Learning Nodes for ROS/ROS2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video Walkthroughs&lt;/h2&gt; &#xA;&lt;p&gt;Below are screencasts of Hello AI World that were recorded for the &lt;a href=&#34;https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs&#34;&gt;Jetson AI Certification&lt;/a&gt; course:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Video&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QXIwdsyK7Rw&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=9&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Hello AI World Setup&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Download and run the Hello AI World container on Jetson Nano, test your camera feed, and see how to stream it over the network via RTP.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QXIwdsyK7Rw&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=9&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_setup.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QatH8iF0Efk&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=10&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Image Classification Inference&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Code your own Python program for image classification using Jetson Nano and deep learning, then experiment with realtime classification on a live camera stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QatH8iF0Efk&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=10&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_imagenet.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sN6aT9TpltU&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=11&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Training Image Classification Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Learn how to train image classification models with PyTorch onboard Jetson Nano, and collect your own classification datasets to create custom models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sN6aT9TpltU&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=11&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_imagenet_training.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=obt60r8ZeB0&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=12&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Object Detection Inference&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Code your own Python program for object detection using Jetson Nano and deep learning, then experiment with realtime detection on a live camera stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=obt60r8ZeB0&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=12&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_detectnet.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2XMkPW_sIGg&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=13&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Training Object Detection Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Learn how to train object detection models with PyTorch onboard Jetson Nano, and collect your own detection datasets to create custom models.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2XMkPW_sIGg&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=13&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_detectnet_training.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AQhkMLaB_fY&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=14&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Semantic Segmentation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;Experiment with fully-convolutional semantic segmentation networks on Jetson Nano, and run realtime segmentation on a live camera stream.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AQhkMLaB_fY&amp;amp;list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&amp;amp;index=14&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_segnet.jpg&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;API Reference&lt;/h2&gt; &#xA;&lt;p&gt;Below are links to reference documentation for the &lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/index.html&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.html&#34;&gt;Python&lt;/a&gt; libraries from the repo:&lt;/p&gt; &#xA;&lt;h4&gt;jetson-inference&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/group__deepVision.html&#34;&gt;C++&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html&#34;&gt;Python&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classimageNet.html&#34;&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#imageNet&#34;&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classdetectNet.html&#34;&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#detectNet&#34;&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classsegNet.html&#34;&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/pytorch/docs/html/python/jetson.inference.html#segNet&#34;&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose Estimation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classposeNet.html&#34;&gt;&lt;code&gt;poseNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#poseNet&#34;&gt;&lt;code&gt;poseNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Monocular Depth&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/classdepthNet.html&#34;&gt;&lt;code&gt;depthNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.inference.html#depthNet&#34;&gt;&lt;code&gt;depthNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;jetson-utils&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/group__util.html&#34;&gt;C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rawgit.com/dusty-nv/jetson-inference/dev/docs/html/python/jetson.utils.html&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These libraries are able to be used in external projects by linking to &lt;code&gt;libjetson-inference&lt;/code&gt; and &lt;code&gt;libjetson-utils&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code Examples&lt;/h2&gt; &#xA;&lt;p&gt;Introductory code walkthroughs of using the library are covered during these steps of the Hello AI World tutorial:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-python-2.md&#34;&gt;Coding Your Own Image Recognition Program (Python)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example-2.md&#34;&gt;Coding Your Own Image Recognition Program (C++)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additional C++ and Python samples for running the networks on static images and live camera streams can be found here:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;C++&lt;/th&gt; &#xA;   &lt;th&gt;Python&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Image Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/imagenet/imagenet.cpp&#34;&gt;&lt;code&gt;imagenet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/imagenet.py&#34;&gt;&lt;code&gt;imagenet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Object Detection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/detectnet/detectnet.cpp&#34;&gt;&lt;code&gt;detectnet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/detectnet.py&#34;&gt;&lt;code&gt;detectnet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Segmentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/segnet/segnet.cpp&#34;&gt;&lt;code&gt;segnet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/segnet.py&#34;&gt;&lt;code&gt;segnet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Pose Estimation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/posenet/posenet.cpp&#34;&gt;&lt;code&gt;posenet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/posenet.py&#34;&gt;&lt;code&gt;posenet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;Monocular Depth&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/examples/depthnet/segnet.cpp&#34;&gt;&lt;code&gt;depthnet.cpp&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/python/examples/depthnet.py&#34;&gt;&lt;code&gt;depthnet.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;: for working with numpy arrays, see &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-image.md#converting-to-numpy-arrays&#34;&gt;Converting to Numpy Arrays&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/aux-image.md#converting-from-numpy-arrays&#34;&gt;Converting from Numpy Arrays&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;These examples will automatically be compiled while &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo-2.md&#34;&gt;Building the Project from Source&lt;/a&gt;, and are able to run the pre-trained models listed below in addition to custom models provided by the user. Launch each example with &lt;code&gt;--help&lt;/code&gt; for usage info.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-Trained Models&lt;/h2&gt; &#xA;&lt;p&gt;The project comes with a number of pre-trained models that are available through the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo-2.md#downloading-models&#34;&gt;&lt;strong&gt;Model Downloader&lt;/strong&gt;&lt;/a&gt; tool:&lt;/p&gt; &#xA;&lt;h4&gt;Image Recognition&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Network&lt;/th&gt; &#xA;   &lt;th&gt;CLI argument&lt;/th&gt; &#xA;   &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AlexNet&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;alexnet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ALEXNET&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GoogleNet&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;googlenet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GOOGLENET&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GoogleNet-12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;googlenet-12&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GOOGLENET_12&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-18&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_18&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-50&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_50&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-101&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_101&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResNet-152&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet-152&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET_152&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VGG-16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;vgg-16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VGG-16&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VGG-19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;vgg-19&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;VGG-19&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Inception-v4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;inception-v4&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;INCEPTION_V4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Object Detection&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Network&lt;/th&gt; &#xA;   &lt;th&gt;CLI argument&lt;/th&gt; &#xA;   &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;   &lt;th&gt;Object classes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD-Mobilenet-v1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ssd-mobilenet-v1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SSD_MOBILENET_V1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91 (&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/data/networks/ssd_coco_labels.txt&#34;&gt;COCO classes&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD-Mobilenet-v2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ssd-mobilenet-v2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SSD_MOBILENET_V2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91 (&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/data/networks/ssd_coco_labels.txt&#34;&gt;COCO classes&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SSD-Inception-v2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;ssd-inception-v2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SSD_INCEPTION_V2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;91 (&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/data/networks/ssd_coco_labels.txt&#34;&gt;COCO classes&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Dog&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-dog&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_DOG&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;dogs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Bottle&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-bottle&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_BOTTLE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;bottles&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Chair&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-chair&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_CHAIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;chairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DetectNet-COCO-Airplane&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coco-airplane&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;COCO_AIRPLANE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;airplanes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ped-100&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pednet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PEDNET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pedestrians&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiped-500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;multiped&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PEDNET_MULTI&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pedestrians, luggage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;facenet-120&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;facenet&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;FACENET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;faces&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Semantic Segmentation&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;CLI Argument&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Jetson Nano&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Jetson Xavier&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x256&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-512x256&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024x512&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-1024x512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;175 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.cityscapes-dataset.com/&#34;&gt;Cityscapes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048x1024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-2048x1024&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.6%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34;&gt;DeepScene&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;576x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-deepscene-576x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.4%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;360 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://deepscene.cs.uni-freiburg.de/&#34;&gt;DeepScene&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;864x480&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-deepscene-864x480&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.9%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;190 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lv-mhp.github.io/&#34;&gt;Multi-Human&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-mhp-512x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;370 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://lv-mhp.github.io/&#34;&gt;Multi-Human&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x360&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-mhp-512x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;325 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://host.robots.ox.ac.uk/pascal/VOC/&#34;&gt;Pascal VOC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;320x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-voc-320x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.9%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;508 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://host.robots.ox.ac.uk/pascal/VOC/&#34;&gt;Pascal VOC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x320&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-voc-512x320&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;375 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://rgbd.cs.princeton.edu/&#34;&gt;SUN RGB-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-sun-512x400&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;340 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://rgbd.cs.princeton.edu/&#34;&gt;SUN RGB-D&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640x512&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fcn-resnet18-sun-640x512&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17 FPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224 FPS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If the resolution is omitted from the CLI argument, the lowest resolution model is loaded&lt;/li&gt; &#xA; &lt;li&gt;Accuracy indicates the pixel classification accuracy across the model&#39;s validation dataset&lt;/li&gt; &#xA; &lt;li&gt;Performance is measured for GPU FP16 mode with JetPack 4.2.1, &lt;code&gt;nvpmodel 0&lt;/code&gt; (MAX-N)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Legacy Segmentation Models&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Network&lt;/th&gt; &#xA;    &lt;th&gt;CLI Argument&lt;/th&gt; &#xA;    &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;    &lt;th&gt;Classes&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Cityscapes (2048x2048)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-cityscapes-hd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_CITYSCAPES_HD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Cityscapes (1024x1024)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-cityscapes-sd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_CITYSCAPES_SD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Pascal VOC (500x356)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-pascal-voc&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_PASCAL_VOC&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Synthia (CVPR16)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-cvpr&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_CVPR&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Synthia (Summer-HD)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-summer-hd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_SUMMER_HD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Synthia (Summer-SD)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-summer-sd&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_SUMMER_SD&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Aerial-FPV (1280x720)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;fcn-alexnet-aerial-fpv-720p&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;FCN_ALEXNET_AERIAL_FPV_720p&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Pose Estimation&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;CLI argument&lt;/th&gt; &#xA;   &lt;th&gt;NetworkType enum&lt;/th&gt; &#xA;   &lt;th&gt;Keypoints&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose-ResNet18-Body&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet18-body&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET18_BODY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose-ResNet18-Hand&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;resnet18-hand&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RESNET18_HAND&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pose-DenseNet121-Body&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;densenet121-body&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;DENSENET121_BODY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Recommended System Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Jetson Nano Developer Kit with JetPack 4.2 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson Nano 2GB Developer Kit with JetPack 4.4.1 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson Xavier NX Developer Kit with JetPack 4.4 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson AGX Xavier Developer Kit with JetPack 4.0 or newer (Ubuntu 18.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson TX2 Developer Kit with JetPack 3.0 or newer (Ubuntu 16.04 aarch64).&lt;/li&gt; &#xA; &lt;li&gt;Jetson TX1 Developer Kit with JetPack 2.3 or newer (Ubuntu 16.04 aarch64).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#training&#34;&gt;Transfer Learning with PyTorch&lt;/a&gt; section of the tutorial speaks from the perspective of running PyTorch onboard Jetson for training DNNs, however the same PyTorch code can be used on a PC, server, or cloud instance with an NVIDIA discrete GPU for faster training.&lt;/p&gt; &#xA;&lt;h2&gt;Extra Resources&lt;/h2&gt; &#xA;&lt;p&gt;In this area, links and resources for deep learning are listed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.github.com/dusty-nv/ros_deep_learning&#34;&gt;ros_deep_learning&lt;/a&gt; - TensorRT inference ROS nodes&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA-AI-IOT&#34;&gt;NVIDIA AI IoT&lt;/a&gt; - NVIDIA Jetson GitHub repositories&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eLinux.org/Jetson&#34;&gt;Jetson eLinux Wiki&lt;/a&gt; - Jetson eLinux Wiki&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Two Days to a Demo (DIGITS)&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;note:&lt;/strong&gt; the DIGITS/Caffe tutorial from below is deprecated. It&#39;s recommended to follow the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#training&#34;&gt;Transfer Learning with PyTorch&lt;/a&gt; tutorial from Hello AI World.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Expand this section to see original DIGITS tutorial (deprecated)&lt;/summary&gt; &#xA; &lt;br&gt; The DIGITS tutorial includes training DNN&#39;s in the cloud or PC, and inference on the Jetson with TensorRT, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your GPU. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/digits-workflow.md&#34;&gt;DIGITS Workflow&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/digits-setup.md&#34;&gt;DIGITS System Setup&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/jetpack-setup.md&#34;&gt;Setting up Jetson with JetPack&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/building-repo.md&#34;&gt;Building the Project from Source&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console.md&#34;&gt;Classifying Images with ImageNet&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-console.md#using-the-console-program-on-jetson&#34;&gt;Using the Console Program on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-example.md&#34;&gt;Coding Your Own Image Recognition Program&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-camera.md&#34;&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md&#34;&gt;Re-Training the Network with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#downloading-image-recognition-dataset&#34;&gt;Downloading Image Recognition Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#customizing-the-object-classes&#34;&gt;Customizing the Object Classes&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#importing-classification-dataset-into-digits&#34;&gt;Importing Classification Dataset into DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#creating-image-classification-model-with-digits&#34;&gt;Creating Image Classification Model with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-training.md#testing-classification-model-in-digits&#34;&gt;Testing Classification Model in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-snapshot.md&#34;&gt;Downloading Model Snapshot to Jetson&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/imagenet-custom.md&#34;&gt;Loading Custom Models on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md&#34;&gt;Locating Objects with DetectNet&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#detection-data-formatting-in-digits&#34;&gt;Detection Data Formatting in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#downloading-the-detection-dataset&#34;&gt;Downloading the Detection Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#importing-the-detection-dataset-into-digits&#34;&gt;Importing the Detection Dataset into DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#creating-detectnet-model-with-digits&#34;&gt;Creating DetectNet Model with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-training.md#testing-detectnet-model-inference-in-digits&#34;&gt;Testing DetectNet Model Inference in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-snapshot.md&#34;&gt;Downloading the Detection Model to Jetson&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-snapshot.md#detectnet-patches-for-tensorrt&#34;&gt;DetectNet Patches for TensorRT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console.md&#34;&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-console.md#multi-class-object-detection-models&#34;&gt;Multi-class Object Detection Models&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/detectnet-camera.md&#34;&gt;Running the Live Camera Detection Demo on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-dataset.md&#34;&gt;Semantic Segmentation with SegNet&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-dataset.md#downloading-aerial-drone-dataset&#34;&gt;Downloading Aerial Drone Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-dataset.md#importing-the-aerial-dataset-into-digits&#34;&gt;Importing the Aerial Dataset into DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-pretrained.md&#34;&gt;Generating Pretrained FCN-Alexnet&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-training.md&#34;&gt;Training FCN-Alexnet with DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-training.md#testing-inference-model-in-digits&#34;&gt;Testing Inference Model in DIGITS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-patches.md&#34;&gt;FCN-Alexnet Patches for TensorRT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/docs/segnet-console.md&#34;&gt;Running Segmentation Models on Jetson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;sup&gt;© 2016-2019 NVIDIA | &lt;/sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-inference/master/#deploying-deep-learning&#34;&gt;&lt;sup&gt;Table of Contents&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ClickHouse/ClickHouse</title>
    <updated>2023-02-26T01:51:16Z</updated>
    <id>tag:github.com,2023-02-26:/ClickHouse/ClickHouse</id>
    <link href="https://github.com/ClickHouse/ClickHouse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ClickHouse® is a free analytics DBMS for big data&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://clickhouse.com&#34;&gt;&lt;img src=&#34;https://github.com/ClickHouse/clickhouse-presentations/raw/master/images/logo-400x240.png&#34; alt=&#34;ClickHouse — open source distributed column-oriented DBMS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ClickHouse® is an open-source column-oriented database management system that allows generating analytical data reports in real-time.&lt;/p&gt; &#xA;&lt;h2&gt;How To Install (Linux, macOS, FreeBSD)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl https://clickhouse.com/ | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Useful Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/&#34;&gt;Official website&lt;/a&gt; has a quick high-level overview of ClickHouse on the main page.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.cloud&#34;&gt;ClickHouse Cloud&lt;/a&gt; ClickHouse as a service, built by the creators and maintainers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/docs/en/getting_started/tutorial/&#34;&gt;Tutorial&lt;/a&gt; shows how to set up and query a small ClickHouse cluster.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/docs/en/&#34;&gt;Documentation&lt;/a&gt; provides more in-depth information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/ClickHouseDB&#34;&gt;YouTube channel&lt;/a&gt; has a lot of content about ClickHouse in video format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/clickhousedb/shared_invite/zt-1gh9ds7f4-PgDhJAaF8ad5RbWBAAjzFg&#34;&gt;Slack&lt;/a&gt; and &lt;a href=&#34;https://telegram.me/clickhouse_en&#34;&gt;Telegram&lt;/a&gt; allow chatting with ClickHouse users in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/blog/&#34;&gt;Blog&lt;/a&gt; contains various ClickHouse-related articles, as well as announcements and reports about events.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/codebrowser/ClickHouse/index.html&#34;&gt;Code Browser (Woboq)&lt;/a&gt; with syntax highlight and navigation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.dev/ClickHouse/ClickHouse&#34;&gt;Code Browser (github.dev)&lt;/a&gt; with syntax highlight, powered by github.dev.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/company/contact&#34;&gt;Contacts&lt;/a&gt; can help to get your questions answered if there are any.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Upcoming Events&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clickhouse.com/company/events/v23-2-release-webinar?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=release-webinar-2023-02&#34;&gt;&lt;strong&gt;v23.2 Release Webinar&lt;/strong&gt;&lt;/a&gt; - Feb 23 - 23.2 is rapidly approaching. Original creator, co-founder, and CTO of ClickHouse Alexey Milovidov will walk us through the highlights of the release.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.meetup.com/clickhouse-netherlands-user-group/events/291485868/&#34;&gt;&lt;strong&gt;ClickHouse Meetup in Amsterdam&lt;/strong&gt;&lt;/a&gt; - Mar 9 - The first ClickHouse Amsterdam Meetup of 2023 is here! 🎉 Join us for short lightning talks and long discussions. Food, drinks &amp;amp; good times on us.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.meetup.com/clickhouse-silicon-valley-meetup-group/events/291490121/&#34;&gt;&lt;strong&gt;ClickHouse Meetup in SF Bay Area&lt;/strong&gt;&lt;/a&gt; - Mar 14 - A night to meet with ClickHouse team in the San Francisco area! Food and drink are a given...but networking is the primary focus.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.meetup.com/clickhouse-austin-user-group/events/291486654/&#34;&gt;&lt;strong&gt;ClickHouse Meetup in Austin&lt;/strong&gt;&lt;/a&gt; - Mar 16 - The first ClickHouse Meetup in Austin is happening soon! Interested in speaking, let us know!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Recent Recordings&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;FOSDEM 2023&lt;/strong&gt;: In the &#34;Fast and Streaming Data&#34; room Alexey gave a talk entitled &#34;Building Analytical Apps With ClickHouse&#34; that looks at the landscape of data tools, an interesting data set, and how you can interact with data quickly. Check out the recording on &lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JlcI2Vfz_uk&#34;&gt;YouTube&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recording available&lt;/strong&gt;: &lt;a href=&#34;https://www.youtube.com/watch?v=zYSZXBnTMSE&#34;&gt;&lt;strong&gt;v23.1 Release Webinar&lt;/strong&gt;&lt;/a&gt; 23.1 is the ClickHouse New Year release. Original creator, co-founder, and CTO of ClickHouse Alexey Milovidov will walk us through the highlights of the release. Inverted indices, query cache, and so -- very -- much more.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>