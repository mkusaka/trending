<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub C++ Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-01T01:59:26Z</updated>
  <subtitle>Weekly Trending of C++ in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>protocolbuffers/protobuf</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/protocolbuffers/protobuf</id>
    <link href="https://github.com/protocolbuffers/protobuf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2008 Google Inc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can find &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;protobuf&#39;s documentation on the Google Developers site&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Compiler Installation&lt;/h2&gt; &#xA;&lt;p&gt;The protocol compiler is written in C++. If you are using C++, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; &#xA;&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our release page:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf/releases&#34;&gt;https://github.com/protocolbuffers/protobuf/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary as well as a set of standard .proto files distributed along with protobuf.&lt;/p&gt; &#xA;&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the maven repo here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&#34;&gt;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&#34;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; &#xA;&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&#34;&gt;src&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&#34;&gt;java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&#34;&gt;python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&#34;&gt;objectivec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C#&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&#34;&gt;csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&#34;&gt;ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf-go&#34;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&#34;&gt;php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dart-lang/protobuf&#34;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The best way to learn how to use protobuf is to follow the tutorials in our developer guide:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/tutorials&#34;&gt;https://developers.google.com/protocol-buffers/docs/tutorials&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The complete documentation for Protocol Buffers is available via the web at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>skyline-emu/skyline</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/skyline-emu/skyline</id>
    <link href="https://github.com/skyline-emu/skyline" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run Nintendo Switch homebrew &amp; games on your Android device!&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/skyline-emu/skyline&#34; target=&#34;_blank&#34;&gt; &lt;img height=&#34;60%&#34; width=&#34;60%&#34; src=&#34;https://raw.github.com/skyline-emu/branding/master/banner/skyline-banner-rounded.png&#34;&gt;&lt;br&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/XnbXNQM&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/545842171459272705.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=5865F2&amp;amp;labelColor=404EED&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/skyline-emu/skyline/actions/workflows/ci.yml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/skyline-emu/skyline/actions/workflows/ci.yml/badge.svg?sanitize=true&#34;&gt;&lt;br&gt; &lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/skyline-emu/skyline/master/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/skyline-emu/skyline/master/BUILDING.md&#34;&gt;Building Guide&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Skyline&lt;/b&gt; is an experimental emulator that runs on &lt;b&gt;ARMv8 Android‚Ñ¢&lt;/b&gt; devices and emulates the functionality of a &lt;b&gt;Nintendo Switch‚Ñ¢&lt;/b&gt; system, licensed under &lt;a href=&#34;https://github.com/skyline-emu/skyline/raw/master/LICENSE.md&#34;&gt;&lt;b&gt;Mozilla Public License 2.0&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;p&gt;You can contact the core developers of Skyline at our &lt;strong&gt;&lt;a href=&#34;https://discord.gg/XnbXNQM&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt;. If you have any questions, feel free to ask. It&#39;s also a good place to just keep up with the emulator, as most talk regarding development goes on over there.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Special Thanks&lt;/h3&gt; &#xA;&lt;p&gt;A few noteworthy teams/projects who&#39;ve helped us along the way are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ryujinx.org/&#34;&gt;Ryujinx&lt;/a&gt;:&lt;/strong&gt; We&#39;ve used Ryujinx for reference throughout the project, the accuracy of their HLE implementations of Switch subsystems make it an amazing reference. The team behind the project has been extremely helpful with any queries we&#39;ve had and have constantly helped us with any issues we&#39;ve come across. &lt;strong&gt;It should be noted that Skyline is not based on Ryujinx&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://yuzu-emu.org/&#34;&gt;yuzu&lt;/a&gt;:&lt;/strong&gt; Skyline&#39;s shader compiler is a &lt;strong&gt;fork&lt;/strong&gt; of &lt;em&gt;yuzu&lt;/em&gt;&#39;s shader compiler with Skyline-specific changes, using it allowed us to focus on the parts of GPU emulation that we could specifically optimize for mobile while having a high-quality shader compiler implementation as a base. The team behind &lt;em&gt;yuzu&lt;/em&gt; has also often helped us and have graciously provided us with a license exemption.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/switchbrew/&#34;&gt;Switchbrew&lt;/a&gt;:&lt;/strong&gt; We&#39;ve extensively used Switchbrew whether that be their &lt;strong&gt;&lt;a href=&#34;https://switchbrew.org/&#34;&gt;wiki&lt;/a&gt;&lt;/strong&gt; with its colossal amount of information on the Switch that has saved us countless hours of time or &lt;strong&gt;&lt;a href=&#34;https://github.com/switchbrew/libnx&#34;&gt;libnx&lt;/a&gt;&lt;/strong&gt; which was crucial to initial development of the emulator to ensure that our HLE kernel and sysmodule implementations were accurate.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nintendo Switch&lt;/strong&gt; is a trademark of &lt;strong&gt;Nintendo Co., Ltd&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt; is a trademark of &lt;strong&gt;Google LLC&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>grpc/grpc</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/grpc/grpc</id>
    <link href="https://github.com/grpc/grpc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The C based gRPC (C++, Python, Ruby, Objective-C, PHP, C#)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gRPC ‚Äì An RPC library and framework&lt;/h1&gt; &#xA;&lt;p&gt;gRPC is a modern, open source, high-performance remote procedure call (RPC) framework that can run anywhere. gRPC enables client and server applications to communicate transparently, and simplifies the building of connected systems.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Homepage:&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://grpc.io/&#34;&gt;grpc.io&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Mailing List:&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://groups.google.com/forum/#!forum/grpc-io&#34;&gt;grpc-io@googlegroups.com&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/grpc/grpc?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/grpc/grpc.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/grpc/grpc&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;To start using gRPC&lt;/h2&gt; &#xA;&lt;p&gt;To maximize usability, gRPC supports the standard method for adding dependencies to a user&#39;s chosen language (if there is one). In most languages, the gRPC runtime comes as a package available in a user&#39;s language package manager.&lt;/p&gt; &#xA;&lt;p&gt;For instructions on how to use the language-specific gRPC runtime for a project, please refer to these documents&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/cpp&#34;&gt;C++&lt;/a&gt;: follow the instructions under the &lt;code&gt;src/cpp&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/csharp&#34;&gt;C#&lt;/a&gt;: NuGet package &lt;code&gt;Grpc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/grpc/grpc-dart&#34;&gt;Dart&lt;/a&gt;: pub package &lt;code&gt;grpc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/grpc/grpc-go&#34;&gt;Go&lt;/a&gt;: &lt;code&gt;go get google.golang.org/grpc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/grpc/grpc-java&#34;&gt;Java&lt;/a&gt;: Use JARs from Maven Central Repository&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/grpc/grpc-kotlin&#34;&gt;Kotlin&lt;/a&gt;: Use JARs from Maven Central Repository&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/grpc/grpc-node&#34;&gt;Node&lt;/a&gt;: &lt;code&gt;npm install grpc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/objective-c&#34;&gt;Objective-C&lt;/a&gt;: Add &lt;code&gt;gRPC-ProtoRPC&lt;/code&gt; dependency to podspec&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/php&#34;&gt;PHP&lt;/a&gt;: &lt;code&gt;pecl install grpc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/python/grpcio&#34;&gt;Python&lt;/a&gt;: &lt;code&gt;pip install grpcio&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/ruby&#34;&gt;Ruby&lt;/a&gt;: &lt;code&gt;gem install grpc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/grpc/grpc-web&#34;&gt;WebJS&lt;/a&gt;: follow the grpc-web instructions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Per-language quickstart guides and tutorials can be found in the &lt;a href=&#34;https://grpc.io/docs/&#34;&gt;documentation section on the grpc.io website&lt;/a&gt;. Code examples are available in the &lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Precompiled bleeding-edge package builds of gRPC &lt;code&gt;master&lt;/code&gt; branch&#39;s &lt;code&gt;HEAD&lt;/code&gt; are uploaded daily to &lt;a href=&#34;https://packages.grpc.io&#34;&gt;packages.grpc.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;To start developing gRPC&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/CONTRIBUTING.md&#34;&gt;How to contribute&lt;/a&gt; which will guide you through the entire workflow of how to build the source code, how to run the tests, and how to contribute changes to the gRPC codebase. The &#34;How to contribute&#34; document also contains info on how the contribution process works and contains best practices for creating contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Sometimes things go wrong. Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/TROUBLESHOOTING.md&#34;&gt;Troubleshooting guide&lt;/a&gt; if you are experiencing issues with gRPC.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://grafana-dot-grpc-testing.appspot.com/&#34;&gt;Performance dashboard&lt;/a&gt; for performance numbers of master branch daily builds.&lt;/p&gt; &#xA;&lt;h2&gt;Concepts&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/CONCEPTS.md&#34;&gt;gRPC Concepts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About This Repository&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains source code for gRPC libraries implemented in multiple languages written on top of a shared C core library &lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/core&#34;&gt;src/core&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Libraries in different languages may be in various states of development. We are seeking contributions for all of these libraries:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shared C [core library]&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/core&#34;&gt;src/core&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/cpp&#34;&gt;src/cpp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/ruby&#34;&gt;src/ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/python&#34;&gt;src/python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/php&#34;&gt;src/php&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C# (core library based)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/csharp&#34;&gt;src/csharp&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Objective-C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grpc/grpc/master/src/objective-c&#34;&gt;src/objective-c&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Source repo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-java&#34;&gt;grpc-java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kotlin&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-kotlin&#34;&gt;grpc-kotlin&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-go&#34;&gt;grpc-go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NodeJS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-node&#34;&gt;grpc-node&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WebJS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-web&#34;&gt;grpc-web&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-dart&#34;&gt;grpc-dart&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;.NET (pure C# impl.)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-dotnet&#34;&gt;grpc-dotnet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swift&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-swift&#34;&gt;grpc-swift&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>odriverobotics/ODrive</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/odriverobotics/ODrive</id>
    <link href="https://github.com/odriverobotics/ODrive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High performance motor control&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://static1.squarespace.com/static/58aff26de4fcb53b5efd2f02/t/59bf2a7959cc6872bd68be7e/1505700483663/Odrive+logo+plus+text+black.png?format=1000w&#34; alt=&#34;ODrive Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is all about accurately driving brushless motors, for cheap. The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects, like &lt;a href=&#34;https://www.youtube.com/watch?v=WT4E5nb3KtY&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Branch&lt;/th&gt; &#xA;   &lt;th&gt;Build Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;master&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://travis-ci.org/madcowswe/ODrive&#34;&gt;&lt;img src=&#34;https://travis-ci.org/madcowswe/ODrive.png?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;devel&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://travis-ci.org/madcowswe/ODrive&#34;&gt;&lt;img src=&#34;https://travis-ci.org/madcowswe/ODrive.png?branch=devel&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/madcowswe/ODrive/actions?query=workflow%3A%22pip+install+odrive+%28nightly%29%22&#34;&gt;&lt;img src=&#34;https://github.com/madcowswe/ODrive/workflows/pip%20install%20odrive%20(nightly)/badge.svg?sanitize=true&#34; alt=&#34;pip install odrive (nightly)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://docs.odriverobotics.com/v/latest/developer-guide.html#&#34;&gt;Developer Guide&lt;/a&gt; to get started with ODrive firmware development.&lt;/p&gt; &#xA;&lt;h3&gt;Repository Structure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Firmware&lt;/strong&gt;: ODrive firmware&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;tools&lt;/strong&gt;: Python library &amp;amp; tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;docs&lt;/strong&gt;: Documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.odriverobotics.com/&#34;&gt;Main Website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.odriverobotics.com/&#34;&gt;User Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discourse.odriverobotics.com/&#34;&gt;Forum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discourse.odriverobotics.com/t/come-chat-with-us/281&#34;&gt;Chat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mriscoc/Ender3V2S1</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/mriscoc/Ender3V2S1</id>
    <link href="https://github.com/mriscoc/Ender3V2S1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is optimized firmware for Ender3 V2/S1 3D printers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Professional Firmware for the Creality Ender 3 V2/S1 Printers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/mriscoc/Ender3V2S1.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release-date/mriscoc/Ender3V2S1.svg?sanitize=true&#34; alt=&#34;GitHub Release Date&#34;&gt; &lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/actions&#34;&gt;&lt;img src=&#34;https://github.com/mriscoc/Ender3V2S1/workflows/CI/badge.svg?branch=Ender3V2S1-Released&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Universal RET6/RCT6 Ender 3 V2/S1 Edition&lt;/h2&gt; &#xA;&lt;p&gt;Please test this firmware and let us know if it misbehaves in any way. Volunteers are standing by!&lt;br&gt; Precompiled binary files can be downloader from: &lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/releases/latest&#34;&gt;Latest Release&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img aling=&#34;left&#34; height=&#34;240&#34; src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/buildroot/share/pixmaps/Ender-3V2.jpg&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/buildroot/share/pixmaps/Ender-3S1.jpg&#34;&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for your support, I receive donations through &lt;a href=&#34;https://www.patreon.com/mriscoc&#34;&gt;Patreon&lt;/a&gt; and &lt;a href=&#34;https://www.paypal.com/paypalme/mriscoc&#34;&gt;Paypal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.paypal.com/donate?business=85SPAAR6UZEE8&amp;amp;currency_code=USD&#34;&gt;&lt;img src=&#34;https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Wiki&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/How-to-install-the-firmware&#34;&gt;How to install the firmware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/3D-BLTouch&#34;&gt;Installing a 3D/BLTouch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/Color-Themes&#34;&gt;Color themes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mriscoc/Ender3V2S1/wiki/Octoprint&#34;&gt;How to use with Octoprint&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/ender3v2s1firmware&#34;&gt;Telegram&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/Ender3v2Firmware&#34;&gt;Reddit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/groups/ender3v2firmware&#34;&gt;Ender 3V2 Facebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.facebook.com/groups/ender3s1printer&#34;&gt;Ender 3S1 Facebook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mriscoc/Ender3V2S1/Ender3V2S1-Released/screenshots/main.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This is a Marlin based firmware and is maintained by &lt;a href=&#34;https://github.com/mriscoc&#34;&gt;@mriscoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This work would not be possible without the supporters, helpers and betatesters at the &lt;strong&gt;Telegram&lt;/strong&gt; group.&lt;/p&gt; &#xA;&lt;p&gt;Marlin firmware is an Open Source project hosted on Github, &lt;a href=&#34;https://marlinfw.org/&#34;&gt;Marlin&lt;/a&gt; is owned and maintained by the maker community.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;THIS FIRMWARE AND ALL OTHER FILES IN THE DOWNLOAD ARE PROVIDED FREE OF CHARGE WITH NO WARRANTY OR GUARANTEE. SUPPORT IS NOT INCLUDED JUST BECAUSE YOU DOWNLOADED THE FIRMWARE. WE ARE NOT LIABLE FOR ANY DAMAGE TO YOUR PRINTER, PERSON, OR ANY OTHER PROPERTY DUE TO USE OF THIS FIRMWARE. IF YOU DO NOT AGREE TO THESE TERMS THEN DO NOT USE THE FIRMWARE.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;For the license, check the header of each file, if the license is not specified there, the project license will be used. Marlin is licensed under the GPL.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SerenityOS/serenity</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/SerenityOS/serenity</id>
    <link href="https://github.com/SerenityOS/serenity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Serenity Operating System üêû&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SerenityOS&lt;/h1&gt; &#xA;&lt;p&gt;Graphical Unix-like operating system for x86 computers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SerenityOS/serenity/actions?query=workflow%3A%22Build%2C%20lint%2C%20and%20test%22&#34;&gt;&lt;img src=&#34;https://github.com/SerenityOS/serenity/workflows/Build,%20lint,%20and%20test/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_build/latest?definitionId=1&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/SerenityOS/SerenityOS/_apis/build/status/CI?branchName=master&#34; alt=&#34;Azure DevOps Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:serenity&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/serenity.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=SerenityOS_serenity&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=SerenityOS_serenity&amp;amp;metric=ncloc&#34; alt=&#34;Sonar Cube Static Analysis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/830522505605283862.svg?logo=discord&amp;amp;logoColor=white&amp;amp;logoWidth=20&amp;amp;labelColor=7289DA&amp;amp;label=Discord&amp;amp;color=17cf48&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is a love letter to &#39;90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.&lt;/p&gt; &#xA;&lt;p&gt;Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix. This is a system by us, for us, based on the things we like.&lt;/p&gt; &#xA;&lt;p&gt;You can watch videos of the system being developed on YouTube:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/andreaskling&#34;&gt;Andreas Kling&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/linusgroh&#34;&gt;Linus Groh&#39;s channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;FAQ&lt;/strong&gt;: &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/FAQ.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshot&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Meta/Screenshots/screenshot-b36968c.png&#34; alt=&#34;Screenshot as of b36968c.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modern x86 32-bit and 64-bit kernel with pre-emptive multi-threading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Applications/Browser/&#34;&gt;Browser&lt;/a&gt; with JavaScript, WebAssembly, and more (check the spec compliance for &lt;a href=&#34;https://libjs.dev/test262/&#34;&gt;JS&lt;/a&gt;, &lt;a href=&#34;https://css.tobyase.de/&#34;&gt;CSS&lt;/a&gt;, and &lt;a href=&#34;https://libjs.dev/wasm/&#34;&gt;WASM&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Security features (hardware protections, limited userland capabilities, W^X memory, &lt;code&gt;pledge&lt;/code&gt; &amp;amp; &lt;code&gt;unveil&lt;/code&gt;, (K)ASLR, OOM-resistance, web-content isolation, state-of-the-art TLS algorithms, ...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Services/&#34;&gt;System services&lt;/a&gt; (WindowServer, LoginServer, AudioServer, WebServer, RequestServer, CrashServer, ...) and modern IPC&lt;/li&gt; &#xA; &lt;li&gt;Good POSIX compatibility (&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/LibC/&#34;&gt;LibC&lt;/a&gt;, Shell, syscalls, signals, pseudoterminals, filesystem notifications, standard Unix &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Utilities/&#34;&gt;utilities&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;POSIX-like virtual file systems (/proc, /dev, /sys, /tmp, ...) and ext2 file system&lt;/li&gt; &#xA; &lt;li&gt;Network stack and applications with support for IPv4, TCP, UDP; DNS, HTTP, Gemini, IMAP, NTP&lt;/li&gt; &#xA; &lt;li&gt;Profiling, debugging and other development tools (Kernel-supported profiling, detailed program analysis with software emulation in UserspaceEmulator, CrashReporter, interactive GUI playground, HexEditor, HackStudio IDE for C++ and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Libraries/&#34;&gt;Libraries&lt;/a&gt; for everything from cryptography to OpenGL, audio, JavaScript, GUI, playing chess, ...&lt;/li&gt; &#xA; &lt;li&gt;Support for many common and uncommon file formats (PNG, JPEG, GIF, MP3, WAV, FLAC, ZIP, TAR, PDF, QOI, Gemini, ...)&lt;/li&gt; &#xA; &lt;li&gt;Unified style and design philosophy, flexible theming system, &lt;a href=&#34;https://fonts.serenityos.net/font-family&#34;&gt;custom (bitmap and vector) fonts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Games/&#34;&gt;Games&lt;/a&gt; (Solitaire, Minesweeper, 2048, chess, Conway&#39;s Game of Life, ...) and &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Userland/Demos/&#34;&gt;demos&lt;/a&gt; (CatDog, Starfield, Eyes, mandelbrot set, WidgetGallery, ...)&lt;/li&gt; &#xA; &lt;li&gt;Every-day GUI programs and utilities (Spreadsheet with JavaScript, TextEditor, Terminal, PixelPaint, various multimedia viewers and players, Mail, Assistant, Calculator, ...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... and all of the above are right in this repository, no extra dependencies, built from-scratch by us :^)&lt;/p&gt; &#xA;&lt;p&gt;Additionally, there are &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Ports/AvailablePorts.md&#34;&gt;over two hundred ports of popular open-source software&lt;/a&gt;, including games, compilers, Unix tools, multimedia apps and more.&lt;/p&gt; &#xA;&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; &#xA;&lt;p&gt;Man pages are available online at &lt;a href=&#34;https://man.serenityos.org&#34;&gt;man.serenityos.org&lt;/a&gt;. These pages are generated from the Markdown source files in &lt;a href=&#34;https://github.com/SerenityOS/serenity/tree/master/Base/usr/share/man&#34;&gt;&lt;code&gt;Base/usr/share/man&lt;/code&gt;&lt;/a&gt; and updated automatically.&lt;/p&gt; &#xA;&lt;p&gt;When running SerenityOS you can use &lt;code&gt;man&lt;/code&gt; for the terminal interface, or &lt;code&gt;help&lt;/code&gt; for the GUI.&lt;/p&gt; &#xA;&lt;p&gt;Code-related documentation can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/Documentation/&#34;&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;How do I build and run this?&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/Documentation/BuildInstructions.md&#34;&gt;SerenityOS build instructions&lt;/a&gt;. Serenity runs on Linux, macOS (aarch64 might be a challenge), Windows (with WSL2) and many other *Nixes with hardware or software virtualization.&lt;/p&gt; &#xA;&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; &#xA;&lt;p&gt;Join our Discord server: &lt;a href=&#34;https://discord.gg/serenityos&#34;&gt;SerenityOS Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before opening an issue, please see the &lt;a href=&#34;https://github.com/SerenityOS/serenity/raw/master/CONTRIBUTING.md#issue-policy&#34;&gt;issue policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A general guide for contributing can be found in &lt;a href=&#34;https://raw.githubusercontent.com/SerenityOS/serenity/master/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andreas Kling&lt;/strong&gt; - &lt;a href=&#34;https://twitter.com/awesomekling&#34;&gt;awesomekling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robin Burchell&lt;/strong&gt; - &lt;a href=&#34;https://github.com/rburchell&#34;&gt;rburchell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conrad Pankoff&lt;/strong&gt; - &lt;a href=&#34;https://github.com/deoxxa&#34;&gt;deoxxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sergey Bugaev&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bugaevc&#34;&gt;bugaevc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Liav A&lt;/strong&gt; - &lt;a href=&#34;https://github.com/supercomputer7&#34;&gt;supercomputer7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Linus Groh&lt;/strong&gt; - &lt;a href=&#34;https://github.com/linusg&#34;&gt;linusg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ali Mohammad Pur&lt;/strong&gt; - &lt;a href=&#34;https://github.com/alimpfard&#34;&gt;alimpfard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Shannon Booth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/shannonbooth&#34;&gt;shannonbooth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;H√ºseyin ASLIT√úRK&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asliturk&#34;&gt;asliturk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Matthew Olsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mattco98&#34;&gt;mattco98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nico Weber&lt;/strong&gt; - &lt;a href=&#34;https://github.com/nico&#34;&gt;nico&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brian Gianforcaro&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bgianfo&#34;&gt;bgianfo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ben Wiederhake&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BenWiederhake&#34;&gt;BenWiederhake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tom&lt;/strong&gt; - &lt;a href=&#34;https://github.com/tomuta&#34;&gt;tomuta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Paul Scharnofske&lt;/strong&gt; - &lt;a href=&#34;https://github.com/asynts&#34;&gt;asynts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Itamar Shenhar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/itamar8910&#34;&gt;itamar8910&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Luke Wilde&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Lubrsi&#34;&gt;Lubrsi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brendan Coles&lt;/strong&gt; - &lt;a href=&#34;https://github.com/bcoles&#34;&gt;bcoles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Andrew Kaster&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ADKaster&#34;&gt;ADKaster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;thankyouverycool&lt;/strong&gt; - &lt;a href=&#34;https://github.com/thankyouverycool&#34;&gt;thankyouverycool&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Idan Horowitz&lt;/strong&gt; - &lt;a href=&#34;https://github.com/IdanHo&#34;&gt;IdanHo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gunnar Beutner&lt;/strong&gt; - &lt;a href=&#34;https://github.com/gunnarbeutner&#34;&gt;gunnarbeutner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Flynn&lt;/strong&gt; - &lt;a href=&#34;https://github.com/trflynn89&#34;&gt;trflynn89&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jean-Baptiste Boric&lt;/strong&gt; - &lt;a href=&#34;https://github.com/boricj&#34;&gt;boricj&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Stephan Unverwerth&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sunverwerth&#34;&gt;sunverwerth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Wipfli&lt;/strong&gt; - &lt;a href=&#34;https://github.com/MaxWipfli&#34;&gt;MaxWipfli&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Daniel Bertalan&lt;/strong&gt; - &lt;a href=&#34;https://github.com/BertalanD&#34;&gt;BertalanD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jelle Raaijmakers&lt;/strong&gt; - &lt;a href=&#34;https://github.com/GMTA&#34;&gt;GMTA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sam Atkins&lt;/strong&gt; - &lt;a href=&#34;https://github.com/AtkinsSJ&#34;&gt;AtkinsSJ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tobias Christiansen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/TobyAsE&#34;&gt;TobyAsE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lenny Maiorani&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ldm5180&#34;&gt;ldm5180&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;sin-ack&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sin-ack&#34;&gt;sin-ack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jesse Buhagiar&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Quaker762&#34;&gt;Quaker762&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Peter Elliott&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Petelliott&#34;&gt;Petelliott&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Karol Kosek&lt;/strong&gt; - &lt;a href=&#34;https://github.com/krkk&#34;&gt;krkk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mustafa Quraish&lt;/strong&gt; - &lt;a href=&#34;https://github.com/mustafaquraish&#34;&gt;mustafaquraish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;David Tuin&lt;/strong&gt; - &lt;a href=&#34;https://github.com/davidot&#34;&gt;davidot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leon Albrecht&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Hendiadyoin1&#34;&gt;Hendiadyoin1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tim Schumacher&lt;/strong&gt; - &lt;a href=&#34;https://github.com/timschumi&#34;&gt;timschumi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Marcus Nilsson&lt;/strong&gt; - &lt;a href=&#34;https://github.com/metmo&#34;&gt;metmo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gegga Thor&lt;/strong&gt; - &lt;a href=&#34;https://github.com/Xexxa&#34;&gt;Xexxa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;kleines Filmr√∂llchen&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kleinesfilmroellchen&#34;&gt;kleinesfilmroellchen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kenneth Myhra&lt;/strong&gt; - &lt;a href=&#34;https://github.com/kennethmyhra&#34;&gt;kennethmyhra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Maciej&lt;/strong&gt; - &lt;a href=&#34;https://github.com/sppmacd&#34;&gt;sppmacd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sahan Fernando&lt;/strong&gt; - &lt;a href=&#34;https://github.com/ccapitalK&#34;&gt;ccapitalK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And many more! &lt;a href=&#34;https://github.com/SerenityOS/serenity/graphs/contributors&#34;&gt;See here&lt;/a&gt; for a full contributor list. The people listed above have landed more than 100 commits in the project. :^)&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SerenityOS is licensed under a 2-clause BSD license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CMU-Perceptual-Computing-Lab/openpose</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/CMU-Perceptual-Computing-Lab/openpose</id>
    <link href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/Logo_main_black.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Build Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Linux&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;MacOS&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;Windows&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Build Status&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions&#34;&gt;&lt;img src=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34;&gt;&lt;strong&gt;OpenPose&lt;/strong&gt;&lt;/a&gt; has represented the &lt;strong&gt;first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is &lt;strong&gt;authored by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;Gin√©s Hidalgo&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~zhecao&#34;&gt;&lt;strong&gt;Zhe Cao&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34;&gt;&lt;strong&gt;Tomas Simon&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=sFQD3k4AAAAJ&amp;amp;hl=en&#34;&gt;&lt;strong&gt;Shih-En Wei&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://jhugestar.github.io&#34;&gt;&lt;strong&gt;Hanbyul Joo&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;http://www.cs.cmu.edu/~yaser&#34;&gt;&lt;strong&gt;Yaser Sheikh&lt;/strong&gt;&lt;/a&gt;. It is &lt;strong&gt;maintained by&lt;/strong&gt; &lt;a href=&#34;https://www.gineshidalgo.com&#34;&gt;&lt;strong&gt;Gin√©s Hidalgo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;https://www.raaj.tech&#34;&gt;&lt;strong&gt;Yaadhav Raaj&lt;/strong&gt;&lt;/a&gt;. OpenPose would not be possible without the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34;&gt;&lt;strong&gt;CMU Panoptic Studio dataset&lt;/strong&gt;&lt;/a&gt;. We would also like to thank all the people who &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/09_authors_and_contributors.md&#34;&gt;has helped OpenPose in any way&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face_hands.gif&#34; width=&#34;480&#34;&gt; &lt;br&gt; &lt;sup&gt;Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Gin√©s Hidalgo&lt;/a&gt; (left) and &lt;a href=&#34;https://jhugestar.github.io&#34; target=&#34;_blank&#34;&gt;Hanbyul Joo&lt;/a&gt; (right) in front of the &lt;a href=&#34;http://domedb.perception.cs.cmu.edu&#34; target=&#34;_blank&#34;&gt;CMU Panoptic Studio&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#related-work&#34;&gt;Related Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#quick-start-overview&#34;&gt;Quick Start Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#send-us-feedback&#34;&gt;Send Us Feedback!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Whole-body (Body, Foot, Face, and Hands) 2D Pose Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/dance_foot.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_face.gif&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/pose_hands.gif&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;Testing OpenPose: (Left) &lt;a href=&#34;https://www.youtube.com/watch?v=2DiQUX11YaY&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;Crazy Uptown Funk flashmob in Sydney&lt;/i&gt;&lt;/a&gt; video sequence. (Center and right) Authors &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Gin√©s Hidalgo&lt;/a&gt; and &lt;a href=&#34;http://www.cs.cmu.edu/~tsimon&#34; target=&#34;_blank&#34;&gt;Tomas Simon&lt;/a&gt; testing face and hands&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Whole-body 3D Pose Reconstruction and Estimation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose3d.gif&#34; width=&#34;360&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; testing the OpenPose 3D Module&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Unity Plugin&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_main.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_body_foot.png&#34; width=&#34;300&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/unity_hand_face.png&#34; width=&#34;300&#34;&gt; &lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://ziutinyat.github.io/&#34; target=&#34;_blank&#34;&gt;Tianyi Zhao&lt;/a&gt; and &lt;a href=&#34;https://www.gineshidalgo.com&#34; target=&#34;_blank&#34;&gt;Gin√©s Hidalgo&lt;/a&gt; testing the &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34; target=&#34;_blank&#34;&gt;OpenPose Unity Plugin&lt;/a&gt;&lt;/sup&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Runtime Analysis&lt;/h3&gt; &#xA;&lt;p&gt;We show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/.github/media/openpose_vs_competition.png&#34; width=&#34;360&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Main Functionality&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;2D real-time multi-person keypoint detection&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;15, 18 or &lt;strong&gt;25-keypoint body/foot keypoint estimation&lt;/strong&gt;, including &lt;strong&gt;6 foot keypoints&lt;/strong&gt;. &lt;strong&gt;Runtime invariant to number of detected people&lt;/strong&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;2x21-keypoint hand keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;70-keypoint face keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Runtime depends on number of detected people&lt;/strong&gt;. See &lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt; for a runtime invariant alternative.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/3d_reconstruction_module.md&#34;&gt;&lt;strong&gt;3D real-time single-person keypoint detection&lt;/strong&gt;&lt;/a&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;3D triangulation from multiple single views.&lt;/li&gt; &#xA;     &lt;li&gt;Synchronization of Flir cameras handled.&lt;/li&gt; &#xA;     &lt;li&gt;Compatible with Flir/Point Grey cameras.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/advanced/calibration_module.md&#34;&gt;&lt;strong&gt;Calibration toolbox&lt;/strong&gt;&lt;/a&gt;: Estimation of distortion, intrinsic, and extrinsic camera parameters.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Single-person tracking&lt;/strong&gt; for further speedup or visual smoothing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware compatibility&lt;/strong&gt;: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Usage Alternatives&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/01_demo.md&#34;&gt;&lt;strong&gt;Command-line demo&lt;/strong&gt;&lt;/a&gt; for built-in functionality.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/04_cpp_api.md/&#34;&gt;&lt;strong&gt;C++ API&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/03_python_api.md&#34;&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/a&gt; for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For further details, check the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/07_major_released_features.md&#34;&gt;major released features&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/08_release_notes.md&#34;&gt;release notes&lt;/a&gt; docs.&lt;/p&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_train&#34;&gt;&lt;strong&gt;OpenPose training code&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/&#34;&gt;&lt;strong&gt;OpenPose foot dataset&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin&#34;&gt;&lt;strong&gt;OpenPose Unity Plugin&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenPose papers published in &lt;strong&gt;IEEE TPAMI and CVPR&lt;/strong&gt;. Cite them in your publications if OpenPose helps your research! (Links and more details in the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/#citation&#34;&gt;Citation&lt;/a&gt; section below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use OpenPose without installing or writing any code, simply &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#windows-portable-demo&#34;&gt;download and use the latest Windows portable version of OpenPose&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, you could &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source&#34;&gt;build OpenPose from source&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installation doc&lt;/a&gt; for all the alternatives.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Overview&lt;/h2&gt; &#xA;&lt;p&gt;Simply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also add any of the available flags in any order. E.g., the following example runs on a video (&lt;code&gt;--video {PATH}&lt;/code&gt;), enables face (&lt;code&gt;--face&lt;/code&gt;) and hands (&lt;code&gt;--hand&lt;/code&gt;), and saves the output keypoints on JSON files on disk (&lt;code&gt;--write_json {PATH}&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Ubuntu&#xA;./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;:: Windows - Portable Demo&#xA;bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally, you can also extend OpenPose&#39;s functionality from its Python and C++ APIs. After &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/installation/0_index.md&#34;&gt;installing&lt;/a&gt; OpenPose, check its &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/00_index.md&#34;&gt;official doc&lt;/a&gt; for a quick overview of all the alternatives and tutorials.&lt;/p&gt; &#xA;&lt;h2&gt;Send Us Feedback!&lt;/h2&gt; &#xA;&lt;p&gt;Our library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.&lt;/li&gt; &#xA; &lt;li&gt;Want to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/10_community_projects.md&#34;&gt;Community-based Projects&lt;/a&gt; section or even integrate it with OpenPose!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on &lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;, while the hand and face detectors also use &lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt; (the face detector was trained using the same procedure than the hand detector).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{8765346,&#xA;  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},&#xA;  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},&#xA;  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2019}&#xA;}&#xA;&#xA;@inproceedings{simon2017hand,&#xA;  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{cao2017realtime,&#xA;  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},&#xA;  year = {2017}&#xA;}&#xA;&#xA;@inproceedings{wei2016cpm,&#xA;  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},&#xA;  booktitle = {CVPR},&#xA;  title = {Convolutional pose machines},&#xA;  year = {2016}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Paper links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8765346&#34;&gt;IEEE TPAMI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.08008&#34;&gt;ArXiv&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.07809&#34;&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.08050&#34;&gt;Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.00134&#34;&gt;Convolutional Pose Machines&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the &lt;a href=&#34;https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/LICENSE&#34;&gt;license&lt;/a&gt; for further details. Interested in a commercial license? Check this &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt;. For commercial queries, use the &lt;code&gt;Contact&lt;/code&gt; section from the &lt;a href=&#34;https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740&#34;&gt;FlintBox link&lt;/a&gt; and also send a copy of that message to &lt;a href=&#34;mailto:yaser@cs.cmu.edu&#34;&gt;Yaser Sheikh&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>abseil/abseil-cpp</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/abseil/abseil-cpp</id>
    <link href="https://github.com/abseil/abseil-cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Abseil Common Libraries (C++)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Abseil - C++ Common Libraries&lt;/h1&gt; &#xA;&lt;p&gt;The repository contains the Abseil C++ library code. Abseil is an open-source collection of C++ code (compliant to C++11) designed to augment the C++ standard library.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#about&#34;&gt;About Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#build&#34;&gt;Building Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#codemap&#34;&gt;Codemap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#releases&#34;&gt;Releases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/#links&#34;&gt;Links&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;about&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About Abseil&lt;/h2&gt; &#xA;&lt;p&gt;Abseil is an open-source collection of C++ library code designed to augment the C++ standard library. The Abseil library code is collected from Google&#39;s own C++ code base, has been extensively tested and used in production, and is the same code we depend on in our daily coding lives.&lt;/p&gt; &#xA;&lt;p&gt;In some cases, Abseil provides pieces missing from the C++ standard; in others, Abseil provides alternatives to the standard for special needs we&#39;ve found through usage in the Google code base. We denote those cases clearly within the library code we provide you.&lt;/p&gt; &#xA;&lt;p&gt;Abseil is not meant to be a competitor to the standard library; we&#39;ve just found that many of these utilities serve a purpose within our code base, and we now want to provide those resources to the C++ community as a whole.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;If you want to just get started, make sure you at least run through the &lt;a href=&#34;https://abseil.io/docs/cpp/quickstart&#34;&gt;Abseil Quickstart&lt;/a&gt;. The Quickstart contains information about setting up your development environment, downloading the Abseil code, running tests, and getting a simple binary working.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Building Abseil&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bazel.build&#34;&gt;Bazel&lt;/a&gt; and &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; are the official build systems for Abseil.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://abseil.io/docs/cpp/quickstart&#34;&gt;quickstart&lt;/a&gt; for more information on building Abseil using the Bazel build system.&lt;/p&gt; &#xA;&lt;p&gt;If you require CMake support, please check the &lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/CMake/README.md&#34;&gt;CMake build instructions&lt;/a&gt; and &lt;a href=&#34;https://abseil.io/docs/cpp/quickstart-cmake&#34;&gt;CMake Quickstart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Abseil is officially supported on many platforms. See the &lt;a href=&#34;https://abseil.io/docs/cpp/platforms/platforms&#34;&gt;Abseil platform support guide&lt;/a&gt; for details on supported operating systems, compilers, CPUs, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Codemap&lt;/h2&gt; &#xA;&lt;p&gt;Abseil contains the following C++ library components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/base/&#34;&gt;&lt;code&gt;base&lt;/code&gt;&lt;/a&gt; Abseil Fundamentals &lt;br&gt; The &lt;code&gt;base&lt;/code&gt; library contains initialization code and other code which all other Abseil code depends on. Code within &lt;code&gt;base&lt;/code&gt; may not depend on any other code (other than the C++ standard library).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/algorithm/&#34;&gt;&lt;code&gt;algorithm&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;algorithm&lt;/code&gt; library contains additions to the C++ &lt;code&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; library and container-based versions of such algorithms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/cleanup/&#34;&gt;&lt;code&gt;cleanup&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;cleanup&lt;/code&gt; library contains the control-flow-construct-like type &lt;code&gt;absl::Cleanup&lt;/code&gt; which is used for executing a callback on scope exit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/container/&#34;&gt;&lt;code&gt;container&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;container&lt;/code&gt; library contains additional STL-style containers, including Abseil&#39;s unordered &#34;Swiss table&#34; containers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/debugging/&#34;&gt;&lt;code&gt;debugging&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;debugging&lt;/code&gt; library contains code useful for enabling leak checks, and stacktrace and symbolization utilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/hash/&#34;&gt;&lt;code&gt;hash&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;hash&lt;/code&gt; library contains the hashing framework and default hash functor implementations for hashable types in Abseil.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/memory/&#34;&gt;&lt;code&gt;memory&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;memory&lt;/code&gt; library contains C++11-compatible versions of &lt;code&gt;std::make_unique()&lt;/code&gt; and related memory management facilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/meta/&#34;&gt;&lt;code&gt;meta&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;meta&lt;/code&gt; library contains C++11-compatible versions of type checks available within C++14 and C++17 versions of the C++ &lt;code&gt;&amp;lt;type_traits&amp;gt;&lt;/code&gt; library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/numeric/&#34;&gt;&lt;code&gt;numeric&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;numeric&lt;/code&gt; library contains C++11-compatible 128-bit integers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/profiling/&#34;&gt;&lt;code&gt;profiling&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;profiling&lt;/code&gt; library contains utility code for profiling C++ entities. It is currently a private dependency of other Abseil libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/status/&#34;&gt;&lt;code&gt;status&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;status&lt;/code&gt; contains abstractions for error handling, specifically &lt;code&gt;absl::Status&lt;/code&gt; and &lt;code&gt;absl::StatusOr&amp;lt;T&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/strings/&#34;&gt;&lt;code&gt;strings&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;strings&lt;/code&gt; library contains a variety of strings routines and utilities, including a C++11-compatible version of the C++17 &lt;code&gt;std::string_view&lt;/code&gt; type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/synchronization/&#34;&gt;&lt;code&gt;synchronization&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;synchronization&lt;/code&gt; library contains concurrency primitives (Abseil&#39;s &lt;code&gt;absl::Mutex&lt;/code&gt; class, an alternative to &lt;code&gt;std::mutex&lt;/code&gt;) and a variety of synchronization abstractions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/time/&#34;&gt;&lt;code&gt;time&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;time&lt;/code&gt; library contains abstractions for computing with absolute points in time, durations of time, and formatting and parsing time within time zones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/types/&#34;&gt;&lt;code&gt;types&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;types&lt;/code&gt; library contains non-container utility types, like a C++11-compatible version of the C++17 &lt;code&gt;std::optional&lt;/code&gt; type.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/absl/utility/&#34;&gt;&lt;code&gt;utility&lt;/code&gt;&lt;/a&gt; &lt;br&gt; The &lt;code&gt;utility&lt;/code&gt; library contains utility and helper code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;Abseil recommends users &#34;live-at-head&#34; (update to the latest commit from the master branch as often as possible). However, we realize this philosophy doesn&#39;t work for every project, so we also provide &lt;a href=&#34;https://github.com/abseil/abseil-cpp/releases&#34;&gt;Long Term Support Releases&lt;/a&gt; to which we backport fixes for severe bugs. See our &lt;a href=&#34;https://abseil.io/about/releases&#34;&gt;release management&lt;/a&gt; document for more details.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Abseil C++ library is licensed under the terms of the Apache license. See &lt;a href=&#34;https://raw.githubusercontent.com/abseil/abseil-cpp/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;p&gt;For more information about Abseil:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Consult our &lt;a href=&#34;https://abseil.io/about/intro&#34;&gt;Abseil Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read &lt;a href=&#34;https://abseil.io/about/philosophy&#34;&gt;Why Adopt Abseil&lt;/a&gt; to understand our design philosophy.&lt;/li&gt; &#xA; &lt;li&gt;Peruse our &lt;a href=&#34;https://abseil.io/about/compatibility&#34;&gt;Abseil Compatibility Guarantees&lt;/a&gt; to understand both what we promise to you, and what we expect of you in return.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/tensorflow</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/tensorflow/tensorflow</id>
    <link href="https://github.com/tensorflow/tensorflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open Source Machine Learning Framework for Everyone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://www.tensorflow.org/images/tf_logo_horizontal.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/tensorflow.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.5281/zenodo.4724125&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/api-reference-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of &lt;a href=&#34;https://www.tensorflow.org/resources/tools&#34;&gt;tools&lt;/a&gt;, &lt;a href=&#34;https://www.tensorflow.org/resources/libraries-extensions&#34;&gt;libraries&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;community&lt;/a&gt; resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google&#39;s Machine Intelligence Research organization to conduct machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow provides stable &lt;a href=&#34;https://www.tensorflow.org/api_docs/python&#34;&gt;Python&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/api_docs/cc&#34;&gt;C++&lt;/a&gt; APIs, as well as non-guaranteed backward compatible API for &lt;a href=&#34;https://www.tensorflow.org/api_docs&#34;&gt;other languages&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep up-to-date with release announcements and security updates by subscribing to &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/announce&#34;&gt;announce@tensorflow.org&lt;/a&gt;. See all the &lt;a href=&#34;https://www.tensorflow.org/community/forums&#34;&gt;mailing lists&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.tensorflow.org/install&#34;&gt;TensorFlow install guide&lt;/a&gt; for the &lt;a href=&#34;https://www.tensorflow.org/install/pip&#34;&gt;pip package&lt;/a&gt;, to &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;enable GPU support&lt;/a&gt;, use a &lt;a href=&#34;https://www.tensorflow.org/install/docker&#34;&gt;Docker container&lt;/a&gt;, and &lt;a href=&#34;https://www.tensorflow.org/install/source&#34;&gt;build from source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To install the current release, which includes support for &lt;a href=&#34;https://www.tensorflow.org/install/gpu&#34;&gt;CUDA-enabled GPU cards&lt;/a&gt; &lt;em&gt;(Ubuntu and Windows)&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A smaller CPU-only package is also available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install tensorflow-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update TensorFlow to the latest version, add &lt;code&gt;--upgrade&lt;/code&gt; flag to the above commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Nightly binaries are available for testing using the &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly&#34;&gt;tf-nightly&lt;/a&gt; and &lt;a href=&#34;https://pypi.python.org/pypi/tf-nightly-cpu&#34;&gt;tf-nightly-cpu&lt;/a&gt; packages on PyPi.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;&lt;em&gt;Try your first TensorFlow program&lt;/em&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf&#xA;&amp;gt;&amp;gt;&amp;gt; tf.add(1, 2).numpy()&#xA;3&#xA;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant(&#39;Hello, TensorFlow!&#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; hello.numpy()&#xA;b&#39;Hello, TensorFlow!&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution guidelines&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want to contribute to TensorFlow, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;. This project adheres to TensorFlow&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We use &lt;a href=&#34;https://github.com/tensorflow/tensorflow/issues&#34;&gt;GitHub issues&lt;/a&gt; for tracking requests and bugs, please see &lt;a href=&#34;https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss&#34;&gt;TensorFlow Discuss&lt;/a&gt; for general questions and discussion, and please direct specific questions to &lt;a href=&#34;https://stackoverflow.com/questions/tagged/tensorflow&#34;&gt;Stack Overflow&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The TensorFlow project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:tensorflow&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg?sanitize=true&#34; alt=&#34;Fuzzing Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/1486&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/1486/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Continuous build status&lt;/h2&gt; &#xA;&lt;p&gt;You can find more community-supported platforms and configurations in the &lt;a href=&#34;https://github.com/tensorflow/build#community-supported-tensorflow-builds&#34;&gt;TensorFlow SIG Build community builds table&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Official Builds&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Artifacts&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux XLA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TBA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pypi.org/project/tf-nightly-gpu/&#34;&gt;PyPI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bintray.com/google/tensorflow/tensorflow/_latestVersion&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 0 and 1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Raspberry Pi 2 and 3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg?sanitize=true&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl&#34;&gt;Py3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow MacOS CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Libtensorflow Windows GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Status Temporarily Unavailable&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz&#34;&gt;Nightly Binary&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow/&#34;&gt;Official GCS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;TensorFlow.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/&#34;&gt;TensorFlow Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official&#34;&gt;TensorFlow Official Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/examples&#34;&gt;TensorFlow Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-in-practice&#34;&gt;DeepLearning.AI TensorFlow Developer Professional Certificate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-data-and-deployment&#34;&gt;TensorFlow: Data and Deployment from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/getting-started-with-tensor-flow2&#34;&gt;Getting Started with TensorFlow 2 from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow-advanced-techniques&#34;&gt;TensorFlow: Advanced Techniques from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/tensorflow2-deeplearning&#34;&gt;TensorFlow 2 for Deep Learning Specialization from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/introduction-tensorflow&#34;&gt;Intro to TensorFlow for A.I, M.L, and D.L from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187&#34;&gt;Intro to TensorFlow for Deep Learning from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-tensorflow-lite--ud190&#34;&gt;Introduction to TensorFlow Lite from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/machine-learning-tensorflow-gcp&#34;&gt;Machine Learning with TensorFlow on GCP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codelabs.developers.google.com/?cat=TensorFlow&#34;&gt;TensorFlow Codelabs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.tensorflow.org&#34;&gt;TensorFlow Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/resources/learn-ml&#34;&gt;Learn ML with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/tensorflow&#34;&gt;TensorFlow Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ&#34;&gt;TensorFlow YouTube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/model_optimization/guide/roadmap&#34;&gt;TensorFlow model optimization roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/about/bib&#34;&gt;TensorFlow White Papers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorboard&#34;&gt;TensorBoard Visualization Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about the &lt;a href=&#34;https://www.tensorflow.org/community&#34;&gt;TensorFlow community&lt;/a&gt; and how to &lt;a href=&#34;https://www.tensorflow.org/community/contribute&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/tensorflow/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/leveldb</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/google/leveldb</id>
    <link href="https://github.com/google/leveldb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/google/leveldb/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;ci&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Authors: Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keys and values are arbitrary byte arrays.&lt;/li&gt; &#xA; &lt;li&gt;Data is stored sorted by key.&lt;/li&gt; &#xA; &lt;li&gt;Callers can provide a custom comparison function to override the sort order.&lt;/li&gt; &#xA; &lt;li&gt;The basic operations are &lt;code&gt;Put(key,value)&lt;/code&gt;, &lt;code&gt;Get(key)&lt;/code&gt;, &lt;code&gt;Delete(key)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiple changes can be made in one atomic batch.&lt;/li&gt; &#xA; &lt;li&gt;Users can create a transient snapshot to get a consistent view of data.&lt;/li&gt; &#xA; &lt;li&gt;Forward and backward iteration is supported over the data.&lt;/li&gt; &#xA; &lt;li&gt;Data is automatically compressed using the &lt;a href=&#34;https://google.github.io/snappy/&#34;&gt;Snappy compression library&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;External activity (file system operations etc.) is relayed through a virtual interface so users can customize the operating system interactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/google/leveldb/raw/main/doc/index.md&#34;&gt;LevelDB library documentation&lt;/a&gt; is online and bundled with the source code.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is not a SQL database. It does not have a relational data model, it does not support SQL queries, and it has no support for indexes.&lt;/li&gt; &#xA; &lt;li&gt;Only a single process (possibly multi-threaded) can access a particular database at a time.&lt;/li&gt; &#xA; &lt;li&gt;There is no client-server support builtin to the library. An application that needs such support will have to wrap their own server around the library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting the Source&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/google/leveldb.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;This project supports &lt;a href=&#34;https://cmake.org/&#34;&gt;CMake&lt;/a&gt; out of the box.&lt;/p&gt; &#xA;&lt;h3&gt;Build for POSIX&lt;/h3&gt; &#xA;&lt;p&gt;Quick start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake -DCMAKE_BUILD_TYPE=Release .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building for Windows&lt;/h3&gt; &#xA;&lt;p&gt;First generate the Visual Studio 2017 project/solution files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;mkdir build&#xA;cd build&#xA;cmake -G &#34;Visual Studio 15&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default default will build for x86. For 64-bit run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmake -G &#34;Visual Studio 15 Win64&#34; ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile the Windows solution from the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;devenv /build Debug leveldb.sln&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or open leveldb.sln in Visual Studio and build from within.&lt;/p&gt; &#xA;&lt;p&gt;Please see the CMake documentation and &lt;code&gt;CMakeLists.txt&lt;/code&gt; for more advanced usage.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing to the leveldb Project&lt;/h1&gt; &#xA;&lt;p&gt;The leveldb project welcomes contributions. leveldb&#39;s primary goal is to be a reliable and fast key/value store. Changes that are in line with the features/limitations outlined above, and meet the requirements below, will be considered.&lt;/p&gt; &#xA;&lt;p&gt;Contribution requirements:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tested platforms only&lt;/strong&gt;. We &lt;em&gt;generally&lt;/em&gt; will only accept changes for platforms that are compiled and tested. This means POSIX (for Linux and macOS) or Windows. Very small changes will sometimes be accepted, but consider that more of an exception than the rule.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stable API&lt;/strong&gt;. We strive very hard to maintain a stable API. Changes that require changes for projects using leveldb &lt;em&gt;might&lt;/em&gt; be rejected without sufficient benefit to the project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tests&lt;/strong&gt;: All changes must be accompanied by a new (or changed) test, or a sufficient explanation as to why a new (or changed) test is not required.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Style&lt;/strong&gt;: This project conforms to the &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google C++ Style Guide&lt;/a&gt;. To ensure your changes are properly formatted please run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;clang-format -i --style=file &amp;lt;file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We are unlikely to accept contributions to the build configuration files, such as &lt;code&gt;CMakeLists.txt&lt;/code&gt;. We are focused on maintaining a build configuration that allows us to test that the project works in a few supported configurations inside Google. We are not currently interested in supporting other requirements, such as different operating systems, compilers, or build systems.&lt;/p&gt; &#xA;&lt;h2&gt;Submitting a Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;Before any pull request will be accepted the author must first sign a Contributor License Agreement (CLA) at &lt;a href=&#34;https://cla.developers.google.com/&#34;&gt;https://cla.developers.google.com/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to keep the commit timeline linear &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History#Squashing-Commits&#34;&gt;squash&lt;/a&gt; your changes down to a single commit and &lt;a href=&#34;https://git-scm.com/docs/git-rebase&#34;&gt;rebase&lt;/a&gt; on google/leveldb/main. This keeps the commit timeline linear and more easily sync&#39;ed with the internal repository at Google. More information at GitHub&#39;s &lt;a href=&#34;https://help.github.com/articles/about-git-rebase/&#34;&gt;About Git rebase&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;Here is a performance report (with explanations) from the run of the included db_bench program. The results are somewhat noisy, but should be enough to get a ballpark performance estimate.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use a database with a million entries. Each entry has a 16 byte key, and a 100 byte value. Values used by the benchmark compress to about half their original size.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LevelDB:    version 1.1&#xA;Date:       Sun May  1 12:11:26 2011&#xA;CPU:        4 x Intel(R) Core(TM)2 Quad CPU    Q6600  @ 2.40GHz&#xA;CPUCache:   4096 KB&#xA;Keys:       16 bytes each&#xA;Values:     100 bytes each (50 bytes after compression)&#xA;Entries:    1000000&#xA;Raw Size:   110.6 MB (estimated)&#xA;File Size:  62.9 MB (estimated)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Write performance&lt;/h2&gt; &#xA;&lt;p&gt;The &#34;fill&#34; benchmarks create a brand new database, in either sequential, or random order. The &#34;fillsync&#34; benchmark flushes data from the operating system to the disk after every operation; the other write operations leave the data sitting in the operating system buffer cache for a while. The &#34;overwrite&#34; benchmark does random writes that update existing keys in the database.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fillseq      :       1.765 micros/op;   62.7 MB/s&#xA;fillsync     :     268.409 micros/op;    0.4 MB/s (10000 ops)&#xA;fillrandom   :       2.460 micros/op;   45.0 MB/s&#xA;overwrite    :       2.380 micros/op;   46.5 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each &#34;op&#34; above corresponds to a write of a single key/value pair. I.e., a random write benchmark goes at approximately 400,000 writes per second.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;fillsync&#34; operation costs much less (0.3 millisecond) than a disk seek (typically 10 milliseconds). We suspect that this is because the hard disk itself is buffering the update in its memory and responding before the data has been written to the platter. This may or may not be safe based on whether or not the hard disk has enough power to save its memory in the event of a power failure.&lt;/p&gt; &#xA;&lt;h2&gt;Read performance&lt;/h2&gt; &#xA;&lt;p&gt;We list the performance of reading sequentially in both the forward and reverse direction, and also the performance of a random lookup. Note that the database created by the benchmark is quite small. Therefore the report characterizes the performance of leveldb when the working set fits in memory. The cost of reading a piece of data that is not present in the operating system buffer cache will be dominated by the one or two disk seeks needed to fetch the data from disk. Write performance will be mostly unaffected by whether or not the working set fits in memory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 16.677 micros/op;  (approximately 60,000 reads per second)&#xA;readseq     :  0.476 micros/op;  232.3 MB/s&#xA;readreverse :  0.724 micros/op;  152.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LevelDB compacts its underlying storage data in the background to improve read performance. The results listed above were done immediately after a lot of random writes. The results after compactions (which are usually triggered automatically) are better.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 11.602 micros/op;  (approximately 85,000 reads per second)&#xA;readseq     :  0.423 micros/op;  261.8 MB/s&#xA;readreverse :  0.663 micros/op;  166.9 MB/s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the high cost of reads comes from repeated decompression of blocks read from disk. If we supply enough cache to the leveldb so it can hold the uncompressed blocks in memory, the read performance improves again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;readrandom  : 9.775 micros/op;  (approximately 100,000 reads per second before compaction)&#xA;readrandom  : 5.215 micros/op;  (approximately 190,000 reads per second after compaction)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository contents&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/index.md&#34;&gt;doc/index.md&lt;/a&gt; for more explanation. See &lt;a href=&#34;https://raw.githubusercontent.com/google/leveldb/main/doc/impl.md&#34;&gt;doc/impl.md&lt;/a&gt; for a brief overview of the implementation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in include/leveldb/*.h. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Guide to header files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/db.h&lt;/strong&gt;: Main interface to the DB: Start here.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/options.h&lt;/strong&gt;: Control over the behavior of an entire database, and also control over the behavior of individual reads and writes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/comparator.h&lt;/strong&gt;: Abstraction for user-specified comparison function. If you want just bytewise comparison of keys, you can use the default comparator, but clients can write their own comparator implementations if they want custom ordering (e.g. to handle different character encodings, etc.).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/iterator.h&lt;/strong&gt;: Interface for iterating over data. You can get an iterator from a DB object.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/write_batch.h&lt;/strong&gt;: Interface for atomically applying multiple updates to a database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/slice.h&lt;/strong&gt;: A simple module for maintaining a pointer and a length into some other byte array.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/status.h&lt;/strong&gt;: Status is returned from many of the public interfaces and is used to report success and various kinds of errors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/env.h&lt;/strong&gt;: Abstraction of the OS environment. A posix implementation of this interface is in util/env_posix.cc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;include/leveldb/table.h, include/leveldb/table_builder.h&lt;/strong&gt;: Lower-level modules that most clients probably won&#39;t use directly.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>hrydgard/ppsspp</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/hrydgard/ppsspp</id>
    <link href="https://github.com/hrydgard/ppsspp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A PSP emulator for Android, Windows, Mac and Linux, written in C++. Want to contribute? Join us on Discord at https://discord.gg/5NJB6dD or just send pull requests / issues. For discussion use the forums on ppsspp.org.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PPSSPP - a fast and portable PSP emulator&lt;/h1&gt; &#xA;&lt;p&gt;Created by Henrik Rydg√•rd&lt;/p&gt; &#xA;&lt;p&gt;Additional code by many contributors, see the Credits screen&lt;/p&gt; &#xA;&lt;p&gt;Originally released under the GPL 2.0 (and later) in November 2012&lt;/p&gt; &#xA;&lt;p&gt;Official website: &lt;a href=&#34;https://www.ppsspp.org/&#34;&gt;https://www.ppsspp.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Discord: &lt;a href=&#34;https://discord.gg/5NJB6dD&#34;&gt;https://discord.gg/5NJB6dD&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;No BIOS file required to play, PPSSPP is an &#34;HLE&#34; emulator. Default settings balance good compatibility and speed.&lt;/p&gt; &#xA;&lt;p&gt;To contribute, see &lt;a href=&#34;https://www.ppsspp.org/development.html&#34;&gt;the development page&lt;/a&gt;. Help testing, investigating, or fixing is always welcome. See &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues&#34;&gt;the list of issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the latest source code, see &lt;a href=&#34;https://github.com/hrydgard/ppsspp&#34;&gt;our GitHub page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For build instructions and other development tutorials, see &lt;a href=&#34;https://github.com/hrydgard/ppsspp/wiki&#34;&gt;the wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to download regularly updated builds for Android, Windows x86 and x64, proceed to this &lt;a href=&#34;https://buildbot.orphis.net/ppsspp/&#34;&gt;page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For game compatibility, see &lt;a href=&#34;https://report.ppsspp.org/games&#34;&gt;community compatibility feedback&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.12.3&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix background music speed. A couple translation fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.12.2&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix joystick detection bug on Android.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.12.1&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bug fixes (control mapping fix, popup menus in the Windows debugger, a few crashfixes)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.12&lt;/h1&gt; &#xA;&lt;p&gt;Platform support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add support for Android 12 Scoped Storage restrictions (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/11997&#34; title=&#34;Android 12 scoped storage hell&#34;&gt;#11997&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;iOS: Fix multitouch tracking (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/5099&#34; title=&#34;IOS touch controls problems.&#34;&gt;#5099&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Android: Fix screenshot orientation on Vulkan (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14053&#34; title=&#34;Toggle screenshot minor issue.&#34;&gt;#14053&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Linux: Improve support for system FFmpeg 3.1+ (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14176&#34; title=&#34;Remove deprecated API calls for new FFmpeg 4.3.x&#34;&gt;#14176&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14188&#34; title=&#34;Additional fixes for FFmpeg 3.1+&#34;&gt;#14188&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14199&#34; title=&#34;Mpeg: Set low latency flag for video decode&#34;&gt;#14199&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;libretro: Always enable function hooks (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14145&#34; title=&#34;libretro: Remove &amp;quot;Unsafe FuncReplacements&amp;quot; option.&#34;&gt;#14145&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;AMD: Enable Vulkan rendering on a thread (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13864&#34; title=&#34;Vulkan: Remove #10097 hack for newer AMD drivers&#34;&gt;#13864&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add iOS version detection, turn off JIT on bootup if &amp;gt;= 14.3. (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14201&#34; title=&#34;Add iOS version detection, turn off JIT on bootup if &gt;= 14.3.&#34;&gt;#14201&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;iOS: Try a different JIT detection method, thanks Halo-Michael (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14241&#34; title=&#34;iOS: Try a different JIT detection method, thanks Halo-Michael.&#34;&gt;#14241&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Windows: Restore window size correctly (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14317&#34; title=&#34;Window size restarts on closing&#34;&gt;#14317&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Game fixes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix NBA Live 08 loading (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/8288&#34; title=&#34;NBA live 08 Invalid address and hang&#34;&gt;#8288&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Display Open Season title screen correctly (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13252&#34; title=&#34;Open Season Title Screen does not display&#34;&gt;#13252&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix Metal Gear Solid Peace Walker Chinese Patched blue screen (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14127&#34; title=&#34;Metal Gear Solid Peace Walker Chinese Patched blue screen&#34;&gt;#14127&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Load Ape Academy 2 correctly (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14271&#34; title=&#34;Ape Academy 2 is broken on versions after 1.8.0(?) - tested on latest nightly and 1.11.3&#34;&gt;#14271&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Many more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Graphics and Sound:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add new texture filtering mode &#34;Auto Max Quality&#34; (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14789&#34; title=&#34;Add new texture filtering mode &amp;quot;Auto Max Quality&amp;quot;&#34;&gt;#14789&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix Princess Maker 5 Portable half screen in Vulkan (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13741&#34; title=&#34;Princess Maker 5 Portable half screen in Vulkan&#34;&gt;#13741&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix Pro Yakyu Spirits 2010 (NPJH50234): Rendering errors with hardware transform off (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14167&#34; title=&#34;[Android] Pro Yakyu Spirits 2010 (NPJH50234): Rendering errors with hardware transform off&#34;&gt;#14167&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Support texture replacement filtering overrides (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14230&#34; title=&#34;Support texture replacement filtering overrides&#34;&gt;#14230&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix Yarudora Portable: Double Cast&#39;s FMVs artifacting (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13759&#34; title=&#34;Yarudora Portable: Double Cast&#34;&gt;#13759&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix Sims 2 Castaway/Pets EA Logo glitched out (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13146&#34; title=&#34;Sims 2 Castaway/Pets EA Logo glitched out - 1.10.2&#34;&gt;#13146&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix bad size &amp;amp; position on Japanese &amp;amp; Numbers &amp;amp; Alphabets (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14209&#34; title=&#34;Fix Size &amp;amp; Position jpn0.pgf/ltn0.pgf/ltn2.pgf/ltn4.pgf/ltn6.pgf&#34;&gt;#14209&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Implement basic depth texturing for OpenGL (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14042&#34; title=&#34;Implement basic depth texturing for OpenGL&#34;&gt;#14042&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Google Cardboard fixes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14966&#34; title=&#34;Config: Correct cardboard setting ini load&#34;&gt;#14966&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14768&#34; title=&#34;Fix the math in cardboard VR mode for wide aspect ratios&#34;&gt;#14768&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Correct mini-map update in Z.H.P. (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14069&#34; title=&#34;Mini-Map in Z.H.P. Updates Incorrectly Without Software Rendering&#34;&gt;#14069&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix crash in vertex jit on ARM32 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14879&#34; title=&#34;vertexjit: Correct morph flag alpha check assert&#34;&gt;#14879&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add a setting for reverb volume (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14711&#34; title=&#34;Sas: Add option to control reverb volume&#34;&gt;#14711&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Option to switch to new devices or not, on Windows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;UI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a setting for choosing background animation in PPSSPP&#39;s menus (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14313&#34; title=&#34;Add a setting for choosing background animation in PPSSPP&#39;s menus&#34;&gt;#14313&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14818&#34; title=&#34;Focus based moving background&#34;&gt;#14818&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14810&#34; title=&#34;Wave animation&#34;&gt;#14810&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14347&#34; title=&#34;UI: Add BG animation for recent games&#34;&gt;#14347&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add CRC calculation on game info screen and feedback screen (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14000&#34; title=&#34;Add CRC32 calc&#34;&gt;#14000&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14041&#34; title=&#34;UI: Add button to show CRC on feedback screen&#34;&gt;#14041&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add a Storage tab to System Information with some path info (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14224&#34; title=&#34;Add a Storage tab to System Information with some path info&#34;&gt;#14224&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14238&#34; title=&#34;UI: Wrap long info items and cleanup storage display&#34;&gt;#14238&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Track and show memory allocation / usage information in debugger (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14056&#34; title=&#34;Track memory allocations and writes for debug info&#34;&gt;#14056&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Allow searching within the savedata manager (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14237&#34; title=&#34;Add initial search to savedata manager&#34;&gt;#14237&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Enable postshaders to access previous frame (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14528&#34; title=&#34;Postshader: Let shaders use the previous frame&#34;&gt;#14528&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add missing japanese keyboard symbol (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14548&#34; title=&#34;Add some PPSSPP&#39;s Japanese keyboard&#34;&gt;#14548&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add Reset button on crash screen, allow load state and related (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14708&#34; title=&#34;Add Reset button on crash screen, allow load state and related&#34;&gt;#14708&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Implement savestate load and save undo (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14676&#34; title=&#34;Add savestate undo UI&#34;&gt;#14676&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14679&#34; title=&#34;Savestate load undo&#34;&gt;#14679&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14697&#34; title=&#34;Add undo last save as well&#34;&gt;#14697&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;A lot of minor debugger improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Controls:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New analog stick calibration menu (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14596&#34; title=&#34;Replace the &amp;quot;Test Analogs&amp;quot; screen with a new screen that lets you directly try the settings.&#34;&gt;#14596&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Improved combo button and moved settings to Customize Touch Control -&amp;gt; Customize -&amp;gt; Custom button (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13869&#34; title=&#34;Make combo button more generic&#34;&gt;#13869&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Improved tilt control, allow to change axis (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12530&#34; title=&#34;Allow tilt input on Z instead of X&#34;&gt;#12530&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add a visual means of control mapping (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14769&#34; title=&#34;Add a visual means of control mapping&#34;&gt;#14769&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add basic motion gesture support (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13107&#34; title=&#34;Basic mappable motion gesture&#34;&gt;#13107&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix touch control DPAD not getting input when dragged over, and make touch analog drag not activate other buttons (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14843&#34; title=&#34;DPad drag fixes&#34;&gt;#14843&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Allow adjusting touch control analog stick head size (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14480&#34; title=&#34;Configurable analog head size&#34;&gt;#14480&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Adhoc/Network:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix multiplayer issue on MGS:PW due to detecting an incorrect source port on incoming data (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14140&#34; title=&#34;[Adhoc] Fix multiplayer issue on MGS:PW due to detecting an incorrect source port on incoming data&#34;&gt;#14140&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Always enable TCPNoDelay to improve response time (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14235&#34; title=&#34;[Adhoc] Always enable TCPNoDelay to improve response time&#34;&gt;#14235&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix Teenage Mutant Ninja Turtles multiplayer (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14284&#34; title=&#34;[Adhoc] Fix Teenage Mutant Ninja Turtles Multiplayer&#34;&gt;#14284&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix FlatOut Head On multiplayer (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14290&#34; title=&#34;Fix FlatOut Head On multiplayer.&#34;&gt;#14290&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Prevent flooding Adhoc Server with connection attempts (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14335&#34; title=&#34;[Adhoc] Prevent flooding Adhoc Server with connection attempts&#34;&gt;#14335&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix crashing issue when leaving a multiplayer game room (ie. GTA Vice City Stories) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14342&#34; title=&#34;[AdhocMatching] Fix crashing issue when leaving a multiplayer game room&#34;&gt;#14342&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix stuck issue when scanning AP to Recruit on MGS:PW (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14345&#34; title=&#34;[APctl] Fix stuck issue when scanning AP to Recruit on MGS:PW&#34;&gt;#14345&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix possible crash issue on blocking socket implementation (ie. Kao Challengers) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14466&#34; title=&#34;[Adhoc] Fix possible crash issue on blocking socket implementation.&#34;&gt;#14466&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create GameMode&#39;s socket after Master and all Replicas have been created (ie. Fading Shadows) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14492&#34; title=&#34;[AdhocGameMode] Create GameMode&#39;s socket after Master and all Replicas have been created&#34;&gt;#14492&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Reduce HLE delays due to multiplayer performance regressions (ie. Ys vs. Sora no Kiseki) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14513&#34; title=&#34;[Adhoc] Reducing HLE delays due to Mutiplayer performance regressions&#34;&gt;#14513&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix socket error 10014 on Windows when hosting a game of Vulcanus Seek and Destroy (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14849&#34; title=&#34;[Adhoc] Fix Socket error 10014 on Windows when hosting a game of Vulcanus Seek and Destroy&#34;&gt;#14849&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.11.3&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix for graphics glitches in the on-screen keyboard&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.11.2&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An additional few crash fixes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14129&#34; title=&#34;GPU: Force reinterpret off without copy image&#34;&gt;#14129&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14134&#34; title=&#34;Android: Ensure shutdown waits for render&#34;&gt;#14134&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14132&#34; title=&#34;Io: Truncate reads/writes to valid memory&#34;&gt;#14132&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.11.1&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A few crash fixes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14085&#34; title=&#34;Handle exec addr errors better - don&#39;t let IgnoreBadMemoryAccesses skip dispatcher exceptions&#34;&gt;#14085&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14089&#34; title=&#34;GL: Call CreateDeviceObjects *after* updating render_.&#34;&gt;#14089&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14091&#34; title=&#34;Only allow sceMpegGetAvcAu warmup for God Eater Series&#34;&gt;#14091&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14092&#34; title=&#34;SaveState: Prevent crash on bad cookie marker&#34;&gt;#14092&lt;/a&gt;), a few adhoc fixes&lt;/li&gt; &#xA; &lt;li&gt;Glitchy menu audio on some devices (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14101&#34; title=&#34;Menu audio glitchfix&#34;&gt;#14101&lt;/a&gt;), in-game UI font memory leak (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14078&#34; title=&#34;PPGe: Decimate text images properly&#34;&gt;#14078&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Couple of adhoc fixes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14106&#34; title=&#34;[Adhoc] Fix frozen (0 FPS) issue on Kao Challengers and Asterix &amp;amp; Obelix XX&#34;&gt;#14106&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/14117&#34; title=&#34;[Adhoc] Fix lob&#34;&gt;#14117&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What&#39;s new in 1.11.0&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lots of minor bug fixes, crash fixes, and performance fixes and improvements.&lt;/li&gt; &#xA; &lt;li&gt;New Browse... button to allow opening SD cards on Android 11&lt;/li&gt; &#xA; &lt;li&gt;Countless AdHoc networking fixes by ANR2ME, for example Dragon Ball Shin Budokai, PowerStone, Bleach Heat The Soul 7, Kingdom Hearts, GTA: VCS and many more.&lt;/li&gt; &#xA; &lt;li&gt;Graphics issue with car reflections fixed in Outrun, Dirt 2 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13636&#34; title=&#34;Reinterpret framebuffer formats as needed. Outrun reflections partial fix&#34;&gt;#13636&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13640&#34; title=&#34;Fix car reflections in Outrun&#34;&gt;#13640&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13760&#34; title=&#34;Fix car lighting issues in DiRT 2.&#34;&gt;#13760&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Cut-off cards in Yu Gi Oh fixed (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/7124&#34; title=&#34;Yu-Gi-Oh! GX Tag Force Card summoning (card cut-off / cropped)&#34;&gt;#7124&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Numerous fixes to the builtin fonts by nassau-tk&lt;/li&gt; &#xA; &lt;li&gt;Added exception handler so PPSSPP stays alive if a game crashes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/11795&#34; title=&#34;Exception handler - catch bad memory accesses&#34;&gt;#11795&lt;/a&gt;/&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13092&#34; title=&#34;Bad memory access handling improvements&#34;&gt;#13092&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Desktop: Support for multiple instance multiplayer (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13172&#34; title=&#34;Generalized multi-instance&#34;&gt;#13172&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Workaround for rendering bugs with flat shading in iOS 14&lt;/li&gt; &#xA; &lt;li&gt;Multiple fixes to the IR interpreter (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13897&#34; title=&#34;LittleBigPlanet - Game Not Loading, Blue Screen (iOS, Unplayable)&#34;&gt;#13897&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;UI: New fullscreen button on desktop platforms, optional navigation sounds (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13239&#34; title=&#34;Add sound effects for PPSSPP interface navigation&#34;&gt;#13239&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Audio and multiple hangs fixes in UWP version (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13792&#34; title=&#34;Fix UWP audio and a hang bug&#34;&gt;#13792&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Partial microphone support (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12336&#34; title=&#34;Microphone support&#34;&gt;#12336&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Workaround for wacky action mirroring bug in Hitman Reborn Battle Arena 2 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13706&#34; title=&#34;Add back the old implementation of vfpu_sin/cos/sincos.&#34;&gt;#13706&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13526&#34; title=&#34;VFPU: Compute sines and cosines in double precision.&#34;&gt;#13526&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Hardware texture upscaling for Vulkan, mipmap generation (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13235&#34; title=&#34;Vulkan: Allow custom texture upscaling shaders&#34;&gt;#13235&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13514&#34; title=&#34;Vulkan: Automatically generate mipmaps for replaced/scaled textures&#34;&gt;#13514&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Added MMPX Vulkan texture upscaling shader (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13986&#34; title=&#34;Vulkan: Add MMPX upscaling texture shader&#34;&gt;#13986&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Depth texturing support in Vulkan and D3D11 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13262&#34; title=&#34;Implement texturing from depth buffers (Vulkan only so far)&#34;&gt;#13262&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13556&#34; title=&#34;D3D11 depth texture support&#34;&gt;#13556&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Performance fix for Test Drive Unlimited (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13355&#34; title=&#34;Refactor framebuffer attachment. Fixes Test Drive Unlimited performance&#34;&gt;#13355&lt;/a&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Allow rewind on mobile (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13866&#34; title=&#34;SaveState: Allow rewind on mobile&#34;&gt;#13866&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Added option to disable on-screen messages (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13695&#34; title=&#34;Add developer setting &amp;quot;Show on-screen messages&amp;quot;. Uncheck to hide them.&#34;&gt;#13695&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Added &#34;Lower resolution for effects&#34; on libretro (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13654&#34; title=&#34;Expose the &amp;quot;Lower resolution for effects&amp;quot; setting in libretro.&#34;&gt;#13654&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Allow chaining multiple post-processing shaders (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12924&#34; title=&#34;Postprocessing: User chain support&#34;&gt;#12924&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Support for loading game-specific plugins (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13335&#34; title=&#34;Support for loading game-specific plugins&#34;&gt;#13335&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fixed Assassin&#39;s Creed: Bloodlines Save issue on Android (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12761&#34; title=&#34;[Android][OpenGL&amp;amp;Vulkan][Save issue] Assassin&#39;s Creed : Bloodlines (ULJM05571)&#34;&gt;#12761&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Hanayaka Nari Wa ga Ichizoku: mono voices fixed (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/5213&#34; title=&#34;Hanayaka Nari Wa ga Ichizoku strange MP3 mono voice&#34;&gt;#5213&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Additional fixed games: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Namco Museum - Battle Collection, Vol 2 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/9523&#34; title=&#34;Namco Museum - Battle Collection - ULUS100035 loading problem&#34;&gt;#9523&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13297&#34; title=&#34;Namco Museum Vol. 2 - ULJS00047 infinite loading in some game&#34;&gt;#13297&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13298&#34; title=&#34;Fix sceKernelExitThread&#34;&gt;#13298&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Dream Club Portable (graphics bugs, GL and Vulkan) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/6025&#34; title=&#34;Dream Club Portable crash after select girl&#34;&gt;#6025&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Capcom Classic Collection Reloaded (stuck in return game) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/4671&#34; title=&#34;Capcom Classic Collection Reloaded stuck in return game&#34;&gt;#4671&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Xyanide Resurrection (freezing) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/8526&#34; title=&#34;Xyanide Resurrection freezing&#34;&gt;#8526&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Dissidia Final Fantasy Chinese (patched game, invalid address) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13204&#34; title=&#34;Dissidia Final Fantasy Chinese patch invalid address&#34;&gt;#13204&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Crazy Taxi (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13368&#34; title=&#34;Reschedule after resuming thread from suspend.&#34;&gt;#13368&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Spiderman: Friend or Foe (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13969&#34; title=&#34;Io: Don&#39;t allow async close while async busy&#34;&gt;#13969&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Downstream Panic (US) (New Game crash) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13633&#34; title=&#34;Downstream Panic (US) New Game crashes&#34;&gt;#13633&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in 1.10.3&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fix for control layout editor (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13125&#34; title=&#34;Refactor and fix touch control layout screen for notch&#34;&gt;#13125&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in 1.10.2&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More crashfixes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13094&#34; title=&#34;Camera initialization crash fix&#34;&gt;#13094&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13093&#34; title=&#34;Add a try/catch to Android camera device listing.&#34;&gt;#13093&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Improve download performance and cancel behavior (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13095&#34; title=&#34;http: Check cancel flag more often&#34;&gt;#13095&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Restore the removed I/O on Thread option (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13096&#34; title=&#34;Revert &amp;quot;Remove the I/O on Thread option - treat it as always on.&amp;quot;&#34;&gt;#13096&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in 1.10.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fixes for common crashes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13077&#34; title=&#34;SaveState: Make sure to default init net data&#34;&gt;#13077&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13076&#34; title=&#34;Add some excessive null checks to GameScreen::render()&#34;&gt;#13076&lt;/a&gt;, see &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13057&#34; title=&#34;The 1.10 Android mystery crash thread!&#34;&gt;#13057&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for offset rendering in D3D9 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13071&#34; title=&#34;D3D9: Fix a sign mistake generating the projection matrix.&#34;&gt;#13071&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new in 1.10.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Graphics and compatibility fixes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12800&#34; title=&#34;x86jit: Force INF * 0 to +NAN&#34;&gt;#12800&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12670&#34; title=&#34;Attempts to replace 0 frame width with valid frame width.(sceMpegAvcCsc)&#34;&gt;#12670&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12635&#34; title=&#34;Kernel: Delay better in sceKernelReferThreadStatus&#34;&gt;#12635&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12857&#34; title=&#34;Mumbo Jumbo games freeze on loading screen since v1.6&#34;&gt;#12857&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12941&#34; title=&#34;Vulkan: Deal with the reformat clear better&#34;&gt;#12941&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/11898&#34; title=&#34;Strike Witches - Hakugin no Tsubasa  missing intro video&#34;&gt;#11898&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12695&#34; title=&#34;New heuristic for getting rid of unnecessary &amp;quot;antialias-lines&amp;quot;.&#34;&gt;#12695&lt;/a&gt;, more)&lt;/li&gt; &#xA; &lt;li&gt;Assorted minor performance improvements, game load speedup (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12462&#34; title=&#34;Vulkan: Enable renderpass merging for all games&#34;&gt;#12462&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12652&#34; title=&#34;ScanForFunctions: Speed up game loading&#34;&gt;#12652&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Screen inset (notch) support on Android (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12779&#34; title=&#34;Support drawing around notches on Android displays. Fixes #12261&#34;&gt;#12779&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Analog stick support for menu navigation (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12685&#34; title=&#34;UI: Simple joystick navigation. Fixes #10996.&#34;&gt;#12685&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fixed audio glitches in SDL builds (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12916&#34; title=&#34;More audio buffering fixes (primarily affects SDL)&#34;&gt;#12916&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12920&#34; title=&#34;Remove the Audio Resampling setting (now always on).&#34;&gt;#12920&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Support more languages in in-game dialogs (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12702&#34; title=&#34;PPGe: Use TextDrawer for save UI if available&#34;&gt;#12702&lt;/a&gt;). Croatian language added to PPSSPP.&lt;/li&gt; &#xA; &lt;li&gt;Simple multiplayer chat (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12667&#34; title=&#34;Chat feature based on Adenovan&#39;s Rechat branch&#34;&gt;#12667&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;More advanced postprocessing (multipass, parameters) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12905&#34; title=&#34;Allow chained post-processing shaders&#34;&gt;#12905&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12901&#34; title=&#34;Post shader setting uniform&#34;&gt;#12901&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add PPSSPP-specific CWCheat (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12816&#34; title=&#34;Implement Xinput vibration CWCheat (PPSSPP specific 0xA code type)&#34;&gt;#12816&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12912&#34; title=&#34;Add CWCHEAT for postprocessing&#34;&gt;#12912&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Reintroduce Cardboard VR, allow more resolutions (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12449&#34; title=&#34;Reintroduce Cardboard VR&#34;&gt;#12449&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/8714&#34; title=&#34;Allow &gt; 5x PSP resolution for devices like iPad Pro 12.9&#34;&gt;#8714&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix some crashes (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12908&#34; title=&#34;Fix &amp;quot;Improved compatibility of sceGeListEnQueue: verify that stackDepth &lt; 256&amp;quot;&#34;&gt;#12908&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12876&#34; title=&#34;Windows: Add safety checks to WASAPI code&#34;&gt;#12876&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Ghost in the Shell graphics fixed (JIT inaccuracy with inf * 0) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12519&#34; title=&#34;Ghost In The Shell - Stand Alone Complex (ULUS10020) - Black Textures and missing screens.&#34;&gt;#12519&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mac build now supports Vulkan on top of MoltenVK (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12583&#34; title=&#34;macOS: Initial support for vulkan on macOS ( MoltenVK )&#34;&gt;#12583&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Raspberry Pi 4 EGL crash fixed (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12474&#34; title=&#34;Egl bug on rpi4 with master mesa?&#34;&gt;#12474&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;VSync now supported on all backends, frame duplication option added for 30 Hz games (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12659&#34; title=&#34;Support vsync in all hardware backends, support runtime update&#34;&gt;#12659&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12602&#34; title=&#34;Add option to improve frame pacing through duplicate frames if below 60hz.&#34;&gt;#12602&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Camera supported on Windows, Linux and Mac (still no microphone though) (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12572&#34; title=&#34;Add camera support for windows.&#34;&gt;#12572&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12580&#34; title=&#34;Add camera support for linux (V4L2)&#34;&gt;#12580&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12607&#34; title=&#34;QT API for camera (Linux/macOS)&#34;&gt;#12607&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Darkstalkers fixed and working through software rendering. SW rendering fixed on GLES 2.0 (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12443&#34; title=&#34;Darkstalkers Chronicle: Add specializations and speedhacks to get it kinda playable&#34;&gt;#12443&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12898&#34; title=&#34;[Android] [Mali GPU] [OpenGL] Lastest build blackscreen on buffered rendering mode&#34;&gt;#12898&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Hot Shots Golf slowdown and flicker on Vulkan fixed (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12873&#34; title=&#34;Vulkan: Framebuffer manager: Use an allocator for &amp;quot;MakePixelTexture&amp;quot; images.&#34;&gt;#12873&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12746&#34; title=&#34;GPU: Assume a scissor of 481x273 is a mistake&#34;&gt;#12746&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Pangya Golf crashes and hangs fixed (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12718&#34; title=&#34;Vpl: Correct allocation order when splitting block&#34;&gt;#12718&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Allow rebinding of right touch screen analog (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12486&#34; title=&#34;Rebindable touch right analog&#34;&gt;#12486&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add option to prevent mipmaps from being dumped (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12818&#34; title=&#34;Add option to prevent Mipmaps from being dumped&#34;&gt;#12818&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tilt control now have a base radius to help with deadzone (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12756&#34; title=&#34;Skip deadzone option on tilt&#34;&gt;#12756&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mappable auto rotating analog stick to pass some game checks (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12749&#34; title=&#34;Auto rotating analog&#34;&gt;#12749&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Touch control position can now be snapped to a grid (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12517&#34; title=&#34;Touch control grid snap&#34;&gt;#12517&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;HiDPI retina display support (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12552&#34; title=&#34;Qt/macOS: enable HiDPI ( retina display ) support&#34;&gt;#12552&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Rapid-fire on touch control (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12601&#34; title=&#34;Add rapid fire to touch control&#34;&gt;#12601&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Toggle mute button (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12643&#34; title=&#34;Toggle mute button&#34;&gt;#12643&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add option to resize game icons and more (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12646&#34; title=&#34;Resizable game icons&#34;&gt;#12646&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12637&#34; title=&#34;Region flag and game ID on game selection screen&#34;&gt;#12637&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Frames in-flight now configurable to reduce input lag at the cost of speed (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12660&#34; title=&#34;GPU: Add setting to control inflight frame usage&#34;&gt;#12660&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add toggle mode to combo button (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12623&#34; title=&#34;Add toggle flag to combo button&#34;&gt;#12623&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;SDL mouse support, Qt menu upgrades (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12612&#34; title=&#34;SDL analog mouse input&#34;&gt;#12612&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12817&#34; title=&#34;Unification of the menu of Linux and Windows versions&#34;&gt;#12817&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Real support for chinese patched version of Hatsune Miku Project Diva Extend (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13007&#34; title=&#34;Real support &amp;quot;Hatsune Miku Project Diva Extend&amp;quot; chinese patched version&#34;&gt;#13007&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Some minor kernel module support (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13028&#34; title=&#34;Real support Code Geass: Lost Colors chinese patched version&#34;&gt;#13028&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/12225&#34; title=&#34;Rebased: Wrap some SysMemForKernel&#39;s nids, fixing #7960&#34;&gt;#12225&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13026&#34; title=&#34;Add some ThreadManForKernel nids&#34;&gt;#13026&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13004&#34; title=&#34;Warp some ThreadManForKernel and sceKernelExitVSHKernel&#34;&gt;#13004&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13038&#34; title=&#34;Add sysclib_strncmp,sysclib_memmove&#34;&gt;#13038&lt;/a&gt;, &lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/13023&#34; title=&#34;Add sysclib_strstr&#34;&gt;#13023&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fixed fullscreen toggling with Vulkan in SDL builds (&lt;a href=&#34;https://github.com/hrydgard/ppsspp/issues/11974&#34; title=&#34;[Linux] [Vulkan] Toggle fullscreen doesn&#39;t update display properly&#34;&gt;#11974&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Looking for &lt;a href=&#34;https://raw.githubusercontent.com/hrydgard/ppsspp/master/history.md&#34;&gt;older news&lt;/a&gt;?&lt;/p&gt; &#xA;&lt;h2&gt;Adhoc support&lt;/h2&gt; &#xA;&lt;p&gt;Not fully functional, but some games work. Check the &lt;a href=&#34;http://forums.ppsspp.org/forumdisplay.php?fid=34&#34;&gt;Ad-Hoc section of the forum&lt;/a&gt; for help.&lt;/p&gt; &#xA;&lt;p&gt;Credit goes to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ANR2ME&lt;/li&gt; &#xA; &lt;li&gt;Igor Calabria&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://code.google.com/archive/p/aemu/&#34;&gt;coldbird&#39;s code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kyhel&lt;/li&gt; &#xA; &lt;li&gt;And more, of course.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>electron/electron</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/electron/electron</id>
    <link href="https://github.com/electron/electron" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://electronjs.org&#34;&gt;&lt;img src=&#34;https://electronjs.org/images/electron-logo.svg?sanitize=true&#34; alt=&#34;Electron Logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/electron/electron/tree/main&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/electron/electron/tree/main.svg?style=shield&#34; alt=&#34;CircleCI Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/electron-bot/electron-ljo26/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/4lggi9dpjc1qob7k/branch/main?svg=true&#34; alt=&#34;AppVeyor Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/APGC3k5yaH&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/745037351163527189?color=%237289DA&amp;amp;label=chat&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Electron Discord Invite&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üìù&lt;/span&gt; Available Translations: üá®üá≥ üáßüá∑ üá™üá∏ üáØüáµ üá∑üá∫ üá´üá∑ üá∫üá∏ üá©üá™. View these docs in other languages at &lt;a href=&#34;https://github.com/electron/i18n/tree/master/content/&#34;&gt;electron/i18n&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Electron framework lets you write cross-platform desktop applications using JavaScript, HTML and CSS. It is based on &lt;a href=&#34;https://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; and &lt;a href=&#34;https://www.chromium.org&#34;&gt;Chromium&lt;/a&gt; and is used by the &lt;a href=&#34;https://github.com/atom/atom&#34;&gt;Atom editor&lt;/a&gt; and many other &lt;a href=&#34;https://electronjs.org/apps&#34;&gt;apps&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/electronjs&#34;&gt;@ElectronJS&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; &#xA;&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&#34;https://github.com/electron/electron/tree/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&#34;mailto:coc@electronjs.org&#34;&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href=&#34;https://docs.npmjs.com/&#34;&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;. The preferred method is to install Electron as a development dependency in your app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install electron --save-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more installation options and troubleshooting tips, see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/installation.md&#34;&gt;installation&lt;/a&gt;. For info on how to manage Electron versions in your apps, see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/electron-versioning.md&#34;&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Platform support&lt;/h2&gt; &#xA;&lt;p&gt;Each Electron release provides binaries for macOS, Windows, and Linux.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS (El Capitan and up): Electron provides 64-bit Intel and ARM binaries for macOS. Apple Silicon support was added in Electron 11.&lt;/li&gt; &#xA; &lt;li&gt;Windows (Windows 7 and up): Electron provides &lt;code&gt;ia32&lt;/code&gt; (&lt;code&gt;x86&lt;/code&gt;), &lt;code&gt;x64&lt;/code&gt; (&lt;code&gt;amd64&lt;/code&gt;), and &lt;code&gt;arm64&lt;/code&gt; binaries for Windows. Windows on ARM support was added in Electron 5.0.8.&lt;/li&gt; &#xA; &lt;li&gt;Linux: The prebuilt binaries of Electron are built on Ubuntu 20.04. They have also been verified to work on: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ubuntu 14.04 and newer&lt;/li&gt; &#xA;   &lt;li&gt;Fedora 24 and newer&lt;/li&gt; &#xA;   &lt;li&gt;Debian 8 and newer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start &amp;amp; Electron Fiddle&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://github.com/electron/fiddle&#34;&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt; to build, run, and package small Electron experiments, to see code examples for all of Electron&#39;s APIs, and to try out different versions of Electron. It&#39;s designed to make the start of your journey with Electron easier.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, clone and run the &lt;a href=&#34;https://github.com/electron/electron-quick-start&#34;&gt;electron/electron-quick-start&lt;/a&gt; repository to see a minimal Electron app in action:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/electron/electron-quick-start&#xA;cd electron-quick-start&#xA;npm install&#xA;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources for learning Electron&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://electronjs.org/docs&#34;&gt;electronjs.org/docs&lt;/a&gt; - All of Electron&#39;s documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/fiddle&#34;&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/electron/electron-quick-start&#34;&gt;electron/electron-quick-start&lt;/a&gt; - A very basic starter Electron app&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://electronjs.org/community#boilerplates&#34;&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Programmatic usage&lt;/h2&gt; &#xA;&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the binary. Use this to spawn Electron from Node scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const electron = require(&#39;electron&#39;)&#xA;const proc = require(&#39;child_process&#39;)&#xA;&#xA;// will print something similar to /Users/maf/.../Electron&#xA;console.log(electron)&#xA;&#xA;// spawn Electron&#xA;const child = proc.spawn(electron)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mirrors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://npmmirror.com/mirrors/electron/&#34;&gt;China&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.electronjs.org/docs/latest/tutorial/installation#mirror&#34;&gt;Advanced Installation Instructions&lt;/a&gt; to learn how to use a custom mirror.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation translations&lt;/h2&gt; &#xA;&lt;p&gt;We crowdsource translations for our documentation via &lt;a href=&#34;https://crowdin.com/project/electron&#34;&gt;Crowdin&lt;/a&gt;. We currently accept translations for Chinese (Simplified), French, German, Japanese, Portuguese, Russian, and Spanish.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href=&#34;https://raw.githubusercontent.com/electron/electron/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we&#39;re looking for and how to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps, and more can be found on the &lt;a href=&#34;https://www.electronjs.org/community&#34;&gt;Community page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/electron/electron/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;When using Electron logos, make sure to follow &lt;a href=&#34;https://openjsf.org/wp-content/uploads/sites/84/2021/01/OpenJS-Foundation-Trademark-Policy-2021-01-12.docx.pdf&#34;&gt;OpenJS Foundation Trademark Policy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/onnxruntime</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/microsoft/onnxruntime</id>
    <link href="https://github.com/microsoft/onnxruntime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/images/ONNX_Runtime_logo_dark.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime is a cross-platform inference and training machine-learning accelerator&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime inference&lt;/strong&gt; can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing&#34;&gt;Learn more ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ONNX Runtime training&lt;/strong&gt; can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. &lt;a href=&#34;https://www.onnxruntime.ai/docs/#onnx-runtime-for-training&#34;&gt;Learn more ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;General Information&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai&#34;&gt;onnxruntime.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage documention and tutorials&lt;/strong&gt;: &lt;a href=&#34;https://onnxruntime.ai/docs&#34;&gt;onnxruntime.ai/docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Companion sample repositories&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ONNX Runtime Inferencing: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-inference-examples&#34;&gt;microsoft/onnxruntime-inference-examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ONNX Runtime Training: &lt;a href=&#34;https://github.com/microsoft/onnxruntime-training-examples&#34;&gt;microsoft/onnxruntime-training-examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build Pipeline Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;EPs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20x64%20NoContribops%20CI%20Pipeline?label=Linux+CPU+x64+No+Contrib+Ops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=78&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/centos7_cpu?label=Linux+CentOS7&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=140&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-distributed?label=Distributed+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20NUPHAR%20CI%20Pipeline?label=Linux+NUPHAR&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=55&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline?label=Linux+OpenVINO&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mac&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=65&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20NoContribops%20CI%20Pipeline?label=MacOS+NoContribops&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Android&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;iOS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WebAssembly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161&#34;&gt;&lt;img src=&#34;https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20WebAssembly%20CI%20Pipeline?label=WASM&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data/Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Windows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/docs/Privacy.md&#34;&gt;privacy statement&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions and Feedback&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For feature requests or bug reports, please file a &lt;a href=&#34;https://github.com/Microsoft/onnxruntime/issues&#34;&gt;GitHub Issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general discussion or questions, please use &lt;a href=&#34;https://github.com/microsoft/onnxruntime/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/onnxruntime/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openvinotoolkit/openvino</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/openvinotoolkit/openvino</id>
    <link href="https://github.com/openvinotoolkit/openvino" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenVINO‚Ñ¢ Toolkit repository&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/docs/img/openvino-logo-purple-black.png&#34; width=&#34;400px&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/releases/tag/2022.1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-2022.1-green.svg?sanitize=true&#34; alt=&#34;Stable release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Apache License Version 2.0&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/checks-status/openvinotoolkit/openvino/master?label=GitHub%20checks&#34; alt=&#34;GitHub branch checks state&#34;&gt; &lt;img src=&#34;https://img.shields.io/azure-devops/build/openvinoci/b2bab62f-ab2f-4871-a538-86ea1be7d20f/13?label=Public%20CI&#34; alt=&#34;Azure DevOps builds (branch)&#34;&gt; &lt;a href=&#34;https://badge.fury.io/py/openvino&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/openvino.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/openvino&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/openvino&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#what-is-openvino-toolkit&#34;&gt;What is OpenVINO?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#components&#34;&gt;Components&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#supported-hardware-matrix&#34;&gt;Supported Hardware matrix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#products-which-use-openvino&#34;&gt;Products which use OpenVINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#system-requirements&#34;&gt;System requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#how-to-build&#34;&gt;How to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#how-to-contribute&#34;&gt;How to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#get-a-support&#34;&gt;Get a support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#see-also&#34;&gt;See also&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is OpenVINO toolkit?&lt;/h2&gt; &#xA;&lt;p&gt;OpenVINO‚Ñ¢ is an open-source toolkit for optimizing and deploying AI inference.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Boost deep learning performance in computer vision, automatic speech recognition, natural language processing and other common tasks&lt;/li&gt; &#xA; &lt;li&gt;Use models trained with popular frameworks like TensorFlow, PyTorch and more&lt;/li&gt; &#xA; &lt;li&gt;Reduce resource demands and efficiently deploy on a range of Intel¬Æ platforms from edge to cloud&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This open-source version includes several components: namely &lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html&#34;&gt;Model Optimizer&lt;/a&gt;, &lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html&#34;&gt;OpenVINO‚Ñ¢ Runtime&lt;/a&gt;, &lt;a href=&#34;https://docs.openvino.ai/latest/pot_introduction.html&#34;&gt;Post-Training Optimization Tool&lt;/a&gt;, as well as CPU, GPU, MYRIAD, multi device and heterogeneous plugins to accelerate deep learning inferencing on Intel¬Æ CPUs and Intel¬Æ Processor Graphics. It supports pre-trained models from the &lt;a href=&#34;https://github.com/openvinotoolkit/open_model_zoo&#34;&gt;Open Model Zoo&lt;/a&gt;, along with 100+ open source and public models in popular formats such as TensorFlow, ONNX, PaddlePaddle, MXNet, Caffe, Kaldi.&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_OV_UG_OV_Runtime_User_Guide.html&#34;&gt;OpenVINO‚Ñ¢ Runtime&lt;/a&gt; - is a set of C++ libraries with C and Python bindings providing a common API to deliver inference solutions on the platform of your choice. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/core&#34;&gt;core&lt;/a&gt; - provides the base API for model representation and modification.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/inference&#34;&gt;inference&lt;/a&gt; - provides an API to infer models on device.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/common/transformations&#34;&gt;transformations&lt;/a&gt; - contains the set of common transformations which are used in OpenVINO plugins.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/common/low_precision_transformations&#34;&gt;low precision transformations&lt;/a&gt; - contains the set of transformations which are used in low precision models&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings&#34;&gt;bindings&lt;/a&gt; - contains all awailable OpenVINO bindings which are maintained by OpenVINO team. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings/c&#34;&gt;c&lt;/a&gt; - provides C API for OpenVINO‚Ñ¢ Runtime&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/bindings/python&#34;&gt;python&lt;/a&gt; - Python API for OpenVINO‚Ñ¢ Runtime&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins&#34;&gt;Plugins&lt;/a&gt; - contains OpenVINO plugins which are maintained in open-source by OpenVINO team. For more information please taje a look to the &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#supported-hardware-matrix&#34;&gt;list of supported devices&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/frontends&#34;&gt;Frontends&lt;/a&gt; - contains available OpenVINO frontends which allow to read model from native framework format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html&#34;&gt;Model Optimizer&lt;/a&gt; - is a cross-platform command-line tool that facilitates the transition between training and deployment environments, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/pot_introduction.html&#34;&gt;Post-Training Optimization Tool&lt;/a&gt; - is designed to accelerate the inference of deep learning models by applying special methods without model retraining or fine-tuning, for example, post-training 8-bit quantization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/samples&#34;&gt;Samples&lt;/a&gt; - applications on C, C++ and Python languages which shows basic use cases of OpenVINO usages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Hardware matrix&lt;/h2&gt; &#xA;&lt;p&gt;The OpenVINO‚Ñ¢ Runtime can infer models on different hardware devices. This section provides the list of supported devices.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;ShortDescription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_CPU.html#doxid-openvino-docs-o-v-u-g-supported-plugins-c-p-u&#34;&gt;Intel CPU&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_cpu&#34;&gt;openvino_intel_cpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Xeon with Intel¬Æ Advanced Vector Extensions 2 (Intel¬Æ AVX2), Intel¬Æ Advanced Vector Extensions 512 (Intel¬Æ AVX-512), and AVX512_BF16, Intel Core Processors with Intel AVX2, Intel Atom Processors with Intel¬Æ Streaming SIMD Extensions (Intel¬Æ SSE)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_ARM_CPU.html&#34;&gt;ARM CPU&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_contrib/tree/master/modules/arm_plugin&#34;&gt;openvino_arm_cpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Raspberry Pi‚Ñ¢ 4 Model B, Apple¬Æ Mac mini with M1 chip, NVIDIA¬Æ Jetson Nano‚Ñ¢, Android‚Ñ¢ devices &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_GPU.html#doxid-openvino-docs-o-v-u-g-supported-plugins-g-p-u&#34;&gt;Intel GPU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_gpu&#34;&gt;openvino_intel_gpu_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Processor Graphics, including Intel HD Graphics and Intel Iris Graphics&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GNA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_supported_plugins_GNA.html#doxid-openvino-docs-o-v-u-g-supported-plugins-g-n-a&#34;&gt;Intel GNA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_gna&#34;&gt;openvino_intel_gna_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel Speech Enabling Developer Kit, Amazon Alexa* Premium Far-Field Developer Kit, Intel Pentium Silver J5005 Processor, Intel Pentium Silver N5000 Processor, Intel Celeron J4005 Processor, Intel Celeron J4105 Processor, Intel Celeron Processor N4100, Intel Celeron Processor N4000, Intel Core i3-8121U Processor, Intel Core i7-1065G7 Processor, Intel Core i7-1060G7 Processor, Intel Core i5-1035G4 Processor, Intel Core i5-1035G7 Processor, Intel Core i5-1035G1 Processor, Intel Core i5-1030G7 Processor, Intel Core i5-1030G4 Processor, Intel Core i3-1005G1 Processor, Intel Core i3-1000G1 Processor, Intel Core i3-1000G4 Processor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_IE_DG_supported_plugins_VPU.html#doxid-openvino-docs-i-e-d-g-supported-plugins-v-p-u&#34;&gt;Myriad plugin&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/intel_myriad&#34;&gt;openvino_intel_myriad_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel¬Æ Neural Compute Stick 2 powered by the Intel¬Æ Movidius‚Ñ¢ Myriad‚Ñ¢ X&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Also OpenVINO‚Ñ¢ Toolkit contains several plugins which should simplify to load model on several hardware devices:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;ShortDescription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_IE_DG_supported_plugins_AUTO.html#doxid-openvino-docs-i-e-d-g-supported-plugins-a-u-t-o&#34;&gt;Auto&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto&#34;&gt;openvino_auto_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto plugin enables selecting Intel device for inference automatically&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Automatic_Batching.html&#34;&gt;Auto Batch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto_batch&#34;&gt;openvino_auto_batch_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto batch plugin performs on-the-fly automatic batching (i.e. grouping inference requests together) to improve device utilization, with no programming effort from the user&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Hetero_execution.html#doxid-openvino-docs-o-v-u-g-hetero-execution&#34;&gt;Hetero&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/hetero&#34;&gt;openvino_hetero_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Heterogeneous execution enables automatic inference splitting between several devices&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Running_on_multiple_devices.html#doxid-openvino-docs-o-v-u-g-running-on-multiple-devices&#34;&gt;Multi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;i&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/tree/master/src/plugins/auto&#34;&gt;openvino_auto_plugin&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multi plugin enables simultaneous inference of the same model on several devices in parallel&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpenVINO‚Ñ¢ Toolkit is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt;. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;User documentation&lt;/h3&gt; &#xA;&lt;p&gt;The latest documentation for OpenVINO‚Ñ¢ Toolkit is availabe &lt;a href=&#34;https://docs.openvino.ai/&#34;&gt;here&lt;/a&gt;. This documentation contains detailed information about all OpenVINO components and provides all important information which could be needed if you create an application which is based on binary OpenVINO distribution or own OpenVINO version without source code modification.&lt;/p&gt; &#xA;&lt;h3&gt;Developer documentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/#todo-add&#34;&gt;Developer documentation&lt;/a&gt; contains information about architectural decisions which are applied inside the OpenVINO components. This documentation has all necessary information which could be needed in order to contribute to OpenVINO.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;The list of OpenVINO tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_notebooks&#34;&gt;Jupiter notebooks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Products which use OpenVINO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opencv.org/&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onnxruntime.ai/&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/build/ovtfoverview.html&#34;&gt;OpenVINO‚Ñ¢ Integration with TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/TNN/tree/master&#34;&gt;TNN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;The full information about system requirements depends on platform and available in section &lt;code&gt;System requirement&lt;/code&gt; on dedicated pages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_linux.html&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_windows.html&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_macos.html&#34;&gt;macOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino_raspbian.html&#34;&gt;Raspbian&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;p&gt;Please take a look to &lt;a href=&#34;https://github.com/openvinotoolkit/openvino/wiki#how-to-build&#34;&gt;OpenVINO Wiki&lt;/a&gt; to get more information about OpenVINO build process.&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/openvino/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; for details. Thank you!&lt;/p&gt; &#xA;&lt;h2&gt;Get a support&lt;/h2&gt; &#xA;&lt;p&gt;Please report questions, issues and suggestions using:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/issues&#34;&gt;GitHub* Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://stackoverflow.com/questions/tagged/openvino&#34;&gt;&lt;code&gt;openvino&lt;/code&gt;&lt;/a&gt; tag on StackOverflow*&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/forums/computer-vision&#34;&gt;Forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;See also&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino/wiki&#34;&gt;OpenVINO Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storage.openvinotoolkit.org/&#34;&gt;OpenVINO Storage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Additional OpenVINO‚Ñ¢ toolkit modules: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_contrib&#34;&gt;openvino_contrib&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html&#34;&gt;Intel¬Æ Distribution of OpenVINO‚Ñ¢ toolkit Product Page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/articles/OpenVINO-RelNotes&#34;&gt;Intel¬Æ Distribution of OpenVINO‚Ñ¢ toolkit Release Notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/nncf&#34;&gt;Neural Network Compression Framework (NNCF)&lt;/a&gt; - a suite of advanced algorithms for model inference optimization including quantization, filter pruning, binarization and sparsity&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/training_extensions&#34;&gt;OpenVINO‚Ñ¢ Training Extensions (OTE)&lt;/a&gt; - convenient environment to train Deep Learning models and convert them using OpenVINO for optimized inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/model_server&#34;&gt;OpenVINO‚Ñ¢ Model Server (OVMS)&lt;/a&gt; - a scalable, high-performance solution for serving deep learning models optimized for Intel architectures&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/nightly/workbench_docs_Workbench_DG_Introduction.html&#34;&gt;DL Workbench&lt;/a&gt; - An alternative, web-based version of OpenVINO designed to make production of pretrained deep learning models significantly easier.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/cvat&#34;&gt;Computer Vision Annotation Tool (CVAT)&lt;/a&gt; - an online, interactive video and image annotation tool for computer vision purposes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/datumaro&#34;&gt;Dataset Management Framework (Datumaro)&lt;/a&gt; - a framework and CLI tool to build, transform, and analyze datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;* Other names and brands may be claimed as the property of others.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InoriRus/Kyty</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/InoriRus/Kyty</id>
    <link href="https://github.com/InoriRus/Kyty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PS4 &amp; PS5 emulator&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://ci.appveyor.com/project/InoriRus/kyty&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/0du32fg9flol63to?svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Kyty&lt;/h1&gt; &#xA;&lt;h2&gt;PS4 &amp;amp; PS5 emulator&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The project is in its early stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:inorirus@gmail.com&#34;&gt;Vladimir M&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the MIT license.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Graphics for PS5 is not yet implemented&lt;/p&gt; &#xA;&lt;p&gt;It is possible to run some simple games for PS4&lt;/p&gt; &#xA;&lt;p&gt;There maybe graphics glitches, crashes, freezes and low FPS. It&#39;s OK for now.&lt;/p&gt; &#xA;&lt;p&gt;Features that are not implemented:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Audio input/output&lt;/li&gt; &#xA; &lt;li&gt;MP4 video&lt;/li&gt; &#xA; &lt;li&gt;Network&lt;/li&gt; &#xA; &lt;li&gt;Multi-user&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Path to Savedata folder is hardcoded and can&#39;t be configured. System parameters (language, date format, etc.) are also hardcoded.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Screenshots&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674296-4185e2da-99f9-4073-8ca9-19dc124c7459.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674298-df817d95-7288-46fe-a040-3c0a40c29a6b.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674301-37a3f947-76cd-4a9b-8c81-adec3d5d9c59.png&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7149418/169674303-13edae7d-24d3-4ec6-ba94-586e13c69df5.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;Supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows 10 x64&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Toolchains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visual Studio + clang-cl + ninja&lt;/li&gt; &#xA; &lt;li&gt;Eclipse CDT + mingw-w64 + gcc/clang + ninja/mingw32-make&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported versions:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Tool&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;cmake&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Visual Studio 2019&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;16.10.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;clang&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;12.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;clang-cl&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;11.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;gcc (MinGW-W64 x86_64-posix-seh)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ninja&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.10.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MinGW-w64&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;8.0.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Eclipse CDT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10.3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qt&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;5.15.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Define environment variable named Qt5_DIR pointing to the proper version of Qt&lt;/p&gt; &#xA;&lt;p&gt;MSVC compiler (cl.exe) is not supported!&lt;/p&gt; &#xA;&lt;p&gt;External dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vulkan SDK 1.2.198.1&lt;/li&gt; &#xA; &lt;li&gt;Qt 5.15.0&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>alibaba/MNN</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/alibaba/MNN</id>
    <link href="https://github.com/alibaba/MNN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/banner.png&#34; alt=&#34;MNN&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/README_CN.md&#34;&gt;‰∏≠ÊñáÁâàÊú¨&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.mnn.zone&#34;&gt;MNN Homepage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Intro&lt;/h2&gt; &#xA;&lt;p&gt;MNN is a highly efficient and lightweight deep learning framework. It supports inference and training of deep learning models, and has industry leading performance for inference and training on-device. At present, MNN has been integrated in more than 20 apps of Alibaba Inc, such as Taobao, Tmall, Youku, Dingtalk, Xianyu and etc., covering more than 70 usage scenarios such as live broadcast, short video capture, search recommendation, product searching by image, interactive marketing, equity distribution, security risk control. In addition, MNN is also used on embedded devices, such as IoT.&lt;/p&gt; &#xA;&lt;p&gt;The design principles and performance data of MNN has been published in an MLSys 2020 paper &lt;a href=&#34;https://arxiv.org/pdf/2002.12418.pdf&#34;&gt;here&lt;/a&gt;. Please cite MNN in your publications if it helps your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{alibaba2020mnn,&#xA;  author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},&#xA;  title = {MNN: A Universal and Efficient Inference Engine},&#xA;  booktitle = {MLSys},&#xA;  year = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation and Tools&lt;/h2&gt; &#xA;&lt;p&gt;MNN&#39;s docs are in placed in &lt;a href=&#34;https://www.yuque.com/mnn/en&#34;&gt;Yuque docs here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MNN Workbench could be downloaded from &lt;a href=&#34;http://www.mnn.zone&#34;&gt;MNN&#39;s homepage&lt;/a&gt;, which provides pretrained models, visualized training tools, and one-click deployment of models to devices.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;High performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implements core computing with lots of optimized assembly code to make full use of the ARM CPU.&lt;/li&gt; &#xA; &lt;li&gt;For iOS, GPU acceleration (Metal) can be turned on, which is faster than Apple&#39;s native CoreML.&lt;/li&gt; &#xA; &lt;li&gt;For Android, &lt;code&gt;OpenCL&lt;/code&gt;, &lt;code&gt;Vulkan&lt;/code&gt;, and &lt;code&gt;OpenGL&lt;/code&gt; are available and deep tuned for mainstream GPUs (&lt;code&gt;Adreno&lt;/code&gt; and &lt;code&gt;Mali&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Convolution and transposition convolution algorithms are efficient and stable. The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3 -&amp;gt; 7x7.&lt;/li&gt; &#xA; &lt;li&gt;Twice speed increase for the new architecture ARM v8.2 with FP16 half-precision calculation support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Lightweight&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optimized for devices, no dependencies, can be easily deployed to mobile devices and a variety of embedded devices.&lt;/li&gt; &#xA; &lt;li&gt;iOS platform: static library size for armv7+arm64 platforms is about 5MB, size increase of linked executables is about 620KB, and metallib file is about 600KB.&lt;/li&gt; &#xA; &lt;li&gt;Android platform: core so size is about 400KB, OpenCL so is about 400KB, Vulkan so is about 400KB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Versatility&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports &lt;code&gt;Tensorflow&lt;/code&gt;, &lt;code&gt;Caffe&lt;/code&gt;, &lt;code&gt;ONNX&lt;/code&gt;, and supports common neural networks such as &lt;code&gt;CNN&lt;/code&gt;, &lt;code&gt;RNN&lt;/code&gt;, &lt;code&gt;GAN&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MNN model converter supports 149 &lt;code&gt;Tensorflow&lt;/code&gt; OPs, 58 &lt;code&gt;TFLite&lt;/code&gt; OPs, 47 &lt;code&gt;Caffe&lt;/code&gt; OPs and 74 &lt;code&gt;ONNX&lt;/code&gt; OPs; Number of OPs by different MNN hardware backends: 111 for CPU, 6 for ARM V8.2, 55 for Metal, 43 for OpenCL, and 32 for Vulkan.&lt;/li&gt; &#xA; &lt;li&gt;Supports iOS 8.0+, Android 4.3+ and embedded devices with POSIX interface.&lt;/li&gt; &#xA; &lt;li&gt;Supports hybrid computing on multiple devices. Currently supports CPU and GPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ease of use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Efficient image processing module, speeding up affine transform and color space transform without libyuv or opencv.&lt;/li&gt; &#xA; &lt;li&gt;Provides callbacks throughout the workflow to extract data or control the execution precisely.&lt;/li&gt; &#xA; &lt;li&gt;Provides options for selecting inference branch and paralleling branches on CPU and GPU.&lt;/li&gt; &#xA; &lt;li&gt;(BETA) MNN Python API helps ML engineers to easily use MNN to build a model, train it and quantize it, without dipping their toes in C++ code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/MNN/master/doc/architecture.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;MNN can be divided into two parts: Converter and Interpreter.&lt;/p&gt; &#xA;&lt;p&gt;Converter consists of Frontends and Graph Optimize. The former is responsible for supporting different training frameworks. MNN currently supports Tensorflow, Tensorflow Lite, Caffe and ONNX (PyTorch/MXNet); the latter optimizes graphs by operator fusion, operator substitution, and layout adjustment.&lt;/p&gt; &#xA;&lt;p&gt;Interpreter consists of Engine and Backends. The former is responsible for the loading of the model and the scheduling of the calculation graph; the latter includes the memory allocation and the Op implementation under each computing device. In Engine and Backends, MNN applies a variety of optimization schemes, including applying Winograd algorithm in convolution and deconvolution, applying Strassen algorithm in matrix multiplication, low-precision calculation, Neon optimization, hand-written assembly, multi-thread optimization, memory reuse, heterogeneous computing, etc.&lt;/p&gt; &#xA;&lt;h2&gt;How to Discuss and Get Help From MNN Community&lt;/h2&gt; &#xA;&lt;p&gt;The group discussions are predominantly Chinese. But we welcome and will help English speakers.&lt;/p&gt; &#xA;&lt;p&gt;Dingtalk discussion groups:&lt;/p&gt; &#xA;&lt;p&gt;Group #1 (Full): 23329087&lt;/p&gt; &#xA;&lt;p&gt;Group #2 (Full): 23350225&lt;/p&gt; &#xA;&lt;p&gt;Group #3: &lt;a href=&#34;https://h5.dingtalk.com/circle/healthCheckin.html?dtaction=os&amp;amp;corpId=ding2c1d5c85a81030b9a483726330e8af54&amp;amp;574b2bb2-c53a-4=497bad6b-25a5-4&amp;amp;cbdbhh=qwertyuiop&#34;&gt;https://h5.dingtalk.com/circle/healthCheckin.html?dtaction=os&amp;amp;corpId=ding2c1d5c85a81030b9a483726330e8af54&amp;amp;574b2bb2-c53a-4=497bad6b-25a5-4&amp;amp;cbdbhh=qwertyuiop&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache 2.0&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MNN participants: Taobao Technology Department, Search Engineering Team, DAMO Team, Youku and other Alibaba Group employees.&lt;/p&gt; &#xA;&lt;p&gt;MNN refers to the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BVLC/caffe&#34;&gt;Caffe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/flatbuffers&#34;&gt;flatbuffer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/gemmlowp&#34;&gt;gemmlowp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.github.com/googlesamples/android-vulkan-tutorials&#34;&gt;Google Vulkan demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halide/Halide&#34;&gt;Halide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/XiaoMi/mace&#34;&gt;Mace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/onnx/onnx&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/protocolbuffers/protobuf&#34;&gt;protobuffer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/skia&#34;&gt;skia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/ncnn&#34;&gt;ncnn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/paddle-mobile&#34;&gt;paddle-mobile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nothings/stb&#34;&gt;stb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/rapidjson&#34;&gt;rapidjson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pybind/pybind11&#34;&gt;pybind11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huawei-noah/bolt&#34;&gt;bolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chromium.googlesource.com/libyuv/libyuv&#34;&gt;libyuv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/libjpeg-turbo/libjpeg-turbo&#34;&gt;libjpeg&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ethereum/solidity</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/ethereum/solidity</id>
    <link href="https://github.com/ethereum/solidity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solidity, the Smart Contract Programming Language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Solidity Contract-Oriented Programming Language&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/#ethereum_solidity:gitter.im&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Matrix%20-chat-brightgreen?style=plastic&amp;amp;logo=matrix&#34; alt=&#34;Matrix Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/ethereum/solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitter%20-chat-brightgreen?style=plastic&amp;amp;logo=gitter&#34; alt=&#34;Gitter Chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.soliditylang.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Solidity_Forum%20-discuss-brightgreen?style=plastic&amp;amp;logo=discourse&#34; alt=&#34;Solidity&amp;nbsp;Forum&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/solidity_lang&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/solidity_lang?style=plastic&amp;amp;logo=twitter&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@solidity&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/000335908?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;logo=mastodon&amp;amp;style=plastic&#34; alt=&#34;Mastodon Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can talk to us on Gitter and Matrix, tweet at us on Twitter or create a new topic in the Solidity forum. Questions, feedback, and suggestions are welcome!&lt;/p&gt; &#xA;&lt;p&gt;Solidity is a statically typed, contract-oriented, high-level language for implementing smart contracts on the Ethereum platform.&lt;/p&gt; &#xA;&lt;p&gt;For a good overview and starting point, please check out the official &lt;a href=&#34;https://soliditylang.org&#34;&gt;Solidity Language Portal&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#build-and-install&#34;&gt;Build and Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#maintainers&#34;&gt;Maintainers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/#security&#34;&gt;Security&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is a statically-typed curly-braces programming language designed for developing smart contracts that run on the Ethereum Virtual Machine. Smart contracts are programs that are executed inside a peer-to-peer network where nobody has special authority over the execution, and thus they allow to implement tokens of value, ownership, voting, and other kinds of logic.&lt;/p&gt; &#xA;&lt;p&gt;When deploying contracts, you should use the latest released version of Solidity. This is because breaking changes, as well as new features and bug fixes are introduced regularly. We currently use a 0.x version number &lt;a href=&#34;https://semver.org/#spec-item-4&#34;&gt;to indicate this fast pace of change&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build and Install&lt;/h2&gt; &#xA;&lt;p&gt;Instructions about how to build and install the Solidity compiler can be found in the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/installing-solidity.html#building-from-source&#34;&gt;Solidity documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;A &#34;Hello World&#34; program in Solidity is of even less use than in other languages, but still:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-solidity&#34;&gt;// SPDX-License-Identifier: MIT&#xA;pragma solidity &amp;gt;=0.6.0 &amp;lt;0.9.0;&#xA;&#xA;contract HelloWorld {&#xA;    function helloWorld() external pure returns (string memory) {&#xA;        return &#34;Hello, World!&#34;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get started with Solidity, you can use &lt;a href=&#34;https://remix.ethereum.org/&#34;&gt;Remix&lt;/a&gt;, which is a browser-based IDE. Here are some example contracts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#voting&#34;&gt;Voting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#blind-auction&#34;&gt;Blind Auction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#safe-remote-purchase&#34;&gt;Safe remote purchase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.soliditylang.org/en/latest/solidity-by-example.html#micropayment-channel&#34;&gt;Micropayment Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The Solidity documentation is hosted at &lt;a href=&#34;https://docs.soliditylang.org&#34;&gt;Read the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is still under development. Contributions are always welcome! Please follow the &lt;a href=&#34;https://docs.soliditylang.org/en/latest/contributing.html&#34;&gt;Developers Guide&lt;/a&gt; if you want to help.&lt;/p&gt; &#xA;&lt;p&gt;You can find our current feature and bug priorities for forthcoming releases in the &lt;a href=&#34;https://github.com/ethereum/solidity/projects&#34;&gt;projects section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/axic&#34;&gt;@axic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chriseth&#34;&gt;@chriseth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Solidity is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/LICENSE.txt&#34;&gt;GNU General Public License v3.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Some third-party code has its &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/cmake/templates/license.h.in&#34;&gt;own licensing terms&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;The security policy may be &lt;a href=&#34;https://raw.githubusercontent.com/ethereum/solidity/develop/SECURITY.md&#34;&gt;found here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleSpeech</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/PaddlePaddle/PaddleSpeech</id>
    <link href="https://github.com/PaddlePaddle/PaddleSpeech" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use Speech Toolkit including SOTA/Streaming ASR with punctuation, influential TTS with text frontend, Speaker Verification System and End-to-End Speech Simultaneous Translation.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;|English)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/PaddleSpeech_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleSpeech?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;support os&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleSpeech?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleSpeech?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleSpeech?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?color=ccf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleSpeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/=https://pypi.org/project/paddlespeech/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/paddlespeech&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt; Quick Start &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt; Quick Start Server &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt; Quick Start Streaming Server&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#documents&#34;&gt; Documents &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt; Models List &lt;/a&gt; | &lt;a href=&#34;https://aistudio.baidu.com/aistudio/education/group/info/25130&#34;&gt; AIStudio Courses &lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2205.12007&#34;&gt; Paper &lt;/a&gt; | &lt;a href=&#34;https://gitee.com/paddlepaddle/PaddleSpeech&#34;&gt; Gitee &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleSpeech&lt;/strong&gt; is an open-source toolkit on &lt;a href=&#34;https://github.com/PaddlePaddle/Paddle&#34;&gt;PaddlePaddle&lt;/a&gt; platform for a variety of critical tasks in speech and audio, with the state-of-art and influential models.&lt;/p&gt; &#xA;&lt;h5&gt;Speech Recognition&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Recognition Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;I knocked at the door on the ancient side of the building.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/zh.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;ÊàëËÆ§‰∏∫Ë∑ëÊ≠•ÊúÄÈáçË¶ÅÁöÑÂ∞±ÊòØÁªôÊàëÂ∏¶Êù•‰∫ÜË∫´‰ΩìÂÅ•Â∫∑„ÄÇ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Speech Translation (English to Chinese)&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt; Input Audio &lt;/th&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Translations Result &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/PaddleAudio/en.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200 style=&#34; max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;    &lt;td&gt;Êàë Âú® ËøôÊ†ã Âª∫Á≠ë ÁöÑ Âè§ËÄÅ Èó®‰∏ä Êï≤Èó®„ÄÇ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h5&gt;Text-to-Speech&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;550&#34;&gt; Input Text&lt;/th&gt; &#xA;    &lt;th&gt;Synthetic Audio&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Life was like a box of chocolates, you never know what you&#39;re gonna get.&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/tacotron2_ljspeech_waveflow_samples_0.2/sentence_1.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Êó©‰∏äÂ•ΩÔºå‰ªäÂ§©ÊòØ2020/10/29ÔºåÊúÄ‰ΩéÊ∏©Â∫¶ÊòØ-3¬∞C„ÄÇ&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/parakeet_espnet_fs2_pwg_demo/tn_g2p/parakeet/001.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Â≠£Âß¨ÂØÇÔºåÈõÜÈ∏°ÔºåÈ∏°Âç≥Ê£òÈ∏°„ÄÇÊ£òÈ∏°È••ÂèΩÔºåÂ≠£Âß¨ÂèäÁÆïÁ®∑ÊµéÈ∏°„ÄÇÈ∏°Êó¢ÊµéÔºåË∑ªÂß¨Á¨àÔºåÂ≠£Âß¨ÂøåÔºåÊÄ•Âí≠È∏°ÔºåÈ∏°ÊÄ•ÔºåÁªßÂúæÂá†ÔºåÂ≠£Âß¨ÊÄ•ÔºåÂç≥Á±çÁÆïÂáªÈ∏°ÔºåÁÆïÁñæÂáªÂá†‰ºéÔºå‰ºéÂç≥ÈΩëÔºåÈ∏°ÂèΩÈõÜÂá†Âü∫ÔºåÂ≠£Âß¨ÊÄ•ÊûÅÂ±êÂáªÈ∏°ÔºåÈ∏°Êó¢ÊÆõÔºåÂ≠£Âß¨ÊøÄÔºåÂç≥ËÆ∞„ÄäÂ≠£Âß¨ÂáªÈ∏°ËÆ∞„Äã„ÄÇ&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt; &lt;a href=&#34;https://paddlespeech.bj.bcebos.com/Parakeet/docs/demos/jijiji.wav&#34; rel=&#34;nofollow&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/images/audio_icon.png&#34; width=&#34;200&#34; style=&#34;max-width: 100%;&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For more synthesized audios, please refer to &lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;PaddleSpeech Text-to-Speech samples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Punctuation Restoration&lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table style=&#34;width:100%&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Input Text &lt;/th&gt; &#xA;    &lt;th width=&#34;390&#34;&gt; Output Text &lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‰ªäÂ§©ÁöÑÂ§©Ê∞îÁúü‰∏çÈîôÂïä‰Ω†‰∏ãÂçàÊúâÁ©∫ÂêóÊàëÊÉ≥Á∫¶‰Ω†‰∏ÄËµ∑ÂéªÂêÉÈ•≠&lt;/td&gt; &#xA;    &lt;td&gt;‰ªäÂ§©ÁöÑÂ§©Ê∞îÁúü‰∏çÈîôÂïäÔºÅ‰Ω†‰∏ãÂçàÊúâÁ©∫ÂêóÔºüÊàëÊÉ≥Á∫¶‰Ω†‰∏ÄËµ∑ÂéªÂêÉÈ•≠„ÄÇ&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Via the easy-to-use, efficient, flexible and scalable implementation, our vision is to empower both industrial application and academic research, including training, inference &amp;amp; testing modules, and deployment process. To be more specific, this toolkit features at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Ease of Use&lt;/strong&gt;: low barriers to install, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start&#34;&gt;CLI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-server&#34;&gt;Server&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quick-start-streaming-server&#34;&gt;Streaming Server&lt;/a&gt; is available to quick-start your journey.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Align to the State-of-the-Art&lt;/strong&gt;: we provide high-speed and ultra-lightweight models, and also cutting-edge technology.&lt;/li&gt; &#xA; &lt;li&gt;üèÜ &lt;strong&gt;Streaming ASR and TTS System&lt;/strong&gt;: we provide production ready streaming asr and streaming tts system.&lt;/li&gt; &#xA; &lt;li&gt;üíØ &lt;strong&gt;Rule-based Chinese frontend&lt;/strong&gt;: our frontend contains Text Normalization and Grapheme-to-Phoneme (G2P, including Polyphone and Tone Sandhi). Moreover, we use self-defined linguistic rules to adapt Chinese context.&lt;/li&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Varieties of Functions that Vitalize both Industrial and Academia&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üõéÔ∏è &lt;em&gt;Implementation of critical audio tasks&lt;/em&gt;: this toolkit contains audio functions like Automatic Speech Recognition, Text-to-Speech Synthesis, Speaker Verfication, KeyWord Spotting, Audio Classification, and Speech Translation, etc.&lt;/li&gt; &#xA;   &lt;li&gt;üî¨ &lt;em&gt;Integration of mainstream models and datasets&lt;/em&gt;: the toolkit implements modules that participate in the whole pipeline of the speech tasks, and uses mainstream datasets like LibriSpeech, LJSpeech, AIShell, CSMSC, etc. See also &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#model-list&#34;&gt;model list&lt;/a&gt; for more details.&lt;/li&gt; &#xA;   &lt;li&gt;üß© &lt;em&gt;Cascaded models application&lt;/em&gt;: as an extension of the typical traditional audio tasks, we combine the workflows of the aforementioned tasks with other fields like Natural language processing (NLP) and Computer Vision (CV).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recent Update&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üëë 2022.05.13: Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/PPASR.md&#34;&gt;PP-ASR&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/PPTTS.md&#34;&gt;PP-TTS&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/vpr/PPVPR.md&#34;&gt;PP-VPR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.05.06: &lt;code&gt;Streaming ASR&lt;/code&gt; with &lt;code&gt;Punctuation Restoration&lt;/code&gt; and &lt;code&gt;Token Timestamp&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.05.06: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;, and &lt;code&gt;Punctuation Restoration&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.04.28: &lt;code&gt;Streaming Server&lt;/code&gt; is available for &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.03.28: &lt;code&gt;Server&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2022.03.28: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Speaker Verification&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ü§ó 2021.12.14: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS&lt;/a&gt; Demos on Hugging Face Spaces are available!&lt;/li&gt; &#xA; &lt;li&gt;üëèüèª 2021.12.10: &lt;code&gt;CLI&lt;/code&gt; is available for &lt;code&gt;Audio Classification&lt;/code&gt;, &lt;code&gt;Automatic Speech Recognition&lt;/code&gt;, &lt;code&gt;Speech Translation (English to Chinese)&lt;/code&gt; and &lt;code&gt;Text-to-Speech&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scan the QR code below with your Wechat, you can access to official technical exchange group and get the bonus ( more than 20GB learning materials, such as papers, codes and videos ) and the live link of the lessons. Look forward to your participation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/23690325/169763015-cbd8e28d-602c-4723-810d-dbc6da49441e.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We strongly recommend our users to install PaddleSpeech in &lt;strong&gt;Linux&lt;/strong&gt; with &lt;em&gt;python&amp;gt;=3.7&lt;/em&gt;. Up to now, &lt;strong&gt;Linux&lt;/strong&gt; supports CLI for the all our tasks, &lt;strong&gt;Mac OSX&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt; only supports PaddleSpeech CLI for Audio Classification, Speech-to-Text and Text-to-Speech. To install &lt;code&gt;PaddleSpeech&lt;/code&gt;, please see &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our models with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/cli/README.md&#34;&gt;PaddleSpeech Command Line&lt;/a&gt;. Change &lt;code&gt;--input&lt;/code&gt; to test your own audio/text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech cls --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech vector --task spk --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Speech Recognition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech asr --lang zh --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Automatic Speech Recognition is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechASR&#34;&gt;ASR Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Translation&lt;/strong&gt; (English to Chinese) (not support for Mac and Windows now)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech st --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech tts --input &#34;‰Ω†Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®È£ûÊ°®Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºÅ&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;web demo for Text to Speech is integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See Demo: &lt;a href=&#34;https://huggingface.co/spaces/KPatrick/PaddleSpeechTTS&#34;&gt;TTS Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Postprocessing&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Punctuation Restoration &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;paddlespeech text --task punc --input ‰ªäÂ§©ÁöÑÂ§©Ê∞îÁúü‰∏çÈîôÂïä‰Ω†‰∏ãÂçàÊúâÁ©∫ÂêóÊàëÊÉ≥Á∫¶‰Ω†‰∏ÄËµ∑ÂéªÂêÉÈ•≠&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Batch Process&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo -e &#34;1 Ê¨¢ËøéÂÖâ‰∏¥„ÄÇ\n2 Ë∞¢Ë∞¢ÊÉ†È°æ„ÄÇ&#34; | paddlespeech tts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shell Pipeline&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ASR + Punctuation Restoration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech asr --input ./zh.wav | paddlespeech text --task punc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos&#34;&gt;demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to try more functions like training and tuning, please have a look at &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Speech-to-Text Quick Start&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech Quick Start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of our speech server with &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/paddlespeech/server/README.md&#34;&gt;PaddleSpeech Server Command Line&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_server start --config_file ./paddlespeech/server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client asr --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client tts --server_ip 127.0.0.1 --port 8090 --input &#34;ÊÇ®Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®ÁôæÂ∫¶È£ûÊ°®ËØ≠Èü≥ÂêàÊàêÊúçÂä°„ÄÇ&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Audio Classification Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;paddlespeech_client cls --server_ip 127.0.0.1 --port 8090 --input input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about server command lines, please see: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server&#34;&gt;speech server demos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quickstartstreamingserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Streaming Server&lt;/h2&gt; &#xA;&lt;p&gt;Developers can have a try of &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt; server.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Speech Recognition Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_asr_server/conf/application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Speech Recognition Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client asr_online --server_ip 127.0.0.1 --port 8090 --input input_16k.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Start Streaming Text to Speech Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_server start --config_file ./demos/streaming_tts_server/conf/tts_online_application.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access Streaming Text to Speech Services&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paddlespeech_client tts_online --server_ip 127.0.0.1 --port 8092 --protocol http --input &#34;ÊÇ®Â•ΩÔºåÊ¨¢Ëøé‰ΩøÁî®ÁôæÂ∫¶È£ûÊ°®ËØ≠Èü≥ÂêàÊàêÊúçÂä°„ÄÇ&#34; --output output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information please see: &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_asr_server/README.md&#34;&gt;streaming asr&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/streaming_tts_server/README.md&#34;&gt;streaming tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;ModelList&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech supports a series of most popular models. They are summarized in &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;released models&lt;/a&gt; and attached with available pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeechToText&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt; contains &lt;em&gt;Acoustic Model&lt;/em&gt;, &lt;em&gt;Language Model&lt;/em&gt;, and &lt;em&gt;Speech Translation&lt;/em&gt;, with the following details:&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Speech-to-Text Module Type&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Speech Recogination&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Aishell&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeech2 RNN + Conv based Models&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr0&#34;&gt;deepspeech2-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell/asr1&#34;&gt;u2.transformer.conformer-aishell&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Librispeech&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based Attention Models &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr0&#34;&gt;deepspeech2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr1&#34;&gt;transformer.conformer.u2-librispeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/librispeech/asr2&#34;&gt;transformer.conformer.u2-kaldi-librispeech&lt;/a&gt; &lt;/td&gt;  &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TIMIT&lt;/td&gt; &#xA;   &lt;td&gt;Unified Streaming &amp;amp; Non-streaming Two-pass&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/timit/asr1&#34;&gt; u2-timit&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment&lt;/td&gt; &#xA;   &lt;td&gt;THCHS30&lt;/td&gt; &#xA;   &lt;td&gt;MFA&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/.examples/thchs30/align0&#34;&gt;mfa-thchs30&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Language Model&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Ngram Language Model&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ngram_lm&#34;&gt;kenlm&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Speech Translation (English to Chinese)&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;TED En-Zh&lt;/td&gt; &#xA;   &lt;td&gt;Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st0&#34;&gt;transformer-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FAT + Transformer + ASR MTL&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ted_en_zh/st1&#34;&gt;fat-st-ted&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;TextToSpeech&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; in PaddleSpeech mainly contains three modules: &lt;em&gt;Text Frontend&lt;/em&gt;, &lt;em&gt;Acoustic Model&lt;/em&gt; and &lt;em&gt;Vocoder&lt;/em&gt;. Acoustic Model and Vocoder models are listed as follow:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Text-to-Speech Module Type &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; Text Frontend &lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt; ‚ÄÉ &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/tn&#34;&gt;tn&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/g2p&#34;&gt;g2p&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Acoustic Model&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts0&#34;&gt;tacotron2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts0&#34;&gt;tacotron2-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Transformer TTS&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts1&#34;&gt;transformer-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeedySpeech&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts2&#34;&gt;speedyspeech-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/tts3&#34;&gt;fastspeech2-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/tts3&#34;&gt;fastspeech2-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/tts3&#34;&gt;fastspeech2-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/tts3&#34;&gt;fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;6&#34;&gt;Vocoder&lt;/td&gt; &#xA;   &lt;td&gt;WaveFlow&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc0&#34;&gt;waveflow-ljspeech&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parallel WaveGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc1&#34;&gt;PWGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc1&#34;&gt;PWGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc1&#34;&gt;PWGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc1&#34;&gt;PWGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi Band MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc3&#34;&gt;Multi Band MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Style MelGAN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc4&#34;&gt;Style MelGAN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HiFiGAN&lt;/td&gt; &#xA;   &lt;td&gt;LJSpeech / VCTK / CSMSC / AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/ljspeech/voc5&#34;&gt;HiFiGAN-ljspeech&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/vctk/voc5&#34;&gt;HiFiGAN-vctk&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc5&#34;&gt;HiFiGAN-csmsc&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/voc5&#34;&gt;HiFiGAN-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WaveRNN&lt;/td&gt; &#xA;   &lt;td&gt;CSMSC&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/csmsc/voc6&#34;&gt;WaveRNN-csmsc&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Voice Cloning&lt;/td&gt; &#xA;   &lt;td&gt;GE2E&lt;/td&gt; &#xA;   &lt;td&gt;Librispeech, etc.&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/other/ge2e&#34;&gt;ge2e&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + Tacotron2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc0&#34;&gt;ge2e-tacotron2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GE2E + FastSpeech2&lt;/td&gt; &#xA;   &lt;td&gt;AISHELL-3&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/aishell3/vc1&#34;&gt;ge2e-fastspeech2-aishell3&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;AudioClassification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Classification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio Classification&lt;/td&gt; &#xA;   &lt;td&gt;ESC-50&lt;/td&gt; &#xA;   &lt;td&gt;PANN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/esc50/cls0&#34;&gt;pann-esc50&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;SpeakerVerification&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speaker Verification&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;VoxCeleb12&lt;/td&gt; &#xA;   &lt;td&gt;ECAPA-TDNN&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/voxceleb/sv0&#34;&gt;ecapa-tdnn-voxceleb12&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a name=&#34;PunctuationRestoration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Punctuation Restoration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Task &lt;/th&gt; &#xA;   &lt;th&gt; Dataset &lt;/th&gt; &#xA;   &lt;th&gt; Model Type &lt;/th&gt; &#xA;   &lt;th&gt; Example &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Punctuation Restoration&lt;/td&gt; &#xA;   &lt;td&gt;IWLST2012_zh&lt;/td&gt; &#xA;   &lt;td&gt;Ernie Linear&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/examples/iwslt2012/punc0&#34;&gt;iwslt2012-punc0&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;p&gt;Normally, &lt;a href=&#34;https://paperswithcode.com/area/speech&#34;&gt;Speech SoTA&lt;/a&gt;, &lt;a href=&#34;https://paperswithcode.com/area/audio&#34;&gt;Audio SoTA&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/area/music&#34;&gt;Music SoTA&lt;/a&gt; give you an overview of the hot academic topics in the related area. To focus on the tasks in PaddleSpeech, you will find the following guidelines are helpful to grasp the core ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/install.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#quickstart&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/README.md&#34;&gt;Some Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/quick_start.md&#34;&gt;Automatic Speech Recognition&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/data_preparation.md&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/asr/ngram_lm.md&#34;&gt;Ngram LM&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/quick_start.md&#34;&gt;Text-to-Speech&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/models_introduction.md&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/advanced_usage.md&#34;&gt;Advanced Usage&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/tts/zh_text_frontend.md&#34;&gt;Chinese Rule Based Text Frontend&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/tts/demo.html&#34;&gt;Test Audio Samples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Speaker Verification &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_searching/README.md&#34;&gt;Audio Searching&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speaker_verification/README.md&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/audio_tagging/README.md&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_translation/README.md&#34;&gt;Speech Translation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/demos/speech_server/README.md&#34;&gt;Speech Server&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/released_model.md&#34;&gt;Released Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeechToText&#34;&gt;Speech-to-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#TextToSpeech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#AudioClassification&#34;&gt;Audio Classification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#SpeakerVerification&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#PunctuationRestoration&#34;&gt;Punctuation Restoration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#contribution&#34;&gt;Welcome to contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/#License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Text-to-Speech module is originally called &lt;a href=&#34;https://github.com/PaddlePaddle/Parakeet&#34;&gt;Parakeet&lt;/a&gt;, and now merged with this repository. If you are interested in academic research about this task, please see &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/docs/source/tts#overview&#34;&gt;TTS research overview&lt;/a&gt;. Also, &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/raw/develop/docs/source/tts/models_introduction.md&#34;&gt;this document&lt;/a&gt; is a good guideline for the pipeline components.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt;: Use PaddleSpeech TTS to generate virtual human voice.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;a href=&#34;https://www.bilibili.com/video/BV1cL411V71o?share_source=copy_web&#34;&gt;&lt;img src=&#34;https://ai-studio-static-online.cdn.bcebos.com/06fd746ab32042f398fb6f33f873e6869e846fe63c214596ae37860fe8103720&#34; width=&#34;500px&#34;&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://paddlespeech.readthedocs.io/en/latest/demo_video.html&#34;&gt;PaddleSpeech Demo Video&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt;: Use PaddleSpeech TTS and ASR to clone voice from videos.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jerryuhoo/VTuberTalk/main/gui/gui.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;To cite PaddleSpeech for research, please use the following format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{zhang2022paddlespeech,&#xA;    title = {PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit},&#xA;    author = {Hui Zhang, Tian Yuan, Junkun Chen, Xintong Li, Renjie Zheng, Yuxin Huang, Xiaojie Chen, Enlei Gong, Zeyu Chen, Xiaoguang Hu, dianhai yu, Yanjun Ma, Liang Huang},&#xA;    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations},&#xA;    year = {2022},&#xA;    publisher = {Association for Computational Linguistics},&#xA;}&#xA;&#xA;@inproceedings{zheng2021fused,&#xA;  title={Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation},&#xA;  author={Zheng, Renjie and Chen, Junkun and Ma, Mingbo and Huang, Liang},&#xA;  booktitle={International Conference on Machine Learning},&#xA;  pages={12736--12746},&#xA;  year={2021},&#xA;  organization={PMLR}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;contribution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to PaddleSpeech&lt;/h2&gt; &#xA;&lt;p&gt;You are warmly welcome to submit questions in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/discussions&#34;&gt;discussions&lt;/a&gt; and bug reports in &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech/issues&#34;&gt;issues&lt;/a&gt;! Also, we highly appreciate if you are willing to contribute to this project!&lt;/p&gt; &#xA;&lt;h3&gt;Contributors&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/zh794390558&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3038472?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackwaterveg&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/87408988?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yt605155624&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24568452?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kuke&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/3064195?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinghai-sun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7038341?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pkuyym&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5782283?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KPatr1ck&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/22954146?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LittleChenCc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/10339970?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/745165806&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/20623194?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Mingxue-Xu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/92848346?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chrisxu2016&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18379485?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfchener&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6771821?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/luotao1&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6836917?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wanghaoshuang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7534971?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gongel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24390500?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mmglove&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/38800877?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/iclementine&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/16222986?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ZeyuChen&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1371212?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AK391&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81195143?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qingqing01&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7845005?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ericxk&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/4719594?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kvinwang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6442159?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jiqiren11&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/82639260?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AshishKarel&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/58069375?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chesterkuo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6285069?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tensor-tang&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/21351065?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hysunflower&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/52739577?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wwhu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6081200?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lispc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/2833376?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24245709?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/harisankarh&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1307053?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Jackiexiao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18050469?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/limpidezza&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/71760778?v=4&#34; width=&#34;75&#34; height=&#34;75&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/yeyupiaoling&#34;&gt;yeyupiaoling&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PPASR&#34;&gt;PPASR&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/PaddlePaddle-DeepSpeech&#34;&gt;PaddlePaddle-DeepSpeech&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle&#34;&gt;VoiceprintRecognition-PaddlePaddle&lt;/a&gt;/&lt;a href=&#34;https://github.com/yeyupiaoling/AudioClassification-PaddlePaddle&#34;&gt;AudioClassification-PaddlePaddle&lt;/a&gt; for years of attention, constructive advice and great help.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/mymagicpower&#34;&gt;mymagicpower&lt;/a&gt; for the Java implementation of ASR upon &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_sdk&#34;&gt;short&lt;/a&gt; and &lt;a href=&#34;https://github.com/mymagicpower/AIAS/tree/main/3_audio_sdks/asr_long_audio_sdk&#34;&gt;long&lt;/a&gt; audio files.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/JiehangXie&#34;&gt;JiehangXie&lt;/a&gt;/&lt;a href=&#34;https://github.com/JiehangXie/PaddleBoBo&#34;&gt;PaddleBoBo&lt;/a&gt; for developing Virtual Uploader(VUP)/Virtual YouTuber(VTuber) with PaddleSpeech TTS function.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;745165806&lt;/a&gt;/&lt;a href=&#34;https://github.com/745165806/PaddleSpeechTask&#34;&gt;PaddleSpeechTask&lt;/a&gt; for contributing Punctuation Restoration model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/745165806&#34;&gt;kslz&lt;/a&gt; for supplementary Chinese documents.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/awmmmm&#34;&gt;awmmmm&lt;/a&gt; for contributing fastspeech2 aishell3 conformer pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/phecda-xu&#34;&gt;phecda-xu&lt;/a&gt;/&lt;a href=&#34;https://github.com/phecda-xu/PaddleDubbing&#34;&gt;PaddleDubbing&lt;/a&gt; for developing a dubbing tool with GUI based on PaddleSpeech TTS model.&lt;/li&gt; &#xA; &lt;li&gt;Many thanks to &lt;a href=&#34;https://github.com/jerryuhoo&#34;&gt;jerryuhoo&lt;/a&gt;/&lt;a href=&#34;https://github.com/jerryuhoo/VTuberTalk&#34;&gt;VTuberTalk&lt;/a&gt; for developing a GUI tool based on PaddleSpeech TTS and code for making datasets from videos based on PaddleSpeech ASR.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Besides, PaddleSpeech depends on a lot of open source repositories. See &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/docs/source/reference.md&#34;&gt;references&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSpeech is provided under the &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSpeech/develop/LICENSE&#34;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PCSX2/pcsx2</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/PCSX2/pcsx2</id>
    <link href="https://github.com/PCSX2/pcsx2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PCSX2 - The Playstation 2 Emulator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PCSX2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%96%A5%EF%B8%8F%20Windows%20Builds/master?label=Windows%20Builds&#34; alt=&#34;Windows Build Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/PCSX2/pcsx2/%F0%9F%90%A7%20Linux%20Builds/master?label=Linux%20Builds&#34; alt=&#34;Linux Build Status&#34;&gt; &lt;a href=&#34;https://www.codacy.com/gh/PCSX2/pcsx2/dashboard?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=PCSX2/pcsx2&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/1f7c0d75fec74d6daa6adb084e5b4f71&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/TCz3t9k&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/309643527816609793?color=%235CA8FA&amp;amp;label=PCSX2%20Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PCSX2 is a free and open-source PlayStation 2 (PS2) emulator. Its purpose is to emulate the PS2&#39;s hardware, using a combination of MIPS CPU &lt;a href=&#34;https://en.wikipedia.org/wiki/Interpreter_(computing)&#34;&gt;Interpreters&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_recompilation&#34;&gt;Recompilers&lt;/a&gt; and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Virtual_machine&#34;&gt;Virtual Machine&lt;/a&gt; which manages hardware states and PS2 system memory. This allows you to play PS2 games on your PC, with many additional features and benefits.&lt;/p&gt; &#xA;&lt;h2&gt;Project Details&lt;/h2&gt; &#xA;&lt;p&gt;The PCSX2 project has been running for more than ten years. Past versions could only run a few public domain game demos, but newer versions can run most games at full speed, including popular titles such as Final Fantasy X and Devil May Cry 3. Visit the &lt;a href=&#34;https://pcsx2.net/compat/&#34;&gt;PCSX2 compatibility list&lt;/a&gt; to check the latest compatibility status of games (with more than 2500 titles tested), or ask for help in the &lt;a href=&#34;https://forums.pcsx2.net/&#34;&gt;official forums&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The latest officially released stable version is version 1.6.0.&lt;/p&gt; &#xA;&lt;p&gt;Installers and binaries for both stable and development builds are available from &lt;a href=&#34;https://pcsx2.net/downloads/&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Minimum&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 8.1 or newer (64 bit) &lt;br&gt; - Ubuntu 18.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports SSE4.1 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 1600 &lt;br&gt; - Two physical cores, with hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D10 support &lt;br&gt; - OpenGL 3.x support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 3000 (GeForce GTX 750) &lt;br&gt; - 2 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;4 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended Single Thread Performance is based on moderately complex games. Games that pushed the PS2 hardware to its limits will struggle on CPUs at this level. Some release titles and 2D games which underutilized the PS2 hardware may run on CPUs rated as low as 1200. A quick reference for CPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:CPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;, &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-The-Most-CPU-Intensive-Games&#34;&gt;Forum&lt;/a&gt; and CPU &lt;strong&gt;light&lt;/strong&gt; games: &lt;a href=&#34;https://forums.pcsx2.net/Thread-LIST-Games-that-don-t-need-a-strong-CPU-to-emulate&#34;&gt;Forum&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recommended&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Operating System&lt;/th&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;RAM&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;- Windows 10 (64 bit) &lt;br&gt; - Ubuntu 19.04/Debian or newer, Arch Linux, or other distro (64 bit)&lt;/td&gt; &#xA;   &lt;td&gt;- Supports AVX2 &lt;br&gt; - &lt;a href=&#34;https://www.cpubenchmark.net/singleThread.html&#34;&gt;PassMark Single Thread Performance&lt;/a&gt; rating near or greater than 2100 &lt;br&gt; - Four physical cores, with or without hyperthreading&lt;/td&gt; &#xA;   &lt;td&gt;- Direct3D11 support &lt;br&gt; - OpenGL 4.6 support &lt;br&gt; - &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 6000 (GeForce GTX 1050 Ti) &lt;br&gt; - 4 GB Video Memory&lt;/td&gt; &#xA;   &lt;td&gt;8 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Recommended GPU is based on 3x Internal, ~1080p resolution requirements. Higher resolutions will require stronger cards; 6x Internal, ~4K resolution will require a &lt;a href=&#34;https://www.videocardbenchmark.net/high_end_gpus.html&#34;&gt;PassMark G3D Mark&lt;/a&gt; rating around 12000 (GeForce GTX 1070 Ti). Just like CPU requirements, this is also highly game dependent. A quick reference for GPU &lt;strong&gt;intensive games&lt;/strong&gt;: &lt;a href=&#34;https://wiki.pcsx2.net/Category:GPU_intensive_games&#34;&gt;Wiki&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Technical Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need the &lt;a href=&#34;https://support.microsoft.com/en-us/help/2977003/&#34;&gt;Visual C++ 2019 x86 Redistributables&lt;/a&gt; to run PCSX2.&lt;/li&gt; &#xA; &lt;li&gt;Windows XP and Direct3D9 support was dropped after stable release 1.4.0.&lt;/li&gt; &#xA; &lt;li&gt;Windows 7 and Windows 8.0 support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;32 bit support was dropped after stable release 1.6.0.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to update your operating system and drivers to ensure you have the best experience possible. Having a newer GPU is also recommended so you have the latest supported drivers.&lt;/li&gt; &#xA; &lt;li&gt;Because of copyright issues, and the complexity of trying to work around it, you need a BIOS dump extracted from a legitimately-owned PS2 console to use the emulator. For more information about the BIOS and how to get it from your console, visit &lt;a href=&#34;https://raw.githubusercontent.com/PCSX2/pcsx2/master/pcsx2/Docs/PCSX2_FAQ.md#question-13-where-do-i-get-a-ps2-bios&#34;&gt;this page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PCSX2 uses two CPU cores for emulation by default. A third core can be used via the MTVU speed hack, which is compatible with most games. This can be a significant speedup on CPUs with 3+ cores, but it may be a slowdown on GS-limited games (or on CPUs with fewer than 2 cores). Software renderers will then additionally use however many rendering threads it is set to and will need higher core counts to run efficiently.&lt;/li&gt; &#xA; &lt;li&gt;Requirements benchmarks are based on a statistic from the Passmark CPU bench marking software. When we say &#34;STR&#34;, we are referring to Passmark&#39;s &#34;Single Thread Rating&#34; statistic. You can look up your CPU on &lt;a href=&#34;https://cpubenchmark.net&#34;&gt;Passmark&#39;s website for CPUs&lt;/a&gt; to see how it compares to PCSX2&#39;s requirements.&lt;/li&gt; &#xA; &lt;li&gt;Vulkan requires an up-to-date GPU driver; old drivers may cause graphical problems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want more? &lt;a href=&#34;https://pcsx2.net/&#34;&gt;Check out the PCSX2 website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/serving</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/tensorflow/serving</id>
    <link href="https://github.com/tensorflow/serving" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A flexible, high-performance serving system for machine learning models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TensorFlow Serving&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu.svg?sanitize=true&#34; alt=&#34;Ubuntu Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu-tf-head.html&#34;&gt;&lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/ubuntu-tf-head.svg?sanitize=true&#34; alt=&#34;Ubuntu Build Status at TF HEAD&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/docker-cpu-nightly.svg?sanitize=true&#34; alt=&#34;Docker CPU Nightly Build Status&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/tensorflow-serving-kokoro-build-badges-bucket/docker-gpu-nightly.svg?sanitize=true&#34; alt=&#34;Docker GPU Nightly Build Status&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. It deals with the &lt;em&gt;inference&lt;/em&gt; aspect of machine learning, taking models after &lt;em&gt;training&lt;/em&gt; and managing their lifetimes, providing clients with versioned access via a high-performance, reference-counted lookup table. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.&lt;/p&gt; &#xA;&lt;p&gt;To note a few features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can serve multiple models, or multiple versions of the same model simultaneously&lt;/li&gt; &#xA; &lt;li&gt;Exposes both gRPC as well as HTTP inference endpoints&lt;/li&gt; &#xA; &lt;li&gt;Allows deployment of new model versions without changing any client code&lt;/li&gt; &#xA; &lt;li&gt;Supports canarying new versions and A/B testing experimental models&lt;/li&gt; &#xA; &lt;li&gt;Adds minimal latency to inference time due to efficient, low-overhead implementation&lt;/li&gt; &#xA; &lt;li&gt;Features a scheduler that groups individual inference requests into batches for joint execution on GPU, with configurable latency controls&lt;/li&gt; &#xA; &lt;li&gt;Supports many &lt;em&gt;servables&lt;/em&gt;: Tensorflow models, embeddings, vocabularies, feature transformations and even non-Tensorflow-based machine learning models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Serve a Tensorflow model in 60 seconds&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download the TensorFlow Serving Docker image and repo&#xA;docker pull tensorflow/serving&#xA;&#xA;git clone https://github.com/tensorflow/serving&#xA;# Location of demo models&#xA;TESTDATA=&#34;$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata&#34;&#xA;&#xA;# Start TensorFlow Serving container and open the REST API port&#xA;docker run -t --rm -p 8501:8501 \&#xA;    -v &#34;$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two&#34; \&#xA;    -e MODEL_NAME=half_plus_two \&#xA;    tensorflow/serving &amp;amp;&#xA;&#xA;# Query the model using the predict API&#xA;curl -d &#39;{&#34;instances&#34;: [1.0, 2.0, 5.0]}&#39; \&#xA;    -X POST http://localhost:8501/v1/models/half_plus_two:predict&#xA;&#xA;# Returns =&amp;gt; { &#34;predictions&#34;: [2.5, 3.0, 4.5] }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;End-to-End Training &amp;amp; Serving Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the official Tensorflow documentations site for &lt;a href=&#34;https://www.tensorflow.org/tfx/tutorials/serving/rest_simple&#34;&gt;a complete tutorial to train and serve a Tensorflow Model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Set up&lt;/h3&gt; &#xA;&lt;p&gt;The easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/docker.md&#34;&gt;Install Tensorflow Serving using Docker&lt;/a&gt; &lt;em&gt;(Recommended)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/setup.md&#34;&gt;Install Tensorflow Serving without Docker&lt;/a&gt; &lt;em&gt;(Not Recommended)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/building_with_docker.md&#34;&gt;Build Tensorflow Serving from Source with Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/serving_kubernetes.md&#34;&gt;Deploy Tensorflow Serving on Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Use&lt;/h3&gt; &#xA;&lt;h4&gt;Export your Tensorflow model&lt;/h4&gt; &#xA;&lt;p&gt;In order to serve a Tensorflow model, simply export a SavedModel from your Tensorflow program. &lt;a href=&#34;https://github.com/tensorflow/tensorflow/raw/master/tensorflow/python/saved_model/README.md&#34;&gt;SavedModel&lt;/a&gt; is a language-neutral, recoverable, hermetic serialization format that enables higher-level systems and tools to produce, consume, and transform TensorFlow models.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://www.tensorflow.org/guide/saved_model#save_and_restore_models&#34;&gt;Tensorflow documentation&lt;/a&gt; for detailed instructions on how to export SavedModels.&lt;/p&gt; &#xA;&lt;h4&gt;Configure and Use Tensorflow Serving&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/serving_basic.md&#34;&gt;Follow a tutorial on Serving Tensorflow models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/serving_config.md&#34;&gt;Configure Tensorflow Serving to make it fit your serving use case&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/performance.md&#34;&gt;Performance Guide&lt;/a&gt; and learn how to &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/tensorboard.md&#34;&gt;use TensorBoard to profile and optimize inference requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/api_rest.md&#34;&gt;REST API Guide&lt;/a&gt; or &lt;a href=&#34;https://github.com/tensorflow/serving/tree/master/tensorflow_serving/apis&#34;&gt;gRPC API definition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/saved_model_warmup.md&#34;&gt;Use SavedModel Warmup if initial inference requests are slow due to lazy initialization of graph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/signature_defs.md&#34;&gt;If encountering issues regarding model signatures, please read the SignatureDef documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If using a model with custom ops, &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/custom_op.md&#34;&gt;learn how to serve models with custom ops&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Extend&lt;/h3&gt; &#xA;&lt;p&gt;Tensorflow Serving&#39;s architecture is highly modular. You can use some parts individually (e.g. batch scheduling) and/or extend it to serve new use cases.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/building_with_docker.md&#34;&gt;Ensure you are familiar with building Tensorflow Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/architecture.md&#34;&gt;Learn about Tensorflow Serving&#39;s architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tfx/serving/api_docs/cc/&#34;&gt;Explore the Tensorflow Serving C++ API reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/custom_servable.md&#34;&gt;Create a new type of Servable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/g3doc/custom_source.md&#34;&gt;Create a custom Source of Servable versions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you&#39;d like to contribute to TensorFlow Serving, be sure to review the &lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/serving/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;For more information&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the official &lt;a href=&#34;http://tensorflow.org&#34;&gt;TensorFlow website&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wenet-e2e/wenet</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/wenet-e2e/wenet</id>
    <link href="https://github.com/wenet-e2e/wenet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Production First and Production Ready End-to-End Speech Recognition Toolkit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WeNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/raw/main/README_CN.md&#34;&gt;&lt;strong&gt;‰∏≠ÊñáÁâà&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7%7C3.8-brightgreen&#34; alt=&#34;Python-Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/discussions&#34;&gt;&lt;strong&gt;Discussions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/papers.html&#34;&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/server/x86&#34;&gt;&lt;strong&gt;Runtime (x86)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet&#34;&gt;&lt;strong&gt;Runtime (android)&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/pretrained_models.md&#34;&gt;&lt;strong&gt;Pretrained Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We&lt;/strong&gt; share neural &lt;strong&gt;Net&lt;/strong&gt; together.&lt;/p&gt; &#xA;&lt;p&gt;The main motivation of WeNet is to close the gap between research and production end-to-end (E2E) speech recognition models, to reduce the effort of productionizing E2E models, and to explore better E2E models for production.&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Production first and production ready&lt;/strong&gt;: The core design principle of WeNet. WeNet provides full stack solutions for speech recognition.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;Unified solution for streaming and non-streaming ASR&lt;/em&gt;: &lt;a href=&#34;https://arxiv.org/pdf/2012.05481.pdf&#34;&gt;U2 framework&lt;/a&gt;--develop, train, and deploy only once.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Runtime solution&lt;/em&gt;: built-in server &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/server/x86&#34;&gt;x86&lt;/a&gt; and on-device &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet&#34;&gt;android&lt;/a&gt; runtime solution.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Model exporting solution&lt;/em&gt;: built-in solution to export model to LibTorch/ONNX for inference.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;LM solution&lt;/em&gt;: built-in production-level &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/lm.md&#34;&gt;LM solution&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Other production solutions&lt;/em&gt;: built-in contextual biasing, time stamp, endpoint, and n-best solutions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Accurate&lt;/strong&gt;: WeNet achieves SOTA results on a lot of public speech datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Light weight&lt;/strong&gt;: WeNet is easy to install, easy to use, well designed, and well documented.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;code&gt;examples/$dataset/s0/README.md&lt;/code&gt; for benchmark on different speech datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Installation(Python Only)&lt;/h2&gt; &#xA;&lt;p&gt;If you just want to use WeNet as a python package for speech recognition application, just install it by &lt;code&gt;pip&lt;/code&gt;, please note python 3.6+ is required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install wenet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And please see &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/runtime/binding/python/README.md&#34;&gt;doc&lt;/a&gt; for usage.&lt;/p&gt; &#xA;&lt;h2&gt;Installation(Training and Developing)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/wenet-e2e/wenet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Conda: please see &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create Conda env:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n wenet python=3.8&#xA;conda activate wenet&#xA;pip install -r requirements.txt&#xA;conda install pytorch=1.10.0 torchvision torchaudio=0.10.0 cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optionally, if you want to use x86 runtime or language model(LM), you have to build the runtime as follows. Otherwise, you can just ignore this step.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# runtime build requires cmake 3.14 or above&#xA;cd runtime/server/x86&#xA;mkdir build &amp;amp;&amp;amp; cd build &amp;amp;&amp;amp; cmake .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; &#xA;&lt;p&gt;Please visit &lt;a href=&#34;https://github.com/wenet-e2e/wenet/discussions&#34;&gt;Discussions&lt;/a&gt; for further discussion.&lt;/p&gt; &#xA;&lt;p&gt;For Chinese users, you can aslo scan the QR code on the left to follow our offical account of WeNet. We created a WeChat group for better discussion and quicker response. Please scan the personal QR code on the right, and the guy is responsible for inviting you to the chat group.&lt;/p&gt; &#xA;&lt;p&gt;If you can not access the QR image, please access it on &lt;a href=&#34;https://gitee.com/robin1001/qr/tree/master&#34;&gt;gitee&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/wenet.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/binbin.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Or you can directly discuss on &lt;a href=&#34;https://github.com/wenet-e2e/wenet/issues&#34;&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.chumenwenwen.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/chumenwenwen.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://lxie.npu-aslp.org&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/colleges/nwpu.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://www.aishelltech.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/aishelltech.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;http://www.ximalaya.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/ximalaya.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://www.jd.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/jd.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://horizon.ai&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/hobot.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://thuhcsi.github.io&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/colleges/thu.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nvidia.com/en-us&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet-contributors/main/companies/nvidia.png&#34; width=&#34;250px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPnet&lt;/a&gt; for transformer based modeling.&lt;/li&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;Kaldi&lt;/a&gt; for WFST based decoding for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred &lt;a href=&#34;https://github.com/srvk/eesen&#34;&gt;EESEN&lt;/a&gt; for building TLG based graph for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred to &lt;a href=&#34;https://github.com/ZhengkunTian/OpenTransformer/&#34;&gt;OpenTransformer&lt;/a&gt; for python batch inference of e2e models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{yao2021wenet,&#xA;  title={WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit},&#xA;  author={Yao, Zhuoyuan and Wu, Di and Wang, Xiong and Zhang, Binbin and Yu, Fan and Yang, Chao and Peng, Zhendong and Chen, Xiaoyu and Xie, Lei and Lei, Xin},&#xA;  booktitle={Proc. Interspeech},&#xA;  year={2021},&#xA;  address={Brno, Czech Republic }&#xA;  organization={IEEE}&#xA;}&#xA;&#xA;@article{zhang2022wenet,&#xA;  title={WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit},&#xA;  author={Zhang, Binbin and Wu, Di and Peng, Zhendong and Song, Xingchen and Yao, Zhuoyuan and Lv, Hang and Xie, Lei and Yang, Chao and Pan, Fuping and Niu, Jianwei},&#xA;  journal={arXiv preprint arXiv:2203.15455},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>sam-astro/Z-Sharp</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/sam-astro/Z-Sharp</id>
    <link href="https://github.com/sam-astro/Z-Sharp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Custom programming interpreter for ZSharp (Z#), a language I made up.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sam-astro/Z-Sharp/master/ExtraResources/ZS-Gem-Icon-Small.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sam-astro/Z-Sharp/master/ExtraResources/ZS-Logo-Light-Small.png&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Z-Sharp is no longer in development! This project was never meant to go beyond the scope of a simple thing I could make pong in, yet people continue to ask for features and fixes, and I continue to oblige. So sadly, even though this was a cool project in which I learned a lot, it will be ending now. I will eventually make some docs and standards for the syntax, and will still leave this repository open. This way anybody can make their own interpreter or compiler for it. I will also still accept pull requests for any changes to this repository.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Z-Sharp is a custom programming language I made because I don&#39;t like c++ very much (Z-Sharp&#39;s interpreter is written in c++ though). Z-Sharp scripts have the file extension .ZS. The base syntax and formatting I would say is quite similar to C# or Python, but differs as task complexity increases. It also has support for graphics using SDL2.&lt;/p&gt; &#xA;&lt;p&gt;Before using Z#: There is &lt;em&gt;&lt;strong&gt;no documentation&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;strings&lt;/strong&gt;&lt;/em&gt; barely work, &lt;em&gt;&lt;strong&gt;performance&lt;/strong&gt;&lt;/em&gt; isn&#39;t great, the syntax is &lt;em&gt;&lt;strong&gt;very specific&lt;/strong&gt;&lt;/em&gt;, and most errors just cause it to &lt;em&gt;&lt;strong&gt;crash without warning&lt;/strong&gt;&lt;/em&gt;. I am just a &lt;em&gt;single developer&lt;/em&gt; working on this during my free time; between school, other projects, and YouTube. Z-Sharp will most likely never be finished, since it was really supposed to end when the video was published about it. If you are trying to use a common programming language feature, ask yourself this: &lt;em&gt;&lt;strong&gt;Is this feature required to play pong?&lt;/strong&gt;&lt;/em&gt; If not, then most likely that feature &lt;em&gt;&lt;strong&gt;has not been implemented yet&lt;/strong&gt;&lt;/em&gt;. I initially only made the language so I could create pong and make a video about it, so it really is the &lt;em&gt;&lt;strong&gt;bare minimum&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and getting started:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stevenrafft.github.io/ZSharpDocs/#/README&#34;&gt;The docs and tutorial&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Downloading or installing is very simple, here is how depending on your version and operating system:&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to &lt;a href=&#34;https://github.com/sam-astro/Z-Sharp/releases&#34;&gt;the most recent release&lt;/a&gt; and download &lt;code&gt;ZSharp-Win-Installer.zip&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Unzip &lt;code&gt;ZSharp-Win-Installer.zip&lt;/code&gt; and open the unzipped folder.&lt;/li&gt; &#xA; &lt;li&gt;Inside is a single file titled &lt;code&gt;ZSharp-Setup.exe&lt;/code&gt;. Run it, and follow the setup instructions.&lt;/li&gt; &#xA; &lt;li&gt;If it fails to run, make sure the &lt;code&gt;MS Visual Runtime and MSVC C++ Redistribute&lt;/code&gt; are installed. You can download them &lt;a href=&#34;https://docs.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist&#34;&gt;here from Microsoft&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Now that it is installed, there are a few ways to use it: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(recommended) Any ZSharp file that ends with .ZS will automatically be associated with the interpreter. Just double-click it, and the interpreter will run.&lt;/li&gt; &#xA;   &lt;li&gt;Drag and drop any .ZS script directly onto the executable.&lt;/li&gt; &#xA;   &lt;li&gt;Use command line, providing path to interpreter and then to script like so: &lt;code&gt;&amp;gt; ./ZSharp.exe ./Pong-Example-Project/script.zs&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Feel free to use and edit the &lt;code&gt;Pong-Example-Project&lt;/code&gt;. It is a single script called &lt;code&gt;script.zs&lt;/code&gt;, and you can open it with any of the methods above. It is also located on the releases page.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you don&#39;t want to install ZSharp on your device, or you want easier acces to the executable and .DLLs, another version is provided called &lt;code&gt;ZS_Win_Base_Raw.zip&lt;/code&gt;. This just contains all of the files the installer puts on your computer.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install requirements:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You need the package &lt;em&gt;&lt;strong&gt;libsdl2-dev&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For Debian based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo apt install libsdl2-dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Arch based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo pacman -S sdl2&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You also need the package &lt;em&gt;&lt;strong&gt;libsdl2-image-dev&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For Debian based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo apt install libsdl2-image-dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Arch based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo pacman -S sdl2_image&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You also need the package &lt;em&gt;&lt;strong&gt;libsdl2-ttf-dev&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For Debian based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo apt install libsdl2-ttf-dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For Arch based operating systems you can install it with&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ sudo pacman -S sdl2_ttf&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to &lt;a href=&#34;https://github.com/sam-astro/Z-Sharp/releases&#34;&gt;the most recent release&lt;/a&gt; and download &lt;code&gt;ZSharp-Linux.zip&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Unzip &lt;code&gt;ZSharp-Linux.zip&lt;/code&gt; and open the unzipped folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You will see some files. The Z# interpreter is &lt;code&gt;ZSharp&lt;/code&gt;. Any time you want to execute a script, this is the program that will be used. You can use it like so:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use terminal, providing path to executable and then to script like so: &lt;code&gt;$ ./ZSharp ./Pong-Example-Project/script.zs&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Feel free to use and edit the included &lt;code&gt;Pong-Example-Project&lt;/code&gt;. It is a single script called &lt;code&gt;script.zs&lt;/code&gt;, and you can open it with any of the methods above.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Here is some example code:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;// Comments are indicated by two forward slashes&#xA;// They can only be on their own line&#xA;//    int j = 4 // &amp;lt;- This is invalid comment placement&#xA;&#xA;// All programs start with a main function&#xA;func Main()&#xA;{&#xA;    int i = 0&#xA;    string s = &#34;r&#34;&#xA;    &#xA;    i += 2&#xA;    i -= 1&#xA;    i /= 3&#xA;    i *= 2&#xA;    &#xA;    while i &amp;lt; 10&#xA;    {&#xA;        i += 1&#xA;    }&#xA;    &#xA;    if s == &#34;r&#34;&#xA;    {&#xA;        Printl(s + &#34; is r&#34;)&#xA;    }&#xA;    &#xA;    int functionNumber = ExampleFunction(&#34;A&#34;, s)&#xA;    ExampleFunction(1, 3)&#xA;    &#xA;    GlobalFunction()&#xA;}&#xA;&#xA;// Declare new function with &#39;func&#39;, then it&#39;s name, and the names of any input variables.&#xA;// The input variables don&#39;t need type, as those are automatic. Also, they don&#39;t need to&#xA;/// be assigned at all on execute and can be left blank&#xA;func ExampleFunction(inputA, inputB)&#xA;{&#xA;    Printl(&#34;In A is: &#34; + inputA)&#xA;    Printl(&#34;In B is: &#34; + inputB)&#xA;    &#xA;    // Return a value to the valling location&#xA;    return 4&#xA;}&#xA;&#xA;func GlobalFunction()&#xA;{&#xA;    // Create variables that can be accessed from anywhere (ex. in Main or ExampleFunction) with the &#39;global&#39; keyword before type&#xA;    global int x = 12&#xA;    global string y = &#34;Y String&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is how to use graphics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;func Main()&#xA;{&#xA;    int screenWidth = 500&#xA;    int screenHeight = 500&#xA;    ZS.Graphics.Init(&#34;Title of window&#34;, screenWidth, screenHeight)&#xA;    // After graphics are initialized, the main function will not finish.&#xA;    // Instead, Start() will be called a single time, then Update() every frame after that.&#xA;}&#xA;&#xA;// Runs once at start of graphics initialization&#xA;func Start()&#xA;{&#xA;    // Vec2 are initialized using function &#39;NVec2(x, y)&#39;&#xA;    Vec2 position = NVec2(250, 250)&#xA;    Vec2 scale = NVec2(20, 20)&#xA;    float rotation = 0&#xA;&#xA;    // Sprite object, stores (and loads from file) the texture, location, scale, and rotation&#xA;    global Sprite exampleSprite = ZS.Graphics.Sprite(&#34;./square.png&#34;, position, scale, rotation)&#xA;}&#xA;&#xA;// Executes each frame&#xA;func Update(deltaTime)&#xA;{&#xA;    // Draws the image created in Start(). This is usually at the end of update.&#xA;    ZS.Graphics.Draw(exampleSprite)   &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, ZSharp is &lt;em&gt;&lt;strong&gt;VERY&lt;/strong&gt;&lt;/em&gt; strict with formatting, and can throw an error if you forget to put a space somewhere. Also, speaking of errors, if your code has any it will show in the console. Errors are colored red, and warnings are colored yellow. A line number will also usually be provided. This is &lt;em&gt;&lt;strong&gt;Not&lt;/strong&gt;&lt;/em&gt; the line relative to the &lt;em&gt;documents&lt;/em&gt; beginning, but rather the &lt;em&gt;functions&lt;/em&gt; beginning. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ERROR: line 5 in function Main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is the 5th line &lt;em&gt;inside of Main&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;func Main()&#xA;{&#xA;   // line 1&#xA;   // line 2&#xA;   // line 3&#xA;   // line 4&#xA;   int g = &#34;s&#34;&#xA;   // ^ above line is the error, since it is line 5&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I am planning to change how error reporting works to report the document line number as well, but this is how it is for now.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/pytorch</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/pytorch/pytorch</id>
    <link href="https://github.com/pytorch/pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/pytorch-logo-dark.png&#34; alt=&#34;PyTorch Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt; &#xA; &lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.&lt;/p&gt; &#xA;&lt;p&gt;Our trunk health (Continuous Integration signals) can be found at &lt;a href=&#34;https://hud.pytorch.org/ci/pytorch/pytorch/master&#34;&gt;hud.pytorch.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#more-about-pytorch&#34;&gt;More About PyTorch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#a-gpu-ready-tensor-library&#34;&gt;A GPU-Ready Tensor Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#dynamic-neural-networks-tape-based-autograd&#34;&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#python-first&#34;&gt;Python First&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#imperative-experiences&#34;&gt;Imperative Experiences&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#fast-and-lean&#34;&gt;Fast and Lean&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#extensions-without-pain&#34;&gt;Extensions Without Pain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#binaries&#34;&gt;Binaries&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#nvidia-jetson-platforms&#34;&gt;NVIDIA Jetson Platforms&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#from-source&#34;&gt;From Source&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-dependencies&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#get-the-pytorch-source&#34;&gt;Get the PyTorch Source&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#install-pytorch&#34;&gt;Install PyTorch&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#adjust-build-options-optional&#34;&gt;Adjust Build Options (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#docker-image&#34;&gt;Docker Image&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#using-pre-built-images&#34;&gt;Using pre-built images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-image-yourself&#34;&gt;Building the image yourself&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#building-the-documentation&#34;&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#previous-versions&#34;&gt;Previous Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#releases-and-contributing&#34;&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#the-team&#34;&gt;The Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/torch.html&#34;&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a Tensor library like NumPy, with strong GPU support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/autograd.html&#34;&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/jit.html&#34;&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/nn.html&#34;&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;a neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/multiprocessing.html&#34;&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/data.html&#34;&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Usually, PyTorch is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A replacement for NumPy to use the power of GPUs.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;A GPU-Ready Tensor Library&lt;/h3&gt; &#xA;&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/tensor_illustration.png&#34; alt=&#34;Tensor illustration&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.&lt;/p&gt; &#xA;&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!&lt;/p&gt; &#xA;&lt;h3&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt; &#xA;&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt; &#xA;&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as &lt;a href=&#34;https://github.com/twitter/torch-autograd&#34;&gt;torch-autograd&lt;/a&gt;, &lt;a href=&#34;https://github.com/HIPS/autograd&#34;&gt;autograd&lt;/a&gt;, &lt;a href=&#34;https://chainer.org&#34;&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt; &#xA;&lt;p&gt;While this technique is not unique to PyTorch, it&#39;s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif&#34; alt=&#34;Dynamic graph&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python First&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; / &lt;a href=&#34;https://www.scipy.org/&#34;&gt;SciPy&lt;/a&gt; / &lt;a href=&#34;https://scikit-learn.org&#34;&gt;scikit-learn&lt;/a&gt; etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt; and &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;. Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt; &#xA;&lt;h3&gt;Imperative Experiences&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn&#39;t an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Lean&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries such as &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel MKL&lt;/a&gt; and NVIDIA (&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/nccl&#34;&gt;NCCL&lt;/a&gt;) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.&lt;/p&gt; &#xA;&lt;p&gt;Hence, PyTorch is quite fast ‚Äì whether you run small or large neural networks.&lt;/p&gt; &#xA;&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We&#39;ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions Without Pain&lt;/h3&gt; &#xA;&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch&#39;s Tensor API was designed to be straightforward and with minimal abstractions.&lt;/p&gt; &#xA;&lt;p&gt;You can write new neural network layers in Python using the torch API &lt;a href=&#34;https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html&#34;&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see &lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_extension.html&#34;&gt;a tutorial here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/extension-cpp&#34;&gt;an example here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;p&gt;Commands to install binaries via Conda or pip wheels are on our website: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;NVIDIA Jetson Platforms&lt;/h4&gt; &#xA;&lt;p&gt;Python wheels for NVIDIA&#39;s Jetson Nano, Jetson TX2, and Jetson AGX Xavier are provided &lt;a href=&#34;https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048&#34;&gt;here&lt;/a&gt; and the L4T container is published &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;They require JetPack 4.2 and above, and &lt;a href=&#34;https://github.com/dusty-nv&#34;&gt;@dusty-nv&lt;/a&gt; and &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;@ptrblck&lt;/a&gt; are maintaining them.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;If you are installing from source, you will need Python 3.7 or later and a C++14 compiler. Also, we highly recommend installing an &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt; &#xA;&lt;p&gt;Once you have &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt; installed, here are the instructions.&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with CUDA support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;NVIDIA CUDA&lt;/a&gt; 10.2 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;NVIDIA cuDNN&lt;/a&gt; v7 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/ax3l/9489132&#34;&gt;Compiler&lt;/a&gt; compatible with CUDA Note: You could refer to the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf&#34;&gt;cuDNN Support Matrix&lt;/a&gt; for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardwares&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable CUDA support, export the environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are building for NVIDIA&#39;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are &lt;a href=&#34;https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/&#34;&gt;available here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to compile with ROCm support, install&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html&#34;&gt;AMD ROCm&lt;/a&gt; 4.0 and above installation&lt;/li&gt; &#xA; &lt;li&gt;ROCm is currently supported only for Linux systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to disable ROCm support, export the environment variable &lt;code&gt;USE_ROCM=0&lt;/code&gt;. Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Install Dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Common&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install astunparse numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# CUDA only: Add LAPACK support for the GPU if needed&#xA;conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed&#xA;conda install pkg-config libuv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add these packages if torch.distributed is needed.&#xA;# Distributed package support on Windows is a prototype feature and is subject to changes.&#xA;conda install -c conda-forge libuv=1.39&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Get the PyTorch Source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/pytorch/pytorch&#xA;cd pytorch&#xA;# if you are updating an existing checkout&#xA;git submodule sync&#xA;git submodule update --init --recursive --jobs 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install PyTorch&lt;/h4&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are compiling for ROCm, you must run this command first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/amd_build/build_amd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you are using &lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Anaconda&lt;/a&gt;, you may experience an error caused by the linker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized&#xA;collect2: error: ld returned 1 exit status&#xA;error: command &#39;g++&#39; failed with exit status 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is caused by &lt;code&gt;ld&lt;/code&gt; from Conda environment shadowing the system &lt;code&gt;ld&lt;/code&gt;. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.7.6+ and 3.8.1+.&lt;/p&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA is not supported on macOS.&lt;/p&gt; &#xA;&lt;p&gt;On Windows&lt;/p&gt; &#xA;&lt;p&gt;Choose Correct Visual Studio Version.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes there are regressions in new versions of Visual Studio, so it&#39;s best to use the same Visual Studio Version &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.circleci/scripts/vs_install.ps1&#34;&gt;16.8.5&lt;/a&gt; as Pytorch CI&#39;s.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from &lt;a href=&#34;https://visualstudio.microsoft.com/visual-cpp-build-tools/&#34;&gt;https://visualstudio.microsoft.com/visual-cpp-build-tools/&lt;/a&gt;. The build tools &lt;em&gt;do not&lt;/em&gt; come with Visual Studio Code by default.&lt;/p&gt; &#xA;&lt;p&gt;If you want to build legacy python code, please refer to &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/CONTRIBUTING.md#building-on-legacy-code-and-cuda&#34;&gt;Building on legacy code and CUDA&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build with CPU&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s fairly easy to build with CPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;conda activate&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#39;ll need to manually download the library and set up the building environment by tweaking &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;LIB&lt;/code&gt;. The instruction &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/docs/source/notes/windows.rst#building-from-source&#34;&gt;here&lt;/a&gt; is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.&lt;/p&gt; &#xA;&lt;p&gt;Build with CUDA&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm&#34;&gt;NVTX&lt;/a&gt; is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called &#34;Nsight Compute&#34;. To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio.&lt;/p&gt; &#xA;&lt;p&gt;Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. &lt;br&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.&lt;/p&gt; &#xA;&lt;p&gt;Additional libraries such as &lt;a href=&#34;https://developer.nvidia.com/magma&#34;&gt;Magma&lt;/a&gt;, &lt;a href=&#34;https://github.com/oneapi-src/oneDNN&#34;&gt;oneDNN, a.k.a MKLDNN or DNNL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/mozilla/sccache&#34;&gt;Sccache&lt;/a&gt; are often needed. Please refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/tree/master/.jenkins/pytorch/win-test-helpers/installation-helpers&#34;&gt;installation-helper&lt;/a&gt; to install them.&lt;/p&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat&#34;&gt;build_pytorch.bat&lt;/a&gt; script for some other environment variables configurations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;cmd&#xA;&#xA;:: Set the environment variables after you have downloaded and upzipped the mkl package,&#xA;:: else CMake would throw an error as `Could NOT find OpenMP`.&#xA;set CMAKE_INCLUDE_PATH={Your directory}\mkl\include&#xA;set LIB={Your directory}\mkl\lib;%LIB%&#xA;&#xA;:: Read the content in the previous section carefully before you proceed.&#xA;:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&#xA;:: &#34;Visual Studio 2019 Developer Command Prompt&#34; will be run automatically.&#xA;:: Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&#xA;set CMAKE_GENERATOR_TOOLSET_VERSION=14.27&#xA;set DISTUTILS_USE_SDK=1&#xA;for /f &#34;usebackq tokens=*&#34; %i in (`&#34;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&#34; -version [15^,17^) -products * -latest -property installationPath`) do call &#34;%i\VC\Auxiliary\Build\vcvarsall.bat&#34; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%&#xA;&#xA;:: [Optional] If you want to override the CUDA host compiler&#xA;set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe&#xA;&#xA;python setup.py install&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Adjust Build Options (Optional)&lt;/h5&gt; &#xA;&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step.&lt;/p&gt; &#xA;&lt;p&gt;On Linux&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On macOS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-&#34;$(dirname $(which conda))/../&#34;}&#xA;MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only&#xA;ccmake build  # or cmake-gui build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;h4&gt;Using pre-built images&lt;/h4&gt; &#xA;&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building the image yourself&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -f docker.Makefile&#xA;# images are tagged as docker.io/${your_docker_username}/pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building the Documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build documentation in various formats, you will need &lt;a href=&#34;http://www.sphinx-doc.org&#34;&gt;Sphinx&lt;/a&gt; and the readthedocs theme.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docs/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the &lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt; &#xA;&lt;p&gt;If you get a katex error run &lt;code&gt;npm install katex&lt;/code&gt;. If it persists, try &lt;code&gt;npm install -g katex&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Previous Versions&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found on &lt;a href=&#34;https://pytorch.org/previous-versions&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Three-pointers to get you started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;Examples: easy to understand PyTorch code across all domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/&#34;&gt;The API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch/raw/master/GLOSSARY.md&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/&#34;&gt;PyTorch Tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/examples&#34;&gt;PyTorch Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;PyTorch Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/deep-learning-pytorch--ud188&#34;&gt;Intro to Deep Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229&#34;&gt;Intro to Machine Learning with PyTorch from Udacity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-networks-with-pytorch&#34;&gt;Deep Neural Networks with PyTorch from Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/PyTorch&#34;&gt;PyTorch Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/blog/&#34;&gt;PyTorch Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw&#34;&gt;PyTorch YouTube&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Forums: Discuss implementations, research, etc. &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt; &#xA; &lt;li&gt;Slack: The &lt;a href=&#34;https://pytorch.slack.com/&#34;&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is &lt;a href=&#34;https://discuss.pytorch.org&#34;&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href=&#34;https://goo.gl/forms/PP1AGvNHpSaJP8to1&#34;&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: &lt;a href=&#34;https://eepurl.com/cbG0rv&#34;&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Facebook Page: Important announcements about PyTorch. &lt;a href=&#34;https://www.facebook.com/pytorch&#34;&gt;https://www.facebook.com/pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For brand guidelines, please visit our website at &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch.org&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases and Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a 90-day release cycle (major releases). Please let us know if you encounter a bug by &lt;a href=&#34;https://github.com/pytorch/pytorch/issues&#34;&gt;filing an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt; &#xA;&lt;p&gt;If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt; &#xA;&lt;p&gt;To learn more about making a contribution to Pytorch, please see our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt; &#xA;&lt;p&gt;PyTorch is currently maintained by &lt;a href=&#34;https://apaszke.github.io/&#34;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&#34;https://github.com/colesbury&#34;&gt;Sam Gross&lt;/a&gt;, &lt;a href=&#34;http://soumith.ch&#34;&gt;Soumith Chintala&lt;/a&gt; and &lt;a href=&#34;https://github.com/gchanan&#34;&gt;Gregory Chanan&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project is unrelated to &lt;a href=&#34;https://github.com/hughperkins/pytorch&#34;&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PyTorch has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/pytorch/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebook/rocksdb</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/facebook/rocksdb</id>
    <link href="https://github.com/facebook/rocksdb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library that provides an embeddable, persistent key-value store for fast storage.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;RocksDB: A Persistent Key-Value Store for Flash and RAM Storage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebook/rocksdb.svg?style=svg&#34; alt=&#34;CircleCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/github/facebook/rocksdb&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/facebook/rocksdb.svg?branch=main&#34; alt=&#34;TravisCI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/Facebook/rocksdb/branch/main&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/fbgfu0so3afcno78/branch/main?svg=true&#34; alt=&#34;Appveyor Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://140-211-168-68-openstack.osuosl.org:8080/job/rocksdb&#34;&gt;&lt;img src=&#34;http://140-211-168-68-openstack.osuosl.org:8080/buildStatus/icon?job=rocksdb&amp;amp;style=plastic&#34; alt=&#34;PPC64le Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RocksDB is developed and maintained by Facebook Database Engineering Team. It is built on earlier work on &lt;a href=&#34;https://github.com/google/leveldb&#34;&gt;LevelDB&lt;/a&gt; by Sanjay Ghemawat (&lt;a href=&#34;mailto:sanjay@google.com&#34;&gt;sanjay@google.com&lt;/a&gt;) and Jeff Dean (&lt;a href=&#34;mailto:jeff@google.com&#34;&gt;jeff@google.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;This code is a library that forms the core building block for a fast key-value server, especially suited for storing data on flash drives. It has a Log-Structured-Merge-Database (LSM) design with flexible tradeoffs between Write-Amplification-Factor (WAF), Read-Amplification-Factor (RAF) and Space-Amplification-Factor (SAF). It has multi-threaded compactions, making it especially suitable for storing multiple terabytes of data in a single database.&lt;/p&gt; &#xA;&lt;p&gt;Start with example usage here: &lt;a href=&#34;https://github.com/facebook/rocksdb/tree/main/examples&#34;&gt;https://github.com/facebook/rocksdb/tree/main/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebook/rocksdb/wiki&#34;&gt;github wiki&lt;/a&gt; for more explanation.&lt;/p&gt; &#xA;&lt;p&gt;The public interface is in &lt;code&gt;include/&lt;/code&gt;. Callers should not include or rely on the details of any other header files in this package. Those internal APIs may be changed without warning.&lt;/p&gt; &#xA;&lt;p&gt;Questions and discussions are welcome on the &lt;a href=&#34;https://www.facebook.com/groups/rocksdb.dev/&#34;&gt;RocksDB Developers Public&lt;/a&gt; Facebook group and &lt;a href=&#34;https://groups.google.com/g/rocksdb&#34;&gt;email list&lt;/a&gt; on Google Groups.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;RocksDB is dual-licensed under both the GPLv2 (found in the COPYING file in the root directory) and Apache 2.0 License (found in the LICENSE.Apache file in the root directory). You may select, at your option, one of the above-listed licenses.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mamedev/mame</title>
    <updated>2022-06-01T01:59:26Z</updated>
    <id>tag:github.com,2022-06-01:/mamedev/mame</id>
    <link href="https://github.com/mamedev/mame" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MAME&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;MAME&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/mamedev/mame?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/mamedev/mame&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Build status:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;OS/Compiler&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux/GCC and clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Linux)/badge.svg?sanitize=true&#34; alt=&#34;CI (Linux)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows/MinGW GCC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(Windows)/badge.svg?sanitize=true&#34; alt=&#34;CI (Windows)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;macOS/clang&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/CI%20(macOS)/badge.svg?sanitize=true&#34; alt=&#34;CI (macOS)&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UI Translations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Compile%20UI%20translations/badge.svg?sanitize=true&#34; alt=&#34;Compile UI translations&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/mamedev/mame/workflows/Build%20documentation/badge.svg?sanitize=true&#34; alt=&#34;Build documentation&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Static analysis status for entire build (except for third-party parts of project):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scan.coverity.com/projects/mame-emulator&#34;&gt;&lt;img src=&#34;https://scan.coverity.com/projects/5727/badge.svg?flat=1&#34; alt=&#34;Coverity Scan Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is MAME?&lt;/h1&gt; &#xA;&lt;p&gt;MAME is a multi-purpose emulation framework.&lt;/p&gt; &#xA;&lt;p&gt;MAME&#39;s purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important &#34;vintage&#34; software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.&lt;/p&gt; &#xA;&lt;h1&gt;How to compile?&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re on a UNIX-like system (including Linux and macOS), it could be as easy as typing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MAME build,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=arcade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for an arcade-only build, or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make SUBTARGET=mess&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a MESS build.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;http://docs.mamedev.org/initialsetup/compilingmame.html&#34;&gt;Compiling MAME&lt;/a&gt; page on our documentation site for more information, including prerequisites for macOS and popular Linux distributions.&lt;/p&gt; &#xA;&lt;p&gt;For recent versions of macOS you need to install &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; including command-line tools and &lt;a href=&#34;https://www.libsdl.org/download-2.0.php&#34;&gt;SDL 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Windows users, we provide a ready-made &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64.&lt;/p&gt; &#xA;&lt;p&gt;Visual Studio builds are also possible, but you still need &lt;a href=&#34;http://mamedev.org/tools/&#34;&gt;build environment&lt;/a&gt; based on MinGW-w64. In order to generate solution and project files just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or use this command to build it directly using msbuild&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make vs2019 MSBUILD=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Where can I find out more?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mamedev.org/&#34;&gt;Official MAME Development Team Site&lt;/a&gt; (includes binary downloads, wiki, forums, and more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mess.redump.net/&#34;&gt;Official MESS Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mametesters.org/&#34;&gt;MAME Testers&lt;/a&gt; (official bug tracker for MAME and MESS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;h2&gt;Coding standard&lt;/h2&gt; &#xA;&lt;p&gt;MAME source code should be viewed and edited with your editor set to use four spaces per tab. Tabs are used for initial indentation of lines, with one tab used per indentation level. Spaces are used for other alignment within a line.&lt;/p&gt; &#xA;&lt;p&gt;Some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#Allman_style&#34;&gt;Allman style&lt;/a&gt;; some parts of the code follow &lt;a href=&#34;https://en.wikipedia.org/wiki/Indent_style#K.26R_style&#34;&gt;K&amp;amp;R style&lt;/a&gt; -- mostly depending on who wrote the original version. &lt;strong&gt;Above all else, be consistent with what you modify, and keep whitespace changes to a minimum when modifying existing source.&lt;/strong&gt; For new code, the majority tends to prefer Allman style, so if you don&#39;t care much, use that.&lt;/p&gt; &#xA;&lt;p&gt;All contributors need to either add a standard header for license info (on new files) or inform us of their wishes regarding which of the following licenses they would like their code to be made available under: the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;BSD-3-Clause&lt;/a&gt; license, the &lt;a href=&#34;http://opensource.org/licenses/LGPL-2.1&#34;&gt;LGPL-2.1&lt;/a&gt;, or the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GPL-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The MAME project as a whole is made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34;&gt;GNU General Public License, version 2&lt;/a&gt; or later (GPL-2.0+), since it contains code made available under multiple GPL-compatible licenses. A great majority of the source files (over 90% including core files) are made available under the terms of the &lt;a href=&#34;http://opensource.org/licenses/BSD-3-Clause&#34;&gt;3-clause BSD License&lt;/a&gt;, and we would encourage new contributors to make their contributions available under the terms of this license.&lt;/p&gt; &#xA;&lt;p&gt;Please note that MAME is a registered trademark of Gregory Ember, and permission is required to use the &#34;MAME&#34; name, logo, or wordmark.&lt;/p&gt; &#xA;&lt;a href=&#34;http://opensource.org/licenses/GPL-2.0&#34; target=&#34;_blank&#34;&gt; &lt;img align=&#34;right&#34; src=&#34;http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png&#34;&gt; &lt;/a&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright (C) 1997-2021  MAMEDev and contributors&#xA;&#xA;This program is free software; you can redistribute it and/or modify it&#xA;under the terms of the GNU General Public License version 2, as provided in&#xA;docs/legal/GPL-2.0.&#xA;&#xA;This program is distributed in the hope that it will be useful, but WITHOUT&#xA;ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or&#xA;FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for&#xA;more details.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see COPYING for more details.&lt;/p&gt;</summary>
  </entry>
</feed>