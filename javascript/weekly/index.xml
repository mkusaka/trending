<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub JavaScript Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:55:23Z</updated>
  <subtitle>Weekly Trending of JavaScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xenova/transformers.js</title>
    <updated>2023-03-26T01:55:23Z</updated>
    <id>tag:github.com,2023-03-26:/xenova/transformers.js</id>
    <link href="https://github.com/xenova/transformers.js" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run ðŸ¤— Transformers in your browser!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformers.js&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/@xenova/transformers&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@xenova/transformers&#34; alt=&#34;npm&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@xenova/transformers&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dw/@xenova/transformers&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xenova/transformers.js/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/xenova/transformers.js&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run ðŸ¤— Transformers in your browser! We currently support &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/bert&#34;&gt;BERT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/albert&#34;&gt;ALBERT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/distilbert&#34;&gt;DistilBERT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/t5&#34;&gt;T5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/t5v1.1&#34;&gt;T5v1.1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/flan-t5&#34;&gt;FLAN-T5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/mt5&#34;&gt;mT5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt2&#34;&gt;GPT2&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt_neo&#34;&gt;GPT Neo&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/bart&#34;&gt;BART&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/codegen&#34;&gt;CodeGen&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/whisper&#34;&gt;Whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/clip&#34;&gt;CLIP&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/vit&#34;&gt;Vision Transformer&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder&#34;&gt;VisionEncoderDecoder&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/detr&#34;&gt;DETR&lt;/a&gt; models, for a variety of tasks including: masked language modelling, text classification, text-to-text generation, translation, summarization, question answering, text generation, automatic speech recognition, image classification, zero-shot image classification, image-to-text, and object detection.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/26504141/221056008-e906614e-e6f0-4e10-b0a8-7d5c99e955b4.gif&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check out our demo at &lt;a href=&#34;https://xenova.github.io/transformers.js/&#34;&gt;https://xenova.github.io/transformers.js/&lt;/a&gt;. As you&#39;ll see, everything runs inside the browser!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;If you use &lt;a href=&#34;https://www.npmjs.com/package/@xenova/transformers&#34;&gt;npm&lt;/a&gt;, you can install it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i @xenova/transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can use it in a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag from a CDN, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;!-- Using jsDelivr --&amp;gt;&#xA;&amp;lt;script src=&#34;https://cdn.jsdelivr.net/npm/@xenova/transformers/dist/transformers.min.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&#xA;&amp;lt;!-- or UNPKG --&amp;gt;&#xA;&amp;lt;script src=&#34;https://www.unpkg.com/@xenova/transformers/dist/transformers.min.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Basic example&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s super easy to translate from existing code!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th width=&#34;440px&#34;&gt;&lt;b&gt;Python (original)&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th width=&#34;440px&#34;&gt;&lt;b&gt;Javascript (ours)&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;&#xA;# Allocate a pipeline for sentiment-analysis&#xA;pipe = pipeline(&#39;sentiment-analysis&#39;)&#xA;&#xA;out = pipe(&#39;I love transformers!&#39;)&#xA;# [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.999806941}]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import { pipeline } from &#34;@xenova/transformers&#34;;&#xA;&#xA;// Allocate a pipeline for sentiment-analysis&#xA;let pipe = await pipeline(&#39;sentiment-analysis&#39;);&#xA;&#xA;let out = await pipe(&#39;I love transformers!&#39;);&#xA;// [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.999817686}]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;In the same way as the Python library, you can use a different model by providing its name as the second argument to the pipeline function. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// Use a different model for sentiment-analysis&#xA;let pipe = await pipeline(&#39;sentiment-analysis&#39;, &#39;nlptown/bert-base-multilingual-uncased-sentiment&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Custom setup&lt;/h3&gt; &#xA;&lt;p&gt;By default, Transformers.js uses &lt;a href=&#34;https://huggingface.co/Xenova/transformers.js/tree/main/quantized&#34;&gt;hosted models&lt;/a&gt; and &lt;a href=&#34;https://cdn.jsdelivr.net/npm/@xenova/transformers/dist/&#34;&gt;precompiled WASM binaries&lt;/a&gt;, which should work out-of-the-box. You can override this behaviour as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;import { env } from &#34;@xenova/transformers&#34;;&#xA;&#xA;// Use a different host for models.&#xA;// - `remoteURL` defaults to use the HuggingFace Hub&#xA;// - `localURL` defaults to &#39;/models/onnx/quantized/&#39;&#xA;env.remoteURL = &#39;https://www.example.com/&#39;;&#xA;env.localURL = &#39;/path/to/models/&#39;;&#xA;&#xA;// Set whether to use remote or local models. Defaults to true.&#xA;//  - If true, use the path specified by `env.remoteURL`.&#xA;//  - If false, use the path specified by `env.localURL`.&#xA;env.remoteModels = false;&#xA;&#xA;// Set parent path of .wasm files. Defaults to use a CDN.&#xA;env.onnx.wasm.wasmPaths = &#39;/path/to/files/&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Node.js&lt;/h4&gt; &#xA;&lt;p&gt;This library uses &lt;code&gt;onnxruntime-web&lt;/code&gt; as its default backend. However, if your application runs with Node.js, you can install &lt;code&gt;onnxruntime-node&lt;/code&gt; in your project (using &lt;code&gt;npm i onnxruntime-node&lt;/code&gt;) to obtain a massive boost in performance (&amp;gt;5x in some cases). The CPU execution provider is much faster than WASM executor provider, most likely due to &lt;a href=&#34;https://github.com/microsoft/onnxruntime/issues/10311&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Convert your PyTorch models to ONNX&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://onnxruntime.ai/&#34;&gt;ONNX Runtime&lt;/a&gt; to run the models in the browser, so you must first convert your PyTorch model to ONNX (which can be done using our conversion script). In general, the command will look something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./scripts/convert.py --model_id &amp;lt;hf_model_id&amp;gt; --from_hub --quantize --task &amp;lt;task&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to use &lt;code&gt;bert-base-uncased&lt;/code&gt; for masked language modelling, you can use the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ./scripts/convert.py --model_id bert-base-uncased --from_hub --quantize --task masked-lm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use a local model, remove the &lt;code&gt;--from_hub&lt;/code&gt; flag from above and place your PyTorch model in the &lt;code&gt;./models/pytorch/&lt;/code&gt; folder. You can also choose a different location by specifying the parent input folder with &lt;code&gt;--input_parent_dir /path/to/parent_dir/&lt;/code&gt; (note: without the model id).&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can find some of the models we have already converted &lt;a href=&#34;https://huggingface.co/Xenova/transformers.js&#34;&gt;here&lt;/a&gt;. For example, to use &lt;code&gt;bert-base-uncased&lt;/code&gt; for masked language modelling, you can use the model found at &lt;a href=&#34;https://huggingface.co/Xenova/transformers.js/tree/main/quantized/bert-base-uncased/masked-lm&#34;&gt;https://huggingface.co/Xenova/transformers.js/tree/main/quantized/bert-base-uncased/masked-lm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; We recommend quantizing the model (&lt;code&gt;--quantize&lt;/code&gt;) to reduce model size and improve inference speeds (at the expense of a slight decrease in accuracy). For more information, run the help command: &lt;code&gt;python ./scripts/convert.py -h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Options&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Coming soon...&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Coming soon... In the meantime, check out the source code for the demo &lt;a href=&#34;https://github.com/xenova/transformers.js/raw/main/assets/js/worker.js&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;Inspired by &lt;a href=&#34;https://github.com/praeclarum/transformers-js&#34;&gt;https://github.com/praeclarum/transformers-js&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TavernAI/TavernAI</title>
    <updated>2023-03-26T01:55:23Z</updated>
    <id>tag:github.com,2023-03-26:/TavernAI/TavernAI</id>
    <link href="https://github.com/TavernAI/TavernAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Atmospheric adventure chat for AI language models (KoboldAI, NovelAI, Pygmalion, OpenAI chatgpt)&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;TavernAI is a adventure atmospheric chat (KoboldAI, NovelAI, Pygmalion, OpenAI)&lt;/h3&gt; &#xA;&lt;p&gt;Examples of interface and output: &lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TavernAI/TavernAI/main/readme/1.png&#34; height=&#34;200&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TavernAI/TavernAI/main/readme/4.png&#34; height=&#34;200&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TavernAI/TavernAI/main/readme/5.png&#34; height=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;h6&gt;Download:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sourceforge.net/projects/tavernaimain/files/TavernAI.rar/download&#34;&gt;Windows .exe&lt;/a&gt;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://github.com/TavernAI/TavernAI/archive/refs/heads/main.zip&#34;&gt;Node.js version&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Run online:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/TavernAI/TavernAI/blob/main/colab/GPU.ipynb&#34;&gt;TavernAI Colab&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Links:&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://boosty.to/tavernai&#34;&gt;TavernAI Boosty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/zmK2gmr45t&#34;&gt;TavernAI Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating characters with personality setup&lt;/li&gt; &#xA; &lt;li&gt;Online character library&lt;/li&gt; &#xA; &lt;li&gt;Choosing name and avatar for your character&lt;/li&gt; &#xA; &lt;li&gt;Deleting and regenerating messages&lt;/li&gt; &#xA; &lt;li&gt;Editing any messages&lt;/li&gt; &#xA; &lt;li&gt;Works with various AI models&lt;/li&gt; &#xA; &lt;li&gt;Importing chats from CAI&lt;/li&gt; &#xA; &lt;li&gt;More longer messages from characters&lt;/li&gt; &#xA; &lt;li&gt;Choosing atmospheric backgrounds&lt;/li&gt; &#xA; &lt;li&gt;Creating and easy choosing preset settings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to install&lt;/h2&gt; &#xA;&lt;h3&gt;In Detail:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TavernAI/TavernAI/wiki/How-to-install&#34;&gt;Install with KoboldAI&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TavernAI/TavernAI/wiki/How-to-install-Novel&#34;&gt;Install with NovelAI&lt;/a&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Briefly:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download &lt;a href=&#34;https://github.com/TavernAI/TavernAI/archive/refs/heads/main.zip&#34;&gt;TavernAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://nodejs.org/download/release/v19.1.0/&#34;&gt;Node.js v19.1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run Start.bat (or use command: &lt;em&gt;npm install&lt;/em&gt;, &lt;em&gt;node server.js&lt;/em&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;AI Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client&#34;&gt;KoboldAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://novelai.net/&#34;&gt;NovelAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rentry.org/pygmalion-ai&#34;&gt;Pygmalion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat.openai.com/&#34;&gt;chatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;GPT-4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;p&gt;Use this button to edit the message&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TavernAI/TavernAI/main/readme/3.png&#34; width=&#34;600&#34;&gt;&lt;br&gt;&lt;br&gt; If the message is not finished, you can simply send the request again, TavernAI will understand that this is a continuation. &lt;br&gt;(Works with KoboldAI and NovelAI models, not with Pygmalion)&lt;br&gt; &lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TavernAI/TavernAI/main/readme/2.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Additional materials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.reddit.com/user/Crataco/comments/zuowi9/opensource_chatbot_companions/&#34;&gt;https://www.reddit.com/user/Crataco/comments/zuowi9/opensource_chatbot_companions/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;For contacts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Discord: Humi#5044 &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>dice2o/BingGPT</title>
    <updated>2023-03-26T01:55:23Z</updated>
    <id>tag:github.com,2023-03-26:/dice2o/BingGPT</id>
    <link href="https://github.com/dice2o/BingGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Desktop application of new Bing&#39;s AI-powered chat (Windows, macOS and Linux)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;180&#34; src=&#34;https://raw.githubusercontent.com/dice2o/BingGPT/main/icon.png&#34; alt=&#34;BingGPT&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;BingGPT&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Desktop application of new Bing&#39;s AI-powered chat&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/license-Apache_2.0-green&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/dice2o/BingGPT/releases&#34;&gt; &lt;img alt=&#34;Downloads&#34; src=&#34;https://img.shields.io/github/downloads/dice2o/BingGPT/total?color=blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-win32-x64-Setup.exe&#34;&gt;BingGPT-0.3.1-win32-x64-Setup.exe&lt;/a&gt; (Installer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-win32-x64.zip&#34;&gt;BingGPT-0.3.1-win32-x64.zip&lt;/a&gt; (Portable)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-win32-arm64-Setup.exe&#34;&gt;BingGPT-0.3.1-win32-arm64-Setup.exe&lt;/a&gt; (Installer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-win32-arm64.zip&#34;&gt;BingGPT-0.3.1-win32-arm64.zip&lt;/a&gt; (Portable)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-darwin-arm64.dmg&#34;&gt;BingGPT-0.3.1-darwin-arm64.dmg&lt;/a&gt; (Apple Silicon)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-darwin-x64.dmg&#34;&gt;BingGPT-0.3.1-darwin-x64.dmg&lt;/a&gt; (Intel chips)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-linux-x64.deb&#34;&gt;BingGPT-0.3.1-linux-x64.deb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-linux-arm64.deb&#34;&gt;BingGPT-0.3.1-linux-arm64.deb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-linux-x64.rpm&#34;&gt;BingGPT-0.3.1-linux-x64.rpm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT/releases/download/v0.3.1/BingGPT-0.3.1-linux-arm64.rpm&#34;&gt;BingGPT-0.3.1-linux-arm64.rpm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get access to the early preview of new Bing - &lt;a href=&#34;https://www.bing.com/new&#34;&gt;Join the waitlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sign in to your Microsoft account&lt;/li&gt; &#xA; &lt;li&gt;Start chatting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: VPN is required when launching the app if new Bing is not available in your area. Make sure &lt;code&gt;bing.com&lt;/code&gt; and its subdomains are included in proxy rules.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chat with new Bing without installing Microsoft Edge or browser plugins&lt;/li&gt; &#xA; &lt;li&gt;Export full conversation to Markdown, PNG or PDF&lt;/li&gt; &#xA; &lt;li&gt;Customize appearance (theme &amp;amp; font size)&lt;/li&gt; &#xA; &lt;li&gt;Multi-platform&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Screenshot&lt;/h2&gt; &#xA;&lt;img width=&#34;601&#34; src=&#34;https://raw.githubusercontent.com/dice2o/BingGPT/main/screenshot.png&#34; alt=&#34;BingGPT Screenshot&#34;&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache-2.0 License&lt;/p&gt;</summary>
  </entry>
</feed>