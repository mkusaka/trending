<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub JavaScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-04T01:36:39Z</updated>
  <subtitle>Daily Trending of JavaScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OptimalScale/LMFlow</title>
    <updated>2023-04-04T01:36:39Z</updated>
    <id>tag:github.com,2023-04-04:/OptimalScale/LMFlow</id>
    <link href="https://github.com/OptimalScale/LMFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Language Model for All. å…±å»ºå¤§æ¨¡å‹ç¤¾åŒºï¼Œè®©æ¯ä¸ªäººéƒ½è®­å¾—èµ·å¤§æ¨¡å‹ã€‚&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/logo.png&#34; alt=&#34;LMFlow&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto; background-color: transparent;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LMFlow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Website-Doc-ff69b4.svg?sanitize=true&#34; alt=&#34;Doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/srGxyazbNs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord&#34; alt=&#34;Embark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://i.328888.xyz/2023/04/03/ibw8dc.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E5%8A%A0%E5%85%A5-brightgreen?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.&lt;/p&gt; &#xA;&lt;p&gt;Large Language Model for All. See our &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#vision&#34;&gt;vision&lt;/a&gt;. å…±å»ºå¤§æ¨¡å‹ç¤¾åŒºï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½è®­å¾—èµ·å¤§æ¨¡å‹ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„&lt;a href=&#34;https://github.com/OptimalScale/LMFlow#vision&#34;&gt;æ„¿æ™¯&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/features.png&#34; alt=&#34;LMFlow-features&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023-04-02] &lt;a href=&#34;https://lmflow.com/&#34;&gt;Web service is online!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-01] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-zoo&#34;&gt;Release Chinese checkpoints in model zoo: Hu (æ¹–ç¾Š), Dongshan (ä¸œå±±ç¾Š), and Hetian (å’Œç”°ç¾Š).&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-01] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-zoo&#34;&gt;Release English checkpoints in model zoo: LLaMA7B-medical, LLaMA13B-medical, and LLaMA33B-medical.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#supported-models&#34;&gt;Support full tuning and lora tuning for all decoder models.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-performance&#34;&gt;Tasked tuned model beats ChatGPT on medical domain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Release code and checkpoints - version 0.0.1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Currently our checkpoint download service is at capacity. We have allocated one more server to support that. If you encounter error &#34;&lt;em&gt;too many HTTP requests&lt;/em&gt;&#34;, please wait for several minutes and try again. Thanks for your understanding.&lt;span&gt;ğŸ™&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We provide four kinds of demos which include&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Online Service: If you don&#39;t want to run any code and just want to try our models, we deploy our instruction-tuned LLaMA-7B and LLaMA-33B for you to have a try.&lt;/li&gt; &#xA; &lt;li&gt;Colab Chatbot(shell): An interactive shell-based chatbot for you to easily deploy a chatbot on colab.&lt;/li&gt; &#xA; &lt;li&gt;Colab Chatbot(web): An interactive web-based chatbot for you to easily deploy your own chatbot on colab.&lt;/li&gt; &#xA; &lt;li&gt;Local Deploy: We also provide a way for you to deploy your model/chatbot locally, which means you can deploy much larger model than previous three methods if you have enough resource.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lmflow.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Online%20Service-Web-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1P9Hf6_mLE7WHH92pw73j9D5kz6GTdkow?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(shell)%20%20chatbot:%20gpt--neo-orange?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1LLtiiQO-ZIIFsTKxYzGWYX9BDRc-v8dq?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(web)%20%20chatbot:%20gpt--neo-blue?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Online Service&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Welcome to visit our &lt;a href=&#34;https://lmflow.com/&#34;&gt;web service&lt;/a&gt;. We deploy Hu (æ¹–ç¾Š), and Hetian (å’Œç”°ç¾Š) online for preview. Due to the high website traffic, sometimes the website may fail to respond. You can also deploy the chatbot referto &lt;code&gt;Local Deploy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Colab chatbot(shell)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/colab-shell-chatbot-demo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We provide a simple shell demo of chatbot with Google Colab&#39;s T4/P100/V100 GPU. Notice that the provided gpt-neo-2.7b model is &lt;strong&gt;a rather weak model&lt;/strong&gt;, which only supports English and may sometimes generate unsatisfactory responses. To improve the performance, users can use their own dataset to finetune and obtain a better model with LMFlow. One can also try other available decoder-only models provided in ğŸ¤— &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;huggingface&lt;/a&gt;, by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_chatbot.sh {another-model-name}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Colab chatbot(web)&lt;/h3&gt; &#xA;&lt;p&gt;We provide a simple web demo of chatbot with Google Colab&#39;s T4/P100/V100 GPU. Notice that the provided gpt-neo-2.7b model is &lt;strong&gt;a rather weak model&lt;/strong&gt;, which only supports English and may sometimes generate unsatisfactory responses.&lt;/p&gt; &#xA;&lt;h3&gt;Local Deploy&lt;/h3&gt; &#xA;&lt;p&gt;If you have resources and want to deploy your own model locally. We provide you an easy way to run a flask server to launch a backend (to further provide services to other frontend) and an interactive web frontend (to let you communicate directly) by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd ./service&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Medical Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PubMedQA (ID)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MedQA-USMLE (OOD)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MedMCQA (ID)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human (pass)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human (expert)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InstructGPT 175B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA 33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Task-tuned LLaMA 7B (Full)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;75.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Task-tuned LLaMA 33B (LoRA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;58.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The LLaMA 33B (LoRA) performance is achieved with only &lt;strong&gt;~16h&lt;/strong&gt; finetuning on the training split of PubMedQA and MedMCQA with a single 8 * A100 server. For more performance, including instruction tuning results, please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;We open-sourced the trained checkpoints to everyone for further training and inference.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Instruct-tuned Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hu (æ¹–ç¾Š)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1x5JLae3akVkfFeDhSe3TEyUbPn_GNFyb/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dongshan (ä¸œå±±ç¾Š)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1m_rpe6rNpN59kWvjJ3GfKeEmS-68TRYr/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hetian (å’Œç”°ç¾Š)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1IqgqLHwNkWQ7BffheZnqD6a-8Zul1bk6/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Altay (é˜¿å‹’æ³°ç¾Š)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/65&#34; alt=&#34;training&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-65B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google Drive&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA7B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Z44tsrRvfDFvucbNGFjHC_vbPcBvg3x-/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA13B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1uoTAXTMyYQkP6N4ummx7tj-c4v1p91ap/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA33B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/14N9o_1pwHmVuSikQ3orMVzZDrLYJC0iM/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA65B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/90&#34; alt=&#34;training&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-65B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google Drive&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Pipelines&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Pipelines&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Task Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Instruction Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parameter-Efficient Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Large Model Inference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;âœ…&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;ğŸ”§&lt;/span&gt; Developing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;Seamlessly supported all the &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;decoder models&lt;/a&gt; in ğŸ¤— huggingface. LLaMA, GPT2, GPT-Neo, Galactica, have been fully tested. We will support encoder models soon.&lt;/p&gt; &#xA;&lt;h2&gt;1.Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OptimalScale/LMFlow.git&#xA;cd LMFlow&#xA;conda create -n lmflow python=3.9 -y&#xA;conda activate lmflow&#xA;conda install mpi4py&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2.Prepare Dataset&lt;/h2&gt; &#xA;&lt;p&gt;You can easily download the example training dataset and test dataset by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd data&#xA;bash download.sh all&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you cannot access Google Drive, you can download the data by &lt;a href=&#34;https://pan.baidu.com/s/1L7AC5Oy-3YhbCp2aNX4tnQ?pwd=dm2s&#34;&gt;BaiduNetDisk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also use your own dataset by simply convert to the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;type&#34;: &#34;text2text&#34;,&#xA;  &#34;instances&#34;: [&#xA;    {&#xA;      &#34;input&#34;: &#34;Question: The Transformer architecture [START_REF]&#34;,&#xA;      &#34;output&#34;: &#34;N/A&#34;&#xA;    },&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;type&#34;: &#34;text_only&#34;,&#xA;  &#34;instances&#34;: [&#xA;    {&#xA;      &#34;text&#34;: &#34;Defintion: In this task, we ask you to write an answer to a question that involves events that may be stationary (not changing over time) or transient (changing over time). For example, the sentence \&#34;he was born in the U.S.\&#34; contains a stationary event since it will last forever; however, \&#34;he is hungry\&#34; contains a transient event since it will remain true for a short period of time. Note that a lot of the questions could have more than one correct answer. We only need a single most-likely answer. Please try to keep your \&#34;answer\&#34; as simple as possible. Concise and simple \&#34;answer\&#34; is preferred over those complex and verbose ones. \n Input: Question: Sentence: It&#39;s hail crackled across the comm, and Tara spun to retake her seat at the helm. \nQuestion: Will the hail storm ever end? \n Output: NA \n\n&#34;&#xA;    },&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Run Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 Run Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;scripts/run_finetune.sh&lt;/code&gt; to finetune a GPT-2 base model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to provide arguments for deepspeed to reflect your machine settings, you may pass the corresponding deepspeed arguments to the script. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune.sh &#34;--num_gpus=8 --master_port 10001&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable LoRA finetuning, you may refer to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune_with_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which can be run in similar manner.&lt;/p&gt; &#xA;&lt;p&gt;For detailed configurations, one may modify these scripts directly. These scripts actually just call python script &lt;code&gt;examples/finetune.py&lt;/code&gt;, which can be run in following manner,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;deepspeed ${deepspeed_args} \&#xA;  examples/finetune.py \&#xA;    --deepspeed configs/ds_config_zero3.json \&#xA;    --bf16 \&#xA;    --run_name finetune_with_lora \&#xA;    --model_name_or_path facebook/galactica-1.3b \&#xA;    --num_train_epochs 0.01 \&#xA;    --learning_rate 2e-5 \&#xA;    --dataset_path ${dataset_path} \&#xA;    --per_device_train_batch_size 1 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --validation_split_percentage 0 \&#xA;    --logging_steps 20 \&#xA;    --block_size 512 \&#xA;    --do_train \&#xA;    --output_dir output_models/finetune \&#xA;    --overwrite_output_dir \&#xA;    --ddp_timeout 72000 \&#xA;    --save_steps 5000 \&#xA;    --dataloader_num_workers 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we set number of epochs &lt;code&gt;--num_train_epochs&lt;/code&gt; to &lt;code&gt;0.01&lt;/code&gt; so that the finetuning process can be finished quickly. If you wish to obtain a model with better performance, feel free to adjust those hyperparameters. You may run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python examples/finetune.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to view all possible finetuning arguments. The finetuned model checkpoint will be saved in the argument specified by &lt;code&gt;--output_dir&lt;/code&gt;, which is &lt;code&gt;output_models/finetune&lt;/code&gt; in the above example.&lt;/p&gt; &#xA;&lt;h3&gt;3.2 Run Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;One can directly run evaluation with an existing huggingface model, e.g. to run GPT2 large, one may execute&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or run the corresponding python script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;    deepspeed examples/evaluate.py \&#xA;    --answer_type medmcqa \&#xA;    --model_name_or_path gpt2-large \&#xA;    --dataset_path data/MedQA-USMLE/validation \&#xA;    --deepspeed examples/ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To load the finetuned model, specify &lt;code&gt;--model_name_or_path&lt;/code&gt; with the saved model checkpoint directory path.&lt;/p&gt; &#xA;&lt;p&gt;For LoRA finetuned models, one may refer to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_evaluation_with_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Those scripts invoke the examples &lt;code&gt;examples/*.py&lt;/code&gt; built based on our APIs. For more API-related examples, one may refer to the methods in the unittest &lt;code&gt;tests&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Additional Notes&lt;/h2&gt; &#xA;&lt;h3&gt;4.1 LLaMA Checkpoint&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;First, you need to get the access of LLaMA model from &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;facebookresearch/llama&lt;/a&gt;. Download the official checkpoints and save them into &lt;code&gt;${llama-path}&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Second, convert the official checkpoints &lt;code&gt;${llama-path}&lt;/code&gt; to HuggingFace supported checkpoints &lt;code&gt;${llama-hf-path}&lt;/code&gt; by running&lt;/p&gt; &lt;p&gt;&lt;code&gt;python ./scripts/convert_llama_weights_to_hf.py --input_dir ${llama-path} --model_size 7B --output_dir ${llama-hf-path}/llama-7b-hf&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then you are good to go by setting the checkpoint path to &lt;code&gt;${llama-hf-path}/llama-7b-hf&lt;/code&gt;. Enjoy it!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(optional) Now you have the original llama-7b-hf pretrained model. With&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd output_models &amp;amp;&amp;amp; ./download.sh all &amp;amp;&amp;amp; cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can obtain the model difference finetuned by ours. By a way similar to &lt;code&gt;./scripts/run_evaluation_with_lora.sh&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;    deepspeed examples/evaluate.py \&#xA;    --answer_type text \&#xA;    --model_name_or_path ${llama-hf-path}/llama-7b-hf \&#xA;    --lora_model_path output_models/${llama-model-diff-path} \&#xA;    --dataset_path data/alpaca/test \&#xA;    --prompt_structure &#34;Input: {input}&#34; \&#xA;    --deepspeed examples/ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now evaluate with the finetuned llama model.&lt;/p&gt; &#xA;&lt;h3&gt;4.2 DeepSpeed Config&lt;/h3&gt; &#xA;&lt;p&gt;You can config the deepspeed under configs. Details can be referred at &lt;a href=&#34;https://www.deepspeed.ai/docs/config-json/&#34;&gt;DeepSpeed Configuration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;5. Model Release&lt;/h2&gt; &#xA;&lt;h3&gt;5.1 Medical Model Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;You can run following script to download our medical model checkpoints :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd output_models&#xA;bash download.sh medical_ckpt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly download our model via google drive link : &lt;a href=&#34;https://drive.google.com/file/d/1bnsQGNGNYchsOfiNyRAmL2fNiowbmFNw/view?usp=share_link&#34;&gt;medical_ckpt.tar.gz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.2 Instruction Model Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;Similarly, you can run following script to download our instruction model checkpoints :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd output_models&#xA;bash download.sh instruction_ckpt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly download our model via google drive link : &lt;a href=&#34;https://drive.google.com/file/d/1d_ioQ-ViVweeifbsFSO4pczc3UORFHZO/view?usp=share_link&#34;&gt;instruction_ckpt.tar.gz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.3 Begin Reproduce&lt;/h3&gt; &#xA;&lt;p&gt;After downloading the model checkpoints. You can replace the &lt;code&gt;--lora_model_path&lt;/code&gt; with &lt;code&gt;output_models/instruction_ckpt/llama7b-lora&lt;/code&gt; (example for llama-7b for instruction) and replace &lt;code&gt;--model_name_or_path&lt;/code&gt; with your converted llama model inside &lt;code&gt;LMFlow/scripts/run_evaluation_with_lora.sh&lt;/code&gt; and run this shell script to reproduce the result.&lt;/p&gt; &#xA;&lt;p&gt;Then you can check the model performance at our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Documentation&lt;/a&gt; for more API reference and experimental results.&lt;/p&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;p&gt;Hello there! We are excited to announce the upcoming release of our code repository that includes a complete LLM training process, enabling users to quickly build their own language models and train them effectively.&lt;/p&gt; &#xA;&lt;p&gt;Our code repository is not just a simple model; it includes the complete training workflow, model optimization, and testing tools. You can use it to build various types of language models, including conversation models, question-answering models, and text generation models, among others.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, we aim to create an open and democratic LLM sharing platform where people can share their checkpoints and experiences to collectively improve the skills of the community. We welcome anyone who is interested in LLM to participate and join us in building an open and friendly community!&lt;/p&gt; &#xA;&lt;p&gt;Whether you are a beginner or an expert, we believe that you can benefit from this platform. Let&#39;s work together to build a vibrant and innovative LLM community!&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å¾ˆé«˜å…´åœ°å¼€æºLMFlowä»£ç åº“ï¼Œå…¶ä¸­åŒ…æ‹¬äº†å®Œæ•´çš„å¤§æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿã€é«˜æ•ˆåœ°è®­ç»ƒå’Œéƒ¨ç½²è‡ªå·±çš„è¯­è¨€æ¨¡å‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘ä»¬çš„ä»£ç åº“ä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼› å®ƒåŒ…æ‹¬å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€æ¨¡å‹æƒé‡å’Œæµ‹è¯•å·¥å…·ã€‚ æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥æ„å»ºå„ç§ç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¯¹è¯æ¨¡å‹ã€é—®ç­”æ¨¡å‹å’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ç­‰ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æ­¤å¤–ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ›å»ºä¸€ä¸ªå¼€æ”¾å’Œæ°‘ä¸»çš„å¤§æ¨¡å‹å…±äº«å¹³å°ï¼Œä»»ä½•äººéƒ½å¯ä»¥åœ¨è¿™ä¸ªå¹³å°ä¸Šåˆ†äº«è®­ç»ƒæ¨¡å‹æƒé‡å’Œç»éªŒã€‚ æˆ‘ä»¬æ¬¢è¿ä»»ä½•å¯¹å¤§æ¨¡å‹æ„Ÿå…´è¶£çš„äººå‚ä¸è¿›æ¥ï¼Œä¸æˆ‘ä»¬ä¸€èµ·å»ºè®¾ä¸€ä¸ªå¼€æ”¾å‹å¥½çš„ç¤¾åŒºï¼&lt;/p&gt; &#xA;&lt;p&gt;æ— è®ºæ‚¨æ˜¯åˆå­¦è€…è¿˜æ˜¯ä¸“å®¶ï¼Œæˆ‘ä»¬ç›¸ä¿¡å¤§å®¶éƒ½èƒ½ä»è¿™ä¸ªå¹³å°ä¸­è·ç›Šã€‚è®©æˆ‘ä»¬å…±åŒåŠªåŠ›ï¼Œå»ºç«‹ä¸€ä¸ªå……æ»¡æ´»åŠ›å’Œåˆ›æ–°çš„å¤§æ¨¡å‹ç¤¾åŒºï¼&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/srGxyazbNs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord&#34; alt=&#34;Embark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://i.328888.xyz/2023/04/03/ibw8dc.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E5%BE%AE%E4%BF%A1-%E5%8A%A0%E5%85%A5-brightgreen?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This package aims to provide a streamlined and user-friendly pipeline for large model tuning. Its functionalities serve as a reference and are intended for use by the user. However, it is important to note that the responsibility for the preparation of the data and pretrained models lies solely with the user. This package does not guarantee the accuracy, completeness, applicability, or legality of the components from the user&#39;s preparation. Users must be aware of and assume all risks and liabilities associated with the preparation of the models and data, and obtain legal, commercial, and technical advice before utilizing this package. The pipeline shall not be held responsible for any direct, indirect, special, incidental, or consequential damages resulting from the user&#39;s improper preparation of the data and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;Our checkpoints, which include both English and Chinese versions, are provided solely for research purposes. The training data contained within these checkpoints includes generated results from the ChatGPT language model. We do not endorse or encourage the distribution or usage of these checkpoints for commercial purposes. Users of these checkpoints are solely responsible for ensuring that they are used correctly and appropriately.&lt;/p&gt; &#xA;&lt;p&gt;It is also crucial to highlight that the results generated by the model are based on probabilistic models and not directly related to this pipeline. The accuracy, reliability, applicability, and legality of the results are not guaranteed by this pipeline. Therefore, users must also be aware of the risks and liabilities associated with the results and seek legal, commercial, and technical advice before relying on the model-generated outcomes. This pipeline shall not be accountable for any direct, indirect, special, incidental, or consequential damages resulting from the user&#39;s reliance on the model-generated results.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you need any help, please submit a &lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;Github&lt;/a&gt; issue.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=OptimalScale/LMFlow&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving â­ and citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lmflow,&#xA;  author = {Shizhe Diao and Rui Pan and Hanze Dong and KaShun Shum and Jipeng Zhang and Wei Xiong and Tong Zhang},&#xA;  title = {LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://optimalscale.github.io/LMFlow/}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>GooseMod/OpenAsar</title>
    <updated>2023-04-04T01:36:39Z</updated>
    <id>tag:github.com,2023-04-04:/GooseMod/OpenAsar</id>
    <link href="https://github.com/GooseMod/OpenAsar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source alternative of Discord desktop&#39;s app.asar&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAsar&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/GooseMod/OpenAsar/actions/workflows/nightly.yml/badge.svg?sanitize=true&#34; alt=&#34;Nightly Status&#34;&gt; &lt;a href=&#34;https://choosealicense.com/licenses/mit/l&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/sponsors/CanadaHonk?label=Sponsors&amp;amp;logo=github&#34; alt=&#34;GitHub Sponsors&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An open-source alternative of Discord desktop&#39;s &lt;code&gt;app.asar&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Startup Speed&lt;/strong&gt;: ~2x faster startup times (up to ~4x with experimental config)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ğŸ“ˆ&lt;/span&gt; Performance&lt;/strong&gt;: OpenAsar can make your client feel snappier (scrolling, switching channels, etc)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ğŸ–Œ&lt;/span&gt; Splash Theming&lt;/strong&gt;: Easy theming for your splash which works with most themes for any client mod&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ğŸ”Œ&lt;/span&gt; Drop-in&lt;/strong&gt;: Replace one file and it&#39;s installed, that&#39;s it (same with uninstall)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;âš™&lt;/span&gt; Configurable&lt;/strong&gt;: Adds many config options for Discord and OpenAsar enhancements (see config section)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ğŸª¶&lt;/span&gt; Lightweight&lt;/strong&gt;: &amp;lt;1% of Discord&#39;s original size (9mb -&amp;gt; ~50kb)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span&gt;ğŸ›¡&lt;/span&gt; No Tracking&lt;/strong&gt;: Removes Discord&#39;s built-in tracking for crashes and errors in the asar (not app itself)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/GooseMod/OpenAsar/main/faq.md&#34;&gt;FAQ&lt;/a&gt; for more details&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/GooseMod/OpenAsar/wiki/Install-Guide&#34;&gt;Install Guide&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Config&lt;/h2&gt; &#xA;&lt;p&gt;You can configure OpenAsar by clicking the &#34;OpenAsar...&#34; version info in the bottom of your settings sidebar, which will open the config window.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dortania/OpenCore-Install-Guide</title>
    <updated>2023-04-04T01:36:39Z</updated>
    <id>tag:github.com,2023-04-04:/dortania/OpenCore-Install-Guide</id>
    <link href="https://github.com/dortania/OpenCore-Install-Guide" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repo for the OpenCore Install Guide&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;p&gt;home: true heroImage: /dortania-logo-clear.png heroText: Dortania&#39;s OpenCore Install Guide actionText: Getting Startedâ†’ actionLink: prerequisites.md&lt;/p&gt; &#xA;&lt;p&gt;meta:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;name: description content: Current supported version 0.8.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;What is OpenCore and who is this guide for&lt;/h1&gt; &#xA;&lt;p&gt;OpenCore is what we refer to as a &#34;boot loader&#34; â€“ it is a complex piece of software that we use to prepare our systems for macOS â€“ specifically by injecting new data for macOS such as SMBIOS, ACPI tables and kexts. How this tool differs from others like Clover is that it has been designed with security and quality in mind, allowing us to use many security features found on real Macs, such as &lt;a href=&#34;https://support.apple.com/en-ca/HT204899&#34;&gt;System Integrity Protection&lt;/a&gt; and &lt;a href=&#34;https://support.apple.com/en-ca/HT204837&#34;&gt;FileVault&lt;/a&gt;. A more in-depth look can be found here: &lt;a href=&#34;https://raw.githubusercontent.com/dortania/OpenCore-Install-Guide/master/why-oc.md&#34;&gt;Why OpenCore over Clover and others&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This guide specifically focuses on two main things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installing macOS on an X86-based PC&lt;/li&gt; &#xA; &lt;li&gt;Teaching you what makes your Hack work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Because of this, you will be expected to read, learn and even use Google. This is not a simple one-click install setup.&lt;/p&gt; &#xA;&lt;p&gt;Please remember that OpenCore is still new and currently in beta. While quite stable, and arguably much more stable than Clover in pretty much every way, it is still being frequently updated, so chunks of configuration change quite often (i.e. new quirks replacing old ones).&lt;/p&gt; &#xA;&lt;p&gt;Lastly, those having issues can visit both the &lt;a href=&#34;https://www.reddit.com/r/hackintosh/&#34;&gt;r/Hackintosh subreddit&lt;/a&gt; and &lt;a href=&#34;https://discord.gg/u8V7N5C&#34;&gt;r/Hackintosh Discord&lt;/a&gt; for more help.&lt;/p&gt;</summary>
  </entry>
</feed>