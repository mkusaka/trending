<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub JavaScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-20T01:37:31Z</updated>
  <subtitle>Daily Trending of JavaScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wenda-LLM/wenda</title>
    <updated>2023-05-20T01:37:31Z</updated>
    <id>tag:github.com,2023-05-20:/wenda-LLM/wenda</id>
    <link href="https://github.com/wenda-LLM/wenda" rel="alternate"></link>
    <summary type="html">&lt;p&gt;闻达：一个LLM调用平台。为小模型外挂知识库查找和设计自动执行动作，实现不亚于于大模型的生成能力&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;闻达：一个大规模语言模型调用平台&lt;/h1&gt; &#xA;&lt;p&gt;针对特定环境的内容生成是LLM使用中的一项重要应用，实现这一目的，主要有&lt;code&gt;全量微调&lt;/code&gt;、&lt;code&gt;lora微调&lt;/code&gt;、和本项目方法。但个人没有做全量微调的，lora微调只能牺牲基础能力换单任务效果（用6B模型lora调出来的单任务效果，专门设计一个0.5B模型也能实现，且推理成本更低）。&lt;/p&gt; &#xA;&lt;p&gt;而本项目采用知识库+auto脚本的形式为LLM提高生成能力，充分考虑个人和中小企业的资源问题，以及国内大背景下知识安全和私密性问题,实现使小模型获得近似于大模型的生成能力。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;目前支持模型：&lt;code&gt;chatGLM-6B&lt;/code&gt;、&lt;code&gt;chatRWKV&lt;/code&gt;、&lt;code&gt;chatYuan&lt;/code&gt;、&lt;code&gt;llama系列&lt;/code&gt;以及&lt;code&gt;openaiapi&lt;/code&gt;和&lt;code&gt;chatglm130b api&lt;/code&gt;，初步支持&lt;code&gt;moss&lt;/code&gt;。&lt;/li&gt; &#xA; &lt;li&gt;使用知识库扩展模型所知信息，使用auto提高模型生成质量和复杂问题解决能力&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;code&gt;chatGLM-6B&lt;/code&gt;、&lt;code&gt;chatRWKV&lt;/code&gt;、&lt;code&gt;llama系列&lt;/code&gt;流式输出和输出过程中中断&lt;/li&gt; &#xA; &lt;li&gt;自动保存对话历史至浏览器（多用户同时使用不会冲突，&lt;code&gt;chatRWKV&lt;/code&gt;历史消息实现方式需使用string）&lt;/li&gt; &#xA; &lt;li&gt;对话历史管理（删除单条、清空）&lt;/li&gt; &#xA; &lt;li&gt;支持局域网、内网部署和多用户同时使用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;交流QQ群：LLM使用和综合讨论群162451840；知识库使用讨论群241773574(满)；Auto开发交流群744842245；&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/l15y/wenda/discussions&#34;&gt;discussions&lt;/a&gt; &lt;a href=&#34;https://pd.qq.com/s/ej03plxks&#34;&gt;QQ频道&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E9%97%BB%E8%BE%BE%E4%B8%80%E4%B8%AA%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E5%B9%B3%E5%8F%B0&#34;&gt;闻达：一个大规模语言模型调用平台&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2&#34;&gt;安装部署&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E5%90%84%E7%89%88%E6%9C%AC%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%AE%89%E8%A3%85%E8%AF%B4%E6%98%8E&#34;&gt;各版本功能及安装说明&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E6%87%92%E4%BA%BA%E5%8C%85&#34;&gt;懒人包&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E8%87%AA%E8%A1%8C%E5%AE%89%E8%A3%85&#34;&gt;自行安装&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#1%E5%AE%89%E8%A3%85%E5%BA%93&#34;&gt;1.安装库&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#2%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B&#34;&gt;2.下载模型&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#3%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE&#34;&gt;3.参数设置&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#auto&#34;&gt;Auto&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#auto-%E5%BC%80%E5%8F%91%E5%87%BD%E6%95%B0%E5%88%97%E8%A1%A8&#34;&gt;Auto 开发函数列表&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#auto-%E5%BC%80%E5%8F%91%E6%B6%89%E5%8F%8A%E4%BB%A3%E7%A0%81%E6%AE%B5&#34;&gt;Auto 开发涉及代码段&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E9%83%A8%E5%88%86%E5%86%85%E7%BD%AE-auto-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E&#34;&gt;部分内置 Auto 使用说明&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93&#34;&gt;知识库&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#rtst%E6%A8%A1%E5%BC%8F&#34;&gt;rtst模式&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#fess%E6%A8%A1%E5%BC%8F&#34;&gt;fess模式&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93%E8%B0%83%E8%AF%95&#34;&gt;知识库调试&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E6%B8%85%E6%B4%97%E7%9F%A5%E8%AF%86%E5%BA%93%E6%96%87%E4%BB%B6&#34;&gt;清洗知识库文件&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE&#34;&gt;模型配置&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#chatglm-6b&#34;&gt;chatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#chatrwkv&#34;&gt;chatRWKV&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#torch&#34;&gt;torch&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#cpp&#34;&gt;cpp&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#llama&#34;&gt;llama&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E5%9F%BA%E4%BA%8E%E6%9C%AC%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91&#34;&gt;基于本项目的二次开发&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#wenda-webui&#34;&gt;wenda-webui&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Created by https://github.com/ekalinin/github-markdown-toc --&gt; &#xA;&lt;!-- Added by: runner, at: Sun May 14 12:45:00 UTC 2023 --&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/setting.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/setting2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;安装部署&lt;/h2&gt; &#xA;&lt;h3&gt;各版本功能及安装说明&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;功能&lt;/th&gt; &#xA;   &lt;th&gt;Windows懒人包&lt;/th&gt; &#xA;   &lt;th&gt;自部署&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93&#34;&gt;知识库&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#rtst%E6%A8%A1%E5%BC%8F&#34;&gt;rtst模式&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;须下载模型text2vec-large-chinese&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93&#34;&gt;知识库&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#fess%E6%A8%A1%E5%BC%8F&#34;&gt;fess模式&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;须安装fess&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93&#34;&gt;知识库&lt;/a&gt; 网络模式&lt;/td&gt; &#xA;   &lt;td&gt;支持&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#auto&#34;&gt;Auto&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;全部支持，&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E9%83%A8%E5%88%86%E5%86%85%E7%BD%AEauto%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E&#34;&gt;部分内置Auto使用说明&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#chatglm-6b&#34;&gt;chatGLM-6B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;支持CUDA，须自行下载模型 。可自行安装组件以支持CPU&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RWKV &lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#torch&#34;&gt;torch&lt;/a&gt;版&lt;/td&gt; &#xA;   &lt;td&gt;全部功能支持，须自行下载模型。在安装vc后支持一键启动CUDA加速&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RWKV &lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#cpp&#34;&gt;cpp&lt;/a&gt;版&lt;/td&gt; &#xA;   &lt;td&gt;全部功能支持，须自行下载模型，也可使用内置脚本对torch版模型转换和量化。&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;replit&lt;/td&gt; &#xA;   &lt;td&gt;支持，须自行下载模型。&lt;/td&gt; &#xA;   &lt;td&gt;同上&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;chatglm130b api&lt;/td&gt; &#xA;   &lt;td&gt;支持，须设置自己的key&lt;/td&gt; &#xA;   &lt;td&gt;支持&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai api&lt;/td&gt; &#xA;   &lt;td&gt;支持，须设置自己的key&lt;/td&gt; &#xA;   &lt;td&gt;支持&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#llama&#34;&gt;llama&lt;/a&gt;.cpp&lt;/td&gt; &#xA;   &lt;td&gt;不支持&lt;/td&gt; &#xA;   &lt;td&gt;支持&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss&lt;/td&gt; &#xA;   &lt;td&gt;不支持&lt;/td&gt; &#xA;   &lt;td&gt;支持&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;懒人包&lt;/h3&gt; &#xA;&lt;p&gt;链接：&lt;a href=&#34;https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz&#34;&gt;https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;提取码：lyqz&lt;/p&gt; &#xA;&lt;p&gt;默认参数在6G显存设备上运行良好。最新版懒人版已集成一键更新功能，建议使用前更新。&lt;/p&gt; &#xA;&lt;p&gt;使用步骤（以glm6b模型为例）：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;下载懒人版主体和模型，模型可以用内置脚本从HF下载，也可以从网盘下载。&lt;/li&gt; &#xA; &lt;li&gt;如果没有安装&lt;code&gt;CUDA11.8&lt;/code&gt;，从网盘下载并安装。&lt;/li&gt; &#xA; &lt;li&gt;双击运行&lt;code&gt;运行GLM6B.bat&lt;/code&gt;。&lt;/li&gt; &#xA; &lt;li&gt;如果需要生成离线知识库，参考 &lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93&#34;&gt;知识库&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;自行安装&lt;/h3&gt; &#xA;&lt;p&gt;PS:一定要看&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;，里面对各功能有更详细的说明！！！&lt;/p&gt; &#xA;&lt;h4&gt;1.安装库&lt;/h4&gt; &#xA;&lt;p&gt;通用依赖：&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; 根据使用的 &lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#%E7%9F%A5%E8%AF%86%E5%BA%93&#34;&gt;知识库&lt;/a&gt;进行相应配置&lt;/p&gt; &#xA;&lt;h4&gt;2.下载模型&lt;/h4&gt; &#xA;&lt;p&gt;根据需要，下载对应模型。&lt;/p&gt; &#xA;&lt;p&gt;建议使用chatRWKV的RWKV-4-Raven-7B-v11，或chatGLM-6B。&lt;/p&gt; &#xA;&lt;h4&gt;3.参数设置&lt;/h4&gt; &#xA;&lt;p&gt;把&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;重命名为&lt;code&gt;config.yml&lt;/code&gt;，根据里面的参数说明，填写你的模型下载位置等信息&lt;/p&gt; &#xA;&lt;h2&gt;Auto&lt;/h2&gt; &#xA;&lt;p&gt;auto功能通过JavaScript脚本实现，使用油猴脚本或直接放到&lt;code&gt;autos&lt;/code&gt;目录的方式注入至程序，为闻达附加各种自动化功能。&lt;/p&gt; &#xA;&lt;h3&gt;Auto 开发函数列表&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;函数 （皆为异步调用）&lt;/th&gt; &#xA;   &lt;th&gt;功能&lt;/th&gt; &#xA;   &lt;th&gt;说明&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;send(s,keyword = &#34;&#34;,show=true)&lt;/td&gt; &#xA;   &lt;td&gt;发送信息至LLM，返回字符串为模型返回值&lt;/td&gt; &#xA;   &lt;td&gt;s：输入模型文本；keyword:聊天界面显示文本；show：是否在聊天界面显示&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;add_conversation(role, content)&lt;/td&gt; &#xA;   &lt;td&gt;添加会话信息&lt;/td&gt; &#xA;   &lt;td&gt;role：&#39;AI&#39;、&#39;user&#39;；content：字符串&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;save_history()&lt;/td&gt; &#xA;   &lt;td&gt;保存会话历史&lt;/td&gt; &#xA;   &lt;td&gt;对话完成后会自动保存，但手动添加的对话须手动保存&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;find(s, step = 1)&lt;/td&gt; &#xA;   &lt;td&gt;从知识库查找&lt;/td&gt; &#xA;   &lt;td&gt;返回json数组&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;find_dynamic(s,step=1,paraJson)&lt;/td&gt; &#xA;   &lt;td&gt;从动态知识库查找；参考闻达笔记Auto&lt;/td&gt; &#xA;   &lt;td&gt;paraJson：{libraryStategy:&#34;sogowx:3&#34;,maxItmes:2}&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;zsk(b=true)&lt;/td&gt; &#xA;   &lt;td&gt;开关知识库&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lsdh(b=true)&lt;/td&gt; &#xA;   &lt;td&gt;开关历史对话&lt;/td&gt; &#xA;   &lt;td&gt;打开知识库时应关闭历史&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;speak(s)&lt;/td&gt; &#xA;   &lt;td&gt;使用TTS引擎朗读文本。&lt;/td&gt; &#xA;   &lt;td&gt;调用系统引擎&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;copy(s)&lt;/td&gt; &#xA;   &lt;td&gt;使用浏览器&lt;code&gt;clipboard-write&lt;/code&gt;复制文本&lt;/td&gt; &#xA;   &lt;td&gt;需要相关权限&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Auto 开发涉及代码段&lt;/h3&gt; &#xA;&lt;p&gt;在左侧功能栏添加内容：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;功能.push({&#xA;    名称: &#34;名称&#34;,&#xA;    问题: async () =&amp;gt; {&#xA;        let answer=await send(app.问题)&#xA;        alert(answer)&#xA;    },&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;在下方选项卡添加内容：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;app.plugins.push({ icon: &#39;note-edit-outline&#39;, url: &#34;/static/wdnote/index.html&#34; })&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;在指定RTST知识库查找:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;find_in_memory = async (s, step, memory_name) =&amp;gt; {&#xA;   response = await fetch(&#34;/api/find_rtst_in_memory&#34;, {&#xA;      method: &#39;post&#39;,&#xA;      body: JSON.stringify({&#xA;         prompt: s,&#xA;         step: step,&#xA;         memory_name: memory_name&#xA;      }),&#xA;      headers: {&#xA;         &#39;Content-Type&#39;: &#39;application/json&#39;&#xA;      }&#xA;   })&#xA;   let json = await response.json()&#xA;   console.table(json)&#xA;   app.zhishiku = json&#xA;   return json&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;上传至指定RTST知识库:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;upload_rtst_zhishiku = async (title, txt,memory_name) =&amp;gt; {&#xA;   response = await fetch(&#34;/api/upload_rtst_zhishiku&#34;, {&#xA;      method: &#39;post&#39;,&#xA;      body: JSON.stringify({&#xA;         title: title,&#xA;         txt: txt,&#xA;         memory_name: memory_name&#xA;      }),&#xA;      headers: { &#39;Content-Type&#39;: &#39;application/json&#39; }&#xA;   })&#xA;   alert(await response.text())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;保存指定RTST知识库:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;save_rtst = async (memory_name) =&amp;gt; {&#xA;   response = await fetch(&#34;/api/save_rtst_zhishiku&#34;, {&#xA;      method: &#39;post&#39;,&#xA;      body: JSON.stringify({&#xA;         memory_name: memory_name&#xA;      }),&#xA;      headers: { &#39;Content-Type&#39;: &#39;application/json&#39; }&#xA;   })&#xA;   alert(await response.text())&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;访问SD_agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;response = await fetch(&#34;/api/sd_agent&#34;, {&#xA;   method: &#39;post&#39;,&#xA;   body: JSON.stringify({&#xA;         prompt: `((masterpiece, best quality)), photorealistic,` + Q,&#xA;         steps: 20,&#xA;         // sampler_name: &#34;DPM++ SDE Karras&#34;,&#xA;         negative_prompt: `paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), skin spots, acnes, skin blemishes, age spot, glans`&#xA;   }),&#xA;   headers: {&#xA;         &#39;Content-Type&#39;: &#39;application/json&#39;&#xA;   }&#xA;})&#xA;try {&#xA;   let json = await response.json()&#xA;   add_conversation(&#34;AI&#34;, &#39;![](data:image/png;base64,&#39; + json.images[0] + &#34;)&#34;)&#xA;} catch (error) {&#xA;   alert(&#34;连接SD API失败，请确认已开启agents库，并将SD API地址设置为127.0.0.1:786&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;部分内置 Auto 使用说明&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;文件名&lt;/th&gt; &#xA;   &lt;th&gt;功能&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0-write_article.js&lt;/td&gt; &#xA;   &lt;td&gt;写论文：根据题目或提纲写论文&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0-zsk.js&lt;/td&gt; &#xA;   &lt;td&gt;知识库增强和管理&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;face-recognition.js&lt;/td&gt; &#xA;   &lt;td&gt;纯浏览器端人脸检测：通过识别嘴巴开合，控制语音输入。因浏览器限制，仅本地或TLS下可用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QQ.js&lt;/td&gt; &#xA;   &lt;td&gt;QQ机器人:配置过程见文件开头注释&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;block_programming.js&lt;/td&gt; &#xA;   &lt;td&gt;猫猫也会的图块化编程:通过拖动图块实现简单Auto功能&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1-draw_use_SD_api.js&lt;/td&gt; &#xA;   &lt;td&gt;通过agents模块（见example.config.yml&lt;code&gt;&amp;lt;Library&amp;gt;&lt;/code&gt;）调用Stable Diffusion接口绘图&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;以上功能主要用于展示auto用法，进一步能力有待广大用户进一步发掘。 &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/auto1.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/auto2.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/auto3.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/auto4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/l15y/wenda/tree/main/autos&#34;&gt;auto例程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;知识库&lt;/h2&gt; &#xA;&lt;p&gt;知识库原理是在搜索后，生成一些提示信息插入到对话里面，知识库的数据就被模型知道了。&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#rtst%E6%A8%A1%E5%BC%8F&#34;&gt;rtst模式&lt;/a&gt;计算语义并在本地数据库中匹配；&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/#fess%E6%A8%A1%E5%BC%8F&#34;&gt;fess模式&lt;/a&gt;（相当于本地搜索引擎）、bing模式均调用搜索引擎搜索获取答案。&lt;/p&gt; &#xA;&lt;p&gt;为防止爆显存和受限于模型理解能力，插入的数据不能太长，所以有字数和条数限制，这一问题可通过知识库增强Auto解决。&lt;/p&gt; &#xA;&lt;p&gt;正常使用中，勾选右上角知识库即开启知识库。 &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/zsk1.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/zsk2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;有以下几种方案：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;rtst模式，sentence_transformers+faiss进行索引，支持预先构建索引和运行中构建。&lt;/li&gt; &#xA; &lt;li&gt;bing模式，cn.bing搜索，仅国内可用&lt;/li&gt; &#xA; &lt;li&gt;bingsite模式，cn.bing站内搜索，仅国内可用&lt;/li&gt; &#xA; &lt;li&gt;fess模式，本地部署的&lt;a href=&#34;https://github.com/codelibs/fess&#34;&gt;fess搜索&lt;/a&gt;，并进行关键词提取&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;rtst模式&lt;/h3&gt; &#xA;&lt;p&gt;sentence_transformers+faiss进行索引、匹配，并连同上下文返回。目前支持txt和pdf格式。&lt;/p&gt; &#xA;&lt;p&gt;支持预先构建索引和运行中构建，其中，预先构建索引强制使用&lt;code&gt;cuda&lt;/code&gt;，运行中构建根据&lt;code&gt;config.yml&lt;/code&gt;(复制&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;)中&lt;code&gt;rtst&lt;/code&gt;段的&lt;code&gt;device(embedding运行设备)&lt;/code&gt;决定，对于显存小于12G的用户建议使用&lt;code&gt;CPU&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;Windows预先构建索引运行：&lt;code&gt;plugins/buils_rtst_default_index.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;Linux直接使用wenda环境执行 &lt;code&gt;python plugins/gen_data_st.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;需下载模型&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt;置于model文件夹，并将txt格式语料置于txt文件夹。&lt;/p&gt; &#xA;&lt;h3&gt;fess模式&lt;/h3&gt; &#xA;&lt;p&gt;在本机使用默认端口安装fess后可直接运行。否则需修改&lt;code&gt;config.yml&lt;/code&gt;(复制&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;)中&lt;code&gt;fess_host&lt;/code&gt;的&lt;code&gt;127.0.0.1:8080&lt;/code&gt;为相应值。&lt;a href=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/install_fess.md&#34;&gt;FESS安装教程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;知识库调试&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/zsk-test.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/zsk-glm.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/zsk-rwkv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;清洗知识库文件&lt;/h3&gt; &#xA;&lt;p&gt;安装 &lt;a href=&#34;https://u.tools/&#34;&gt;utool&lt;/a&gt; 工具，uTools 是一个极简、插件化的桌面软件，可以安装各种使用 nodejs 开发的插件。您可以使用插件对闻达的知识库进行数据清洗。请自行安装以下推荐插件：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;插件“解散文件夹”，用于将子目录的文件移动到根目录，并删除所有子目录。&lt;/li&gt; &#xA; &lt;li&gt;插件“重复文件查找”，用于删除目录中的重复文件，原理是对比文件 md5。&lt;/li&gt; &#xA; &lt;li&gt;插件“文件批量重命名”，用于使用正则匹配和修改文件名，并将分类后的文件名进行知识库的分区操作。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型配置&lt;/h2&gt; &#xA;&lt;h3&gt;chatGLM-6B&lt;/h3&gt; &#xA;&lt;p&gt;运行：&lt;code&gt;run_GLM6B.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;模型位置等参数：修改&lt;code&gt;config.yml&lt;/code&gt;(复制&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;)。&lt;/p&gt; &#xA;&lt;p&gt;默认参数在GTX1660Ti（6G显存）上运行良好。&lt;/p&gt; &#xA;&lt;h3&gt;chatRWKV&lt;/h3&gt; &#xA;&lt;p&gt;支持torch和cpp两种后端实现，运行：&lt;code&gt;run_rwkv.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;模型位置等参数：见&lt;code&gt;config.yml&lt;/code&gt;(复制&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;)。&lt;/p&gt; &#xA;&lt;h4&gt;torch&lt;/h4&gt; &#xA;&lt;p&gt;可使用内置脚本对模型量化，运行：&lt;code&gt;cov_torch_rwkv.bat&lt;/code&gt;。此操作可以加快启动速度。&lt;/p&gt; &#xA;&lt;p&gt;在安装vc后支持一键启动CUDA加速，运行：&lt;code&gt;run_rwkv_with_vc.bat&lt;/code&gt;。强烈建议安装！！！&lt;/p&gt; &#xA;&lt;h4&gt;cpp&lt;/h4&gt; &#xA;&lt;p&gt;可使用内置脚本对torch版模型转换和量化。 运行：&lt;code&gt;cov_ggml_rwkv.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;设置strategy诸如&#34;Q8_0-&amp;gt;8&#34;即支持量化在cpu运行，速度较慢，没有显卡或者没有nvidia显卡的用户使用。&lt;/p&gt; &#xA;&lt;p&gt;注意：默认windows版本文件为AVX2，默认Liunx版本文件是在debian sid编译的，其他linux发行版本未知。&lt;/p&gt; &#xA;&lt;p&gt;可以查看：&lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp&#34;&gt;saharNooby/rwkv.cpp&lt;/a&gt;，下载其他版本，或者自行编译。&lt;/p&gt; &#xA;&lt;h3&gt;llama&lt;/h3&gt; &#xA;&lt;p&gt;运行：&lt;code&gt;run_llama.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;模型位置等参数：见&lt;code&gt;config.yml&lt;/code&gt;(复制&lt;a href=&#34;https://github.com/l15y/wenda/raw/main/example.config.yml&#34;&gt;example.config.yml&lt;/a&gt;)。&lt;/p&gt; &#xA;&lt;h1&gt;基于本项目的二次开发&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/AlanLee1996/wenda-webui&#34;&gt;wenda-webui&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;项目调用闻达的 api 接口实现类似于 new bing 的功能。 技术栈：vue3 + element-plus + ts &lt;img src=&#34;https://raw.githubusercontent.com/wenda-LLM/wenda/main/imgs/webui.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#l15y/wenda&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=l15y/wenda&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tailwindlabs/tailwindcss.com</title>
    <updated>2023-05-20T01:37:31Z</updated>
    <id>tag:github.com,2023-05-20:/tailwindlabs/tailwindcss.com</id>
    <link href="https://github.com/tailwindlabs/tailwindcss.com" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Tailwind CSS documentation website.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tailwind CSS Documentation&lt;/h1&gt; &#xA;&lt;p&gt;Tailwind CSS uses &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; for its documentation.&lt;/p&gt; &#xA;&lt;p&gt;To run the project locally, first install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the development server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is not licensed under an open-source license and is the intellectual property of Tailwind Labs Inc. The source is available only as an educational resource and to accept fixes for minor mistakes.&lt;/p&gt;</summary>
  </entry>
</feed>