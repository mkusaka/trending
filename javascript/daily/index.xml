<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub JavaScript Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-01T01:34:12Z</updated>
  <subtitle>Daily Trending of JavaScript in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>plasma-umass/scalene</title>
    <updated>2023-09-01T01:34:12Z</updated>
    <id>tag:github.com,2023-09-01:/plasma-umass/scalene</id>
    <link href="https://github.com/plasma-umass/scalene" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scalene: a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/plasma-umass/scalene/raw/master/docs/scalene-image.png&#34; alt=&#34;scalene&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Scalene: a Python CPU+GPU+memory profiler with AI-powered optimization proposals&lt;/h1&gt; &#xA;&lt;p&gt;by &lt;a href=&#34;https://emeryberger.com&#34;&gt;Emery Berger&lt;/a&gt;, &lt;a href=&#34;https://samstern.me/&#34;&gt;Sam Stern&lt;/a&gt;, and &lt;a href=&#34;https://github.com/jaltmayerpizzorno&#34;&gt;Juan Altmayer Pizzorno&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg&#34;&gt;&lt;img src=&#34;https://github.com/plasma-umass/scalene/raw/master/docs/images/slack-logo.png&#34; alt=&#34;Scalene community Slack&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://join.slack.com/t/scaleneprofil-jge3234/shared_invite/zt-110vzrdck-xJh5d4gHnp5vKXIjYD3Uwg&#34;&gt;Scalene community Slack&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/scalene/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/scalene.svg?sanitize=true&#34; alt=&#34;PyPI Latest Release&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://anaconda.org/conda-forge/scalene&#34;&gt;&lt;img src=&#34;https://anaconda.org/conda-forge/scalene/badges/version.svg?sanitize=true&#34; alt=&#34;Anaconda-Server Badge&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://pepy.tech/project/scalene&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/scalene&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/scalene&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/scalene/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/scalene.svg?style=flat-square&#34; alt=&#34;Python versions&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/plasma-umass/scalene&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/plasma-umass/scalene/raw/master/docs/Ozsvald-tweet.png&#34; alt=&#34;Ozsvald tweet&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(tweet from Ian Ozsvald, author of &lt;a href=&#34;https://smile.amazon.com/High-Performance-Python-Performant-Programming/dp/1492055026/ref=sr_1_1?crid=texbooks&#34;&gt;&lt;em&gt;High Performance Python&lt;/em&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/plasma-umass/scalene/raw/master/docs/semantic-scholar-success.png&#34; alt=&#34;Semantic Scholar success story&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About Scalene&lt;/h2&gt; &#xA;&lt;p&gt;Scalene is a high-performance CPU, GPU &lt;em&gt;and&lt;/em&gt; memory profiler for Python that does a number of things that other Python profilers do not and cannot do. It runs orders of magnitude faster than many other profilers while delivering far more detailed information. It is also the first profiler ever to incorporate AI-powered proposed optimizations. To enable these, you need to enter an &lt;a href=&#34;https://openai.com/api/&#34;&gt;OpenAI key&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img width=&#34;487&#34; alt=&#34;Scalene advanced options&#34; src=&#34;https://user-images.githubusercontent.com/1612723/211639253-ec926b38-3efe-4a20-8514-e10dde94ec01.png&#34;&gt; &#xA;&lt;p&gt;Once a valid key is entered, click on the lightning bolt (⚡) beside any line or the explosion (💥) for an entire region of code to generate a proposed optimization. Click on a proposed optimization to copy it to the clipboard.&lt;/p&gt; &#xA;&lt;img width=&#34;571&#34; alt=&#34;example proposed optimization&#34; src=&#34;https://user-images.githubusercontent.com/1612723/211639968-37cf793f-3290-43d1-9282-79e579558388.png&#34;&gt; &#xA;&lt;p&gt;You can click as many times as you like on the lightning bolt or explosion, and it will generate different suggested optimizations. Your mileage may vary, but in some cases, the suggestions are quite impressive (e.g., order-of-magnitude improvements).&lt;/p&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;h4&gt;Installing Scalene:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;python3 -m pip install -U scalene&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;conda install -c conda-forge scalene&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Scalene:&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Commonly used options: &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;scalene your_prog.py                             # full profile (outputs to web interface)&#xA;python3 -m scalene your_prog.py                  # equivalent alternative&#xA;&#xA;scalene --cli your_prog.py                       # use the command-line only (no web interface)&#xA;&#xA;scalene --cpu your_prog.py                       # only profile CPU&#xA;scalene --cpu --gpu your_prog.py                 # only profile CPU and GPU&#xA;scalene --cpu --gpu --memory your_prog.py        # profile everything (same as no options)&#xA;&#xA;scalene --reduced-profile your_prog.py           # only profile lines with significant usage&#xA;scalene --profile-interval 5.0 your_prog.py      # output a new profile every five seconds&#xA;&#xA;scalene (Scalene options) --- your_prog.py (...) # use --- to tell Scalene to ignore options after that point&#xA;scalene --help                                   # lists all options&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Using Scalene programmatically in your code: &lt;/summary&gt; &#xA; &lt;p&gt;Invoke using &lt;code&gt;scalene&lt;/code&gt; as above and then:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from scalene import scalene_profiler&#xA;&#xA;# Turn profiling on&#xA;scalene_profiler.start()&#xA;&#xA;# Turn profiling off&#xA;scalene_profiler.stop()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Using Scalene to profile only specific functions via &lt;code&gt;@profile&lt;/code&gt;: &lt;/summary&gt; &#xA; &lt;p&gt;Just preface any functions you want to profile with the &lt;code&gt;@profile&lt;/code&gt; decorator and run it with Scalene:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;# do not import profile!&#xA;&#xA;@profile&#xA;def slow_function():&#xA;    import time&#xA;    time.sleep(3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Web-based GUI&lt;/h4&gt; &#xA;&lt;p&gt;Scalene has both a CLI and a web-based GUI &lt;a href=&#34;http://plasma-umass.org/scalene-gui/&#34;&gt;(demo here)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default, once Scalene has profiled your program, it will open a tab in a web browser with an interactive user interface (all processing is done locally). Hover over bars to see breakdowns of CPU and memory consumption, and click on underlined column headers to sort the columns. The generated file &lt;code&gt;profile.html&lt;/code&gt; is self-contained and can be saved for later use.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/scalene-gui-example-full.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/scalene-gui-example.png&#34; alt=&#34;Scalene web GUI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Scalene Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Scalene talk (PyCon US 2021)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/5iEf-_7mM1k&#34;&gt;This talk&lt;/a&gt; presented at PyCon 2021 walks through Scalene&#39;s advantages and how to use it to debug the performance of an application (and provides some technical details on its internals). We highly recommend watching this video!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/5iEf-_7mM1k&#34; title=&#34;Scalene presentation at PyCon 2021&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/scalene-video-img.png&#34; alt=&#34;Scalene presentation at PyCon 2021&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fast and Accurate&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Scalene is &lt;strong&gt;&lt;em&gt;fast&lt;/em&gt;&lt;/strong&gt;. It uses sampling instead of instrumentation or relying on Python&#39;s tracing facilities. Its overhead is typically no more than 10-20% (and often less).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Scalene is &lt;strong&gt;accurate&lt;/strong&gt;. We tested CPU profiler accuracy and found that Scalene is among the most accurate profilers, correctly measuring time taken.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/plasma-umass/scalene/raw/master/docs/cpu-accuracy-comparison.png&#34; alt=&#34;Profiler accuracy&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scalene performs profiling &lt;strong&gt;&lt;em&gt;at the line level&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;&lt;em&gt;per function&lt;/em&gt;&lt;/strong&gt;, pointing to the functions and the specific lines of code responsible for the execution time in your program.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CPU profiling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scalene &lt;strong&gt;separates out time spent in Python from time in native code&lt;/strong&gt; (including libraries). Most Python programmers aren&#39;t going to optimize the performance of native code (which is usually either in the Python implementation or external libraries), so this helps developers focus their optimization efforts on the code they can actually improve.&lt;/li&gt; &#xA; &lt;li&gt;Scalene &lt;strong&gt;highlights hotspots&lt;/strong&gt; (code accounting for significant percentages of CPU time or memory allocation) in red, making them even easier to spot.&lt;/li&gt; &#xA; &lt;li&gt;Scalene also separates out &lt;strong&gt;system time&lt;/strong&gt;, making it easy to find I/O bottlenecks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;GPU profiling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scalene reports &lt;strong&gt;GPU time&lt;/strong&gt; (currently limited to NVIDIA-based systems).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Memory profiling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scalene &lt;strong&gt;profiles memory usage&lt;/strong&gt;. In addition to tracking CPU usage, Scalene also points to the specific lines of code responsible for memory growth. It accomplishes this via an included specialized memory allocator.&lt;/li&gt; &#xA; &lt;li&gt;Scalene separates out the percentage of &lt;strong&gt;memory consumed by Python code vs. native code&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Scalene produces &lt;strong&gt;&lt;em&gt;per-line&lt;/em&gt; memory profiles&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Scalene &lt;strong&gt;identifies lines with likely memory leaks&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Scalene &lt;strong&gt;profiles &lt;em&gt;copying volume&lt;/em&gt;&lt;/strong&gt;, making it easy to spot inadvertent copying, especially due to crossing Python/library boundaries (e.g., accidentally converting &lt;code&gt;numpy&lt;/code&gt; arrays into Python arrays, and vice versa).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scalene can produce &lt;strong&gt;reduced profiles&lt;/strong&gt; (via &lt;code&gt;--reduced-profile&lt;/code&gt;) that only report lines that consume more than 1% of CPU or perform at least 100 allocations.&lt;/li&gt; &#xA; &lt;li&gt;Scalene supports &lt;code&gt;@profile&lt;/code&gt; decorators to profile only specific functions.&lt;/li&gt; &#xA; &lt;li&gt;When Scalene is profiling a program launched in the background (via &lt;code&gt;&amp;amp;&lt;/code&gt;), you can &lt;strong&gt;suspend and resume profiling&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Comparison to Other Profilers&lt;/h1&gt; &#xA;&lt;h2&gt;Performance and Features&lt;/h2&gt; &#xA;&lt;p&gt;Below is a table comparing the &lt;strong&gt;performance and features&lt;/strong&gt; of various profilers to Scalene.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/profiler-comparison.png&#34; alt=&#34;Performance and feature comparison&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Slowdown&lt;/strong&gt;: the slowdown when running a benchmark from the Pyperformance suite. Green means less than 2x overhead. Scalene&#39;s overhead is just a 35% slowdown.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Scalene has all of the following features, many of which only Scalene supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lines or functions&lt;/strong&gt;: does the profiler report information only for entire functions, or for every line -- Scalene does both.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unmodified Code&lt;/strong&gt;: works on unmodified code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: supports Python threads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiprocessing&lt;/strong&gt;: supports use of the &lt;code&gt;multiprocessing&lt;/code&gt; library -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python vs. C time&lt;/strong&gt;: breaks out time spent in Python vs. native code (e.g., libraries) -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;System time&lt;/strong&gt;: breaks out system time (e.g., sleeping or performing I/O) -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Profiles memory&lt;/strong&gt;: reports memory consumption per line / function&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: reports time spent on an NVIDIA GPU (if present) -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory trends&lt;/strong&gt;: reports memory use over time per line / function -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Copy volume&lt;/strong&gt;: reports megabytes being copied per second -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Detects leaks&lt;/strong&gt;: automatically pinpoints lines responsible for likely memory leaks -- &lt;em&gt;Scalene only&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Output&lt;/h2&gt; &#xA;&lt;p&gt;If you include the &lt;code&gt;--cli&lt;/code&gt; option, Scalene prints annotated source code for the program being profiled (as text, JSON (&lt;code&gt;--json&lt;/code&gt;), or HTML (&lt;code&gt;--html&lt;/code&gt;)) and any modules it uses in the same directory or subdirectories (you can optionally have it &lt;code&gt;--profile-all&lt;/code&gt; and only include files with at least a &lt;code&gt;--cpu-percent-threshold&lt;/code&gt; of time). Here is a snippet from &lt;code&gt;pystone.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/plasma-umass/scalene/master/docs/images/sample-profile-pystone.png&#34; alt=&#34;Example profile&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory usage at the top&lt;/strong&gt;: Visualized by &#34;sparklines&#34;, memory consumption over the runtime of the profiled code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Time Python&#34;&lt;/strong&gt;: How much time was spent in Python code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;native&#34;&lt;/strong&gt;: How much time was spent in non-Python code (e.g., libraries written in C/C++).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;system&#34;&lt;/strong&gt;: How much time was spent in the system (e.g., I/O).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;GPU&#34;&lt;/strong&gt;: (not shown here) How much time spent on the GPU, if your system has an NVIDIA GPU installed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Memory Python&#34;&lt;/strong&gt;: How much of the memory allocation happened on the Python side of the code, as opposed to in non-Python code (e.g., libraries written in C/C++).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;net&#34;&lt;/strong&gt;: Positive net memory numbers indicate total memory allocation in megabytes; negative net memory numbers indicate memory reclamation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;timeline / %&#34;&lt;/strong&gt;: Visualized by &#34;sparklines&#34;, memory consumption generated by this line over the program runtime, and the percentages of total memory activity this line represents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&#34;Copy (MB/s)&#34;&lt;/strong&gt;: The amount of megabytes being copied per second (see &#34;About Scalene&#34;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Scalene&lt;/h2&gt; &#xA;&lt;p&gt;The following command runs Scalene on a provided example program.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;scalene test/testme.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to see all Scalene&#39;s options (available by running with &lt;code&gt;--help&lt;/code&gt;) &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;    % scalene --help&#xA;     usage: scalene [-h] [--outfile OUTFILE] [--html] [--reduced-profile]&#xA;                    [--profile-interval PROFILE_INTERVAL] [--cpu-only]&#xA;                    [--profile-all] [--profile-only PROFILE_ONLY]&#xA;                    [--use-virtual-time]&#xA;                    [--cpu-percent-threshold CPU_PERCENT_THRESHOLD]&#xA;                    [--cpu-sampling-rate CPU_SAMPLING_RATE]&#xA;                    [--malloc-threshold MALLOC_THRESHOLD]&#xA;     &#xA;     Scalene: a high-precision CPU and memory profiler.&#xA;     https://github.com/plasma-umass/scalene&#xA;     &#xA;     command-line:&#xA;        % scalene [options] yourprogram.py&#xA;     or&#xA;        % python3 -m scalene [options] yourprogram.py&#xA;     &#xA;     in Jupyter, line mode:&#xA;        %scrun [options] statement&#xA;     &#xA;     in Jupyter, cell mode:&#xA;        %%scalene [options]&#xA;        code...&#xA;        code...&#xA;     &#xA;     optional arguments:&#xA;       -h, --help            show this help message and exit&#xA;       --outfile OUTFILE     file to hold profiler output (default: stdout)&#xA;       --html                output as HTML (default: text)&#xA;       --reduced-profile     generate a reduced profile, with non-zero lines only (default: False)&#xA;       --profile-interval PROFILE_INTERVAL&#xA;                             output profiles every so many seconds (default: inf)&#xA;       --cpu-only            only profile CPU time (default: profile CPU, memory, and copying)&#xA;       --profile-all         profile all executed code, not just the target program (default: only the target program)&#xA;       --profile-only PROFILE_ONLY&#xA;                             profile only code in filenames that contain the given strings, separated by commas (default: no restrictions)&#xA;       --use-virtual-time    measure only CPU time, not time spent in I/O or blocking (default: False)&#xA;       --cpu-percent-threshold CPU_PERCENT_THRESHOLD&#xA;                             only report profiles with at least this percent of CPU time (default: 1%)&#xA;       --cpu-sampling-rate CPU_SAMPLING_RATE&#xA;                             CPU sampling rate (default: every 0.01s)&#xA;       --malloc-threshold MALLOC_THRESHOLD&#xA;                             only report profiles with at least this many allocations (default: 100)&#xA;     &#xA;     When running Scalene in the background, you can suspend/resume profiling&#xA;     for the process ID that Scalene reports. For example:&#xA;     &#xA;        % python3 -m scalene [options] yourprogram.py &amp;amp;&#xA;      Scalene now profiling process 12345&#xA;        to suspend profiling: python3 -m scalene.profile --off --pid 12345&#xA;        to resume profiling:  python3 -m scalene.profile --on  --pid 12345&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Scalene with Jupyter&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Instructions for installing and using Scalene with Jupyter notebooks &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/plasma-umass/scalene/blob/master/docs/scalene-demo.ipynb&#34;&gt;This notebook&lt;/a&gt; illustrates the use of Scalene in Jupyter.&lt;/p&gt; &#xA; &lt;p&gt;Installation:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;!pip install scalene&#xA;%load_ext scalene&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Line mode:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;%scrun [options] statement&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Cell mode:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;%%scalene [options]&#xA;code...&#xA;code...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Using &lt;code&gt;pip&lt;/code&gt; (Mac OS X, Linux, Windows, and WSL2)&lt;/summary&gt; &#xA; &lt;p&gt;Scalene is distributed as a &lt;code&gt;pip&lt;/code&gt; package and works on Mac OS X, Linux (including Ubuntu in &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/wsl2-index&#34;&gt;Windows WSL2&lt;/a&gt;) and (with limitations) Windows platforms. (&lt;strong&gt;Note&lt;/strong&gt;: the Windows version isn&#39;t yet complete; it currently only supports CPU and GPU profiling, but not memory profiling.)&lt;/p&gt; &#xA; &lt;p&gt;You can install it as follows:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;  % pip install -U scalene&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;or&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;  % python3 -m pip install -U scalene&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You may need to install some packages first.&lt;/p&gt; &#xA; &lt;p&gt;See &lt;a href=&#34;https://stackoverflow.com/a/19344978/4954434&#34;&gt;https://stackoverflow.com/a/19344978/4954434&lt;/a&gt; for full instructions for all Linux flavors.&lt;/p&gt; &#xA; &lt;p&gt;For Ubuntu/Debian:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;  % sudo apt install git python3-all-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Using &lt;code&gt;Homebrew&lt;/code&gt; (Mac OS X)&lt;/summary&gt; &#xA; &lt;p&gt;As an alternative to &lt;code&gt;pip&lt;/code&gt;, you can use Homebrew to install the current version of Scalene from this repository:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;  % brew tap plasma-umass/scalene&#xA;  % brew install --head plasma-umass/scalene/scalene&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;On ArchLinux&lt;/summary&gt; &#xA; &lt;p&gt;You can install Scalene on Arch Linux via the &lt;a href=&#34;https://aur.archlinux.org/packages/python-scalene-git/&#34;&gt;AUR package&lt;/a&gt;. Use your favorite AUR helper, or manually download the &lt;code&gt;PKGBUILD&lt;/code&gt; and run &lt;code&gt;makepkg -cirs&lt;/code&gt; to build. Note that this will place &lt;code&gt;libscalene.so&lt;/code&gt; in &lt;code&gt;/usr/lib&lt;/code&gt;; modify the below usage instructions accordingly.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Asked Questions&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Can I use Scalene with PyTest? &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes! You can run it as follows (for example):&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;python3 -m scalene --- -m pytest your_test.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Is there any way to get shorter profiles or do more targeted profiling? &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes! There are several options:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Use &lt;code&gt;--reduced-profile&lt;/code&gt; to include only lines and files with memory/CPU/GPU activity.&lt;/li&gt; &#xA;  &lt;li&gt;Use &lt;code&gt;--profile-only&lt;/code&gt; to include only filenames containing specific strings (as in, &lt;code&gt;--profile-only foo,bar,baz&lt;/code&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;Decorate functions of interest with &lt;code&gt;@profile&lt;/code&gt; to have Scalene report &lt;em&gt;only&lt;/em&gt; those functions.&lt;/li&gt; &#xA;  &lt;li&gt;Turn profiling on and off programmatically by importing Scalene (&lt;code&gt;import scalene&lt;/code&gt;) and then turning profiling on and off via &lt;code&gt;scalene_profiler.start()&lt;/code&gt; and &lt;code&gt;scalene_profiler.stop()&lt;/code&gt;. By default, Scalene runs with profiling on, so to delay profiling until desired, use the &lt;code&gt;--off&lt;/code&gt; command-line option (&lt;code&gt;python3 -m scalene --off yourprogram.py&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; How do I run Scalene in PyCharm? &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; In PyCharm, you can run Scalene at the command line by opening the terminal at the bottom of the IDE and running a Scalene command (e.g., &lt;code&gt;python -m scalene &amp;lt;your program&amp;gt;&lt;/code&gt;). Use the options &lt;code&gt;--cli&lt;/code&gt;, &lt;code&gt;--html&lt;/code&gt;, and &lt;code&gt;--outfile &amp;lt;your output.html&amp;gt;&lt;/code&gt; to generate an HTML file that you can then view in the IDE.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; How do I use Scalene with Django? &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Pass in the &lt;code&gt;--noreload&lt;/code&gt; option (see &lt;a href=&#34;https://github.com/plasma-umass/scalene/issues/178&#34;&gt;https://github.com/plasma-umass/scalene/issues/178&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; How do I use Scalene with PyTorch on the Mac? &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Scalene works with PyTorch version 1.5.1 on Mac OS X. There&#39;s a bug in newer versions of PyTorch (&lt;a href=&#34;https://github.com/pytorch/pytorch/issues/57185&#34;&gt;https://github.com/pytorch/pytorch/issues/57185&lt;/a&gt;) that interferes with Scalene (discussion here: &lt;a href=&#34;https://github.com/plasma-umass/scalene/issues/110&#34;&gt;https://github.com/plasma-umass/scalene/issues/110&lt;/a&gt;), but only on Macs.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Technical Information&lt;/h1&gt; &#xA;&lt;p&gt;For details about how Scalene works, please see the following paper, which won the Jay Lepreau Best Paper Award at &lt;a href=&#34;https://www.usenix.org/conference/osdi23/presentation/berger&#34;&gt;OSDI 2023&lt;/a&gt;: &lt;a href=&#34;https://arxiv.org/pdf/2212.07597&#34;&gt;Triangulating Python Performance Issues with Scalene&lt;/a&gt;. (Note that this paper does not include information about the AI-driven proposed optimizations.)&lt;/p&gt; &#xA;&lt;h1&gt;Success Stories&lt;/h1&gt; &#xA;&lt;p&gt;If you use Scalene to successfully debug a performance problem, please &lt;a href=&#34;https://github.com/plasma-umass/scalene/issues/58&#34;&gt;add a comment to this issue&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Logo created by &lt;a href=&#34;https://www.linkedin.com/in/sophia-berger/&#34;&gt;Sophia Berger&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This material is based upon work supported by the National Science Foundation under Grant No. 1955610. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Shaunwei/RealChar</title>
    <updated>2023-09-01T01:34:12Z</updated>
    <id>tag:github.com,2023-09-01:/Shaunwei/RealChar</id>
    <link href="https://github.com/Shaunwei/RealChar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🎙️🤖Create, Customize and Talk to your AI Character/Companion in Realtime (All in One Codebase!). Have a natural seamless conversation with AI everywhere (mobile, web and terminal) using LLM OpenAI GPT3.5/4, Anthropic Claude2, Chroma Vector DB, Whisper Speech2Text, ElevenLabs Text2Speech🎙️🤖&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://storage.googleapis.com/assistly/static/realchar/realchar.svg?sanitize=true&#34; height=&#34;24px&#34; style=&#34;padding-top:4px&#34;&gt;RealChar. - Your Realtime AI Character&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://storage.googleapis.com/assistly/static/realchar/logo.png&#34; alt=&#34;RealChar-logo&#34; width=&#34;80%&#34; style=&#34;padding: 40px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🎙️🤖&lt;em&gt;Create, customize and talk to your AI Character/Companion in realtime&lt;/em&gt;🎙️🤖 &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/e4AYNnFg2F&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/discord-join%20chat-blue.svg?style=for-the-badge&#34; alt=&#34;Join our Discord&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/agishaun&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/twitter/follow/agishaun?style=for-the-badge&#34; height=&#34;20&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/Shaunwei/RealChar&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/stars/Shaunwei/RealChar?style=for-the-badge&amp;amp;color=gold&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Shaunwei/RealChar/commits/main&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/last-commit/Shaunwei/RealChar/main?style=for-the-badge&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/Shaunwei/RealChar/raw/main/README.md&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=license&amp;amp;message=MIT&amp;amp;color=green&amp;amp;style=for-the-badge&#34; alt=&#34;License&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://hub.docker.com/repository/docker/shaunly/real_char/general&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Docker Pulls&#34; src=&#34;https://img.shields.io/docker/pulls/shaunly/real_char?style=for-the-badge&#34; height=&#34;20&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;✨ Demo&lt;/h2&gt; &#xA;&lt;p&gt;Try our site at &lt;a href=&#34;https://realchar.ai/&#34;&gt;RealChar.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(We are also beta-testing our iOS mobile app📱! Sign up &lt;a href=&#34;https://testflight.apple.com/join/JA6p9sZQ&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Not sure how to pronounce RealChar? Listen to this 👉 &lt;a href=&#34;https://github.com/Shaunwei/RealChar/assets/6148473/45d4773c-eb4f-41e5-a162-f9513d650b76&#34;&gt;audip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo 1 - with AI Elon about cage fight!&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Shaunwei/RealChar/assets/5101573/5de0b023-6cf3-4947-84cb-596f429d109e&#34;&gt;https://github.com/Shaunwei/RealChar/assets/5101573/5de0b023-6cf3-4947-84cb-596f429d109e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo 2 - with AI Raiden about AI and &#34;real&#34; memory&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Shaunwei/RealChar/assets/5101573/62a1f3d1-1166-4254-9119-97647be52c42&#34;&gt;https://github.com/Shaunwei/RealChar/assets/5101573/62a1f3d1-1166-4254-9119-97647be52c42&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Demo settings: Web, GPT4, ElevenLabs with voice clone, Chroma, Google Speech to Text&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎯 Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy to use&lt;/strong&gt;: No coding required to create your own AI character.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: You can customize your AI character&#39;s personality, background, and even voice&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Realtime&lt;/strong&gt;: Talk to or message your AI character in realtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Platform&lt;/strong&gt;: You can talk to your AI character on web, terminal and mobile(Yes. we open source our mobile app)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Most up-to-date AI&lt;/strong&gt;: We use the most up-to-date AI technology to power your AI character, including OpenAI, Anthropic Claude 2, Chroma, Whisper, ElevenLabs, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modular&lt;/strong&gt;: You can easily swap out different modules to customize your flow. Less opinionated, more flexible. Great project to start your AI Engineering journey.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔬 Tech stack&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://storage.googleapis.com/assistly/static/realchar/techstackv003.jpeg&#34; alt=&#34;RealChar-tech-stack&#34; width=&#34;100%&#34; style=&#34;padding: 20px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Web&lt;/strong&gt;: &lt;a href=&#34;https://react.dev/&#34;&gt;React JS&lt;/a&gt;, &lt;a href=&#34;http://vanilla-js.com/&#34;&gt;Vanilla JS&lt;/a&gt;, &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API&#34;&gt;WebSockets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Mobile&lt;/strong&gt;: &lt;a href=&#34;https://developer.apple.com/swift/&#34;&gt;Swift&lt;/a&gt;, &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API&#34;&gt;WebSockets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Backend&lt;/strong&gt;: &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt;, &lt;a href=&#34;https://www.sqlite.org/index.html&#34;&gt;SQLite&lt;/a&gt;, &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Data Ingestion&lt;/strong&gt;: &lt;a href=&#34;https://www.llamaindex.ai/&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://www.trychroma.com/&#34;&gt;Chroma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;LLM Orchestration&lt;/strong&gt;: &lt;a href=&#34;https://langchain.com/&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://www.trychroma.com/&#34;&gt;Chroma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;LLM&lt;/strong&gt;: &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI GPT3.5/4&lt;/a&gt;, &lt;a href=&#34;https://docs.anthropic.com/claude/docs/getting-started-with-claude&#34;&gt;Anthropic Claude 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Speech to Text&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Local Whisper&lt;/a&gt;, &lt;a href=&#34;https://platform.openai.com/docs/api-reference/audio&#34;&gt;OpenAI Whisper API&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/speech-to-text/docs#docs&#34;&gt;Google Speech to Text&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Text to Speech&lt;/strong&gt;: &lt;a href=&#34;https://beta.elevenlabs.io/&#34;&gt;ElevenLabs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;✅&lt;strong&gt;Voice Clone&lt;/strong&gt;: &lt;a href=&#34;https://beta.elevenlabs.io/voice-lab&#34;&gt;ElevenLabs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📚 Comparison with existing products&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://storage.googleapis.com/assistly/static/realchar/compare.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;👨‍🚀 Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before you begin setting up this project, please ensure you have completed the following tasks:&lt;/p&gt; &#xA;&lt;h3&gt;0. Setup Tutorial&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Q16ZH3kJWxw&#34;&gt;Tutorial - YouTuBe&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. LLM - OpenAI API Token&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; This application utilizes the OpenAI API to access its powerful language model capabilities. In order to use the OpenAI API, you will need to obtain an API token. &#xA; &lt;p&gt;To get your OpenAI API token, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Go to the &lt;a href=&#34;https://beta.openai.com/signup/&#34;&gt;OpenAI website&lt;/a&gt; and sign up for an account if you haven&#39;t already.&lt;/li&gt; &#xA;  &lt;li&gt;Once you&#39;re logged in, navigate to the &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;API keys page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Generate a new API key by clicking on the &#34;Create API Key&#34; button.&lt;/li&gt; &#xA;  &lt;li&gt;Copy the API key and store it safely.&lt;/li&gt; &#xA;  &lt;li&gt;Add the API key to your environment variable, e.g. &lt;code&gt;export OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;(Optional) To use Azure OpenAI API instead, refer to the following section:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set API type &lt;code&gt;export OPENAI_API_TYPE=azure&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;If you want to use the earlier version &lt;code&gt;2023-03-15-preview&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;export OPENAI_API_VERSION=2023-03-15-preview&lt;/code&gt;&lt;/p&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;To set the base URL for your Azure OpenAI resource. You can find this in the Azure portal under your Azure OpenAI resource.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;code&gt;export OPENAI_API_BASE=https://your-base-url.openai.azure.com&lt;/code&gt;&lt;/p&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;To set the OpenAI model deployment name for your Azure OpenAI resource.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;code&gt;export OPENAI_API_MODEL_DEPLOYMENT_NAME=gpt-35-turbo-16k&lt;/code&gt;&lt;/p&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;To set the OpenAIEmbeddings model deployment name for your Azure OpenAI resource.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;code&gt;export OPENAI_API_EMBEDDING_DEPLOYMENT_NAME=text-embedding-ada-002&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;1.1 (Optional) Prepare LLM - Anthropic(Claude 2) API Token&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; &#xA; &lt;p&gt;To get your Anthropic API token, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Go to the &lt;a href=&#34;https://docs.anthropic.com/claude/docs/getting-started-with-claude&#34;&gt;Anthropic website&lt;/a&gt; and sign up for an account if you haven&#39;t already.&lt;/li&gt; &#xA;  &lt;li&gt;Once you&#39;re logged in, navigate to the &lt;a href=&#34;https://console.anthropic.com/account/keys&#34;&gt;API keys page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Generate a new API key by clicking on the &#34;Create Key&#34; button.&lt;/li&gt; &#xA;  &lt;li&gt;Copy the API key and store it safely.&lt;/li&gt; &#xA;  &lt;li&gt;Add the API key to your environment variable, e.g. &lt;code&gt;export ANTHROPIC_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;2. (Optional) Prepare Speech to Text - Google Cloud API&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; &#xA; &lt;p&gt;To get your Google Cloud API credentials.json, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Go to the &lt;a href=&#34;https://cloud.google.com/speech-to-text/docs/before-you-begin&#34;&gt;GCP website&lt;/a&gt; and sign up for an account if you haven&#39;t already.&lt;/li&gt; &#xA;  &lt;li&gt;Follow the guide to create a project and enable Speech to Text API&lt;/li&gt; &#xA;  &lt;li&gt;Put &lt;code&gt;google_credentials.json&lt;/code&gt; in the root folder of this project. Check &lt;a href=&#34;https://cloud.google.com/speech-to-text/docs/before-you-begin#set_your_authentication_environment_variable&#34;&gt;GCP website&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Change &lt;code&gt;SPEECH_TO_TEXT_USE&lt;/code&gt; to use &lt;code&gt;GOOGLE&lt;/code&gt; in your &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;3. Prepare Text to Speech - ElevenLabs API Key&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Creating an ElevenLabs Account&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Visit &lt;a href=&#34;https://beta.elevenlabs.io/&#34;&gt;ElevenLabs&lt;/a&gt; to create an account. You&#39;ll need this to access the text to speech and voice cloning features.&lt;/p&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt; &lt;p&gt;In your Profile Setting, you can get an API Key. Save it in a safe place.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Set API key in your .env file:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code&gt;ELEVEN_LABS_API_KEY=&amp;lt;api key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;💿 Installation via Python&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;. Clone the repo &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/Shaunwei/RealChar.git &amp;amp;&amp;amp; cd RealChar&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;. Install requirements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install &lt;a href=&#34;https://people.csail.mit.edu/hubert/pyaudio/&#34;&gt;portaudio&lt;/a&gt; and &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;ffmpeg&lt;/a&gt; for audio&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# for mac&#xA;brew install portaudio&#xA;brew install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# for ubuntu&#xA;sudo apt update&#xA;sudo apt install portaudio19-dev&#xA;sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Then install all python requirements&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;. Create an empty &lt;a href=&#34;https://www.sqlite.org/index.html&#34;&gt;sqlite&lt;/a&gt; database if you have not done so before &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sqlite3 test.db &#34;VACUUM;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;. Run db upgrade &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;alembic upgrade head&#xA;&lt;/code&gt;&lt;/pre&gt; This ensures your database schema is up to date. Please run this after every time you pull the main branch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt;. Setup &lt;code&gt;.env&lt;/code&gt;: update API keys and select module &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 6&lt;/strong&gt;. Run server with &lt;code&gt;cli.py&lt;/code&gt; or use uvicorn directly &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Build the web fronend.&#xA;python cli.py web-build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python cli.py run-uvicorn&#xA;# or&#xA;uvicorn realtime_ai_character.main:app&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 7&lt;/strong&gt;. Run client: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use &lt;strong&gt;GPT4&lt;/strong&gt; for better conversation and &lt;strong&gt;Wear headphone&lt;/strong&gt; for best audio(avoid echo)&lt;/li&gt; &#xA;   &lt;li&gt;There are two ways to access the web client: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Option 1&lt;/strong&gt; Open your web browser and navigate to &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; (NOT 0.0.0.0:8000) &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;strong&gt;Make sure you have ran &lt;code&gt;python cli.py web-build&lt;/code&gt; before starting the server.&lt;/strong&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Option 2&lt;/strong&gt;: Running the client in React. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd client/web&#xA;npm install&#xA;npm start&#xA;&lt;/code&gt;&lt;/pre&gt; After running these commands, a local development server will start, and your default web browser will open a new tab/window pointing to this server (usually &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;Option 3 (experimental)&lt;/strong&gt;: Running the client in Nextjs. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd client/next-web&#xA;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; After running these commands, a local development server will start, and your default web browser will open a new tab/window pointing to this server (usually &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;(Optional) Terminal client: Run the following command in your terminal&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python client/cli.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(Optional) mobile client: open &lt;code&gt;client/mobile/ios/rac/rac.xcodeproj/project.pbxproj&lt;/code&gt; in Xcode and run the app&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step 8&lt;/strong&gt;. Select one character to talk to, then start talking&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note if you want to remotely connect to a RealChar server, SSL set up is required to establish the audio connection.&lt;/p&gt; &#xA;&lt;h2&gt;(Optional) 📀 Installation via Docker&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Docker image: you can use our docker image directly (if you are not using Apple M1/M2 CPUs)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker pull shaunly/real_char:latest&#xA;docker tag shaunly/real_char:latest realtime-ai-character&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Or you want build yourself) Build docker image&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python cli.py docker-build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you have issues with docker (especially on a non-Linux machine), please refer to &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;https://docs.docker.com/get-docker/&lt;/a&gt; (installation) and &lt;a href=&#34;https://docs.docker.com/desktop/troubleshoot/overview/&#34;&gt;https://docs.docker.com/desktop/troubleshoot/overview/&lt;/a&gt; (troubleshooting).&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Run docker image with &lt;code&gt;.env&lt;/code&gt; file&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python cli.py docker-run&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; (NOT 0.0.0.0:8000) to start talking or use terminal client&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python client/cli.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;🆕! Anyscale and LangSmith integration&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;👇click me&lt;/summary&gt; &#xA; &lt;h3&gt;Anyscale&lt;/h3&gt; &#xA; &lt;p&gt;You can now use &lt;a href=&#34;https://app.endpoints.anyscale.com/landing&#34;&gt;Anyscale Endpoint&lt;/a&gt; to serve Llama-2 models in your RealChar easily! Simply register an account with Anyscale Endpoint. Once you get the API key, set this environment variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;ANYSCALE_ENDPOINT_API_KEY=&amp;lt;your API Key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;By default, we show the largest servable Llama-2 model (70B) in the Web UI. You can change the model name (&lt;code&gt;meta-llama/Llama-2-70b-chat-hf&lt;/code&gt;) to other models, e.g. 13b or 7b versions.&lt;/p&gt; &#xA; &lt;h3&gt;LangSmith&lt;/h3&gt; &#xA; &lt;p&gt;If you have access to LangSmith, you can edit these environment variables to enable:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;LANGCHAIN_TRACING_V2=false # default off&#xA;LANGCHAIN_ENDPOINT=https://api.smith.langchain.com&#xA;LANGCHAIN_API_KEY=YOUR_LANGCHAIN_API_KEY&#xA;LANGCHAIN_PROJECT=YOUR_LANGCHAIN_PROJECT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;And it should work out of the box.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;📍 Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Launch v0.0.3&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create a new character via web UI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add additional tts service&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better UI/UX for home page&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better UI/UX for conversation page&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support MultiOn&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support SocialAGI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🫶 Contribute to RealChar&lt;/h2&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://raw.githubusercontent.com/Shaunwei/RealChar/main/contribute.md&#34;&gt;Contribution Guide&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;💪 Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/Shaunwei/RealChar&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Shaunwei/RealChar&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;🎲 Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join us on &lt;a href=&#34;https://discord.gg/e4AYNnFg2F&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>94xy/miniprogram-privacy</title>
    <updated>2023-09-01T01:34:12Z</updated>
    <id>tag:github.com,2023-09-01:/94xy/miniprogram-privacy</id>
    <link href="https://github.com/94xy/miniprogram-privacy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;小程序用户隐私保护授权弹窗组件&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;小程序隐私保护授权弹窗组件&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;微信 2023 年 8 月 10 日发布 &lt;a href=&#34;https://developers.weixin.qq.com/community/develop/doc/00042e3ef54940ce8520e38db61801&#34;&gt;关于小程序隐私保护指引设置的公告&lt;/a&gt;，9 月 15 日起所有隐私接口需用户点击同意并同步给微信之后才可以使用。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;当前组件做了隐私保护指引弹窗界面，直接引用，需要授权时展示弹窗，当用户点击“拒绝”直接退出小程序，点击“同意”同步结果给微信且以后不再弹窗，之后可以正常使用隐私接口。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;8 月 28 更新：今天才发现小程序可以使用页面的生命周期，对其进行了简化，只需要使用引入组件即可，不再需要任何其它代码&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;8 月 29 更新：采纳网页 Liu 的方案，解决多个 tabbar 情况下同意之后还有弹窗的问题&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;8 月 31 更新：兼容基础库版本小于2.32.3的情况，不加也不影响小程序运行&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;注意事项&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;2023 年 9 月 15 号之前，默认不会启用隐私相关功能，所以检测不到需要弹窗的情况，可以在 app.json 中配置 &lt;code&gt;&#34;__usePrivacyCheck__&#34;: true&lt;/code&gt; 之后，接口才可以检测到是否需要弹窗。个人实际情况：我在开发者工具中配置了 &lt;code&gt;&#34;__usePrivacyCheck__&#34;: true&lt;/code&gt; ，&lt;code&gt;needAuthorization&lt;/code&gt; 无论如何返回的都是 &lt;code&gt;false&lt;/code&gt;，但在真机模拟的情况下可以返回 &lt;code&gt;true&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;自动打开隐私保护指引界面需在「小程序管理后台」配置《小程序用户隐私保护指引》，官方&lt;a href=&#34;https://developers.weixin.qq.com/miniprogram/dev/framework/user-privacy/&#34;&gt;用户隐私保护指引填写说明&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;效果&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/94xy/miniprogram-privacy/main/img/demo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;使用方法&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;拷贝 &lt;code&gt;component&lt;/code&gt; 文件夹中的 &lt;code&gt;privacy&lt;/code&gt; 文件夹到小程序项目中的组件目录&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在 page.json 中引入组件&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;usingComponents&#34;: {&#xA;    &#34;Privacy&#34;: &#34;/component/privacy/privacy&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;在 page.wxml 中使用组件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsx&#34;&gt;&amp;lt;Privacy /&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;可以在所有使用了隐私接口的页面都加上该组件，授权一次之后使用所有隐私接口不再需要授权&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;取消授权&lt;/h1&gt; &#xA;&lt;p&gt;微信中「微信下拉-最近-最近使用的小程序」中删除小程序可取消授权。 开发者工具中「清除模拟器缓存-清除授权数据」可取消授权。&lt;/p&gt; &#xA;&lt;h1&gt;相关链接&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.weixin.qq.com/miniprogram/dev/framework/user-privacy/PrivacyAuthorize.html&#34;&gt;官方：小程序隐私协议开发指南&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;在线预览效果&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/94xy/miniprogram-privacy/main/img/qrcode.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>