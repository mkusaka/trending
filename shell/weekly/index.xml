<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-02T01:48:15Z</updated>
  <subtitle>Weekly Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>containers/ramalama</title>
    <updated>2025-02-02T01:48:15Z</updated>
    <id>tag:github.com,2025-02-02:/containers/ramalama</id>
    <link href="https://github.com/containers/ramalama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The goal of RamaLama is to make working with AI boring.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/containers/ramalama/main/logos/PNG/ramalama-logo-full-vertical-added-bg.png&#34; alt=&#34;RAMALAMA logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;RamaLama&lt;/h1&gt; &#xA;&lt;p&gt;The RamaLama project&#39;s goal is to make working with AI boring through the use of OCI containers.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama tool facilitates local management and serving of AI Models.&lt;/p&gt; &#xA;&lt;p&gt;On first run RamaLama inspects your system for GPU support, falling back to CPU support if no GPUs are present.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama uses container engines like Podman or Docker to pull the appropriate OCI image with all of the software necessary to run an AI Model for your systems setup.&lt;/p&gt; &#xA;&lt;p&gt;Running in containers eliminates the need for users to configure the host system for AI. After the initialization, RamaLama runs the AI Models within a container based on the OCI image.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama then pulls AI Models from model registries. Starting a chatbot or a rest API service from a simple single command. Models are treated similarly to how Podman and Docker treat container images.&lt;/p&gt; &#xA;&lt;p&gt;When both Podman and Docker are installed, RamaLama defaults to Podman, The &lt;code&gt;RAMALAMA_CONTAINER_ENGINE=docker&lt;/code&gt; environment variable can override this behaviour. When neither are installed RamaLama will attempt to run the model with software on the local system.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama supports multiple AI model registries types called transports. Supported transports:&lt;/p&gt; &#xA;&lt;h2&gt;TRANSPORTS&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Transports&lt;/th&gt; &#xA;   &lt;th&gt;Web Site&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HuggingFace&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.huggingface.co&#34;&gt;&lt;code&gt;huggingface.co&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ollama&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ollama.com&#34;&gt;&lt;code&gt;ollama.com&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OCI Container Registries&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opencontainers.org&#34;&gt;&lt;code&gt;opencontainers.org&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Examples: &lt;a href=&#34;https://quay.io&#34;&gt;&lt;code&gt;quay.io&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://docker.io&#34;&gt;&lt;code&gt;Docker Hub&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://artifactory.com&#34;&gt;&lt;code&gt;Artifactory&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;RamaLama uses the Ollama registry transport by default. Use the RAMALAMA_TRANSPORTS environment variable to modify the default. &lt;code&gt;export RAMALAMA_TRANSPORT=huggingface&lt;/code&gt; Changes RamaLama to use huggingface transport.&lt;/p&gt; &#xA;&lt;p&gt;Individual model transports can be modifies when specifying a model via the &lt;code&gt;huggingface://&lt;/code&gt;, &lt;code&gt;oci://&lt;/code&gt;, or &lt;code&gt;ollama://&lt;/code&gt; prefix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ramalama pull huggingface://afrideva/Tiny-Vicuna-1B-GGUF/tiny-vicuna-1b.q2_k.gguf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To make it easier for users, RamaLama uses shortname files, which container alias names for fully specified AI Models allowing users to specify the shorter names when referring to models. RamaLama reads shortnames.conf files if they exist . These files contain a list of name value pairs for specification of the model. The following table specifies the order which RamaLama reads the files . Any duplicate names that exist override previously defined shortnames.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Shortnames type&lt;/th&gt; &#xA;   &lt;th&gt;Path&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distribution&lt;/td&gt; &#xA;   &lt;td&gt;/usr/share/ramalama/shortnames.conf&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Administrators&lt;/td&gt; &#xA;   &lt;td&gt;/etc/ramamala/shortnames.conf&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Users&lt;/td&gt; &#xA;   &lt;td&gt;$HOME/.config/ramalama/shortnames.conf&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-code&#34;&gt;$ cat /usr/share/ramalama/shortnames.conf&#xA;[shortnames]&#xA;  &#34;tiny&#34; = &#34;ollama://tinyllama&#34;&#xA;  &#34;granite&#34; = &#34;huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;granite:7b&#34; = &#34;huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;ibm/granite&#34; = &#34;huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;merlinite&#34; = &#34;huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;merlinite:7b&#34; = &#34;huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf&#34;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h2&gt;Install via PyPi&lt;/h2&gt; &#xA;&lt;p&gt;RamaLama is available via PyPi &lt;a href=&#34;https://pypi.org/project/ramalama&#34;&gt;https://pypi.org/project/ramalama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ramalama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install by script&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you are a macOS user, this is the preferred method.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Install RamaLama by running this one-liner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/containers/ramalama/s/install.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hardware Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hardware&lt;/th&gt; &#xA;   &lt;th&gt;Enabled&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple Silicon GPU (Linux / Asahi)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple Silicon GPU (macOS)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple Silicon GPU (podman-machine)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nvidia GPU (cuda)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD GPU (rocm)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;COMMANDS&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama.1.md&#34;&gt;ramalama(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;primary RamaLama man page&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-containers.1.md&#34;&gt;ramalama-containers(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;list all RamaLama containers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-info.1.md&#34;&gt;ramalama-info(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;display RamaLama configuration information&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-list.1.md&#34;&gt;ramalama-list(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;list all downloaded AI Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-login.1.md&#34;&gt;ramalama-login(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;login to remote registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-logout.1.md&#34;&gt;ramalama-logout(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;logout from remote registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-pull.1.md&#34;&gt;ramalama-pull(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pull AI Model from Model registry to local storage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-push.1.md&#34;&gt;ramalama-push(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;push AI Model from local storage to remote registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-rm.1.md&#34;&gt;ramalama-rm(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;remove AI Model from local storage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-run.1.md&#34;&gt;ramalama-run(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;run specified AI Model as a chatbot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-serve.1.md&#34;&gt;ramalama-serve(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;serve REST API on specified AI Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-stop.1.md&#34;&gt;ramalama-stop(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;stop named container that is running AI Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-version.1.md&#34;&gt;ramalama-version(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;display version of AI Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Running Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;run&lt;/code&gt; a chatbot on a model using the &lt;code&gt;run&lt;/code&gt; command. By default, it pulls from the Ollama registry.&lt;/p&gt; &#xA;&lt;p&gt;Note: RamaLama will inspect your machine for native GPU support and then will use a container engine like Podman to pull an OCI container image with the appropriate code and libraries to run the AI Model. This can take a long time to setup, but only on the first run.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama run instructlab/merlinite-7b-lab&#xA;Copying blob 5448ec8c0696 [--------------------------------------] 0.0b / 63.6MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob cbd7e392a514 [--------------------------------------] 0.0b / 65.3MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob 5d6c72bcd967 done  208.5MiB / 208.5MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob 9ccfa45da380 [--------------------------------------] 0.0b / 7.6MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob 4472627772b1 [--------------------------------------] 0.0b / 120.0b (skipped: 0.0b = 0.00%)&#xA;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the initial container image has been downloaded, you can interact with different models, using the container image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama run granite3-moe&#xA;&amp;gt; Write a hello world application in python&#xA;&#xA;print(&#34;Hello World&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In a different terminal window see the running podman container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ podman ps&#xA;CONTAINER ID  IMAGE                             COMMAND               CREATED        STATUS        PORTS       NAMES&#xA;91df4a39a360  quay.io/ramalama/ramalama:latest  /home/dwalsh/rama...  4 minutes ago  Up 4 minutes              gifted_volhard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;list&lt;/code&gt; all models pulled into local storage.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama list&#xA;NAME                                                                MODIFIED     SIZE&#xA;ollama://smollm:135m                                                16 hours ago 5.5M&#xA;huggingface://afrideva/Tiny-Vicuna-1B-GGUF/tiny-vicuna-1b.q2_k.gguf 14 hours ago 460M&#xA;ollama://moondream:latest                                           6 days ago   791M&#xA;ollama://phi4:latest                                                6 days ago   8.43 GB&#xA;ollama://tinyllama:latest                                           1 week ago   608.16 MB&#xA;ollama://granite3-moe:3b                                            1 week ago   1.92 GB&#xA;ollama://granite3-moe:latest                                        3 months ago 1.92 GB&#xA;ollama://llama3.1:8b                                                2 months ago 4.34 GB&#xA;ollama://llama3.1:latest                                            2 months ago 4.34 GB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pulling Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;pull&lt;/code&gt; a model using the &lt;code&gt;pull&lt;/code&gt; command. By default, it pulls from the Ollama registry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama pull granite3-moe&#xA; 31% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    |  250.11 MB/ 783.77 MB  36.95 MB/s       14s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Serving Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;serve&lt;/code&gt; multiple models using the &lt;code&gt;serve&lt;/code&gt; command. By default, it pulls from the Ollama registry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama serve --name mylama llama3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stopping servers&lt;/h3&gt; &#xA;&lt;p&gt;You can stop a running model if it is running in a container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama stop mylama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;UI support&lt;/h3&gt; &#xA;&lt;p&gt;To use a UI, run a &lt;code&gt;ramalama serve&lt;/code&gt; command, then connect via your browser at:&lt;/p&gt; &#xA;&lt;p&gt;127.0.0.1:8080&lt;/p&gt; &#xA;&lt;h2&gt;Diagram&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;+---------------------------+&#xA;|                           |&#xA;| ramalama run granite3-moe |&#xA;|                           |&#xA;+-------+-------------------+&#xA;&#x9;|&#xA;&#x9;|&#xA;        |           +------------------+           +------------------+&#xA;        |           | Pull inferencing |           | Pull model layer |&#xA;        +-----------| runtime (cuda)   |----------&amp;gt;| granite3-moe     |&#xA;                    +------------------+           +------------------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | Repo options:    |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   +-+-------+------+-+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     |       |      |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     v       v      v&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     +---------+ +------+ +----------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     | Hugging | | OCI  | | Ollama   |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     | Face    | |      | | Registry |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     +-------+-+ +---+--+ +-+--------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     |       |      |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     v       v      v&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   +------------------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | Start with       |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | cuda runtime     |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | and              |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | granite3-moe     |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   +------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;In development&lt;/h2&gt; &#xA;&lt;p&gt;Regard this alpha, everything is under development, so expect breaking changes, luckily it&#39;s easy to reset everything and re-install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rm -rf /var/lib/ramalama # only required if running as root user&#xA;rm -rf $HOME/.local/share/ramalama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and install again.&lt;/p&gt; &#xA;&lt;h2&gt;Credit where credit is due&lt;/h2&gt; &#xA;&lt;p&gt;This project wouldn&#39;t be possible without the help of other projects like:&lt;/p&gt; &#xA;&lt;p&gt;llama.cpp&lt;br&gt; whisper.cpp&lt;br&gt; vllm&lt;br&gt; podman&lt;br&gt; huggingface&lt;/p&gt; &#xA;&lt;p&gt;so if you like this tool, give some of these repos a &lt;span&gt;‚≠ê&lt;/span&gt;, and hey, give us a &lt;span&gt;‚≠ê&lt;/span&gt; too while you are at it.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/%23ramalama:fedoraproject.org&#34;&gt;&lt;code&gt;Matrix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Open to contributors&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/containers/ramalama/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=containers/ramalama&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>actions/upload-pages-artifact</title>
    <updated>2025-02-02T01:48:15Z</updated>
    <id>tag:github.com,2025-02-02:/actions/upload-pages-artifact</id>
    <link href="https://github.com/actions/upload-pages-artifact" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A composite action for packaging and uploading an artifact that can be deployed to GitHub Pages.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;upload-pages-artifact&lt;/h1&gt; &#xA;&lt;p&gt;A composite Action for packaging and uploading artifact that can be deployed to &lt;a href=&#34;https://pages.github.com&#34;&gt;GitHub Pages&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/actions/upload-pages-artifact/main/action.yml&#34;&gt;action.yml&lt;/a&gt; for the various &lt;code&gt;inputs&lt;/code&gt; this action supports (or &lt;a href=&#34;https://raw.githubusercontent.com/actions/upload-pages-artifact/main/#inputs-%F0%9F%93%A5&#34;&gt;below&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;If you breakdown your workflow in two jobs (&lt;code&gt;build&lt;/code&gt; and &lt;code&gt;deploy&lt;/code&gt;), we recommend this action to be used in your &lt;code&gt;build&lt;/code&gt; job:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;jobs:&#xA;  # Build job&#xA;  build:&#xA;    # Specify runner +  build &amp;amp; upload the static files as an artifact&#xA;    runs-on: ubuntu-latest&#xA;    steps:&#xA;      - name: Build static files&#xA;        id: build&#xA;        run: |&#xA;          # &amp;lt;Not provided for brevity&amp;gt;&#xA;          # At a minimum this step should build the static files of your site&#xA;          # &amp;lt;Not provided for brevity&amp;gt;&#xA;&#xA;      - name: Upload static files as artifact&#xA;        id: deployment&#xA;        uses: actions/upload-pages-artifact@v3 # or specific &#34;vX.X.X&#34; version tag for this action&#xA;        with:&#xA;          path: build_outputs_folder/&#xA;&#xA;  # Deployment job&#xA;  deploy:&#xA;    environment:&#xA;      name: github-pages&#xA;      url: ${{ steps.deployment.outputs.page_url }}&#xA;    runs-on: ubuntu-latest&#xA;    needs: build&#xA;    steps:&#xA;      - name: Deploy to GitHub Pages&#xA;        id: deployment&#xA;        uses: actions/deploy-pages@v4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inputs üì•&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Input&lt;/th&gt; &#xA;   &lt;th&gt;Required?&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;github-pages&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Artifact name&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;path&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;_site/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Path of the directory containing the static assets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;retention-days&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Duration after which artifact will expire in days&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Outputs üì§&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Output&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;artifact_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The ID of the artifact that was uploaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Artifact validation&lt;/h2&gt; &#xA;&lt;p&gt;While choosing to use this action as part of your approach to deploying to GitHub Pages is technically optional, we highly recommend it since it takes care of producing (mostly) valid artifacts.&lt;/p&gt; &#xA;&lt;p&gt;However, if you &lt;em&gt;&lt;strong&gt;do not&lt;/strong&gt;&lt;/em&gt; choose to use this action but still want to deploy to Pages using an Actions workflow, then you must upload an Actions artifact that meets the following criteria:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Be named &lt;code&gt;github-pages&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Be a single &lt;a href=&#34;https://en.wikipedia.org/wiki/Gzip&#34;&gt;&lt;code&gt;gzip&lt;/code&gt; archive&lt;/a&gt; containing a single &lt;a href=&#34;https://en.wikipedia.org/wiki/Tar_(computing)&#34;&gt;&lt;code&gt;tar&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Tar_(computing)&#34;&gt;&lt;code&gt;tar&lt;/code&gt; file&lt;/a&gt; must:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;be under 10GB in size (we recommend under 1 GB!) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;span&gt;‚ö†&lt;/span&gt; The GitHub Pages &lt;a href=&#34;https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages#usage-limits&#34;&gt;officially supported maximum size limit is 1GB&lt;/a&gt;, so the subsequent deployment of larger tarballs are not guaranteed to succeed ‚Äî often because they are more prone to exceeding the maximum deployment timeout of 10 minutes.&lt;/li&gt; &#xA;   &lt;li&gt;‚õî However, there is also an &lt;em&gt;unofficial&lt;/em&gt; absolute maximum size limit of 10GB, which Pages will not even &lt;em&gt;attempt&lt;/em&gt; to deploy.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;not contain any symbolic or hard links&lt;/li&gt; &#xA; &lt;li&gt;contain only files and directories&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Release instructions&lt;/h2&gt; &#xA;&lt;p&gt;In order to release a new version of this Action:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Locate the semantic version of the &lt;a href=&#34;https://github.com/actions/upload-pages-artifact/releases&#34;&gt;upcoming release&lt;/a&gt; (a draft is maintained by the &lt;a href=&#34;https://raw.githubusercontent.com/actions/upload-pages-artifact/main/.github/workflows/draft-release.yml&#34;&gt;&lt;code&gt;draft-release&lt;/code&gt; workflow&lt;/a&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Publish the draft release from the &lt;code&gt;main&lt;/code&gt; branch with semantic version as the tag name, &lt;em&gt;with&lt;/em&gt; the checkbox to publish to the GitHub Marketplace checked. &lt;span&gt;‚òë&lt;/span&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After publishing the release, the &lt;a href=&#34;https://raw.githubusercontent.com/actions/upload-pages-artifact/main/.github/workflows/release.yml&#34;&gt;&lt;code&gt;release&lt;/code&gt; workflow&lt;/a&gt; will automatically run to create/update the corresponding major version tag such as &lt;code&gt;v0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è Environment approval is required. Check the &lt;a href=&#34;https://github.com/actions/upload-pages-artifact/actions/workflows/release.yml&#34;&gt;Release workflow run list&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The scripts and documentation in this project are released under the &lt;a href=&#34;https://raw.githubusercontent.com/actions/upload-pages-artifact/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- references --&gt;</summary>
  </entry>
  <entry>
    <title>JCluzet/42_EXAM</title>
    <updated>2025-02-02T01:48:15Z</updated>
    <id>tag:github.com,2025-02-02:/JCluzet/42_EXAM</id>
    <link href="https://github.com/JCluzet/42_EXAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A program almost identical to the 42 EXAMS for practice. (Pool EXAM &amp; Stud EXAM)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;42_EXAM v2 üéì&lt;/h1&gt; &#xA;&lt;h2&gt;All 42 exams auto-correct, to practice.&lt;/h2&gt; &#xA;&lt;h2&gt;ExamRank02, 03, 04, 05, 06 &amp;amp; PiscineExam&lt;/h2&gt; &#xA;&lt;h2&gt;üëâ 42_EXAM is now part of &lt;strong&gt;&lt;a href=&#34;https://grademe.fr&#34;&gt;GRADEME.FR&lt;/a&gt;&lt;/strong&gt; üòé&lt;/h2&gt; &#xA;&lt;h4&gt;Disclaimer: This project is not made by 42School, it&#39;s not identical to the proposed exams and does not reflect your exams at 42&lt;/h4&gt; &#xA;&lt;h3&gt;NEW ExamRank02 available ü•≥&lt;/h3&gt; &#xA;&lt;img width=&#34;711&#34; alt=&#34;Screenshot 2022-09-06 at 17 00 20&#34; src=&#34;https://user-images.githubusercontent.com/55356071/188669215-6681228d-e6b5-4229-b177-45d2699e29ae.png&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;‚ö†Ô∏è This project is available on MAC and LINUX (the real exam will be on Linux)&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Practice for the 42 exam üèä‚Äç‚ôÇÔ∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; New Subjects ExamRank02 APRIL 2022&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Subject drawn at random&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Exponential waiting for a correction&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; X Hours maximum&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatic correction (without internet)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Traceback available&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Detect infinite loop in your program (ExamRank02)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Detect infinite loop in all ExamRank&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Detect leaks in all exercices&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;711&#34; alt=&#34;Screenshot 2022-09-06 at 17 00 47&#34; src=&#34;https://user-images.githubusercontent.com/55356071/188669367-504411a5-5c62-4848-932e-405c9ee05a45.png&#34;&gt; &#xA;&lt;img width=&#34;711&#34; alt=&#34;Screenshot 2022-09-06 at 17 01 15&#34; src=&#34;https://user-images.githubusercontent.com/55356071/188669418-04f8bb1b-9cc2-4c07-9d07-5e9c570283b4.png&#34;&gt; &#xA;&lt;h1&gt;üëì CHEAT code :&lt;/h1&gt; &#xA;&lt;p&gt;(sorry don&#39;t work in exam at 42)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;remove_grade_time&lt;/strong&gt; : remove grade time between two push&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  &amp;gt; The maximum time to complete the exam is only information. &#xA;  Here, the exercise can always be corrected even after the time limit.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;force_success&lt;/strong&gt; : force an exercice to success&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;new_ex&lt;/strong&gt; : generate a new exercice on the same level&lt;/p&gt; &#xA;&lt;h1&gt;üï∂ VIP CHEAT code :&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/JCluzet&#34;&gt;Sponsor to become VIP&lt;/a&gt; OR &lt;a href=&#34;https://raw.githubusercontent.com/JCluzet/42_EXAM/main/CONTRIBUTING.md&#34;&gt;Contribute to become VIP&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can contribute by adding new exercises or improving the program&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;gradenow&lt;/strong&gt; : Get an instant correction (no grademe cooldown)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Being a sponsor brings VIP on GradeMe and 42_EXAM.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;VIPs also have exactly the same display as the 42 examshell.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;More features coming&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Launch 42_EXAM in ONE COMMAND :&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;bash -c &#34;$(curl https://grademe.fr)&#34;&lt;/code&gt; --&amp;gt; Visit &lt;a href=&#34;https://grademe.fr&#34;&gt;Grademe.fr&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üëÆ‚Äç‚ôÄÔ∏è RGPD Information :&lt;/h1&gt; &#xA;&lt;p&gt;Some data may be collected for the sole purpose of improving the service, for example to check if an error is coming from the student and not from the program. The data that can be collected are :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The host name of your machine.&lt;/li&gt; &#xA; &lt;li&gt;The exam number you choose.&lt;/li&gt; &#xA; &lt;li&gt;Name of exercise, fail or success, current assignement and level.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribution by adding more exercises :&lt;/h1&gt; &#xA;&lt;p&gt;You can contribute by adding new exercises very easily with 42_EXAM.&lt;/p&gt; &#xA;&lt;p&gt;üìÑ Read this documentation: &lt;a href=&#34;https://raw.githubusercontent.com/JCluzet/42_EXAM/main/CONTRIBUTING.md&#34;&gt;Contribution DOC&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ü•≥ If your contribution is accepted, your name will be listed in the ReadMe as a contributor, thanks!&lt;/p&gt; &#xA;&lt;p&gt;Contributor : pandaero Kuninoto ComlanGiovanni&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer for 42 staff :&lt;/h1&gt; &#xA;&lt;p&gt;This project is not identical to the proposed exams and does not reflect 42 official exam. It is only meant to help 42 students to train on some exercises found on Github. It works simply by doing a DIFF on both versions. All the subjects have been found on Github. During my schooling at 42, this project helped me to understand a lot of notions like how to host a website, working with javascript, php and others. This project has been created with the sole purpose of helping students to understand their mistakes and improve themselves. If It‚Äôs not the case, I am ready to make changes and corrections on it, 42_EXAM/GradeMe has only a non-lucrative and educational purpose. It is completely free and open-source.&lt;/p&gt; &#xA;&lt;h1&gt;Your help is welcome&lt;/h1&gt; &#xA;&lt;p&gt;üëã If you have any problem with any test, please create an &#34;Issue&#34; here on Github, it will only take 3 minutes of your time and it will help me to make the test more accurate.&lt;/p&gt; &#xA;&lt;p&gt;üìå Remember that if you encounter an error, you&#39;re probably not the only one and your &#34;Issue&#34; will surely help other people.&lt;/p&gt;</summary>
  </entry>
</feed>