<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-21T01:35:16Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QwenLM/Qwen1.5</title>
    <updated>2024-03-21T01:35:16Z</updated>
    <id>tag:github.com,2024-03-21:/QwenLM/Qwen1.5</id>
    <link href="https://github.com/QwenLM/Qwen1.5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen1.5 is the improved version of Qwen, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen1.5&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate.aliyuncs.com/assets/blog/qwen1.5/logo_qwen1.5.jpg&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://qwenlm.github.io&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen1.5-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This time, we upgrade Qwen to Qwen1.5, the beta version of Qwen2. Similar to Qwen, it is still a decoder-only transformer model with SwiGLU activation, RoPE, multi-head attention. At this moment, we have achieved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;6 model sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B;&lt;/li&gt; &#xA; &lt;li&gt;Significant model quality improvements in chat models;&lt;/li&gt; &#xA; &lt;li&gt;Strengthened multilingual capabilities in both base and chat models;&lt;/li&gt; &#xA; &lt;li&gt;All models support the context length of &lt;code&gt;32768&lt;/code&gt; tokens;&lt;/li&gt; &#xA; &lt;li&gt;System prompts enabled for all models, which means roleplay is possible.&lt;/li&gt; &#xA; &lt;li&gt;No need of &lt;code&gt;trust_remote_code&lt;/code&gt; anymore.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We have not integrated GQA and mixture of SWA and full attention in this version and we will add the features in the future version.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Detailed evaluation results are reported in this &lt;a href=&#34;https://qwenlm.github.io&#34;&gt; üìë blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&amp;gt;=4.37.0&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; üö® This is a must because `transformers` integrated Qwen2 codes since `4.37.0`. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;ü§ó Hugging Face Transformers&lt;/h3&gt; &#xA;&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;Qwen/Qwen1.5-72B-Chat&#34;,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen1.5-72B-Chat&#34;)&#xA;&#xA;prompt = &#34;Give me a short introduction to large language model.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;generated_ids = model.generate(&#xA;    model_inputs.input_ids,&#xA;    max_new_tokens=512&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For quantized models, we advise you to use the GPTQ and AWQ correspondents, namely &lt;code&gt;Qwen1.5-7B-Chat-GPTQ-Int8&lt;/code&gt;, &lt;code&gt;Qwen1.5-7B-Chat-AWQ&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;üíª Run locally&lt;/h3&gt; &#xA;&lt;h4&gt;Ollama&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   Ollama provides an &#xA;  &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/openai.md&#34;&gt;OpenAI-compatible API&lt;/a&gt;, which however does NOT support &#xA;  &lt;b&gt;function calling&lt;/b&gt;. For tool use capabilities, consider using &#xA;  &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt;, which offers a wrapper for function calling over the API. &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/README.md&#34;&gt;installing ollama&lt;/a&gt;, you can initiate the ollama service with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama serve&#xA;# You need to keep this service running whenever you are using ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen&lt;/code&gt;, such as &lt;code&gt;:0.5b&lt;/code&gt;, &lt;code&gt;:1.8b&lt;/code&gt;, &lt;code&gt;:4b&lt;/code&gt;, &lt;code&gt;:7b&lt;/code&gt;, &lt;code&gt;:14b&lt;/code&gt;, or &lt;code&gt;:72b&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run qwen:4b&#xA;# To exit, type &#34;/bye&#34; and press ENTER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen:4b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import OpenAI&#xA;client = OpenAI(&#xA;    base_url=&#39;http://localhost:11434/v1/&#39;,&#xA;    api_key=&#39;ollama&#39;,  # required but ignored&#xA;)&#xA;chat_completion = client.chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#39;role&#39;: &#39;user&#39;,&#xA;            &#39;content&#39;: &#39;Say this is a test&#39;,&#xA;        }&#xA;    ],&#xA;    model=&#39;qwen:4b&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional details, please visit &lt;a href=&#34;https://ollama.ai/&#34;&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;llama.cpp&lt;/h4&gt; &#xA;&lt;p&gt;Download our provided GGUF files or create them by yourself, and you can directly use them with the latest &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; with a one-line command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./main -m &amp;lt;path-to-file&amp;gt; -n 512 --color -i -cml -f prompts/chat-with-qwen.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;LMStudio&lt;/h4&gt; &#xA;&lt;p&gt;Qwen1.5 has already been supported by &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; &#xA;&lt;h4&gt;OpenVINO&lt;/h4&gt; &#xA;&lt;p&gt;Qwen1.5 has already been supported by &lt;a href=&#34;https://github.com/openvinotoolkit&#34;&gt;OpenVINO toolkit&lt;/a&gt;. You can install and run this &lt;a href=&#34;https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot&#34;&gt;example notebook&lt;/a&gt; with Intel CPU, integrated GPU or discrete GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;h4&gt;Text generation web UI&lt;/h4&gt; &#xA;&lt;p&gt;You can directly use &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;&lt;code&gt;text-generation-webui&lt;/code&gt;&lt;/a&gt; for creating a web UI demo. If you use GGUF, remember to install the latest wheel of &lt;code&gt;llama.cpp&lt;/code&gt; with the support of Qwen1.5.&lt;/p&gt; &#xA;&lt;h4&gt;llamafile&lt;/h4&gt; &#xA;&lt;p&gt;Clone &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;&lt;code&gt;llamafile&lt;/code&gt;&lt;/a&gt;, run source install, and then create your own llamafile with the GGUF file following the guide &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#creating-llamafiles&#34;&gt;here&lt;/a&gt;. You are able to run one line of command, say &lt;code&gt;./qwen.llamafile&lt;/code&gt;, to create a demo.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Now, Qwen1.5 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;vLLM&lt;/code&gt; and &lt;code&gt;SGLang&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   The OpenAI-compatible APIs provided by vLLM and SGLang currently do NOT support &#xA;  &lt;b&gt;function calling&lt;/b&gt;. For tool use capabilities, &#xA;  &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt; provides a wrapper around these APIs to support function calling. &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;We advise you to use &lt;code&gt;vLLM&amp;gt;=0.3.0&lt;/code&gt; to build OpenAI-compatible API service. Start the server with a chat model, e.g. &lt;code&gt;Qwen1.5-7B-Chat&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m vllm.entrypoints.openai.api_server --served-model-name Qwen1.5-7B-Chat --model Qwen/Qwen1.5-7B-Chat &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use the chat API as demonstrated below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -d &#39;{&#xA;    &#34;model&#34;: &#34;Qwen1.5-7B-Chat&#34;,&#xA;    &#34;messages&#34;: [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}&#xA;    ]&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;# Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.&#xA;openai_api_key = &#34;EMPTY&#34;&#xA;openai_api_base = &#34;http://localhost:8000/v1&#34;&#xA;&#xA;client = OpenAI(&#xA;    api_key=openai_api_key,&#xA;    base_url=openai_api_base,&#xA;)&#xA;&#xA;chat_response = client.chat.completions.create(&#xA;    model=&#34;Qwen1.5-7B-Chat&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;},&#xA;    ]&#xA;)&#xA;print(&#34;Chat response:&#34;, chat_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SGLang&lt;/h3&gt; &#xA;&lt;p&gt;Please install &lt;code&gt;SGLang&lt;/code&gt; from source. Similar to &lt;code&gt;vLLM&lt;/code&gt;, you need to launch a server and use OpenAI-compatible API service. Start the server first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sglang.launch_server --model-path Qwen/Qwen1.5-7B-Chat --port 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use it in Python as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint&#xA;&#xA;@function&#xA;def multi_turn_question(s, question_1, question_2):&#xA;    s += system(&#34;You are a helpful assistant.&#34;)&#xA;    s += user(question_1)&#xA;    s += assistant(gen(&#34;answer_1&#34;, max_tokens=256))&#xA;    s += user(question_2)&#xA;    s += assistant(gen(&#34;answer_2&#34;, max_tokens=256))&#xA;&#xA;set_default_backend(RuntimeEndpoint(&#34;http://localhost:30000&#34;))&#xA;&#xA;state = multi_turn_question.run(&#xA;    question_1=&#34;What is the capital of China?&#34;,&#xA;    question_2=&#34;List two local attractions.&#34;,&#xA;)&#xA;&#xA;for m in state.messages():&#xA;    print(m[&#34;role&#34;], &#34;:&#34;, m[&#34;content&#34;])&#xA;&#xA;print(state[&#34;answer_1&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;We advise you to use training frameworks, including &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;Axolotl&lt;/a&gt;, &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;Llama-Factory&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift&#34;&gt;Swift&lt;/a&gt;, etc., to finetune your models with SFT, DPO, PPO, etc.&lt;/p&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;Qwen1.5 models are now deployed on both &lt;a href=&#34;https://dashscope.aliyun.com/&#34;&gt;DashScope&lt;/a&gt; and &lt;a href=&#34;https://api.together.ai/&#34;&gt;Together&lt;/a&gt;. Check &lt;a href=&#34;https://api.together.xyz/playground/chat/Qwen/Qwen1.5-72B-Chat&#34;&gt;this&lt;/a&gt; out and have fun with Qwen1.5-72B-Chat!&lt;/p&gt; &#xA;&lt;h2&gt;üê≥ Docker&lt;/h2&gt; &#xA;&lt;p&gt;To simplify the deployment process, we provide docker images with pre-built environments: &lt;a href=&#34;https://hub.docker.com/r/qwenllm/qwen&#34;&gt;qwenllm/qwen&lt;/a&gt;. You only need to install the driver and download model files to launch demos and finetune the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen1.5 -it qwenllm/qwen:1.5-cu121 bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;Check the license of each model inside its HF repo. It is NOT necessary for you to submit a request for commercial usage.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qwen,&#xA;  title={Qwen Technical Report},&#xA;  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},&#xA;  journal={arXiv preprint arXiv:2309.16609},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen1.5/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
</feed>