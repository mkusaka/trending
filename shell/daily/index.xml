<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-22T01:37:32Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tekakutli/anime_translation</title>
    <updated>2022-12-22T01:37:32Z</updated>
    <id>tag:github.com,2022-12-22:/tekakutli/anime_translation</id>
    <link href="https://github.com/tekakutli/anime_translation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI-helped transcription and translation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anime Translation Initiative&lt;/h1&gt; &#xA;&lt;p&gt;AI-helped transcription and translation&lt;br&gt; Everything works offline&lt;/p&gt; &#xA;&lt;p&gt;LOAD THE FUNCTIONS&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;source snippets/enviromentvariables.sh #YOU MUST EDIT THIS ONE&#xA;source snippets/functions.sh&#xA;source snippets/opus.sh&#xA;source snippets/timeformat.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Workflow&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I&#39;m assuming you are using linux, and check &lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#Dependencies&#34;&gt;Dependencies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;More information about each component best researched in their own websites&lt;/li&gt; &#xA; &lt;li&gt;The main workflow is as follows: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Setup the Model, Whisper here, and use it to translate directly from japanese audio to english text. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;To get the audio, formated appropriately, you use ffmpeg.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#Model-Usage&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The timestamps often aren&#39;t completely aligned with the sound, so we can use an AutoSync: ffsubsync. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#AutoSync-the-Subs&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Next comes the human to fix the translation, split long captions, further align them, etc. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;I propose the usage of Subed, which is an Emacs package &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Subed allows us to: &#xA;        &lt;ul&gt; &#xA;         &lt;li&gt;Watch where are captioning in MPV&lt;/li&gt; &#xA;         &lt;li&gt;Efficiently move, merge, split the timestamps, with precision.&lt;/li&gt; &#xA;        &lt;/ul&gt; &lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#VTT-efficient-creation-or-edit&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Then, to fix grammar or spelling mistakes, we can use the Language-Tool &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#Grammar-Spelling-Checking-Language-Tool&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Finally, we can load the .vtt file with mpv and enjoy, &lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#MPV&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Some extra tools at your disposal: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The Opus Model is a text-to-text translator model, like Google-Translate &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#Local-Text-Translation&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;There are two extra tools to align the captions: a Visual Scene Detector(Scene-timestamps), and a Human Voice Detector(Speech Timestamps) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Often the captions align with those&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#Get-Event-Timestamps&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;You can use Whisper to translate a snapshot of what you are hearing from your speakers, using the Speakers-Stream thing &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tekakutli/anime_translation/master/#Translate-the-Speakers-Stream&#34;&gt;Instructions Here&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Model Setup&lt;/h3&gt; &#xA;&lt;p&gt;used model: WHISPER&lt;br&gt; will download model &lt;em&gt;ggml-large.bin&lt;/em&gt; from: &lt;a href=&#34;https://huggingface.co/datasets/ggerganov/whisper.cpp&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Model Usage&lt;/h4&gt; &#xA;&lt;p&gt;get audio from video file for &lt;em&gt;whisper&lt;/em&gt; to use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;useWhisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the -tr flag activates translation into english, without it transcribes into japanese&lt;/p&gt; &#xA;&lt;h5&gt;Warning&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;whisper often breaks with music segments&lt;/li&gt; &#xA; &lt;li&gt;if you see it start outputing the same thing over and over, interrupt it &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;then use the -ot &lt;em&gt;milliseconds&lt;/em&gt; flag to resume at that point&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;After interrupting, copy from Terminal, then format appropriately with:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;formatToVtt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MPV&lt;/h3&gt; &#xA;&lt;p&gt;get mpv to load some subs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mpvLoadSubs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;what subs?&lt;br&gt; git clone &lt;a href=&#34;https://github.com/tekakutli/anime_translation/&#34;&gt;https://github.com/tekakutli/anime_translation/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;VTT efficient creation or edit&lt;/h3&gt; &#xA;&lt;p&gt;I use &lt;em&gt;subed&lt;/em&gt;&lt;br&gt; git clone &lt;a href=&#34;https://github.com/sachac/subed&#34;&gt;https://github.com/sachac/subed&lt;/a&gt;&lt;br&gt; add &lt;em&gt;Subed&lt;/em&gt; from &lt;em&gt;configAdd.el&lt;/em&gt; to Emacs &lt;em&gt;config.el&lt;/em&gt;&lt;br&gt; alternatively, add this extra:&lt;br&gt; git clone &lt;a href=&#34;https://gist.github.com/mooseyboots/d9a183795e5704d3f517878703407184&#34;&gt;https://gist.github.com/mooseyboots/d9a183795e5704d3f517878703407184&lt;/a&gt;&lt;br&gt; add &lt;em&gt;Subed Extra Section&lt;/em&gt; from &lt;em&gt;configAdd.el&lt;/em&gt; to Emacs &lt;em&gt;config.el&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AutoSync the Subs&lt;/h3&gt; &#xA;&lt;p&gt;This ffsubsync-script first autosyncs japanese captions with japanese audio, and then uses those timestamps to sync english captions to japanese captions.&lt;br&gt; The japanese captions only need to be phonetically close, which means that we could use a smaller-faster model to get them instead, &lt;em&gt;ggml-small.bin&lt;/em&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ggerganov/whisper.cpp/tree/main&#34;&gt;here&lt;/a&gt;.&lt;br&gt; This is the reason behind the names, why some are called whisper_small vs whisper_large (the model used).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make installffsubsync&#xA;autosync&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other Utils&lt;/h3&gt; &#xA;&lt;p&gt;To .srt Conversion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;vttToSrt subs.vtt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Export final .mp4 with subtitles&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;exportSubs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To format a given time in milliseconds or as timestamps, example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#timeformat.sh has these two commodity functions:&#xA;milliformat &#34;2.3&#34; #2 minutes 3 seconds&#xA;stampformat &#34;3.2.1&#34; #3 hours 2 minutes 1 second&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Grammar-Spelling Checking Language-Tool&lt;/h4&gt; &#xA;&lt;p&gt;Install full-version of Language Tool&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make installlanguagetool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Activate it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;languagetool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;add LanguageTool section from configAdd.el to Emacs config.el&lt;br&gt; Emacs use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(langtool-check)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Local Text Translation&lt;/h4&gt; &#xA;&lt;p&gt;your FROM-TO model is either &lt;a href=&#34;https://github.com/Helsinki-NLP/Opus-MT-train/tree/master/models&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models&#34;&gt;here&lt;/a&gt;&lt;br&gt; example, to get the models I use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make opusInstallExample&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;edit PATH_TO_SUBS/Opus-MT/services.json appropriately, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make installopus&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To activate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#opus.sh has commodity functions&#xA;Opus-MT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;t &#34;text to translate&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get Event Timestamps&lt;/h3&gt; &#xA;&lt;h4&gt;Scene-timestamps&lt;/h4&gt; &#xA;&lt;p&gt;Visual Scene timestamps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make installSceneTimestamps&#xA;&#xA;sceneTimestamps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;VAD, Speech timestamps&lt;/h4&gt; &#xA;&lt;p&gt;What is VAD? VAD means: Voice Activity Detector&lt;br&gt; It gives you the speech timestamps, when human voice is detected&lt;br&gt; first install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;torch&lt;/a&gt;, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;speechTimestamps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Translate the Speakers-Stream&lt;/h3&gt; &#xA;&lt;p&gt;you&#39;ll need to Ctrl-C to stop recording, after which it will translate the temporal recording&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamtranslate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you were to have sway, you could put this in your sway config, and have an easy keybinding to translate what you are hearing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bindsym $mod+Shift+return exec alacritty -e bash /home/$USER/files/code/anime_translation/snippets/streamtranslate.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux, Bash, Mpv, Ffmpeg, Emacs, Subed&lt;/li&gt; &#xA; &lt;li&gt;Whatever model you wish to use&lt;/li&gt; &#xA; &lt;li&gt;Python if you use the &#34;Get Event Timestamps&#34; things &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The vad.py thing downloads silero-vad by itself&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Docker for the LibreGrammar(Language Tool) or the Opus things&lt;/li&gt; &#xA; &lt;li&gt;If you want to translate your Speakers, you need pipewire &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;As commodity, you will need: wl-copy and wl-paste, if running on wayland &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;If you don&#39;t want them, remove them from streamtranslate.sh&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why X&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Why Git over Google-Docs or similar? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Version Control Systems (git) is an ergonomic tool to pick or disregard from contributions, it enables trully parallel work distribution&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Why .vtt over others? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;whisper&lt;/em&gt; can output vtt or srt&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;subed&lt;/em&gt; can work with vtt or srt&lt;/li&gt; &#xA;   &lt;li&gt;why vtt over srt? personal choice, but: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;vtt has no need for numbers as id&lt;/li&gt; &#xA;     &lt;li&gt;seems shorter and more efficient&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>