<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-18T01:45:08Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>geerlingguy/pi-cluster</title>
    <updated>2023-01-18T01:45:08Z</updated>
    <id>tag:github.com,2023-01-18:/geerlingguy/pi-cluster</id>
    <link href="https://github.com/geerlingguy/pi-cluster" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Raspberry Pi Cluster automation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Raspberry Pi Cluster&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/geerlingguy/pi-cluster/actions?query=workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/geerlingguy/pi-cluster/workflows/CI/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kgVz4-SEhbE&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geerlingguy/pi-cluster/master/images/turing-pi-2-hero.jpg?raw=true&#34; width=&#34;500&#34; height=&#34;auto&#34; alt=&#34;Turing Pi 2 - Raspberry Pi Compute Module Cluster&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains examples and automation used in various Raspberry Pi clustering scenarios, as seen on &lt;a href=&#34;https://www.youtube.com/c/JeffGeerling&#34;&gt;Jeff Geerling&#39;s YouTube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ecdm3oA-QdQ&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geerlingguy/pi-cluster/master/images/deskpi-super6c-running.jpg?raw=true&#34; width=&#34;500&#34; height=&#34;auto&#34; alt=&#34;DeskPi Super6c Mini ITX Raspberry Pi Compute Module Cluster&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The inspiration for this project was my first Pi cluster, the &lt;a href=&#34;https://www.pidramble.com&#34;&gt;Raspberry Pi Dramble&lt;/a&gt;, which is still running in my basement to this day!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you have &lt;a href=&#34;https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html&#34;&gt;Ansible&lt;/a&gt; installed.&lt;/li&gt; &#xA; &lt;li&gt;Copy the &lt;code&gt;example.hosts.ini&lt;/code&gt; inventory file to &lt;code&gt;hosts.ini&lt;/code&gt;. Make sure it has the &lt;code&gt;control_plane&lt;/code&gt; and &lt;code&gt;node&lt;/code&gt;s configured correctly (for my examples I named my nodes &lt;code&gt;node[1-4].local&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Copy the &lt;code&gt;example.config.yml&lt;/code&gt; file to &lt;code&gt;config.yml&lt;/code&gt;, and modify the variables to your liking.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Raspberry Pi Setup&lt;/h3&gt; &#xA;&lt;p&gt;I am running Raspberry Pi OS on various Pi clusters. You can run this on any Pi cluster, but I tend to use Compute Modules without eMMC (&#39;Lite&#39; versions) and I often run them using &lt;a href=&#34;https://amzn.to/3G35QbY&#34;&gt;32 GB SanDisk Extreme microSD cards&lt;/a&gt; to boot each node. For some setups (like when I run the &lt;a href=&#34;https://computeblade.com&#34;&gt;Compute Blade&lt;/a&gt; or &lt;a href=&#34;https://deskpi.com/collections/deskpi-super6c&#34;&gt;DeskPi Super6c&lt;/a&gt;, I boot off NVMe SSDs instead.&lt;/p&gt; &#xA;&lt;p&gt;In every case, I flashed Raspberry Pi OS (64-bit, lite) to the storage devices using Raspberry Pi Imager.&lt;/p&gt; &#xA;&lt;p&gt;To make network discovery and integration easier, I edit the advanced configuration in Imager, and set the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set hostname: &lt;code&gt;node1.local&lt;/code&gt; (set to &lt;code&gt;2&lt;/code&gt; for node 2, &lt;code&gt;3&lt;/code&gt; for node 3, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Enable SSH: &#39;Allow public-key&#39;, and paste in my public SSH key(s)&lt;/li&gt; &#xA; &lt;li&gt;Configure wifi: (ONLY on node 1, if desired) enter SSID and password for local WiFi network&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After setting all those options, making sure only node 1 has WiFi configured, and the hostname is unique to each node (and matches what is in &lt;code&gt;hosts.ini&lt;/code&gt;), I inserted the microSD cards into the respective Pis, or installed the NVMe SSDs into the correct slots, and booted the cluster.&lt;/p&gt; &#xA;&lt;h3&gt;SSH connection test&lt;/h3&gt; &#xA;&lt;p&gt;To test the SSH connection from my Ansible controller (my main workstation, where I&#39;m running all the playbooks), I connected to each server individually, and accepted the hostkey:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ssh pi@node1.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures Ansible will also be able to connect via SSH in the following steps. You can test Ansible&#39;s connection with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ansible all -m ping&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It should respond with a &#39;SUCCESS&#39; message for each node.&lt;/p&gt; &#xA;&lt;h3&gt;Storage Configuration&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: This playbook is configured to set up a ZFS mirror volume on node 3, with two drives connected to the built-in SATA ports on the Turing Pi 2.&lt;/p&gt; &#xA; &lt;p&gt;It is not yet genericized for other use cases (e.g. boards that are &lt;em&gt;not&lt;/em&gt; the Turing Pi 2).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This playbook will create a storage location on node 3 by default. You can use one of the storage configurations by switching the &lt;code&gt;storage_type&lt;/code&gt; variable from &lt;code&gt;filesystem&lt;/code&gt; to &lt;code&gt;zfs&lt;/code&gt; in your &lt;code&gt;config.yml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h4&gt;Filesystem Storage&lt;/h4&gt; &#xA;&lt;p&gt;If using filesystem (&lt;code&gt;storage_type: filesystem&lt;/code&gt;), make sure to use the appropriate &lt;code&gt;storage_nfs_dir&lt;/code&gt; variable in &lt;code&gt;config.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;ZFS Storage&lt;/h4&gt; &#xA;&lt;p&gt;If using ZFS (&lt;code&gt;storage_type: zfs&lt;/code&gt;, you should have two volumes available on node 3, &lt;code&gt;/dev/sda&lt;/code&gt;, and &lt;code&gt;/dev/sdb&lt;/code&gt;, able to be pooled into a mirror. Make sure your two SATA drives are wiped:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pi@node3:~ $ sudo wipefs --all --force /dev/sda?; sudo wipefs --all --force /dev/sda&#xA;pi@node3:~ $ sudo wipefs --all --force /dev/sdb?; sudo wipefs --all --force /dev/sdb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you run &lt;code&gt;lsblk&lt;/code&gt;, you should see &lt;code&gt;sda&lt;/code&gt; and &lt;code&gt;sdb&lt;/code&gt; have no partitions, and are ready to use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pi@node3:~ $ lsblk&#xA;NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT&#xA;sda           8:0    0  1.8T  0 disk &#xA;sdb           8:16   0  1.8T  0 disk &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should also make sure the &lt;code&gt;storage_nfs_dir&lt;/code&gt; variable is set appropriately for ZFS in your &lt;code&gt;config.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This ZFS layout was configured originally for the Turing Pi 2 board, which has two built-in SATA ports connected directly to node 3. In the future, the configuration may be genericized a bit better.&lt;/p&gt; &#xA;&lt;h4&gt;Ceph Storage Configuration&lt;/h4&gt; &#xA;&lt;p&gt;You could also run Ceph on a Pi cluster—see the storage configuration playbook inside the &lt;code&gt;ceph&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;This configuration is not yet integrated into the general K3s setup.&lt;/p&gt; &#xA;&lt;h3&gt;Cluster configuration and K3s installation&lt;/h3&gt; &#xA;&lt;p&gt;Run the playbook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ansible-playbook main.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At the end of the playbook, there should be an instance of Drupal running on the cluster. If you log into node 1, you should be able to access it with &lt;code&gt;curl localhost&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, if you have SSH tunnelling configured (see later section), you could access &lt;code&gt;http://[your-vps-ip-or-hostname]:8080/&lt;/code&gt; and you&#39;d see the site.&lt;/p&gt; &#xA;&lt;p&gt;You can also log into node 1, switch to the root user account (&lt;code&gt;sudo su&lt;/code&gt;), then use &lt;code&gt;kubectl&lt;/code&gt; to manage the cluster (e.g. view Drupal pods with &lt;code&gt;kubectl get pods -n drupal&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The Kubernetes Ingress object for Drupal (how HTTP requests from outside the cluster make it to Drupal) can be found by running &lt;code&gt;kubectl get ingress -n drupal&lt;/code&gt;. Take the IP address or hostname there and enter it in your browser on a computer on the same network, and voila! You should see Drupal&#39;s installer.&lt;/p&gt; &#xA;&lt;p&gt;K3s&#39; &lt;code&gt;kubeconfig&lt;/code&gt; file is located at &lt;code&gt;/etc/rancher/k3s/k3s.yaml&lt;/code&gt;. If you&#39;d like to manage the cluster from other hosts (or using a tool like Lens), copy the contents of that file, replacing &lt;code&gt;localhost&lt;/code&gt; with the IP address or hostname of the control plane node, and paste the contents into a file &lt;code&gt;~/.kube/config&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Upgrading the cluster&lt;/h3&gt; &#xA;&lt;p&gt;Run the upgrade playbook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ansible-playbook upgrade.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Monitoring the cluster&lt;/h3&gt; &#xA;&lt;p&gt;Prometheus and Grafana are used for monitoring. Grafana can be accessed via port forwarding (or you could choose to expose it another way).&lt;/p&gt; &#xA;&lt;p&gt;To access Grafana:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you set up a valid &lt;code&gt;~/.kube/config&lt;/code&gt; file (see &#39;K3s installation&#39; above).&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;kubectl port-forward service/cluster-monitoring-grafana :80&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Grab the port that&#39;s output, and browse to &lt;code&gt;localhost:[port]&lt;/code&gt;, and bingo! Grafana.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The default login is &lt;code&gt;admin&lt;/code&gt; / &lt;code&gt;prom-operator&lt;/code&gt;, but you can also get the secret with &lt;code&gt;kubectl get secret cluster-monitoring-grafana -o jsonpath=&#34;{.data.admin-password}&#34; | base64 -D&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can then browse to all the Kubernetes and Pi-related dashboards by browsing the Dashboards in the &#39;General&#39; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Benchmarking the cluster&lt;/h3&gt; &#xA;&lt;p&gt;See the README file within the &lt;code&gt;benchmarks&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Shutting down the cluster&lt;/h3&gt; &#xA;&lt;p&gt;The safest way to shut down the cluster is to run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ansible all -m community.general.shutdown -b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: If using the SSH tunnel, you might want to run the command &lt;em&gt;first&lt;/em&gt; on nodes 2-4, &lt;em&gt;then&lt;/em&gt; on node 1. So first run &lt;code&gt;ansible &#39;all:!control_plane&#39; [...]&lt;/code&gt;, then run it again just for &lt;code&gt;control_plane&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Then after you confirm the nodes are shut down (with K3s running, it can take a few minutes), press the cluster&#39;s power button (or yank the Ethernet cables if using PoE) to power down all Pis physically. Then you can switch off or disconnect your power supply.&lt;/p&gt; &#xA;&lt;h3&gt;Static network configuration (optional, but recommended)&lt;/h3&gt; &#xA;&lt;p&gt;I using my cluster both on-premise and remote (using a 4G LTE modem connected to the first Pi), I set it up on its own subnet (10.1.1.x). You can change the subnet that&#39;s used via the &lt;code&gt;ipv4_subnet_prefix&lt;/code&gt; variable in &lt;code&gt;config.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To configure the local network for the Pi cluster (this is optional—you can still use the rest of the configurations without a custom local network), run the playbook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ansible-playbook networking.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the playbook, until a reboot, the Pis will still be accessible over their former DHCP-assigned IP address. After the nodes are rebooted, you will need to make sure your workstation is connected to an interface using the same subnet as the cluster (e.g. 10.1.1.x).&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: After the networking changes are made, since this playbook uses DNS names (e.g. &lt;code&gt;node1.local&lt;/code&gt;) instead of IP addresses, your computer will still be able to connect to the nodes directly—assuming your network has IPv6 support. Pinging the nodes on their new IP addresses will &lt;em&gt;not&lt;/em&gt; work, however. For better network compatibility, it&#39;s recommended you set up a separate network interface on the Ansible controller that&#39;s on the same subnet as the Pis in the cluster:&lt;/p&gt; &#xA; &lt;p&gt;On my Mac, I connected a second network interface and manually configured its IP address as &lt;code&gt;10.1.1.10&lt;/code&gt;, with subnet mask &lt;code&gt;255.255.255.0&lt;/code&gt;, and that way I could still access all the nodes via IP address or their hostnames (e.g. &lt;code&gt;node2.local&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Because the cluster subnet needs its own router, node 1 is configured as a router, using &lt;code&gt;wlan0&lt;/code&gt; as the primary interface for Internet traffic by default. The other nodes get their Internet access through node 1.&lt;/p&gt; &#xA;&lt;h4&gt;Switch between 4G LTE and WiFi (optional)&lt;/h4&gt; &#xA;&lt;p&gt;The network configuration defaults to an &lt;code&gt;active_internet_interface&lt;/code&gt; of &lt;code&gt;wlan0&lt;/code&gt;, meaning node 1 will route all Internet traffic for the cluster through it&#39;s WiFi interface.&lt;/p&gt; &#xA;&lt;p&gt;Assuming you have a &lt;a href=&#34;https://www.jeffgeerling.com/blog/2022/using-4g-lte-wireless-modems-on-raspberry-pi&#34;&gt;working 4G card in slot 1&lt;/a&gt;, you can switch node 1 to route through an alternate interface (e.g. &lt;code&gt;usb0&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set &lt;code&gt;active_internet_interface: &#34;usb0&#34;&lt;/code&gt; in your &lt;code&gt;config.yml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the networking playbook again: &lt;code&gt;ansible-playbook networking.yml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can switch back and forth between interfaces using the steps above.&lt;/p&gt; &#xA;&lt;h4&gt;Reverse SSH and HTTP tunnel configuration (optional)&lt;/h4&gt; &#xA;&lt;p&gt;For my own experimentation, I ran my Pi cluster &#39;off-grid&#39;, using a 4G LTE modem, as mentioned above.&lt;/p&gt; &#xA;&lt;p&gt;Because my mobile network provider uses CG-NAT, there is no way to remotely access the cluster, or serve web traffic to the public internet from it, at least not out of the box.&lt;/p&gt; &#xA;&lt;p&gt;I am using a reverse SSH tunnel to enable direct remote SSH and HTTP access. To set that up, I configured a VPS I run to use TCP Forwarding (see &lt;a href=&#34;https://www.jeffgeerling.com/blog/2022/ssh-and-http-raspberry-pi-behind-cg-nat&#34;&gt;this blog post for details&lt;/a&gt;), and I configured an SSH key so node 1 could connect to my VPS (e.g. &lt;code&gt;ssh my-vps-username@my-vps-hostname-or-ip&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Then I set the &lt;code&gt;reverse_tunnel_enable&lt;/code&gt; variable to &lt;code&gt;true&lt;/code&gt; in my &lt;code&gt;config.yml&lt;/code&gt;, and configured the VPS username and hostname options.&lt;/p&gt; &#xA;&lt;p&gt;Doing that and running the &lt;code&gt;main.yml&lt;/code&gt; playbook configures &lt;code&gt;autossh&lt;/code&gt; on node 1, and will try to get a connection through to the VPS on ports 2222 (to node 1&#39;s port 22) and 8080 (to node 1&#39;s port 80).&lt;/p&gt; &#xA;&lt;p&gt;After that&#39;s done, you should be able to log into the cluster &lt;em&gt;through&lt;/em&gt; your VPS with a command like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ssh -p 2222 pi@[my-vps-hostname]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: If autossh isn&#39;t working, it could be that it didn&#39;t exit cleanly, and a tunnel is still reserving the port on the remote VPS. That&#39;s often the case if you run &lt;code&gt;sudo systemctl status autossh&lt;/code&gt; and see messages like &lt;code&gt;Warning: remote port forwarding failed for listen port 2222&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;In that case, log into the remote VPS and run &lt;code&gt;pgrep ssh | xargs kill&lt;/code&gt; to kill off all active SSH sessions, then &lt;code&gt;autossh&lt;/code&gt; should pick back up again.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Use this feature at your own risk. Security is your own responsibility, and for better protection, you should probably avoid directly exposing your cluster (e.g. by disabling the &lt;code&gt;GatewayPorts&lt;/code&gt; option) so you can only access the cluster while already logged into your VPS).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Caveats&lt;/h2&gt; &#xA;&lt;p&gt;These playbooks are used in both production and test clusters, but security is &lt;em&gt;always&lt;/em&gt; your responsibility. If you want to use any of this configuration in production, take ownership of it and understand how it works so you don&#39;t wake up to a hacked Pi cluster one day!&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;The repository was created in 2023 by &lt;a href=&#34;https://www.jeffgeerling.com&#34;&gt;Jeff Geerling&lt;/a&gt;, author of &lt;a href=&#34;https://www.ansiblefordevops.com&#34;&gt;Ansible for DevOps&lt;/a&gt;, &lt;a href=&#34;https://www.ansibleforkubernetes.com&#34;&gt;Ansible for Kubernetes&lt;/a&gt;, and &lt;a href=&#34;https://www.kubernetes101book.com&#34;&gt;Kubernetes 101&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>