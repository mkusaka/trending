<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-16T01:39:06Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aws-samples/awsome-distributed-training</title>
    <updated>2025-07-16T01:39:06Z</updated>
    <id>tag:github.com,2025-07-16:/aws-samples/awsome-distributed-training</id>
    <link href="https://github.com/aws-samples/awsome-distributed-training" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of best practices, reference architectures, model training examples and utilities to train large models on AWS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ML Training Reference Architectures &amp;amp; Tests &#xA; &lt;!-- omit from toc --&gt;&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; We are currently undergoing a major refactoring of this repository, particularly focused on the test cases section. If you prefer to use the previous directory structure and deprecated test cases, please refer to &lt;a href=&#34;https://github.com/aws-samples/awsome-distributed-training/releases/tag/v1.1.0&#34;&gt;v1.1.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This repository contains reference architectures and test cases for distributed model training with &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html&#34;&gt;Amazon SageMaker Hyperpod&lt;/a&gt;, &lt;a href=&#34;https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html&#34;&gt;AWS ParallelCluster&lt;/a&gt;, &lt;a href=&#34;https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html&#34;&gt;AWS Batch&lt;/a&gt;, and &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html&#34;&gt;Amazon EKS&lt;/a&gt;. The test cases cover different types and sizes of models as well as different frameworks and parallel optimizations (Pytorch DDP/FSDP, MegatronLM, NemoMegatron...).&lt;/p&gt; &#xA;&lt;p&gt;The major components of this directory are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;reference-architectures/&#xA;|-- 1.architectures/               # CloudFormation templates for reference arch&#xA;|-- 2.ami_and_containers/          # Scripts to create AMIs and container images&#xA;|-- 3.test_cases/                  # Reference test cases and/or benchmark scripts&#xA;|-- 4.validation_observability/    # Tools to measure performance or troubleshoot&#xA;`-- ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: the architectures are designed to work with the S3 bucket and VPC created using reference templates &lt;code&gt;1.architectures/0.s3/&lt;/code&gt; and &lt;code&gt;1.architectures/1.vpc_network/&lt;/code&gt;. &lt;em&gt;You&#39;re strongly recommended to deploy these two templates &lt;strong&gt;before&lt;/strong&gt; deploying any of the reference architectures.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;0. Workshops&lt;/h2&gt; &#xA;&lt;p&gt;You can follow the workshop below to train models on AWS. Each contains examples for several test cases as well as nuggets of information on operating a cluster for LLM training.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://catalog.workshops.aws/sagemaker-hyperpod/en-US&#34;&gt;Amazon SageMaker HyperPod&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Workshop for SageMaker HyperPod, shows how to deploy and monitor it&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://catalog.workshops.aws/ml-on-aws-parallelcluster&#34;&gt;AWS ParallelCluster&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Similar workshop as HyperPod but on ParallelCluster&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://catalog.workshops.aws/sagemaker-hyperpod-eks&#34;&gt;Amazon SageMaker HyperPod EKS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Workshop for SageMaker HyperPod EKS, shows how to deploy and monitor it&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;1. Architectures&lt;/h2&gt; &#xA;&lt;p&gt;Architectures are located in &lt;code&gt;1.architectures&lt;/code&gt; and consists of utilities and service related architectures.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Category&lt;/th&gt; &#xA;   &lt;th&gt;Usage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/0.s3&#34;&gt;&lt;code&gt;0.s3&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Storage&lt;/td&gt; &#xA;   &lt;td&gt;Create an S3 bucket&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/1.vpc_network&#34;&gt;&lt;code&gt;1.vpc_network&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Network&lt;/td&gt; &#xA;   &lt;td&gt;Create a VPC with subnets required resources&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/2.aws-parallelcluster&#34;&gt;&lt;code&gt;2.aws-parallelcluster&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Compute&lt;/td&gt; &#xA;   &lt;td&gt;Cluster templates for GPU &amp;amp; custom silicon training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/3.aws-batch&#34;&gt;&lt;code&gt;3.aws-batch&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Compute&lt;/td&gt; &#xA;   &lt;td&gt;AWS Batch template for distributed training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/4.amazon-eks&#34;&gt;&lt;code&gt;4.amazon-eks&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Compute&lt;/td&gt; &#xA;   &lt;td&gt;Manifest files to train with Amazon EKS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/5.sagemaker-hyperpod&#34;&gt;&lt;code&gt;5.sagemaker-hyperpod&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Compute&lt;/td&gt; &#xA;   &lt;td&gt;SageMaker HyperPod template for distributed training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More will come, feel free to add new ones (ex. Ray?). You will also find &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/efa-cheatsheet.md&#34;&gt;documentation&lt;/a&gt; for EFA and the recommended environment variables.&lt;/p&gt; &#xA;&lt;h2&gt;2. Custom Amazon Machine Images&lt;/h2&gt; &#xA;&lt;p&gt;Custom machine images can be built using &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/www.packer.io&#34;&gt;Packer&lt;/a&gt; for AWS ParallelCluster, Amazon EKS and plain EC2. These images are based are on Ansible roles and playbooks.&lt;/p&gt; &#xA;&lt;h2&gt;3. Test cases&lt;/h2&gt; &#xA;&lt;p&gt;Test cases are organized by framework and cover various distributed training scenarios. Each test case includes the necessary scripts and configurations to run distributed training jobs.&lt;/p&gt; &#xA;&lt;h3&gt;PyTorch Test Cases&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/FSDP&#34;&gt;&lt;code&gt;FSDP/&lt;/code&gt;&lt;/a&gt; - Fully Sharded Data Parallel training examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/megatron-lm&#34;&gt;&lt;code&gt;megatron-lm/&lt;/code&gt;&lt;/a&gt; - Megatron-LM distributed training examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/nemo-launcher&#34;&gt;&lt;code&gt;nemo-launcher/&lt;/code&gt;&lt;/a&gt; - NeMo Launcher examples for distributed training. This test case is for NeMo version 1.0 only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/nemo-run&#34;&gt;&lt;code&gt;nemo-run/&lt;/code&gt;&lt;/a&gt; - NeMo framework distributed training examples. This test case is for NeMo version 2.0+.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/neuronx-distributed&#34;&gt;&lt;code&gt;neuronx-distributed/&lt;/code&gt;&lt;/a&gt; - AWS Trainium distributed training examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/mosaicml-composer&#34;&gt;&lt;code&gt;mosaicml-composer/&lt;/code&gt;&lt;/a&gt; - MosaicML Composer examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/picotron&#34;&gt;&lt;code&gt;picotron/&lt;/code&gt;&lt;/a&gt; - PicoTron distributed training examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/torchtitan&#34;&gt;&lt;code&gt;torchtitan/&lt;/code&gt;&lt;/a&gt; - TorchTitan examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/cpu-ddp&#34;&gt;&lt;code&gt;cpu-ddp/&lt;/code&gt;&lt;/a&gt; - CPU-based Distributed Data Parallel examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/pytorch/bionemo&#34;&gt;&lt;code&gt;bionemo/&lt;/code&gt;&lt;/a&gt; - BioNeMo distributed training examples&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;JAX Test Cases&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/3.test_cases/jax&#34;&gt;&lt;code&gt;jax/&lt;/code&gt;&lt;/a&gt; - JAX-based distributed training examples using PaxML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each test case includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Training scripts and configurations&lt;/li&gt; &#xA; &lt;li&gt;Container definitions (where applicable)&lt;/li&gt; &#xA; &lt;li&gt;Launch scripts for different cluster types&lt;/li&gt; &#xA; &lt;li&gt;Performance monitoring and validation tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;4. Validation scripts&lt;/h2&gt; &#xA;&lt;p&gt;Utilities scripts and micro-benchmarks examples are set under &lt;code&gt;4.validation_scripts/&lt;/code&gt;. The EFA Prometheus exporter can be found in this &lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/4.validation_and_observability/3.efa-node-exporter&#34;&gt;directory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Comments&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/4.validation_and_observability/1.pytorch-env-validation&#34;&gt;&lt;code&gt;1.pytorch-env-validation&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Validates your PyTorch environment&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/4.validation_and_observability/3.efa-node-exporter&#34;&gt;&lt;code&gt;3.efa-node-exporter&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Node exporter with Amazon EFA monitoring modules&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/4.validation_and_observability/4.prometheus-grafana&#34;&gt;&lt;code&gt;4.prometheus-grafana&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deployment assets to monitor SageMaker Hyperpod Clusters&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/4.validation_and_observability/5.nsight&#34;&gt;&lt;code&gt;5.nsight&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Shows how to run Nvidia Nsight Systems to profile your workload&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/efa-versions.py&#34;&gt;&lt;code&gt;efa-versions.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Get the versions of Nvidia libraries, drivers and EFA drivers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;5. CI&lt;/h2&gt; &#xA;&lt;p&gt;Integration tests are written in &lt;a href=&#34;https://docs.pytest.org&#34;&gt;pytest&lt;/a&gt;. Just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pytest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can run tests with out capturing stdout and keeping all docker images an other artifacts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pytest -s --keep-artifacts=t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to all the contributors for building, reviewing and testing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aws-samples/awsome-distributed-training/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=aws-samples/awsome-distributed-training&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;7.Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#aws-samples/awsome-distributed-training&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=aws-samples/awsome-distributed-training&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>