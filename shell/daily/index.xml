<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-29T01:49:56Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LesnyRumcajs/grpc_bench</title>
    <updated>2023-06-29T01:49:56Z</updated>
    <id>tag:github.com,2023-06-29:/LesnyRumcajs/grpc_bench</id>
    <link href="https://github.com/LesnyRumcajs/grpc_bench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Various gRPC benchmarks&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;583&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/9642092/189478887-a9ad0a81-8cdc-4983-8954-80347feb002e.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/LesnyRumcajs/grpc_bench/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://discord.gg/NAjC2RRdPg&#34;&gt;&lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/932736311733915668.svg?style=for-the-badge&amp;amp;label=Discord&amp;amp;logo=discord&#34; height=&#34;20&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;One repo to finally have a clear, objective gRPC benchmark with code for everyone to verify and improve.&lt;/p&gt; &#xA;&lt;p&gt;Contributions are most welcome! Feel free to use &lt;a href=&#34;https://github.com/LesnyRumcajs/grpc_bench/discussions&#34;&gt;discussions&lt;/a&gt; if you have questions/issues or ideas. There is also a &lt;a href=&#34;https://github.com/LesnyRumcajs/grpc_bench/discussions/categories/benchmark-results&#34;&gt;category&lt;/a&gt; where you are encouraged to submit your own benchmark results!&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://www.nexthink.com/blog/comparing-grpc-performance/&#34;&gt;Nexthink blog post&lt;/a&gt; for a deeper overview of the project and recent results.&lt;/p&gt; &#xA;&lt;h1&gt;Goal&lt;/h1&gt; &#xA;&lt;p&gt;The goal of this benchmark is to compare the performance and resource usage of various gRPC libraries across different programming languages and technologies. To achieve that, a minimal protobuf contract is used to not pollute the results with other concepts (e.g. performances of hash maps) and to make the implementations simple.&lt;/p&gt; &#xA;&lt;p&gt;That being said, the service implementations should &lt;strong&gt;NOT&lt;/strong&gt; take advantage of that and keep the code generic and maintainable. What does generic mean? One should be able to easily adapt the existing code to some fundamental use cases (e.g. having a thread-safe hash map on server side to provide values to client given some key, performing blocking I/O or retrieving a network resource).&lt;br&gt; Keep in mind the following guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No inline assembly or other, language specific, tricks / hacks should be used&lt;/li&gt; &#xA; &lt;li&gt;The code should be (reasonably) idiomatic, built upon the modern patterns of the language&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t make any assumption on the kind of work done inside the server&#39;s request handler&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t assume all client requests will have the exact same content&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;You decide what is better&lt;/h1&gt; &#xA;&lt;p&gt;Although in the end results are sorted according to the number of requests served, one should go beyond and look at the resource usage - perhaps one implementation is slightly better in terms of raw speed but uses three times more CPU to achieve that. Maybe it&#39;s better to take the first one if you&#39;re running on a Raspberry Pi and want to get the most of it. Maybe it&#39;s better to use the latter in a big server with 32 CPUs because it scales. It all depends on your use case. This benchmark is created to help people make an informed decision (and get ecstatic when their favourite technology seems really good, without doubts).&lt;/p&gt; &#xA;&lt;h1&gt;Metrics&lt;/h1&gt; &#xA;&lt;p&gt;We try to provide some metrics to make this decision easier:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;req/s - the number of requests the service was able to successfully serve&lt;/li&gt; &#xA; &lt;li&gt;average latency, and 90/95/99 percentiles - time from sending a request to receiving the response&lt;/li&gt; &#xA; &lt;li&gt;average CPU, memory - average resource usage during the benchmark, as reported by &lt;code&gt;docker stats&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What this benchmark does NOT take into account&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Completeness of the gRPC library. We test only basic unary RPC at the moment. This is the most common service method which may be enough for some business use cases, but not for the others. When you&#39;re happy about the results of some technology, you should check out it&#39;s documentation (if it exists) and decide yourself if is it production-ready.&lt;/li&gt; &#xA; &lt;li&gt;Taste. Some may find beauty in Ruby, some may feel like Java is the only real deal. Others treat languages as tools and don&#39;t care at all. We don&#39;t judge (officially ðŸ˜‰ ). Unless it&#39;s a huge state machine with raw &lt;code&gt;void&lt;/code&gt; pointers. Ups!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Prerequisites&lt;/h1&gt; &#xA;&lt;p&gt;Linux or MacOS with Docker. Keep in mind that the results on MacOS may not be that reliable, Docker for Mac runs on a VM.&lt;/p&gt; &#xA;&lt;h1&gt;Running benchmark&lt;/h1&gt; &#xA;&lt;p&gt;To build the benchmarks images use: &lt;code&gt;./build.sh [BENCH1] [BENCH2] ...&lt;/code&gt; . You need them to run the benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;To run the benchmarks use: &lt;code&gt;./bench.sh [BENCH1] [BENCH2] ...&lt;/code&gt; . They will be run sequentially.&lt;/p&gt; &#xA;&lt;p&gt;To clean-up the benchmark images use: &lt;code&gt;./clean.sh [BENCH1] [BENCH2] ...&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Configuring the benchmark&lt;/h2&gt; &#xA;&lt;p&gt;The benchmark can be configured through the following environment variables:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Default value&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_BENCHMARK_DURATION&lt;/td&gt; &#xA;   &lt;td&gt;Duration of the benchmark.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_BENCHMARK_WARMUP&lt;/td&gt; &#xA;   &lt;td&gt;Duration of the warmup. Stats won&#39;t be collected.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_REQUEST_SCENARIO&lt;/td&gt; &#xA;   &lt;td&gt;Scenario (from &lt;a href=&#34;https://raw.githubusercontent.com/LesnyRumcajs/grpc_bench/master/scenarios/&#34;&gt;scenarios/&lt;/a&gt;) containing the protobuf and the data to be sent in the client request.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;complex_proto&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_SERVER_CPUS&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of cpus used by the server.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_SERVER_RAM&lt;/td&gt; &#xA;   &lt;td&gt;Maximum memory used by the server.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512m&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_CLIENT_CONNECTIONS&lt;/td&gt; &#xA;   &lt;td&gt;Number of connections to use.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_CLIENT_CONCURRENCY&lt;/td&gt; &#xA;   &lt;td&gt;Number of requests to run concurrently. It can&#39;t be smaller than the number of connections.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_CLIENT_QPS&lt;/td&gt; &#xA;   &lt;td&gt;Rate limit, in queries per second (QPS).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0 (&lt;em&gt;unlimited&lt;/em&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_CLIENT_CPUS&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of cpus used by the client.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRPC_IMAGE_NAME&lt;/td&gt; &#xA;   &lt;td&gt;Name of Docker image built by &lt;code&gt;./build.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;&#39;grpc_bench&#39;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Parameter recommendations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GRPC_BENCHMARK_DURATION&lt;/code&gt; should not be too small. Some implementations need a &lt;em&gt;warm-up&lt;/em&gt; before achieving their optimal performance and most real-life gRPC services are expected to be long running processes. From what we measured, &lt;strong&gt;300s&lt;/strong&gt; should be enough.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GRPC_SERVER_CPUS&lt;/code&gt; + &lt;code&gt;GRPC_CLIENT_CPUS&lt;/code&gt; should not exceed total number of cores on the machine. The reason for this is that you don&#39;t want the &lt;code&gt;ghz&lt;/code&gt; client to steal precious CPU cycles from the service under test. Keep in mind that having the &lt;code&gt;GRPC_CLIENT_CPUS&lt;/code&gt; too low may not saturate the service in some of the more performant implementations. Also keep in mind limiting the number of &lt;code&gt;GRPC_SERVER_CPUS&lt;/code&gt; to 1 will severely hamper the performance for some technologies - is running a service on 1 CPU your use case? It may be, but keep in mind eventual load balancer also incurs some costs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GRPC_REQUEST_SCENARIO&lt;/code&gt; is a parameter to both &lt;code&gt;build.sh&lt;/code&gt; and &lt;code&gt;bench.sh&lt;/code&gt;. The images must be rebuilt each time you intend to use a scenario having a different &lt;code&gt;helloworld.proto&lt;/code&gt; from the one ran previously.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other parameters will depend on your use-case. Choose wisely.&lt;/p&gt; &#xA;&lt;h1&gt;Results&lt;/h1&gt; &#xA;&lt;p&gt;You can find our old sample results in the &lt;a href=&#34;https://github.com/LesnyRumcajs/grpc_bench/wiki&#34;&gt;Wiki&lt;/a&gt;. Be sure to run the benchmarks yourself if you have sufficient hardware, especially for multi-core scenarios. New results will be posted to &lt;a href=&#34;https://github.com/LesnyRumcajs/grpc_bench/discussions/categories/benchmark-results&#34;&gt;discussions&lt;/a&gt; and you are encouraged to publish yours as well!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mosaicml/examples</title>
    <updated>2023-06-29T01:49:56Z</updated>
    <id>tag:github.com,2023-06-29:/mosaicml/examples</id>
    <link href="https://github.com/mosaicml/examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast and flexible reference benchmarks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MosaicML Examples&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains reference examples for using the MosaicML platform to train and deploy machine learning models at scale. It&#39;s designed to be easily forked/copied and modified.&lt;/p&gt; &#xA;&lt;p&gt;It is structured with four different types of examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mosaicml/examples/main/examples/benchmarks/&#34;&gt;benchmarks&lt;/a&gt;: Instructions for how to reproduce the cost estimates that we publish in our blogs. Start here if you are looking to verify or learn more about our cost estimates.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mosaicml/examples/main/examples/end-to-end-examples/&#34;&gt;end-to-end-examples&lt;/a&gt;: Complete examples of using the MosaicML platform, starting from data processing and ending with model deployment. Start here if you are looking full MosaicML platform usage examples.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mosaicml/examples/main/examples/inference-deployments/&#34;&gt;inference-deployments&lt;/a&gt;: Example model handlers and deployment yamls for deploying a model with MosaicML inference. Start here if you are looking to deploy a model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mosaicml/examples/main/examples/third-party/&#34;&gt;third-party&lt;/a&gt;: Example usages of the MosaicML platform with third-party distributed training libraries. Start here if you are looking to try out the MosaicML platform with non-MosaicML training software.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see the README in each folder for more information about each type of example.&lt;/p&gt; &#xA;&lt;h2&gt;Tests and Linting&lt;/h2&gt; &#xA;&lt;p&gt;To run the lint and test suites for a specific folder, you can use the &lt;code&gt;lint_subdirectory.sh&lt;/code&gt; and &lt;code&gt;test_subdirectory.sh&lt;/code&gt; scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/lint_subdirectory.sh benchmarks/bert&#xA;bash ./scripts/test_subdirectory.sh benchmarks/bert&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other MosaicML repositories and documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://forms.mosaicml.com/demo?utm_source=home&amp;amp;utm_medium=mosaicml.com&amp;amp;utm_campaign=always-on&#34;&gt;MosaicML platform sign up&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mosaicml/llm-foundry&#34;&gt;LLM-foundry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mosaicml/diffusion&#34;&gt;Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mosaicml/composer&#34;&gt;Composer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mosaicml/streaming&#34;&gt;Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.mosaicml.com/en/latest/&#34;&gt;MosaicML docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mosaicml.com/blog&#34;&gt;MosaicML blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>wolfi-dev/os</title>
    <updated>2023-06-29T01:49:56Z</updated>
    <id>tag:github.com,2023-06-29:/wolfi-dev/os</id>
    <link href="https://github.com/wolfi-dev/os" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Main package repository for production Wolfi images&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/wolfi-dev/.github/raw/main/profile/wolfi-logo-dark-mode.svg#gh-dark-mode-only&#34; alt=&#34;wolfi logo&#34;&gt; &lt;img src=&#34;https://github.com/wolfi-dev/.github/raw/main/profile/wolfi-logo-light-mode.svg#gh-light-mode-only&#34; alt=&#34;wolfi logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Wolfi&lt;/h1&gt; &#xA;&lt;p&gt;This is the main package repository for the Wolfi project.&lt;/p&gt; &#xA;&lt;p&gt;Named after the &lt;a href=&#34;https://en.wikipedia.org/wiki/Octopus_wolfi&#34;&gt;smallest octopus&lt;/a&gt;, Wolfi is a lightweight GNU software distribution which is designed around minimalism, making it well-suited for containerized environments built with &lt;a href=&#34;https://github.com/chainguard-dev/apko&#34;&gt;apko&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is built using &lt;a href=&#34;https://github.com/chainguard-dev/melange&#34;&gt;melange&lt;/a&gt;, and is sponsored by &lt;a href=&#34;https://chainguard.dev/&#34;&gt;Chainguard&lt;/a&gt;, which uses it to provide &lt;a href=&#34;https://chainguard.dev/chainguard-images&#34;&gt;lightweight GNU/Linux runtime images&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Wolfi APK package repository is located at &lt;a href=&#34;https://packages.wolfi.dev/os&#34;&gt;https://packages.wolfi.dev/os&lt;/a&gt; and the signing public key is at &lt;a href=&#34;https://packages.wolfi.dev/os/wolfi-signing.rsa.pub&#34;&gt;https://packages.wolfi.dev/os/wolfi-signing.rsa.pub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Mixing packages with other distributions&lt;/h2&gt; &#xA;&lt;p&gt;Mixing packages with other distributions is not supported and can create security problems.&lt;/p&gt; &#xA;&lt;h2&gt;If Wolfi is missing a package you require&lt;/h2&gt; &#xA;&lt;p&gt;Wolfi is not currently intended to be a general purpose desktop operating system. Our priority is to provide packages that enable containerized and embedded system workflows. Please keep this in mind when proposing adding packages to Wolfi. Also note that some packages may not be appropriately licensed for inclusion. FSF or OSI approved &lt;a href=&#34;https://spdx.org/licenses/&#34;&gt;licenses&lt;/a&gt; are ideal.&lt;/p&gt; &#xA;&lt;p&gt;Wolfi also aims to keep its package set as up-to-date with security patches as possible. It is a requirement that any package/version contributed to Wolfi has an actively maintained upstream.&lt;/p&gt; &#xA;&lt;p&gt;To request inclusion of a package into Wolfi please use our &lt;a href=&#34;https://wolfi.dev/os/issues/new/choose&#34;&gt;New Package Request Template&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>