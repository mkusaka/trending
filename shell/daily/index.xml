<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-03T01:36:15Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qzeleza/kvas</title>
    <updated>2024-08-03T01:36:15Z</updated>
    <id>tag:github.com,2024-08-03:/qzeleza/kvas</id>
    <link href="https://github.com/qzeleza/kvas" rel="alternate"></link>
    <summary type="html">&lt;p&gt;vpn –∏ shadowsocks –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–æ—É—Ç–µ—Ä–æ–≤ keenetic&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/qzeleza/kvas?color=orange&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-closed/qzeleza/kvas?color=success&#34; alt=&#34;GitHub closed issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/qzeleza/kvas&#34; alt=&#34;GitHub last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/commit-activity/y/qzeleza/kvas&#34; alt=&#34;GitHub commit activity&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/qzeleza/kvas&#34; alt=&#34;GitHub top language&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/code-size/qzeleza/kvas&#34; alt=&#34;GitHub code size in bytes&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://forum.keenetic.com/topic/14415-%D0%BF%D1%80%D0%BE%D0%B1%D1%83%D0%B5%D0%BC-%D0%BA%D0%B2%D0%B0%D1%81-shadowsocks-%D0%B8-%D0%B4%D1%80%D1%83%D0%B3%D0%B8%D0%B5-vpn-%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D1%8B&#34;&gt;–ö–í–ê–°&lt;/a&gt; - –∑–∞—â–∏—Ç–∞ –≤–∞—à–∏—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;–í–Ω–∏–º–∞–Ω–∏–µ!&lt;/h4&gt; &#xA;&lt;p&gt;–û—Ç–∫—Ä—ã—Ç–∞ &lt;a href=&#34;https://t.me/kvas_pro&#34;&gt;–≥—Ä—É–ø–ø–∞ –≤ –¢–µ–ª–µ–≥—Ä–∞–º–º&lt;/a&gt; —Å —Ü–µ–ª—å—é –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±–º–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –ø–æ –ø—Ä–æ–µ–∫—Ç—É.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;VPN –∏ SHADOWSOCKS –∫–ª–∏–µ–Ω—Ç –¥–ª—è &lt;a href=&#34;https://keenetic.ru/ru/&#34;&gt;—Ä–æ—É—Ç–µ—Ä–æ–≤ Keenetic&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;–ü–∞–∫–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–≤—è–∑–∫—É –∏–ª–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –í–∞—à–µ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –¥–æ–º–µ–Ω–∞–º.&lt;/h4&gt; &#xA;&lt;h4&gt;–í –ø–∞–∫–µ—Ç–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —Å–≤—è–∑–∫–∞: &lt;strong&gt;ipset&lt;/strong&gt; + –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–≤—è–∑–∫–∏ DNS —Å–µ—Ä–≤–µ—Ä–∞:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;dnsmasq (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π wildcard)&lt;/strong&gt; + &lt;strong&gt;dnscrypt-proxy2&lt;/strong&gt; + –±–ª–æ–∫–∏—Ä–æ–≤—â–∏–∫ —Ä–µ–∫–ª–∞–º—ã &lt;strong&gt;adblock&lt;/strong&gt; –∏–ª–∏&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AdGuardHome&lt;/strong&gt; (—É–∂–µ –≤—Å–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∏ —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ &lt;strong&gt;DNS&lt;/strong&gt; —Ç—Ä–∞—Ñ–∏–∫–∞ –∏ –±–ª–æ–∫–∏—Ä–æ–≤—â–∏–∫ —Ä–µ–∫–ª–∞–º—ã).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;–í —Å–≤—è–∑–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ –ø–∞–∫–µ—Ç–µ —É—Ç–∏–ª–∏—Ç—ã dnsmasq —Å &lt;strong&gt;wildcard&lt;/strong&gt;, –º–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ª—é–±—ã–º–∏ –¥–æ–º–µ–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ —Ç—Ä–µ—Ç—å–µ–≥–æ –∏ –≤—ã—à–µ —É—Ä–æ–≤–Ω–µ–π. –¢.–µ. –≤ –±–µ–ª—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å *&lt;strong&gt;domen.com&lt;/strong&gt; –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Ç—Ä–∞—Ñ–∏–∫–∞ –±—É–¥–µ—Ç –∏–¥—Ç–∏ –∫–∞–∫ –∫ &lt;strong&gt;sub1.domen.com&lt;/strong&gt;, —Ç–∞–∫ –∏ –∫ –ª—é–±–æ–º—É –¥—Ä—É–≥–æ–º—É –ø–æ–¥–¥–æ–º–µ–Ω–Ω–æ–º—É –∏–º–µ–Ω–∏ —Ç–∏–ø–∞ &lt;strong&gt;subN.domen.com&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–Ω—ã—Ö &lt;strong&gt;Keenetic&lt;/strong&gt; —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –≤–≤–∏–¥—É –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ—Å—Ç–∏ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤: &lt;strong&gt;mips, mipsel, aarch64&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; –∏—Å–ø–æ–ª—å–∑—É–µ—Ç &lt;strong&gt;dnsmasq&lt;/strong&gt;, &lt;em&gt;&lt;strong&gt;—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π&lt;/strong&gt;&lt;/em&gt;, –∞ —ç—Ç–æ –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å –¥–∞–µ—Ç –æ–¥–Ω–æ, –Ω–æ –±–æ–ª—å—à–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –º–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏ –∏ –ø—Ä–æ—á–∏–º–∏ –≤—ã—Å–æ–∫–æ-–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ —Å–∞–π—Ç–∞–º–∏, –¥–æ–±–∞–≤–∏–≤ –ª–∏—à—å –∫–æ—Ä–Ω–µ–≤—ã–µ –¥–æ–º–µ–Ω—ã –ø–æ —ç—Ç–∏–º —Å–∞–π—Ç–∞–º.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; –ø–æ–∑–≤–æ–ª—è–µ—Ç &lt;strong&gt;–æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ç–∞—Ç—É—Å/–æ—Ç–∫–ª—é—á–∞—Ç—å/–≤–∫–ª—é—á–∞—Ç—å&lt;/strong&gt; –±–ª–æ–∫–∏—Ä–æ–≤–∫—É —Ä–µ–∫–ª–∞–º—ã (–º–æ–¥—É–ª—å &lt;strong&gt;adblock&lt;/strong&gt; + &lt;strong&gt;dnsmasq&lt;/strong&gt;);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; –ø–æ–∑–≤–æ–ª—è–µ—Ç &lt;strong&gt;–æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ç–∞—Ç—É—Å/–æ—Ç–∫–ª—é—á–∞—Ç—å/–≤–∫–ª—é—á–∞—Ç—å&lt;/strong&gt; —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ &lt;strong&gt;DNS&lt;/strong&gt; (–ø–∞–∫–µ—Ç &lt;strong&gt;dnscrypt-proxy2&lt;/strong&gt;);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–≤–æ–¥–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤—Å–µ–º —ç–ª–µ–º–µ–Ω—Ç–∞–º —Å–≤—è–∑–∫–∏ &lt;strong&gt;ipset + ( dnsmasq + dnscrypt-proxy2 ) | AdGuardHome&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–¥–∫–ª—é—á–∏—Ç—å &lt;strong&gt;AdGuardHome&lt;/strong&gt; –≤ –∫–∞—á–µ—Å—Ç–≤–µ &lt;strong&gt;DNS&lt;/strong&gt; —Å–µ—Ä–≤–µ—Ä–∞, –≤–º–µ—Å—Ç–æ —Å–≤—è–∑–∫–∏ &lt;strong&gt;dnsmasq + dnscrypt-proxy2 + adblock&lt;/strong&gt;. 7&lt;strong&gt;–ö–≤–∞—Å&lt;/strong&gt; –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ —Å–ø–∏—Å–∫–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ä–µ–∫–ª–∞–º—ã, –¥–æ–±–∞–≤–ª—è–µ—Ç –∏ —É–¥–∞–ª—è–µ—Ç –¥–æ–º–µ–Ω—ã –≤ —ç—Ç–æ–º —Å–ø–∏—Å–∫–µ.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–∞&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;–ó–∞–π–¥–∏—Ç–µ –≤ &lt;strong&gt;entware&lt;/strong&gt; —Å–≤–æ–µ–≥–æ —Ä–æ—É—Ç–µ—Ä–∞ –∏ –≤–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É &lt;code&gt;curl -sOfL http://kvas.zeleza.ru/install &amp;amp;&amp;amp; sh install&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;–î–∞–ª–µ–µ, —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ.&lt;/li&gt; &#xA; &lt;li&gt;–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —á–∏—Ç–∞–π—Ç–µ &lt;a href=&#34;https://github.com/qzeleza/kvas/wiki/%D0%A3%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0-%D0%BF%D0%B0%D0%BA%D0%B5%D1%82%D0%B0&#34;&gt;–∑–¥–µ—Å—å&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –ø—Ä–æ–µ–∫—Ç–µ –ø—Ä–æ–¥—É–∫—Ç—ã&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;–î–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤, –≤ –ø—Ä–æ–µ–∫—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞–∫–µ—Ç &lt;a href=&#34;https://github.com/bats-core/bats-core/raw/master/LICENSE.md&#34;&gt;BATS&lt;/a&gt; –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö &lt;a href=&#34;https://github.com/bats-core/bats-core/raw/master/AUTHORS&#34;&gt;–ê–í–¢–û–†–û–í&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;–ü–æ–º–æ—â—å –ø—Ä–æ–µ–∫—Ç—É&lt;/h2&gt; &#xA;&lt;p&gt;–ü–æ–º–æ—á—å –º–æ–∂–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–æ–º —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ &lt;a href=&#34;https://yoomoney.ru/to/4100117756734493&#34;&gt;—ç—Ç–æ—Ç –∫–æ—à–µ–ª–µ–∫ –Æ–ú–∞–Ω–∏&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø—Ä–æ–µ–∫—Ç—É&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/qzeleza/kvas/wiki&#34;&gt;–ü–µ—Ä–µ–π—Ç–∏ –ø–æ c—Å—ã–ª–∫–µ&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;–ö–∞—Ç–∞–ª–æ–≥ –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π –ø—Ä–æ–µ–∫—Ç–∞&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/qzeleza/kvas/tree/main/ipk&#34;&gt;–ü–µ—Ä–µ–π—Ç–∏ –ø–æ c—Å—ã–ª–∫–µ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;–ò—Å—Ç–æ—Ä–∏—è &#34;–ó–≤–µ–∑–¥&#34;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#qzeleza/kvas&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=qzeleza/kvas&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen2</title>
    <updated>2024-08-03T01:36:15Z</updated>
    <id>tag:github.com,2024-08-03:/QwenLM/Qwen2</id>
    <link href="https://github.com/QwenLM/Qwen2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen2 is the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen2&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen2.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://arxiv.org/abs/2407.10671&#34;&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen2-&lt;/code&gt; or visit the &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f&#34;&gt;Qwen2 collection&lt;/a&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;p&gt;To learn more about Qwen2, feel free to read our documentation [&lt;a href=&#34;https://qwen.readthedocs.io/en/latest/&#34;&gt;EN&lt;/a&gt;|&lt;a href=&#34;https://qwen.readthedocs.io/zh-cn/latest/&#34;&gt;ZH&lt;/a&gt;]. Our documentation consists of the following sections:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quickstart: the basic usages and demonstrations;&lt;/li&gt; &#xA; &lt;li&gt;Inference: the guidance for the inference with transformers, including batch inference, streaming, etc.;&lt;/li&gt; &#xA; &lt;li&gt;Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;Ollama&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;TGI&lt;/code&gt;, etc.;&lt;/li&gt; &#xA; &lt;li&gt;Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;&lt;/li&gt; &#xA; &lt;li&gt;Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.&lt;/li&gt; &#xA; &lt;li&gt;Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.&lt;/li&gt; &#xA; &lt;li&gt;Benchmark: the statistics about inference speed and memory footprint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and &lt;strong&gt;Qwen2-72B&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Having been trained on data in &lt;strong&gt;27&lt;/strong&gt; additional languages besides English and Chinese;&lt;/li&gt; &#xA; &lt;li&gt;State-of-the-art performance in a large number of benchmark evaluations;&lt;/li&gt; &#xA; &lt;li&gt;Significantly improved performance in coding and mathematics;&lt;/li&gt; &#xA; &lt;li&gt;Extended context length support up to &lt;strong&gt;128K&lt;/strong&gt; tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024.06.06: We released the Qwen2 series. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen-moe/&#34;&gt;blog&lt;/a&gt; for more information!&lt;/li&gt; &#xA; &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Detailed evaluation results are reported in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt; üìë blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&amp;gt;=4.40.0&lt;/code&gt; for Qwen2 dense and MoE models. The latest version is recommended.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; üö® This is a must because `transformers` integrated Qwen2 codes since `4.37.0` and Qwen2Moe code since `4.40.0`. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For requirements on GPU memory and the respective throughput, see results &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;ü§ó Hugging Face Transformers&lt;/h3&gt; &#xA;&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen2-7B-Instruct&#34;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;prompt = &#34;Give me a short introduction to large language model.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=512&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For quantized models, we advise you to use the GPTQ and AWQ correspondents, namely &lt;code&gt;Qwen2-7B-Instruct-GPTQ-Int8&lt;/code&gt;, &lt;code&gt;Qwen2-7B-Instruct-AWQ&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;üíª Run locally&lt;/h3&gt; &#xA;&lt;h4&gt;Ollama&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning] You need &lt;code&gt;ollama&amp;gt;=0.1.42&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   Ollama provides an &#xA;  &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/openai.md&#34;&gt;OpenAI-compatible API&lt;/a&gt;, which however does NOT support &#xA;  &lt;b&gt;function calling&lt;/b&gt;. For tool use capabilities, consider using &#xA;  &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt;, which offers a wrapper for function calling over the API. &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/README.md&#34;&gt;installing ollama&lt;/a&gt;, you can initiate the ollama service with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama serve&#xA;# You need to keep this service running whenever you are using ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen2&lt;/code&gt;, such as &lt;code&gt;:0.5b&lt;/code&gt;, &lt;code&gt;:1.5b&lt;/code&gt;, &lt;code&gt;:7b&lt;/code&gt;, or &lt;code&gt;:72b&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run qwen2:7b&#xA;# To exit, type &#34;/bye&#34; and press ENTER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen2:7b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import OpenAI&#xA;client = OpenAI(&#xA;    base_url=&#39;http://localhost:11434/v1/&#39;,&#xA;    api_key=&#39;ollama&#39;,  # required but ignored&#xA;)&#xA;chat_completion = client.chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#39;role&#39;: &#39;user&#39;,&#xA;            &#39;content&#39;: &#39;Say this is a test&#39;,&#xA;        }&#xA;    ],&#xA;    model=&#39;qwen2:7b&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional details, please visit &lt;a href=&#34;https://ollama.ai/&#34;&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;llama.cpp&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning] You need &lt;code&gt;llama.cpp&amp;gt;=b3370&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Download our provided GGUF files or create them by yourself, and you can directly use them with the latest &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; with a one-line command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./llama-cli -m &amp;lt;path-to-file&amp;gt; -n 512 -co -i -if -f prompts/chat-with-qwen.txt --in-prefix &#34;&amp;lt;|im_start|&amp;gt;user\n&#34; --in-suffix &#34;&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;MLX-LM&lt;/h4&gt; &#xA;&lt;p&gt;If you are running on Apple Silicon, we have also provided checkpoints compatible with &lt;a href=&#34;https://github.com/ml-explore/mlx-examples/raw/main/llms/README.md&#34;&gt;&lt;code&gt;mlx-lm&lt;/code&gt;&lt;/a&gt;. Look for models ending with MLX on HuggingFace Hub, like &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-7B-Instruct-MLX&#34;&gt;Qwen2-7B-Instruct-MLX&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;LMStudio&lt;/h4&gt; &#xA;&lt;p&gt;Qwen2 has already been supported by &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; &#xA;&lt;h4&gt;OpenVINO&lt;/h4&gt; &#xA;&lt;p&gt;Qwen2 has already been supported by &lt;a href=&#34;https://github.com/openvinotoolkit&#34;&gt;OpenVINO toolkit&lt;/a&gt;. You can install and run this &lt;a href=&#34;https://github.com/OpenVINO-dev-contest/Qwen2.openvino&#34;&gt;chatbot example&lt;/a&gt; with Intel CPU, integrated GPU or discrete GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;h4&gt;Text generation web UI&lt;/h4&gt; &#xA;&lt;p&gt;You can directly use &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;&lt;code&gt;text-generation-webui&lt;/code&gt;&lt;/a&gt; for creating a web UI demo. If you use GGUF, remember to install the latest wheel of &lt;code&gt;llama.cpp&lt;/code&gt; with the support of Qwen2.&lt;/p&gt; &#xA;&lt;h4&gt;llamafile&lt;/h4&gt; &#xA;&lt;p&gt;Clone &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;&lt;code&gt;llamafile&lt;/code&gt;&lt;/a&gt;, run source install, and then create your own llamafile with the GGUF file following the guide &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#creating-llamafiles&#34;&gt;here&lt;/a&gt;. You are able to run one line of command, say &lt;code&gt;./qwen.llamafile&lt;/code&gt;, to create a demo.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Qwen2 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;vLLM&lt;/code&gt; and &lt;code&gt;SGLang&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   The OpenAI-compatible APIs provided by vLLM and SGLang currently do NOT support &#xA;  &lt;b&gt;function calling&lt;/b&gt;. For tool use capabilities, &#xA;  &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt; provides a wrapper around these APIs to support function calling. &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;We advise you to use &lt;code&gt;vLLM&amp;gt;=0.4.0&lt;/code&gt; to build OpenAI-compatible API service. Start the server with a chat model, e.g. &lt;code&gt;Qwen2-7B-Instruct&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-7B-Instruct --model Qwen/Qwen2-7B-Instruct &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use the chat API as demonstrated below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl http://localhost:8000/v1/chat/completions \&#xA;    -H &#34;Content-Type: application/json&#34; \&#xA;    -d &#39;{&#xA;    &#34;model&#34;: &#34;Qwen2-7B-Instruct&#34;,&#xA;    &#34;messages&#34;: [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}&#xA;    ]&#xA;    }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;# Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.&#xA;openai_api_key = &#34;EMPTY&#34;&#xA;openai_api_base = &#34;http://localhost:8000/v1&#34;&#xA;&#xA;client = OpenAI(&#xA;    api_key=openai_api_key,&#xA;    base_url=openai_api_base,&#xA;)&#xA;&#xA;chat_response = client.chat.completions.create(&#xA;    model=&#34;Qwen2-7B-Instruct&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;},&#xA;    ]&#xA;)&#xA;print(&#34;Chat response:&#34;, chat_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SGLang&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt;&#xA;   SGLang now does NOT support the &#xA;  &lt;b&gt;Qwen2MoeForCausalLM&lt;/b&gt; architecture, thus making &#xA;  &lt;b&gt;Qwen2-57B-A14B&lt;/b&gt; incompatible. &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Please install &lt;code&gt;SGLang&lt;/code&gt; from source. Similar to &lt;code&gt;vLLM&lt;/code&gt;, you need to launch a server and use OpenAI-compatible API service. Start the server first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sglang.launch_server --model-path Qwen/Qwen2-7B-Instruct --port 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use it in Python as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint&#xA;&#xA;@function&#xA;def multi_turn_question(s, question_1, question_2):&#xA;    s += system(&#34;You are a helpful assistant.&#34;)&#xA;    s += user(question_1)&#xA;    s += assistant(gen(&#34;answer_1&#34;, max_tokens=256))&#xA;    s += user(question_2)&#xA;    s += assistant(gen(&#34;answer_2&#34;, max_tokens=256))&#xA;&#xA;set_default_backend(RuntimeEndpoint(&#34;http://localhost:30000&#34;))&#xA;&#xA;state = multi_turn_question.run(&#xA;    question_1=&#34;What is the capital of China?&#34;,&#xA;    question_2=&#34;List two local attractions.&#34;,&#xA;)&#xA;&#xA;for m in state.messages():&#xA;    print(m[&#34;role&#34;], &#34;:&#34;, m[&#34;content&#34;])&#xA;&#xA;print(state[&#34;answer_1&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;We advise you to use training frameworks, including &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;Axolotl&lt;/a&gt;, &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;Llama-Factory&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift&#34;&gt;Swift&lt;/a&gt;, etc., to finetune your models with SFT, DPO, PPO, etc.&lt;/p&gt; &#xA;&lt;h2&gt;üê≥ Docker&lt;/h2&gt; &#xA;&lt;p&gt;To simplify the deployment process, we provide docker images with pre-built environments: &lt;a href=&#34;https://hub.docker.com/r/qwenllm/qwen&#34;&gt;qwenllm/qwen&lt;/a&gt;. You only need to install the driver and download model files to launch demos and finetune the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen2 -it qwenllm/qwen:2-cu121 bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;Check the license of each model inside its HF repo. It is NOT necessary for you to submit a request for commercial usage.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qwen2,&#xA;      title={Qwen2 Technical Report}, &#xA;      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},&#xA;      journal={arXiv preprint arXiv:2407.10671},&#xA;      year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
</feed>