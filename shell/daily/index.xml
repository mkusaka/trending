<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-28T01:38:37Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kvaps/kubectl-node-shell</title>
    <updated>2024-11-28T01:38:37Z</updated>
    <id>tag:github.com,2024-11-28:/kvaps/kubectl-node-shell</id>
    <link href="https://github.com/kvaps/kubectl-node-shell" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Exec into node via kubectl&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;kubectl node-shell&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;(formerly known as &lt;strong&gt;kubectl-enter&lt;/strong&gt;)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start a root shell in the node&#39;s host OS running. Uses an alpine pod with nsenter for Linux nodes and a &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/&#34;&gt;HostProcess pod&lt;/a&gt; with PowerShell for Windows nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gist.githubusercontent.com/kvaps/2e3d77975a844654ec297893e21a0829/raw/c778a8405ff8c686e4e807a97e9721b423e7208f/kubectl-node-shell.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;using &lt;a href=&#34;https://krew.sigs.k8s.io/&#34;&gt;krew&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Plugin can be installed from the official krew repository:&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;kubectl krew install node-shell&#xA;&lt;/pre&gt; &#xA;&lt;p&gt;Or from our own krew repository:&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;kubectl krew index add kvaps &lt;a href=&#34;https://github.com/kvaps/krew-index&#34;&gt;https://github.com/kvaps/krew-index&lt;/a&gt;&#xA;kubectl krew install kvaps/node-shell&#xA;&lt;/pre&gt; &#xA;&lt;p&gt;or using curl:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -LO https://github.com/kvaps/kubectl-node-shell/raw/master/kubectl-node_shell&#xA;chmod +x ./kubectl-node_shell&#xA;sudo mv ./kubectl-node_shell /usr/local/bin/kubectl-node_shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get standard bash shell&#xA;kubectl node-shell &amp;lt;node&amp;gt;&#xA;&#xA;# Use custom image for pod&#xA;kubectl node-shell &amp;lt;node&amp;gt; --image &amp;lt;image&amp;gt;&#xA;&#xA;# Use X-mode (mount /host, and do not enter host namespace)&#xA;kubectl node-shell -x &amp;lt;node&amp;gt;&#xA;&#xA;# Skip specific namespace types to enter, choose any of ipc, mount, pid, net, uts&#xA;kubectl node-shell &amp;lt;node&amp;gt; --no-ipc&#xA;&#xA;# Execute custom command&#xA;kubectl node-shell &amp;lt;node&amp;gt; -- echo 123&#xA;&#xA;# Use stdin&#xA;cat /etc/passwd | kubectl node-shell &amp;lt;node&amp;gt; -- sh -c &#39;cat &amp;gt; /tmp/passwd&#39;&#xA;&#xA;# Run oneliner script&#xA;kubectl node-shell &amp;lt;node&amp;gt; -- sh -c &#39;cat /tmp/passwd; rm -f /tmp/passwd&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;X-mode&lt;/h2&gt; &#xA;&lt;p&gt;X-mode can be useful for debugging minimal systems that do not have a built-in shell (eg. Talos).&lt;br&gt; Here&#39;s an example of how you can debug the network for a rootless kube-apiserver container without a filesystem:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl node-shell -x &amp;lt;node&amp;gt;&#xA;&#xA;# Download crictl&#xA;wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.28.0/crictl-v1.28.0-linux-amd64.tar.gz -O- | \&#xA;  tar -xzf- -C /usr/local/bin/&#xA;&#xA;# Setup CRI endpoint&#xA;export CONTAINER_RUNTIME_ENDPOINT=unix:///host/run/containerd/containerd.sock&#xA;&#xA;# Find your container&#xA;crictl ps | grep kube-apiserver&#xA;#3ff4626a9f10e       e7972205b6614       6 hours ago         Running             kube-apiserver         0                   215107b47bd7e       kube-apiserver-talos-rzq-nkg&#xA;&#xA;# Find pid of the container&#xA;crictl inspect 3ff4626a9f10e | grep pid&#xA;#    &#34;pid&#34;: 2152,&#xA;#            &#34;pid&#34;: 1&#xA;#            &#34;type&#34;: &#34;pid&#34;&#xA;#                &#34;getpid&#34;,&#xA;#                &#34;getppid&#34;,&#xA;#                &#34;pidfd_open&#34;,&#xA;#                &#34;pidfd_send_signal&#34;,&#xA;#                &#34;waitpid&#34;,&#xA;&#xA;# Go to network namespace of the pid, but keep mount namespace of the debug container&#xA;nsenter -t 2152 -n&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;You need to be able to start privileged containers for that.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mounting External CSI Volumes&lt;/h2&gt; &#xA;&lt;p&gt;You can mount volumes from your CSI storage layer using the &lt;code&gt;-m&lt;/code&gt; flag. This allows you to move data to/from node devices seamlessly. The PVC will be mounted at &lt;code&gt;/opt-pvc&lt;/code&gt;. This is useful for failover in minimal systems that do not have a built in shell (eg. Talos). Here is an example of how you can retrieve zfs/lvm data from a volume on a failed CSI node and put it back in your distributed storage layer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;k node-shell -n &amp;lt;namespace&amp;gt; -x &amp;lt;node_with_data&amp;gt; -m &amp;lt;pvc_name&amp;gt;&#xA;&#xA;# install rsync&#xA;apk add rsync&#xA;&#xA;# Add lvm/zfs libs&#xA;# ZFS&#xA;mount -o bind /host/dev /dev&#xA;mount -o bind /host/usr/local /usr/local&#xA;touch /lib/libuuid.so.1&#xA;mount -o bind /host/lib/libuuid.so.1 /lib/libuuid.so.1&#xA;touch /lib/libuuid.so.1.3.0&#xA;mount -o bind /host/lib/libuuid.so.1.3.0 /lib/libuuid.so.1.3.0&#xA;touch /lib/libblkid.so.1&#xA;mount -o bind /host/lib/libblkid.so.1 /lib/libblkid.so.1&#xA;touch /lib/libblkid.so.1.1.0&#xA;mount -o bind /host/lib/libblkid.so.1.1.0 /lib/libblkid.so.1.1.0&#xA;#LVM&#xA;touch /usr/lib/libaio.so.1&#xA;mount -o bind /host/usr/lib/libaio.so.1.0.2 /usr/lib/libaio.so.1&#xA;touch /usr/lib/libudev.so.1&#xA;mount -o bind /host/usr/lib/libudev.so.1 /usr/lib/libudev.so.1&#xA;export PATH=$PATH:/host/sbin&#xA;mkdir /lib/modules&#xA;mount -o bind /host/lib/modules /lib/modules&#xA;&#xA;# look for data to recover&#xA;zfs list&#xA;NAME                                                     USED  AVAIL  REFER  MOUNTPOINT&#xA;hdd-1                                                   15.9T  7.52T    96K  /hdd-1&#xA;hdd-1/SOME-OLD-PVC-FROM-PREVIOUS-NODE-INSTALL            361G  7.52T   361G  -                  -&#xA;&#xA;# mount the failed volume&#xA;zfs set mountpoint=/mnt hdd-1/SOME-OLD-PVC-FROM-PREVIOUS-NODE-INSTALL&#xA;zfs mount /hdd-1/SOME-OLD-PVC-FROM-PREVIOUS-NODE-INSTALL&#xA;&#xA;# recover the data : copy it to the mounted CSI volume&#xA;rsync -avh --info=progress2 /mnt/ /opt-pvc/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the above exemple assumes &lt;code&gt;pvc_name&lt;/code&gt; already exists in &lt;code&gt;namespace&lt;/code&gt;. &lt;em&gt;You need to be able to start privileged containers.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>