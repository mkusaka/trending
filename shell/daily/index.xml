<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Shell Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-26T01:43:44Z</updated>
  <subtitle>Daily Trending of Shell in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>thuml/Time-Series-Library</title>
    <updated>2023-04-26T01:43:44Z</updated>
    <id>tag:github.com,2023-04-26:/thuml/Time-Series-Library</id>
    <link href="https://github.com/thuml/Time-Series-Library" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Library for Advanced Deep Time Series Models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Time Series Library (TSlib)&lt;/h1&gt; &#xA;&lt;p&gt;TSlib is an open-source library for deep learning researchers, especially deep time series analysis.&lt;/p&gt; &#xA;&lt;p&gt;We provide a neat code base to evaluate advanced deep time series models or develop your own model, which covers five mainstream tasks: &lt;strong&gt;long- and short-term forecasting, imputation, anomaly detection, and classification.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Leaderboard for Time Series Analysis&lt;/h2&gt; &#xA;&lt;p&gt;Till February 2023, the top three models for five different tasks are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;br&gt;Ranking&lt;/th&gt; &#xA;   &lt;th&gt;Long-term&lt;br&gt;Forecasting&lt;/th&gt; &#xA;   &lt;th&gt;Short-term&lt;br&gt;Forecasting&lt;/th&gt; &#xA;   &lt;th&gt;Imputation&lt;/th&gt; &#xA;   &lt;th&gt;Anomaly&lt;br&gt;Detection&lt;/th&gt; &#xA;   &lt;th&gt;Classification&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ðŸ¥‡ 1st&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ðŸ¥ˆ 2nd&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cure-lab/LTSF-Linear&#34;&gt;DLinear&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;FEDformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ðŸ¥‰ 3rd&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;FEDformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhouhaoyi/Informer2020&#34;&gt;Informer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: We will keep updating this leaderborad.&lt;/strong&gt; If you have proposed advanced and awesome models, welcome to send your paper/code link to us or raise a pull request. We will add them to this repo and update the leaderborad as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compared models of this leaderboard.&lt;/strong&gt; â˜‘ means that their codes have already been included in this repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TimesNet&lt;/strong&gt; - TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis &lt;a href=&#34;https://openreview.net/pdf?id=ju_Uqw384Oq&#34;&gt;[ICLR 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TimesNet.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;DLinear&lt;/strong&gt; - Are Transformers Effective for Time Series Forecasting? &lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;[AAAI 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/DLinear.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;LightTS&lt;/strong&gt; - Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures &lt;a href=&#34;https://arxiv.org/abs/2207.01186&#34;&gt;[arXiv 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/LightTS.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;ETSformer&lt;/strong&gt; - ETSformer: Exponential Smoothing Transformers for Time-series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2202.01381&#34;&gt;[arXiv 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/ETSformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Non-stationary Transformer&lt;/strong&gt; - Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=ucNDIDRNjjv&#34;&gt;[NeurIPS 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Nonstationary_Transformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FEDformer&lt;/strong&gt; - FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting &lt;a href=&#34;https://proceedings.mlr.press/v162/zhou22g.html&#34;&gt;[ICML 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FEDformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Pyraformer&lt;/strong&gt; - Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=0EXmFzUn5I&#34;&gt;[ICLR 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Pyraformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Autoformer&lt;/strong&gt; - Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=I55UqU-M11y&#34;&gt;[NeurIPS 2021]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Autoformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Informer&lt;/strong&gt; - Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132&#34;&gt;[AAAI 2021]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Informer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Reformer&lt;/strong&gt; - Reformer: The Efficient Transformer &lt;a href=&#34;https://openreview.net/forum?id=rkgNKkHtvB&#34;&gt;[ICLR 2020]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Reformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Transformer&lt;/strong&gt; - Attention is All You Need &lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;[NeurIPS 2017]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Transformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See our latest paper &lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;[TimesNet]&lt;/a&gt; for the comprehensive benchmark. We will release a real-time updated online version in March.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Newly added baselines.&lt;/strong&gt; We will add them into the leaderboard after a comprehensive evaluation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FiLM&lt;/strong&gt; - FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting &lt;a href=&#34;https://openreview.net/forum?id=zTQdHSQUQWc&#34;&gt;[NeurIPS 2022]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FiLM.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;PatchTST&lt;/strong&gt; - A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. &lt;a href=&#34;https://openreview.net/pdf?id=Jbdc0vTOcol&#34;&gt;[ICLR 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/PatchTST.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;MICN&lt;/strong&gt; - MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=zt53IDUR1U&#34;&gt;[ICLR 2023]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/MICN.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Crossformer&lt;/strong&gt; - Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=vSVLM2j9eie&#34;&gt;[ICLR 2023]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Crossformer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Python 3.8. For convenience, execute the following command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Prepare Data. You can obtained the well pre-processed datasets from &lt;a href=&#34;https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt;, &lt;a href=&#34;https://cloud.tsinghua.edu.cn/f/84fbc752d0e94980a610/&#34;&gt;[Tsinghua Cloud]&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy&#34;&gt;[Baidu Drive]&lt;/a&gt;. Then place the downloaded data under the folder &lt;code&gt;./dataset&lt;/code&gt;. Here is a summary of supported datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;.\pic\dataset.png&#34; height=&#34;200&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Train and evaluate model. We provide the experiment scripts of all benchmarks under the folder &lt;code&gt;./scripts/&lt;/code&gt;. You can reproduce the experiment results as the following examples:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# long-term forecast&#xA;bash ./scripts/long_term_forecast/ETT_script/TimesNet_ETTh1.sh&#xA;# short-term forecast&#xA;bash ./scripts/short_term_forecast/TimesNet_M4.sh&#xA;# imputation&#xA;bash ./scripts/imputation/ETT_script/TimesNet_ETTh1.sh&#xA;# anomaly detection&#xA;bash ./scripts/anomaly_detection/PSM/TimesNet.sh&#xA;# classification&#xA;bash ./scripts/classification/TimesNet.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Develop your own model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the model file to the folder &lt;code&gt;./models&lt;/code&gt;. You can follow the &lt;code&gt;./models/Transformer.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Include the newly added model in the &lt;code&gt;Exp_Basic.model_dict&lt;/code&gt; of &lt;code&gt;./exp/exp_basic.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create the corresponding scripts under the folder &lt;code&gt;./scripts&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wu2023timesnet,&#xA;  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},&#xA;  author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},&#xA;  booktitle={International Conference on Learning Representations},&#xA;  year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, feel free to contact:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Haixu Wu (&lt;a href=&#34;mailto:whx20@mails.tsinghua.edu.cn&#34;&gt;whx20@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tengge Hu (&lt;a href=&#34;mailto:htg21@mails.tsinghua.edu.cn&#34;&gt;htg21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Haoran Zhang (&lt;a href=&#34;mailto:z-hr20@mails.tsinghua.edu.cn&#34;&gt;z-hr20@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;or describe it in Issues.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This library is constructed based on the following repos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Forecasting: &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Anomaly Detection: &lt;a href=&#34;https://github.com/thuml/Anomaly-Transformer&#34;&gt;https://github.com/thuml/Anomaly-Transformer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classification: &lt;a href=&#34;https://github.com/thuml/Flowformer&#34;&gt;https://github.com/thuml/Flowformer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All the experiment datasets are public and we obtain them from the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Long-term Forecasting and Imputation: &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Short-term Forecasting: &lt;a href=&#34;https://github.com/ServiceNow/N-BEATS&#34;&gt;https://github.com/ServiceNow/N-BEATS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Anomaly Detection: &lt;a href=&#34;https://github.com/thuml/Anomaly-Transformer&#34;&gt;https://github.com/thuml/Anomaly-Transformer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classification: &lt;a href=&#34;https://www.timeseriesclassification.com/&#34;&gt;https://www.timeseriesclassification.com/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>