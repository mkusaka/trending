<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-01T02:11:42Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>crealytics/spark-excel</title>
    <updated>2024-03-01T02:11:42Z</updated>
    <id>tag:github.com,2024-03-01:/crealytics/spark-excel</id>
    <link href="https://github.com/crealytics/spark-excel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Spark plugin for reading and writing Excel files&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Excel Library&lt;/h1&gt; &#xA;&lt;p&gt;A library for querying Excel files with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/crealytics/spark-excel/actions&#34;&gt;&lt;img src=&#34;https://github.com/crealytics/spark-excel/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.crealytics/spark-excel_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.crealytics/spark-excel_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Co-maintainers wanted&lt;/h2&gt; &#xA;&lt;p&gt;Due to personal and professional constraints, the development of this library has been rather slow. If you find value in this library, please consider stepping up as a co-maintainer by leaving a comment &lt;a href=&#34;https://github.com/crealytics/spark-excel/issues/191&#34;&gt;here&lt;/a&gt;. Help is very welcome e.g. in the following areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional features&lt;/li&gt; &#xA; &lt;li&gt;Code improvements and reviews&lt;/li&gt; &#xA; &lt;li&gt;Bug analysis and fixing&lt;/li&gt; &#xA; &lt;li&gt;Documentation improvements&lt;/li&gt; &#xA; &lt;li&gt;Build / test infrastructure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 2.0+.&lt;/p&gt; &#xA;&lt;p&gt;List of spark versions, those are automatically tested:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark: [&#34;2.4.1&#34;, &#34;2.4.7&#34;, &#34;2.4.8&#34;, &#34;3.0.1&#34;, &#34;3.0.3&#34;, &#34;3.1.1&#34;, &#34;3.1.2&#34;, &#34;3.2.4&#34;, &#34;3.3.2&#34;, &#34;3.4.1&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more detail, please refer to project CI: &lt;a href=&#34;https://github.com/crealytics/spark-excel/raw/main/.github/workflows/ci.yml#L10&#34;&gt;ci.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.12&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.crealytics&#xA;artifactId: spark-excel_2.12&#xA;version: &amp;lt;spark-version&amp;gt;_0.18.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.crealytics&#xA;artifactId: spark-excel_2.11&#xA;version: &amp;lt;spark-version&amp;gt;_0.13.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.12&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.12:&amp;lt;spark-version&amp;gt;_0.18.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.11:&amp;lt;spark-version&amp;gt;_0.13.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This package allows querying Excel spreadsheets as &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;From spark-excel &lt;a href=&#34;https://github.com/crealytics/spark-excel/releases/tag/v0.14.0&#34;&gt;0.14.0&lt;/a&gt; (August 24, 2021), there are two implementation of spark-excel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Original Spark-Excel with Spark data source API 1.0&lt;/li&gt; &#xA;   &lt;li&gt;Spark-Excel V2 with data source API V2.0+, which supports loading from multiple files, corrupted record handling and some improvement on handling data types. See below for further details&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use V2 implementation, just change your .format from &lt;code&gt;.format(&#34;com.crealytics.spark.excel&#34;)&lt;/code&gt; to &lt;code&gt;.format(&#34;excel&#34;)&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/#excel-api-based-on-datasourcev2&#34;&gt;below&lt;/a&gt; for some details&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/CHANGELOG.md&#34;&gt;changelog&lt;/a&gt; for latest features, fixes etc.&lt;/p&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 2.0+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Create a DataFrame from an Excel file&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read&#xA;    .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;    .option(&#34;dataAddress&#34;, &#34;&#39;My Sheet&#39;!B3:C35&#34;) // Optional, default: &#34;A1&#34;&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Required&#xA;    .option(&#34;treatEmptyValuesAsNulls&#34;, &#34;false&#34;) // Optional, default: true&#xA;    .option(&#34;setErrorCellsToFallbackValues&#34;, &#34;true&#34;) // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column&#39;s data type.&#xA;    .option(&#34;usePlainNumberFormat&#34;, &#34;false&#34;) // Optional, default: false, If true, format the cells without rounding and scientific notations&#xA;    .option(&#34;inferSchema&#34;, &#34;false&#34;) // Optional, default: false&#xA;    .option(&#34;addColorColumns&#34;, &#34;true&#34;) // Optional, default: false&#xA;    .option(&#34;timestampFormat&#34;, &#34;MM-dd-yyyy HH:mm:ss&#34;) // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]&#xA;    .option(&#34;maxRowsInMemory&#34;, 20) // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)&#xA;    .option(&#34;maxByteArraySize&#34;, 2147483647) // Optional, default None. See https://poi.apache.org/apidocs/5.0/org/apache/poi/util/IOUtils.html#setByteArrayMaxOverride-int-&#xA;    .option(&#34;tempFileThreshold&#34;, 10000000) // Optional, default None. Number of bytes at which a zip entry is regarded as too large for holding in memory and the data is put in a temp file instead&#xA;    .option(&#34;excerptSize&#34;, 10) // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from&#xA;    .option(&#34;workbookPassword&#34;, &#34;pass&#34;) // Optional, default None. Requires unlimited strength JCE for older JVMs&#xA;    .schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings&#xA;    .load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For convenience, there is an implicit that wraps the &lt;code&gt;DataFrameReader&lt;/code&gt; returned by &lt;code&gt;spark.read&lt;/code&gt; and provides a &lt;code&gt;.excel&lt;/code&gt; method which accepts all possible options and provides default values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;import com.crealytics.spark.excel._&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read.excel(&#xA;    header = true,  // Required&#xA;    dataAddress = &#34;&#39;My Sheet&#39;!B3:C35&#34;, // Optional, default: &#34;A1&#34;&#xA;    treatEmptyValuesAsNulls = false,  // Optional, default: true&#xA;    setErrorCellsToFallbackValues = false, // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column&#39;s data type.&#xA;    usePlainNumberFormat = false,  // Optional, default: false. If true, format the cells without rounding and scientific notations&#xA;    inferSchema = false,  // Optional, default: false&#xA;    addColorColumns = true,  // Optional, default: false&#xA;    timestampFormat = &#34;MM-dd-yyyy HH:mm:ss&#34;,  // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]&#xA;    maxRowsInMemory = 20,  // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)&#xA;    maxByteArraySize = 2147483647,  // Optional, default None. See https://poi.apache.org/apidocs/5.0/org/apache/poi/util/IOUtils.html#setByteArrayMaxOverride-int-&#xA;    tempFileThreshold = 10000000, // Optional, default None. Number of bytes at which a zip entry is regarded as too large for holding in memory and the data is put in a temp file instead&#xA;    excerptSize = 10,  // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from&#xA;    workbookPassword = &#34;pass&#34;  // Optional, default None. Requires unlimited strength JCE for older JVMs&#xA;).schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings&#xA; .load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the sheet name is unavailable, it is possible to pass in an index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = spark.read.excel(&#xA;  header = true,&#xA;  dataAddress = &#34;0!B3:C35&#34;&#xA;).load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to read in the names dynamically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.crealytics.spark.excel.WorkbookReader&#xA;val sheetNames = WorkbookReader( Map(&#34;path&#34; -&amp;gt; &#34;Worktime.xlsx&#34;)&#xA;                               , spark.sparkContext.hadoopConfiguration&#xA;                               ).sheetNames&#xA;val df = spark.read.excel(&#xA;  header = true,&#xA;  dataAddress = sheetNames(0)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Create a DataFrame from an Excel file using custom schema&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;import org.apache.spark.sql.types._&#xA;&#xA;val peopleSchema = StructType(Array(&#xA;    StructField(&#34;Name&#34;, StringType, nullable = false),&#xA;    StructField(&#34;Age&#34;, DoubleType, nullable = false),&#xA;    StructField(&#34;Occupation&#34;, StringType, nullable = false),&#xA;    StructField(&#34;Date of birth&#34;, StringType, nullable = false)))&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read&#xA;    .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;    .option(&#34;dataAddress&#34;, &#34;&#39;Info&#39;!A1&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .schema(peopleSchema)&#xA;    .load(&#34;People.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Write a DataFrame to an Excel file&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val df: DataFrame = ???&#xA;df.write&#xA;  .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;  .option(&#34;dataAddress&#34;, &#34;&#39;My Sheet&#39;!B3:C35&#34;)&#xA;  .option(&#34;header&#34;, &#34;true&#34;)&#xA;  .option(&#34;dateFormat&#34;, &#34;yy-mmm-d&#34;) // Optional, default: yy-m-d h:mm&#xA;  .option(&#34;timestampFormat&#34;, &#34;mm-dd-yyyy hh:mm:ss&#34;) // Optional, default: yyyy-mm-dd hh:mm:ss.000&#xA;  .mode(&#34;append&#34;) // Optional, default: overwrite.&#xA;  .save(&#34;Worktime2.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Data Addresses&lt;/h4&gt; &#xA;&lt;p&gt;As you can see in the examples above, the location of data to read or write can be specified with the &lt;code&gt;dataAddress&lt;/code&gt; option. Currently the following address styles are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;B3&lt;/code&gt;: Start cell of the data. Reading will return all rows below and all columns to the right. Writing will start here and use as many columns and rows as required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;B3:F35&lt;/code&gt;: Cell range of data. Reading will return only rows and columns in the specified range. Writing will start in the first cell (&lt;code&gt;B3&lt;/code&gt; in this example) and use only the specified columns and rows. If there are more rows or columns in the DataFrame to write, they will be truncated. Make sure this is what you want.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;My Sheet&#39;!B3:F35&lt;/code&gt;: Same as above, but with a specific sheet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MyTable[#All]&lt;/code&gt;: Table of data. Reading will return all rows and columns in this table. Writing will only write within the current range of the table. No growing of the table will be performed. PRs to change this are welcome.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Excel API based on DataSourceV2&lt;/h3&gt; &#xA;&lt;p&gt;The V2 API offers you several improvements when it comes to file and folder handling. and works in a very similar way than data sources like csv and parquet.&lt;/p&gt; &#xA;&lt;p&gt;To use V2 implementation, just change your .format from &lt;code&gt;.format(&#34;com.crealytics.spark.excel&#34;)&lt;/code&gt; to &lt;code&gt;.format(&#34;excel&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The big difference is the fact that you provide a path to read / write data from/to and not an individual single file only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dataFrame.write&#xA;        .format(&#34;excel&#34;)&#xA;        .save(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;spark.read&#xA;        .format(&#34;excel&#34;)&#xA;        // ... insert excel read specific options you need&#xA;        .load(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because folders are supported you can read/write from/to a &#34;partitioned&#34; folder structure, just the same way as csv or parquet. Note that writing partitioned structures is only available for spark &amp;gt;=3.0.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dataFrame.write&#xA;        .partitionBy(&#34;col1&#34;)&#xA;        .format(&#34;excel&#34;)&#xA;        .save(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Need some more examples? Check out the &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/src/test/scala/com/crealytics/spark/v2/excel/DataFrameWriterApiComplianceSuite.scala&#34;&gt;test cases&lt;/a&gt; or have a look at our wiki&lt;/p&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;https://github.com/com-lihaoyi/mill&#34;&gt;Mill&lt;/a&gt;. To build a JAR file simply run e.g. &lt;code&gt;mill spark-excel[2.13.10,3.3.1].assembly&lt;/code&gt; from the project root, where &lt;code&gt;2.13.10&lt;/code&gt; is the Scala version and &lt;code&gt;3.3.1&lt;/code&gt; the Spark version. To list all available combinations of Scala and Spark, run &lt;code&gt;mill resolve spark-excel[__]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#crealytics/spark-excel&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=crealytics/spark-excel&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scalameta/metals</title>
    <updated>2024-03-01T02:11:42Z</updated>
    <id>tag:github.com,2024-03-01:/scalameta/metals</id>
    <link href="https://github.com/scalameta/metals" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala language server with rich IDE features ðŸš€&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Metals&lt;/h1&gt; &#xA;&lt;a href=&#34;https://discord.gg/FaVDrJegEh&#34;&gt; &lt;img alt=&#34;Chat with us on discord&#34; src=&#34;https://img.shields.io/discord/632642981228314653&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://twitter.com/scalameta&#34;&gt; &lt;img alt=&#34;Follow scalameta on Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/scalameta.svg?logo=twitter&amp;amp;color=blue&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://index.scala-lang.org/scalameta/metals/metals&#34;&gt; &lt;img alt=&#34;Find us on scaladex&#34; src=&#34;https://index.scala-lang.org/scalameta/metals/metals/latest.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See the website: &lt;a href=&#34;https://scalameta.org/metals/&#34;&gt;https://scalameta.org/metals/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See the contributing guide: &lt;a href=&#34;https://scalameta.org/metals/docs/contributors/getting-started.html&#34;&gt;https://scalameta.org/metals/docs/contributors/getting-started.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To learn more about how Metals works, see &lt;a href=&#34;https://raw.githubusercontent.com/scalameta/metals/main/architecture.md&#34;&gt;./architecture.md&lt;/a&gt;. It explains the high-level layout of the source code. Do skim through that document.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements and Development&lt;/h3&gt; &#xA;&lt;p&gt;For more information on the current maintainers, companies that have/are sponsoring the development of Metals, and acknowledgements of previous work, please see &lt;a href=&#34;https://scalameta.org/metals/docs/acknowledgements/development.html&#34;&gt;https://scalameta.org/metals/docs/acknowledgements/development.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Alternatives&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jetbrains.com/help/idea/discover-intellij-idea-for-scala.html&#34;&gt;IntelliJ IDEA&lt;/a&gt;: the most widely used IDE for Scala using a re-implementation of the Scala typechecker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Metals?&lt;/h2&gt; &#xA;&lt;p&gt;Metals = Meta (from Scalameta) + LS (from Language Server)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rockthejvm/scala-at-light-speed</title>
    <updated>2024-03-01T02:11:42Z</updated>
    <id>tag:github.com,2024-03-01:/rockthejvm/scala-at-light-speed</id>
    <link href="https://github.com/rockthejvm/scala-at-light-speed" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The repository for the free Scala at Light Speed mini-course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The official repository for the Rock the JVM Scala at Light Speed course&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code we wrote during &lt;a href=&#34;https://rockthejvm.com/course/scala-at-light-speed&#34;&gt;Rock the JVM&#39;s Scala at Light Speed&lt;/a&gt; mini-course/video series. Unless explicitly mentioned, the code in this repository is exactly what was caught on camera.&lt;/p&gt; &#xA;&lt;h2&gt;How to install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;if you don&#39;t have IntelliJ IDEA installed, install it from the &lt;a href=&#34;https://jetbrains.com&#34;&gt;official site&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;either clone the repo or download as zip&lt;/li&gt; &#xA; &lt;li&gt;open with IntelliJ as an SBT project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to start&lt;/h3&gt; &#xA;&lt;p&gt;Clone this repository and checkout the &lt;code&gt;start&lt;/code&gt; tag by running the following in the repo folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to see the final code&lt;/h3&gt; &#xA;&lt;p&gt;The master branch contains the final code, so you can download the repository as is, or if you&#39;ve navigated somewhere in the code history, then do&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git checkout master&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to run an intermediate state&lt;/h3&gt; &#xA;&lt;p&gt;The repository was built while recording the videos. Prior to each video, I tagged each commit so you can easily go back to an earlier state of the repo!&lt;/p&gt; &#xA;&lt;p&gt;The tags are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;basics&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;object-orientation&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;functional-programming&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pattern-matching&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When you watch a video, you can &lt;code&gt;git checkout&lt;/code&gt; the appropriate tag and the repo will go back to the exact code I had when I started the lecture.&lt;/p&gt; &#xA;&lt;h3&gt;For questions or suggestions&lt;/h3&gt; &#xA;&lt;p&gt;If you have changes to suggest to this repo, either&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;submit a GitHub issue&lt;/li&gt; &#xA; &lt;li&gt;tell me in the comments to the videos&lt;/li&gt; &#xA; &lt;li&gt;submit a pull request!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>