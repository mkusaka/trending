<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-01T02:09:23Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>delta-io/delta-sharing</title>
    <updated>2024-02-01T02:09:23Z</updated>
    <id>tag:github.com,2024-02-01:/delta-io/delta-sharing</id>
    <link href="https://github.com/delta-io/delta-sharing" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open protocol for secure data sharing&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/1446829/144671151-b095e1b9-2d24-4d3b-b3c6-a7041e491077.png&#34; alt=&#34;Delta Sharing Logo&#34; width=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Delta Sharing: An Open Protocol for Secure Data Sharing&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta-sharing/actions/workflows/build-and-test.yml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta-sharing/actions/workflows/build-and-test.yml/badge.svg?sanitize=true&#34; alt=&#34;Build and Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta-sharing/raw/main/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-sharing/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-sharing.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://delta.io/sharing&#34;&gt;Delta Sharing&lt;/a&gt; is an open protocol for secure real-time exchange of large datasets, which enables organizations to share data in real time regardless of which computing platforms they use. It is a simple &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;REST protocol&lt;/a&gt; that securely shares access to part of a cloud dataset and leverages modern cloud storage systems, such as S3, ADLS, or GCS, to reliably transfer data.&lt;/p&gt; &#xA;&lt;p&gt;With Delta Sharing, a user accessing shared data can directly connect to it through pandas, Tableau, Apache Spark, Rust, or other systems that support the open protocol, without having to deploy a specific compute platform first. Data providers can share a dataset once to reach a broad range of consumers, while consumers can begin using the data in minutes.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/images/delta-sharing.png&#34; width=&#34;85%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This repo includes the following components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Delta Sharing &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;protocol specification&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Python Connector: A Python library that implements the Delta Sharing Protocol to read shared tables as &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt; DataFrame or &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; DataFrames.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; Connector: An Apache Spark connector that implements the Delta Sharing Protocol to read shared tables from a Delta Sharing Server. The tables can then be accessed in SQL, Python, Java, Scala, or R.&lt;/li&gt; &#xA; &lt;li&gt;Delta Sharing Server: A reference implementation server for the Delta Sharing Protocol for development purposes. Users can deploy this server to share existing tables in Delta Lake and Apache Parquet format on modern cloud storage systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Python Connector&lt;/h1&gt; &#xA;&lt;p&gt;The Delta Sharing Python Connector is a Python library that implements the &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;Delta Sharing Protocol&lt;/a&gt; to read tables from a Delta Sharing Server. You can load shared tables as a &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt; DataFrame, or as an &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; DataFrame if running in PySpark with the Apache Spark Connector installed.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Python 3.6+&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install delta-sharing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using &lt;a href=&#34;https://docs.databricks.com/runtime/dbr.html&#34;&gt;Databricks Runtime&lt;/a&gt;, you can follow &lt;a href=&#34;https://docs.databricks.com/libraries/index.html&#34;&gt;Databricks Libraries doc&lt;/a&gt; to install the library on your clusters.&lt;/p&gt; &#xA;&lt;h2&gt;Accessing Shared Data&lt;/h2&gt; &#xA;&lt;p&gt;The connector accesses shared tables based on &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md#profile-file-format&#34;&gt;profile files&lt;/a&gt;, which are JSON files containing a user&#39;s credentials to access a Delta Sharing Server. We have several ways to get started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the profile file to access an open, example Delta Sharing Server that we&#39;re hosting &lt;a href=&#34;https://databricks-datasets-oregon.s3-us-west-2.amazonaws.com/delta-sharing/share/open-datasets.share&#34;&gt;here&lt;/a&gt;. You can try the connectors with this sample data.&lt;/li&gt; &#xA; &lt;li&gt;Start your own &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/#delta-sharing-reference-server&#34;&gt;Delta Sharing Server&lt;/a&gt; and create your own profile file following &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md#profile-file-format&#34;&gt;profile file format&lt;/a&gt; to connect to this server.&lt;/li&gt; &#xA; &lt;li&gt;Download a profile file from your data provider.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;After you save the profile file, you can use it in the connector to access shared tables.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import delta_sharing&#xA;&#xA;# Point to the profile file. It can be a file on the local file system or a file on a remote storage.&#xA;profile_file = &#34;&amp;lt;profile-file-path&amp;gt;&#34;&#xA;&#xA;# Create a SharingClient.&#xA;client = delta_sharing.SharingClient(profile_file)&#xA;&#xA;# List all shared tables.&#xA;client.list_all_tables()&#xA;&#xA;# Create a url to access a shared table.&#xA;# A table path is the profile file path following with `#` and the fully qualified name of a table &#xA;# (`&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;`).&#xA;table_url = profile_file + &#34;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;&#xA;&#xA;# Fetch 10 rows from a table and convert it to a Pandas DataFrame. This can be used to read sample data &#xA;# from a table that cannot fit in the memory.&#xA;delta_sharing.load_as_pandas(table_url, limit=10)&#xA;&#xA;# Load a table as a Pandas DataFrame. This can be used to process tables that can fit in the memory.&#xA;delta_sharing.load_as_pandas(table_url)&#xA;&#xA;# If the code is running with PySpark, you can use `load_as_spark` to load the table as a Spark DataFrame.&#xA;delta_sharing.load_as_spark(table_url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the table supports history sharing(&lt;code&gt;tableConfig.cdfEnabled=true&lt;/code&gt; in the OSS Delta Sharing Server), the connector can query table changes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load table changes from version 0 to version 5, as a Pandas DataFrame.&#xA;delta_sharing.load_table_changes_as_pandas(table_url, starting_version=0, ending_version=5)&#xA;&#xA;# If the code is running with PySpark, you can load table changes as Spark DataFrame.&#xA;delta_sharing.load_table_changes_as_spark(table_url, starting_version=0, ending_version=5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can try this by running our &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/examples/README.md&#34;&gt;examples&lt;/a&gt; with the open, example Delta Sharing Server.&lt;/p&gt; &#xA;&lt;h3&gt;Details on Profile Paths&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The profile file path for &lt;code&gt;SharingClient&lt;/code&gt; and &lt;code&gt;load_as_pandas&lt;/code&gt; can be any URL supported by &lt;a href=&#34;https://filesystem-spec.readthedocs.io/en/latest/index.html&#34;&gt;FSSPEC&lt;/a&gt; (such as &lt;code&gt;s3a://my_bucket/my/profile/file&lt;/code&gt;). If you are using &lt;a href=&#34;https://docs.databricks.com/data/databricks-file-system.html&#34;&gt;Databricks File System&lt;/a&gt;, you can also &lt;a href=&#34;https://docs.databricks.com/data/databricks-file-system.html#dbfs-and-local-driver-node-paths&#34;&gt;preface the path with &lt;code&gt;/dbfs/&lt;/code&gt;&lt;/a&gt; to access the profile file as if it were a local file.&lt;/li&gt; &#xA; &lt;li&gt;The profile file path for &lt;code&gt;load_as_spark&lt;/code&gt; can be any URL supported by Hadoop FileSystem (such as &lt;code&gt;s3a://my_bucket/my/profile/file&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;A table path is the profile file path following with &lt;code&gt;#&lt;/code&gt; and the fully qualified name of a table (&lt;code&gt;&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Apache Spark Connector&lt;/h1&gt; &#xA;&lt;p&gt;The Apache Spark Connector implements the &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;Delta Sharing Protocol&lt;/a&gt; to read shared tables from a Delta Sharing Server. It can be used in SQL, Python, Java, Scala and R.&lt;/p&gt; &#xA;&lt;h2&gt;System Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 8+&lt;/li&gt; &#xA; &lt;li&gt;Scala 2.12.x&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark 3+ or &lt;a href=&#34;https://docs.databricks.com/runtime/dbr.html&#34;&gt;Databricks Runtime&lt;/a&gt; 7+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Accessing Shared Data&lt;/h2&gt; &#xA;&lt;p&gt;The connector loads user credentials from profile files. Please see &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/#accessing-shared-data&#34;&gt;Accessing Shared Data&lt;/a&gt; to download a profile file for our example server or for your own data sharing server.&lt;/p&gt; &#xA;&lt;h2&gt;Configuring Apache Spark&lt;/h2&gt; &#xA;&lt;p&gt;You can set up Apache Spark to load the Delta Sharing connector in the following two ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run interactively: Start the Spark shell (Scala or Python) with the Delta Sharing connector and run the code snippets interactively in the shell.&lt;/li&gt; &#xA; &lt;li&gt;Run as a project: Set up a Maven or SBT project (Scala or Java) with the Delta Sharing connector, copy the code snippets into a source file, and run the project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are using &lt;a href=&#34;https://docs.databricks.com/runtime/dbr.html&#34;&gt;Databricks Runtime&lt;/a&gt;, you can skip this section and follow &lt;a href=&#34;https://docs.databricks.com/libraries/index.html&#34;&gt;Databricks Libraries doc&lt;/a&gt; to install the connector on your clusters.&lt;/p&gt; &#xA;&lt;h3&gt;Set up an interactive shell&lt;/h3&gt; &#xA;&lt;p&gt;To use Delta Sharing connector interactively within the Spark’s Scala/Python shell, you can launch the shells as follows.&lt;/p&gt; &#xA;&lt;h4&gt;PySpark shell&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyspark --packages io.delta:delta-sharing-spark_2.12:0.6.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Scala Shell&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;bin/spark-shell --packages io.delta:delta-sharing-spark_2.12:0.6.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Set up a standalone project&lt;/h3&gt; &#xA;&lt;p&gt;If you want to build a Java/Scala project using Delta Sharing connector from Maven Central Repository, you can use the following Maven coordinates.&lt;/p&gt; &#xA;&lt;h4&gt;Maven&lt;/h4&gt; &#xA;&lt;p&gt;You include Delta Sharing connector in your Maven project by adding it as a dependency in your POM file. Delta Sharing connector is compiled with Scala 2.12.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;io.delta&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;delta-sharing-spark_2.12&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;0.6.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SBT&lt;/h4&gt; &#xA;&lt;p&gt;You include Delta Sharing connector in your SBT project by adding the following line to your &lt;code&gt;build.sbt&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;io.delta&#34; %% &#34;delta-sharing-spark&#34; % &#34;0.6.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;After you save the profile file and launch Spark with the connector library, you can access shared tables using any language.&lt;/p&gt; &#xA;&lt;h3&gt;SQL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- A table path is the profile file path following with `#` and the fully qualified name &#xA;-- of a table (`&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;`).&#xA;CREATE TABLE mytable USING deltaSharing LOCATION &#39;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#39;;&#xA;SELECT * FROM mytable;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# A table path is the profile file path following with `#` and the fully qualified name &#xA;# of a table (`&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;`).&#xA;table_path = &#34;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;&#xA;df = spark.read.format(&#34;deltaSharing&#34;).load(table_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// A table path is the profile file path following with `#` and the fully qualified name &#xA;// of a table (`&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;`).&#xA;val tablePath = &#34;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;&#xA;val df = spark.read.format(&#34;deltaSharing&#34;).load(tablePath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// A table path is the profile file path following with `#` and the fully qualified name &#xA;// of a table (`&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;`).&#xA;String tablePath = &#34;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;;&#xA;Dataset&amp;lt;Row&amp;gt; df = spark.read.format(&#34;deltaSharing&#34;).load(tablePath);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# A table path is the profile file path following with `#` and the fully qualified name &#xA;# of a table (`&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;`).&#xA;table_path &amp;lt;- &#34;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;&#xA;df &amp;lt;- read.df(table_path, &#34;deltaSharing&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can try this by running our &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/examples/README.md&#34;&gt;examples&lt;/a&gt; with the open, example Delta Sharing Server.&lt;/p&gt; &#xA;&lt;h3&gt;CDF&lt;/h3&gt; &#xA;&lt;p&gt;Starting from release 0.5.0, querying &lt;a href=&#34;https://docs.databricks.com/delta/delta-change-data-feed.html&#34;&gt;Change Data Feed&lt;/a&gt; is supported with Delta Sharing. Once the provider turns on CDF on the original delta table and shares it through Delta Sharing, the recipient can query CDF of a Delta Sharing table similar to CDF of a delta table.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val tablePath = &#34;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;&#xA;val df = spark.read.format(&#34;deltaSharing&#34;)&#xA;  .option(&#34;readChangeFeed&#34;, &#34;true&#34;)&#xA;  .option(&#34;startingVersion&#34;, &#34;3&#34;)&#xA;  .load(tablePath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;p&gt;Starting from release 0.6.0, Delta Sharing table can be used as a data source for &lt;a href=&#34;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html&#34;&gt;Spark Structured Streaming&lt;/a&gt;. Once the provider shares a table with history, the recipient can perform a streaming query on the table.&lt;/p&gt; &#xA;&lt;p&gt;Note: Trigger.AvailableNow is not supported in delta sharing streaming because it&#39;s supported since spark 3.3.0, while delta sharing is still using spark 3.1.1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val tablePath = &#34;&amp;lt;profile-file-path&amp;gt;#&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&#34;&#xA;val df = spark.readStream.format(&#34;deltaSharing&#34;)&#xA;  .option(&#34;startingVersion&#34;, &#34;1&#34;)&#xA;  .option(&#34;skipChangeCommits&#34;, &#34;true&#34;)&#xA;  .load(tablePath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Table paths&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A profile file path can be any URL supported by Hadoop FileSystem (such as &lt;code&gt;s3a://my_bucket/my/profile/file&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;A table path is the profile file path following with &lt;code&gt;#&lt;/code&gt; and the fully qualified name of a table (&lt;code&gt;&amp;lt;share-name&amp;gt;.&amp;lt;schema-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;The Community&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/87341375/212409874-a4ef350f-3b32-4031-b2cd-8c4e47cc42e2.jpeg&#34; alt=&#34;Delta Sharing OSS Connectors&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Supported Features&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Power BI&lt;/td&gt; &#xA;   &lt;td&gt;Databricks owned&lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Node.js&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/goodwillpunning/nodejs-sharing-client&#34;&gt;goodwillpunning/nodejs-sharing-client&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/databrickslabs/delta-sharing-java-connector&#34;&gt;databrickslabs/delta-sharing-java-connector&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arcuate&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/databrickslabs/arcuate&#34;&gt;databrickslabs/arcuate&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rust&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/r3stl355/delta-sharing-rust-client&#34;&gt;r3stl355/delta-sharing-rust-client&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/magpierre/delta-sharing/tree/golangdev/golang/delta_sharing_go&#34;&gt;magpierre/delta-sharing&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C++&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/magpierre/delta-sharing/tree/cppdev/cpp/DeltaSharingClient&#34;&gt;magpierre/delta-sharing&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;R&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/zacdav-db/delta-sharing-r&#34;&gt;zacdav-db/delta-sharing-r&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Released&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Spreadsheet&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/delta-incubator/delta-sharing-connectors/tree/main/google_workspace_add_on&#34;&gt;delta-incubator/delta-sharing-connectors&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Beta&lt;/td&gt; &#xA;   &lt;td&gt;QueryTableVersion&lt;br&gt;QueryTableMetadata&lt;br&gt;QueryTableLatestSnapshot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Airflow&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/apache/airflow/pull/22692&#34;&gt;apache/airflow&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Un-released&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Excel-Connector&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://www.exponam.com/solutions/&#34;&gt;https://www.exponam.com/solutions/&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;limited-release&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lakehouse Sharing&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/rajagurunath/lakehouse-sharing&#34;&gt;rajagurunath/lakehouse-sharing&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Preview&lt;/td&gt; &#xA;   &lt;td&gt; &lt;p&gt;&lt;a href=&#34;https://guruengineering.substack.com/p/lakehouse-sharing&#34;&gt;Demonstrates&lt;/a&gt; a table format agnostic data sharing&lt;br&gt; server (based on delta-sharing protocol) implemented in python for both Delta Lake and Iceberg formats.&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Delta Sharing Reference Server&lt;/h1&gt; &#xA;&lt;p&gt;The Delta Sharing Reference Server is a reference implementation server for the &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;Delta Sharing Protocol&lt;/a&gt;. This can be used to set up a small service to test your own connector that implements the &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;Delta Sharing Protocol&lt;/a&gt;. Please note that this is not a completed implementation of secure web server. We highly recommend you to put this behind a secure proxy if you would like to expose it to public.&lt;/p&gt; &#xA;&lt;p&gt;Some vendors offer managed services for Delta Sharing too (for example, &lt;a href=&#34;https://databricks.com/product/delta-sharing&#34;&gt;Databricks&lt;/a&gt;). Please refer to your vendor&#39;s website for how to set up sharing there. Vendors that are interested in being listed as a service provider should open an issue on GitHub to be added to this README and our project&#39;s website.&lt;/p&gt; &#xA;&lt;p&gt;Here are the steps to setup the reference server to share your own data.&lt;/p&gt; &#xA;&lt;h2&gt;Get the pre-built package&lt;/h2&gt; &#xA;&lt;p&gt;Download the pre-built package &lt;code&gt;delta-sharing-server-x.y.z.zip&lt;/code&gt; from &lt;a href=&#34;https://github.com/delta-io/delta-sharing/releases&#34;&gt;GitHub Releases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Server configuration and adding Shared Data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Unpack the pre-built package and copy the server config template file &lt;code&gt;conf/delta-sharing-server.yaml.template&lt;/code&gt; to create your own server yaml file, such as &lt;code&gt;conf/delta-sharing-server.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Make changes to your yaml file. You may also need to update some server configs for special requirements.&lt;/li&gt; &#xA; &lt;li&gt;To add Shared Data, add reference to Delta Lake tables you would like to share from this server in this config file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Config the server to access tables on cloud storage&lt;/h2&gt; &#xA;&lt;p&gt;We support sharing Delta Lake tables on S3, Azure Blob Storage and Azure Data Lake Storage Gen2.&lt;/p&gt; &#xA;&lt;h3&gt;S3&lt;/h3&gt; &#xA;&lt;p&gt;The server is using &lt;code&gt;hadoop-aws&lt;/code&gt; to access S3. Table paths in the server config file should use &lt;code&gt;s3a://&lt;/code&gt; paths rather than &lt;code&gt;s3://&lt;/code&gt; paths. There are multiple ways to config S3 authentication.&lt;/p&gt; &#xA;&lt;h4&gt;EC2 IAM Metadata Authentication (Recommended)&lt;/h4&gt; &#xA;&lt;p&gt;Applications running in EC2 may associate an IAM role with the VM and query the &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html&#34;&gt;EC2 Instance Metadata Service&lt;/a&gt; for credentials to access S3.&lt;/p&gt; &#xA;&lt;h4&gt;Authenticating via the AWS Environment Variables&lt;/h4&gt; &#xA;&lt;p&gt;We support configuration via &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-environment&#34;&gt;the standard AWS environment variables&lt;/a&gt;. The core environment variables are for the access key and associated secret:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export AWS_ACCESS_KEY_ID=my.aws.key&#xA;export AWS_SECRET_ACCESS_KEY=my.secret.key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Other S3 authentication methods&lt;/h4&gt; &#xA;&lt;p&gt;You can find other approaches in &lt;a href=&#34;https://hadoop.apache.org/docs/r2.10.1/hadoop-aws/tools/hadoop-aws/index.html#S3A_Authentication_methods&#34;&gt;hadoop-aws doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Azure Blob Storage&lt;/h3&gt; &#xA;&lt;p&gt;The server is using &lt;code&gt;hadoop-azure&lt;/code&gt; to read Azure Blob Storage. Using Azure Blob Storage requires &lt;a href=&#34;https://hadoop.apache.org/docs/current/hadoop-azure/index.html#Configuring_Credentials&#34;&gt;configuration of credentials&lt;/a&gt;. You can create a Hadoop configuration file named &lt;code&gt;core-site.xml&lt;/code&gt; and add it to the server&#39;s &lt;code&gt;conf&lt;/code&gt; directory. Then add the following content to the xml file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&#34;1.0&#34;?&amp;gt;&#xA;&amp;lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&amp;gt;&#xA;&amp;lt;configuration&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.azure.account.key.YOUR-ACCOUNT-NAME.blob.core.windows.net&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;YOUR-ACCOUNT-KEY&amp;lt;/value&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;&amp;lt;/configuration&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;YOUR-ACCOUNT-NAME&lt;/code&gt; is your Azure storage account and &lt;code&gt;YOUR-ACCOUNT-KEY&lt;/code&gt; is your account key.&lt;/p&gt; &#xA;&lt;h3&gt;Azure Data Lake Storage Gen2&lt;/h3&gt; &#xA;&lt;p&gt;The server is using &lt;code&gt;hadoop-azure&lt;/code&gt; to read Azure Data Lake Storage Gen2. We support &lt;a href=&#34;https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html#Default:_Shared_Key&#34;&gt;the Shared Key authentication&lt;/a&gt;. You can create a Hadoop configuration file named &lt;code&gt;core-site.xml&lt;/code&gt; and add it to the server&#39;s &lt;code&gt;conf&lt;/code&gt; directory. Then add the following content to the xml file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&#34;1.0&#34;?&amp;gt;&#xA;&amp;lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&amp;gt;&#xA;&amp;lt;configuration&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.azure.account.auth.type.YOUR-ACCOUNT-NAME.dfs.core.windows.net&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;SharedKey&amp;lt;/value&amp;gt;&#xA;    &amp;lt;description&amp;gt;&#xA;    &amp;lt;/description&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.azure.account.key.YOUR-ACCOUNT-NAME.dfs.core.windows.net&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;YOUR-ACCOUNT-KEY&amp;lt;/value&amp;gt;&#xA;    &amp;lt;description&amp;gt;&#xA;    The secret password. Never share these.&#xA;    &amp;lt;/description&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;&amp;lt;/configuration&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;YOUR-ACCOUNT-NAME&lt;/code&gt; is your Azure storage account and &lt;code&gt;YOUR-ACCOUNT-KEY&lt;/code&gt; is your account key.&lt;/p&gt; &#xA;&lt;h3&gt;Google Cloud Storage&lt;/h3&gt; &#xA;&lt;p&gt;We support using &lt;a href=&#34;https://cloud.google.com/iam/docs/service-accounts&#34;&gt;Service Account&lt;/a&gt; to read Google Cloud Storage. You can find more details in &lt;a href=&#34;https://cloud.google.com/docs/authentication/getting-started&#34;&gt;GCP Authentication Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To set up the Service Account credentials, you can specify the environment GOOGLE_APPLICATION_CREDENTIALS before starting the Delta Sharing Server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export GOOGLE_APPLICATION_CREDENTIALS=&#34;KEY_PATH&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;KEY_PATH&lt;/code&gt; with path of the JSON file that contains your service account key.&lt;/p&gt; &#xA;&lt;h3&gt;Cloudflare R2&lt;/h3&gt; &#xA;&lt;p&gt;We use an R2 implementation of the &lt;a href=&#34;https://developers.cloudflare.com/r2/api/s3/api/&#34;&gt;S3 API&lt;/a&gt; and &lt;code&gt;hadoop-aws&lt;/code&gt; to read Cloudflare R2. Table paths in the server config file should use the &lt;code&gt;s3a://&lt;/code&gt; scheme. You must &lt;a href=&#34;https://developers.cloudflare.com/r2/api/s3/tokens/&#34;&gt;generate an API token&lt;/a&gt; for usage with existing S3-compatible SDKs. These credentials can be specified in substitute of the S3 credentials in a Hadoop configuration file named &lt;code&gt;core-site.xml&lt;/code&gt; within the server&#39;s &lt;code&gt;conf&lt;/code&gt; directory. For R2 to work, you also need to directly specify the S3 endpoint and reduce &lt;code&gt;fs.s3a.paging.maximum&lt;/code&gt; from Hadoop&#39;s default of 5000 to 1000 since R2 only supports &lt;code&gt;MaxKeys&lt;/code&gt; &amp;lt;= 1000.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&#34;1.0&#34;?&amp;gt;&#xA;&amp;lt;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&amp;gt;&#xA;&amp;lt;configuration&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.s3a.access.key&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;YOUR-ACCESS-KEY&amp;lt;/value&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.s3a.secret.key&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;YOUR-SECRET-KEY&amp;lt;/value&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.s3a.endpoint&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;https://YOUR-ACCOUNT-ID.r2.cloudflarestorage.com&amp;lt;/value&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;  &amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.s3a.paging.maximum&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;1000&amp;lt;/value&amp;gt;&#xA;  &amp;lt;/property&amp;gt;&#xA;&amp;lt;/configuration&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;YOUR-ACCESS-KEY&lt;/code&gt; with your generated API token&#39;s R2 access key ID, &lt;code&gt;YOUR-SECRET-KEY&lt;/code&gt; with your generated API token&#39;s secret access key, and &lt;code&gt;YOUR-ACCOUNT-ID&lt;/code&gt; with your Cloudflare account ID.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: S3 and R2 credentials cannot be configured simultaneously.&lt;/p&gt; &#xA;&lt;h2&gt;Authorization&lt;/h2&gt; &#xA;&lt;p&gt;The server supports a basic authorization with pre-configed bearer token. You can add the following config to your server yaml file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;authorization:&#xA;  bearerToken: &amp;lt;token&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then any request should send with the above token, otherwise, the server will refuse the request.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t config the bearer token in the server yaml file, all requests will be accepted without authorization.&lt;/p&gt; &#xA;&lt;p&gt;To be more secure, you recommend you to put the server behind a secure proxy such as &lt;a href=&#34;https://www.nginx.com/&#34;&gt;NGINX&lt;/a&gt; to set up &lt;a href=&#34;https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-jwt-authentication/&#34;&gt;JWT Authentication&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Start the server&lt;/h2&gt; &#xA;&lt;p&gt;Run the following shell command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bin/delta-sharing-server -- --config &amp;lt;the-server-config-yaml-file&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;&amp;lt;the-server-config-yaml-file&amp;gt;&lt;/code&gt; should be the path of the yaml file you created in the previous step. You can find options to config JVM in &lt;a href=&#34;https://www.scala-sbt.org/sbt-native-packager/archetypes/java_app/index.html#start-script-options&#34;&gt;sbt-native-packager&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Use the pre-built Docker image&lt;/h2&gt; &#xA;&lt;p&gt;You can use the pre-built docker image from &lt;a href=&#34;https://hub.docker.com/r/deltaio/delta-sharing-server&#34;&gt;https://hub.docker.com/r/deltaio/delta-sharing-server&lt;/a&gt; by running the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p &amp;lt;host-port&amp;gt;:&amp;lt;container-port&amp;gt; \&#xA;  --mount type=bind,source=&amp;lt;the-server-config-yaml-file&amp;gt;,target=/config/delta-sharing-server-config.yaml \&#xA;  deltaio/delta-sharing-server:0.6.4 -- --config /config/delta-sharing-server-config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;&amp;lt;container-port&amp;gt;&lt;/code&gt; should be the same as the port defined inside the config file.&lt;/p&gt; &#xA;&lt;h2&gt;API Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;The REST APIs provided by Delta Sharing Server are stable public APIs. They are defined by &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;Delta Sharing Protocol&lt;/a&gt; and we will follow the entire protocol strictly.&lt;/p&gt; &#xA;&lt;p&gt;The interfaces inside Delta Sharing Server are not public APIs. They are considered internal, and they are subject to change across minor/patch releases.&lt;/p&gt; &#xA;&lt;h1&gt;Delta Sharing Protocol&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/PROTOCOL.md&#34;&gt;Delta Sharing Protocol specification&lt;/a&gt; details the protocol.&lt;/p&gt; &#xA;&lt;h1&gt;Building this Project&lt;/h1&gt; &#xA;&lt;h2&gt;Python Connector&lt;/h2&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python/dev/pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install in develop mode, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd python/&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install locally, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd python/&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate a wheel file, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd python/&#xA;python setup.py sdist bdist_wheel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will generate &lt;code&gt;python/dist/delta_sharing-x.y.z-py3-none-any.whl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Apache Spark Connector and Delta Sharing Server&lt;/h2&gt; &#xA;&lt;p&gt;Apache Spark Connector and Delta Sharing Server are compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate the Apache Spark Connector, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt spark/package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will generate &lt;code&gt;spark/target/scala-2.12/delta-sharing-spark_2.12-x.y.z.jar&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To generate the pre-built Delta Sharing Server package, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt server/universal:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will generate &lt;code&gt;server/target/universal/delta-sharing-server-x.y.z.zip&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To build the Docker image for Delta Sharing Server, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt server/docker:publishLocal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will build a Docker image tagged &lt;code&gt;delta-sharing-server:x.y.z&lt;/code&gt;, which you can run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p &amp;lt;host-port&amp;gt;:&amp;lt;container-port&amp;gt; \&#xA;  --mount type=bind,source=&amp;lt;the-server-config-yaml-file&amp;gt;,target=/config/delta-sharing-server-config.yaml \&#xA;  delta-sharing-server:x.y.z -- --config /config/delta-sharing-server-config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;&amp;lt;container-port&amp;gt;&lt;/code&gt; should be the same as the port defined inside the config file.&lt;/p&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h1&gt;Reporting Issues&lt;/h1&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta-sharing/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Sharing. See our &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta-sharing/main/LICENSE.txt&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;We use the same community resources as the Delta Lake project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Public Slack Channel&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://go.delta.io/slack&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ucb-bar/chiseltest</title>
    <updated>2024-02-01T02:09:23Z</updated>
    <id>tag:github.com,2024-02-01:/ucb-bar/chiseltest</id>
    <link href="https://github.com/ucb-bar/chiseltest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The batteries-included testing and formal verification library for Chisel-based RTL designs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chiseltest&lt;/h1&gt; &#xA;&lt;p&gt;Chiseltest is the &lt;em&gt;batteries-included&lt;/em&gt; testing and formal verification library for &lt;a href=&#34;https://github.com/chipsalliance/chisel3&#34;&gt;Chisel&lt;/a&gt;-based RTL designs. Chiseltest emphasizes tests that are lightweight (minimizes boilerplate code), easy to read and write (understandability), and compose (for better test code reuse).&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use chisel-testers as a managed dependency, add this in your build.sbt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chiseltest&#34; % &#34;5.0-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Starting with &lt;code&gt;chisel5&lt;/code&gt;, please make sure to pick a matching major version, to avoid linking errors. For older versions, if you are also directly depending on the &lt;code&gt;chisel3&lt;/code&gt; library, please &lt;a href=&#34;https://www.chisel-lang.org/chisel3/docs/appendix/versioning.html&#34;&gt;make sure that your chisel3 and chiseltest versions match&lt;/a&gt; to avoid linking errors.&lt;/p&gt; &#xA;&lt;h2&gt;Writing a Test&lt;/h2&gt; &#xA;&lt;p&gt;ChiselTest integrates with the &lt;a href=&#34;http://scalatest.org&#34;&gt;ScalaTest&lt;/a&gt; framework, which provides good IDE and continuous integration support for launching unit tests.&lt;/p&gt; &#xA;&lt;p&gt;Assuming a typical Chisel project with &lt;code&gt;MyModule&lt;/code&gt; defined in &lt;code&gt;src/main/scala/MyModule.scala&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class MyModule extend Module {&#xA;    val io = IO(new Bundle {&#xA;        val in = Input(UInt(16.W))&#xA;        val out = Output(UInt(16.W))&#xA;    })&#xA;&#xA;    io.out := RegNext(io.in)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a new file in &lt;code&gt;src/test/scala/&lt;/code&gt;, for example, &lt;code&gt;BasicTest.scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this file:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add the necessary imports: &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import chisel3._&#xA;import chiseltest._&#xA;import org.scalatest.flatspec.AnyFlatSpec&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a test class: &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class BasicTest extends AnyFlatSpec with ChiselScalatestTester {&#xA;  behavior of &#34;MyModule&#34;&#xA;  // test class body here&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;AnyFlatSpec&lt;/code&gt; is the &lt;a href=&#34;http://www.scalatest.org/user_guide/selecting_a_style&#34;&gt;default and recommended ScalaTest style for unit testing&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ChiselScalatestTester&lt;/code&gt; provides testdriver functionality and integration (like signal value assertions) within the context of a ScalaTest environment.&lt;/li&gt; &#xA;   &lt;li&gt;For those interested in additional ScalaTest assertion expressibility, &lt;code&gt;Matchers&lt;/code&gt; provides additional &lt;a href=&#34;http://www.scalatest.org/user_guide/using_matchers&#34;&gt;assertion syntax options&lt;/a&gt;. &lt;code&gt;Matchers&lt;/code&gt; is optional as it&#39;s mainly for Scala-land assertions and does not inter-operate with circuit operations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;In the test class, define a test case: &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;it should &#34;do something&#34; in {&#xA;  // test case body here&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; There can be multiple test cases per test class, and we recommend one test class per Module being tested, and one test case per individual test.&lt;/li&gt; &#xA; &lt;li&gt;In the test case, define the module being tested: &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;test(new MyModule) { c =&amp;gt;&#xA;  // test body here&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;code&gt;test&lt;/code&gt; automatically runs the default simulator (which is &lt;a href=&#34;https://github.com/freechipsproject/treadle&#34;&gt;treadle&lt;/a&gt;), and runs the test stimulus in the block. The argument to the test stimulus block (&lt;code&gt;c&lt;/code&gt; in this case) is a handle to the module under test.&lt;/li&gt; &#xA; &lt;li&gt;In the test body, use &lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;step&lt;/code&gt;, and &lt;code&gt;expect&lt;/code&gt; operations to write the test: &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;c.io.in.poke(0.U)&#xA;c.clock.step()&#xA;c.io.out.expect(0.U)&#xA;c.io.in.poke(42.U)&#xA;c.clock.step()&#xA;c.io.out.expect(42.U)&#xA;println(&#34;Last output value :&#34; + c.io.out.peek().litValue)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;With your test case complete, you can run all the test cases in your project by invoking ScalaTest. If you&#39;re using &lt;a href=&#34;http://scala-sbt.org&#34;&gt;sbt&lt;/a&gt;, you can either run &lt;code&gt;sbt test&lt;/code&gt; from the command line, or &lt;code&gt;test&lt;/code&gt; from the sbt console. &lt;code&gt;testOnly&lt;/code&gt; can also be used to run specific tests.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Usage References&lt;/h3&gt; &#xA;&lt;p&gt;See the test cases for examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/BasicTest.scala&#34;&gt;BasicTest&lt;/a&gt; shows basic &lt;code&gt;peek&lt;/code&gt;, &lt;code&gt;poke&lt;/code&gt;, and &lt;code&gt;step&lt;/code&gt; functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/QueueTest.scala&#34;&gt;QueueTest&lt;/a&gt; shows example uses of the DecoupledDriver library, providing functions like &lt;code&gt;enqueueNow&lt;/code&gt;, &lt;code&gt;expectDequeueNow&lt;/code&gt;, their sequence variants, &lt;code&gt;expectPeek&lt;/code&gt;, and &lt;code&gt;expectInvalid&lt;/code&gt;. Also, check out the &lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/main/scala/chiseltest/DecoupledDriver.scala&#34;&gt;DecoupledDriver&lt;/a&gt; implementation, and note that it is not a special case, but code that any user can write.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/BundleLiteralsSpec.scala&#34;&gt;BundleLiteralsSpec&lt;/a&gt; shows examples of using bundle literals to poke and expect bundle wires. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Note: Bundle literals are still an experimental chisel3 feature and need to be explicitly imported: &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import chisel3.experimental.BundleLiterals._&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/AluTest.scala&#34;&gt;AlutTest&lt;/a&gt; shows an example of re-using the same test for different data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/ShiftRegisterTest.scala&#34;&gt;ShiftRegisterTest&lt;/a&gt; shows an example of using fork/join to define a test helper function, where multiple invocations of it are pipelined using &lt;code&gt;fork&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;New Constructs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;fork&lt;/code&gt; to spawn threads, and &lt;code&gt;join&lt;/code&gt; to block (wait) on a thread. Pokes and peeks/expects to wires from threads are checked during runtime to ensure no collisions or unexpected behavior. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;fork&lt;/code&gt;ed threads provide a concurrency abstraction for writing testbenches only, without real parallelism. The test infrastructure schedules threads one at a time, with threads running once per simulation cycle.&lt;/li&gt; &#xA;   &lt;li&gt;Thread order is deterministic, and attempts to follow lexical order (as it would appear from the code text): &lt;code&gt;fork&lt;/code&gt;ed (child) threads run immediately, then return to the spawning (parent) thread. On future cycles, child threads run before their parent, in the order they were spawned.&lt;/li&gt; &#xA;   &lt;li&gt;Only cross-thread operations that round-trip through the simulator (eg, peek-after-poke) are checked. You can do cross-thread operations in Scala (eg, using shared variables) that aren&#39;t checked, but it is up to you to make sure they are correct and intuitive. This is not recommended. In the future, we may provide checked mechanisms for communicating between test threads.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Regions can be associated with a thread, with &lt;code&gt;fork.withRegion(...)&lt;/code&gt;, which act as a synchronization barrier within simulator time steps. This can be used to create monitors that run after other main testdriver threads have been run, and can read wires those threads have poked.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Simulator Backends&lt;/h2&gt; &#xA;&lt;p&gt;One of our goals is to keep your tests independent of the underlying simulator as much as possible. Thus, in most cases you should be able to choose from one of our four supported backends and get the exact same test results albeit with differences in execution speed and wave dump quality.&lt;/p&gt; &#xA;&lt;p&gt;We provide full bindings to two popular open-source simulator:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chipsalliance/treadle&#34;&gt;treadle&lt;/a&gt;: default, fast startup times, slow execution for larger circuits, supports only VCD&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.veripool.org/wiki/verilator&#34;&gt;verilator&lt;/a&gt;: enable with &lt;code&gt;VerilatorBackendAnnotation&lt;/code&gt;, slow startup, fast execution, supports VCD and FST&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also provide bindings with some feature limitations to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://iverilog.icarus.com/&#34;&gt;iverilog&lt;/a&gt;: open-source, enable with &lt;code&gt;IcarusBackendAnnotation&lt;/code&gt;, supports VCD, FST and LXT&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.synopsys.com/verification/simulation/vcs.html&#34;&gt;vcs&lt;/a&gt;: commercial, enable with &lt;code&gt;VcsBackendAnnotation&lt;/code&gt;, supports VCD and FSDB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Verilator Versions&lt;/h3&gt; &#xA;&lt;p&gt;We currently support the following versions of the &lt;a href=&#34;https://www.veripool.org/wiki/verilator&#34;&gt;verilator&lt;/a&gt; simulator:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;v4.028&lt;/code&gt;: &lt;a href=&#34;https://packages.ubuntu.com/focal/verilator&#34;&gt;Ubuntu 20.04&lt;/a&gt;, &lt;a href=&#34;https://src.fedoraproject.org/rpms/verilator&#34;&gt;Fedora 32&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;v4.032&lt;/code&gt;: &lt;a href=&#34;https://src.fedoraproject.org/rpms/verilator&#34;&gt;Fedora 33&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;v4.034&lt;/code&gt;: &lt;a href=&#34;https://chipyard.readthedocs.io/en/latest/Chipyard-Basics/Initial-Repo-Setup.html#requirements&#34;&gt;Chipyard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;v4.038&lt;/code&gt;: &lt;a href=&#34;https://packages.ubuntu.com/groovy/verilator&#34;&gt;Ubuntu 20.10&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;v4.108&lt;/code&gt;: &lt;a href=&#34;https://src.fedoraproject.org/rpms/verilator&#34;&gt;Fedora 34&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;v4.202&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;h3&gt;How do I rerun with --full-stacktrace?&lt;/h3&gt; &#xA;&lt;p&gt;Whereas Chisel accepts command-line arguments, chiseltest exposes the underlying annotation interface. You can pass annotations to a test by using &lt;code&gt;.withAnnotations(...)&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Top of file&#xA;import chisel3.stage.PrintFullStackTraceAnnotation&#xA;&#xA;// ...&#xA;&#xA;    // Inside your test spec&#xA;    test(new MyModule).withChiselAnnotations(Seq(PrintFullStackTraceAnnotation)) { c =&amp;gt;&#xA;      // test body here&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will remove the chisel3 stacktrace suppression (ie. &lt;code&gt;at ... ()&lt;/code&gt;). However, if you are using ScalaTest, you may notice a shortened stack trace with &lt;code&gt;...&lt;/code&gt; at the end. You can tell ScalaTest to stop suppressing the stack trace by passing &lt;code&gt;-oF&lt;/code&gt; to it. For example (using SBT):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sbt&#xA;&amp;gt; testOnly &amp;lt;spec name&amp;gt; -- -oF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Any arguments after &lt;code&gt;--&lt;/code&gt; pass to ScalaTest directly instead of being interpreted by SBT.&lt;/p&gt; &#xA;&lt;h2&gt;Stability&lt;/h2&gt; &#xA;&lt;p&gt;Most APIs that can be accessed through &lt;code&gt;import chiseltest._&lt;/code&gt; are going to remain stable. We are also trying to keep the API provided through &lt;code&gt;import chiseltest.formal._&lt;/code&gt; relatively stable. All other packages are considered internal and thus might change at any time.&lt;/p&gt; &#xA;&lt;h2&gt;Migrating from chisel-testers / iotesters&lt;/h2&gt; &#xA;&lt;h3&gt;Port to new API&lt;/h3&gt; &#xA;&lt;p&gt;The core abstractions (&lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;expect&lt;/code&gt;, &lt;code&gt;step&lt;/code&gt;) are similar to &lt;a href=&#34;https://github.com/freechipsproject/chisel-testers&#34;&gt;chisel-testers&lt;/a&gt;, but the syntax is inverted: instead of doing &lt;code&gt;tester.poke(wire, value)&lt;/code&gt; with a Scala number value, in ChiselTest you would write &lt;code&gt;wire.poke(value)&lt;/code&gt; with a Chisel literal value. Furthermore, as no reference to the tester context is needed, test helper functions can be defined outside a test class and written as libraries.&lt;/p&gt; &#xA;&lt;h3&gt;PeekPokeTester compatibility&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;chiseltest&lt;/code&gt; now provides a compatibility layer that makes it possible to re-use old &lt;code&gt;PeekPokeTester&lt;/code&gt; based tests with little to no changes to the code. We ported the majority of &lt;a href=&#34;https://github.com/freechipsproject/chisel-testers/tree/master/src/test/scala&#34;&gt;tests from the chisel-testers repository&lt;/a&gt; to our &lt;a href=&#34;https://github.com/ucb-bar/chiseltest/tree/main/src/test/scala/chiseltest/iotesters&#34;&gt;new compatibility layer&lt;/a&gt;. While the test itself can mostly remain unchanged, the old &lt;code&gt;Driver&lt;/code&gt; is removed and instead tests are launched with the new &lt;code&gt;test&lt;/code&gt; syntax.&lt;/p&gt; &#xA;&lt;h3&gt;Hardware testers&lt;/h3&gt; &#xA;&lt;p&gt;Hardware testers are synthesizeable tests, most often extending the &lt;code&gt;BasicTester&lt;/code&gt; class provided by &lt;code&gt;chisel3&lt;/code&gt;. You can now directly &lt;a href=&#34;https://github.com/ucb-bar/chiseltest/raw/main/src/test/scala/chiseltest/tests/HardwareTestsTest.scala&#34;&gt;use these tests with &lt;code&gt;chiseltest&lt;/code&gt; through the &lt;code&gt;runUntilStop&lt;/code&gt; function&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chipsalliance/rocket-chip-blocks</title>
    <updated>2024-02-01T02:09:23Z</updated>
    <id>tag:github.com,2024-02-01:/chipsalliance/rocket-chip-blocks</id>
    <link href="https://github.com/chipsalliance/rocket-chip-blocks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RTL blocks compatible with the Rocket Chip Generator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RTL Blocks for the Rocket Chip Generator&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains RTL generators for a variety of IO peripheral blocks that are designed to be compatible with the Rocket Chip SoC Generator&lt;/p&gt; &#xA;&lt;p&gt;This repository is a fork of &lt;a href=&#34;https://github.com/sifive/sifive-blocks&#34;&gt;https://github.com/sifive/sifive-blocks&lt;/a&gt;, which it replaces.&lt;/p&gt;</summary>
  </entry>
</feed>