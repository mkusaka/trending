<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-01T02:49:30Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>streamxhub/streamx</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/streamxhub/streamx</id>
    <link href="https://github.com/streamxhub/streamx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make stream processing easier! Flink &amp; Spark development scaffold, The original intention of StreamX is to make the development of Flink easier. StreamX focuses on the management of development phases and tasks. Our ultimate goal is to build a one-stop big data solution integrating stream processing, batch processing, data warehouse and data laker.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt; &lt;a href=&#34;http://www.streamxhub.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img width=&#34;600&#34; src=&#34;https://user-images.githubusercontent.com/13284744/166133644-ed3cc4f5-aae5-45bc-bfbe-29c540612446.png&#34; alt=&#34;StreamX logo&#34;&gt; &lt;/a&gt; &lt;/h1&gt; &#xA; &lt;strong style=&#34;font-size: 1.5rem&#34;&gt;Make stream processing easier!!!&lt;/strong&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-4EB1BA.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://tokei.rs/b1/github/streamxhub/streamx&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/streamxhub/streamx.svg?sanitize=true&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/streamxhub/streamx&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/streamxhub/streamx&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/count/streamxhub/streamx&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.streamxhub.com&#34;&gt;Official Website&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamxhub/streamx/dev/#&#34;&gt;Change Log&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href=&#34;https://www.streamxhub.com/docs/intro&#34;&gt;Document&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/streamxhub/streamx/dev/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h1&gt;StreamX&lt;/h1&gt; &#xA;&lt;p&gt;Make stream processing easier&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A magical framework that make stream processing easier!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üöÄ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The original intention of &lt;code&gt;StreamX&lt;/code&gt; is to make stream processing easier. &lt;code&gt;StreamX&lt;/code&gt; focuses on the management of development phases and tasks. Our ultimate goal is to build a one-stop big data solution integrating stream processing, batch processing, data warehouse and data laker.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://assets.streamxhub.com/streamx-video.mp4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/166101616-50a44d38-3ffb-4296-8a77-92f76a4c21b5.png&#34; alt=&#34;StreamX video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üéâ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scaffolding&lt;/li&gt; &#xA; &lt;li&gt;Out-of-the-box connectors&lt;/li&gt; &#xA; &lt;li&gt;Support maven compilation&lt;/li&gt; &#xA; &lt;li&gt;Configuration&lt;/li&gt; &#xA; &lt;li&gt;Multi version flink support(1.12.x,1.13.x,1.14.x, 1.15.x)&lt;/li&gt; &#xA; &lt;li&gt;Scala 2.11 / 2.12 support&lt;/li&gt; &#xA; &lt;li&gt;restapi support.&lt;/li&gt; &#xA; &lt;li&gt;All Flink deployment mode support(&lt;code&gt;Remote&lt;/code&gt;/&lt;code&gt;K8s-Native-Application&lt;/code&gt;/&lt;code&gt;K8s-Native-Session&lt;/code&gt;/&lt;code&gt;YARN-Application&lt;/code&gt;/&lt;code&gt;YARN-Per-Job&lt;/code&gt;/&lt;code&gt;YARN-Session&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, &lt;code&gt;savepoint&lt;/code&gt;, resume from &lt;code&gt;savepoint&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Various companies and organizations use &lt;code&gt;StreamX&lt;/code&gt; for production and commercial products.&lt;/li&gt; &#xA; &lt;li&gt;Flame graph&lt;/li&gt; &#xA; &lt;li&gt;Notebook&lt;/li&gt; &#xA; &lt;li&gt;Project configuration and dependency version management&lt;/li&gt; &#xA; &lt;li&gt;Task backup and rollback&lt;/li&gt; &#xA; &lt;li&gt;Manage dependencies&lt;/li&gt; &#xA; &lt;li&gt;UDF&lt;/li&gt; &#xA; &lt;li&gt;Flink SQL Connector&lt;/li&gt; &#xA; &lt;li&gt;Flink SQL WebIDE&lt;/li&gt; &#xA; &lt;li&gt;Catalog„ÄÅHive&lt;/li&gt; &#xA; &lt;li&gt;Full support from task &lt;code&gt;development&lt;/code&gt; to &lt;code&gt;deployment&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746863-856ef1cd-fa0e-4010-b359-c16ca2ad2fb7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746864-d807d728-423f-41c3-b90d-45ce2c21936b.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üè≥‚Äçüåà Components&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Streamx&lt;/code&gt; consists of three parts,&lt;code&gt;streamx-core&lt;/code&gt;,&lt;code&gt;streamx-pump&lt;/code&gt; and &lt;code&gt;streamx-console&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746859-f6a4dedc-ec42-4ed5-933b-c27d559b9988.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1Ô∏è‚É£ streamx-core&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;streamx-core&lt;/code&gt; is a framework that focuses on coding, standardizes configuration, and develops in a way that is better than configuration by convention. Also it provides a development-time &lt;code&gt;RunTime Content&lt;/code&gt; and a series of &lt;code&gt;Connector&lt;/code&gt; out of the box. At the same time, it extends &lt;code&gt;DataStream&lt;/code&gt; some methods, and integrates &lt;code&gt;DataStream&lt;/code&gt; and &lt;code&gt;Flink sql&lt;/code&gt; api to simplify tedious operations, focus on the business itself, and improve development efficiency and development experience.&lt;/p&gt; &#xA;&lt;h3&gt;2Ô∏è‚É£ streamx-pump&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;streamx-pump&lt;/code&gt; is a planned data extraction component, similar to &lt;code&gt;flinkx&lt;/code&gt;. Based on the various &lt;code&gt;connector&lt;/code&gt; provided in &lt;code&gt;streamx-core&lt;/code&gt;, the purpose is to create a convenient, fast, out-of-the-box real-time data extraction and migration component for big data, and it will be integrated into the &lt;code&gt;streamx-console&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3Ô∏è‚É£ streamx-console&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;streamx-console&lt;/code&gt; is a stream processing and &lt;code&gt;Low Code&lt;/code&gt; platform, capable of managing &lt;code&gt;Flink&lt;/code&gt; tasks, integrating project compilation, deploy, configuration, startup, &lt;code&gt;savepoint&lt;/code&gt;, &lt;code&gt;flame graph&lt;/code&gt;, &lt;code&gt;Flink SQL&lt;/code&gt;, monitoring and many other features. Simplify the daily operation and maintenance of the &lt;code&gt;Flink&lt;/code&gt; task.&lt;/p&gt; &#xA;&lt;p&gt;Our ultimate goal is to build a one-stop big data solution integrating stream processing, batch processing, data warehouse and data laker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://flink.apache.org&#34;&gt;Apache Flink&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hadoop.apache.org&#34;&gt;Apache YARN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spring.io/projects/spring-boot/&#34;&gt;Spring Boot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mybatis.org&#34;&gt;Mybatis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mp.baomidou.com&#34;&gt;Mybatis-Plus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.brendangregg.com/FlameGraphs&#34;&gt;Flame Graph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/uber-common/jvm-profiler&#34;&gt;JVM-Profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cn.vuejs.org/&#34;&gt;Vue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vuepress.vuejs.org/&#34;&gt;VuePress&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://antdv.com/&#34;&gt;Ant Design of Vue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pro.antdv&#34;&gt;ANTD PRO VUE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xtermjs.org/&#34;&gt;xterm.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/monaco-editor/&#34;&gt;Monaco Editor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to the above excellent open source projects and many outstanding open source projects that are not mentioned, for giving the greatest respect,Thanks to &lt;a href=&#34;http://flink.apache.org&#34;&gt;Apache Flink&lt;/a&gt; for creating a great project! Thanks to the &lt;a href=&#34;http://zeppelin.apache.org&#34;&gt;Apache Zeppelin&lt;/a&gt; project for the early inspiration.&lt;/p&gt; &#xA;&lt;h3&gt;üöÄ Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;click &lt;a href=&#34;http://www.streamxhub.com/zh-CN/docs/intro/&#34;&gt;Document&lt;/a&gt; for more information&lt;/p&gt; &#xA;&lt;h2&gt;üíã our users&lt;/h2&gt; &#xA;&lt;p&gt;Various companies and organizations use StreamX for research, production and commercial products. Are you using this project ? &lt;a href=&#34;https://github.com/streamxhub/streamx/issues/163&#34;&gt;you can add your company&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/160220085-11f1e011-e7a0-421f-9294-c14213c0bc22.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèÜ Our honor&lt;/h2&gt; &#xA;&lt;p&gt;We have received some precious honors, which belong to everyone who contributes to StreamX, Thank you !&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746797-85ebf7b4-4105-4b5b-a023-0689c7fd1d2d.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/163530071-a5b6f334-9af5-439c-96c9-2bb9b4eec6a6.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contribution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/streamxhub/streamx/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can submit any ideas as &lt;a href=&#34;https://github.com/streamxhub/streamx/pulls&#34;&gt;pull requests&lt;/a&gt; or as &lt;a href=&#34;https://github.com/streamxhub/streamx/issues/new/choose&#34;&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re new to posting issues, we ask that you read &lt;a href=&#34;http://www.catb.org/~esr/faqs/smart-questions.html&#34;&gt;&lt;em&gt;How To Ask Questions The Smart Way&lt;/em&gt;&lt;/a&gt; (&lt;strong&gt;This guide does not provide actual support services for this project!&lt;/strong&gt;), &lt;a href=&#34;http://www.chiark.greenend.org.uk/~sgtatham/bugs.html&#34;&gt;How to Report Bugs Effectively&lt;/a&gt; prior to posting. Well written bug reports help us help you!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Thank you to all the people who already contributed to StreamX!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/streamxhub/streamx/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/streamx/contributors.svg?width=890&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚è∞ Contributor Over Time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://git-contributor.com?chart=contributorOverTime&amp;amp;repo=streamxhub/streamx&#34;&gt;&lt;img src=&#34;https://contributor-overtime-api.git-contributor.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=streamxhub/streamx&#34; alt=&#34;Contributor Over Time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí∞ Donation&lt;/h2&gt; &#xA;&lt;p&gt;Are you &lt;strong&gt;enjoying this project&lt;/strong&gt; ? üëã&lt;/p&gt; &#xA;&lt;p&gt;If you like this framework, and appreciate the work done for it to exist, you can still support the developers by donating ‚òÄÔ∏è üëä&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;WeChat Pay&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Alipay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746857-35e7f823-7160-4505-be3f-e748a2d0a233.png&#34; alt=&#34;Buy Me A Coffee&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746860-e14a8183-d973-44ca-83bf-e5f9d4da1510.png&#34; alt=&#34;Buy Me A Coffee&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üèÜ Our sponsors (Coffee Suppliers)&lt;/h2&gt; &#xA;&lt;h3&gt;üíú Monthly Supplier&lt;/h3&gt; &#xA;&lt;p&gt;Welcome individuals and enterprises to sponsor, your support will help us better develop the project&lt;/p&gt; &#xA;&lt;h3&gt;ü•á Gold Supplier&lt;/h3&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/wolfboys&#34; alt=&#34;benjobs&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/13284744?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Kitming25&#34; alt=&#34;Kitming25&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/11773106?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Narcasserun&#34; alt=&#34;Narcasserun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/39329477?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;ü•à Platinum Supplier&lt;/h3&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/lianxiaobao&#34; alt=&#34;lianxiaobao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/36557317?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/su94998&#34; alt=&#34;su94998&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/33316193?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;ü•à Silver Supplier&lt;/h3&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/CrazyJugger&#34; alt=&#34;leohantaoluo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/30514978?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhaizhirui&#34; alt=&#34;zhaizhirui&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/39609947?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.jetbrains.com/?from=streamx&#34;&gt;JetBrains&lt;/a&gt; for supporting us free open source licenses.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jetbrains.com/?from=streamx&#34;&gt;&lt;img src=&#34;https://img.alicdn.com/tfs/TB1sSomo.z1gK0jSZLeXXb9kVXa-120-130.svg?sanitize=true&#34; alt=&#34;JetBrains&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üèÖ Backers&lt;/h3&gt; &#xA;&lt;p&gt;Thank you to all our backers!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üí¨ Join us&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamxhub/streamx/dev/(http://www.streamxhub.com/#/)&#34;&gt;StreamX&lt;/a&gt; enters the high-speed development stage, we need your contribution.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://starchart.cc/streamxhub/streamx.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/13284744/152627523-de455a4d-97c7-46cd-815f-3328a3fe3663.png&#34; alt=&#34;Join the Group&#34; height=&#34;300px&#34;&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>JohnSnowLabs/spark-nlp</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/JohnSnowLabs/spark-nlp</id>
    <link href="https://github.com/JohnSnowLabs/spark-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State of the Art Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark NLP: State of the Art Natural Language Processing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/actions&#34; alt=&#34;build&#34;&gt; &lt;img src=&#34;https://github.com/JohnSnowLabs/spark-nlp/workflows/build/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34; alt=&#34;Current Release Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/JohnSnowLabs/spark-nlp.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/artifact/com.johnsnowlabs.nlp/spark-nlp_2.12&#34; alt=&#34;Maven Central&#34;&gt; &lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.johnsnowlabs.nlp/spark-nlp_2.12/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/spark-nlp&#34; alt=&#34;PyPI version&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/spark-nlp.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34; alt=&#34;Anaconda-Cloud&#34;&gt; &lt;img src=&#34;https://anaconda.org/johnsnowlabs/spark-nlp/badges/version.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/LICENSE&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34; alt=&#34;PyPi downloads&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/spark-nlp?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=pip%20downloads&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark. It provides &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;performant&lt;/strong&gt; &amp;amp; &lt;strong&gt;accurate&lt;/strong&gt; NLP annotations for machine learning pipelines that &lt;strong&gt;scale&lt;/strong&gt; easily in a distributed environment. Spark NLP comes with &lt;strong&gt;4000+&lt;/strong&gt; pretrained &lt;strong&gt;pipelines&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt; in more than &lt;strong&gt;200+&lt;/strong&gt; languages. It also offers tasks such as &lt;strong&gt;Tokenization&lt;/strong&gt;, &lt;strong&gt;Word Segmentation&lt;/strong&gt;, &lt;strong&gt;Part-of-Speech Tagging&lt;/strong&gt;, Word and Sentence &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Named Entity Recognition&lt;/strong&gt;, &lt;strong&gt;Dependency Parsing&lt;/strong&gt;, &lt;strong&gt;Spell Checking&lt;/strong&gt;, &lt;strong&gt;Text Classification&lt;/strong&gt;, &lt;strong&gt;Sentiment Analysis&lt;/strong&gt;, &lt;strong&gt;Token Classification&lt;/strong&gt;, &lt;strong&gt;Machine Translation&lt;/strong&gt; (+180 languages), &lt;strong&gt;Summarization&lt;/strong&gt; &amp;amp; &lt;strong&gt;Question Answering&lt;/strong&gt;, &lt;strong&gt;Text Generation&lt;/strong&gt;, and many more &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;NLP tasks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark NLP&lt;/strong&gt; is the only open-source NLP library in &lt;strong&gt;production&lt;/strong&gt; that offers state-of-the-art transformers such as &lt;strong&gt;BERT&lt;/strong&gt;, &lt;strong&gt;ALBERT&lt;/strong&gt;, &lt;strong&gt;ELECTRA&lt;/strong&gt;, &lt;strong&gt;XLNet&lt;/strong&gt;, &lt;strong&gt;DistilBERT&lt;/strong&gt;, &lt;strong&gt;RoBERTa&lt;/strong&gt;, &lt;strong&gt;DeBERTa&lt;/strong&gt;, &lt;strong&gt;XLM-RoBERTa&lt;/strong&gt;, &lt;strong&gt;Longformer&lt;/strong&gt;, &lt;strong&gt;ELMO&lt;/strong&gt;, &lt;strong&gt;Universal Sentence Encoder&lt;/strong&gt;, &lt;strong&gt;Google T5&lt;/strong&gt;, &lt;strong&gt;MarianMT&lt;/strong&gt;, and &lt;strong&gt;GPT2&lt;/strong&gt; not only to &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt;, but also to &lt;strong&gt;JVM&lt;/strong&gt; ecosystem (&lt;strong&gt;Java&lt;/strong&gt;, &lt;strong&gt;Scala&lt;/strong&gt;, and &lt;strong&gt;Kotlin&lt;/strong&gt;) at &lt;strong&gt;scale&lt;/strong&gt; by extending &lt;strong&gt;Apache Spark&lt;/strong&gt; natively.&lt;/p&gt; &#xA;&lt;h2&gt;Project&#39;s website&lt;/h2&gt; &#xA;&lt;p&gt;Take a look at our official Spark NLP page: &lt;a href=&#34;http://nlp.johnsnowlabs.com/&#34;&gt;http://nlp.johnsnowlabs.com/&lt;/a&gt; for user documentation and examples&lt;/p&gt; &#xA;&lt;h2&gt;Community support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.johnsnowlabs.com/slack-redirect/&#34;&gt;Slack&lt;/a&gt; For live discussion with the Spark NLP community and the team&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp&#34;&gt;GitHub&lt;/a&gt; Bug reports, feature requests, and contributions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/discussions&#34;&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/spark-nlp&#34;&gt;Medium&lt;/a&gt; Spark NLP articles&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos&#34;&gt;YouTube&lt;/a&gt; Spark NLP video tutorials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-spark-support&#34;&gt;Apache Spark Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala-and-python-support&#34;&gt;Scala &amp;amp; Python Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-support&#34;&gt;Databricks Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-support&#34;&gt;EMR Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#usage&#34;&gt;Using Spark NLP&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-packages&#34;&gt;Spark Packages&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala&#34;&gt;Scala&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#maven&#34;&gt;Maven&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python&#34;&gt;Python&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipconda&#34;&gt;Pip/Conda&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;Compiled JARs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-zeppelin&#34;&gt;Apache Zeppelin&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#jupyter-notebook-python&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#google-colab-notebook&#34;&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#kaggle-kernel&#34;&gt;Kaggle Kernel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-cluster&#34;&gt;Databricks Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-cluster&#34;&gt;EMR Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#gcp-dataproc&#34;&gt;GCP Dataproc&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-nlp-configuration&#34;&gt;Spark NLP Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines-and-models&#34;&gt;Pipelines &amp;amp; Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines&#34;&gt;Pipelines&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#offline&#34;&gt;Offline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tokenization&lt;/li&gt; &#xA; &lt;li&gt;Trainable Word Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Stop Words Removal&lt;/li&gt; &#xA; &lt;li&gt;Token Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Document Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Stemmer&lt;/li&gt; &#xA; &lt;li&gt;Lemmatizer&lt;/li&gt; &#xA; &lt;li&gt;NGrams&lt;/li&gt; &#xA; &lt;li&gt;Regex Matching&lt;/li&gt; &#xA; &lt;li&gt;Text Matching&lt;/li&gt; &#xA; &lt;li&gt;Chunking&lt;/li&gt; &#xA; &lt;li&gt;Date Matcher&lt;/li&gt; &#xA; &lt;li&gt;Sentence Detector&lt;/li&gt; &#xA; &lt;li&gt;Deep Sentence Detector (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Dependency parsing (Labeled/unlabeled)&lt;/li&gt; &#xA; &lt;li&gt;Part-of-speech tagging&lt;/li&gt; &#xA; &lt;li&gt;Sentiment Detection (ML models)&lt;/li&gt; &#xA; &lt;li&gt;Spell Checker (ML and DL models)&lt;/li&gt; &#xA; &lt;li&gt;Word Embeddings (GloVe and Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;Doc2Vec (based on Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;BERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;CamemBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa Embeddings (HuggingFace v2 &amp;amp; v3 models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Longformer Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;ALBERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLNet Embeddings&lt;/li&gt; &#xA; &lt;li&gt;ELMO Embeddings (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;Universal Sentence Encoder (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;BERT Sentence Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Sentence Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Chunk Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised keywords extraction&lt;/li&gt; &#xA; &lt;li&gt;Language Detection &amp;amp; Identification (up to 375 languages)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-label Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Text Classification (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;BERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;ALBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLNet for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Longformer for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation (MarianMT)&lt;/li&gt; &#xA; &lt;li&gt;Text-To-Text Transfer Transformer (Google T5)&lt;/li&gt; &#xA; &lt;li&gt;Generative Pre-trained Transformer 2 (OpenAI GPT2)&lt;/li&gt; &#xA; &lt;li&gt;Named entity recognition (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Easy TensorFlow integration&lt;/li&gt; &#xA; &lt;li&gt;GPU Support&lt;/li&gt; &#xA; &lt;li&gt;Full integration with Spark ML functions&lt;/li&gt; &#xA; &lt;li&gt;+3200 pre-trained models in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;+1700 pre-trained pipelines in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, and Urdu.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To use Spark NLP you need the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 8 and 11&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark 3.2.x, 3.1.x, 3.0.x, 2.4.x, or 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Java 11 is only supported if you are using Spark NLP with Spark/PySpark 3.x and above&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU (optional):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 is built with TensorFlow 2.4.1 and requires the followings if you need GPU support&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA11 and cuDNN 8.0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This is a quick example of how to use Spark NLP pre-trained pipeline in Python and PySpark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ java -version&#xA;# should be Java 8 (Oracle or OpenJDK)&#xA;$ conda create -n sparknlp python=3.7 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Python console or Jupyter &lt;code&gt;Python3&lt;/code&gt; kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import Spark NLP&#xA;from sparknlp.base import *&#xA;from sparknlp.annotator import *&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;import sparknlp&#xA;&#xA;# Start SparkSession with Spark NLP&#xA;# start() functions has 5 parameters: gpu, spark23, spark24, spark32, and memory&#xA;# sparknlp.start(gpu=True) will start the session with GPU support&#xA;# sparknlp.start(spark23=True) is when you have Apache Spark 2.3.x installed&#xA;# sparknlp.start(spark24=True) is when you have Apache Spark 2.4.x installed&#xA;# sparknlp.start(spark32=True) is when you have Apache Spark 3.2.x installed&#xA;# sparknlp.start(memory=&#34;16G&#34;) to change the default driver memory in SparkSession&#xA;spark = sparknlp.start()&#xA;&#xA;# Download a pre-trained pipeline&#xA;pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;&#xA;# Your testing dataset&#xA;text = &#34;&#34;&#34;&#xA;The Mona Lisa is a 16th century oil painting created by Leonardo.&#xA;It&#39;s held at the Louvre in Paris.&#xA;&#34;&#34;&#34;&#xA;&#xA;# Annotate your testing dataset&#xA;result = pipeline.annotate(text)&#xA;&#xA;# What&#39;s in the pipeline&#xA;list(result.keys())&#xA;Output: [&#39;entities&#39;, &#39;stem&#39;, &#39;checked&#39;, &#39;lemma&#39;, &#39;document&#39;,&#xA;&#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;]&#xA;&#xA;# Check the results&#xA;result[&#39;entities&#39;]&#xA;Output: [&#39;Mona Lisa&#39;, &#39;Leonardo&#39;, &#39;Louvre&#39;, &#39;Paris&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, you can visit our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;repository&lt;/a&gt; to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;h2&gt;Apache Spark Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP &lt;em&gt;3.4.4&lt;/em&gt; has been built on top of Apache Spark 3.x while fully supports Apache Spark 2.3.x, 2.4.x, 3.0.x, 3.1.x, and 3.2.x:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.3.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.4.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.0.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.1.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.2.x&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starting 3.0.0 release, the default &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;spark-nlp-gpu&lt;/code&gt; pacakges are based on Scala 2.12 and Apache Spark 3.x by default.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find out more about &lt;code&gt;Spark NLP&lt;/code&gt; versions from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Scala and Python Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.6&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.8&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.9&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.11&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.12&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Databricks Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following runtimes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5.5 LTS&lt;/li&gt; &#xA; &lt;li&gt;5.5 LTS ML&lt;/li&gt; &#xA; &lt;li&gt;6.4&lt;/li&gt; &#xA; &lt;li&gt;6.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.3&lt;/li&gt; &#xA; &lt;li&gt;7.3 ML&lt;/li&gt; &#xA; &lt;li&gt;7.4&lt;/li&gt; &#xA; &lt;li&gt;7.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.5&lt;/li&gt; &#xA; &lt;li&gt;7.5 ML&lt;/li&gt; &#xA; &lt;li&gt;7.6&lt;/li&gt; &#xA; &lt;li&gt;7.6 ML&lt;/li&gt; &#xA; &lt;li&gt;8.0&lt;/li&gt; &#xA; &lt;li&gt;8.0 ML&lt;/li&gt; &#xA; &lt;li&gt;8.1&lt;/li&gt; &#xA; &lt;li&gt;8.1 ML&lt;/li&gt; &#xA; &lt;li&gt;8.2&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML&lt;/li&gt; &#xA; &lt;li&gt;8.3&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML&lt;/li&gt; &#xA; &lt;li&gt;8.4&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML&lt;/li&gt; &#xA; &lt;li&gt;9.0&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML&lt;/li&gt; &#xA; &lt;li&gt;9.1&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.0&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML&lt;/li&gt; &#xA; &lt;li&gt;10.1&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.2&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML&lt;/li&gt; &#xA; &lt;li&gt;10.3&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML &amp;amp; GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Spark NLP 3.4.4 is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 8.x and above as listed under GPU.&lt;/p&gt; &#xA;&lt;h2&gt;EMR Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following EMR releases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;emr-5.20.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.22.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.23.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.25.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.26.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.27.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.28.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.29.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.31.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.32.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.34.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.1.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.2.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.1&lt;/li&gt; &#xA; &lt;li&gt;emr-6.4.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.5.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html&#34;&gt;Amazon EMR 5.x releases&lt;/a&gt; Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html&#34;&gt;Amazon EMR 6.x releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.4.4&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h2&gt;Packages Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;This is a cheatsheet for corresponding Spark NLP Maven package to Apache Spark / PySpark major version:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Apache Spark&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on CPU&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on GPU&lt;/th&gt; &#xA;   &lt;th&gt;Start()&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x/3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark32=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark24=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark23=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Spark Packages&lt;/h2&gt; &#xA;&lt;h3&gt;Command line (requires internet connection)&lt;/h3&gt; &#xA;&lt;p&gt;Spark NLP supports all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, Apache Spark 3.1.x, and Apache Spark 3.2.x. That&#39;s being said, you need to choose the right package name for the right Apache Spark major release:&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.x (3.0.x and 3.1.x - Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.2.x (Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.4.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.3.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scala&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x, 3.1.x, and 3.2.x versions. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Maven&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark24_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maven Central: &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects &lt;a href=&#34;https://github.com/maziyarpanahi/spark-nlp-starter&#34;&gt;Spark NLP SBT Starter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Python 3.6.x and above depending on your major PySpark version.&lt;/p&gt; &#xA;&lt;h3&gt;Python without explicit Pyspark installation&lt;/h3&gt; &#xA;&lt;h3&gt;Pip/Conda&lt;/h3&gt; &#xA;&lt;p&gt;If you installed pyspark through pip/conda, you can install &lt;code&gt;spark-nlp&lt;/code&gt; through the same channel.&lt;/p&gt; &#xA;&lt;p&gt;Pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PyPI &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34;&gt;spark-nlp package&lt;/a&gt; / Anaconda &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34;&gt;spark-nlp package&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you&#39;ll have to create a SparkSession either from Spark NLP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;&#xA;spark = sparknlp.start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[4]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using local jars, you can use &lt;code&gt;spark.jars&lt;/code&gt; instead for comma-delimited jar files. For cluster setups, of course, you&#39;ll have to put the jars in a reachable location for all driver and executor nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;&#xA;#create or get Spark Session&#xA;&#xA;spark = sparknlp.start()&#xA;&#xA;sparknlp.version()&#xA;spark.version&#xA;&#xA;#download, load and annotate a text by pre-trained pipeline&#xA;&#xA;pipeline = PretrainedPipeline(&#39;recognize_entities_dl&#39;, &#39;en&#39;)&#xA;result = pipeline.annotate(&#39;The Mona Lisa is a 16th century oil painting created by Leonardo&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiled JARs&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;h4&gt;spark-nlp&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the jar manually&lt;/h3&gt; &#xA;&lt;p&gt;If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;Maven Central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To add JARs to spark programs use the &lt;code&gt;--jars&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell --jars spark-nlp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The preferred way to use the library when running spark programs is using the &lt;code&gt;--packages&lt;/code&gt; option as specified in the &lt;code&gt;spark-packages&lt;/code&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Apache Zeppelin&lt;/h2&gt; &#xA;&lt;p&gt;Use either one of the following options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the following Maven Coordinates to the interpreter&#39;s library list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a path to pre-built jar from &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;here&lt;/a&gt; in the interpreter&#39;s library list making sure the jar is available to driver path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Python in Zeppelin&lt;/h3&gt; &#xA;&lt;p&gt;Apart from the previous step, install the python module through pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can install &lt;code&gt;spark-nlp&lt;/code&gt; from inside Zeppelin by using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python.conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.&lt;/p&gt; &#xA;&lt;p&gt;Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. &lt;code&gt;python3&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;An alternative option would be to set &lt;code&gt;SPARK_SUBMIT_OPTIONS&lt;/code&gt; (zeppelin-env.sh) and make sure &lt;code&gt;--packages&lt;/code&gt; is there as shown earlier since it includes both scala and python side installation.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook (Python)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recomended:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to get this done on Linux and macOS is to simply install &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;pyspark&lt;/code&gt; PyPI packages and launch the Jupyter from the same Python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda create -n sparknlp python=3.8 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2 jupyter&#xA;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The you can use &lt;code&gt;python3&lt;/code&gt; kernel to run your code with creating SparkSession via &lt;code&gt;spark = sparknlp.start()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_HOME=/path/to/your/spark/folder&#xA;export PYSPARK_PYTHON=python3&#xA;export PYSPARK_DRIVER_PYTHON=jupyter&#xA;export PYSPARK_DRIVER_PYTHON_OPTS=notebook&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can mix in using &lt;code&gt;--jars&lt;/code&gt; option for pyspark + &lt;code&gt;pip install spark-nlp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If not using pyspark at all, you&#39;ll have to run the instructions pointed &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python-without-explicit-Pyspark-installation&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Google Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code in Google Colab notebook and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# This is only to setup PySpark and Spark NLP on Colab&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script comes with the two options to define &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-nlp&lt;/code&gt; versions via options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# -p is for pyspark&#xA;# -s is for spark-nlp&#xA;# by default they are set to the latest&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.1.2 -s 3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb&#34;&gt;Spark NLP quick start on Google Colab&lt;/a&gt; is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines.&lt;/p&gt; &#xA;&lt;h2&gt;Kaggle Kernel&lt;/h2&gt; &#xA;&lt;p&gt;Run the following code in Kaggle Kernel and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Let&#39;s setup Kaggle for Spark NLP and PySpark&#xA;!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mozzie/spark-nlp-named-entity-recognition&#34;&gt;Spark NLP quick start on Kaggle Kernel&lt;/a&gt; is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Databricks Cluster&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a cluster if you don&#39;t have one already&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;code&gt;Libraries&lt;/code&gt; tab inside your cluster you need to follow these steps:&lt;/p&gt; &lt;p&gt;3.1. Install New -&amp;gt; PyPI -&amp;gt; &lt;code&gt;spark-nlp==3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;p&gt;3.2. Install New -&amp;gt; Maven -&amp;gt; Coordinates -&amp;gt; &lt;code&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you can attach your notebook to the cluster and use Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE: Databrick&#39;s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name for your runtime from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;EMR Cluster&lt;/h2&gt; &#xA;&lt;p&gt;To launch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration.&lt;/p&gt; &#xA;&lt;p&gt;A sample of your bootstrap script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;#!/bin/bash&#xA;set -x -e&#xA;&#xA;echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3&#xA;export HADOOP_CONF_DIR=/etc/hadoop/conf&#xA;export SPARK_JARS_DIR=/usr/lib/spark/jars&#xA;export SPARK_HOME=/usr/lib/spark&#39; &amp;gt;&amp;gt; $HOME/.bashrc &amp;amp;&amp;amp; source $HOME/.bashrc&#xA;&#xA;sudo python3 -m pip install awscli boto spark-nlp&#xA;&#xA;set +x&#xA;exit 0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of your software configuration in JSON on S3 (must be public access):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.json&#34;&gt;[{&#xA;  &#34;Classification&#34;: &#34;spark-env&#34;,&#xA;  &#34;Configurations&#34;: [{&#xA;    &#34;Classification&#34;: &#34;export&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;PYSPARK_PYTHON&#34;: &#34;/usr/bin/python3&#34;&#xA;    }&#xA;  }]&#xA;},&#xA;{&#xA;  &#34;Classification&#34;: &#34;spark-defaults&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;spark.yarn.stagingDir&#34;: &#34;hdfs:///tmp&#34;,&#xA;      &#34;spark.yarn.preserve.staging.files&#34;: &#34;true&#34;,&#xA;      &#34;spark.kryoserializer.buffer.max&#34;: &#34;2000M&#34;,&#xA;      &#34;spark.serializer&#34;: &#34;org.apache.spark.serializer.KryoSerializer&#34;,&#xA;      &#34;spark.driver.maxResultSize&#34;: &#34;0&#34;,&#xA;      &#34;spark.jars.packages&#34;: &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;&#xA;    }&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of AWS CLI to launch EMR cluster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;aws emr create-cluster \&#xA;--name &#34;Spark NLP 3.4.4&#34; \&#xA;--release-label emr-6.2.0 \&#xA;--applications Name=Hadoop Name=Spark Name=Hive \&#xA;--instance-type m4.4xlarge \&#xA;--instance-count 3 \&#xA;--use-default-roles \&#xA;--log-uri &#34;s3://&amp;lt;S3_BUCKET&amp;gt;/&#34; \&#xA;--bootstrap-actions Path=s3://&amp;lt;S3_BUCKET&amp;gt;/emr-bootstrap.sh,Name=custome \&#xA;--configurations &#34;https://&amp;lt;public_access&amp;gt;/sparknlp-config.json&#34; \&#xA;--ec2-attributes KeyName=&amp;lt;your_ssh_key&amp;gt;,EmrManagedMasterSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt;,EmrManagedSlaveSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt; \&#xA;--profile &amp;lt;aws_profile_credentials&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP Dataproc&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a cluster if you don&#39;t have one already as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At gcloud shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud services enable dataproc.googleapis.com \&#xA;  compute.googleapis.com \&#xA;  storage-component.googleapis.com \&#xA;  bigquery.googleapis.com \&#xA;  bigquerystorage.googleapis.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;ZONE=&amp;lt;zone&amp;gt;&#xA;CLUSTER_NAME=&amp;lt;cluster_name&amp;gt;&#xA;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. Don&#39;t forget to set the maven coordinates for the jar in properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud dataproc clusters create ${CLUSTER_NAME} \&#xA;  --region=${REGION} \&#xA;  --zone=${ZONE} \&#xA;  --image-version=2.0 \&#xA;  --master-machine-type=n1-standard-4 \&#xA;  --worker-machine-type=n1-standard-2 \&#xA;  --master-boot-disk-size=128GB \&#xA;  --worker-boot-disk-size=128GB \&#xA;  --num-workers=2 \&#xA;  --bucket=${BUCKET_NAME} \&#xA;  --optional-components=JUPYTER \&#xA;  --enable-component-gateway \&#xA;  --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; \&#xA;  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh \&#xA;  --properties spark:spark.serializer=org.apache.spark.serializer.KryoSerializer,spark:spark.driver.maxResultSize=0,spark:spark.kryoserializer.buffer.max=2000M,spark:spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, you can attach your notebook to the cluster and use the Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Spark NLP Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You can change the following Spark NLP configurations via Spark Configuration:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Property Name&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.pretrained.cache_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/cache_pretrained&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to download and exctract pretrained &lt;code&gt;Models&lt;/code&gt; and &lt;code&gt;Pipelines&lt;/code&gt;. By default, it will be in User&#39;s Home directory under &lt;code&gt;cache_pretrained&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.storage.cluster_tmp_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to use on a cluster for temporarily files such as unpacking indexes for WordEmbeddings. By default, this locations is the location of &lt;code&gt;hadoop.tmp.dir&lt;/code&gt; set via Hadoop configuration for Apache Spark. NOTE: &lt;code&gt;S3&lt;/code&gt; is not supported and it must be local, HDFS, or DBFS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.annotator.log_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/annotator_logs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to save logs from annotators during training such as &lt;code&gt;NerDLApproach&lt;/code&gt;, &lt;code&gt;ClassifierDLApproach&lt;/code&gt;, &lt;code&gt;SentimentDLApproach&lt;/code&gt;, &lt;code&gt;MultiClassifierDLApproach&lt;/code&gt;, etc. By default, it will be in User&#39;s Home directory under &lt;code&gt;annotator_logs&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.access_key_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.secret_access_key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS secret access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.session_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS MFA session token to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.s3_bucket&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.region&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS region to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How to set Spark NLP Configuration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparkSession:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.config()&lt;/code&gt; during SparkSession creation to set Spark NLP configurations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder \&#xA;        .master(&#34;local[*]&#34;) \&#xA;        .config(&#34;spark.driver.memory&#34;, &#34;16G&#34;) \&#xA;        .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;        .config(&#34;spark.serializer&#34;, &#34;org.apache.spark.serializer.KryoSerializer&#34;) \&#xA;        .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000m&#34;) \&#xA;        .config(&#34;spark.jsl.settings.pretrained.cache_folder&#34;, &#34;sample_data/pretrained&#34;) \&#xA;        .config(&#34;spark.jsl.settings.storage.cluster_tmp_dir&#34;, &#34;sample_data/storage&#34;) \&#xA;        .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;) \&#xA;        .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-shell:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;pyspark:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pyspark \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Databricks:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;spark.jsl.settings.pretrained.cache_folder dbfs:/PATH_TO_CACHE&#xA;spark.jsl.settings.storage.cluster_tmp_dir dbfs:/PATH_TO_STORAGE&#xA;spark.jsl.settings.annotator.log_folder dbfs:/PATH_TO_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If this is an existing cluster, after adding new configs or changing existing properties you need to restart it.&lt;/p&gt; &#xA;&lt;h3&gt;S3 Integration&lt;/h3&gt; &#xA;&lt;p&gt;In Spark NLP we can define S3 locations to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Export log files of training models&lt;/li&gt; &#xA; &lt;li&gt;Store tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To configure S3 path for logging while training models. We need to set up AWS credentials as well as an S3 path&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.annotator.log_folder&#34;, &#34;s3://my/s3/path/logs&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.s3_bucket&#34;, &#34;my.bucket&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can check the log on your S3 path defined in &lt;em&gt;spark.jsl.settings.annotator.log_folder&lt;/em&gt; property. Make sure to use the prefix &lt;em&gt;s3://&lt;/em&gt;, otherwise it will use the default configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensorflow Graphs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To reference S3 location for downloading graphs. We need to set up AWS credentials&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;MFA Configuration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case your AWS account is configured with MFA. You will need first to get temporal credentials and add session token to the configuration as shown in the examples below For logging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.session_token&#34;, &#34;MY_TOKEN&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of a bash script that gets temporal AWS credentials can be found &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/scripts/aws_tmp_credentials.sh&#34;&gt;here&lt;/a&gt; This script requires three arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./aws_tmp_credentials.sh iam_user duration serial_number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pipelines and Models&lt;/h2&gt; &#xA;&lt;h3&gt;Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;&#xA;SparkNLP.version()&#xA;&#xA;val testData = spark.createDataFrame(Seq(&#xA;(1, &#34;Google has announced the release of a beta version of the popular TensorFlow machine learning library&#34;),&#xA;(2, &#34;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&#34;)&#xA;)).toDF(&#34;id&#34;, &#34;text&#34;)&#xA;&#xA;val pipeline = PretrainedPipeline(&#34;explain_document_dl&#34;, lang=&#34;en&#34;)&#xA;&#xA;val annotation = pipeline.transform(testData)&#xA;&#xA;annotation.show()&#xA;/*&#xA;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;2.5.0&#xA;testData: org.apache.spark.sql.DataFrame = [id: int, text: string]&#xA;pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)&#xA;annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|&#xA;|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;*/&#xA;&#xA;annotation.select(&#34;entities.result&#34;).show(false)&#xA;&#xA;/*&#xA;+----------------------------------+&#xA;|result                            |&#xA;+----------------------------------+&#xA;|[Google, TensorFlow]              |&#xA;|[Donald John Trump, United States]|&#xA;+----------------------------------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Pipelines&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all of the available Pipelines of a particular language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;)&#xA;/*&#xA;+--------------------------------------------+------+---------+&#xA;| Pipeline                                   | lang | version |&#xA;+--------------------------------------------+------+---------+&#xA;| dependency_parse                           |  en  | 2.0.2   |&#xA;| analyze_sentiment_ml                       |  en  | 2.0.2   |&#xA;| check_spelling                             |  en  | 2.1.0   |&#xA;| match_datetime                             |  en  | 2.1.0   |&#xA;                               ...&#xA;| explain_document_ml                        |  en  | 3.1.3   |&#xA;+--------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+---------------------------------------+------+---------+&#xA;| Pipeline                              | lang | version |&#xA;+---------------------------------------+------+---------+&#xA;| dependency_parse                      |  en  | 2.0.2   |&#xA;                               ...&#xA;| clean_slang                           |  en  | 3.0.0   |&#xA;| clean_pattern                         |  en  | 3.0.0   |&#xA;| check_spelling                        |  en  | 3.0.0   |&#xA;| dependency_parse                      |  en  | 3.0.0   |&#xA;+---------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained pipelines&lt;/a&gt; with examples, demos, benchmarks, and more&lt;/h4&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some selected languages:&lt;/strong&gt; &lt;code&gt;Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick online example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load NER model trained by deep learning approach and GloVe word embeddings&#xA;ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;)&#xA;# load NER model trained by deep learning approach and BERT word embeddings&#xA;ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// load French POS tagger model trained by Universal Dependencies&#xA;val french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;// load Italain LemmatizerModel&#xA;val italian_lemma = LemmatizerModel.pretrained(&#34;lemma_dxc&#34;, lang=&#34;it&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick offline example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading &lt;code&gt;PerceptronModel&lt;/code&gt; annotator model inside Spark NLP Pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Models&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all the available Models of a particular Annotator and language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;)&#xA;/*&#xA;+---------------------------------------------+------+---------+&#xA;| Model                                       | lang | version |&#xA;+---------------------------------------------+------+---------+&#xA;| onto_100                                    |  en  | 2.1.0   |&#xA;| onto_300                                    |  en  | 2.1.0   |&#xA;| ner_dl_bert                                 |  en  | 2.2.0   |&#xA;| onto_100                                    |  en  | 2.4.0   |&#xA;| ner_conll_elmo                              |  en  | 3.2.2   |&#xA;+---------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+----------------------------+------+---------+&#xA;| Model                      | lang | version |&#xA;+----------------------------+------+---------+&#xA;| onto_100                   |  en  | 2.1.0   |&#xA;| ner_aspect_based_sentiment |  en  | 2.6.2   |&#xA;| ner_weibo_glove_840B_300d  |  en  | 2.6.2   |&#xA;| nerdl_atis_840b_300d       |  en  | 2.7.1   |&#xA;| nerdl_snips_100d           |  en  | 2.7.3   |&#xA;+----------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to see a list of available annotators, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showAvailableAnnotators()&#xA;/*&#xA;AlbertEmbeddings&#xA;AlbertForTokenClassification&#xA;AssertionDLModel&#xA;...&#xA;XlmRoBertaSentenceEmbeddings&#xA;XlnetEmbeddings&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained models&lt;/a&gt; with examples, demo, benchmark, and more&lt;/h4&gt; &#xA;&lt;h2&gt;Offline&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of using the Maven package, you need to load our Fat JAR&lt;/li&gt; &#xA; &lt;li&gt;Instead of using PretrainedPipeline for pretrained pipelines or the &lt;code&gt;.pretrained()&lt;/code&gt; function to download pretrained models, you will need to manually download your pipeline/model from &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;Models Hub&lt;/a&gt;, extract it, and load it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of &lt;code&gt;SparkSession&lt;/code&gt; with Fat JAR to have Spark NLP offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[*]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars&#34;, &#34;/tmp/spark-nlp-assembly-3.4.4.jar&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download provided Fat JARs from each &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x)&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/spark-nlp-assembly-3.4.4.jar&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of using pretrained Models and Pipelines in offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# instead of using pretrained() for online:&#xA;# french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;# you download this model, extract it, and use .load&#xA;french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)\&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)\&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&#xA;# example for pipelines&#xA;# instead of using PretrainedPipeline&#xA;# pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;# you download this pipeline, extract it, and use PipelineModel&#xA;PipelineModel.load(&#34;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Need more &lt;strong&gt;examples&lt;/strong&gt;? Check out our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;Spark NLP Showcase&lt;/a&gt; repository to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;p&gt;Also, don&#39;t forget to check &lt;a href=&#34;https://nlp.johnsnowlabs.com/demo&#34;&gt;Spark NLP in Action&lt;/a&gt; built by Streamlit.&lt;/p&gt; &#xA;&lt;h3&gt;All examples: &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;spark-nlp-workshop&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nlp.johnsnowlabs.com/learn&#34;&gt;Check our Articles and Videos page here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;We have published a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2665963821000063&#34;&gt;paper&lt;/a&gt; that you can cite for the Spark NLP library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{KOCAMAN2021100058,&#xA;    title = {Spark NLP: Natural language understanding at scale},&#xA;    journal = {Software Impacts},&#xA;    pages = {100058},&#xA;    year = {2021},&#xA;    issn = {2665-9638},&#xA;    doi = {https://doi.org/10.1016/j.simpa.2021.100058},&#xA;    url = {https://www.sciencedirect.com/science/article/pii/S2665963.2.100063},&#xA;    author = {Veysel Kocaman and David Talby},&#xA;    keywords = {Spark, Natural language processing, Deep learning, Tensorflow, Cluster},&#xA;    abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant &amp;amp; accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world‚Äôs most widely used NLP library in the enterprise.}&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any sort of contributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ideas&lt;/li&gt; &#xA; &lt;li&gt;feedback&lt;/li&gt; &#xA; &lt;li&gt;documentation&lt;/li&gt; &#xA; &lt;li&gt;bug reports&lt;/li&gt; &#xA; &lt;li&gt;NLP training and testing corpora&lt;/li&gt; &#xA; &lt;li&gt;Development and testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone the repo and submit your pull-requests! Or directly create issues in this repo.&lt;/p&gt; &#xA;&lt;h2&gt;John Snow Labs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://johnsnowlabs.com&#34;&gt;http://johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>spark-examples/spark-scala-examples</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/spark-examples/spark-scala-examples</id>
    <link href="https://github.com/spark-examples/spark-scala-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project provides Apache Spark SQL, RDD, DataFrame and Dataset examples in Scala language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Explanation of all Spark SQL, RDD, DataFrame and Dataset examples present on this project are available at &lt;a href=&#34;https://sparkbyexamples.com/&#34;&gt;https://sparkbyexamples.com/&lt;/a&gt; , All these examples are coded in Scala language and tested in our development environment.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents (Spark Examples in Scala)&lt;/h1&gt; &#xA;&lt;h2&gt;Spark RDD Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/&#34;&gt;Create a Spark RDD using Parallelize&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-read-multiple-text-files-into-a-single-rdd/&#34;&gt;Spark ‚Äì Read multiple text files into single RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-load-csv-file-into-rdd/&#34;&gt;Spark load CSV file into RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/different-ways-to-create-spark-rdd/&#34;&gt;Different ways to create Spark RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-how-to-create-an-empty-rdd/&#34;&gt;Spark ‚Äì How to create an empty RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/&#34;&gt;Spark RDD Transformations with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/&#34;&gt;Spark RDD Actions with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-pair-rdd-functions/&#34;&gt;Spark Pair RDD Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/&#34;&gt;Spark Repartition() vs Coalesce()&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-shuffle-partitions/&#34;&gt;Spark Shuffle Partitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-persistence-storage-levels/&#34;&gt;Spark Persistence Storage Levels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-cache-and-persist-example/&#34;&gt;Spark RDD Cache and Persist with Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-broadcast-variables/&#34;&gt;Spark Broadcast Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-accumulators/&#34;&gt;Spark Accumulators Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/&#34;&gt;Convert Spark RDD to DataFrame | Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/&#34;&gt;Spark Create DataFrame with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/&#34;&gt;Spark DataFrame withColumn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/&#34;&gt;Ways to Rename column on Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-drop-column-from-dataframe-dataset/&#34;&gt;Spark ‚Äì How to Drop a DataFrame/Dataset column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-where-filter/&#34;&gt;Working with Spark DataFrame Where Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/&#34;&gt;Spark SQL ‚Äúcase when‚Äù and ‚Äúwhen otherwise‚Äù&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-collect/&#34;&gt;Collect() ‚Äì Retrieve data from Spark RDD/DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-remove-duplicate-rows/&#34;&gt;Spark ‚Äì How to remove duplicate rows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/&#34;&gt;How to Pivot and Unpivot a Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/&#34;&gt;Spark SQL Data Types with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/&#34;&gt;Spark SQL StructType &amp;amp; StructField with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/&#34;&gt;Spark schema ‚Äì explained with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-groupby-on-dataframe/&#34;&gt;Spark Groupby Example with DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-how-to-sort-dataframe-column-explained/&#34;&gt;Spark ‚Äì How to Sort DataFrame column explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-join/&#34;&gt;Spark SQL Join Types with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-union-and-union-all/&#34;&gt;Spark DataFrame Union and UnionAll&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/&#34;&gt;Spark map vs mapPartitions transformation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-foreachpartition-vs-foreach-explained/&#34;&gt;Spark foreachPartition vs foreach | what to use?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/&#34;&gt;Spark DataFrame Cache and Persist Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-udf/&#34;&gt;Spark SQL UDF (User Defined Functions)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/&#34;&gt;Spark SQL DataFrame Array (ArrayType) Column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/&#34;&gt;Working with Spark DataFrame Map (MapType) column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-struct-column/&#34;&gt;Spark SQL ‚Äì Flatten Nested Struct column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/&#34;&gt;Spark ‚Äì Flatten nested array to single array column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/&#34;&gt;Spark explode array and map columns to rows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/&#34;&gt;Spark SQL String Functions Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/&#34;&gt;Spark SQL Date and Time Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-array-functions/&#34;&gt;Spark SQL Array functions complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-map-functions/&#34;&gt;Spark SQL Map functions ‚Äì complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-sort-functions/&#34;&gt;Spark SQL Sort functions ‚Äì complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/&#34;&gt;Spark SQL Aggregate Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-window-functions/&#34;&gt;Spark Window Functions with Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Data Source API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&#34;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-and-write-json-file/&#34;&gt;Spark Read and Write JSON file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/&#34;&gt;Spark Read and Write Apache Parquet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-xml/&#34;&gt;Spark Read XML file using Databricks API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/&#34;&gt;Read &amp;amp; Write Avro files using Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-avro-data-files-from-spark-sql-2-3-x/&#34;&gt;Using Avro Data Files From Spark SQL 2.3.x or earlier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/&#34;&gt;Spark Read from &amp;amp; Write to HBase table | Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/create-spark-dataframe-from-hbase-using-hortonworks/&#34;&gt;Create Spark DataFrame from HBase using Hortonworks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/&#34;&gt;Spark Read ORC file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/&#34;&gt;Spark 3.0 Read Binary File into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Streaming &amp;amp; Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-outputmode/&#34;&gt;Spark Streaming ‚Äì Different Output modes explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-read-json-files-from-directory/&#34;&gt;Spark Streaming files from a directory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-from-tcp-socket/&#34;&gt;Spark Streaming ‚Äì Reading data from TCP Socket&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-with-kafka/&#34;&gt;Spark Streaming with Kafka Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-consume-and-produce-kafka-messages-in-avro-format/&#34;&gt;Spark Streaming ‚Äì Kafka messages in Avro format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-batch-processing-produce-consume-kafka-topic/&#34;&gt;Spark SQL Batch Processing ‚Äì Produce and Consume Apache Kafka Topic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>apache/openwhisk</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/apache/openwhisk</id>
    <link href="https://github.com/apache/openwhisk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache OpenWhisk is an open source serverless cloud platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenWhisk&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/github/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://travis-ci.com/apache/openwhisk.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openwhisk-team.slack.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join-slack-9B69A0.svg?sanitize=true&#34; alt=&#34;Join Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/openwhisk/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=openwhisk&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/openwhisk.svg?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenWhisk is a serverless functions platform for building cloud applications. OpenWhisk offers a rich programming model for creating serverless APIs from functions, composing functions into serverless workflows, and connecting events to functions using rules and triggers. Learn more at &lt;a href=&#34;http://openwhisk.apache.org&#34;&gt;http://openwhisk.apache.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt; (Deploy and Use OpenWhisk on your machine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#deploy-to-kubernetes&#34;&gt;Deploy to Kubernetes&lt;/a&gt; (For development and production)&lt;/li&gt; &#xA; &lt;li&gt;For project contributors and Docker deployments: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/macos/README.md&#34;&gt;Deploy to Docker for Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/ubuntu-setup/README.md&#34;&gt;Deploy to Docker for Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#learn-concepts-and-commands&#34;&gt;Learn Concepts and Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#openwhisk-community-and-support&#34;&gt;OpenWhisk Community and Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#project-repository-structure&#34;&gt;Project Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to start using OpenWhisk is to install the &#34;Standalone&#34; OpenWhisk stack. This is a full-featured OpenWhisk stack running as a Java process for convenience. Serverless functions run within Docker containers. You will need &lt;a href=&#34;https://docs.docker.com/install&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://java.com/en/download/help/download_options.xml&#34;&gt;Java&lt;/a&gt; and &lt;a href=&#34;https://nodejs.org&#34;&gt;Node.js&lt;/a&gt; available on your machine.&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk.git&#xA;cd openwhisk&#xA;./gradlew core:standalone:bootRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When the OpenWhisk stack is up, it will open your browser to a functions &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/playground-ui.png&#34;&gt;Playground&lt;/a&gt;, typically served from &lt;a href=&#34;http://localhost:3232&#34;&gt;http://localhost:3232&lt;/a&gt;. The Playground allows you create and run functions directly from your browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make use of all OpenWhisk features, you will need the OpenWhisk command line tool called &lt;code&gt;wsk&lt;/code&gt; which you can download from &lt;a href=&#34;https://s.apache.org/openwhisk-cli-download&#34;&gt;https://s.apache.org/openwhisk-cli-download&lt;/a&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/cli.md&#34;&gt;CLI configuration&lt;/a&gt; for additional details. Typically you configure the CLI for Standalone OpenWhisk as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;wsk property set \&#xA;  --apihost &#39;http://localhost:3233&#39; \&#xA;  --auth &#39;23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone OpenWhisk can be configured to deploy additional capabilities when that is desirable. Additional resources are available &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/core/standalone/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deploy to Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;OpenWhisk can also be installed on a Kubernetes cluster. You can use a managed Kubernetes cluster provisioned from a public cloud provider (e.g., AKS, EKS, IKS, GKE), or a cluster you manage yourself. Additionally for local development, OpenWhisk is compatible with Minikube, and Kubernetes for Mac using the support built into Docker 18.06 (or higher).&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk-deploy-kube.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow the instructions in the &lt;a href=&#34;https://github.com/apache/openwhisk-deploy-kube/raw/master/README.md&#34;&gt;OpenWhisk on Kubernetes README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Learn Concepts and Commands&lt;/h3&gt; &#xA;&lt;p&gt;Browse the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/&#34;&gt;documentation&lt;/a&gt; to learn more. Here are some topics you may be interested in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/about.md&#34;&gt;System overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/README.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions.md&#34;&gt;Create and invoke actions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/triggers_rules.md&#34;&gt;Create triggers and rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/packages.md&#34;&gt;Use and create packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/catalog.md&#34;&gt;Browse and use the catalog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/reference.md&#34;&gt;OpenWhisk system details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/feeds.md&#34;&gt;Implementing feeds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions-actionloop.md&#34;&gt;Developing a runtime for a new language&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenWhisk Community and Support&lt;/h3&gt; &#xA;&lt;p&gt;Report bugs, ask questions and request features &lt;a href=&#34;https://raw.githubusercontent.com/apache/issues&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also join the OpenWhisk Team on Slack &lt;a href=&#34;https://openwhisk-team.slack.com&#34;&gt;https://openwhisk-team.slack.com&lt;/a&gt; and chat with developers. To get access to our public Slack team, request an invite &lt;a href=&#34;https://openwhisk.apache.org/slack.html&#34;&gt;https://openwhisk.apache.org/slack.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Project Repository Structure&lt;/h3&gt; &#xA;&lt;p&gt;The OpenWhisk system is built from a &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/dev/modules.md&#34;&gt;number of components&lt;/a&gt;. The picture below groups the components by their GitHub repos. Please open issues for a component against the appropriate repo (if in doubt just open against the main openwhisk repo).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/components_to_repos.png&#34; alt=&#34;component/repo mapping&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>delta-io/delta</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/delta-io/delta</id>
    <link href="https://github.com/delta-io/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://docs.delta.io/latest/_static/delta-lake-white.png&#34; width=&#34;200&#34; alt=&#34;Delta Lake Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-spark/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&#34;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&#34;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io&#34;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/quick-start.html&#34;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; &#xA; &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&#34;https://github.com/delta-io&#34;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&#34;https://github.com/delta-io/connectors&#34;&gt;connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta&#34;&gt;delta&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-sharing&#34;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/kafka-delta-ingest&#34;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&#34;https://github.com/delta-io/website&#34;&gt;website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&#34;https://delta.io/integrations/&#34;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/&#34;&gt;Apache Spark‚Ñ¢&lt;/a&gt;: This connector allows Apache Spark‚Ñ¢ to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/connectors/tree/master/flink&#34;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prestodb.io/docs/current/connector/deltalake.html&#34;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trino.io/docs/current/connector/delta-lake.html&#34;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/hive-integration.html&#34;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.rs/deltalake/latest/deltalake/&#34;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&#34;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&#34;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&#34;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&#34;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&#34;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&#34;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&#34;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Latest Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/&#34;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; &#xA;&lt;h2&gt;API Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/java/index.html&#34;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/python/index.html&#34;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table‚Äôs metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; &#xA;&lt;h3&gt;API Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; &#xA; &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/releases.html&#34;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; &#xA;&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/core/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&#34;&gt;action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&#34;http://delta.io/roadmap&#34;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For the detailed timeline, see the &lt;a href=&#34;https://github.com/delta-io/delta/milestones&#34;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transaction Protocol&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&#34;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/delta-storage.html&#34;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Concurrency Control&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&#34;https://docs.delta.io/latest/delta-concurrency.html&#34;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting issues&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake is compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate artifacts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSuite&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly *.OptimizeCompactionSuite -- -z &#34;optimize command: on partitioned table - all partitions&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; &#xA;&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Verification&lt;/h3&gt; &#xA;&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you see errors of the form&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser&#xA;import io.delta.sql.parser.DeltaSqlBaseParser._&#xA;...&#xA;Error:(91, 22) not found: type DeltaSqlBaseParser&#xA;    val parser = new DeltaSqlBaseParser(tokenStream)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt compile&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-core&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0, see &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public Slack Channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/delta-users/shared_invite/zt-165gcm2g7-0Sc57w7dX0FbfilR9EPwVQ&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/deltalake&#34;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/deltalake&#34;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>zio/zio</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/zio/zio</id>
    <link href="https://github.com/zio/zio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ZIO ‚Äî A type-safe, composable library for async and concurrent programming in Scala&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/ZIO.png&#34; alt=&#34;ZIO Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Project Stage&lt;/th&gt; &#xA;   &lt;th&gt;CI&lt;/th&gt; &#xA;   &lt;th&gt;Release&lt;/th&gt; &#xA;   &lt;th&gt;Snapshot&lt;/th&gt; &#xA;   &lt;th&gt;Issues&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zio/zio/wiki/Project-Stages&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project%20Stage-Production%20Ready-brightgreen.svg?sanitize=true&#34; alt=&#34;Project stage&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/zio/zio/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/releases/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/r/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Release Artifacts&#34; title=&#34;Sonatype Releases&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss.sonatype.org/content/repositories/snapshots/dev/zio/zio_2.12/&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;img src=&#34;https://img.shields.io/nexus/s/https/oss.sonatype.org/dev.zio/zio_2.12.svg?sanitize=true&#34; alt=&#34;Snapshot Artifacts&#34; title=&#34;Sonatype Snapshots&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://isitmaintained.com/project/zio/zio&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/zio/zio.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Scaladoc&lt;/th&gt; &#xA;   &lt;th&gt;Scaladex&lt;/th&gt; &#xA;   &lt;th&gt;Discord&lt;/th&gt; &#xA;   &lt;th&gt;Twitter&lt;/th&gt; &#xA;   &lt;th&gt;Gitpod&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://javadoc.io/doc/dev.zio/zio_2.12/latest/zio/index.html&#34;&gt;Scaladoc&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://index.scala-lang.org/zio/zio/zio&#34; title=&#34;Scaladex&#34;&gt;&lt;img src=&#34;https://index.scala-lang.org/zio/zio/zio/latest.svg?sanitize=true&#34; alt=&#34;Badge-Scaladex-page&#34; title=&#34;Scaladex&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/zioscala&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/zioscala.svg?style=plastic&amp;amp;label=follow&amp;amp;logo=twitter&#34; alt=&#34;Badge-Twitter&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/zio/zio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod&#34; alt=&#34;Gitpod ready-to-code&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Welcome to ZIO&lt;/h1&gt; &#xA;&lt;p&gt;ZIO is a zero-dependency Scala library for asynchronous and concurrent programming.&lt;/p&gt; &#xA;&lt;p&gt;Powered by highly-scalable, non-blocking fibers that never waste or leak resources, ZIO lets you build scalable, resilient, and reactive applications that meet the needs of your business.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance&lt;/strong&gt;. Build scalable applications with 100x the performance of Scala&#39;s &lt;code&gt;Future&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type-safe&lt;/strong&gt;. Use the full power of the Scala compiler to catch bugs at compile time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent&lt;/strong&gt;. Easily build concurrent apps without deadlocks, race conditions, or complexity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;. Write sequential code that looks the same whether it&#39;s asynchronous or synchronous.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource-safe&lt;/strong&gt;. Build apps that never leak resources (including threads!), even when they fail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testable&lt;/strong&gt;. Inject test services into your app for fast, deterministic, and type-safe testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resilient&lt;/strong&gt;. Build apps that never lose errors, and which respond to failure locally and flexibly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Functional&lt;/strong&gt;. Rapidly compose solutions to complex problems from simple building blocks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about ZIO, see the following references:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/contributing.md&#34;&gt;Contributor&#39;s Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zio/zio/pulls&#34;&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Adopters&lt;/h1&gt; &#xA;&lt;p&gt;Following is a partial list of companies happily using ZIO in production to craft concurrent applications.&lt;/p&gt; &#xA;&lt;p&gt;Want to see your company here? &lt;a href=&#34;https://github.com/zio/zio/edit/master/README.md&#34;&gt;Submit a PR&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://adgear.com/en/&#34;&gt;AdGear / Samsung Ads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adidas.com/&#34;&gt;Adidas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adpulse.io/&#34;&gt;adpulse.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.adsquare.com/&#34;&gt;adsquare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ayolab.com/&#34;&gt;Ayolab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://asana.com/&#34;&gt;Asana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aurinko.io/&#34;&gt;Aurinko&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://auto.ru&#34;&gt;auto.ru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.de&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.avast.com&#34;&gt;Avast&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bofa.com&#34;&gt;Bank of America&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bpp.it/&#34;&gt;Bpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://broad.app&#34;&gt;Broad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.caesars.com/sportsbook-and-casino&#34;&gt;Caesars Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.calcbank.com.br&#34;&gt;CalcBank&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.callhandling.co.uk/&#34;&gt;Call Handling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.carvana.com&#34;&gt;Carvana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cellular.de&#34;&gt;Cellular&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudfarms.com&#34;&gt;Cloudfarms&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codecomprehension.com&#34;&gt;CodeComprehension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codept.de/&#34;&gt;Codept&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com/en&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.collibra.com/&#34;&gt;Collibra&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.compellon.com/&#34;&gt;Compellon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.complicatedrobot.com/&#34;&gt;Complicated Robot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.conduktor.io&#34;&gt;Conduktor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.contramap.dev&#34;&gt;Contramap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://coralogix.com&#34;&gt;Coralogix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creditkarma.com&#34;&gt;Credit Karma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.currencycloud.com/&#34;&gt;CurrencyCloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://de-solution.com/&#34;&gt;D.E.Solution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datachef.co&#34;&gt;DataChef&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.demandbase.com&#34;&gt;Demandbase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demyst.com&#34;&gt;Demyst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devsisters.com/&#34;&gt;Devsisters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijdhl.nl/it&#34;&gt;DHL Parcel The Netherlands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.disneyplus.com/&#34;&gt;Disney+ Streaming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doomoolmori.com/&#34;&gt;Doomoolmori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dowjones.com&#34;&gt;Dow Jones&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dpgrecruitment.nl&#34;&gt;DPG recruitment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dream11.com&#34;&gt;Dream11&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iot.telekom.com/en&#34;&gt;Deutsche Telekom IoT GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ebay.com&#34;&gt;eBay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.eaglescience.nl&#34;&gt;Eaglescience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.edf.fr/&#34;&gt;Electricit√© de France (EDF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.enelx.com&#34;&gt;EnelX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evolution.engineering&#34;&gt;Evolution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://evo.company&#34;&gt;Evo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flipp.com/&#34;&gt;Flipp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fugo.ai&#34;&gt;Fugo.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.garnercorp.com/&#34;&gt;Garner Distributed Workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gleancompany.com&#34;&gt;Glean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://grandparade.co.uk&#34;&gt;GrandParade&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://greyflower.media&#34;&gt;greyflower.media GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hunters.ai&#34;&gt;Hunters.AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hypefactors.com/&#34;&gt;Hypefactors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.iheart.com/&#34;&gt;iHeartRadio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ihsmarkit.com/&#34;&gt;IHS Markit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://investsuite.com/&#34;&gt;Investsuite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaizen-solutions.net/&#34;&gt;Kaizen Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kamon.io/&#34;&gt;Kamon APM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.kodmagi.se&#34;&gt;Kodmagi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kensu.io&#34;&gt;Kensu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lambdaworks.io/&#34;&gt;LambdaWorks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://leadiq.com&#34;&gt;LeadIQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.lernkunst.com/&#34;&gt;Lernkunst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liveintent.com&#34;&gt;LiveIntent Inc.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lottoland.com&#34;&gt;Lottoland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matechs.com&#34;&gt;MATECHS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://megogo.net&#34;&gt;Megogo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mylivn.com/&#34;&gt;Mylivn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newmotion.com&#34;&gt;NewMotion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nexxchange.com&#34;&gt;Nexxchange&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nike.com&#34;&gt;Nike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nslookup.io&#34;&gt;NsLookup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://olyro.de&#34;&gt;Olyro GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optrak.com&#34;&gt;Optrak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.performance-immo.com/&#34;&gt;Performance Immo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.playtika.com&#34;&gt;Playtika&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ppcsamurai.com/&#34;&gt;PPC Samurai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com/&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.radix.bio/&#34;&gt;Radix Labs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.railroad19.com&#34;&gt;Railroad19&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.werkenbijrandstad.nl&#34;&gt;Randstad Groep Nederland&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rapidor.co&#34;&gt;Rapidor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pimsolutions.ru/&#34;&gt;PIM Solutions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rewe-digital.com/&#34;&gt;REWE Digital&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://riskident.com/&#34;&gt;Risk Ident&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://rocker.com/&#34;&gt;Rocker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rudder.io/&#34;&gt;Rudder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sanjagh.pro/&#34;&gt;Sanjagh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scalac.io/&#34;&gt;Scalac&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.securityscorecard.io/&#34;&gt;SecurityScorecard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sentinelone.com/&#34;&gt;SentinelOne&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.signicat.com/&#34;&gt;Signicat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://info.sgmarkets.com/en/&#34;&gt;Soci√©t√© G√©n√©rale Corporate and Investment Banking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://softwaremill.com/&#34;&gt;SoftwareMill&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.streamweaver.com/&#34;&gt;StreamWeaver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stuart.com/&#34;&gt;Stuart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://teads.com&#34;&gt;Teads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pokemon.com/us/about-pokemon/&#34;&gt;The Pokemon Company International&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tomtom.com&#34;&gt;TomTom&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tinka.com/&#34;&gt;Tinka&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trackabus.com&#34;&gt;Trackabus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.trainor.no&#34;&gt;Trainor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://treutech.io&#34;&gt;TreuTech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tweddle.com&#34;&gt;Tweddle Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.undo.app&#34;&gt;Undo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://unit.co&#34;&gt;Unit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://univalence.io&#34;&gt;Univalence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.unzer.com&#34;&gt;Unzer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vakantiediscounter.nl&#34;&gt;Vakantiediscounter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.verbund.com&#34;&gt;Verbund AG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.waylay.io/&#34;&gt;Waylay&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wehkamp.nl&#34;&gt;Wehkamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wolt.com/&#34;&gt;Wolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://o.yandex.ru&#34;&gt;Yandex.Classifieds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://audela.ca&#34;&gt;Audela&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valamis.com&#34;&gt;Valamis Group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valsea.com&#34;&gt;Valsea&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://virtuslab.com/&#34;&gt;VirtusLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://getvish.com&#34;&gt;Vish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vivid.money&#34;&gt;Vivid Money&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.com/&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zooz.com/&#34;&gt;Zooz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/ziverge.png&#34; alt=&#34;Ziverge&#34; title=&#34;Ziverge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ziverge.com&#34; title=&#34;Ziverge&#34;&gt;Ziverge&lt;/a&gt; is a leading contributor to ZIO.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/scalac.svg?sanitize=true&#34; alt=&#34;Scalac&#34; title=&#34;Scalac&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scalac.io&#34; title=&#34;Scalac&#34;&gt;Scalac&lt;/a&gt; sponsors ZIO Hackathons and contributes work to multiple projects in ZIO ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/septimal_mind.svg?sanitize=true&#34; alt=&#34;Septimal Mind&#34; title=&#34;Septimal Mind&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://7mind.io&#34; title=&#34;Septimal Mind&#34;&gt;Septimal Mind&lt;/a&gt; sponsors work on ZIO Tracing and continuous maintenance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/softwaremill.svg?sanitize=true&#34; alt=&#34;SoftwareMill&#34; title=&#34;SoftwareMill&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://softwaremill.com&#34; title=&#34;SoftwareMill&#34;&gt;SoftwareMill&lt;/a&gt; generously provides ZIO with paid-for CircleCI build infrastructure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zio/zio/master/website/static/img/yourkit.png&#34; alt=&#34;YourKit&#34; title=&#34;YourKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com&#34; title=&#34;YourKit&#34;&gt;YourKit&lt;/a&gt; generously provides use of their monitoring and profiling tools to maximize the performance of ZIO applications.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://zio.dev/&#34;&gt;Learn More on the ZIO Homepage&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/zio/zio/master/docs/about/code_of_conduct.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Come chat with us on &lt;a href=&#34;https://discord.gg/2ccFBr4&#34; title=&#34;Discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/629491597070827530?logo=discord&#34; alt=&#34;Badge-Discord&#34; title=&#34;chat on discord&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Legal&lt;/h3&gt; &#xA;&lt;p&gt;Copyright 2017 - 2020 John A. De Goes and the ZIO Contributors. All rights reserved.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>flix/flix</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/flix/flix</id>
    <link href="https://github.com/flix/flix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Flix Programming Language&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/logo.png&#34; height=&#34;91px&#34; alt=&#34;The Flix Programming Language&#34; title=&#34;The Flix Programming Language&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flix&lt;/strong&gt; is a statically typed functional, imperative, and logic programming language.&lt;/p&gt; &#xA;&lt;p&gt;We refer you to the &lt;a href=&#34;https://flix.dev/&#34;&gt;official Flix website (flix.dev)&lt;/a&gt; for more information about Flix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/flix/Lobby&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/example.png&#34; height=&#34;627px&#34; alt=&#34;Example Flix Program&#34; title=&#34;Example Flix Program&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/BUILD.md&#34;&gt;docs/BUILD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Flix is available under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;We kindly thank &lt;a href=&#34;https://www.ej-technologies.com/&#34;&gt;EJ Technologies&lt;/a&gt; for providing us with &lt;a href=&#34;http://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler&lt;/a&gt; and &lt;a href=&#34;https://www.jetbrains.com/&#34;&gt;JetBrains&lt;/a&gt; for providing us with &lt;a href=&#34;https://www.jetbrains.com/idea/&#34;&gt;IntelliJ IDEA&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xxf098/shadowsocksr-v2ray-trojan-android</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/xxf098/shadowsocksr-v2ray-trojan-android</id>
    <link href="https://github.com/xxf098/shadowsocksr-v2ray-trojan-android" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A V2Ray, Trojan, ShadowsocksR client for Android&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;A ShadowsocksR, V2Ray and Trojan Client for Android&lt;/h2&gt; &#xA;&lt;p&gt;A fully featured &lt;a href=&#34;https://github.com/breakwa11/shadowsocks-rss/&#34;&gt;ShadowsocksR&lt;/a&gt;, &lt;a href=&#34;https://github.com/v2ray/v2ray-core&#34;&gt;V2Ray&lt;/a&gt; and &lt;a href=&#34;https://trojan-gfw.github.io/trojan/protocol&#34;&gt;Trojan&lt;/a&gt; client for Android, written in Scala.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/xxf098/shadowsocksr-v2ray-android/workflows/build/badge.svg?branch=xxf098%2Fmaster&amp;amp;event=push&#34; alt=&#34;build&#34;&gt; &lt;a href=&#34;https://github.com/xxf098/shadowsocksr-v2ray-android/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/xxf098/shadowsocksr-v2ray-android&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/xxf098/shadowsocksr-v2ray-android/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/xxf098/shadowsocksr-v2ray-android.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;PREREQUISITES&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A really fast proxy&lt;/li&gt; &#xA; &lt;li&gt;JDK 1.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    sudo apt-get install openjdk-8-jdk&#xA;    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64&#xA;    export PATH=$PATH:$JAVA_HOME/bin&#xA;    java -version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SBT &lt;a href=&#34;https://www.scala-sbt.org/download.html&#34;&gt;0.13.8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Android SDK &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Build Tools 30+&lt;/li&gt; &#xA;   &lt;li&gt;Android Support Repository and Google Repository (see &lt;code&gt;build.sbt&lt;/code&gt; for version)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Android NDK r21e+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;BUILD with Android Studio&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Warnning: Cannot build in windows&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://developer.android.com/studio&#34;&gt;Android Studio&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://developer.android.com/ndk/downloads/older_releases&#34;&gt;Android NDK r20b&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://plugins.jetbrains.com/plugin/1347-scala&#34;&gt;Scala&lt;/a&gt; plugin for IntelliJ IDEA&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set proxy for Android Studio: &lt;code&gt;File | Settings | Appearance &amp;amp; Behavior | System Settings | HTTP Proxy&lt;/code&gt;&lt;br&gt; Set proxy for sbt: &lt;code&gt;File | Settings | Build, Execution, Deployment | Build Tools | sbt&lt;/code&gt;, in &lt;code&gt;VM parameters&lt;/code&gt; input:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  -Dhttps.proxyHost=127.0.0.1&#xA;  -Dhttps.proxyPort=8080&#xA;  -Dhttp.proxyHost=127.0.0.1&#xA;  -Dhttp.proxyPort=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set environment variable &lt;code&gt;ANDROID_HOME&lt;/code&gt; to &lt;code&gt;/path/to/Android/Sdk&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set environment variable &lt;code&gt;ANDROID_NDK_HOME&lt;/code&gt; to &lt;code&gt;/path/to/Android/android-ndk-r21e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create your key following the instructions at &lt;a href=&#34;https://developer.android.com/studio/publish/app-signing.html&#34;&gt;https://developer.android.com/studio/publish/app-signing.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Put your key in ~/.keystore or any other place&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create &lt;code&gt;local.properties&lt;/code&gt; from &lt;code&gt;local.properties.example&lt;/code&gt; with your own key information&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  key.alias: abc&#xA;  key.store: /path/to/Android/abc.jks&#xA;  key.store.password: abc&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;if you installed multiple versions of Java, use &lt;code&gt;sudo update-alternatives --config java&lt;/code&gt; to select Java 8&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Before build apk, make sure inside &lt;code&gt;./project/build.properties&lt;/code&gt;, sbt.version=0.13.18&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Invoke the building like this&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    export https_proxy=http://127.0.0.1:8080 # !important&#xA;    export ANDROID_HOME=/path/to/Android/Sdk&#xA;    export ANDROID_NDK_HOME=/path/to/Android/android-ndk-r20b&#xA;    # install and update all git submodule&#xA;    git submodule update --init&#xA;    # cd ./src/main/jni/shadowsocks-libev &amp;amp;&amp;amp; git checkout Akkariiin/master&#xA;    # Build the App and fix the problems as the error messages indicated&#xA;    sbt native-build clean android:package-release&#xA;    # run app&#xA;    sbt android:run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;If you use x64 linux like Archlinux x86_64, or your linux have new version ncurses lib, you may need install the 32bit version ncurses and link it as follow (make sure all these *.so files in the right location under your system, otherwise you have to copy them to /usr/lib/ and /usr/lib32/ directory):&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    # use Archlinux x86_64 as example&#xA;    &#xA;    # install ncurses x64 and x86 version&#xA;    sudo pacman -S lib32-ncurses ncurses&#xA;    &#xA;    # link the version-6 ncurses to version-5&#xA;    sudo ln -s /usr/lib/libncursesw.so /usr/lib/libncurses.so.5&#xA;    sudo ln -s /usr/lib32/libncursesw.so /usr/lib32/libncurses.so.5&#xA;    &#xA;    # link libncurses to libtinfo&#xA;    sudo ln -s /usr/lib/libncurses.so.5 /usr/lib/libtinfo.so.5&#xA;    sudo ln -s /usr/lib32/libncurses.so.5 /usr/lib32/libtinfo.so.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Integration with Android Studio&lt;/h4&gt; &#xA;&lt;p&gt;Checkout this &lt;a href=&#34;http://srodrigo.me/setting-up-scala-on-android/&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;rm -rf ~/.android/sbt/exploded-aars/*&lt;/li&gt; &#xA; &lt;li&gt;In Project Settings -&amp;gt; Modules -&amp;gt; shadowsocksr-v2ray-trojan-android, change &lt;code&gt;Structure&lt;/code&gt;, &lt;code&gt;Generated Sources&lt;/code&gt; to correct file path&lt;/li&gt; &#xA; &lt;li&gt;In Run/Debug Configuration -&amp;gt; Before launch, replace &lt;code&gt;Gradle-aware Make&lt;/code&gt; with &lt;code&gt;android:run&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;BUILD on Mac OS X (with HomeBrew)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Android SDK and NDK by run &lt;code&gt;brew install android-ndk android-sdk&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;export ANDROID_HOME=/usr/local/Cellar/android-sdk/$version&lt;/code&gt; to your .bashrc , then reopen the shell to load it.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;export ANDROID_NDK_HOME=/usr/local/Cellar/android-ndk/$version&lt;/code&gt; to your .bashrc , then reopen the shell to load it.&lt;/li&gt; &#xA; &lt;li&gt;echo &#34;y&#34; | android update sdk --filter tools,platform-tools,build-tools-23.0.2,android-23,extra-google-m2repository --no-ui -a&lt;/li&gt; &#xA; &lt;li&gt;echo &#34;y&#34; | android update sdk --filter extra-android-m2repository --no-ui --no-https -a&lt;/li&gt; &#xA; &lt;li&gt;Create your key following the instructions at &lt;a href=&#34;http://developer.android.com/guide/publishing/app-signing.html#cert&#34;&gt;http://developer.android.com/guide/publishing/app-signing.html#cert&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Put your key in ~/.keystore&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;code&gt;local.properties&lt;/code&gt; from &lt;code&gt;local.properties.example&lt;/code&gt; with your own key information .&lt;/li&gt; &#xA; &lt;li&gt;Invoke the building like this&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    git submodule update --init&#xA;&#xA;    # Build native binaries&#xA;    ./build.sh&#xA;&#xA;    # Build the apk&#xA;    sbt clean android:package-release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;OPEN SOURCE LICENSES&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;shadowsocks-libev: &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-libev/raw/master/LICENSE&#34;&gt;GPLv3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;tun2socks: &lt;a href=&#34;https://github.com/shadowsocks/badvpn/raw/shadowsocks-android/COPYING&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;redsocks: &lt;a href=&#34;https://github.com/shadowsocks/redsocks/raw/master/README&#34;&gt;APL 2.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenSSL: &lt;a href=&#34;https://github.com/shadowsocks/openssl-android/raw/master/NOTICE&#34;&gt;OpenSSL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pdnsd: &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-android/raw/master/src/main/jni/pdnsd/COPYING&#34;&gt;GPLv3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;libev: &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-android/raw/master/src/main/jni/libev/LICENSE&#34;&gt;GPLv2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;libevent: &lt;a href=&#34;https://github.com/shadowsocks/libevent/raw/master/LICENSE&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;v2ray-core: &lt;a href=&#34;https://github.com/v2fly/v2ray-core/raw/master/LICENSE&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;go-tun2socks: &lt;a href=&#34;https://github.com/eycorsican/go-tun2socks/raw/master/LICENSE&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LICENSE&lt;/h3&gt; &#xA;&lt;p&gt;This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt; &#xA;&lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt; &#xA;&lt;p&gt;You should have received a copy of the GNU General Public License along with this program. If not, see &lt;a href=&#34;http://www.gnu.org/licenses/&#34;&gt;http://www.gnu.org/licenses/&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gatling/gatling</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/gatling/gatling</id>
    <link href="https://github.com/gatling/gatling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modern Load Testing as Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gatling &lt;a href=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/gatling/gatling/actions/workflows/build.yml/badge.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/io.gatling/gatling-core/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;What is Gatling ?&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is a load test tool. It officially supports HTTP, WebSocket, Server-Sent-Events and JMS.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finding fancy GUIs not that convenient for describing load tests, what you want is a friendly expressive DSL?&lt;/li&gt; &#xA; &lt;li&gt;Wanting something more convenient than huge XML dumps to store in your source version control system?&lt;/li&gt; &#xA; &lt;li&gt;Fed up with having to host a farm of injecting servers because your tool uses blocking IO and one-thread-per-user architecture?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Gatling is for you!&lt;/p&gt; &#xA;&lt;h2&gt;Underlying technologies&lt;/h2&gt; &#xA;&lt;p&gt;Gatling is developed in Scala and built upon :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netty.io&#34;&gt;Netty&lt;/a&gt; for non blocking HTTP&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://akka.io&#34;&gt;Akka&lt;/a&gt; for virtual users orchestration ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions, help?&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://gatling.io/docs/current/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Join the &lt;a href=&#34;https://groups.google.com/forum/#!forum/gatling&#34;&gt;Gatling User Group&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Found a real bug? Raise an &lt;a href=&#34;https://github.com/gatling/gatling/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Partners&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Takima&#34; src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/logo-takima-1-nom-bas.png&#34; width=&#34;80&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/gatling/gatling/main/images/highsoft_logo.png&#34; alt=&#34;Highsoft AS&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>polomarcus/tp</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/polomarcus/tp</id>
    <link href="https://github.com/polomarcus/tp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Practices - Data engineering&lt;/h1&gt; &#xA;&lt;h2&gt;Tools you need&lt;/h2&gt; &#xA;&lt;p&gt;Have a stackoverflow account : &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;https://stackoverflow.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have a github account : &lt;a href=&#34;https://github.com/&#34;&gt;https://github.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And a github repo to push your code.&lt;/p&gt; &#xA;&lt;h3&gt;Fork the repo on your own Github account&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/tp/fork&#34;&gt;https://github.com/polomarcus/tp/fork&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker and Compose&lt;/h3&gt; &#xA;&lt;p&gt;Take time to read and install&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/get-started/overview/&#34;&gt;https://docs.docker.com/get-started/overview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker --version&#xA;Docker version 20.10.14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;https://docs.docker.com/compose/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose --version&#xA;docker-compose version 1.29.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TP1 - &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-docker-kafka-bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP2 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-functional-programming-scala&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP3 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-data-processing-framework&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP 4 - Kafka Streams to read and write to Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/streams/&#34;&gt;https://kafka.apache.org/documentation/streams/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&#34;&gt;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kafka User Interface (UI) : &lt;a href=&#34;https://www.conduktor.io/download/&#34;&gt;https://www.conduktor.io/download/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>awslabs/deequ</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/awslabs/deequ</id>
    <link href="https://github.com/awslabs/deequ" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/awslabs/deequ/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://travis-ci.com/awslabs/deequ&#34;&gt;&lt;img src=&#34;https://travis-ci.com/awslabs/deequ.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &#34;unit tests for data&#34;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&#34;&gt;contributions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&#34;https://github.com/awslabs/python-deequ&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://pydeequ.readthedocs.io/en/latest/README.html&#34;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&#34;https://pypi.org/project/pydeequ/&#34;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; &#xA;&lt;p&gt;Available via &lt;a href=&#34;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&#34;&gt;maven central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&#34;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&#34;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;com.amazon.deequ&#34; % &#34;deequ&#34; % &#34;2.0.0-spark-3.1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &#34;unit-test&#34; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Item(&#xA;  id: Long,&#xA;  productName: String,&#xA;  description: String,&#xA;  priority: String,&#xA;  numViews: Long&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our library is built on &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = spark.sparkContext.parallelize(Seq(&#xA;  Item(1, &#34;Thingy A&#34;, &#34;awesome thing.&#34;, &#34;high&#34;, 0),&#xA;  Item(2, &#34;Thingy B&#34;, &#34;available at http://thingb.com&#34;, null, 0),&#xA;  Item(3, null, null, &#34;low&#34;, 5),&#xA;  Item(4, &#34;Thingy D&#34;, &#34;checkout https://thingd.ca&#34;, &#34;low&#34;, 10),&#xA;  Item(5, &#34;Thingy E&#34;, null, &#34;high&#34;, 12)))&#xA;&#xA;val data = spark.createDataFrame(rdd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &#34;unit-test&#34; for data, which can be verified on a piece of data at hand. If the data has errors, we can &#34;quarantine&#34; and fix it, before we feed it to an application.&lt;/p&gt; &#xA;&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&#34;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&#34;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;there are 5 rows in total&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; &#xA; &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &#34;high&#34; or &#34;low&#34; as value&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; &#xA; &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; &#xA; &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In code this looks as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.VerificationSuite&#xA;import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}&#xA;&#xA;&#xA;val verificationResult = VerificationSuite()&#xA;  .onData(data)&#xA;  .addCheck(&#xA;    Check(CheckLevel.Error, &#34;unit testing my data&#34;)&#xA;      .hasSize(_ == 5) // we expect 5 rows&#xA;      .isComplete(&#34;id&#34;) // should never be NULL&#xA;      .isUnique(&#34;id&#34;) // should not contain duplicates&#xA;      .isComplete(&#34;productName&#34;) // should never be NULL&#xA;      // should only contain the values &#34;high&#34; and &#34;low&#34;&#xA;      .isContainedIn(&#34;priority&#34;, Array(&#34;high&#34;, &#34;low&#34;))&#xA;      .isNonNegative(&#34;numViews&#34;) // should not contain negative values&#xA;      // at least half of the descriptions should contain a url&#xA;      .containsURL(&#34;description&#34;, _ &amp;gt;= 0.5)&#xA;      // half of the items should have less than 10 views&#xA;      .hasApproxQuantile(&#34;numViews&#34;, 0.5, _ &amp;lt;= 10))&#xA;    .run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&#34;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.amazon.deequ.constraints.ConstraintStatus&#xA;&#xA;&#xA;if (verificationResult.status == CheckStatus.Success) {&#xA;  println(&#34;The data passed the test, everything is fine!&#34;)&#xA;} else {&#xA;  println(&#34;We found errors in the data:\n&#34;)&#xA;&#xA;  val resultsForAllConstraints = verificationResult.checkResults&#xA;    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }&#xA;&#xA;  resultsForAllConstraints&#xA;    .filter { _.status != ConstraintStatus.Success }&#xA;    .foreach { result =&amp;gt; println(s&#34;${result.constraint}: ${result.message.get}&#34;) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;We found errors in the data:&#xA;&#xA;CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!&#xA;PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&#34;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&#34;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&#34;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&#34;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&#34;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&#34;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&#34;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&#34;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>snowplow/snowplow</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/snowplow/snowplow</id>
    <link href="https://github.com/snowplow/snowplow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The enterprise-grade behavioral data engine (web, mobile, server-side, webhooks), running cloud-natively on AWS and GCP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Snowplow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/releases/tag/22.01&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Snowplow-22.01%20Western%20Ghats-6638b8&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2-blue.svg?style=flat&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/discourse/posts?server=https%3A%2F%2Fdiscourse.snowplowanalytics.com%2F&#34; alt=&#34;Discourse posts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://snowplowanalytics.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/media/snowplow_logo.png&#34; alt=&#34;Snowplow logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Snowplow is an enterprise-strength marketing and product analytics platform. It does three things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Identifies your users, and tracks the way they engage with your website or application&lt;/li&gt; &#xA; &lt;li&gt;Stores your users&#39; behavioral data in a scalable &#34;event data warehouse&#34; you control: Amazon Redshift, Google BigQuery, Snowflake or Elasticsearch&lt;/li&gt; &#xA; &lt;li&gt;Lets you leverage the biggest range of tools to analyze that data, including big data tools (e.g. Spark) via EMR or more traditional tools e.g. Looker, Mode, Superset, Re:dash to analyze that behavioral data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;To find out more, please check out the &lt;a href=&#34;https://snowplowanalytics.com&#34;&gt;Snowplow website&lt;/a&gt; and the &lt;a href=&#34;https://docs.snowplowanalytics.com/open-source-docs/&#34;&gt;docs website&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Version Compatibility Matrix&lt;/h3&gt; &#xA;&lt;p&gt;For compatibility assurance, the version compatibility matrix offers clarity on our recommended stack. It is strongly recommended when setting up a Snowplow pipeline to use the versions listed in the version compatibility matrix which can be found &lt;a href=&#34;https://docs.snowplowanalytics.com/docs/pipeline-components-and-applications/version-compatibility-matrix/&#34;&gt;within our docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Public Roadmap&lt;/h3&gt; &#xA;&lt;p&gt;This repository also contains the &lt;a href=&#34;https://github.com/snowplow/snowplow/projects&#34;&gt;Snowplow Public Roadmap&lt;/a&gt;. The Public Roadmap lets you stay up to date and find out what&#39;s happening on the Snowplow Platform. Help us prioritize our cards: open the issue and leave a üëç to vote for your favorites. Want us to build a feature or function? Tell us by heading to our &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse forum&lt;/a&gt; üí¨.&lt;/p&gt; &#xA;&lt;h3&gt;Try Snowplow&lt;/h3&gt; &#xA;&lt;p&gt;Setting up a full open-source Snowplow pipeline requires a non-trivial amount of engineering expertise and time investment. You might be interested in finding out what Snowplow can do first, by setting up &lt;a href=&#34;https://try.snowplowanalytics.com/?utm_source=github&amp;amp;utm_medium=post&amp;amp;utm_campaign=try-snowplow&#34;&gt;Try Snowplow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Open Source Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://docs.snowplowanalytics.com/docs/open-source-quick-start/&#34;&gt;Open Source Quick Start&lt;/a&gt; will help you get up and running with a Snowplow open source pipeline. Snowplow publishes a &lt;a href=&#34;https://registry.terraform.io/modules/snowplow-devops&#34;&gt;set of terraform modules&lt;/a&gt;, which automate the setting up &amp;amp; deployment of the required infrastructure &amp;amp; applications for an operational Snowplow open source pipeline, with just a handful of input variables required on your side.&lt;/p&gt; &#xA;&lt;h3&gt;Join the Snowplow Research Panel and help shape the future of open source&lt;/h3&gt; &#xA;&lt;p&gt;As part of our ongoing efforts to improve the Snowplow Open Source experience, we&#39;re looking for users of our open-source software and members of our community to take part in research studies. &lt;a href=&#34;https://forms.gle/pCtYx8naum7A8vvw5&#34;&gt;Join here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Our Commercial Offering&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to get everything setup and managed for you, you can consider &lt;a href=&#34;https://snowplowanalytics.com/products/snowplow-bdp/&#34;&gt;Snowplow BDP&lt;/a&gt;. You can also &lt;a href=&#34;https://go.snowplowanalytics.com/l/571483/2021-05-04/3sv1pg8&#34;&gt;request a demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Snowplow technology 101&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/ARCHITECTURE.md&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/media/snowplow_architecture.png&#34; alt=&#34;Snowplow architecture&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The repository structure follows the conceptual architecture of Snowplow, which consists of six loosely-coupled sub-systems connected by five standardized data protocols/formats.&lt;/p&gt; &#xA;&lt;p&gt;To briefly explain these six sub-systems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/1-trackers&#34;&gt;Trackers&lt;/a&gt;&lt;/strong&gt; fire Snowplow events. Currently we have 15 trackers, covering web, mobile, desktop, server and IoT&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/2-collectors&#34;&gt;Collector&lt;/a&gt;&lt;/strong&gt; receives Snowplow events from trackers. Currently we have one official collector implementation with different sinks: Amazon Kinesis, Google PubSub, Amazon SQS, Apache Kafka and NSQ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/3-enrich&#34;&gt;Enrich&lt;/a&gt;&lt;/strong&gt; cleans up the raw Snowplow events, enriches them and puts them into storage. Currently we have several implementations, built for different environments (GCP, AWS, Apache Kafka) and one core library&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/4-storage&#34;&gt;Storage&lt;/a&gt;&lt;/strong&gt; is where the Snowplow events live. Currently we store the Snowplow events in a flat file structure on S3, and in the Redshift, Postgres, Snowflake and BigQuery databases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/5-data-modeling&#34;&gt;Data modeling&lt;/a&gt;&lt;/strong&gt; is where event-level data is joined with other data sets and aggregated into smaller data sets, and business logic is applied. This produces a clean set of tables which make it easier to perform analysis on the data. We officially support data models for Redshift, Snowflake and BigQuery.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.snowplowanalytics.com/docs/modeling-your-data/analytics-sdk/&#34;&gt;Analytics&lt;/a&gt;&lt;/strong&gt; are performed on the Snowplow events or on the aggregate tables.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more information on the current Snowplow architecture, please see the &lt;a href=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/ARCHITECTURE.md&#34;&gt;Technical architecture&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About this repository&lt;/h2&gt; &#xA;&lt;p&gt;This repository is an umbrella repository for all loosely-coupled Snowplow components and is updated on each component release.&lt;/p&gt; &#xA;&lt;p&gt;Since June 2020, all components have been extracted into their dedicated repositories (more info &lt;a href=&#34;https://snowplowanalytics.com/blog/2020/07/16/changing-releasing/&#34;&gt;here&lt;/a&gt;) and this repository serves as an entry point for Snowplow users, the home of our public roadmap and as a historical artifact.&lt;/p&gt; &#xA;&lt;p&gt;Components that have been extracted to their own repository are still here as &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;git submodules&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Trackers&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Web&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mobile&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Gaming&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TV&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Desktop &amp;amp; Server&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-javascript-tracker&#34;&gt;JavaScript&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-android-tracker&#34;&gt;Android&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-unity-tracker&#34;&gt;Unity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-roku-tracker&#34;&gt;Roku&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-tracking-cli&#34;&gt;Command line&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/google-amp-tracker/&#34;&gt;AMP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-objc-tracker&#34;&gt;iOS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-dotnet-tracker&#34;&gt;.NET&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-react-native-tracker&#34;&gt;React Native&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-golang-tracker&#34;&gt;Go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-flutter-tracker&#34;&gt;Flutter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-java-tracker&#34;&gt;Java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-javascript-tracker&#34;&gt;Node.js&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-php-tracker&#34;&gt;PHP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-python-tracker&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-ruby-tracker&#34;&gt;Ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-scala-tracker&#34;&gt;Scala&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow/stream-collector&#34;&gt;Collector&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow/enrich&#34;&gt;Enrich&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Loaders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-bigquery-loader&#34;&gt;BigQuery (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-rdb-loader&#34;&gt;Redshift (batch)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-snowflake-loader&#34;&gt;Snowflake (batch)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-google-cloud-storage-loader&#34;&gt;Google Cloud Storage (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-s3-loader&#34;&gt;Amazon S3 (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-postgres-loader&#34;&gt;Postgres (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-elasticsearch-loader&#34;&gt;Elasticsearch (streaming)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Iglu&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/iglu-server/&#34;&gt;Iglu Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/igluctl/&#34;&gt;igluctl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/iglu-central/&#34;&gt;Iglu Central&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data modeling&lt;/h3&gt; &#xA;&lt;h4&gt;Web&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/data-models/tree/master/web/v1&#34;&gt;Web model: SQL-Runner version&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/dbt-snowplow-web&#34;&gt;Web model: dbt version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Mobile&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/data-models/tree/master/mobile/v1&#34;&gt;Mobile model: SQL-Runner version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-mini&#34;&gt;Mini&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-micro&#34;&gt;Micro&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Parsing enriched event&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-scala-analytics-sdk&#34;&gt;Analytics SDK Scala&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-python-analytics-sdk&#34;&gt;Analytics SDK Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-dotnet-analytics-sdk&#34;&gt;Analytics SDK .NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-js-analytics-sdk/&#34;&gt;Analytics SDK Javascript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-golang-analytics-sdk&#34;&gt;Analytics SDK Golang&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-badrows&#34;&gt;Bad rows&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://registry.terraform.io/modules/snowplow-devops&#34;&gt;Terraform Modules&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Need help?&lt;/h2&gt; &#xA;&lt;p&gt;We want to make it super-easy for Snowplow users and contributors to talk to us and connect with each other, to share ideas, solve problems and help make Snowplow awesome. Here are the main channels we&#39;re running currently, we&#39;d love to hear from you on one of them:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is for all Snowplow users: engineers setting up Snowplow, data modelers structuring the data and data consumers building insights. You can find guides, recipes, questions and answers from Snowplow users including the Snowplow team.&lt;/p&gt; &#xA;&lt;p&gt;We welcome all questions and contributions!&lt;/p&gt; &#xA;&lt;h3&gt;Twitter&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/SnowplowData&#34;&gt;@SnowplowData&lt;/a&gt; for official news or &lt;a href=&#34;https://twitter.com/SnowplowLabs&#34;&gt;@SnowplowLabs&lt;/a&gt; for engineering-heavy conversations and release updates.&lt;/p&gt; &#xA;&lt;h3&gt;GitHub&lt;/h3&gt; &#xA;&lt;p&gt;If you spot a bug, then please raise an issue in the GitHub repository of the component in question. Likewise if you have developed a cool new feature or an improvement, please open a pull request, we&#39;ll be glad to integrate it in the codebase!&lt;/p&gt; &#xA;&lt;p&gt;If you want to brainstorm a potential new feature, then &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse&lt;/a&gt; is the best place to start.&lt;/p&gt; &#xA;&lt;h3&gt;Email&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:community@snowplowanalytics.com&#34;&gt;community@snowplowanalytics.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to talk directly to us (e.g. about a commercially sensitive issue), email is the easiest way.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and license&lt;/h2&gt; &#xA;&lt;p&gt;Snowplow is copyright 2012-2022 Snowplow Analytics Ltd.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the &lt;strong&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/strong&gt; (the &#34;License&#34;); you may not use this software except in compliance with the License.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lampepfl/dotty</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/lampepfl/dotty</id>
    <link href="https://github.com/lampepfl/dotty" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/actions?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/lampepfl/dotty/workflows/Dotty/badge.svg?branch=master&#34; alt=&#34;Dotty CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/scala&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/632150470000902164&#34; alt=&#34;Join the chat at https://discord.com/invite/scala&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Try it out&lt;/h1&gt; &#xA;&lt;p&gt;To try it in your project see also the &lt;a href=&#34;https://docs.scala-lang.org/scala3/getting-started.html&#34;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt dist/packArchive&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;Dotty uses the &lt;a href=&#34;https://www.scala-lang.org/conduct.html&#34;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; &#xA;&lt;h1&gt;How to Contribute&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&#34;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lampepfl/dotty/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Dotty is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-sql-perf</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/databricks/spark-sql-perf</id>
    <link href="https://github.com/databricks/spark-sql-perf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark SQL Performance Tests&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-sql-perf&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-sql-perf.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a performance testing framework for &lt;a href=&#34;https://spark.apache.org/sql/&#34;&gt;Spark SQL&lt;/a&gt; in &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; 2.2+.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This README is still under development. Please also check our source code for more information.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Running from command line.&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/run --help&#xA;&#xA;spark-sql-perf 0.2.0&#xA;Usage: spark-sql-perf [options]&#xA;&#xA;  -b &amp;lt;value&amp;gt; | --benchmark &amp;lt;value&amp;gt;&#xA;        the name of the benchmark to run&#xA;  -m &amp;lt;value&amp;gt; | --master &amp;lt;value&#xA;        the master url to use&#xA;  -f &amp;lt;value&amp;gt; | --filter &amp;lt;value&amp;gt;&#xA;        a filter on the name of the queries to run&#xA;  -i &amp;lt;value&amp;gt; | --iterations &amp;lt;value&amp;gt;&#xA;        the number of iterations to run&#xA;  --help&#xA;        prints this usage text&#xA;        &#xA;$ bin/run --benchmark DatasetPerformance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first run of &lt;code&gt;bin/run&lt;/code&gt; will build the library.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;sbt package&lt;/code&gt; or &lt;code&gt;sbt assembly&lt;/code&gt; to build the library jar.&lt;br&gt; Use &lt;code&gt;sbt +package&lt;/code&gt; to build for scala 2.11 and 2.12.&lt;/p&gt; &#xA;&lt;h2&gt;Local performance tests&lt;/h2&gt; &#xA;&lt;p&gt;The framework contains twelve benchmarks that can be executed in local mode. They are organized into three classes and target different components and functions of Spark:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/DatasetPerformance.scala&#34;&gt;DatasetPerformance&lt;/a&gt; compares the performance of the old RDD API with the new Dataframe and Dataset APIs. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark DatasetPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/JoinPerformance.scala&#34;&gt;JoinPerformance&lt;/a&gt; compares the performance of joining different table sizes and shapes with different join types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark JoinPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/AggregationPerformance.scala&#34;&gt;AggregationPerformance&lt;/a&gt; compares the performance of aggregating different table sizes using different aggregation types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark AggregationPerformance&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;MLlib tests&lt;/h1&gt; &#xA;&lt;p&gt;To run MLlib tests, run &lt;code&gt;/bin/run-ml yamlfile&lt;/code&gt;, where &lt;code&gt;yamlfile&lt;/code&gt; is the path to a YAML configuration file describing tests to run and their parameters.&lt;/p&gt; &#xA;&lt;h1&gt;TPC-DS&lt;/h1&gt; &#xA;&lt;h2&gt;Setup a benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Before running any query, a dataset needs to be setup by creating a &lt;code&gt;Benchmark&lt;/code&gt; object. Generating the TPCDS data requires dsdgen built and available on the machines. We have a fork of dsdgen that you will need. The fork includes changes to generate TPCDS data to stdout, so that this library can pipe them directly to Spark, without intermediate files. Therefore, this library will not work with the vanilla TPCDS kit.&lt;/p&gt; &#xA;&lt;p&gt;TPCDS kit needs to be installed on all cluster executor nodes under the same path!&lt;/p&gt; &#xA;&lt;p&gt;It can be found &lt;a href=&#34;https://github.com/databricks/tpcds-kit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Generate the data&#xA;build/sbt &#34;test:runMain com.databricks.spark.sql.perf.tpcds.GenTPCDSData -d &amp;lt;dsdgenDir&amp;gt; -s &amp;lt;scaleFactor&amp;gt; -l &amp;lt;location&amp;gt; -f &amp;lt;format&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Create the specified database&#xA;sql(s&#34;create database $databaseName&#34;)&#xA;// Create metastore tables in a specified database for your data.&#xA;// Once tables are created, the current database will be switched to the specified database.&#xA;tables.createExternalTables(rootDir, &#34;parquet&#34;, databaseName, overwrite = true, discoverPartitions = true)&#xA;// Or, if you want to create temporary tables&#xA;// tables.createTemporaryTables(location, format)&#xA;&#xA;// For CBO only, gather statistics on all columns:&#xA;tables.analyzeTables(databaseName, analyzeColumns = true) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run benchmarking queries&lt;/h2&gt; &#xA;&lt;p&gt;After setup, users can use &lt;code&gt;runExperiment&lt;/code&gt; function to run benchmarking queries and record query execution time. Taking TPC-DS as an example, you can start an experiment by using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import com.databricks.spark.sql.perf.tpcds.TPCDS&#xA;&#xA;val tpcds = new TPCDS (sqlContext = sqlContext)&#xA;// Set:&#xA;val databaseName = ... // name of database with TPCDS data.&#xA;val resultLocation = ... // place to write results&#xA;val iterations = 1 // how many iterations of queries to run.&#xA;val queries = tpcds.tpcds2_4Queries // queries to run.&#xA;val timeout = 24*60*60 // timeout, in seconds.&#xA;// Run:&#xA;sql(s&#34;use $databaseName&#34;)&#xA;val experiment = tpcds.runExperiment(&#xA;  queries, &#xA;  iterations = iterations,&#xA;  resultLocation = resultLocation,&#xA;  forkThread = true)&#xA;experiment.waitForFinish(timeout)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, experiment will be started in a background thread. For every experiment run (i.e. every call of &lt;code&gt;runExperiment&lt;/code&gt;), Spark SQL Perf will use the timestamp of the start time to identify this experiment. Performance results will be stored in the sub-dir named by the timestamp in the given &lt;code&gt;spark.sql.perf.results&lt;/code&gt; (for example &lt;code&gt;/tmp/results/timestamp=1429213883272&lt;/code&gt;). The performance results are stored in the JSON format.&lt;/p&gt; &#xA;&lt;h2&gt;Retrieve results&lt;/h2&gt; &#xA;&lt;p&gt;While the experiment is running you can use &lt;code&gt;experiment.html&lt;/code&gt; to get a summary, or &lt;code&gt;experiment.getCurrentResults&lt;/code&gt; to get complete current results. Once the experiment is complete, you can still access &lt;code&gt;experiment.getCurrentResults&lt;/code&gt;, or you can load the results from disk.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Get all experiments results.&#xA;val resultTable = spark.read.json(resultLocation)&#xA;resultTable.createOrReplaceTempView(&#34;sqlPerformance&#34;)&#xA;sqlContext.table(&#34;sqlPerformance&#34;)&#xA;// Get the result of a particular run by specifying the timestamp of that run.&#xA;sqlContext.table(&#34;sqlPerformance&#34;).filter(&#34;timestamp = 1429132621024&#34;)&#xA;// or&#xA;val specificResultTable = spark.read.json(experiment.resultPath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get a basic summary by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;experiment.getCurrentResults // or: spark.read.json(resultLocation).filter(&#34;timestamp = 1429132621024&#34;)&#xA;  .withColumn(&#34;Name&#34;, substring(col(&#34;name&#34;), 2, 100))&#xA;  .withColumn(&#34;Runtime&#34;, (col(&#34;parsingTime&#34;) + col(&#34;analysisTime&#34;) + col(&#34;optimizationTime&#34;) + col(&#34;planningTime&#34;) + col(&#34;executionTime&#34;)) / 1000.0)&#xA;  .select(&#39;Name, &#39;Runtime)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;TPC-H&lt;/h1&gt; &#xA;&lt;p&gt;TPC-H can be run similarly to TPC-DS replacing &lt;code&gt;tpcds&lt;/code&gt; for &lt;code&gt;tpch&lt;/code&gt;. Take a look at the data generator and &lt;code&gt;tpch_run&lt;/code&gt; notebook code below.&lt;/p&gt; &#xA;&lt;h2&gt;Running in Databricks workspace (or spark-shell)&lt;/h2&gt; &#xA;&lt;p&gt;There are example notebooks in &lt;code&gt;src/main/notebooks&lt;/code&gt; for running TPCDS and TPCH in the Databricks environment. &lt;em&gt;These scripts can also be run from spark-shell command line with minor modifications using &lt;code&gt;:load file_name.scala&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TPC-multi_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook (or scala script) can be use to generate both TPCDS and TPCH data at selected scale factors. It is a newer version from the &lt;code&gt;tpcds_datagen&lt;/code&gt; notebook below. To use it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edit the config variables the top of the script.&lt;/li&gt; &#xA; &lt;li&gt;Run the whole notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpcds_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to install dsdgen on all worker nodes, run data generation, and create the TPCDS database. Note that because of the way dsdgen is installed, it will not work on an autoscaling cluster, and &lt;code&gt;num_workers&lt;/code&gt; has to be updated to the number of worker instances on the cluster. Data generation may also break if any of the workers is killed - the restarted worker container will not have &lt;code&gt;dsdgen&lt;/code&gt; anymore.&lt;/p&gt; &#xA;&lt;h3&gt;tpcds_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCDS queries.&lt;/p&gt; &#xA;&lt;p&gt;For running parallel TPCDS streams:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Cluster and attach the spark-sql-perf library to it.&lt;/li&gt; &#xA; &lt;li&gt;Create a Job using the notebook and attaching to the created cluster as &#34;existing cluster&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Allow concurrent runs of the created job.&lt;/li&gt; &#xA; &lt;li&gt;Launch appriopriate number of Runs of the Job to run in parallel on the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpch_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCH queries. Data needs be generated first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-csv</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/databricks/spark-csv</id>
    <link href="https://github.com/databricks/spark-csv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CSV Data Source for Apache Spark 1.x&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CSV Data Source for Apache Spark 1.x&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE: This functionality has been inlined in Apache Spark 2.x. This package is in maintenance mode and we only accept critical bug fixes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A library for parsing and querying CSV data with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-csv&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-csv.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://codecov.io/github/databricks/spark-csv?branch=master&#34;&gt;&lt;img src=&#34;http://codecov.io/github/databricks/spark-csv/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 1.3+&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.10&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.11&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This package allows reading CSV files in local or distributed filesystem as &lt;a href=&#34;https://spark.apache.org/docs/1.6.0/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;. When reading files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files. Similar to Spark can accept standard Hadoop globbing expressions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true the first line of files will be used to name columns and will not be included in data. All types will be assumed string. Default value is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. Delimiters inside quotes are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;parserLib&lt;/code&gt;: by default it is &#34;commons&#34; can be set to &#34;univocity&#34; to use that library for CSV parsing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: determines the parsing mode. By default it is PERMISSIVE. Possible values are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;PERMISSIVE&lt;/code&gt;: tries to parse all lines: nulls are inserted for missing tokens and extra tokens are ignored.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;DROPMALFORMED&lt;/code&gt;: drops lines which have fewer or more tokens than expected or tokens which do not match the schema&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;FAILFAST&lt;/code&gt;: aborts with a RuntimeException if encounters any malformed line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;charset&lt;/code&gt;: defaults to &#39;UTF-8&#39; but can be set to other valid charset names&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inferSchema&lt;/code&gt;: automatically infers column types. It requires one extra pass over the data and is false by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;comment&lt;/code&gt;: skip lines beginning with this character. Default is &lt;code&gt;&#34;#&#34;&lt;/code&gt;. Disable comments by setting this to &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, any fields matching this string will be set as nulls in the DataFrame&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use when reading dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. By default, it is &lt;code&gt;null&lt;/code&gt; which means trying to parse times and date by &lt;code&gt;java.sql.Timestamp.valueOf()&lt;/code&gt; and &lt;code&gt;java.sql.Date.valueOf()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The package also supports saving simple (non-nested) DataFrame. When writing files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true, the header (from the schema in the DataFrame) will be written at the first line.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. This is written according to &lt;code&gt;quoteMode&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are written.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, nulls in the DataFrame will be written as this string.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use writing dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. If no dateFormat is specified, then &#34;yyyy-MM-dd HH:mm:ss.S&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;codec&lt;/code&gt;: compression codec to use when saving to file. Should be the fully qualified name of a class implementing &lt;code&gt;org.apache.hadoop.io.compress.CompressionCodec&lt;/code&gt; or one of case-insensitive shorten names (&lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, and &lt;code&gt;snappy&lt;/code&gt;). Defaults to no compression when a codec is not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quoteMode&lt;/code&gt;: when to quote fields (&lt;code&gt;ALL&lt;/code&gt;, &lt;code&gt;MINIMAL&lt;/code&gt; (default), &lt;code&gt;NON_NUMERIC&lt;/code&gt;, &lt;code&gt;NONE&lt;/code&gt;), see &lt;a href=&#34;https://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/QuoteMode.html&#34;&gt;Quote Modes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These examples use a CSV file available for download &lt;a href=&#34;https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL API&lt;/h3&gt; &#xA;&lt;p&gt;CSV data source for Spark can infer data types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;, inferSchema &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify column names and types in DDL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars (yearMade double, carMake string, carModel string, comments string, blank string)&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .schema(customSchema)&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv.gz&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;, &#34;inferSchema&#34; -&amp;gt; &#34;true&#34;))&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    schema = customSchema,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;))&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .schema(customSchema)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, customSchema, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.SaveMode;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; saveOptions = new HashMap&amp;lt;String, String&amp;gt;();&#xA;saveOptions.put(&#34;header&#34;, &#34;true&#34;);&#xA;saveOptions.put(&#34;path&#34;, &#34;newcars.csv&#34;);&#xA;saveOptions.put(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;com.databricks.spark.csv&#34;, SaveMode.Overwrite,&#xA;                                saveOptions);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.read \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .options(header=&#39;true&#39;) \&#xA;    .load(&#39;cars.csv&#39;, schema = customSchema)&#xA;&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).options(codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, schema = customSchema, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;customSchema &amp;lt;- structType(&#xA;    structField(&#34;year&#34;, &#34;integer&#34;),&#xA;    structField(&#34;make&#34;, &#34;string&#34;),&#xA;    structField(&#34;model&#34;, &#34;string&#34;),&#xA;    structField(&#34;comment&#34;, &#34;string&#34;),&#xA;    structField(&#34;blank&#34;, &#34;string&#34;))&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, schema = customSchema)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;, which is automatically downloaded by the included shell script. To build a JAR file simply run &lt;code&gt;sbt/sbt package&lt;/code&gt; from the project root. The build configuration includes support for both Scala 2.10 and 2.11.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/sjsonnet</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/databricks/sjsonnet</id>
    <link href="https://github.com/databricks/sjsonnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sjsonnet&lt;/h1&gt; &#xA;&lt;p&gt;A JVM implementation of the &lt;a href=&#34;https://jsonnet.org/&#34;&gt;Jsonnet&lt;/a&gt; configuration language.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet can be used from Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.databricks&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sjsonnet_2.13&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.4.2&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    new String[]{&#34;foo.jsonnet&#34;},&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.package$.MODULE$.pwd(),&#xA;    scala.None$.empty()&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&#34;com.databricks&#34; %% &#34;sjsonnet&#34; % &#34;0.4.2&#34; // SBT&#xA;ivy&#34;com.databricks::sjsonnet:0.4.2&#34; // Mill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    Array(&#34;foo.jsonnet&#34;),&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.pwd, // working directory&#xA;    None&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a standalone executable assembly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&#34;&gt;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar &amp;gt; sjsonnet.jar&#xA;&#xA;$ chmod +x sjsonnet.jar&#xA;&#xA;$ ./sjsonnet.jar&#xA;error: Need to pass in a jsonnet file to evaluate&#xA;usage: sjsonnet [sjsonnet-options] script-file&#xA;&#xA;  -i, --interactive  Run Mill in interactive mode, suitable for opening REPLs and taking user input&#xA;  -n, --indent       How much to indent your output JSON&#xA;  -J, --jpath        Specify an additional library search dir (right-most wins)&#xA;  -o, --output-file  Write to the output file rather than stdout&#xA;  ...&#xA;&#xA;$ ./sjsonnet.jar foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or from Javascript:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.js &amp;gt; sjsonnet.js&#xA;&#xA;$ node&#xA;&#xA;&amp;gt; require(&#34;./sjsonnet.js&#34;)&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#34;local f = function(x) x * x; f(11)&#34;, {}, {}, &#34;&#34;, (wd, imported) =&amp;gt; null)&#xA;121&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#xA;    &#34;local f = import &#39;foo&#39;; f + &#39;bar&#39;&#34;, // code&#xA;    {}, // extVars&#xA;    {}, // tlaVars&#xA;    &#34;&#34;, // initial working directory&#xA;&#xA;    // import callback: receives a base directory and the imported path string,&#xA;    // returns a tuple of the resolved file path and file contents or file contents resolve method&#xA;    (wd, imported) =&amp;gt; [wd + &#34;/&#34; + imported, &#34;local bar = 123; bar + bar&#34;],&#xA;    // loader callback: receives the tuple from the import callback and returns the file contents&#xA;    ([path, content]) =&amp;gt; content&#xA;    )&#xA;&#39;246bar&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that since Javascript does not necessarily have access to the filesystem, you have to provide an explicit import callback that you can use to resolve imports yourself (whether through Node&#39;s &lt;code&gt;fs&lt;/code&gt; module, or by emulating a filesystem in-memory)&lt;/p&gt; &#xA;&lt;h3&gt;Running deeply recursive Jsonnet programs&lt;/h3&gt; &#xA;&lt;p&gt;The depth of recursion is limited by JVM stack size. You can run Sjsonnet with increased stack size as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java -Xss100m -cp sjsonnet.jar sjsonnet.SjsonnetMain foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The -Xss option above is responsible for JVM stack size. Please try this if you ever run into &lt;code&gt;sjsonnet.Error: Internal Error ... Caused by: java.lang.StackOverflowError ...&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is no analog of &lt;code&gt;--max-stack&lt;/code&gt;/&lt;code&gt;-s&lt;/code&gt; option of &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;. The only stack size limit is the one of the JVM.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet is implementated as an optimizing interpreter. There are roughly 4 phases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Parser&lt;/code&gt;: parses an input &lt;code&gt;String&lt;/code&gt; into a &lt;code&gt;sjsonnet.Expr&lt;/code&gt;, which is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_syntax_tree&#34;&gt;Syntax Tree&lt;/a&gt; representing the Jsonnet document syntax, using the &lt;a href=&#34;https://github.com/lihaoyi/fastparse&#34;&gt;Fastparse&lt;/a&gt; parsing library&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.StaticOptimizer&lt;/code&gt; is a single AST transform that performs static checking, essential rewriting (e.g. assigning indices in the symbol table for variables) and optimizations. The result is another &lt;code&gt;sjsonnet.Expr&lt;/code&gt; per input file that can be stored in the parse cache and reused.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Evaluator&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Expr&lt;/code&gt; produced by the optimizer and converts it into a &lt;code&gt;sjsonnet.Val&lt;/code&gt;, a data structure representing the Jsonnet runtime values (basically lazy JSON which can contain function values).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Materializer&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Val&lt;/code&gt; and converts it into an output &lt;code&gt;ujson.Expr&lt;/code&gt;: a non-lazy JSON structure without any remaining un-evaluated function values. This can be serialized to a string formatted in a variety of ways&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These three phases are encapsulated in the &lt;code&gt;sjsonnet.Interpreter&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;Some notes on the values used in parts of the pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Expr&lt;/code&gt;: this represents &lt;code&gt;{...}&lt;/code&gt; object literal nodes, &lt;code&gt;a + b&lt;/code&gt; binary operation nodes, &lt;code&gt;function(a) {...}&lt;/code&gt; definitions and &lt;code&gt;f(a)&lt;/code&gt; invocations, etc.. Also keeps track of source-offset information so failures can be correlated with line numbers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Val&lt;/code&gt;: essentially the JSON structure (objects, arrays, primitives) but with two modifications. The first is that functions like &lt;code&gt;function(a){...}&lt;/code&gt; can still be present in the structure: in Jsonnet you can pass around functions as values and call then later on. The second is that object values &amp;amp; array entries are &lt;em&gt;lazy&lt;/em&gt;: e.g. &lt;code&gt;[error 123, 456][1]&lt;/code&gt; does not raise an error because the first (erroneous) entry of the array is un-used and thus not evaluated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classes representing literals extend &lt;code&gt;sjsonnet.Val.Literal&lt;/code&gt; which in turn extends &lt;em&gt;both&lt;/em&gt;, &lt;code&gt;Expr&lt;/code&gt; and &lt;code&gt;Val&lt;/code&gt;. This allows the evaluator to skip over them instead of having to convert them from one representation to the other.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Due to pervasive caching, sjsonnet is much faster than google/jsonnet. See this blog post for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://databricks.com/blog/2018/10/12/writing-a-faster-jsonnet-compiler.html&#34;&gt;Writing a Faster Jsonnet Compiler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s the latest set of benchmarks I&#39;ve run comparing Sjsonnet against google/jsonnet and google/go-jsonnet, measuring the time taken to&lt;br&gt; evaluate the &lt;code&gt;test_suite/&lt;/code&gt; folder (smaller is better):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.5&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.6&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.26ms ¬± 0.22&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.59ms ¬± 0.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.07ms ¬± 0.30&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.29ms ¬± 0.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/jsonnet&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/go-jsonnet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~1277ms&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~274ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;google/jsonnet was built from source on commit f59758d1904bccda99598990f582dd2e1e9ad263, while google/go-jsonnet was &lt;code&gt;go get&lt;/code&gt;ed at version &lt;code&gt;v0.13.0&lt;/code&gt;. You can see the source code of the benchmark in&lt;br&gt; &lt;a href=&#34;https://github.com/databricks/sjsonnet/raw/master/sjsonnet/test/src-jvm/sjsonnet/SjsonnetTestMain.scala&#34;&gt;SjsonnetTestMain.scala&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sjsonnet 0.4.0 and 0.4.1 further improve the performance significantly on our internal benchmarks. A set of new JMH benchmarks provide detailed performance data of an entire run (&lt;code&gt;MainBenchmark&lt;/code&gt;) and the non-evaluation-related parts (&lt;code&gt;MaterializerBenchmark&lt;/code&gt;, &lt;code&gt;OptimizerBenchmark&lt;/code&gt;, &lt;code&gt;ParserBenchmark&lt;/code&gt;). They can be run from the (JVM / Scala 2.13 only) sbt build. The Sjsonnet profiler is located in the same sbt project:&lt;/p&gt; &#xA;&lt;p&gt;The Sjsonnet command line which is run by all of these is defined in &lt;code&gt;MainBenchmark.mainArgs&lt;/code&gt;. You need to change it to point to a suitable input before running a benchmark or the profiler. (For Databricks employees who want to reproduce our benchmarks, the pre-configured command line is expected to be run against databricks/universe @ 7cbd8d7cb071983077d41fcc34f0766d0d2a247d).&lt;/p&gt; &#xA;&lt;p&gt;Benchmark example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/jmh:run -jvmArgs &#34;-XX:+UseStringDeduplication&#34; sjsonnet.MainBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profiler:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Laziness&lt;/h2&gt; &#xA;&lt;p&gt;The Jsonnet language is &lt;em&gt;lazy&lt;/em&gt;: expressions don&#39;t get evaluated unless their value is needed, and thus even erroneous expressions do not cause a failure if un-used. This is represented in the Sjsonnet codebase by &lt;code&gt;sjsonnet.Lazy&lt;/code&gt;: a wrapper type that encapsulates an arbitrary computation that returns a &lt;code&gt;sjsonnet.Val&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sjsonnet.Lazy&lt;/code&gt; is used in several places, representing where laziness is present in the language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Scope&lt;/code&gt;, representing local variable name bindings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Arr&lt;/code&gt;, representing the contents of array cells&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Obj&lt;/code&gt;, representing the contents of object values&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Val&lt;/code&gt; extends &lt;code&gt;Lazy&lt;/code&gt; so that an already computed value can be treated as lazy without having to wrap it.&lt;/p&gt; &#xA;&lt;p&gt;Unlike &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet caches the results of lazy computations the first time they are evaluated, avoiding wasteful re-computation when a value is used more than once.&lt;/p&gt; &#xA;&lt;h2&gt;Standard Library&lt;/h2&gt; &#xA;&lt;p&gt;Different from &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet does not implement the Jsonnet standard library &lt;code&gt;std&lt;/code&gt; in Jsonnet code. Rather, those functions are implemented as intrinsics directly in the host language (in &lt;code&gt;Std.scala&lt;/code&gt;). This allows both better error messages when the input types are wrong, as well as better performance for the more computationally-intense builtin functions.&lt;/p&gt; &#xA;&lt;h2&gt;Client-Server&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet comes with a built in thin-client and background server, to help mitigate the unfortunate JVM warmup overhead that adds ~1s to every invocation down to 0.2-0.3s. For the simple non-client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create the executable. For the client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].server.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the Sjsonnet background server lives in &lt;code&gt;~/.sjsonnet&lt;/code&gt;, and lasts 5 minutes before shutting itself when inactive.&lt;/p&gt; &#xA;&lt;p&gt;Since the Sjsonnet client still has 0.2-0.3s of overhead, if using Sjsonnet heavily it is still better to include it in your JVM classpath and invoke it programmatically via &lt;code&gt;new Interpreter(...).interpret(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Publishing&lt;/h2&gt; &#xA;&lt;p&gt;To publish, make sure the version number in &lt;code&gt;build.sc&lt;/code&gt; is correct, then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill -i mill.scalalib.PublishModule/publishAll --sonatypeCreds lihaoyi:$SONATYPE_PASSWORD --publishArtifacts __.publishArtifacts --release true&#xA;&#xA;./mill -i show sjsonnet[2.13.4].js.fullOpt&#xA;./mill -i show sjsonnet[2.13.4].jvm.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;0.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make lazy initialization of static Val.Obj thread-safe &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/136&#34;&gt;#136&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deduplicate strings in the parser &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/137&#34;&gt;#137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update the JS example &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/141&#34;&gt;#141&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional significant performance improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/119&#34;&gt;#119&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Error handling fixes and improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/125&#34;&gt;#125&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements with lots of internal changes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/117&#34;&gt;#117&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avoid catching fatal exceptions during evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--yaml-debug&lt;/code&gt; flag to add source-line comments showing where each line of YAML came from &lt;a href=&#34;&#34;&gt;#105&lt;/a&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/105&#34;&gt;https://github.com/databricks/sjsonnet/pull/105&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;objectValues&lt;/code&gt; and &lt;code&gt;objectVlauesAll&lt;/code&gt; to stdlib &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/104&#34;&gt;#104&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow direct YAML output generation via &lt;code&gt;--yaml-out&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Do not allow duplicate field in object when evaluating list list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/100&#34;&gt;#100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix compiler crash when &#39;+&#39; signal is true in a field declaration inside a list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/98&#34;&gt;#98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix error message for too many arguments with at least one named arg &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/97&#34;&gt;#97&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming JSON output to disk for lower memory usage &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/85&#34;&gt;#85&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Static detection of duplicate fields &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/86&#34;&gt;#86&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Strict mode to disallow error-prone adjacent object literals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/88&#34;&gt;#88&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.flatMap&lt;/code&gt;, &lt;code&gt;std.repeat&lt;/code&gt;, &lt;code&gt;std.clamp&lt;/code&gt;, &lt;code&gt;std.member&lt;/code&gt;, &lt;code&gt;std.stripChars&lt;/code&gt;, &lt;code&gt;std.rstripChars&lt;/code&gt;, &lt;code&gt;std.lstripChars&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add support for syntactical key ordering &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/53&#34;&gt;#53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bump dependency versions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump verion of Scalatags, uPickle&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.9&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump version of FastParse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump versions of OS-Lib, uJson, Scalatags&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support std lib methods that take a key lambda &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/40&#34;&gt;#40&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Handle hex in unicode escaoes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/41&#34;&gt;#41&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add encodeUTF8, decodeUTF8 std lib methdos &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/42&#34;&gt;#42&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Properly fail on non-boolean conditionals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/44&#34;&gt;#44&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support YAML-steam output &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/45&#34;&gt;#45&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~2x performance increase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Javascript support, allowing Sjsonnet to be used in the browser or on Node.js&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala 2.13 support&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.mod&lt;/code&gt;, &lt;code&gt;std.min&lt;/code&gt; and &lt;code&gt;std.max&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improvements to error reporting when types do not match&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements to the parser via upgrading to Fastparse 2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First release&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>apache/spark</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/apache/spark</id>
    <link href="https://github.com/apache/spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;GitHub Action Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor&#34; alt=&#34;AppVeyor Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;PySpark Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&#34;https://spark.apache.org/documentation.html&#34;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Spark&lt;/h2&gt; &#xA;&lt;p&gt;Spark is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/mvn -DskipTests clean package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html&#34;&gt;&#34;Building Spark&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;&#34;Useful Developer Tools&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/spark-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Programs&lt;/h2&gt; &#xA;&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run the Pi example locally.&lt;/p&gt; &#xA;&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be a mesos:// or spark:// URL, &#34;yarn&#34; to run on YARN, and &#34;local&#34; to run locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing first requires &lt;a href=&#34;https://raw.githubusercontent.com/apache/spark/master/#building-spark&#34;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the guidance on how to &lt;a href=&#34;https://spark.apache.org/developer-tools.html#individual-tests&#34;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; &#xA;&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; &#xA;&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the build documentation at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&#34;&gt;&#34;Specifying the Hadoop Version and Enabling YARN&#34;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://spark.apache.org/contributing.html&#34;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>circe/circe</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/circe/circe</id>
    <link href="https://github.com/circe/circe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Yet another JSON library for Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;circe&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/circe/circe/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/circe/circe/Continuous%20Integration.svg?sanitize=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/circe/circe&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/circe/circe/master.svg?sanitize=true&#34; alt=&#34;Coverage status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/circe/circe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter-join%20chat-green.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/io.circe/circe-core_2.13&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/io.circe/circe-core_2.13.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;circe is a JSON library for Scala (and &lt;a href=&#34;http://www.scala-js.org/&#34;&gt;Scala.js&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://circe.github.io/circe/&#34;&gt;guide&lt;/a&gt; for more information about why circe exists and how to use it.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Adopters&lt;/h3&gt; &#xA;&lt;p&gt;Are you using circe? Please consider opening a pull request to list your organization here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://abacusprotocol.com/&#34;&gt;Abacus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apolloagriculture.com/&#34;&gt;Apollo Agriculture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.com/&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.babylonhealth.com/&#34;&gt;Babylon Health&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://banno.com/&#34;&gt;Banno inside of Jack Henry &amp;amp; Associates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.bbc.co.uk&#34;&gt;BBC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.broadinstitute.org/data-sciences-platform&#34;&gt;Broad Institute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chartboost.com/&#34;&gt;Chartboost&lt;/a&gt; (sending hundreds of thousands of messages per second on our Ad Exchange)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cibotechnologies.com&#34;&gt;CiBO Technologies&lt;/a&gt; (using circe to (de)serialize data in support of a sustainable revolution in agriculture)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.clearscore.com&#34;&gt;ClearScore&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codacy.com&#34;&gt;Codacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.connio.com&#34;&gt;Connio&lt;/a&gt; (creating and managing digital twins with Circe and Akka)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coya.com/&#34;&gt;Coya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.criteo.com/&#34;&gt;Criteo&lt;/a&gt; (&lt;a href=&#34;https://medium.com/criteo-labs/migrate-a-service-getting-200kqps-from-jackson-to-circe-a475b2718206&#34;&gt;collecting 200.000 events per second from our banners&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datalogue.io&#34;&gt;Datalogue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dreamlines.com/&#34;&gt;Dreamlines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drivetribe.com&#34;&gt;DriveTribe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.earnest.com&#34;&gt;Earnest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.elastic.co&#34;&gt;Elastic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://emmy-sharing.de/en/&#34;&gt;Emmy Sharing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://folio-sec.com/&#34;&gt;FOLIO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://glngn.com&#34;&gt;GLNGN Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gympass.com/&#34;&gt;Gympass&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gutefrage.net&#34;&gt;Gutefrage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hellosoda.com/&#34;&gt;Hello Soda&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.here.com/&#34;&gt;HERE Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.holidaycheck.de&#34;&gt;HolidayCheck&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.immobilienscout24.de/&#34;&gt;ImmobilienScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.immozentral.com/&#34;&gt;Immozentral&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.indix.com&#34;&gt;Indix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.itv.com/&#34;&gt;ITV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kinoplan.ru/&#34;&gt;Kinoplan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.latitudefinancial.com.au/&#34;&gt;Latitude Financial Services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.matchesfashion.com&#34;&gt;MatchesFashion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://merits.com&#34;&gt;Merit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.metacommerce.ru&#34;&gt;Metacommerce&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://miras-tech.com/&#34;&gt;Miras Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mobile.de&#34;&gt;Mobile GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.new-work.se/en/&#34;&gt;New Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onairentertainment.com/&#34;&gt;On Air Entertainment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onfocus.io&#34;&gt;Onfocus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opt-technologies.jp/&#34;&gt;Opt Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ovoenergy.com&#34;&gt;OVO Energy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://panaseer.com&#34;&gt;Panaseer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://permutive.com&#34;&gt;Permutive&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.projectseptember.com&#34;&gt;Project September&lt;/a&gt; (using circe to exchange and store data within the platform and serve data using GraphQL with Sangria)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/raster-foundry/raster-foundry/&#34;&gt;Raster Foundry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ravellaw.com/technology/&#34;&gt;Ravel Law&lt;/a&gt; (using circe to (de)serialize data for search, analytics, and visualization of tens of millions of legal opinions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.realestate.com.au/&#34;&gt;REA Group - realestate.com.au&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reonomy.com/&#34;&gt;Reonomy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://resilientplc.com/&#34;&gt;Resilient plc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sky.com/&#34;&gt;Sky&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://snowplowanalytics.com/&#34;&gt;Snowplow Analytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.soundcloud.com&#34;&gt;SoundCloud&lt;/a&gt; (transforming 200,000,000 JSON events every hour in MapReduce ETLs)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.spotify.com&#34;&gt;Spotify&lt;/a&gt; (using circe for JSON IO in &lt;a href=&#34;https://github.com/spotify/scio&#34;&gt;Scio&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.spotx.tv/&#34;&gt;SpotX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stripe.com&#34;&gt;Stripe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stylight.de&#34;&gt;Stylight&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://tabmo-group.io/&#34;&gt;TabMo&lt;/a&gt; (parsing more than 100k events per second with Akka Stream and Spark)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru/&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.theguardian.com&#34;&gt;The Guardian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.threatstack.com/&#34;&gt;Threat Stack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com/&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.twilio.com&#34;&gt;Twilio&lt;/a&gt; (sending many, many millions of messages a day with Circe and Akka)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://veact.net/&#34;&gt;VEACT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wework.com&#34;&gt;WeWork&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://whisk.com&#34;&gt;Whisk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.de&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zendesk.com&#34;&gt;Zendesk&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other circe organization projects&lt;/h3&gt; &#xA;&lt;p&gt;Please get in touch on &lt;a href=&#34;https://gitter.im/circe/circe&#34;&gt;Gitter&lt;/a&gt; if you have a circe-related project that you&#39;d like to discuss hosting under the &lt;a href=&#34;https://github.com/circe&#34;&gt;circe organization&lt;/a&gt; on GitHub.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-benchmarks&#34;&gt;circe-benchmarks&lt;/a&gt;: Benchmarks for comparing the performance of circe and other JSON libraries for the JVM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-config&#34;&gt;circe-config&lt;/a&gt;: A library for translating between HOCON, Java properties, and JSON documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-derivation&#34;&gt;circe-derivation&lt;/a&gt;: Experimental generic derivation with improved compile times.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-fs2&#34;&gt;circe-fs2&lt;/a&gt;: A library that provides streaming JSON parsing and decoding built on &lt;a href=&#34;https://github.com/functional-streams-for-scala/fs2&#34;&gt;fs2&lt;/a&gt; and &lt;a href=&#34;https://github.com/non/jawn&#34;&gt;Jawn&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-iteratee&#34;&gt;circe-iteratee&lt;/a&gt;: A library that provides streaming JSON parsing and decoding built on &lt;a href=&#34;https://github.com/travisbrown/iteratee&#34;&gt;iteratee.io&lt;/a&gt; and &lt;a href=&#34;https://github.com/non/jawn&#34;&gt;Jawn&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-jackson&#34;&gt;circe-jackson&lt;/a&gt;: A library that provides &lt;a href=&#34;https://github.com/FasterXML/jackson&#34;&gt;Jackson&lt;/a&gt;-supported parsing and printing for circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-spray&#34;&gt;circe-spray&lt;/a&gt;: A library that provides JSON marshallers and unmarshallers for &lt;a href=&#34;http://spray.io/&#34;&gt;Spray&lt;/a&gt; using circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-yaml&#34;&gt;circe-yaml&lt;/a&gt;: A library that uses &lt;a href=&#34;https://bitbucket.org/asomov/snakeyaml&#34;&gt;SnakeYAML&lt;/a&gt; to support parsing YAML 1.1 into circe&#39;s &lt;code&gt;Json&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Related projects&lt;/h3&gt; &#xA;&lt;p&gt;The following open source projects are either built on circe or provide circe support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://actor.im/&#34;&gt;Actor Messenger&lt;/a&gt;: A platform for instant messaging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hseeberger/akka-http-json&#34;&gt;akka-http-json&lt;/a&gt;: A library that supports using circe for JSON marshalling and unmarshalling in &lt;a href=&#34;http://doc.akka.io/docs/akka/current/scala/http/&#34;&gt;Akka HTTP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/knutwalker/akka-stream-json&#34;&gt;akka-stream-json&lt;/a&gt;: A library that provides JSON support for stream based applications using Jawn as a parser with a convenience example for circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishfenton/Argus&#34;&gt;Argus&lt;/a&gt;: Generates models and circe encoders and decoders from JSON schemas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/blackdoor/jose&#34;&gt;Blackdoor JOSE&lt;/a&gt;: circe JSON support for blackdoor JOSE and JWT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sirthias.github.io/borer/&#34;&gt;borer&lt;/a&gt;: Allows circe encoders/decoders to be reused for CBOR (de)serialization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/compstak/circe-debezium&#34;&gt;circe-debezium&lt;/a&gt;: Circe codecs for &lt;a href=&#34;https://debezium.io/&#34;&gt;Debezium&lt;/a&gt; payload types&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/compstak/circe-geojson&#34;&gt;circe-geojson&lt;/a&gt;: Circe support for GeoJSON (RFC 7946)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NeQuissimus/circe-kafka&#34;&gt;circe-kafka&lt;/a&gt;: Implicit conversion of Encoder and Decoder into Kafka Serializer/Deserializer/Serde&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agourlay/cornichon&#34;&gt;cornichon&lt;/a&gt;: A DSL for JSON API testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dcos/cosmos&#34;&gt;Cosmos&lt;/a&gt;: An API for &lt;a href=&#34;https://dcos.io/&#34;&gt;DCOS&lt;/a&gt; services that uses circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fthomas/crjdt&#34;&gt;crjdt&lt;/a&gt;: A conflict-free replicated JSON datatype in Scala.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gnieh/diffson&#34;&gt;diffson&lt;/a&gt;: A Scala diff / patch library for JSON.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sksamuel/elastic4s&#34;&gt;elastic4s&lt;/a&gt;: A Scala client for &lt;a href=&#34;https://www.elastic.co/&#34;&gt;Elasticsearch&lt;/a&gt; with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lloydmeta/enumeratum&#34;&gt;Enumeratum&lt;/a&gt;: Enumerations for Scala with circe integration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/finagle/featherbed&#34;&gt;Featherbed&lt;/a&gt;: A REST client library with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/finagle/finch&#34;&gt;Finch&lt;/a&gt;: A library for building web services with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/daviddenton/fintrospect&#34;&gt;fintrospect&lt;/a&gt;: HTTP contracts for &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;Finagle&lt;/a&gt; with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tkrs/fluflu&#34;&gt;fluflu&lt;/a&gt;: A &lt;a href=&#34;http://www.fluentd.org/&#34;&gt;Fluentd&lt;/a&gt; logger.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/47deg/github4s&#34;&gt;Github4s&lt;/a&gt;: A GitHub API wrapper written in Scala.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guardian/content-api-models&#34;&gt;content-api-models&lt;/a&gt;: The Guardian&#39;s Content API Thrift models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/http4s/http4s&#34;&gt;http4s&lt;/a&gt;: A purely functional HTTP library for client and server applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pshirshov/izumi-r2&#34;&gt;IdeaLingua&lt;/a&gt;: Staged Interface Definition and Data Modeling Language &amp;amp; RPC system currently targeting Scala, Go, C# and TypeScript. Scala codegen generates models and JSON codecs using circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/iglu&#34;&gt;Iglu Schema Repository&lt;/a&gt;: A &lt;a href=&#34;http://json-schema.org/&#34;&gt;JSON Schema&lt;/a&gt; repository with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/codemettle/jsactor&#34;&gt;jsactor&lt;/a&gt;: An actor library for Scala.js with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plokhotnyuk/jsoniter-scala/tree/master/jsoniter-scala-circe&#34;&gt;jsoniter-scala-circe&lt;/a&gt;: A booster for faster parsing/printing to/from circe AST and decoding/encoding of &lt;code&gt;java.time._&lt;/code&gt; and &lt;code&gt;BigInt&lt;/code&gt; types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pauldijou.fr/jwt-scala/samples/jwt-circe/&#34;&gt;jwt-circe&lt;/a&gt;: A &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32&#34;&gt;JSON Web Token&lt;/a&gt; implementation with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bitbucket.org/atlassian/kadai-log&#34;&gt;kadai-log&lt;/a&gt;: A logging library with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/msgpack4z/msgpack4z-circe&#34;&gt;msgpack4z-circe&lt;/a&gt;: A &lt;a href=&#34;https://github.com/msgpack/msgpack/raw/master/spec.md&#34;&gt;MessagePack&lt;/a&gt; implementation with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/djx314/ohNoMyCirce&#34;&gt;ohNoMyCirce&lt;/a&gt;: Friendly compile error messages for &lt;a href=&#34;https://github.com/milessabin/shapeless&#34;&gt;shapeless&lt;/a&gt;&#39;s Generic, &lt;a href=&#34;https://github.com/circe&#34;&gt;circe&lt;/a&gt;&#39;s Encoder &amp;amp; Decoder and &lt;a href=&#34;http://slick.lightbend.com/&#34;&gt;slick&lt;/a&gt;&#39;s case class mapping.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jilen/play-circe&#34;&gt;play-circe&lt;/a&gt;: circe support for &lt;a href=&#34;https://www.playframework.com/&#34;&gt;Play!&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sksamuel/pulsar4s&#34;&gt;pulsar4s&lt;/a&gt;: A Scala client for &lt;a href=&#34;https://pulsar.apache.org/&#34;&gt;Apache-Pulsar&lt;/a&gt; with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rapture.io/&#34;&gt;Rapture&lt;/a&gt;: Support for using circe&#39;s parsing and AST in Rapture JSON.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/finagle/roc&#34;&gt;roc&lt;/a&gt;: A PostgreSQL client built on Finagle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sangria-graphql/sangria-circe&#34;&gt;sangria-circe&lt;/a&gt;: circe marshalling for &lt;a href=&#34;http://sangria-graphql.org/&#34;&gt;Sangria&lt;/a&gt;, a &lt;a href=&#34;http://graphql.org/docs/getting-started/&#34;&gt;GraphQL&lt;/a&gt; implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vpavkin/scalist&#34;&gt;scalist&lt;/a&gt;: A &lt;a href=&#34;https://developer.todoist.com/&#34;&gt;Todoist&lt;/a&gt; API client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scala-jsonapi/scala-jsonapi&#34;&gt;scala-jsonapi&lt;/a&gt;: Scala support library for integrating the JSON API spec with Spray, Play! or Circe&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shogowada/scala-json-rpc&#34;&gt;scala-json-rpc&lt;/a&gt;: &lt;a href=&#34;http://www.jsonrpc.org&#34;&gt;JSON-RPC&lt;/a&gt; 2.0 library for Scala and Scala.js&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephennancekivell/scalatest-json&#34;&gt;scalatest-json-circe&lt;/a&gt;: Scalatest matchers for Json with appropriate equality and descriptive error messages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/scio&#34;&gt;Scio&lt;/a&gt;: A Scala API for Apache Beam and Google Cloud Dataflow, uses circe for JSON IO&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/durban/seals/&#34;&gt;seals&lt;/a&gt;: Tools for schema evolution and language-integrated schemata (derives circe encoders and decoders).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labra/shaclex&#34;&gt;shaclex&lt;/a&gt;: RDF validation using SHACL or ShEx.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tminglei/slick-pg&#34;&gt;Slick-pg&lt;/a&gt;: &lt;a href=&#34;http://slick.lightbend.com/&#34;&gt;Slick&lt;/a&gt; extensions for PostgreSQL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/softwaremill/sttp&#34;&gt;sttp&lt;/a&gt;: Scala HTTP client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mrdimosthenis.github.io/Synapses&#34;&gt;Synapses&lt;/a&gt;: A lightweight Neural Network library, for js, jvm and .net.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nikdon/telepooz&#34;&gt;telepooz&lt;/a&gt;: A Scala wrapper for the &lt;a href=&#34;https://core.telegram.org/bots/api&#34;&gt;Telegram Bot API&lt;/a&gt; built on circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sungiant/zenith&#34;&gt;Zenith&lt;/a&gt;: Functional HTTP library built on circe.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;The following projects provide examples, templates, or benchmarks that include circe:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alanphillips78/akka-http-microservice-blueprint&#34;&gt;https://github.com/alanphillips78/akka-http-microservice-blueprint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bneil/fcs_boilerplate&#34;&gt;https://github.com/bneil/fcs_boilerplate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gvolpe/simple-http4s-api&#34;&gt;https://github.com/gvolpe/simple-http4s-api&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vitorsvieira/akka-http-circe-json-template&#34;&gt;https://github.com/vitorsvieira/akka-http-circe-json-template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephennancekivell/some-jmh-json-benchmarks-circe-jackson&#34;&gt;https://github.com/stephennancekivell/some-jmh-json-benchmarks-circe-jackson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pauljamescleary/scala-pet-store&#34;&gt;https://github.com/pauljamescleary/scala-pet-store&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors and participation&lt;/h2&gt; &#xA;&lt;p&gt;circe is a fork of &lt;a href=&#34;http://argonaut.io/&#34;&gt;Argonaut&lt;/a&gt;, and if you find it at all useful, you should thank &lt;a href=&#34;https://github.com/markhibberd&#34;&gt;Mark Hibberd&lt;/a&gt;, &lt;a href=&#34;https://github.com/tonymorris&#34;&gt;Tony Morris&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuwei-k&#34;&gt;Kenji Yoshida&lt;/a&gt;, and the rest of the &lt;a href=&#34;https://github.com/argonaut-io/argonaut/graphs/contributors&#34;&gt;Argonaut contributors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;circe is currently maintained by &lt;a href=&#34;https://github.com/zarthross/&#34;&gt;Darren Gibson&lt;/a&gt; and &lt;a href=&#34;https://github.com/zmccoy/&#34;&gt;Zach McCoy&lt;/a&gt;. After the 1.0 release, all pull requests will require two sign-offs by a maintainer to be merged.&lt;/p&gt; &#xA;&lt;p&gt;The circe project supports the &lt;a href=&#34;https://www.scala-lang.org/conduct/&#34;&gt;Scala code of conduct&lt;/a&gt; and wants all of its channels (Gitter, GitHub, etc.) to be inclusive environments.&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/circe/circe/main/CONTRIBUTING.md&#34;&gt;contributors&#39; guide&lt;/a&gt; for details on how to submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;circe is licensed under the &lt;strong&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/strong&gt; (the &#34;License&#34;); you may not use this software except in compliance with the License.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/aws-sqsd</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/olxbr/aws-sqsd</id>
    <link href="https://github.com/olxbr/aws-sqsd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple alternative to the Amazon SQS Daemon (&#34;sqsd&#34;) used on AWS Beanstalk worker tier instances, based on https://github.com/mozart-analytics/sqsd&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AWS SQS Worker Daemon&lt;/h1&gt; &#xA;&lt;p&gt;A simple alternative to the Amazon SQS Daemon (&#34;sqsd&#34;) used on AWS Beanstalk worker tier instances.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;IMPORTANT:&lt;/em&gt; In order for &lt;code&gt;sqsd&lt;/code&gt; to work, you have to have configured the AWS Authentication Keys on you environment either as ENV VARS or using any of the other methods that AWS provides. For ways to do this, go &lt;a href=&#34;http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Using Environment Variables&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Property&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_DEFAULT_REGION&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;us-east-1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;The region name of the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;The access key to access the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;The secret key to access the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_QUEUE_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;Your queue URL.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_CONCURRENCY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Max number of messages process in parallel.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WAIT_TIME_SECONDS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;20&lt;/code&gt; (max: &lt;code&gt;20&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Long polling wait time when querying the queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HTTP_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://127.0.0.1:80/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Your service endpoint/path where to POST the messages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HTTP_REQUEST_CONTENT_TYPE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;application/json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;Message MIME Type.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_TIMEOUT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Max time that waiting for a worker response in milliseconds.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HEALTH_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://127.0.0.1:80/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Your service endpoint/path for your service health.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HEALTH_WAIT_TIME&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Time to between health checks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt compile&#xA;sbt universal:packageZipTarball&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using an SBT&#39;s docker image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm -it -v $PWD:/target -v $HOME/.ivy2:/root/.ivy2 -v $HOME/.m2:/root/.m2 -w /target hseeberger/scala-sbt:8u151-2.12.4-1.1.1 sbt compile&#xA;docker run --rm -it -v $PWD:/target -v $HOME/.ivy2:/root/.ivy2 -v $HOME/.m2:/root/.m2 -w /target hseeberger/scala-sbt:8u151-2.12.4-1.1.1 sbt universal:packageZipTarball&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to build the docker image&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build --tag aws-sqsd:&amp;lt;some version&amp;gt; .&#xA;docker build --tag &amp;lt;some_company&amp;gt;/aws-sqsd:&amp;lt;some version&amp;gt; .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;You should use the pre created GZVR&#39;s image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e SQSD_QUEUE_URL=&amp;lt;queue-url&amp;gt; -it -d run vivareal/aws-sqsd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or the image created by yourself&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e SQSD_QUEUE_URL=&amp;lt;queue-url&amp;gt; -it -d run some_image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug in the source code or if you want to contribute with new features, you can help submitting an issue, even better if you can submit a pull request :)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-integration-tests</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/databricks/spark-integration-tests</id>
    <link href="https://github.com/databricks/spark-integration-tests" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Integration tests for Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Integration Tests&lt;/h1&gt; &#xA;&lt;p&gt;This project contains &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;-based integration tests for Spark, including fault-tolerance tests for Spark&#39;s standalone cluster manager.&lt;/p&gt; &#xA;&lt;h2&gt;Installation / Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install Docker&lt;/h3&gt; &#xA;&lt;p&gt;This project depends on Docker &amp;gt;= 1.3.0 (it may work with earlier versions, but this hasn&#39;t been tested).&lt;/p&gt; &#xA;&lt;h4&gt;On Linux&lt;/h4&gt; &#xA;&lt;p&gt;Install Docker. This test suite requires that Docker can run without &lt;code&gt;sudo&lt;/code&gt; (see &lt;a href=&#34;http://docs.docker.io/en/latest/use/basics/&#34;&gt;http://docs.docker.io/en/latest/use/basics/&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;On OSX&lt;/h4&gt; &#xA;&lt;p&gt;On OSX, these integration tests can be run using &lt;a href=&#34;https://github.com/boot2docker/boot2docker&#34;&gt;boot2docker&lt;/a&gt;. First, &lt;a href=&#34;https://github.com/boot2docker/osx-installer/releases/tag/v1.3.2&#34;&gt;download &lt;code&gt;boot2docker&lt;/code&gt;&lt;/a&gt;, run the installer, then run &lt;code&gt;~/Applications/boot2docker&lt;/code&gt; to perform some one-time setup (create the VM, etc.). This project has been tested with &lt;code&gt;boot2docker&lt;/code&gt; 1.3.0+.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;boot2docker&lt;/code&gt;, the Docker containers will be run inside of a VirtualBox VM, which creates some difficulties for communication between the Mac host and the containers. Follow these instructions to work around those issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Network access&lt;/strong&gt;: Our tests currently run the SparkContext from outside of the containers, so we need both host &amp;lt;-&amp;gt; container and container &amp;lt;-&amp;gt; container networking to work properly. This is complicated by the fact that &lt;code&gt;boot2docker&lt;/code&gt; runs the containers behind a NAT in VirtualBox.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/boot2docker/boot2docker/issues/528&#34;&gt;One workaround&lt;/a&gt; is to add a routing table entry that routes traffic to containers to the VirtualBox VM&#39;s IP address:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo route -n add 172.17.0.0/16 `boot2docker ip`    &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You&#39;ll have to re-run this command if you restart your computer or assign a new IP to the VirtualBox VM.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install Docker images&lt;/h3&gt; &#xA;&lt;p&gt;The integration tests depend on several Docker images. To set them up, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./docker/build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to build our custom Docker images and download other images from the Docker repositories. This needs to download a fair amount of stuff, so make sure that you&#39;re on a fast internet connection (or be prepared to wait a while).&lt;/p&gt; &#xA;&lt;h3&gt;Configure your environment&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;: Running &lt;code&gt;./init.sh&lt;/code&gt; will perform environment sanity checking and tell you which shell exports to perform.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;code&gt;SPARK_HOME&lt;/code&gt; environment variable should to a Spark source checkout where an assembly has been built. This directory will be shared with Docker containers; Spark workers and masters will use this &lt;code&gt;SPARK_HOME/work&lt;/code&gt; as their work directory. This effectively treats host machine&#39;s &lt;code&gt;SPARK_HOME&lt;/code&gt; directory as a directory on a network-mounted filesystem.&lt;/p&gt; &lt;p&gt;Additionally, this Spark sbt project will added as a dependency of this sbt project, so the integration test code will be compiled against that Spark version.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test-specific requirements&lt;/h3&gt; &#xA;&lt;h4&gt;Mesos&lt;/h4&gt; &#xA;&lt;p&gt;The Mesos integration tests require &lt;code&gt;MESOS_NATIVE_LIBRARY&lt;/code&gt; to be set. For Mac users, the easiest way to install Mesos is through Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install mesos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export MESOS_NATIVE_LIBRARY=$(brew --repository)/lib/libmesos.dylib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark on Mesos requires a Spark binary distribution &lt;code&gt;.tgz&lt;/code&gt; file. To build this, run &lt;code&gt;./make-distribution.sh --tgz&lt;/code&gt; in your Spark checkout.&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;These integration tests are implemented as ScalaTest suites and can be run through sbt. Note that you will probably need to give sbt extra memory; with newer versions of the sbt launcher script, this can be done with the &lt;code&gt;-mem&lt;/code&gt; option, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt -mem 2048 test:package &#34;test-only org.apache.spark.integrationtests.MesosSuite&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Although our Docker-based test suites attempt to clean up the containers that they create, this cleanup may not be performed if the test runner&#39;s JVM exits abruptly. To kill &lt;strong&gt;all&lt;/strong&gt; Docker containers (including ones that may not have been launched by our tests), you can run &lt;code&gt;docker kill $(docker ps -q)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See LICENSE for full license text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/devbox</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/databricks/devbox</id>
    <link href="https://github.com/databricks/devbox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Databricks main line of development is now in the monorepo. Please see &lt;code&gt;devtools/devbox&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Devbox syncer&lt;/h1&gt; &#xA;&lt;p&gt;A one-way sync from laptop to an EC2 instance.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;To prepare an assembly jar, ready to be tested and deployed in the universe/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill launcher.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result can be found in &lt;code&gt;out/launcher/assembly/dest/out.jar&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run all tests (takes a long time):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill devbox.test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive console (REPL)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill -i devbox.repl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;There is a &lt;a href=&#34;https://github.com/databricks/devbox/actions?query=workflow%3ARelease&#34;&gt;Github Action&lt;/a&gt; to release Devbox.&lt;/p&gt; &#xA;&lt;p&gt;Just run the workflow on the target branch (usually master) with the new version number and check the &lt;a href=&#34;https://github.com/databricks/devbox/releases&#34;&gt;releases&lt;/a&gt; page&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/load-test</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/olxbr/load-test</id>
    <link href="https://github.com/olxbr/load-test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;load-test&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;strong&gt;SearchAPI&lt;/strong&gt; uses &lt;a href=&#34;http://gatling.io&#34;&gt;Gatling&lt;/a&gt; to executes load tests.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/src/gatling/resources/conf/application.conf&#34;&gt;application.conf&lt;/a&gt; file that provides all configuration to load tests.&lt;/p&gt; &#xA;&lt;p&gt;You can override each config above using Java property. For example, if you can override &lt;code&gt;gatling.users&lt;/code&gt; property you must use &lt;code&gt;-Dgatling.users=&amp;lt;value&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more details about how to do override, see &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/#how-to-run&#34;&gt;How To Run&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;How To Build&lt;/h2&gt; &#xA;&lt;p&gt;To build this project, you need to execute these commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;vivareal/load-test:load-test&lt;/code&gt; docker image will be created.&lt;/p&gt; &#xA;&lt;h2&gt;How To Run&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://gatling.io&#34;&gt;Gatling&lt;/a&gt; works with a &lt;a href=&#34;http://gatling.io/docs/current/quickstart/#a-word-on-scala&#34;&gt;Simulation&lt;/a&gt; concept and for &lt;strong&gt;SearchAPI&lt;/strong&gt; we creates the &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/src/gatling/scala/com/vivareal/search/simulation/SearchAPIv2Simulation.scala&#34;&gt;SearchAPIv2Simulation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There some steps when you run the load tests:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If not already built, builds and runs &lt;code&gt;load-test&lt;/code&gt; docker image.&lt;/li&gt; &#xA; &lt;li&gt;Executes your simulations.&lt;/li&gt; &#xA; &lt;li&gt;Uploads you simulation report on Amazon S3.&lt;/li&gt; &#xA; &lt;li&gt;Notifies report status on Slack with link to access them.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There are two parameters to use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LT_ENDPOINT&lt;/code&gt;*: load tests target endpoint.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;: gatling extra args&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The environment variables with &lt;code&gt;*&lt;/code&gt; are required. You can override each config using &lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;p&gt;To run local, you simple use &lt;code&gt;make run-local&lt;/code&gt; with the target ip to load test, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run-local LT_ENDPOINT=&#34;&amp;lt;TARGET_IP&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;To run using &lt;a href=&#34;https://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt;, you simple use &lt;code&gt;make run&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run LT_ENDPOINT=&#34;&amp;lt;TARGET_IP&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Yon can use &lt;code&gt;K8S_RUN_ARGS&lt;/code&gt; to configure &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-overview&#34;&gt;kubectl run&lt;/a&gt; params.&lt;/p&gt; &#xA;&lt;h3&gt;Running with Gradle&lt;/h3&gt; &#xA;&lt;p&gt;To run using Gradle too and you simple use &lt;code&gt;gatlingRun&lt;/code&gt; task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlinRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The upload and notification process is separated and to do this you must use &lt;code&gt;uploadReport&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlinRun uploadReport&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to Test&lt;/h3&gt; &#xA;&lt;p&gt;No tests currently implemented&lt;/p&gt; &#xA;&lt;h2&gt;How To Deploy&lt;/h2&gt; &#xA;&lt;p&gt;load-test is a tool/lib project and the deploy is subjective according to the project that uses.&lt;/p&gt; &#xA;&lt;p&gt;You may use docker to deploy: &lt;code&gt;make push&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;To push successful docker image, make sure you setup docker credentials.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Creating your own Simulation&lt;/h2&gt; &#xA;&lt;p&gt;Just implement the simulation in package &lt;code&gt;com.vivareal.search.simulation&lt;/code&gt; and sent it in &lt;code&gt;gatling.simulations.include&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlingRun -Dscenario.users=1 -Dgatling.rampUp=30 -Dgatling.maxDuration=60 -Dgatling.simulations.include=**/SimpleRequestSimulation.scala&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can send it in &lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make run LT_EXTRA_ARGS=&#34;-Dgatling.simulations.include=**/SimpleRequestSimulation.scala&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Simple Request Simularion&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s a &lt;code&gt;Simulation&lt;/code&gt; that downloads a &lt;code&gt;csv&lt;/code&gt; file from &lt;code&gt;Graylog&lt;/code&gt; API based on a query and executes the resulting &lt;code&gt;URI&lt;/code&gt;s requests in &lt;code&gt;circle&lt;/code&gt; and &lt;code&gt;during&lt;/code&gt; a configured time.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Param&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.query&lt;/td&gt; &#xA;   &lt;td&gt;Graylog query to fetch URIs&lt;/td&gt; &#xA;   &lt;td&gt;application:cloudflare&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.urisFile&lt;/td&gt; &#xA;   &lt;td&gt;file name to save the Graylog API result&lt;/td&gt; &#xA;   &lt;td&gt;uris.csv&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.uriField&lt;/td&gt; &#xA;   &lt;td&gt;the field name of the csv file generated by Graylog API&lt;/td&gt; &#xA;   &lt;td&gt;ClientRequestURI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.authorization&lt;/td&gt; &#xA;   &lt;td&gt;Graylog API encoded basic auth header&lt;/td&gt; &#xA;   &lt;td&gt;Basic LALALALA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.range&lt;/td&gt; &#xA;   &lt;td&gt;Time range in seconds for Graylog query&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.limit&lt;/td&gt; &#xA;   &lt;td&gt;Limit of the results for Graylog query&lt;/td&gt; &#xA;   &lt;td&gt;5000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All above params can be overriden using System Properties.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yanns/play2-scala-DI</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/yanns/play2-scala-DI</id>
    <link href="https://github.com/yanns/play2-scala-DI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;play2-scala-DI&lt;/h1&gt; &#xA;&lt;p&gt;Technical prototyp to test different dependency injection solutions.&lt;/p&gt; &#xA;&lt;p&gt;The target platform is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Play framework v2&lt;/li&gt; &#xA; &lt;li&gt;Scala&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Tested:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cake pattern&lt;/li&gt; &#xA; &lt;li&gt;injection with implicits.&lt;/li&gt; &#xA; &lt;li&gt;spring&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/scala-utils</title>
    <updated>2022-06-01T02:49:30Z</updated>
    <id>tag:github.com,2022-06-01:/olxbr/scala-utils</id>
    <link href="https://github.com/olxbr/scala-utils" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Utility code for Scala: logging, testing, configuration and more&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;scala-utils&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/grupozap/scala-utils&#34;&gt;&lt;img src=&#34;https://travis-ci.org/grupozap/scala-utils.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;scala-utils&lt;/code&gt; is an utility library that attempts to add useful code rapidly in your development pipeline, so that you can focus on what is really needed. It does not replace any existing library, instead it allows you to add production-ready features.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt;: Add features such as a configurable GELF log formatter without the need of a full Graylog connector library (&lt;a href=&#34;https://github.com/grupozap/scala-utils/tree/master/src/main/scala/com/grupozap/scalautils/logging&#34;&gt;https://github.com/grupozap/scala-utils/tree/master/src/main/scala/com/grupozap/scalautils/logging&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use it in your project&lt;/h2&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;br.com.gzvr&#34; %% &#34;scala-utils&#34; % &#34;1.1.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll need to add our JFrog repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;resolvers += &#34;Artifactory&#34; at &#34;https://squadzapquality.jfrog.io/artifactory/olxbr-sbt-release/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supported Scala versions: &lt;code&gt;2.11&lt;/code&gt; and &lt;code&gt;2.12&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Renato Silva (&lt;a href=&#34;https://github.com/resilva87&#34;&gt;https://github.com/resilva87&lt;/a&gt;) - maintainer&lt;/li&gt; &#xA; &lt;li&gt;Thiago Pereira (&lt;a href=&#34;https://github.com/thiagoandrade6&#34;&gt;https://github.com/thiagoandrade6&lt;/a&gt;) - maintainer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug in the source code or if you want to contribute with new features, you can help submitting an issue; even better if you can submit a pull request :)&lt;/p&gt; &#xA;&lt;h3&gt;Publish&lt;/h3&gt; &#xA;&lt;p&gt;Once you merge your code to the master branch, &lt;a href=&#34;https://github.com/olxbr/scala-utils/actions&#34;&gt;GitHub Actions&lt;/a&gt; should automatically publish it.&lt;/p&gt; &#xA;&lt;p&gt;To publish manually, create a &lt;code&gt;credentials.properties&lt;/code&gt; file in the project&#39;s directory, with &lt;a href=&#34;https://vault.grupozap.io/ui/vault/secrets/squad-quality/show/servicos/jfrog-quality&#34;&gt;the content you can find here&lt;/a&gt;, and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sbt clean compile package publish&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>