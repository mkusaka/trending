<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-01T02:22:28Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apache/kyuubi</title>
    <updated>2023-02-01T02:22:28Z</updated>
    <id>tag:github.com,2023-02-01:/apache/kyuubi</id>
    <link href="https://github.com/apache/kyuubi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Kyuubi is a distributed and multi-tenant gateway to provide serverless SQL on data warehouses and lakehouses.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Kyuubi&lt;/h1&gt; &#xA;&lt;img src=&#34;https://svn.apache.org/repos/asf/comdev/project-logos/originals/kyuubi-1.svg?sanitize=true&#34; alt=&#34;Kyuubi logo&#34; height=&#34;120px&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/kyuubi/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/apache/kyuubi?label=release&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apache/kyuubi&#34;&gt;&lt;img src=&#34;https://tokei.rs/b1/github.com/apache/kyuubi&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/kyuubi&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/kyuubi/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/apache/kyuubi/Kyuubi/master?style=plastic&#34; alt=&#34;GitHub Workflow Status&#34;&gt; &lt;a href=&#34;https://travis-ci.com/apache/kyuubi&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/apache/kyuubi.svg?branch=master&#34; alt=&#34;Travis&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://kyuubi.readthedocs.io/en/master/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/kyuubi/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/top/apache/kyuubi&#34; alt=&#34;GitHub top language&#34;&gt; &lt;a href=&#34;https://github.com/apache/kyuubi/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/apache/kyuubi&#34; alt=&#34;Commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/kyuubi&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/apache/kyuubi.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/apache/kyuubi&#34; title=&#34;Percentage of issues still open&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/open/apache/kyuubi.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Kyuubi?&lt;/h2&gt; &#xA;&lt;p&gt;Apache Kyuubiâ„¢ is a distributed and multi-tenant gateway to provide serverless SQL on data warehouses and lakehouses.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi provides a pure SQL gateway through Thrift JDBC/ODBC interface for end-users to manipulate large-scale data with pre-programmed and extensible Spark SQL engines. This &#34;out-of-the-box&#34; model minimizes the barriers and costs for end-users to use Spark at the client side. At the server-side, Kyuubi server and engines&#39; multi-tenant architecture provides the administrators a way to achieve computing resource isolation, data security, high availability, high client concurrency, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/kyuubi/master/docs/imgs/kyuubi_positioning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A HiveServer2-like API&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-tenant Spark Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Running Spark in a serverless way&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target Users&lt;/h3&gt; &#xA;&lt;p&gt;Kyuubi&#39;s goal is to make it easy and efficient for &lt;code&gt;anyone&lt;/code&gt; to use Spark(maybe other engines soon) and facilitate users to handle big data like ordinary data. Here, &lt;code&gt;anyone&lt;/code&gt; means that users do not need to have a Spark technical background but a human language, SQL only. Sometimes, SQL skills are unnecessary when integrating Kyuubi with Apache Superset, which supports rich visualizations and dashboards.&lt;/p&gt; &#xA;&lt;p&gt;In typical big data production environments with Kyuubi, there should be system administrators and end-users.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System administrators: A small group consists of Spark experts responsible for Kyuubi deployment, configuration, and tuning.&lt;/li&gt; &#xA; &lt;li&gt;End-users: Focus on business data of their own, not where it stores, how it computes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, the Kyuubi community will continuously optimize the whole system with various features, such as History-Based Optimizer, Auto-tuning, Materialized View, SQL Dialects, Functions, e.t.c.&lt;/p&gt; &#xA;&lt;h3&gt;Usage scenarios&lt;/h3&gt; &#xA;&lt;h4&gt;Port workloads from HiveServer2 to Spark SQL&lt;/h4&gt; &#xA;&lt;p&gt;In typical big data production environments, especially secured ones, all bundled services manage access control lists to restricting access to authorized users. For example, Hadoop YARN divides compute resources into queues. With Queue ACLs, it can identify and control which users/groups can take actions on particular queues. Similarly, HDFS ACLs control access of HDFS files by providing a way to set different permissions for specific users/groups.&lt;/p&gt; &#xA;&lt;p&gt;Apache Spark is a unified analytics engine for large-scale data processing. It provides a Distributed SQL Engine, a.k.a, the Spark Thrift Server(STS), designed to be seamlessly compatible with HiveServer2 and get even better performance.&lt;/p&gt; &#xA;&lt;p&gt;HiveServer2 can identify and authenticate a caller, and then if the caller also has permissions for the YARN queue and HDFS files, it succeeds. Otherwise, it fails. However, on the one hand, STS is a single Spark application. The user and queue to which STS belongs are uniquely determined at startup. Consequently, STS cannot leverage cluster managers such as YARN and Kubernetes for resource isolation and sharing or control the access for callers by the single user inside the whole system. On the other hand, the Thrift Server is coupled in the Spark driver&#39;s JVM process. This coupled architecture puts a high risk on server stability and makes it unable to handle high client concurrency or apply high availability such as load balancing as it is stateful.&lt;/p&gt; &#xA;&lt;p&gt;Kyuubi extends the use of STS in a multi-tenant model based on a unified interface and relies on the concept of multi-tenancy to interact with cluster managers to finally gain the ability of resources sharing/isolation and data security. The loosely coupled architecture of the Kyuubi server and engine dramatically improves the client concurrency and service stability of the service itself.&lt;/p&gt; &#xA;&lt;h4&gt;DataLake/LakeHouse Support&lt;/h4&gt; &#xA;&lt;p&gt;The vision of Kyuubi is to unify the portal and become an easy-to-use data lake management platform. Different kinds of workloads, such as ETL processing and BI analytics, can be supported by one platform, using one copy of data, with one SQL interface.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logical View support via Kyuubi DataLake Metadata APIs&lt;/li&gt; &#xA; &lt;li&gt;Multiple Catalogs support&lt;/li&gt; &#xA; &lt;li&gt;SQL Standard Authorization support for DataLake(coming)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Cloud Native Support&lt;/h4&gt; &#xA;&lt;p&gt;Kyuubi can deploy its engines on different kinds of Cluster Managers, such as, Hadoop YARN, Kubernetes, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/kyuubi/master/docs/imgs/kyuubi_migrating_yarn_to_k8s.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The Kyuubi Ecosystem(present and future)&lt;/h3&gt; &#xA;&lt;p&gt;The figure below shows our vision for the Kyuubi Ecosystem. Some of them have been realized, some in development, and others would not be possible without your help.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/kyuubi/master/docs/imgs/kyuubi_ecosystem.drawio.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Since Kyuubi 1.3.0-incubating, the Kyuubi online documentation is hosted by &lt;a href=&#34;https://kyuubi.apache.org/&#34;&gt;https://kyuubi.apache.org/&lt;/a&gt;. You can find the latest Kyuubi documentation on &lt;a href=&#34;https://kyuubi.readthedocs.io/en/master/&#34;&gt;this web page&lt;/a&gt;. For 1.2 and earlier versions, please check the &lt;a href=&#34;https://kyuubi.readthedocs.io/en/v1.2.0/&#34;&gt;Readthedocs&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Ready? &lt;a href=&#34;https://kyuubi.readthedocs.io/en/master/quick_start/&#34;&gt;Getting Started&lt;/a&gt; with Kyuubi.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/kyuubi/master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Contributor over time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://api7.ai/contributor-graph?chart=contributorOverTime&amp;amp;repo=apache/kyuubi&#34;&gt;&lt;img src=&#34;https://contributor-graph-api.apiseven.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=apache/kyuubi&#34; alt=&#34;Contributor over time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Aside&lt;/h2&gt; &#xA;&lt;p&gt;The project took its name from a character of a popular Japanese manga - &lt;code&gt;Naruto&lt;/code&gt;. The character is named &lt;code&gt;Kyuubi Kitsune/Kurama&lt;/code&gt;, which is a nine-tailed fox in mythology. &lt;code&gt;Kyuubi&lt;/code&gt; spread the power and spirit of fire, which is used here to represent the powerful &lt;a href=&#34;http://spark.apache.org&#34;&gt;Apache Spark&lt;/a&gt;. Its nine tails stand for end-to-end multi-tenancy support of this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See the &lt;a href=&#34;https://raw.githubusercontent.com/apache/kyuubi/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>http4s/http4s</title>
    <updated>2023-02-01T02:22:28Z</updated>
    <id>tag:github.com,2023-02-01:/http4s/http4s</id>
    <link href="https://github.com/http4s/http4s" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal, idiomatic Scala interface for HTTP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Http4s &lt;a href=&#34;https://github.com/http4s/http4s/actions?query=branch%3Aseries%2F0.23+workflow%3A%22Continuous+Integration%22&#34;&gt;&lt;img src=&#34;https://github.com/http4s/http4s/workflows/Continuous%20Integration/badge.svg?branch=series/0.23&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/maven-central/v/org.http4s/http4s-core_2.13?versionPrefix=0.23&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/org.http4s/http4s-core_2.13?versionPrefix=0.23&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://typelevel.org/projects/#http4s&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/typelevel-library-green.svg?sanitize=true&#34; alt=&#34;Typelevel library&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://typelevel.org/cats/&#34;&gt;&lt;img src=&#34;https://typelevel.org/cats/img/cats-badge.svg?sanitize=true&#34; height=&#34;40px&#34; align=&#34;right&#34; alt=&#34;Cats friendly&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Http4s is a minimal, idiomatic Scala interface for HTTP services. Http4s is Scala&#39;s answer to Ruby&#39;s Rack, Python&#39;s WSGI, Haskell&#39;s WAI, and Java&#39;s Servlets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val http = HttpRoutes.of {&#xA;  case GET -&amp;gt; Root / &#34;hello&#34; =&amp;gt;&#xA;    Ok(&#34;Hello, better world.&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Learn more at &lt;a href=&#34;https://http4s.org/&#34;&gt;http4s.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you run into any difficulties please enable partial unification in your &lt;code&gt;build.sbt&lt;/code&gt; (not needed for Scala 2.13 and beyond, because Scala 2.13.0+ has partial unification switched on by default)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scalacOptions ++= Seq(&#34;-Ypartial-unification&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Running the &lt;strong&gt;blaze&lt;/strong&gt; backend requires a modern, supported version of the JVM to build and run, as it relies on server APIs unavailable before JDK8u252. Any JDK newer than JDK8u252, including 9+ is supported.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;http4s is proud to be a &lt;a href=&#34;https://typelevel.org/&#34;&gt;Typelevel&lt;/a&gt; project. We are committed to providing a friendly, safe and welcoming environment for all, and ask that the community adhere to the &lt;a href=&#34;https://http4s.org/code-of-conduct/&#34;&gt;Scala Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This software is licensed under the Apache 2 license, quoted below.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Copyright 2013-2021 http4s [&lt;a href=&#34;https://http4s.org/&#34;&gt;https://http4s.org&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.yourkit.com/&#34;&gt;&lt;img src=&#34;https://www.yourkit.com/images/yklogo.png&#34; alt=&#34;YourKit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://www.yourkit.com/&#34;&gt;YourKit&lt;/a&gt; for supporting this project&#39;s ongoing performance tuning efforts with licenses to their excellent product.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2023-02-01T02:22:28Z</updated>
    <id>tag:github.com,2023-02-01:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP / LDAP + StartTLS / LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: LDAP is unencrypted and insecure. LDAPS is a commonly implemented extension that implements an encryption layer in a manner similar to how HTTPS adds encryption to an HTTP. LDAPS has not been documented, and the specification is not formally defined anywhere. LDAP + StartTLS is the currently recommended way to start an encrypted channel, and it upgrades an existing LDAP connection to achieve this encryption.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server (typically 389 for LDAP and LDAP + StartTLS and typically 636 for LDAPS) &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; maximum number of connection to LDAP server &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS (usually incompatible with StartTLS) &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.starttls=&amp;lt; Boolean flat to enable StartTLS (usually incompatible with SSL) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthetication.ldap.starttls=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>