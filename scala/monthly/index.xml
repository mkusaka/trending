<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-02T02:52:08Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>crealytics/spark-excel</title>
    <updated>2022-06-02T02:52:08Z</updated>
    <id>tag:github.com,2022-06-02:/crealytics/spark-excel</id>
    <link href="https://github.com/crealytics/spark-excel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Spark plugin for reading Excel files via Apache POI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Excel Library&lt;/h1&gt; &#xA;&lt;p&gt;A library for querying Excel files with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/crealytics/spark-excel/actions&#34;&gt;&lt;img src=&#34;https://github.com/crealytics/spark-excel/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.crealytics/spark-excel_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.crealytics/spark-excel_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitpod.io/#https://github.com/crealytics/spark-excel&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Co-maintainers wanted&lt;/h2&gt; &#xA;&lt;p&gt;Due to personal and professional constraints, the development of this library has been rather slow. If you find value in this library, please consider stepping up as a co-maintainer by leaving a comment &lt;a href=&#34;https://github.com/crealytics/spark-excel/issues/191&#34;&gt;here&lt;/a&gt;. Help is very welcome e.g. in the following areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional features&lt;/li&gt; &#xA; &lt;li&gt;Code improvements and reviews&lt;/li&gt; &#xA; &lt;li&gt;Bug analysis and fixing&lt;/li&gt; &#xA; &lt;li&gt;Documentation improvements&lt;/li&gt; &#xA; &lt;li&gt;Build / test infrastructure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 2.0+.&lt;/p&gt; &#xA;&lt;p&gt;List of spark versions, those are automatically tested:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark: [&#34;2.4.1&#34;, &#34;2.4.7&#34;, &#34;2.4.8&#34;, &#34;3.0.1&#34;, &#34;3.0.3&#34;, &#34;3.1.1&#34;, &#34;3.1.2&#34;, &#34;3.2.1&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more detail, please refer to project CI: &lt;a href=&#34;https://github.com/crealytics/spark-excel/raw/main/.github/workflows/ci.yml#L10&#34;&gt;ci.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.12&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.crealytics&#xA;artifactId: spark-excel_2.12&#xA;version: &amp;lt;spark-version&amp;gt;_0.17.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.crealytics&#xA;artifactId: spark-excel_2.11&#xA;version: &amp;lt;spark-version&amp;gt;_0.13.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.12&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.12:&amp;lt;spark-version&amp;gt;_0.17.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.11:&amp;lt;spark-version&amp;gt;_0.13.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This package allows querying Excel spreadsheets as &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;From spark-excel &lt;a href=&#34;https://github.com/crealytics/spark-excel/releases/tag/v0.14.0&#34;&gt;0.14.0&lt;/a&gt; (August 24, 2021), there are two implementation of spark-excel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Original Spark-Excel with Spark data source API 1.0&lt;/li&gt; &#xA;   &lt;li&gt;Spark-Excel V2 with data source API V2.0+, which supports loading from multiple files, corrupted record handling and some improvement on handling data types. See below for further details&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use V2 implementation, just change your .format from &lt;code&gt;.format(&#34;com.crealytics.spark.excel&#34;)&lt;/code&gt; to &lt;code&gt;.format(&#34;excel&#34;)&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/#excel-api-based-on-datasourcev2&#34;&gt;below&lt;/a&gt; for some details&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/CHANGELOG.md&#34;&gt;changelog&lt;/a&gt; for latest features, fixes etc.&lt;/p&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 2.0+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Create a DataFrame from an Excel file&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read&#xA;    .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;    .option(&#34;dataAddress&#34;, &#34;&#39;My Sheet&#39;!B3:C35&#34;) // Optional, default: &#34;A1&#34;&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Required&#xA;    .option(&#34;treatEmptyValuesAsNulls&#34;, &#34;false&#34;) // Optional, default: true&#xA;    .option(&#34;setErrorCellsToFallbackValues&#34;, &#34;true&#34;) // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column&#39;s data type.&#xA;    .option(&#34;usePlainNumberFormat&#34;, &#34;false&#34;) // Optional, default: false, If true, format the cells without rounding and scientific notations&#xA;    .option(&#34;inferSchema&#34;, &#34;false&#34;) // Optional, default: false&#xA;    .option(&#34;addColorColumns&#34;, &#34;true&#34;) // Optional, default: false&#xA;    .option(&#34;timestampFormat&#34;, &#34;MM-dd-yyyy HH:mm:ss&#34;) // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]&#xA;    .option(&#34;maxRowsInMemory&#34;, 20) // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)&#xA;    .option(&#34;excerptSize&#34;, 10) // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from&#xA;    .option(&#34;workbookPassword&#34;, &#34;pass&#34;) // Optional, default None. Requires unlimited strength JCE for older JVMs&#xA;    .schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings&#xA;    .load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For convenience, there is an implicit that wraps the &lt;code&gt;DataFrameReader&lt;/code&gt; returned by &lt;code&gt;spark.read&lt;/code&gt; and provides a &lt;code&gt;.excel&lt;/code&gt; method which accepts all possible options and provides default values:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;import com.crealytics.spark.excel._&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read.excel(&#xA;    header = true,  // Required&#xA;    dataAddress = &#34;&#39;My Sheet&#39;!B3:C35&#34;, // Optional, default: &#34;A1&#34;&#xA;    treatEmptyValuesAsNulls = false,  // Optional, default: true&#xA;    setErrorCellsToFallbackValues = false, // Optional, default: false, where errors will be converted to null. If true, any ERROR cell values (e.g. #N/A) will be converted to the zero values of the column&#39;s data type.&#xA;    usePlainNumberFormat = false,  // Optional, default: false. If true, format the cells without rounding and scientific notations&#xA;    inferSchema = false,  // Optional, default: false&#xA;    addColorColumns = true,  // Optional, default: false&#xA;    timestampFormat = &#34;MM-dd-yyyy HH:mm:ss&#34;,  // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]&#xA;    maxRowsInMemory = 20,  // Optional, default None. If set, uses a streaming reader which can help with big files (will fail if used with xls format files)&#xA;    excerptSize = 10,  // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from&#xA;    workbookPassword = &#34;pass&#34;  // Optional, default None. Requires unlimited strength JCE for older JVMs&#xA;).schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings&#xA; .load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the sheet name is unavailable, it is possible to pass in an index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = spark.read.excel(&#xA;  header = true,&#xA;  dataAddress = &#34;0!B3:C35&#34;&#xA;).load(&#34;Worktime.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or to read in the names dynamically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.crealytics.spark.excel.WorkbookReader&#xA;val sheetNames = WorkbookReader( Map(&#34;path&#34; -&amp;gt; &#34;Worktime.xlsx&#34;)&#xA;                               , spark.sparkContext.hadoopConfiguration&#xA;                               ).sheetNames&#xA;val df = spark.read.excel(&#xA;  header = true,&#xA;  dataAddress = sheetNames(0)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Create a DataFrame from an Excel file using custom schema&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;import org.apache.spark.sql.types._&#xA;&#xA;val peopleSchema = StructType(Array(&#xA;    StructField(&#34;Name&#34;, StringType, nullable = false),&#xA;    StructField(&#34;Age&#34;, DoubleType, nullable = false),&#xA;    StructField(&#34;Occupation&#34;, StringType, nullable = false),&#xA;    StructField(&#34;Date of birth&#34;, StringType, nullable = false)))&#xA;&#xA;val spark: SparkSession = ???&#xA;val df = spark.read&#xA;    .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;    .option(&#34;dataAddress&#34;, &#34;&#39;Info&#39;!A1&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .schema(peopleSchema)&#xA;    .load(&#34;People.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Write a DataFrame to an Excel file&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql._&#xA;&#xA;val df: DataFrame = ???&#xA;df.write&#xA;  .format(&#34;com.crealytics.spark.excel&#34;) // Or .format(&#34;excel&#34;) for V2 implementation&#xA;  .option(&#34;dataAddress&#34;, &#34;&#39;My Sheet&#39;!B3:C35&#34;)&#xA;  .option(&#34;header&#34;, &#34;true&#34;)&#xA;  .option(&#34;dateFormat&#34;, &#34;yy-mmm-d&#34;) // Optional, default: yy-m-d h:mm&#xA;  .option(&#34;timestampFormat&#34;, &#34;mm-dd-yyyy hh:mm:ss&#34;) // Optional, default: yyyy-mm-dd hh:mm:ss.000&#xA;  .mode(&#34;append&#34;) // Optional, default: overwrite.&#xA;  .save(&#34;Worktime2.xlsx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Data Addresses&lt;/h4&gt; &#xA;&lt;p&gt;As you can see in the examples above, the location of data to read or write can be specified with the &lt;code&gt;dataAddress&lt;/code&gt; option. Currently the following address styles are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;B3&lt;/code&gt;: Start cell of the data. Reading will return all rows below and all columns to the right. Writing will start here and use as many columns and rows as required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;B3:F35&lt;/code&gt;: Cell range of data. Reading will return only rows and columns in the specified range. Writing will start in the first cell (&lt;code&gt;B3&lt;/code&gt; in this example) and use only the specified columns and rows. If there are more rows or columns in the DataFrame to write, they will be truncated. Make sure this is what you want.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;My Sheet&#39;!B3:F35&lt;/code&gt;: Same as above, but with a specific sheet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MyTable[#All]&lt;/code&gt;: Table of data. Reading will return all rows and columns in this table. Writing will only write within the current range of the table. No growing of the table will be performed. PRs to change this are welcome.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Excel API based on DataSourceV2&lt;/h3&gt; &#xA;&lt;p&gt;The V2 API offers you several improvements when it comes to file and folder handling. and works in a very similar way than data sources like csv and parquet.&lt;/p&gt; &#xA;&lt;p&gt;To use V2 implementation, just change your .format from &lt;code&gt;.format(&#34;com.crealytics.spark.excel&#34;)&lt;/code&gt; to &lt;code&gt;.format(&#34;excel&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The big difference is the fact that you provide a path to read / write data from/to and not an individual single file only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dataFrame.write&#xA;        .format(&#34;excel&#34;)&#xA;        .save(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;spark.read&#xA;        .format(&#34;excel&#34;)&#xA;        // ... insert excel read specific options you need&#xA;        .load(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because folders are supported you can read/write from/to a &#34;partitioned&#34; folder structure, just the same way as csv or parquet. Note that writing partitioned structures is only available for spark &amp;gt;=3.0.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dataFrame.write&#xA;        .partitionBy(&#34;col1&#34;)&#xA;        .format(&#34;excel&#34;)&#xA;        .save(&#34;some/path&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Need some more examples? Check out the &lt;a href=&#34;https://raw.githubusercontent.com/crealytics/spark-excel/main/src/test/scala/com/crealytics/spark/v2/excel/DataFrameWriterApiComplianceSuite.scala&#34;&gt;test cases&lt;/a&gt; or have a look at our wiki&lt;/p&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;. To build a JAR file simply run &lt;code&gt;sbt assembly&lt;/code&gt; from the project root. To build for a specific spark version, for example spark-2.4.1, run &lt;code&gt;sbt -Dspark.testVersion=2.4.1 assembly&lt;/code&gt;, also from the project root. The build configuration includes support for Scala 2.12 and 2.11.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TheHive-Project/TheHive</title>
    <updated>2022-06-02T02:52:08Z</updated>
    <id>tag:github.com,2022-06-02:/TheHive-Project/TheHive</id>
    <link href="https://github.com/TheHive-Project/TheHive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TheHive: a Scalable, Open Source and Free Security Incident Response Platform&lt;/p&gt;&lt;hr&gt;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/thehive-logo.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://chat.thehive-project.org&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-7289da.svg?sanitize=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href&gt;&lt;img src=&#34;https://drone.strangebee.com/api/badges/TheHive-Project/TheHive/status.svg?ref=refs/heads/master-th4&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/LICENSE&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/TheHive-Project/TheHive&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;TheHive&lt;/a&gt; is a scalable 3-in-1 open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. It is the perfect companion to &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt;. You can synchronize it with one or multiple MISP instances to start investigations out of MISP events. You can also export an investigation&#39;s results as a MISP event to help your peers detect and react to attacks you&#39;ve dealt with. Additionally, when TheHive is used in conjunction with &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt;, security analysts and researchers can easily analyze tens if not hundred of observables.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Current_cases.png&#34; alt=&#34;Current Cases View&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Collaborate&lt;/h2&gt; &#xA;&lt;p&gt;Collaboration is at the heart of TheHive.&lt;/p&gt; &#xA;&lt;p&gt;Multiple analysts from one organisations can work together on the same case simultaneously. For example, an analyst may deal with malware analysis while another may work on tracking C2 beaconing activity on proxy logs as soon as IOCs have been added by their coworker. Using TheHive&#39;s live stream, everyone can keep an eye on what&#39;s happening on the platform, in real time.&lt;/p&gt; &#xA;&lt;p&gt;Multi-tenancy and fine grained user profiles let organisations and analysts work and collaborate on a same case accross organisations. For example, one case can be created by a first organisation who start investigating and ask for contribution from other teams or escalate to another organisation.&lt;/p&gt; &#xA;&lt;h2&gt;Elaborate&lt;/h2&gt; &#xA;&lt;p&gt;Within TheHive, every investigation corresponds to a case. Cases can be created from scratch or from &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; events, SIEM alerts, email reports and any other noteworthy source of security events.&lt;/p&gt; &#xA;&lt;p&gt;Each case can be broken down into one or more tasks. Instead of adding the same tasks to a given type of case every time one is created, analysts can use TheHive&#39;s template engine to create them once and for all. Case templates can also be used to associate metrics to specific case types in order to drive the team&#39;s activity, identify the type of investigations that take significant time and seek to automate tedious tasks.&lt;/p&gt; &#xA;&lt;p&gt;Each task can be assigned to a given analyst. Team members can also take charge of a task without waiting for someone to assign it to them.&lt;/p&gt; &#xA;&lt;p&gt;Tasks may contain multiple work logs that contributing analysts can use to describe what they are up to, what was the outcome, attach pieces of evidence or noteworthy files and so on. Logs can be written using a rich text editor or Markdown.&lt;/p&gt; &#xA;&lt;h2&gt;Analyze&lt;/h2&gt; &#xA;&lt;p&gt;You can add one or hundreds if not thousands of observables to each case you create. You can also create a case out of a &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; event. TheHive can be very easily linked to one or several MISP instances and MISP events can be previewed to decide whether they warrant an investigation or not. If an investigation is in order, the analyst can then add the event to an existing case or import it as a new case using a customizable template.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://thehive-project.org/#section_thehive4py&#34;&gt;TheHive4py&lt;/a&gt;, TheHive&#39;s Python API client, it is possible to send SIEM alerts, phishing and other suspicious emails and other security events to TheHive. They will appear in its &lt;code&gt;Alerts&lt;/code&gt; panel along with new or updated MISP events, where they can be previewed, imported into cases or ignored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Alerts_Panel.png&#34; alt=&#34;The Alerts Pane&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TheHive has the ability to automatically identify observables that have been already seen in previous cases. Observables can also be associated with a TLP and the source which provided or generated them using tags. The analyst can also easily mark observables as IOCs and isolate those using a search query then export them for searching in a SIEM or other data stores.&lt;/p&gt; &#xA;&lt;p&gt;Analysts can analyze tens or hundreds of observables in a few clicks by leveraging the analyzers of one or several &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; instances depending on your OPSEC needs: DomainTools, VirusTotal, PassiveTotal, Joe Sandbox, geolocation, threat feed lookups and so on.&lt;/p&gt; &#xA;&lt;p&gt;Security analysts with a knack for scripting can easily add their own analyzers to Cortex in order to automate actions that must be performed on observables or IOCs. They can also decide how analyzers behave according to the TLP. For example, a file added as observable can be submitted to VirusTotal if the associated TLP is WHITE or GREEN. If it&#39;s AMBER, its hash is computed and submitted to VT but not the file. If it&#39;s RED, no VT lookup is done.&lt;/p&gt; &#xA;&lt;h1&gt;Try it&lt;/h1&gt; &#xA;&lt;p&gt;To try TheHive, you can use the &lt;a href=&#34;https://www.strangebee.com/tryit&#34;&gt;training VM&lt;/a&gt; or install it by reading the &lt;a href=&#34;https://docs.thehive-project.org/thehive/installation-and-configuration/installation/step-by-step-guide/&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Details&lt;/h1&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;We have made several guides available in the &lt;a href=&#34;https://docs.thehive-project.org/thehive/&#34;&gt;Documentation repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;h3&gt;Multi-tenancy&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a special multi-tenancy support. It allows the following strategies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use a siloed multi-tenancy: many organisations can be defined without allowing them to share data;&lt;/li&gt; &#xA; &lt;li&gt;Use a collaborative multi-tenancy: a set of organisations can be allowed to collaborate on specific cases/tasks/observables, using custom defined user profiles (RBAC).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RBAC&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a set of permissions and several pre-configured user profiles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;admin&lt;/code&gt;: full administrative permissions on the platform ; can&#39;t manage any Cases or other data related to investigations;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;org-admin&lt;/code&gt;: manage users and all organisation-level configuration, can create and edit Cases, Tasks, Observables and run Analyzers and Responders;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;analyst&lt;/code&gt;: can create and edit &lt;em&gt;Cases&lt;/em&gt;, &lt;em&gt;Tasks&lt;/em&gt;, &lt;em&gt;Observables&lt;/em&gt; and run &lt;em&gt;Analyzers&lt;/em&gt; &amp;amp; &lt;em&gt;Responders&lt;/em&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read-only&lt;/code&gt;: Can only read, Cases, Tasks and Observables details;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;New profiles can be created by administrators of the platform.&lt;/p&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;TheHive 4 supports authentication methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;local accounts&lt;/li&gt; &#xA; &lt;li&gt;Active Directory&lt;/li&gt; &#xA; &lt;li&gt;LDAP&lt;/li&gt; &#xA; &lt;li&gt;Basic Auth&lt;/li&gt; &#xA; &lt;li&gt;API keys&lt;/li&gt; &#xA; &lt;li&gt;OAUTH2&lt;/li&gt; &#xA; &lt;li&gt;Multi Factor Authentication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Statistics &amp;amp; Dashboards&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a powerful statistics module that allows you to create meaningful dashboards to drive your activity and support your budget requests.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;MISP and Cortex&lt;/h3&gt; &#xA;&lt;p&gt;TheHive can be configured to import events from one or multiple &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; instances. You can also use TheHive to export cases as MISP events to one or several MISP servers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; is the perfect companion for TheHive. Use one or several to analyze observables at scale.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Digital Shadows&lt;/h3&gt; &#xA;&lt;p&gt;TheHive Project provides &lt;a href=&#34;https://github.com/TheHive-Project/DigitalShadows2TH&#34;&gt;DigitalShadows2TH&lt;/a&gt;, a free, open source &lt;a href=&#34;https://www.digitalshadows.com/&#34;&gt;Digital Shadows&lt;/a&gt; alert feeder for TheHive. You can use it to import Digital Shadows &lt;em&gt;incidents&lt;/em&gt; and &lt;em&gt;intel-incidents&lt;/em&gt; as alerts in TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Zerofox&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Zerofox2TH&#34;&gt;Zerofox2TH&lt;/a&gt; is a free, open source &lt;a href=&#34;https://www.zerofox.com/&#34;&gt;ZeroFOX&lt;/a&gt; alert feeder for TheHive, written by TheHive Project. You can use it to feed ZeroFOX alerts into TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;And many more&lt;/h3&gt; &#xA;&lt;p&gt;Lots of &lt;strong&gt;awesome&lt;/strong&gt; integrations shared by the community could be listed there. If you&#39;re looking for a specific one, &lt;strong&gt;a dedicated repository&lt;/strong&gt; containing all known details and references about existing integrations is updated frequently, and can be found here: &lt;a href=&#34;https://github.com/TheHive-Project/awesome&#34;&gt;https://github.com/TheHive-Project/awesome&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;TheHive is an open source and free software released under the &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/raw/master/LICENSE&#34;&gt;AGPL&lt;/a&gt; (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.&lt;/p&gt; &#xA;&lt;h1&gt;Updates&lt;/h1&gt; &#xA;&lt;p&gt;Information, news and updates are regularly posted on &lt;a href=&#34;https://twitter.com/thehive_project&#34;&gt;TheHive Project Twitter account&lt;/a&gt; and on &lt;a href=&#34;https://blog.thehive-project.org/&#34;&gt;the blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/code_of_conduct.md&#34;&gt;Code of conduct&lt;/a&gt;. We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests via &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;Please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;open an issue on GitHub&lt;/a&gt; if you&#39;d like to report a bug or request a feature. We are also available on &lt;a href=&#34;https://chat.thehive-project.org&#34;&gt;Discord&lt;/a&gt; to help you out.&lt;/p&gt; &#xA;&lt;p&gt;If you need to contact the project team, send an email to &lt;a href=&#34;mailto:support@thehive-project.org&#34;&gt;support@thehive-project.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have problems with &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py&#34;&gt;TheHive4py&lt;/a&gt;, please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py/issues/new&#34;&gt;open an issue on its dedicated repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you encounter an issue with Cortex or would like to request a Cortex-related feature, please &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/issues/new&#34;&gt;open an issue on its dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the &lt;a href=&#34;https://github.com/TheHive-Project/cortex-analyzers/issues/new&#34;&gt;analyzers&#39; dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community Discussions&lt;/h1&gt; &#xA;&lt;p&gt;We have set up a Google forum at &lt;a href=&#34;https://groups.google.com/a/thehive-project.org/d/forum/users&#34;&gt;https://groups.google.com/a/thehive-project.org/d/forum/users&lt;/a&gt;. To request access, you need a Google account. You may create one &lt;a href=&#34;https://accounts.google.com/SignUp?hl=en&#34;&gt;using a Gmail address&lt;/a&gt; or &lt;a href=&#34;https://accounts.google.com/SignUpWithoutGmail?hl=en&#34;&gt;without it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Website&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;https://thehive-project.org/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>geekyouth/SZT-bigdata</title>
    <updated>2022-06-02T02:52:08Z</updated>
    <id>tag:github.com,2022-06-02:/geekyouth/SZT-bigdata</id>
    <link href="https://github.com/geekyouth/SZT-bigdata" rel="alternate"></link>
    <summary type="html">&lt;p&gt;深圳地铁大数据客流分析系统🚇🚄🌟&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SZT-bigdata 深圳地铁大数据客流分析系统 🚇🚇🚇&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/full-logo.png&#34; alt=&#34;logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/stargazers&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/network/members&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/watchers&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/releases&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/issues&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata/raw/master/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/geekyouth/SZT-bigdata?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://java666.cn&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/博客：-https://java666.cn-red?style=for-the-badge&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code&gt;   ___     ____   _____           _         _      __ _      _             _&#xA;  / __|   |_  /  |_   _|   ___   | |__     (_)    / _` |  __| |   __ _    | |_    __ _&#xA;  \__ \    / /     | |    |___|  | &#39;_ \    | |    \__, | / _` |  / _` |   |  _|  / _` |&#xA;  |___/   /___|   _|_|_   _____  |_.__/   _|_|_   |___/  \__,_|  \__,_|   _\__|  \__,_|&#xA;_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|     |_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|_|&#34;&#34;&#34;&#34;&#34;|&#xA;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#34;`-0-0-&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;项目说明🚩：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🎈 该项目主要分析深圳通刷卡数据，通过大数据技术角度来研究深圳地铁客运能力，探索深圳地铁优化服务的方向；&lt;/li&gt; &#xA; &lt;li&gt;✨ 强调学以致用，本项目的原则是尽可能使用较多的常用技术框架，加深对各技术栈的理解和运用，在使用过程中体验各框架的差异和优劣，为以后的项目开发技术选型做基础；&lt;/li&gt; &#xA; &lt;li&gt;👑 解决同一个问题，可能有多种技术实现，实际的企业开发应当遵守最佳实践原则；&lt;/li&gt; &#xA; &lt;li&gt;🎉 学习过程优先选择较新的软件版本，因为新版踩坑一定比老版更多，坑踩的多了，技能也就提高了，遇到新问题可以见招拆招、对症下药；&lt;/li&gt; &#xA; &lt;li&gt;🚀 ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;第一期架构图&lt;/h2&gt; &#xA;&lt;p&gt;原图 &lt;a href=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/SZT-bigdata-2.png&#34;&gt;.file/.doc/SZT-bigdata-2.png&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/SZT-bigdata-2+.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;数字标记不分先后顺序，对应代码：&#xA;1-cn.java666.sztcommon.util.SZTData&#xA;2-cn.java666.etlflink.app.Jsons2Redis&#xA;3-cn.java666.etlspringboot.controller.RedisController#get&#xA;4-cn.java666.etlflink.app.Redis2ES&#xA;5-cn.java666.etlflink.app.Redis2Csv&#xA;6-Hive sql 脚本（开发维护成本最低）&#xA;7-Saprk 程序（开发维护成本最高，但是功能更强）&#xA;8-HUE 方便查询和展示 Hive 数据&#xA;9-cn.java666.etlflink.app.Redis2HBase&#xA;10、14-cn.java666.szthbase.controller.KafkaListen#sink2Hbase&#xA;11-cn.java666.etlflink.app.Redis2HBase&#xA;12-CDH HDFS+HUE+Hbase+Hive 一站式查询&#xA;13-cn.java666.etlflink.app.Redis2Kafka&#xA;15-cn.java666.sztflink.realtime.Kafka2MyCH&#xA;16-cn.java666.sztflink.realtime.sink.MyClickhouseSinkFun&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;下一步，计划开发数据湖中台解决方案&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;核心技术栈 + 版本选择 + 点评 (持续更新)⚡：&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/stack2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java-1.8/Scala-2.11，生态丰富，轮子够多；&lt;/li&gt; &#xA; &lt;li&gt;Flink-1.10，流式业务、ETL 首选。发展势头如日中天，阿里巴巴背书，轻快灵活、健步如飞；就问你信不信马云？？？😚😚😚&lt;/li&gt; &#xA; &lt;li&gt;Redis-3.2，天然去重，自动排序，除了快还是快。廉价版硬盘实现同类产品 SSDB。Win10|CentOS7|Docker Redis-3.2 三选一，CentOS REPL yum 安装默认使用3.2版本；&lt;/li&gt; &#xA; &lt;li&gt;Kafka-2.1，消息队列业务解耦、流量消峰、订阅发布场景首选。最佳 CP：kafka-eagle-1.4.5，集生产、消费、Ksql、大屏、监控、报警于一身，同时监控 zk。其他我用过的 Kafka 监控组件最后都放弃了： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;KafkaOffsetMonitor 问题太多，丑拒；&lt;/li&gt; &#xA;   &lt;li&gt;Kafka Manager，已更名为 CMAK，老外写的软件用起来就觉得很别扭，而且最高只兼容 Kafka 0.11，但是 Kafka 官方已经升级到 2.4 了啊喂；&lt;/li&gt; &#xA;   &lt;li&gt;其他各种开源的 Kafka 监控基本都试过，一个能打的都没有。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Zookeeper-3.4.5，集群基础依赖，选举时 ID 越大越优势，通过会话机制维护各组件在线状态；&lt;/li&gt; &#xA; &lt;li&gt;CDH-6.2，解决了程序员最难搞的软件兼容性问题，全家桶服务一键安装；&lt;/li&gt; &#xA; &lt;li&gt;Docker-19，最快速度部署一款新软件，无侵入、无污染、快速扩容、服务打包。如果当前没有合适的运行环境，那么 docker 一定是首选；&lt;/li&gt; &#xA; &lt;li&gt;SpringBoot-2.13，通用 JAVA 生态，敏捷开发必备；&lt;/li&gt; &#xA; &lt;li&gt;knife4j-2.0，前身为 swagger-bootstrap-ui，REST API 项目调试简直不要太方便，秒杀原版丝袜哥十个数量级；&lt;/li&gt; &#xA; &lt;li&gt;Elasticsearch-7，全文检索领域唯一靠谱的数据库，搜索引擎核心服务，亿级数据毫秒响应，真实时，坑也多🔊🔊🔊；&lt;/li&gt; &#xA; &lt;li&gt;Kibana-7.4，ELK 全家桶成员，前端可视化，小白也不怕；&lt;/li&gt; &#xA; &lt;li&gt;ClickHouse，家喻户晓的 nginx 服务器就是俄罗斯的代表作，接下来大红大紫的 clickhouse 同样身轻如燕，但是性能远超目前市面所有同类数据库，存储容量可达PB级别。目前资料还不多，正在学习中；&lt;/li&gt; &#xA; &lt;li&gt;MongoDB-4.0，文档数据库，对 Json 数据比较友好，主要用于爬虫数据库；&lt;/li&gt; &#xA; &lt;li&gt;Spark-2.3，目前国内大数据框架实时微批处理、离线批处理主流方案。这个组件太吃资源了，曾经在我开发时，把我的笔记本搞到蓝屏，于是我直接远程提交到 spark 集群了。接下来预计 Flink 开始表演了🦘，真的用了更快的框架就爱上了😍😍😍；&lt;/li&gt; &#xA; &lt;li&gt;Hive-2.1，Hadoop 生态数仓必备，大数据离线处理 OLAP 结构化数据库，准确来说是个 HQL 解析器，查询语法接近 Mysql，就是窗口函数比较复杂😭😭😭；&lt;/li&gt; &#xA; &lt;li&gt;Impala-3.2，像羚羊一样轻快矫健，同样的 hive sql 复杂查询，impala 毫秒级返回，hive 却需要80秒左右甚至更多；&lt;/li&gt; &#xA; &lt;li&gt;HBase-2.1 + Phoenix，Hadoop 生态下的非结构化数据库，HBase 的灵魂设计就是 rowkey 和多版本控制，凤凰嫁接 hbase 可以实现更复杂的业务；&lt;/li&gt; &#xA; &lt;li&gt;Kylin-2.5，麒麟多维预分析系统，依赖内存快速计算，但是局限性有点多啊，适用于业务特别稳定，纬度固定少变的场景，渣渣机器就别试了，内存太小带不起；&lt;/li&gt; &#xA; &lt;li&gt;HUE-4.3，CDH 全家桶赠送的，强调用户体验，操作数仓很方便，权限控制、hive + impala 查询、hdfs 文件管理、oozie 任务调度脚本编写全靠他了；&lt;/li&gt; &#xA; &lt;li&gt;阿里巴巴 DataX，异构数据源同步工具，主持大部分主流数据库，甚至可以自己开发插件，马云家的东西，我选你！！！如果你觉得这还满足不了你的特殊业务需求，那么推荐你用 FlinkX，基于 Flink 的分布式数据同步工具。理论上你也可以自己开发插件；&lt;/li&gt; &#xA; &lt;li&gt;Oozie-5.1，本身 UI 奇丑，但是配合 HUE 食用尚可接受，主要用来编写和运行任务调度脚本；&lt;/li&gt; &#xA; &lt;li&gt;Sqoop-1.4，主要用来从 Mysql 导出业务数据到 HDFS 数仓，反过来也行；&lt;/li&gt; &#xA; &lt;li&gt;Mysql-5.7，程序员都要用的吧，如果说全世界程序员都会用的语言，那一定是 SQL。Mysql 8.0 普及率不够高，MariaDB 暂不推荐，复杂的函数不兼容 Mysql，数据库这么基础的依赖组件出了问题你就哭吧；&lt;/li&gt; &#xA; &lt;li&gt;Hadoop3.0（HDFS+Yarn），HDFS 是目前大数据领域最主流的分布式海量数据存储系统，这里的 Yarn 特指 hadoop 生态，主要用来分配集群资源，自带执行引擎 MR；&lt;/li&gt; &#xA; &lt;li&gt;阿里巴巴 DataV 可视化展示；&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;我发现越来越多的国产开源软件用户体验值得肯定。。。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;准备工作🍬：&lt;/h2&gt; &#xA;&lt;p&gt;以下是我的开发环境，仅作参考：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Win10 IDEA 2019.3 旗舰版，JAVA|Scala 开发必备，集万般功能于一身；&lt;/li&gt; &#xA; &lt;li&gt;Win10 DBeaver 企业版 6.3，秒杀全宇宙所有数据库客户端，几乎一切常用数据库都可以连，选好驱动是关键；&lt;/li&gt; &#xA; &lt;li&gt;Win10 Sublime Text3，地表最强轻量级编辑器，光速启动，无限量插件，主要用来编辑零散文件、markdown 实时预览、写前端特别友好（虽然我不擅长🖐🖐🖐），速度快到完全不用担心软件跟不上你的手速；&lt;/li&gt; &#xA; &lt;li&gt;其他一些实用工具参考我的博客：&lt;a href=&#34;https://java666.cn/#/AboutMe&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://java666.cn/#/AboutMe&#34;&gt;https://java666.cn/#/AboutMe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CentOS7 CDH-6.2 集群，包含如下组件，对应的主机角色和配置如图，集群至少需要40 GB 总内存，才可以满足基本使用，不差钱的前提下，RAM 当然是合理范围内越大越好啦，鲁迅都说“天下武功唯快不破”；我们的追求是越快越好；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-cdh-view.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-cdh-host.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-cdh-role.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你选用原版 Apache 组件搭建大数据集群，那么你会有踩不完的坑。我的头发不够掉了，所以我选 CDH！！！⚙🛠😏😏😏&lt;/p&gt; &#xA;&lt;h2&gt;物理机配置💎：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;以上软件分开部署在我的三台电脑上，Win10 笔记本 VMware + Win10 台式机 VMware + 古董笔记本 CentOS7。物理机全都配置 SSD + 千兆以太网卡，HDFS 需要最快的网卡。好马配好鞍，当然你得有个千兆交换机配合千兆网线，木桶原理警告！！！🎈🎈🎈&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;有个机架当然再好不过了，哈哈哈。。。 &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-pcs.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果你想避免网线牵来牵去，可以采用电力猫实现分布式家庭组网方案；&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;数据源🌍：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;深圳市政府数据开放平台，深圳通刷卡数据 133.7 万条【离线数据】貌似已经停止服务😒：&lt;br&gt; &lt;a href=&#34;https://opendata.sz.gov.cn/data/api/toApiDetails/29200_00403601&#34;&gt;https://opendata.sz.gov.cn/data/api/toApiDetails/29200_00403601&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;备用数据源(之前上传的一批 jsons 数据有些纰漏，于是重新整理压缩后放到本仓库中，速度慢的同学可以尝试码云 &lt;a href=&#34;https://gitee.com/geekyouth/SZT-bigdata&#34;&gt;https://gitee.com/geekyouth/SZT-bigdata&lt;/a&gt; )：&lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/2018record3.zip&#34;&gt;.file/2018record3.zip&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;理论上可以当作实时数据，但是这个接口响应太慢了，如果采用 kafka 队列方式，也可以模拟出实时效果。&lt;/p&gt; &#xA;&lt;p&gt;本项目采用离线 + 实时思路 多种方案处理。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;开发进度🥇：&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;准备好 java、scala、大数据开发常用的环境，比如 IDEA、VMware 虚拟机、CDH等，然后手机静音盖上，跟我一起左手画个龙，右手划一道彩虹，开始表演吧🤪&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1- 获取数据源的 appKey：&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://opendata.sz.gov.cn/data/api/toApiDetails/29200_00403601&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2- 代码开发：&lt;/h3&gt; &#xA;&lt;h4&gt;2.1- 调用 &lt;code&gt;cn.java666.etlspringboot.source.SZTData#saveData&lt;/code&gt; 获取原始数据存盘 &lt;code&gt;/tmp/szt-data/szt-data-page.jsons&lt;/code&gt;，核对数据量 1337，注意这里每条数据包含1000条子数据；&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.2- 调用 &lt;code&gt;cn.java666.etlflink.sink.RedisSinkPageJson#main&lt;/code&gt; 实现 etl 清洗，去除重复数据，redis 天然去重排序，保证数据干净有序，跑完后核对 redis 数据量 1337。&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.3- redis 查询，redis-cli 登录后执行 &lt;code&gt;hget szt:pageJson 1&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;或者 dbeaver 可视化查询：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/redis-szt-pageJson.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.4- &lt;code&gt;cn.java666.etlspringboot.EtlSApp#main&lt;/code&gt; 启动后，也可以用 knife4j 在线调试 REST API：&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/api-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/api-debug.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.5- &lt;code&gt;cn.java666.etlflink.source.MyRedisSourceFun#run&lt;/code&gt; 清洗数据发现 133.7 万数据中，有小部分源数据字段数为9，缺少两个字段：station、car_no；丢弃脏数据。&lt;/h4&gt; &#xA;&lt;p&gt;合格源数据示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;deal_date&#34;: &#34;2018-08-31 21:15:55&#34;,&#xA;&#x9;&#34;close_date&#34;: &#34;2018-09-01 00:00:00&#34;,&#xA;&#x9;&#34;card_no&#34;: &#34;CBHGDEEJB&#34;,&#xA;&#x9;&#34;deal_value&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_type&#34;: &#34;地铁入站&#34;,&#xA;&#x9;&#34;company_name&#34;: &#34;地铁五号线&#34;,&#xA;&#x9;&#34;car_no&#34;: &#34;IGT-104&#34;,&#xA;&#x9;&#34;station&#34;: &#34;布吉&#34;,&#xA;&#x9;&#34;conn_mark&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_money&#34;: &#34;0&#34;,&#xA;&#x9;&#34;equ_no&#34;: &#34;263032104&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;不合格的源数据示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;deal_date&#34;: &#34;2018-09-01 05:24:22&#34;,&#xA;&#x9;&#34;close_date&#34;: &#34;2018-09-01 00:00:00&#34;,&#xA;&#x9;&#34;card_no&#34;: &#34;HHAAABGEH&#34;,&#xA;&#x9;&#34;deal_value&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_type&#34;: &#34;地铁入站&#34;,&#xA;&#x9;&#34;company_name&#34;: &#34;地铁一号线&#34;,&#xA;&#x9;&#34;conn_mark&#34;: &#34;0&#34;,&#xA;&#x9;&#34;deal_money&#34;: &#34;0&#34;,&#xA;&#x9;&#34;equ_no&#34;: &#34;268005140&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.6- &lt;code&gt;cn.java666.etlflink.app.Redis2Kafka#main&lt;/code&gt; 根据需求推送满足业务要求的源数据到 kafka，&lt;code&gt;topic-flink-szt-all&lt;/code&gt; 保留了所有源数据 1337000 条， &lt;code&gt;topic-flink-szt&lt;/code&gt; 仅包含清洗合格的源数据 1266039 条。&lt;/h4&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.7- kafka-eagle 监控查看 topic，基于原版去掉了背景图，漂亮多了：&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kafka-eagle02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kafka-eagle01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ksql 命令查询： &lt;code&gt;select * from &#34;topic-flink-szt&#34; where &#34;partition&#34; in (0) limit 1000&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/ksql.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.8- &lt;code&gt;cn.java666.etlflink.app.Redis2Csv#main&lt;/code&gt; 实现了 flink sink csv 格式文件，并且支持按天分块保存。&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/csv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.9- &lt;code&gt;cn.java666.etlflink.app.Redis2ES#main&lt;/code&gt; 实现了 ES 存储源数据。实现实时全文检索，实时跟踪深圳通刷卡数据。&lt;/h4&gt; &#xA;&lt;p&gt;这个模块涉及技术细节比较多，如果没有 ES 使用经验，可以先做下功课，不然的话会很懵。&lt;/p&gt; &#xA;&lt;p&gt;我之前在处理 ES 各种问题踩了不少坑，熬了不少通宵，掉了很多头发。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;遇到问题心态要稳，因为你今天处理了一个问题，明天接触新的版本新的框架大概率又会出现新的问题&lt;/strong&gt;。。🥺🥺🥺&lt;/p&gt; &#xA;&lt;p&gt;所以最佳实践很重要！！！&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;👇👇👇这部分内容有更新：修正了上一个版本时区问题。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;🎬接下来，让我们时光倒流，回到 2018-09-01这一天，调整 kibana 面板时间范围 &lt;code&gt;2018-09-01 00:00:00.000~2018-09-01 23:59:59.999&lt;/code&gt;，看看当天深圳通刷卡记录的统计图曲线走向是否科学，间接验证数据源的完整性。&lt;/p&gt; &#xA;&lt;p&gt;修正时区后统计数量，字段完整的合格源数据 1266039 条，2018-09-01全天 1229180 条。&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/2018-09-01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;图中可以看出 2018-09-01 这一天刷卡记录集中在上午6点~12点之间，早高峰数据比较吻合，虽然这一天是周六，高峰期不是特别明显。我们继续缩放 kibana 时间轴看看更详细的曲线： &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/2018-09-01-am.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;回顾一下本项目 ETL 处理流程：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;1337000 条源数据清洗去除字段不全的脏数据，剩余的合格数据条数 1266039 已经进入 ES 索引 &lt;code&gt;szt-data&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;在 1266039 条合格数据中，有 1227234 条数据集中在 2018-09-01 这一天的上午时段；&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;我们暂且相信上午时段的数据是真实的，那么是否说明官方提供的数据并不是全部的当天完整刷卡数据？？？&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如果按照上午的刷卡量来估测全天的刷卡量，考虑到是周六，那么深圳通全天的刷卡记录数据应该在 122万 X 2 左右，当然这么武断的判断方式不是程序员的风格，接下来我们用科学的大数据分析方式来研究这些数据背后的意义。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;注意，ES 大坑：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ES 存数据时，带有时间字段的数据如何实时展示到 kibana 的图表面板上？&lt;br&gt; 🤣需要在存入 index 之前设置字段映射。参考格式，不要照抄！！！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;properties&#34;: {&#xA;&#x9;&#34;deal_date&#34;: {&#xA;&#x9;  &#34;format&#34;: &#34;yyyy-MM-dd HH:mm:ss&#34;,&#xA;&#x9;  &#34;type&#34;: &#34;date&#34;&#xA;&#x9;}&#xA;  }&#xA;}  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;这里并没有指定时区信息，但是 ES 默认使用 0 时区，这个软件很坑，无法设置全局默认时区。但是很多软件产生的数据都是默认机器所在时区，国内就是东八区。因为我们的源始数据本身也没有包含时区信息，这里我不想改源数据，那就假装自己在 ES 的 0 时区。同时需要修改 kibana 默认时区为 UTC，才可以保证 kibana 索引图表时间轴正确对位。不过这并不是一个科学的解决方案。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;如果是企业项目，必须要用数据质量监控软件！！！要不然得有多少背锅侠要杀去祭天😂😂😂，数据可以没有但是千万不能错。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ES 存数据时，需要使用 json 格式包装数据，不符合json 语法的纯字符无法保存；&lt;/li&gt; &#xA; &lt;li&gt;ES 序列化复杂的 bean 对象时，如果 fastjson 报错，推荐使用 Gson，很强！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TIPS😙😙😙：&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gson 相比 fastjson：Gson 序列化能力更强，但是 反序列化时，fastjson 速度更快。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.10- 查看 ES 数据库卡号，对比自己的深圳通地铁卡，逐渐发现了一些脱敏规律。&lt;/h4&gt; &#xA;&lt;p&gt;日志当中卡号脱敏字段密文反解猜想：&lt;br&gt; 由脱敏的密文卡号反推真实卡号，因为所有卡号密文当中没有J开头的数据， 但是有A开头的数据，A != 0，而且出现了 BCDEFGHIJ 没有 K，所以猜想卡号映射关系如图！！！&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/parse_card_no.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; 类似摩斯电码解密。。。我现在还不确定这个解密方式是否正确🙄🙄🙄&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;2.11- &lt;code&gt;cn.java666.sztcommon.util.ParseCardNo#parse&lt;/code&gt; 实现了支持自动识别卡号明文和密文、一键互转功能。 &lt;code&gt;cn.java666.etlspringboot.controller.CardController#get&lt;/code&gt; 实现了卡号明文和密文互转 REST API。&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/parse_no.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3- 搭建数仓：深圳地铁数仓建模&lt;/h3&gt; &#xA;&lt;h4&gt;3.1- 第一步，分析业务&lt;/h4&gt; &#xA;&lt;p&gt;确定业务流程 ---&amp;gt; 声明粒度 ---&amp;gt; 确定维度 ---&amp;gt; 确定事实&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.doc/dim.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2- 第二步，规划数仓结构&lt;/h4&gt; &#xA;&lt;p&gt;参考行业通用的数仓分层模式：ODS、DWD、DWS、ADS，虽然原始数据很简单，但是我们依然使用规范的流程设计数据仓库。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第一层：ODS 原始数据层&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ods/ods_szt_data/day=2018-09-01/   &#xA;# szt_szt_page/day=2018-09-01/  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第二层：DWD 清洗降维层&lt;br&gt; 区分维表 dim_ 和事实表 fact_，为了使粒度更加细化，我们把进站和出站记录分开，巴士数据暂不考虑。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;dwd_fact_szt_in_detail      进站事实详情表&#xA;dwd_fact_szt_out_detail     出站事实详情表&#xA;dwd_fact_szt_in_out_detail  地铁进出站总表&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第三层：DWS 宽表层&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;dws_card_record_day_wide  每卡每日行程记录宽表【单卡单日所有出行记录】&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第四层：ADS 业务指标层【待补充】&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;【体现进站压力】 每站进站人次排行榜      &#xA;&#x9;ads_in_station_day_top&#xA;【体现出站压力】 每站出站人次排行榜      &#xA;&#x9;ads_out_station_day_top&#xA;【体现进出站压力】 每站进出站人次排行榜      &#xA;&#x9;ads_in_out_station_day_top&#xA;【体现通勤车费最多】 每卡日消费排行      &#xA;&#x9;ads_card_deal_day_top  &#xA;【体现线路运输贡献度】 每线路单日运输乘客总次数排行榜，进站算一次，出站并且联程算一次     &#xA;&#x9;ads_line_send_passengers_day_top  &#xA;【体现利用率最高的车站区间】 每日运输乘客最多的车站区间排行榜       &#xA;&#x9;ads_stations_send_passengers_day_top&#xA;【体现线路的平均通勤时间，运输效率】 每条线路单程直达乘客耗时平均值排行榜     &#xA;&#x9;ads_line_single_ride_average_time_day_top&#xA;【体现深圳地铁全市乘客平均通勤时间】 所有乘客从上车到下车间隔时间平均值    &#xA;&#x9;ads_all_passengers_single_ride_spend_time_average&#xA;【体现通勤时间最长的乘客】 单日从上车到下车间隔时间排行榜     &#xA;&#x9;ads_passenger_spend_time_day_top&#xA;【体现车站配置】 每个站点进出站闸机数量排行榜&#xA;&#x9;每个站点入站闸机数量  &#x9;&#x9;ads_station_in_equ_num_top&#xA;&#x9;每个站点出站闸机数量    &#x9;&#x9;ads_station_out_equ_num_top&#xA;【体现各线路综合服务水平】 各线路进出站闸机数排行榜&#xA;&#x9;各线路进站闸机数排行榜 &#x9;&#x9;ads_line_in_equ_num_top.png&#xA;&#x9;各线路出站闸机数排行榜 &#x9;&#x9;ads_line_out_equ_num_top&#xA;【体现收入最多的车站】 出站交易收入排行榜   &#xA;&#x9;ads_station_deal_day_top&#xA;【体现收入最多的线路】 出站交易所在线路收入排行榜   &#xA;&#x9;ads_line_deal_day_top&#xA;【体现换乘比例、乘车体验】 每天每线路换乘出站乘客百分比排行榜  &#xA;&#x9;ads_conn_ratio_day_top&#xA;【体现每条线的深圳通乘车卡普及程度 9.5 折优惠】 出站交易优惠人数百分比排行榜     &#xA;&#x9;ads_line_sale_ratio_top&#xA;【体现换乘的心酸】 换乘耗时最久的乘客排行榜&#x9;&#xA;&#x9;ads_conn_spend_time_top&#xA;【体现线路拥挤程度】 上车以后还没下车，每分钟、小时每条线在线人数   &#xA;&#x9;ads_on_line_min_top&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3.3- 第三步：建库建表计算指标&lt;/h4&gt; &#xA;&lt;p&gt;hdfs 关闭权限检查。hive 设置保存目录 /warehouse；&lt;br&gt; hue 创建 hue 用户，赋予超级组。hue 切换到 hue 用户，执行 hive sql 建库 szt；&lt;br&gt; 库下面建目录 ods dwd dws ads；&lt;/p&gt; &#xA;&lt;p&gt;上传原始数据到 /warehouse/szt.db/ods/&lt;br&gt; szt-etl-data.csv szt-etl-data_2018-09-01.csv szt-page.jsons&lt;/p&gt; &#xA;&lt;p&gt;查看： &lt;code&gt;hdfs dfs -ls -h hdfs://cdh231:8020/warehouse/szt.db/ods/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;接下来使用 HUE 按照 &lt;code&gt;sql/hive.sql&lt;/code&gt; 依次执行 HQL 语句.....&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;也可以使用 IDEA Database 工具栏操作，附送idea cdh hive 完美驱动 &lt;a href=&#34;https://github.com/timveil/hive-jdbc-uber-jar/releases&#34;&gt;https://github.com/timveil/hive-jdbc-uber-jar/releases&lt;/a&gt;：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/idea-dev+hive.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;也可以使用 DBeaver （我只想说， 上古产品 Sqlyog、navicat、heidisql、workbench 全都是战五渣），因为有时候复杂的查询可以一边执行一边在另一个客户端工具查看结果，这对于复杂的嵌套查询 debug 非常有助于分析和跟踪问题。DBeaver 客户端自带图表，不过没有 HUE 好看：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/dbeaver-dev+hive.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;已经完成的指标分析：&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.1 - 深圳地铁进站人次排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天依次为：五和、布吉、丹竹头，数据说明当天这几个站点进站人数最多。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_in_station_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_in_station_day_top2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.2 - 深圳地铁出站人次排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天出站乘客主要去向分别为：深圳北高铁站、罗湖火车站、福田口岸。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_out_station_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_out_station_day_top2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.3- 深圳地铁进出站总人次排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天车站吞吐量排行榜：&lt;br&gt; 五和站？？？、布吉站（深圳东火车站）、罗湖站（深圳火车站）、深圳北（深圳北高铁站）。。。&lt;br&gt; 五和站为什么这么秀？？？ 🚀&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_in_out_station_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.4- 深圳地铁乘客车费排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天车费最高的乘客花了 48 元人民币&lt;br&gt; 🚄🚄🚄 说明：深圳通地铁卡不记名，未涉及个人隐私！！！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_card_deal_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.5- 深圳地铁各线路单日发送旅客排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天五号线客运量遥遥领先，龙岗线碾压一号线，心疼龙岗人民！😳&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_send_passengers_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.6- 深圳地铁每日运输乘客最多的区间排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01当天前三名分别是：赤尾&amp;gt;华强北，福民&amp;gt;福田口岸，五和&amp;gt;深圳北&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_stations_send.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.7- 深圳地铁每条线路单程直达乘客耗时平均值排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天五号线单程直达乘客平均耗时1500s，约合25分钟，平均值最长的是 11号线，平均耗时 40 分钟&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_single_ride_average_time_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.8- 深圳地铁所有乘客通勤时间平均值：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天所有乘客通勤时间平均值 1791 s，约合 30 分钟&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_all_passengers_single_ride_spend_time_average.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.9- 深圳地铁所有乘客通勤时间排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天所有乘客通勤时间排行榜，站内滞留最久的乘客间隔 17123 秒，约合 4.75 小时，实际情况只需要 20 分钟车程，难道是进站搞事情？？？&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kibana-search-card-1.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/baiduMap1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_passenger_spend_time_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.10- 深圳地铁每个站点进出站闸机数量排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天福田站双项第一&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_station_in_equ_num_top.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_station_out_equ_num_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.11- 深圳地铁各线路进出站闸机数量排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天深圳地铁一号线长脸了@_@，两个指标都是第一，港铁四号线全部垫底，后妈养的？？？&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_in_equ_num_top.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_out_equ_num_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.12- 深圳地铁各站收入排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，当天上午深圳北站收入 4 万元人民币，排名第一&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_station_deal_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.12- 深圳地铁各线路收入排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;2018-09-01，数据显示一号线依然是深圳地铁最多收入的线路，1号线上午收入 30 万元人民币，其次是五号线紧随其后&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_deal_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.13- 深圳地铁各线路换乘出站乘客百分比排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;换乘后从五号线出来的乘客是占比最高的 15.6%，从九号线出站的乘客，换乘比例最低，仅 9.42%&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_conn_ratio_day_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.14- 深圳地铁各线路直达乘客优惠人次百分比排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;目前可以确定的是，持有深圳通地铁卡可以享受9.5折优惠乘坐地铁，从统计结果看，2018-09-01当天，七号线使用地铁卡优惠的乘客人次占比最高，达到 90.36%，排名最低的是五号线，占比 84.3%&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_line_sale_ratio_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h5&gt;3.3.15- 深圳地铁换乘时间最久的乘客排行榜：&lt;/h5&gt; &#xA;&lt;p&gt;&lt;strong&gt;统计过程发现难以理解的现象，有几个乘客进站以后，没有刷卡出站就换乘了公交车，于是出现了同一个地铁站进出站，但是标记为联程的记录&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/WTF.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/.ads/ads_conn_spend_time_top.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4- 新增模块：SZT-kafka-hbase&lt;/h3&gt; &#xA;&lt;p&gt;SZT-kafka-hbase project for Spring Boot2&lt;br&gt; 看过开源的 spring-boot-starter-hbase、spring-data-hadoop-hbase，基础依赖过于老旧，长期不更新；引入过程繁琐，而且 API 粒度受限；数据库连接没有复用，导致数据库服务读写成本太高。&lt;/p&gt; &#xA;&lt;p&gt;于是自己实现了 hbase-2.1 + springboot-2.1.13 + kafka-2.0 的集成，一个长会话完成 hbase 连续的增删改查👑👑👑，降低服务器资源的开销。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase666.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;主要特色：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;knife4j 在线调试，点击鼠标即可完成 hbase 写入和查询，再也不用记住繁琐的命令😏😏😏。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;hbase 列族版本历史设置为 10，支持配置文件级别的修改。可以查询某卡号最近 10 次交易记录。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;hbase rowkey 设计为卡号反转，使得字典排序过程消耗的服务器算力在分布式环境更加均衡。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;全自动的建库建表【本项目的 hbase 命名空间为 szt】，实现幂等操作，无需担心 hbase 数据库的污染。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;效果展示：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;准备部署完成的 hbase，适当修改本项目配置文件，运行 SZT-kafka-hbase 项目，效果如下：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;启动：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-run.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;api-debug，随便写点东西进去，狂点发送。能写多快就考验你的手速了😏😏😏：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-api-debug.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;hue-hbase 查表：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hue-hbase-szt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;hue-hbase 查看历史版本：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hue-hbase-szt-versions-10.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;hbase-shell 命令：&lt;br&gt; 全表扫描，返回十个版本格式化为字符串显示，压榨服务器性能的时候到啦！！！😝😝😝&lt;br&gt; &lt;code&gt;scan &#39;szt:data&#39;, {FORMATTER =&amp;gt; &#39;toString&#39;,VERSIONS=&amp;gt;10}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-shell-toString.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;接下来接入 kafka 🎯🎯🎯&lt;br&gt; 启动 &lt;code&gt;cn.java666.etlflink.app.Redis2Kafka&lt;/code&gt;，生产消息，适当调慢生产速度，以免机器崩溃。&lt;br&gt; 不出意外的话，你会看到 SZT-kafka-hbase 项目的控制台打印了日志：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/kafka2hbase.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果 hbase 崩溃了，看看内存够不够，我就直接怼上 2GB X 3 个节点🌟🌟🌟：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/SZT-kafka-hbase/.pic/hbase-2GB.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5- &lt;code&gt;SZT-flink&lt;/code&gt; 模块新增 &lt;code&gt;cn.java666.etlflink.app.Json2HBase&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;实现了从 redis 或者其他数据源取出 json 串，保存到 hbase 表。本项目中从 redis 获取 json（当然更推荐 kafka），通过 flink 清洗存到 hbase flink:flink2hbase 表中。用于实时保存深圳通刷卡记录，通过卡号查询可以获取卡号最近10次（如果有10次）交易记录。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/flink2hbase.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;简化了上一版 hbase 写入 bean 的方式，JSON 再一次赢得掌声😏😏😏。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val keys = jsonObj.keySet().toList&#xA;val size = keys.size()&#xA;&#xA;for (i &amp;lt;- 0 until size) {&#xA;&#x9;val key = keys.get(i)&#xA;&#x9;val value = jsonObj.getStr(key)&#xA;&#x9;putCell(card_no_re, cf, key, value)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6- 新增实时处理模块 SZT-flink&lt;/h3&gt; &#xA;&lt;p&gt;完成 flink 读取 kafka，存到 clickhouse 功能。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/clickhouse-tabix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/clickhouse-sql.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;...继续开发中🛠🛠🛠...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO🔔🔔🔔:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 解析 redis pageJson，转换数据格式为最小数据单元存到 csv，减少原始数据的冗余字符，方便存取和传输。丰富数据源的格式，兼容更多的实现方案；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 推送 kafka，使用队列传输数据；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 存入 elasticsearch，使用全文检索实现实时搜索，kibana 可视化展示；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 数仓建模：ODS、DWD、DWS、ADS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; hive on spark 数仓建模、分析计算；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; spark on hive，本地开发 spark 程序，操作远程 hive 数据库；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 刷卡记录实时写入 hbase，支持最近交易记录的查询；&lt;/li&gt; &#xA; &lt;li&gt;[-] &lt;del&gt;oozie 调度，数据太少啊 嘤嘤嘤&lt;/del&gt;😮😮😮;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 实时思路分析数据：flink 流式实时分析早晚高峰站点压力排行；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 离线思路分析数据：spark 微批处理；&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DataV 可视化大屏展示；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;更新日志🌥：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2022-05-28:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;更新 fastjson，修复高危漏洞&lt;/li&gt; &#xA;   &lt;li&gt;格式化代码，使用空格替换制表符&lt;/li&gt; &#xA;   &lt;li&gt;添加 “反996”、apache-2.0 开源许可证&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-25：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;flink 实时流处理功能部分上线。完成 flink 读取 kafka，存到 clickhouse 模块；&lt;/li&gt; &#xA;   &lt;li&gt;补充第一期开发计划架构图；&lt;/li&gt; &#xA;   &lt;li&gt;下一步，计划开发数据湖中台解决方案，规模比较大。目前这个项目已经初现雏形，短期内以维护和优化为主【原则就是先上线后迭代】；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-22:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;补充第一期开发计划的架构图，帮助理解整个业务流程；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-14：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;RedisSinkPageJson&lt;/code&gt; 从 &lt;code&gt;package cn.java666.etlflink.sink&lt;/code&gt; 移到 &lt;code&gt;package cn.java666.etlflink.app&lt;/code&gt; 更名为 &lt;code&gt;Jsons2Redis&lt;/code&gt;，方便归类，该模块用于解析原始数据多行json到redis；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-05-01：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;实现了从 redis 或者其他数据源取出 json 串，保存到 hbase 表；&lt;/li&gt; &#xA;   &lt;li&gt;实现了 hbase-2.1 + springboot-2.1.13 + kafka-2.0 的集成；&lt;/li&gt; &#xA;   &lt;li&gt;实时消费 kafka 消息存到 hbase 数据库，支持实时查询某卡号最近 n 次交易记录；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-30：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;实现了 hbase-2.1 + springboot-2.1.13 的集成，一个长会话完成 hbase 连续的增删改查👑👑👑，降低服务器资源的开销。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-27：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;彻底的解决了静态资源无法热部署的问题；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;&#x9;&amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;&#xA;&#x9;&amp;lt;artifactId&amp;gt;spring-boot-devtools&amp;lt;/artifactId&amp;gt;&#xA;&#x9;&amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;&#xA;&#x9;&amp;lt;optional&amp;gt;true&amp;lt;/optional&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&#xA;######################### 实时热部署 ###################################&#xA;#&#34;关闭缓存, 即时刷新&#34;&#xA;spring.freemarker.cache=false&#xA;spring.thymeleaf.cache=false&#xA;&#xA;#热部署生效&#xA;spring.devtools.restart.enabled=true&#xA;#是否支持livereload&#xA;spring.devtools.livereload.enabled=true&#xA;#设置重启的目录,添加那个目录的文件需要restart&#xA;spring.devtools.restart.additional-paths=src/main/*&#xA;#设置不需要重启的目录&#xA;#spring.devtools.restart.exclude=static/**,public/**&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;202-04-27: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;完成所有线路规划+换乘方案的抓取入库，合计 45932 条；&lt;/li&gt; &#xA;   &lt;li&gt;解决了 hive 注释乱码问题；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;&#xA;alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;&#xA;alter table PARTITION_PARAMS  modify column PARAM_VALUE varchar(4000) character set utf8;&#xA;alter table PARTITION_KEYS  modify column PKEY_COMMENT varchar(4000) character set utf8;&#xA;alter table  INDEX_PARAMS  modify column PARAM_VALUE  varchar(4000) character set utf8;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-24：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各线路换乘出站乘客百分比排行榜；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各线路直达乘客优惠人次百分比排行榜；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-23：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各线路单程直达乘客耗时平均值排行榜；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁所有乘客通勤时间平均值；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁所有乘客通勤时间排行榜（倒序）；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各站点、线路，进站、出站闸机数排行榜；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：深圳地铁各站点、线路，收入排行榜；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-22：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;更新文档；&lt;/li&gt; &#xA;   &lt;li&gt;完成新的指标计算任务：每日运输乘客最多的区间排行榜；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-21:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;新增模块：SZT-spark-hive，本地开发 spark 程序，操作远程 Hive 数据库；&lt;/li&gt; &#xA;   &lt;li&gt;Debug：spark on hive 本地开发，远程提交 yarn 踩坑，主要是为了缓解开发主机的压力；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-20：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;更新项目文档；&lt;/li&gt; &#xA;   &lt;li&gt;自制项目 logo；&lt;/li&gt; &#xA;   &lt;li&gt;继续写 SQL 计算新指标，本打算切到 hive 3.1 使用 TEZ 引擎，但是 hive on spark 速度已经很给力了，至少是 MR 引擎的 10 倍速度，先用着；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-19：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;vmware 虚拟机扩容时误删系统文件&lt;code&gt;rm -rf /usr/&lt;/code&gt; 🥵，好在 HDFS、Kafka、ES 自带副本机制，而且大部分业务数据都是挂载到外部磁盘，所以重要数据和组件日志基本没丢。cdh 集群添加了新的节点；&lt;/li&gt; &#xA;   &lt;li&gt;恢复工作环境，从 hive on MR 切换到 hive on spark；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-18：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;规划数仓，搭建数仓环境；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-17&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;修正错别字；&lt;/li&gt; &#xA;   &lt;li&gt;发布v0.12;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-16&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;重构项目；&lt;/li&gt; &#xA;   &lt;li&gt;补充文档&lt;/li&gt; &#xA;   &lt;li&gt;发布v0.1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-15&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;增加 common 模块，拆分解耦；&lt;/li&gt; &#xA;   &lt;li&gt;支持自动识别卡号明文和密文，一键互转，提供 REST API；&lt;/li&gt; &#xA;   &lt;li&gt;修复 ES 时区导致的错误统计数量；&lt;/li&gt; &#xA;   &lt;li&gt;Redis2Csv 实现了按天转换 csv 存盘；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-14&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;重构；&lt;/li&gt; &#xA;   &lt;li&gt;完成 csv 格式文件的抽取；&lt;/li&gt; &#xA;   &lt;li&gt;添加 GPL-3 开源证书，鼓励开源分发；&lt;/li&gt; &#xA;   &lt;li&gt;添加徽标；&lt;/li&gt; &#xA;   &lt;li&gt;完成写入 ES 数据库，添加时间映射,kibana 实时查看刷卡数据统计曲线的变化；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2020-04-13&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;项目初始化；&lt;/li&gt; &#xA;   &lt;li&gt;完成数据源清洗去重，存到 redis；&lt;/li&gt; &#xA;   &lt;li&gt;完成 redis 查询 REST API 的开发；&lt;/li&gt; &#xA;   &lt;li&gt;完成 flink 自定义 source redis 的开发，并且更细粒度清洗源数据；&lt;/li&gt; &#xA;   &lt;li&gt;完成 推送源数据到 kafka；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;联系😪：&lt;/h2&gt; &#xA;&lt;p&gt;欢迎交流技术，接头暗号&lt;code&gt;github&lt;/code&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/geekyouth/SZT-bigdata/master/.file/.pic/0-wexin.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;百度和谷歌能找到的问题就不要再问了！很累的😕😕😕&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;补充💌💌💌：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;不开小密圈；&lt;/li&gt; &#xA; &lt;li&gt;不卖课、不卖教程；&lt;/li&gt; &#xA; &lt;li&gt;不求赞，不求粉；&lt;/li&gt; &#xA; &lt;li&gt;不发广告、不骚扰；&lt;/li&gt; &#xA; &lt;li&gt;不割韭菜&lt;/li&gt; &#xA; &lt;li&gt;不恰饭&lt;/li&gt; &#xA; &lt;li&gt;偶尔发点视频教程&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;坚持原则和底线。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;比心🤞🤞🤞&lt;/p&gt; &#xA;&lt;h2&gt;吐个槽🍦🍦🍦：&lt;/h2&gt; &#xA;&lt;p&gt;程序员这辈子一定会遇到的三个问题：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;乱码问题🌚；&lt;/li&gt; &#xA; &lt;li&gt;时区不一致问题🌗；&lt;/li&gt; &#xA; &lt;li&gt;软件版本不兼容问题❄；&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;教训：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;大数据程序员千万不能生产错误的数据，容忍程序运行失败、甚至没有输出数据，失败了可以跟踪原因，至少不会有脏数据。&lt;/li&gt; &#xA; &lt;li&gt;一旦数据错误，会影响后面的所有计算流程，甚至导致错误决策。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;统计信息：&lt;/h2&gt; &#xA;&lt;div align=&#34;right&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://starchart.cc/geekyouth/SZT-bigdata.svg?sanitize=true&#34; alt=&#34;关注曲线&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github-readme-stats.vercel.app/api?username=geekyouth&amp;amp;show_icons=true&amp;amp;theme=monokai&#34; alt=&#34;个人概况&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github-readme-stats.vercel.app/api/pin?username=geekyouth&amp;amp;repo=SZT-bigdata&amp;amp;show_icons=true&amp;amp;theme=monokai&amp;amp;show_owner=true&#34; alt=&#34;仓库概况&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://www.jetbrains.com/?from=https://github.com/geekyouth/SZT-bigdata&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://www.jetbrains.com/company/brand/img/logo1.svg?sanitize=true&#34; alt=&#34;赞助伙伴&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>stripe/bonsai</title>
    <updated>2022-06-02T02:52:08Z</updated>
    <id>tag:github.com,2022-06-02:/stripe/bonsai</id>
    <link href="https://github.com/stripe/bonsai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Beautiful trees, without the landscaping.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bonsai&lt;/h1&gt; &#xA;&lt;p&gt;Beautiful trees, without the landscaping. Bonsai is a Scala library for transforming arbitrary tree structures into read-only versions that take up a fraction of the space.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Bonsai compresses trees in 2 ways: by using significantly less space to store the tree structure itself (tree compression), and by encoding the node labels in a memory efficient structure (label compression).&lt;/p&gt; &#xA;&lt;h3&gt;What is a &#34;Tree&#34;?&lt;/h3&gt; &#xA;&lt;p&gt;Bonsai works over arbitrary trees, so it assumes a fairly generic interface for interacting with trees. In Bonsai a tree;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;has 0 or 1 root nodes&lt;/li&gt; &#xA; &lt;li&gt;each node has 0 or more children&lt;/li&gt; &#xA; &lt;li&gt;each node has a label attached to it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The actual type of the node is unimportant. What is important is the node labels and the relationships between the nodes (parent, child, sibling, etc). This structure is enough to describe most of the types of trees you are familiar with.&lt;/p&gt; &#xA;&lt;p&gt;Bonsai encodes this notion of trees with the &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/TreeOps.scala&#34;&gt;TreeOps type class&lt;/a&gt;. Here is a truncated version of the type class:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;trait TreeOps[Tree, Label] {&#xA;&#xA;  /** The type of the nodes in the tree. */&#xA;  type Node&#xA;&#xA;  /**&#xA;   * Returns the root node of the tree.&#xA;   */&#xA;  def root(t: Tree): Option[Node]&#xA;&#xA;  /**&#xA;   * Returns all the direct children of the given node. The order may or may&#xA;   * not matter. TreeOps does not provide any guarantees here.&#xA;   */&#xA;  def children(node: Node): Iterable[Node]&#xA;&#xA;  /**&#xA;   * Returns the label attached to the given node.&#xA;   */&#xA;  def label(node: Node): Label&#xA;&#xA;  ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The type &lt;code&gt;T&lt;/code&gt; is our actual tree type. The &lt;code&gt;Node&lt;/code&gt; type is the way we reference internal nodes in our tree &lt;code&gt;T&lt;/code&gt;. The actual type of &lt;code&gt;Node&lt;/code&gt; isn&#39;t important, however, and is mostly an implementation detail. The important bit is the &lt;code&gt;Label&lt;/code&gt; type, which is the user-facing data associated with each node.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;bonsai-example&lt;/code&gt; subproject has &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-example/src/main/scala/com/stripe/bonsai/example/Huffman.scala&#34;&gt;an example of a Huffman tree&lt;/a&gt;. A Huffman tree is used to store a Huffman coding for decoding a compressed message (a bitstring). We decode the bitstring, bit-by-bit, using the tree.&lt;/p&gt; &#xA;&lt;p&gt;Starting at the root of the tree, we follow the left child if the current bit is a 0 and the right child if it is a 1. We continue until reaching a leaf node, at which poitn we output the symbol associated with it, then start back at the beginning of the tree. When we&#39;ve exhausted the entire bitstring, we&#39;ll have our decoded message.&lt;/p&gt; &#xA;&lt;p&gt;Here is how we may implement a Huffman tree in Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sealed trait HuffmanTree[+A]&#xA;case class Branch[+A](zero: HuffmanTree[A], one: HuffmanTree[A]) extends HuffmanTree[A]&#xA;case class Leaf[+A](value: A) extends HuffmanTree[A]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And here is how we would implement its &lt;code&gt;TreeOps&lt;/code&gt; instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.stripe.bonsai.TreeOps&#xA;&#xA;object HuffmanTree {&#xA;  implicit def huffmanTreeOps[A]: TreeOps[HuffmanTree[A], Option[A]] =&#xA;    new TreeOps[HuffmanTree[A], Option[A]] {&#xA;      type Node = HuffmanTree[A]&#xA;&#xA;      def root(tree: HuffmanTree[A]): Option[HuffmanTree[A]] = Some(tree)&#xA;      def children(tree: HuffmanTree[A]): Iterable[HuffmanTree[A]] = tree match {&#xA;        case Branch(l, r) =&amp;gt; l :: r :: Nil&#xA;        case _ =&amp;gt; Nil&#xA;      }&#xA;      def label(tree: HuffmanTree[A]): Option[A] = tree match {&#xA;        case Leaf(value) =&amp;gt; Some(value)&#xA;        case _ =&amp;gt; None&#xA;      }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As long as we are careful to implement all our operations on a Huffman tree by using its more generic &lt;code&gt;TreeOps&lt;/code&gt; interface, rather than &lt;code&gt;HuffmanTree&lt;/code&gt; directly, we can then swap out the actual tree data structure, without affecting the code using it.&lt;/p&gt; &#xA;&lt;p&gt;For example, below we implement a &lt;code&gt;decode&lt;/code&gt; operation as an implicit class using just &lt;code&gt;TreeOps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import scala.collection.immutable.BitSet&#xA;&#xA;implicit class HuffmanTreeOps[T, A](tree: T)(implicit treeOps: TreeOps[T, Option[A]]) {&#xA;  // Importing treeOps gives us some useful methods on `tree`&#xA;  import treeOps._&#xA;&#xA;  def decode(bits: BitSet, len: Int): Vector[A] = {&#xA;    val root = tree.root.get&#xA;    val (_, result) = (0 until len)&#xA;      .foldLeft((root, Vector.empty[A])) { case ((node, acc), i) =&amp;gt;&#xA;        node.label match {&#xA;          case Some(value) =&amp;gt; (root, acc :+ value)&#xA;          case None if bits(i) =&amp;gt; (node.children.head, acc)&#xA;          case None =&amp;gt; (node.children.iterator.drop(1).next, acc)&#xA;        }&#xA;      }&#xA;    result&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The goal of this indirection through &lt;code&gt;TreeOps&lt;/code&gt; is to let us use a compressed version of the &lt;code&gt;tree&lt;/code&gt; instead of an actual &lt;code&gt;HuffmanTree&lt;/code&gt;, which will see below.&lt;/p&gt; &#xA;&lt;h3&gt;Tree Compression&lt;/h3&gt; &#xA;&lt;p&gt;Bonsai&#39;s tree compression is based off of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Succinct_data_structure&#34; title=&#34;Succinct Data Structures&#34;&gt;succinct data structure&lt;/a&gt; for binary trees. Bonsai supports k-ary trees by first transforming the original tree into a &lt;a href=&#34;https://en.wikipedia.org/wiki/Left-child_right-sibling_binary_tree&#34;&gt;left-child right-sibling tree&lt;/a&gt;, which preserves all the relationships from the original tree, but ensures we have at most 2 children per node. You can read more about the details of the actual compression algorithm used in &lt;a href=&#34;http://www.dcc.uchile.cl/~gnavarro/algoritmos/ps/wea05.pdf&#34;&gt;&#34;Practical Implementation of Rank and Select Queries&#34;&lt;/a&gt;. &lt;strong&gt;The upshot is that we can store the entire structure of a tree in only ~2.73bits per node.&lt;/strong&gt; This replaces the normal strategy of using JVM objects for nodes and references to store the relationships.&lt;/p&gt; &#xA;&lt;p&gt;We actually compress trees by transforming them into &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/Tree.scala&#34;&gt;Bonsai &lt;code&gt;Tree&lt;/code&gt;s&lt;/a&gt;. Bonsai&#39;s &lt;code&gt;Tree&lt;/code&gt; constructor takes any arbitrary tree &lt;code&gt;T&lt;/code&gt; that has a &lt;code&gt;TreeOps[T]&lt;/code&gt; available and will return a &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/Tree.scala&#34;&gt;&lt;code&gt;Tree&lt;/code&gt;&lt;/a&gt; with the same structure and labels (and &lt;code&gt;Label&lt;/code&gt; type) as the original tree. However, the entire structure and labels of the tree will have been compressed, so this new tree requires significantly less space.&lt;/p&gt; &#xA;&lt;p&gt;In the example in &lt;code&gt;bonsai-example&lt;/code&gt;, we use the Huffman encoding described above to construct a simple Huffman tree for the printable ASCII characters (0x20 -&amp;gt; 0x7E) and compress it using Bonsai&#39;s &lt;code&gt;Tree&lt;/code&gt;. The result is a &lt;strong&gt;11x reduction&lt;/strong&gt; in memory requirements. Since our &lt;code&gt;decode&lt;/code&gt; operation was implemented using &lt;code&gt;TreeOps&lt;/code&gt;, we can use this compressed tree just as we would&#39;ve used the original tree.&lt;/p&gt; &#xA;&lt;p&gt;This example is a bit contrived, since the trees are small to begin with, but you can imagine that applying this to a large random forest yields great results.&lt;/p&gt; &#xA;&lt;h3&gt;Label Compression&lt;/h3&gt; &#xA;&lt;p&gt;Bonsai provides a &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-core/src/main/scala/com/stripe/bonsai/Layout.scala&#34;&gt;Layout&lt;/a&gt; type class, along with some simple combinators, for describing how to (de)serialize your labels. At the lowest level are a set of Layout &#34;primitives&#34; that can encode simple data types into compact data structures. The combinators then allow more complex structures to be described (tuples, &lt;code&gt;Either&lt;/code&gt;, mappings to case classes, etc), without adding much, if any, overhead.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of a &lt;code&gt;Layout&lt;/code&gt; for some &lt;code&gt;Widget&lt;/code&gt; type we made up:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.stripe.bonsai.Layout&#xA;&#xA;sealed trait Widget&#xA;case class Sprocket(radius: Int, weight: Option[Double]) extends Widget&#xA;case class Doodad(length: Int, width: Int, weight: Option[Double]) extends Widget&#xA;&#xA;object Widget {&#xA;  implicit val WidgetLayout: Layout[Widget] = {&#xA;    Layout[Either[(Int, Option[Double]), ((Int, Int), Option[Double])]].transform(&#xA;      {&#xA;        case Left((r, wt)) =&amp;gt; Sprocket(r, wt)&#xA;        case Right(((l, w), wt)) =&amp;gt; Doodad(l, w, wt)&#xA;      },&#xA;      {&#xA;        case Sprocket(r, wt) =&amp;gt; Left((r, wt))&#xA;        case Doodad(l, w, wt) =&amp;gt; Right(((l, w), wt))&#xA;      }&#xA;    )&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can see the &lt;a href=&#34;https://github.com/stripe/bonsai/raw/master/bonsai-example/src/main/scala/com/stripe/bonsai/example/Widget.scala&#34;&gt;full Widget code/example&lt;/a&gt; in the &lt;code&gt;bonsai-example&lt;/code&gt; sub project. In that example, we compress a &lt;code&gt;Vector[Option[Widget]]&lt;/code&gt; using the layout and end up with over a &lt;strong&gt;6x reduction&lt;/strong&gt; in memory requirements.&lt;/p&gt; &#xA;&lt;p&gt;Currently, Bonsai focuses mainly on compressing the overhead of the structure your data requires (eg options, eithers, tuples), rather than the data itself. This will likely change in future releases, and we&#39;ll support better compression for primitive types, as well as things like dictionary encoding for all types.&lt;/p&gt; &#xA;&lt;h1&gt;Using Bonsai in SBT or Maven&lt;/h1&gt; &#xA;&lt;p&gt;Bonsai is published on sonatype. To use it in your SBT project, you can add the following to your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;libraryDependencies += &#34;com.stripe&#34; %% &#34;bonsai&#34; % &#34;0.3.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Miscellaneous&lt;/h1&gt; &#xA;&lt;p&gt;Bonsai is Open Source and available under the MIT License.&lt;/p&gt; &#xA;&lt;p&gt;For more help, feel free to contact the authors or create an issue.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>svtk/lecture4-scala</title>
    <updated>2022-06-02T02:52:08Z</updated>
    <id>tag:github.com,2022-06-02:/svtk/lecture4-scala</id>
    <link href="https://github.com/svtk/lecture4-scala" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The code used in a lecture # 4 (28.09)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lecture#4&lt;/h1&gt; &#xA;&lt;p&gt;The code used in a lecture # 4 (28.09)&lt;/p&gt; &#xA;&lt;h1&gt;Task&lt;/h1&gt; &#xA;&lt;p&gt;Implement normalization of a lambda term. Lambda term should be specified like this:&lt;/p&gt; &#xA;&lt;p&gt;Appl(Abst(Var(&#34;x&#34;), Appl(Var(&#34;x&#34;),Var(&#34;y&#34;))),Abst(Var(&#34;x&#34;),Var(&#34;x&#34;)))&lt;/p&gt; &#xA;&lt;p&gt;Use case classes and pattern matching!&lt;/p&gt; &#xA;&lt;p&gt;Send it by e-mail: &lt;a href=&#34;mailto:course.scala@gmail.com&#34;&gt;course.scala@gmail.com&lt;/a&gt; Deadline is 4 October 2012, 21:00&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/tvm-vta</title>
    <updated>2022-06-02T02:52:08Z</updated>
    <id>tag:github.com,2022-06-02:/apache/tvm-vta</id>
    <link href="https://github.com/apache/tvm-vta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open, Modular, Deep Learning Accelerator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VTA Hardware Design Stack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ci.tlcpack.ai/job/tvm-vta/job/main/&#34;&gt;&lt;img src=&#34;https://ci.tlcpack.ai/job/tvm-vta/job/main/badge/icon&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VTA (versatile tensor accelerator) is an open-source deep learning accelerator complemented with an end-to-end TVM-based compiler stack.&lt;/p&gt; &#xA;&lt;p&gt;The key features of VTA include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generic, modular, open-source hardware &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Streamlined workflow to deploy to FPGAs.&lt;/li&gt; &#xA;   &lt;li&gt;Simulator support to prototype compilation passes on regular workstations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Driver and JIT runtime for both simulator and FPGA hardware back-end.&lt;/li&gt; &#xA; &lt;li&gt;End-to-end TVM stack integration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Direct optimization and deployment of models from deep learning frameworks via TVM.&lt;/li&gt; &#xA;   &lt;li&gt;Customized and extensible TVM compiler back-end.&lt;/li&gt; &#xA;   &lt;li&gt;Flexible RPC support to ease deployment, and program FPGAs with the convenience of Python.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>