<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T02:50:42Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>flix/flix</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/flix/flix</id>
    <link href="https://github.com/flix/flix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Flix Programming Language&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/logo.png&#34; height=&#34;91px&#34; alt=&#34;The Flix Programming Language&#34; title=&#34;The Flix Programming Language&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flix&lt;/strong&gt; is a statically typed functional, imperative, and logic programming language.&lt;/p&gt; &#xA;&lt;p&gt;We refer you to the &lt;a href=&#34;https://flix.dev/&#34;&gt;official Flix website (flix.dev)&lt;/a&gt; for more information about Flix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/flix/Lobby&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/example.png&#34; height=&#34;627px&#34; alt=&#34;Example Flix Program&#34; title=&#34;Example Flix Program&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/flix/flix/master/docs/BUILD.md&#34;&gt;docs/BUILD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Flix is available under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;We kindly thank &lt;a href=&#34;https://www.ej-technologies.com/&#34;&gt;EJ Technologies&lt;/a&gt; for providing us with &lt;a href=&#34;http://www.ej-technologies.com/products/jprofiler/overview.html&#34;&gt;JProfiler&lt;/a&gt; and &lt;a href=&#34;https://www.jetbrains.com/&#34;&gt;JetBrains&lt;/a&gt; for providing us with &lt;a href=&#34;https://www.jetbrains.com/idea/&#34;&gt;IntelliJ IDEA&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>polomarcus/tp</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/polomarcus/tp</id>
    <link href="https://github.com/polomarcus/tp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Practices - Data engineering&lt;/h1&gt; &#xA;&lt;h2&gt;Tools you need&lt;/h2&gt; &#xA;&lt;p&gt;Have a stackoverflow account : &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;https://stackoverflow.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have a github account : &lt;a href=&#34;https://github.com/&#34;&gt;https://github.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And a github repo to push your code.&lt;/p&gt; &#xA;&lt;h3&gt;Fork the repo on your own Github account&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/tp/fork&#34;&gt;https://github.com/polomarcus/tp/fork&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker and Compose&lt;/h3&gt; &#xA;&lt;p&gt;Take time to read and install&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/get-started/overview/&#34;&gt;https://docs.docker.com/get-started/overview/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker --version&#xA;Docker version 20.10.14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;https://docs.docker.com/compose/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose --version&#xA;docker-compose version 1.29.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TP1 - &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-docker-kafka-bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP2 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-functional-programming-scala&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP3 - Functional programming for data engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open &lt;code&gt;tp-data-processing-framework&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TP 4 - Kafka Streams to read and write to Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/streams/&#34;&gt;https://kafka.apache.org/documentation/streams/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&#34;&gt;https://github.com/polomarcus/Spark-Structured-Streaming-Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kafka User Interface (UI) : &lt;a href=&#34;https://www.conduktor.io/download/&#34;&gt;https://www.conduktor.io/download/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>snowplow/snowplow</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/snowplow/snowplow</id>
    <link href="https://github.com/snowplow/snowplow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The enterprise-grade behavioral data engine (web, mobile, server-side, webhooks), running cloud-natively on AWS and GCP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Snowplow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/releases/tag/22.01&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Snowplow-22.01%20Western%20Ghats-6638b8&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2-blue.svg?style=flat&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/discourse/posts?server=https%3A%2F%2Fdiscourse.snowplowanalytics.com%2F&#34; alt=&#34;Discourse posts&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://snowplowanalytics.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/media/snowplow_logo.png&#34; alt=&#34;Snowplow logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Snowplow is an enterprise-strength marketing and product analytics platform. It does three things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Identifies your users, and tracks the way they engage with your website or application&lt;/li&gt; &#xA; &lt;li&gt;Stores your users&#39; behavioral data in a scalable &#34;event data warehouse&#34; you control: Amazon Redshift, Google BigQuery, Snowflake or Elasticsearch&lt;/li&gt; &#xA; &lt;li&gt;Lets you leverage the biggest range of tools to analyze that data, including big data tools (e.g. Spark) via EMR or more traditional tools e.g. Looker, Mode, Superset, Re:dash to analyze that behavioral data&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;To find out more, please check out the &lt;a href=&#34;https://snowplowanalytics.com&#34;&gt;Snowplow website&lt;/a&gt; and the &lt;a href=&#34;https://docs.snowplowanalytics.com/open-source-docs/&#34;&gt;docs website&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Version Compatibility Matrix&lt;/h3&gt; &#xA;&lt;p&gt;For compatibility assurance, the version compatibility matrix offers clarity on our recommended stack. It is strongly recommended when setting up a Snowplow pipeline to use the versions listed in the version compatibility matrix which can be found &lt;a href=&#34;https://docs.snowplowanalytics.com/docs/pipeline-components-and-applications/version-compatibility-matrix/&#34;&gt;within our docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Public Roadmap&lt;/h3&gt; &#xA;&lt;p&gt;This repository also contains the &lt;a href=&#34;https://github.com/snowplow/snowplow/projects&#34;&gt;Snowplow Public Roadmap&lt;/a&gt;. The Public Roadmap lets you stay up to date and find out what&#39;s happening on the Snowplow Platform. Help us prioritize our cards: open the issue and leave a üëç to vote for your favorites. Want us to build a feature or function? Tell us by heading to our &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse forum&lt;/a&gt; üí¨.&lt;/p&gt; &#xA;&lt;h3&gt;Try Snowplow&lt;/h3&gt; &#xA;&lt;p&gt;Setting up a full open-source Snowplow pipeline requires a non-trivial amount of engineering expertise and time investment. You might be interested in finding out what Snowplow can do first, by setting up &lt;a href=&#34;https://try.snowplowanalytics.com/?utm_source=github&amp;amp;utm_medium=post&amp;amp;utm_campaign=try-snowplow&#34;&gt;Try Snowplow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Open Source Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://docs.snowplowanalytics.com/docs/open-source-quick-start/&#34;&gt;Open Source Quick Start&lt;/a&gt; will help you get up and running with a Snowplow open source pipeline. Snowplow publishes a &lt;a href=&#34;https://registry.terraform.io/modules/snowplow-devops&#34;&gt;set of terraform modules&lt;/a&gt;, which automate the setting up &amp;amp; deployment of the required infrastructure &amp;amp; applications for an operational Snowplow open source pipeline, with just a handful of input variables required on your side.&lt;/p&gt; &#xA;&lt;h3&gt;Join the Snowplow Research Panel and help shape the future of open source&lt;/h3&gt; &#xA;&lt;p&gt;As part of our ongoing efforts to improve the Snowplow Open Source experience, we&#39;re looking for users of our open-source software and members of our community to take part in research studies. &lt;a href=&#34;https://forms.gle/pCtYx8naum7A8vvw5&#34;&gt;Join here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Our Commercial Offering&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to get everything setup and managed for you, you can consider &lt;a href=&#34;https://snowplowanalytics.com/products/snowplow-bdp/&#34;&gt;Snowplow BDP&lt;/a&gt;. You can also &lt;a href=&#34;https://go.snowplowanalytics.com/l/571483/2021-05-04/3sv1pg8&#34;&gt;request a demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Snowplow technology 101&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/ARCHITECTURE.md&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/media/snowplow_architecture.png&#34; alt=&#34;Snowplow architecture&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The repository structure follows the conceptual architecture of Snowplow, which consists of six loosely-coupled sub-systems connected by five standardized data protocols/formats.&lt;/p&gt; &#xA;&lt;p&gt;To briefly explain these six sub-systems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/1-trackers&#34;&gt;Trackers&lt;/a&gt;&lt;/strong&gt; fire Snowplow events. Currently we have 15 trackers, covering web, mobile, desktop, server and IoT&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/2-collectors&#34;&gt;Collector&lt;/a&gt;&lt;/strong&gt; receives Snowplow events from trackers. Currently we have one official collector implementation with different sinks: Amazon Kinesis, Google PubSub, Amazon SQS, Apache Kafka and NSQ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/3-enrich&#34;&gt;Enrich&lt;/a&gt;&lt;/strong&gt; cleans up the raw Snowplow events, enriches them and puts them into storage. Currently we have several implementations, built for different environments (GCP, AWS, Apache Kafka) and one core library&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/4-storage&#34;&gt;Storage&lt;/a&gt;&lt;/strong&gt; is where the Snowplow events live. Currently we store the Snowplow events in a flat file structure on S3, and in the Redshift, Postgres, Snowflake and BigQuery databases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow/tree/master/5-data-modeling&#34;&gt;Data modeling&lt;/a&gt;&lt;/strong&gt; is where event-level data is joined with other data sets and aggregated into smaller data sets, and business logic is applied. This produces a clean set of tables which make it easier to perform analysis on the data. We officially support data models for Redshift, Snowflake and BigQuery.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.snowplowanalytics.com/docs/modeling-your-data/analytics-sdk/&#34;&gt;Analytics&lt;/a&gt;&lt;/strong&gt; are performed on the Snowplow events or on the aggregate tables.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For more information on the current Snowplow architecture, please see the &lt;a href=&#34;https://raw.githubusercontent.com/snowplow/snowplow/master/ARCHITECTURE.md&#34;&gt;Technical architecture&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About this repository&lt;/h2&gt; &#xA;&lt;p&gt;This repository is an umbrella repository for all loosely-coupled Snowplow components and is updated on each component release.&lt;/p&gt; &#xA;&lt;p&gt;Since June 2020, all components have been extracted into their dedicated repositories (more info &lt;a href=&#34;https://snowplowanalytics.com/blog/2020/07/16/changing-releasing/&#34;&gt;here&lt;/a&gt;) and this repository serves as an entry point for Snowplow users, the home of our public roadmap and as a historical artifact.&lt;/p&gt; &#xA;&lt;p&gt;Components that have been extracted to their own repository are still here as &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Tools-Submodules&#34;&gt;git submodules&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Trackers&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Web&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mobile&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Gaming&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TV&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Desktop &amp;amp; Server&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-javascript-tracker&#34;&gt;JavaScript&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-android-tracker&#34;&gt;Android&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-unity-tracker&#34;&gt;Unity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-roku-tracker&#34;&gt;Roku&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-tracking-cli&#34;&gt;Command line&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://docs.snowplowanalytics.com/docs/collecting-data/collecting-from-own-applications/google-amp-tracker/&#34;&gt;AMP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-objc-tracker&#34;&gt;iOS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-dotnet-tracker&#34;&gt;.NET&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-react-native-tracker&#34;&gt;React Native&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-golang-tracker&#34;&gt;Go&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-flutter-tracker&#34;&gt;Flutter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-java-tracker&#34;&gt;Java&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-javascript-tracker&#34;&gt;Node.js&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-php-tracker&#34;&gt;PHP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-python-tracker&#34;&gt;Python&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-ruby-tracker&#34;&gt;Ruby&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-scala-tracker&#34;&gt;Scala&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow/stream-collector&#34;&gt;Collector&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow/enrich&#34;&gt;Enrich&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Loaders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-bigquery-loader&#34;&gt;BigQuery (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-rdb-loader&#34;&gt;Redshift (batch)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-snowflake-loader&#34;&gt;Snowflake (batch)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-google-cloud-storage-loader&#34;&gt;Google Cloud Storage (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-s3-loader&#34;&gt;Amazon S3 (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-postgres-loader&#34;&gt;Postgres (streaming)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-elasticsearch-loader&#34;&gt;Elasticsearch (streaming)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Iglu&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/iglu-server/&#34;&gt;Iglu Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/igluctl/&#34;&gt;igluctl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/iglu-central/&#34;&gt;Iglu Central&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data modeling&lt;/h3&gt; &#xA;&lt;h4&gt;Web&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/data-models/tree/master/web/v1&#34;&gt;Web model: SQL-Runner version&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/dbt-snowplow-web&#34;&gt;Web model: dbt version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Mobile&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/data-models/tree/master/mobile/v1&#34;&gt;Mobile model: SQL-Runner version&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-mini&#34;&gt;Mini&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-micro&#34;&gt;Micro&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Parsing enriched event&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-scala-analytics-sdk&#34;&gt;Analytics SDK Scala&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-python-analytics-sdk&#34;&gt;Analytics SDK Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-dotnet-analytics-sdk&#34;&gt;Analytics SDK .NET&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-js-analytics-sdk/&#34;&gt;Analytics SDK Javascript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/snowplow-golang-analytics-sdk&#34;&gt;Analytics SDK Golang&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/snowplow-incubator/snowplow-badrows&#34;&gt;Bad rows&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://registry.terraform.io/modules/snowplow-devops&#34;&gt;Terraform Modules&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Need help?&lt;/h2&gt; &#xA;&lt;p&gt;We want to make it super-easy for Snowplow users and contributors to talk to us and connect with each other, to share ideas, solve problems and help make Snowplow awesome. Here are the main channels we&#39;re running currently, we&#39;d love to hear from you on one of them:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This is for all Snowplow users: engineers setting up Snowplow, data modelers structuring the data and data consumers building insights. You can find guides, recipes, questions and answers from Snowplow users including the Snowplow team.&lt;/p&gt; &#xA;&lt;p&gt;We welcome all questions and contributions!&lt;/p&gt; &#xA;&lt;h3&gt;Twitter&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/SnowplowData&#34;&gt;@SnowplowData&lt;/a&gt; for official news or &lt;a href=&#34;https://twitter.com/SnowplowLabs&#34;&gt;@SnowplowLabs&lt;/a&gt; for engineering-heavy conversations and release updates.&lt;/p&gt; &#xA;&lt;h3&gt;GitHub&lt;/h3&gt; &#xA;&lt;p&gt;If you spot a bug, then please raise an issue in the GitHub repository of the component in question. Likewise if you have developed a cool new feature or an improvement, please open a pull request, we&#39;ll be glad to integrate it in the codebase!&lt;/p&gt; &#xA;&lt;p&gt;If you want to brainstorm a potential new feature, then &lt;a href=&#34;http://discourse.snowplowanalytics.com/&#34;&gt;Discourse&lt;/a&gt; is the best place to start.&lt;/p&gt; &#xA;&lt;h3&gt;Email&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;mailto:community@snowplowanalytics.com&#34;&gt;community@snowplowanalytics.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to talk directly to us (e.g. about a commercially sensitive issue), email is the easiest way.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and license&lt;/h2&gt; &#xA;&lt;p&gt;Snowplow is copyright 2012-2022 Snowplow Analytics Ltd.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the &lt;strong&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/strong&gt; (the &#34;License&#34;); you may not use this software except in compliance with the License.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yahoo/CMAK</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/yahoo/CMAK</id>
    <link href="https://github.com/yahoo/CMAK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CMAK is a tool for managing Apache Kafka clusters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CMAK (Cluster Manager for Apache Kafka, previously known as Kafka Manager)&lt;/h1&gt; &#xA;&lt;p&gt;CMAK (previously known as Kafka Manager) is a tool for managing &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt; clusters. &lt;em&gt;See below for details about the name change.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;CMAK supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage multiple clusters&lt;/li&gt; &#xA; &lt;li&gt;Easy inspection of cluster state (topics, consumers, offsets, brokers, replica distribution, partition distribution)&lt;/li&gt; &#xA; &lt;li&gt;Run preferred replica election&lt;/li&gt; &#xA; &lt;li&gt;Generate partition assignments with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Run reassignment of partition (based on generated assignments)&lt;/li&gt; &#xA; &lt;li&gt;Create a topic with optional topic configs (0.8.1.1 has different configs than 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Delete topic (only supported on 0.8.2+ and remember set delete.topic.enable=true in broker config)&lt;/li&gt; &#xA; &lt;li&gt;Topic list now indicates topics marked for deletion (only supported on 0.8.2+)&lt;/li&gt; &#xA; &lt;li&gt;Batch generate partition assignments for multiple topics with option to select brokers to use&lt;/li&gt; &#xA; &lt;li&gt;Batch run reassignment of partition for multiple topics&lt;/li&gt; &#xA; &lt;li&gt;Add partitions to existing topic&lt;/li&gt; &#xA; &lt;li&gt;Update config for existing topic&lt;/li&gt; &#xA; &lt;li&gt;Optionally enable JMX polling for broker level and topic level metrics.&lt;/li&gt; &#xA; &lt;li&gt;Optionally filter out consumers that do not have ids/ owners/ &amp;amp; offsets/ directories in zookeeper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cluster Management&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/cluster.png&#34; alt=&#34;cluster&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic-list.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/topic.png&#34; alt=&#34;topic&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumer List View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumer-list.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Consumed Topic View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/consumed-topic.png&#34; alt=&#34;consumer&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker List&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker-list.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Broker View&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yahoo/CMAK/master/img/broker.png&#34; alt=&#34;broker&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://kafka.apache.org/downloads.html&#34;&gt;Kafka 0.8.&lt;em&gt;.&lt;/em&gt; or 0.9.&lt;em&gt;.&lt;/em&gt; or 0.10.&lt;em&gt;.&lt;/em&gt; or 0.11.&lt;em&gt;.&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java 11+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The minimum configuration is the zookeeper hosts which are to be used for CMAK (pka kafka manager) state. This can be found in the application.conf file in conf directory. The same file will be packaged in the distribution zip file; you may modify settings after unzipping the file on the desired server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify multiple zookeeper hosts by comma delimiting them, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmak.zkhosts=&#34;my.zookeeper.host.com:2181,other.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the environment variable &lt;code&gt;ZK_HOSTS&lt;/code&gt; if you don&#39;t want to hardcode any values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ZK_HOSTS=&#34;my.zookeeper.host.com:2181&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally enable/disable the following functionality by modifying the default list in application.conf :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;application.features=[&#34;KMClusterManagerFeature&#34;,&#34;KMTopicManagerFeature&#34;,&#34;KMPreferredReplicaElectionFeature&#34;,&#34;KMReassignPartitionsFeature&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KMClusterManagerFeature - allows adding, updating, deleting cluster from CMAK (pka Kafka Manager)&lt;/li&gt; &#xA; &lt;li&gt;KMTopicManagerFeature - allows adding, updating, deleting topic from a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMPreferredReplicaElectionFeature - allows running of preferred replica election for a Kafka cluster&lt;/li&gt; &#xA; &lt;li&gt;KMReassignPartitionsFeature - allows generating partition assignments and reassigning partitions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Consider setting these parameters for larger clusters with jmx enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=&amp;lt; 3 * number_of_brokers&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=&amp;lt; 3 * total # of partitions across all topics&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=&amp;lt; cmak.broker-view-max-queue-size / (10 * number_of_brokers) &amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an example for a kafka cluster with 10 brokers, 100 topics, with each topic having 10 partitions giving 1000 total partitions with JMX enabled :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.broker-view-thread-pool-size=30&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-max-queue-size=3000&lt;/li&gt; &#xA; &lt;li&gt;cmak.broker-view-update-seconds=30&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The follow control consumer offset cache&#39;s thread pool and queue :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmak.offset-cache-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.offset-cache-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-thread-pool-size=&amp;lt; default is # of processors&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;cmak.kafka-admin-client-max-queue-size=&amp;lt; default is 1000&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should increase the above for large # of consumers with consumer polling enabled. Though it mainly affects ZK based consumer polling.&lt;/p&gt; &#xA;&lt;p&gt;Kafka managed consumer offset is now consumed by KafkaManagedOffsetCache from the &#34;__consumer_offsets&#34; topic. Note, this has not been tested with large number of offsets being tracked. There is a single thread per cluster consuming this topic so it may not be able to keep up on large # of offsets being pushed to the topic.&lt;/p&gt; &#xA;&lt;h3&gt;Authenticating a User with LDAP&lt;/h3&gt; &#xA;&lt;p&gt;Warning, you need to have SSL configured with CMAK (pka Kafka Manager) to ensure your credentials aren&#39;t passed unencrypted. Authenticating a User with LDAP is possible by passing the user credentials with the Authorization header. LDAP authentication is done on first visit, if successful, a cookie is set. On next request, the cookie value is compared with credentials from Authorization header. LDAP support is through the basic authentication filter.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Configure basic authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.realm=&amp;lt; basic authentication realm&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Encryption parameters (optional, otherwise randomly generated on startup) :&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.salt=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.iv=&#34;some-hex-string-representing-byte-array&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.secret=&#34;my-secret-string&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Configure LDAP/LDAPS authentication&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=&amp;lt; Boolean flag to enable/disable ldap authentication &amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&amp;lt; fqdn of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=&amp;lt; port of LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&amp;lt; LDAP search username&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&amp;lt; LDAP search password&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&amp;lt; LDAP search base&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&amp;lt; LDAP search filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=&amp;lt; number of connection to LDAP server&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=&amp;lt; Boolean flag to enable/disable LDAPS&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;(Optional) Limit access to a specific LDAP Group&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&amp;lt; LDAP group filter&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=&amp;lt; Boolean flag to allow non-expired invalid certificates&amp;gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Example (Online LDAP Test Server):&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.enabled=true&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.server=&#34;ldap.forumsys.com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.port=389&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.username=&#34;cn=read-only-admin,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.password=&#34;password&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-base-dn=&#34;dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.search-filter=&#34;(uid=$capturedLogin$)&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.group-filter=&#34;cn=allowed-group,ou=groups,dc=example,dc=com&#34;&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.connection-pool-size=10&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl=false&lt;/li&gt; &#xA; &lt;li&gt;basicAuthentication.ldap.ssl-trust-all=false&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;The command below will create a zip file which can be used to deploy the application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sbt clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to play framework documentation on &lt;a href=&#34;https://www.playframework.com/documentation/2.4.x/ProductionConfiguration&#34;&gt;production deployment/configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If java is not in your path, or you need to build against a specific java version, please use the following (the example assumes zulu java11):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ PATH=/usr/lib/jvm/zulu-11-amd64/bin:$PATH \&#xA;  JAVA_HOME=/usr/lib/jvm/zulu-11-amd64 \&#xA;  /path/to/sbt -java-home /usr/lib/jvm/zulu-11-amd64 clean dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures that the &#39;java&#39; and &#39;javac&#39; binaries in your path are first looked up in the correct location. Next, for all downstream tools that only listen to JAVA_HOME, it points them to the java11 location. Lastly, it tells sbt to use the java11 location as well.&lt;/p&gt; &#xA;&lt;h2&gt;Starting the service&lt;/h2&gt; &#xA;&lt;p&gt;After extracting the produced zipfile, and changing the working directory to it, you can run the service like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, it will choose port 9000. This is overridable, as is the location of the configuration file. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Dconfig.file=/path/to/application.conf -Dhttp.port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if java is not in your path, or you need to run against a different version of java, add the -java-home option as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -java-home /usr/lib/jvm/zulu-11-amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the service with Security&lt;/h2&gt; &#xA;&lt;p&gt;To add JAAS configuration for SASL, add the config file location at start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/cmak -Djava.security.auth.login.config=/path/to/my-jaas.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Make sure the user running CMAK (pka kafka manager) has read permissions on the jaas config file&lt;/p&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to create a Debian or RPM package instead, you can run one of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt debian:packageBin&#xA;&#xA;sbt rpm:packageBin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Most of the utils code has been adapted to work with &lt;a href=&#34;http://curator.apache.org&#34;&gt;Apache Curator&lt;/a&gt; from &lt;a href=&#34;http://kafka.apache.org&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Name and Management&lt;/h2&gt; &#xA;&lt;p&gt;CMAK was renamed from its previous name due to &lt;a href=&#34;https://github.com/yahoo/kafka-manager/issues/713&#34;&gt;this issue&lt;/a&gt;. CMAK is designed to be used with Apache Kafka and is offered to support the needs of the Kafka community. This project is currently managed by employees at Verizon Media and the community who supports this project.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the terms of the Apache License 2.0. See accompanying LICENSE file for terms.&lt;/p&gt; &#xA;&lt;h2&gt;Consumer/Producer Lag&lt;/h2&gt; &#xA;&lt;p&gt;Producer offset is polled. Consumer offset is read from the offset topic for Kafka based consumers. This means the reported lag may be negative since we are consuming offset from the offset topic faster then polling the producer offset. This is normal and not a problem.&lt;/p&gt; &#xA;&lt;h2&gt;Migration from Kafka Manager to CMAK&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy config files from old version to new version (application.conf, consumer.properties)&lt;/li&gt; &#xA; &lt;li&gt;Change start script to use bin/cmak instead of bin/kafka-manager&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>JohnSnowLabs/spark-nlp</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/JohnSnowLabs/spark-nlp</id>
    <link href="https://github.com/JohnSnowLabs/spark-nlp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State of the Art Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark NLP: State of the Art Natural Language Processing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/actions&#34; alt=&#34;build&#34;&gt; &lt;img src=&#34;https://github.com/JohnSnowLabs/spark-nlp/workflows/build/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34; alt=&#34;Current Release Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/JohnSnowLabs/spark-nlp.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/artifact/com.johnsnowlabs.nlp/spark-nlp_2.12&#34; alt=&#34;Maven Central&#34;&gt; &lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/com.johnsnowlabs.nlp/spark-nlp_2.12/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/spark-nlp&#34; alt=&#34;PyPI version&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/spark-nlp.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34; alt=&#34;Anaconda-Cloud&#34;&gt; &lt;img src=&#34;https://anaconda.org/johnsnowlabs/spark-nlp/badges/version.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/LICENSE&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34; alt=&#34;PyPi downloads&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/spark-nlp?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=pip%20downloads&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark. It provides &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;performant&lt;/strong&gt; &amp;amp; &lt;strong&gt;accurate&lt;/strong&gt; NLP annotations for machine learning pipelines that &lt;strong&gt;scale&lt;/strong&gt; easily in a distributed environment. Spark NLP comes with &lt;strong&gt;4000+&lt;/strong&gt; pretrained &lt;strong&gt;pipelines&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt; in more than &lt;strong&gt;200+&lt;/strong&gt; languages. It also offers tasks such as &lt;strong&gt;Tokenization&lt;/strong&gt;, &lt;strong&gt;Word Segmentation&lt;/strong&gt;, &lt;strong&gt;Part-of-Speech Tagging&lt;/strong&gt;, Word and Sentence &lt;strong&gt;Embeddings&lt;/strong&gt;, &lt;strong&gt;Named Entity Recognition&lt;/strong&gt;, &lt;strong&gt;Dependency Parsing&lt;/strong&gt;, &lt;strong&gt;Spell Checking&lt;/strong&gt;, &lt;strong&gt;Text Classification&lt;/strong&gt;, &lt;strong&gt;Sentiment Analysis&lt;/strong&gt;, &lt;strong&gt;Token Classification&lt;/strong&gt;, &lt;strong&gt;Machine Translation&lt;/strong&gt; (+180 languages), &lt;strong&gt;Summarization&lt;/strong&gt; &amp;amp; &lt;strong&gt;Question Answering&lt;/strong&gt;, &lt;strong&gt;Text Generation&lt;/strong&gt;, and many more &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;NLP tasks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark NLP&lt;/strong&gt; is the only open-source NLP library in &lt;strong&gt;production&lt;/strong&gt; that offers state-of-the-art transformers such as &lt;strong&gt;BERT&lt;/strong&gt;, &lt;strong&gt;ALBERT&lt;/strong&gt;, &lt;strong&gt;ELECTRA&lt;/strong&gt;, &lt;strong&gt;XLNet&lt;/strong&gt;, &lt;strong&gt;DistilBERT&lt;/strong&gt;, &lt;strong&gt;RoBERTa&lt;/strong&gt;, &lt;strong&gt;DeBERTa&lt;/strong&gt;, &lt;strong&gt;XLM-RoBERTa&lt;/strong&gt;, &lt;strong&gt;Longformer&lt;/strong&gt;, &lt;strong&gt;ELMO&lt;/strong&gt;, &lt;strong&gt;Universal Sentence Encoder&lt;/strong&gt;, &lt;strong&gt;Google T5&lt;/strong&gt;, &lt;strong&gt;MarianMT&lt;/strong&gt;, and &lt;strong&gt;GPT2&lt;/strong&gt; not only to &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt;, but also to &lt;strong&gt;JVM&lt;/strong&gt; ecosystem (&lt;strong&gt;Java&lt;/strong&gt;, &lt;strong&gt;Scala&lt;/strong&gt;, and &lt;strong&gt;Kotlin&lt;/strong&gt;) at &lt;strong&gt;scale&lt;/strong&gt; by extending &lt;strong&gt;Apache Spark&lt;/strong&gt; natively.&lt;/p&gt; &#xA;&lt;h2&gt;Project&#39;s website&lt;/h2&gt; &#xA;&lt;p&gt;Take a look at our official Spark NLP page: &lt;a href=&#34;http://nlp.johnsnowlabs.com/&#34;&gt;http://nlp.johnsnowlabs.com/&lt;/a&gt; for user documentation and examples&lt;/p&gt; &#xA;&lt;h2&gt;Community support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.johnsnowlabs.com/slack-redirect/&#34;&gt;Slack&lt;/a&gt; For live discussion with the Spark NLP community and the team&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp&#34;&gt;GitHub&lt;/a&gt; Bug reports, feature requests, and contributions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/discussions&#34;&gt;Discussions&lt;/a&gt; Engage with other community members, share ideas, and show off how you use Spark NLP!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/spark-nlp&#34;&gt;Medium&lt;/a&gt; Spark NLP articles&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos&#34;&gt;YouTube&lt;/a&gt; Spark NLP video tutorials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-spark-support&#34;&gt;Apache Spark Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala-and-python-support&#34;&gt;Scala &amp;amp; Python Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-support&#34;&gt;Databricks Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-support&#34;&gt;EMR Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#usage&#34;&gt;Using Spark NLP&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-packages&#34;&gt;Spark Packages&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#scala&#34;&gt;Scala&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#maven&#34;&gt;Maven&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#sbt&#34;&gt;SBT&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python&#34;&gt;Python&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipconda&#34;&gt;Pip/Conda&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;Compiled JARs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#apache-zeppelin&#34;&gt;Apache Zeppelin&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#jupyter-notebook-python&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#google-colab-notebook&#34;&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#kaggle-kernel&#34;&gt;Kaggle Kernel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#databricks-cluster&#34;&gt;Databricks Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#emr-cluster&#34;&gt;EMR Cluser&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#gcp-dataproc&#34;&gt;GCP Dataproc&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#spark-nlp-configuration&#34;&gt;Spark NLP Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines-and-models&#34;&gt;Pipelines &amp;amp; Models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#pipelines&#34;&gt;Pipelines&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#offline&#34;&gt;Offline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tokenization&lt;/li&gt; &#xA; &lt;li&gt;Trainable Word Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Stop Words Removal&lt;/li&gt; &#xA; &lt;li&gt;Token Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Document Normalizer&lt;/li&gt; &#xA; &lt;li&gt;Stemmer&lt;/li&gt; &#xA; &lt;li&gt;Lemmatizer&lt;/li&gt; &#xA; &lt;li&gt;NGrams&lt;/li&gt; &#xA; &lt;li&gt;Regex Matching&lt;/li&gt; &#xA; &lt;li&gt;Text Matching&lt;/li&gt; &#xA; &lt;li&gt;Chunking&lt;/li&gt; &#xA; &lt;li&gt;Date Matcher&lt;/li&gt; &#xA; &lt;li&gt;Sentence Detector&lt;/li&gt; &#xA; &lt;li&gt;Deep Sentence Detector (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Dependency parsing (Labeled/unlabeled)&lt;/li&gt; &#xA; &lt;li&gt;Part-of-speech tagging&lt;/li&gt; &#xA; &lt;li&gt;Sentiment Detection (ML models)&lt;/li&gt; &#xA; &lt;li&gt;Spell Checker (ML and DL models)&lt;/li&gt; &#xA; &lt;li&gt;Word Embeddings (GloVe and Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;Doc2Vec (based on Word2Vec)&lt;/li&gt; &#xA; &lt;li&gt;BERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;CamemBERT Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa Embeddings (HuggingFace v2 &amp;amp; v3 models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Longformer Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;ALBERT Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLNet Embeddings&lt;/li&gt; &#xA; &lt;li&gt;ELMO Embeddings (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;Universal Sentence Encoder (TF Hub models)&lt;/li&gt; &#xA; &lt;li&gt;BERT Sentence Embeddings (TF Hub &amp;amp; HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBerta Sentence Embeddings (HuggingFace models)&lt;/li&gt; &#xA; &lt;li&gt;Sentence Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Chunk Embeddings&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised keywords extraction&lt;/li&gt; &#xA; &lt;li&gt;Language Detection &amp;amp; Identification (up to 375 languages)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-label Sentiment analysis (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Multi-class Text Classification (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;BERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DistilBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;ALBERT for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;DeBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLM-RoBERTa for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;XLNet for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Longformer for Token &amp;amp; Sequence Classification&lt;/li&gt; &#xA; &lt;li&gt;Neural Machine Translation (MarianMT)&lt;/li&gt; &#xA; &lt;li&gt;Text-To-Text Transfer Transformer (Google T5)&lt;/li&gt; &#xA; &lt;li&gt;Generative Pre-trained Transformer 2 (OpenAI GPT2)&lt;/li&gt; &#xA; &lt;li&gt;Named entity recognition (Deep learning)&lt;/li&gt; &#xA; &lt;li&gt;Easy TensorFlow integration&lt;/li&gt; &#xA; &lt;li&gt;GPU Support&lt;/li&gt; &#xA; &lt;li&gt;Full integration with Spark ML functions&lt;/li&gt; &#xA; &lt;li&gt;+3200 pre-trained models in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;+1700 pre-trained pipelines in +200 languages!&lt;/li&gt; &#xA; &lt;li&gt;Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, and Urdu.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;To use Spark NLP you need the following requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java 8 and 11&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark 3.2.x, 3.1.x, 3.0.x, 2.4.x, or 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Java 11 is only supported if you are using Spark NLP with Spark/PySpark 3.x and above&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU (optional):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 is built with TensorFlow 2.4.1 and requires the followings if you need GPU support&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA11 and cuDNN 8.0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This is a quick example of how to use Spark NLP pre-trained pipeline in Python and PySpark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ java -version&#xA;# should be Java 8 (Oracle or OpenJDK)&#xA;$ conda create -n sparknlp python=3.7 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Python console or Jupyter &lt;code&gt;Python3&lt;/code&gt; kernel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import Spark NLP&#xA;from sparknlp.base import *&#xA;from sparknlp.annotator import *&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;import sparknlp&#xA;&#xA;# Start SparkSession with Spark NLP&#xA;# start() functions has 5 parameters: gpu, spark23, spark24, spark32, and memory&#xA;# sparknlp.start(gpu=True) will start the session with GPU support&#xA;# sparknlp.start(spark23=True) is when you have Apache Spark 2.3.x installed&#xA;# sparknlp.start(spark24=True) is when you have Apache Spark 2.4.x installed&#xA;# sparknlp.start(spark32=True) is when you have Apache Spark 3.2.x installed&#xA;# sparknlp.start(memory=&#34;16G&#34;) to change the default driver memory in SparkSession&#xA;spark = sparknlp.start()&#xA;&#xA;# Download a pre-trained pipeline&#xA;pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;&#xA;# Your testing dataset&#xA;text = &#34;&#34;&#34;&#xA;The Mona Lisa is a 16th century oil painting created by Leonardo.&#xA;It&#39;s held at the Louvre in Paris.&#xA;&#34;&#34;&#34;&#xA;&#xA;# Annotate your testing dataset&#xA;result = pipeline.annotate(text)&#xA;&#xA;# What&#39;s in the pipeline&#xA;list(result.keys())&#xA;Output: [&#39;entities&#39;, &#39;stem&#39;, &#39;checked&#39;, &#39;lemma&#39;, &#39;document&#39;,&#xA;&#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;]&#xA;&#xA;# Check the results&#xA;result[&#39;entities&#39;]&#xA;Output: [&#39;Mona Lisa&#39;, &#39;Leonardo&#39;, &#39;Louvre&#39;, &#39;Paris&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, you can visit our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;repository&lt;/a&gt; to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;h2&gt;Apache Spark Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP &lt;em&gt;3.4.4&lt;/em&gt; has been built on top of Apache Spark 3.x while fully supports Apache Spark 2.3.x, 2.4.x, 3.0.x, 3.1.x, and 3.2.x:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.3.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 2.4.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.0.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.1.x&lt;/th&gt; &#xA;   &lt;th&gt;Apache Spark 3.2.x&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;Partially&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starting 3.0.0 release, the default &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;spark-nlp-gpu&lt;/code&gt; pacakges are based on Scala 2.12 and Apache Spark 3.x by default.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Find out more about &lt;code&gt;Spark NLP&lt;/code&gt; versions from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Scala and Python Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Spark NLP&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.6&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.7&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.8&lt;/th&gt; &#xA;   &lt;th&gt;Python 3.9&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.11&lt;/th&gt; &#xA;   &lt;th&gt;Scala 2.12&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.3.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.8.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.7.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.6.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.5.x&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;   &lt;td&gt;YES&lt;/td&gt; &#xA;   &lt;td&gt;NO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Databricks Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following runtimes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5.5 LTS&lt;/li&gt; &#xA; &lt;li&gt;5.5 LTS ML&lt;/li&gt; &#xA; &lt;li&gt;6.4&lt;/li&gt; &#xA; &lt;li&gt;6.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.3&lt;/li&gt; &#xA; &lt;li&gt;7.3 ML&lt;/li&gt; &#xA; &lt;li&gt;7.4&lt;/li&gt; &#xA; &lt;li&gt;7.4 ML&lt;/li&gt; &#xA; &lt;li&gt;7.5&lt;/li&gt; &#xA; &lt;li&gt;7.5 ML&lt;/li&gt; &#xA; &lt;li&gt;7.6&lt;/li&gt; &#xA; &lt;li&gt;7.6 ML&lt;/li&gt; &#xA; &lt;li&gt;8.0&lt;/li&gt; &#xA; &lt;li&gt;8.0 ML&lt;/li&gt; &#xA; &lt;li&gt;8.1&lt;/li&gt; &#xA; &lt;li&gt;8.1 ML&lt;/li&gt; &#xA; &lt;li&gt;8.2&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML&lt;/li&gt; &#xA; &lt;li&gt;8.3&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML&lt;/li&gt; &#xA; &lt;li&gt;8.4&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML&lt;/li&gt; &#xA; &lt;li&gt;9.0&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML&lt;/li&gt; &#xA; &lt;li&gt;9.1&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.0&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML&lt;/li&gt; &#xA; &lt;li&gt;10.1&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML&lt;/li&gt; &#xA; &lt;li&gt;10.2&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML&lt;/li&gt; &#xA; &lt;li&gt;10.3&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.3 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;8.4 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;9.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.0 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.1 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.2 ML &amp;amp; GPU&lt;/li&gt; &#xA; &lt;li&gt;10.3 ML &amp;amp; GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Spark NLP 3.4.4 is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 8.x and above as listed under GPU.&lt;/p&gt; &#xA;&lt;h2&gt;EMR Support&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP 3.4.4 has been tested and is compatible with the following EMR releases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;emr-5.20.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.21.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.22.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.23.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.24.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.25.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.26.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.27.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.28.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.29.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.30.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.31.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.32.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.0&lt;/li&gt; &#xA; &lt;li&gt;emr-5.33.1&lt;/li&gt; &#xA; &lt;li&gt;emr-5.34.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.1.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.2.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.3.1&lt;/li&gt; &#xA; &lt;li&gt;emr-6.4.0&lt;/li&gt; &#xA; &lt;li&gt;emr-6.5.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html&#34;&gt;Amazon EMR 5.x releases&lt;/a&gt; Full list of &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html&#34;&gt;Amazon EMR 6.x releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.4.4&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h2&gt;Packages Cheatsheet&lt;/h2&gt; &#xA;&lt;p&gt;This is a cheatsheet for corresponding Spark NLP Maven package to Apache Spark / PySpark major version:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Apache Spark&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on CPU&lt;/th&gt; &#xA;   &lt;th&gt;Spark NLP on GPU&lt;/th&gt; &#xA;   &lt;th&gt;Start()&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.0.x/3.1.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start()&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.2.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark32&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark32=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark24=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.3.x&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;sparknlp.start(spark23=True)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Spark Packages&lt;/h2&gt; &#xA;&lt;h3&gt;Command line (requires internet connection)&lt;/h3&gt; &#xA;&lt;p&gt;Spark NLP supports all major releases of Apache Spark 2.3.x, Apache Spark 2.4.x, Apache Spark 3.0.x, Apache Spark 3.1.x, and Apache Spark 3.2.x. That&#39;s being said, you need to choose the right package name for the right Apache Spark major release:&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.x (3.0.x and 3.1.x - Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 3.2.x (Scala 2.12)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark32_2.12:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.4.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark24&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Apache Spark 2.3.x (Scala 2.11)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# GPU&#xA;&#xA;spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.4.4&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spark-nlp-gpu-spark23&lt;/code&gt; has been published to the &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#34;&gt;Maven Repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scala&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x, 3.1.x, and 3.2.x versions. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Maven&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark32_2.12&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark24_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.johnsnowlabs.nlp&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-nlp-gpu-spark23_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;3.4.4&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.0.x and 3.1.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 3.2.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark32&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark32&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.4.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark24&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp&lt;/strong&gt; on Apache Spark 2.3.x:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-nlp-gpu:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbtshell&#34;&gt;// https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23&#xA;libraryDependencies += &#34;com.johnsnowlabs.nlp&#34; %% &#34;spark-nlp-gpu-spark23&#34; % &#34;3.4.4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maven Central: &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects &lt;a href=&#34;https://github.com/maziyarpanahi/spark-nlp-starter&#34;&gt;Spark NLP SBT Starter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP supports Python 3.6.x and above depending on your major PySpark version.&lt;/p&gt; &#xA;&lt;h3&gt;Python without explicit Pyspark installation&lt;/h3&gt; &#xA;&lt;h3&gt;Pip/Conda&lt;/h3&gt; &#xA;&lt;p&gt;If you installed pyspark through pip/conda, you can install &lt;code&gt;spark-nlp&lt;/code&gt; through the same channel.&lt;/p&gt; &#xA;&lt;p&gt;Pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PyPI &lt;a href=&#34;https://pypi.org/project/spark-nlp/&#34;&gt;spark-nlp package&lt;/a&gt; / Anaconda &lt;a href=&#34;https://anaconda.org/JohnSnowLabs/spark-nlp&#34;&gt;spark-nlp package&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you&#39;ll have to create a SparkSession either from Spark NLP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;&#xA;spark = sparknlp.start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[4]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using local jars, you can use &lt;code&gt;spark.jars&lt;/code&gt; instead for comma-delimited jar files. For cluster setups, of course, you&#39;ll have to put the jars in a reachable location for all driver and executor nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sparknlp&#xA;from sparknlp.pretrained import PretrainedPipeline&#xA;&#xA;#create or get Spark Session&#xA;&#xA;spark = sparknlp.start()&#xA;&#xA;sparknlp.version()&#xA;spark.version&#xA;&#xA;#download, load and annotate a text by pre-trained pipeline&#xA;&#xA;pipeline = PretrainedPipeline(&#39;recognize_entities_dl&#39;, &#39;en&#39;)&#xA;result = pipeline.annotate(&#39;The Mona Lisa is a 16th century oil painting created by Leonardo&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compiled JARs&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;h4&gt;spark-nlp&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.0.x and 3.1.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 3.2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark32=true -Dis_gpu=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.4.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark24=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for CPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FAT-JAR for GPU on Apache Spark 2.3.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt -Dis_gpu=true -Dis_spark23=true assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the jar manually&lt;/h3&gt; &#xA;&lt;p&gt;If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from &lt;a href=&#34;https://mvnrepository.com/artifact/com.johnsnowlabs.nlp&#34;&gt;Maven Central&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To add JARs to spark programs use the &lt;code&gt;--jars&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell --jars spark-nlp.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The preferred way to use the library when running spark programs is using the &lt;code&gt;--packages&lt;/code&gt; option as specified in the &lt;code&gt;spark-packages&lt;/code&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Apache Zeppelin&lt;/h2&gt; &#xA;&lt;p&gt;Use either one of the following options&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the following Maven Coordinates to the interpreter&#39;s library list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add a path to pre-built jar from &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#compiled-jars&#34;&gt;here&lt;/a&gt; in the interpreter&#39;s library list making sure the jar is available to driver path&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Python in Zeppelin&lt;/h3&gt; &#xA;&lt;p&gt;Apart from the previous step, install the python module through pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install spark-nlp==3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can install &lt;code&gt;spark-nlp&lt;/code&gt; from inside Zeppelin by using Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python.conda install -c johnsnowlabs spark-nlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose.&lt;/p&gt; &#xA;&lt;p&gt;Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. &lt;code&gt;python3&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;An alternative option would be to set &lt;code&gt;SPARK_SUBMIT_OPTIONS&lt;/code&gt; (zeppelin-env.sh) and make sure &lt;code&gt;--packages&lt;/code&gt; is there as shown earlier since it includes both scala and python side installation.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook (Python)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recomended:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to get this done on Linux and macOS is to simply install &lt;code&gt;spark-nlp&lt;/code&gt; and &lt;code&gt;pyspark&lt;/code&gt; PyPI packages and launch the Jupyter from the same Python environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ conda create -n sparknlp python=3.8 -y&#xA;$ conda activate sparknlp&#xA;# spark-nlp by default is based on pyspark 3.x&#xA;$ pip install spark-nlp==3.4.4 pyspark==3.1.2 jupyter&#xA;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The you can use &lt;code&gt;python3&lt;/code&gt; kernel to run your code with creating SparkSession via &lt;code&gt;spark = sparknlp.start()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export SPARK_HOME=/path/to/your/spark/folder&#xA;export PYSPARK_PYTHON=python3&#xA;export PYSPARK_DRIVER_PYTHON=jupyter&#xA;export PYSPARK_DRIVER_PYTHON_OPTS=notebook&#xA;&#xA;pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can mix in using &lt;code&gt;--jars&lt;/code&gt; option for pyspark + &lt;code&gt;pip install spark-nlp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If not using pyspark at all, you&#39;ll have to run the instructions pointed &lt;a href=&#34;https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/#python-without-explicit-Pyspark-installation&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Google Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code in Google Colab notebook and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# This is only to setup PySpark and Spark NLP on Colab&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script comes with the two options to define &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-nlp&lt;/code&gt; versions via options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# -p is for pyspark&#xA;# -s is for spark-nlp&#xA;# by default they are set to the latest&#xA;!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.1.2 -s 3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/quick_start_google_colab.ipynb&#34;&gt;Spark NLP quick start on Google Colab&lt;/a&gt; is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines.&lt;/p&gt; &#xA;&lt;h2&gt;Kaggle Kernel&lt;/h2&gt; &#xA;&lt;p&gt;Run the following code in Kaggle Kernel and start using spark-nlp right away.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Let&#39;s setup Kaggle for Spark NLP and PySpark&#xA;!wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/mozzie/spark-nlp-named-entity-recognition&#34;&gt;Spark NLP quick start on Kaggle Kernel&lt;/a&gt; is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Databricks Cluster&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a cluster if you don&#39;t have one already&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;code&gt;Libraries&lt;/code&gt; tab inside your cluster you need to follow these steps:&lt;/p&gt; &lt;p&gt;3.1. Install New -&amp;gt; PyPI -&amp;gt; &lt;code&gt;spark-nlp==3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;p&gt;3.2. Install New -&amp;gt; Maven -&amp;gt; Coordinates -&amp;gt; &lt;code&gt;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&lt;/code&gt; -&amp;gt; Install&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now you can attach your notebook to the cluster and use Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;NOTE: Databrick&#39;s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name for your runtime from our &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp#packages-cheatsheet&#34;&gt;Pacakges Chetsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;EMR Cluster&lt;/h2&gt; &#xA;&lt;p&gt;To launch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration.&lt;/p&gt; &#xA;&lt;p&gt;A sample of your bootstrap script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;#!/bin/bash&#xA;set -x -e&#xA;&#xA;echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3&#xA;export HADOOP_CONF_DIR=/etc/hadoop/conf&#xA;export SPARK_JARS_DIR=/usr/lib/spark/jars&#xA;export SPARK_HOME=/usr/lib/spark&#39; &amp;gt;&amp;gt; $HOME/.bashrc &amp;amp;&amp;amp; source $HOME/.bashrc&#xA;&#xA;sudo python3 -m pip install awscli boto spark-nlp&#xA;&#xA;set +x&#xA;exit 0&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of your software configuration in JSON on S3 (must be public access):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.json&#34;&gt;[{&#xA;  &#34;Classification&#34;: &#34;spark-env&#34;,&#xA;  &#34;Configurations&#34;: [{&#xA;    &#34;Classification&#34;: &#34;export&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;PYSPARK_PYTHON&#34;: &#34;/usr/bin/python3&#34;&#xA;    }&#xA;  }]&#xA;},&#xA;{&#xA;  &#34;Classification&#34;: &#34;spark-defaults&#34;,&#xA;    &#34;Properties&#34;: {&#xA;      &#34;spark.yarn.stagingDir&#34;: &#34;hdfs:///tmp&#34;,&#xA;      &#34;spark.yarn.preserve.staging.files&#34;: &#34;true&#34;,&#xA;      &#34;spark.kryoserializer.buffer.max&#34;: &#34;2000M&#34;,&#xA;      &#34;spark.serializer&#34;: &#34;org.apache.spark.serializer.KryoSerializer&#34;,&#xA;      &#34;spark.driver.maxResultSize&#34;: &#34;0&#34;,&#xA;      &#34;spark.jars.packages&#34;: &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;&#xA;    }&#xA;}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A sample of AWS CLI to launch EMR cluster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.sh&#34;&gt;aws emr create-cluster \&#xA;--name &#34;Spark NLP 3.4.4&#34; \&#xA;--release-label emr-6.2.0 \&#xA;--applications Name=Hadoop Name=Spark Name=Hive \&#xA;--instance-type m4.4xlarge \&#xA;--instance-count 3 \&#xA;--use-default-roles \&#xA;--log-uri &#34;s3://&amp;lt;S3_BUCKET&amp;gt;/&#34; \&#xA;--bootstrap-actions Path=s3://&amp;lt;S3_BUCKET&amp;gt;/emr-bootstrap.sh,Name=custome \&#xA;--configurations &#34;https://&amp;lt;public_access&amp;gt;/sparknlp-config.json&#34; \&#xA;--ec2-attributes KeyName=&amp;lt;your_ssh_key&amp;gt;,EmrManagedMasterSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt;,EmrManagedSlaveSecurityGroup=&amp;lt;security_group_with_ssh&amp;gt; \&#xA;--profile &amp;lt;aws_profile_credentials&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GCP Dataproc&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a cluster if you don&#39;t have one already as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At gcloud shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud services enable dataproc.googleapis.com \&#xA;  compute.googleapis.com \&#xA;  storage-component.googleapis.com \&#xA;  bigquery.googleapis.com \&#xA;  bigquerystorage.googleapis.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REGION=&amp;lt;region&amp;gt;&#xA;ZONE=&amp;lt;zone&amp;gt;&#xA;CLUSTER_NAME=&amp;lt;cluster_name&amp;gt;&#xA;BUCKET_NAME=&amp;lt;bucket_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. Don&#39;t forget to set the maven coordinates for the jar in properties.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud dataproc clusters create ${CLUSTER_NAME} \&#xA;  --region=${REGION} \&#xA;  --zone=${ZONE} \&#xA;  --image-version=2.0 \&#xA;  --master-machine-type=n1-standard-4 \&#xA;  --worker-machine-type=n1-standard-2 \&#xA;  --master-boot-disk-size=128GB \&#xA;  --worker-boot-disk-size=128GB \&#xA;  --num-workers=2 \&#xA;  --bucket=${BUCKET_NAME} \&#xA;  --optional-components=JUPYTER \&#xA;  --enable-component-gateway \&#xA;  --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; \&#xA;  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh \&#xA;  --properties spark:spark.serializer=org.apache.spark.serializer.KryoSerializer,spark:spark.driver.maxResultSize=0,spark:spark.kryoserializer.buffer.max=2000M,spark:spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, you can attach your notebook to the cluster and use the Spark NLP!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Spark NLP Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You can change the following Spark NLP configurations via Spark Configuration:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Property Name&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Meaning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.pretrained.cache_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/cache_pretrained&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to download and exctract pretrained &lt;code&gt;Models&lt;/code&gt; and &lt;code&gt;Pipelines&lt;/code&gt;. By default, it will be in User&#39;s Home directory under &lt;code&gt;cache_pretrained&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.storage.cluster_tmp_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;hadoop.tmp.dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to use on a cluster for temporarily files such as unpacking indexes for WordEmbeddings. By default, this locations is the location of &lt;code&gt;hadoop.tmp.dir&lt;/code&gt; set via Hadoop configuration for Apache Spark. NOTE: &lt;code&gt;S3&lt;/code&gt; is not supported and it must be local, HDFS, or DBFS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.annotator.log_folder&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;~/annotator_logs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The location to save logs from annotators during training such as &lt;code&gt;NerDLApproach&lt;/code&gt;, &lt;code&gt;ClassifierDLApproach&lt;/code&gt;, &lt;code&gt;SentimentDLApproach&lt;/code&gt;, &lt;code&gt;MultiClassifierDLApproach&lt;/code&gt;, etc. By default, it will be in User&#39;s Home directory under &lt;code&gt;annotator_logs&lt;/code&gt; directory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.access_key_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.secret_access_key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS secret access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.credentials.session_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS MFA session token to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.s3_bucket&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;spark.jsl.settings.aws.region&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Your AWS region to use your S3 bucket to store log files of training models or access tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How to set Spark NLP Configuration&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparkSession:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;.config()&lt;/code&gt; during SparkSession creation to set Spark NLP configurations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder \&#xA;        .master(&#34;local[*]&#34;) \&#xA;        .config(&#34;spark.driver.memory&#34;, &#34;16G&#34;) \&#xA;        .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;        .config(&#34;spark.serializer&#34;, &#34;org.apache.spark.serializer.KryoSerializer&#34;) \&#xA;        .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000m&#34;) \&#xA;        .config(&#34;spark.jsl.settings.pretrained.cache_folder&#34;, &#34;sample_data/pretrained&#34;) \&#xA;        .config(&#34;spark.jsl.settings.storage.cluster_tmp_dir&#34;, &#34;sample_data/storage&#34;) \&#xA;        .config(&#34;spark.jars.packages&#34;, &#34;com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#34;) \&#xA;        .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;spark-shell:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;spark-shell \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;pyspark:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pyspark \&#xA;  --driver-memory 16g \&#xA;  --conf spark.driver.maxResultSize=0 \&#xA;  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer&#xA;  --conf spark.kryoserializer.buffer.max=2000M \&#xA;  --conf spark.jsl.settings.pretrained.cache_folder=&#34;sample_data/pretrained&#34; \&#xA;  --conf spark.jsl.settings.storage.cluster_tmp_dir=&#34;sample_data/storage&#34; \&#xA;  --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Databricks:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On a new cluster or existing one you need to add the following to the &lt;code&gt;Advanced Options -&amp;gt; Spark&lt;/code&gt; tab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.kryoserializer.buffer.max 2000M&#xA;spark.serializer org.apache.spark.serializer.KryoSerializer&#xA;spark.jsl.settings.pretrained.cache_folder dbfs:/PATH_TO_CACHE&#xA;spark.jsl.settings.storage.cluster_tmp_dir dbfs:/PATH_TO_STORAGE&#xA;spark.jsl.settings.annotator.log_folder dbfs:/PATH_TO_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If this is an existing cluster, after adding new configs or changing existing properties you need to restart it.&lt;/p&gt; &#xA;&lt;h3&gt;S3 Integration&lt;/h3&gt; &#xA;&lt;p&gt;In Spark NLP we can define S3 locations to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Export log files of training models&lt;/li&gt; &#xA; &lt;li&gt;Store tensorflow graphs used in &lt;code&gt;NerDLApproach&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Logging:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To configure S3 path for logging while training models. We need to set up AWS credentials as well as an S3 path&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.annotator.log_folder&#34;, &#34;s3://my/s3/path/logs&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.s3_bucket&#34;, &#34;my.bucket&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can check the log on your S3 path defined in &lt;em&gt;spark.jsl.settings.annotator.log_folder&lt;/em&gt; property. Make sure to use the prefix &lt;em&gt;s3://&lt;/em&gt;, otherwise it will use the default configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tensorflow Graphs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To reference S3 location for downloading graphs. We need to set up AWS credentials&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.access_key_id&#34;, &#34;MY_KEY_ID&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.secret_access_key&#34;, &#34;MY_SECRET_ACCESS_KEY&#34;)&#xA;spark.conf.set(&#34;spark.jsl.settings.aws.region&#34;, &#34;my-region&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;MFA Configuration:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In case your AWS account is configured with MFA. You will need first to get temporal credentials and add session token to the configuration as shown in the examples below For logging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark.conf.set(&#34;spark.jsl.settings.aws.credentials.session_token&#34;, &#34;MY_TOKEN&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of a bash script that gets temporal AWS credentials can be found &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/raw/master/scripts/aws_tmp_credentials.sh&#34;&gt;here&lt;/a&gt; This script requires three arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./aws_tmp_credentials.sh iam_user duration serial_number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pipelines and Models&lt;/h2&gt; &#xA;&lt;h3&gt;Pipelines&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;&#xA;SparkNLP.version()&#xA;&#xA;val testData = spark.createDataFrame(Seq(&#xA;(1, &#34;Google has announced the release of a beta version of the popular TensorFlow machine learning library&#34;),&#xA;(2, &#34;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&#34;)&#xA;)).toDF(&#34;id&#34;, &#34;text&#34;)&#xA;&#xA;val pipeline = PretrainedPipeline(&#34;explain_document_dl&#34;, lang=&#34;en&#34;)&#xA;&#xA;val annotation = pipeline.transform(testData)&#xA;&#xA;annotation.show()&#xA;/*&#xA;import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline&#xA;import com.johnsnowlabs.nlp.SparkNLP&#xA;2.5.0&#xA;testData: org.apache.spark.sql.DataFrame = [id: int, text: string]&#xA;pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models)&#xA;annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields]&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;| id|                text|            document|               token|            sentence|             checked|               lemma|                stem|                 pos|          embeddings|                 ner|            entities|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;|  1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...|&#xA;|  2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...|&#xA;+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+&#xA;*/&#xA;&#xA;annotation.select(&#34;entities.result&#34;).show(false)&#xA;&#xA;/*&#xA;+----------------------------------+&#xA;|result                            |&#xA;+----------------------------------+&#xA;|[Google, TensorFlow]              |&#xA;|[Donald John Trump, United States]|&#xA;+----------------------------------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Pipelines&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all of the available Pipelines of a particular language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;)&#xA;/*&#xA;+--------------------------------------------+------+---------+&#xA;| Pipeline                                   | lang | version |&#xA;+--------------------------------------------+------+---------+&#xA;| dependency_parse                           |  en  | 2.0.2   |&#xA;| analyze_sentiment_ml                       |  en  | 2.0.2   |&#xA;| check_spelling                             |  en  | 2.1.0   |&#xA;| match_datetime                             |  en  | 2.1.0   |&#xA;                               ...&#xA;| explain_document_ml                        |  en  | 3.1.3   |&#xA;+--------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicPipelines(lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+---------------------------------------+------+---------+&#xA;| Pipeline                              | lang | version |&#xA;+---------------------------------------+------+---------+&#xA;| dependency_parse                      |  en  | 2.0.2   |&#xA;                               ...&#xA;| clean_slang                           |  en  | 3.0.0   |&#xA;| clean_pattern                         |  en  | 3.0.0   |&#xA;| check_spelling                        |  en  | 3.0.0   |&#xA;| dependency_parse                      |  en  | 3.0.0   |&#xA;+---------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained pipelines&lt;/a&gt; with examples, demos, benchmarks, and more&lt;/h4&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some selected languages:&lt;/strong&gt; &lt;code&gt;Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick online example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load NER model trained by deep learning approach and GloVe word embeddings&#xA;ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;)&#xA;# load NER model trained by deep learning approach and BERT word embeddings&#xA;ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// load French POS tagger model trained by Universal Dependencies&#xA;val french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;// load Italain LemmatizerModel&#xA;val italian_lemma = LemmatizerModel.pretrained(&#34;lemma_dxc&#34;, lang=&#34;it&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick offline example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading &lt;code&gt;PerceptronModel&lt;/code&gt; annotator model inside Spark NLP Pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Showing Available Models&lt;/h4&gt; &#xA;&lt;p&gt;There are functions in Spark NLP that will list all the available Models of a particular Annotator and language for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;)&#xA;/*&#xA;+---------------------------------------------+------+---------+&#xA;| Model                                       | lang | version |&#xA;+---------------------------------------------+------+---------+&#xA;| onto_100                                    |  en  | 2.1.0   |&#xA;| onto_300                                    |  en  | 2.1.0   |&#xA;| ner_dl_bert                                 |  en  | 2.2.0   |&#xA;| onto_100                                    |  en  | 2.4.0   |&#xA;| ner_conll_elmo                              |  en  | 3.2.2   |&#xA;+---------------------------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if we want to check for a particular version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showPublicModels(annotator=&#34;NerDLModel&#34;, lang=&#34;en&#34;, version=&#34;3.1.0&#34;)&#xA;/*&#xA;+----------------------------+------+---------+&#xA;| Model                      | lang | version |&#xA;+----------------------------+------+---------+&#xA;| onto_100                   |  en  | 2.1.0   |&#xA;| ner_aspect_based_sentiment |  en  | 2.6.2   |&#xA;| ner_weibo_glove_840B_300d  |  en  | 2.6.2   |&#xA;| nerdl_atis_840b_300d       |  en  | 2.7.1   |&#xA;| nerdl_snips_100d           |  en  | 2.7.3   |&#xA;+----------------------------+------+---------+&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And to see a list of available annotators, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.johnsnowlabs.nlp.pretrained.ResourceDownloader&#xA;&#xA;ResourceDownloader.showAvailableAnnotators()&#xA;/*&#xA;AlbertEmbeddings&#xA;AlbertForTokenClassification&#xA;AssertionDLModel&#xA;...&#xA;XlmRoBertaSentenceEmbeddings&#xA;XlnetEmbeddings&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Please check out our Models Hub for the full list of &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;pre-trained models&lt;/a&gt; with examples, demo, benchmark, and more&lt;/h4&gt; &#xA;&lt;h2&gt;Offline&lt;/h2&gt; &#xA;&lt;p&gt;Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instead of using the Maven package, you need to load our Fat JAR&lt;/li&gt; &#xA; &lt;li&gt;Instead of using PretrainedPipeline for pretrained pipelines or the &lt;code&gt;.pretrained()&lt;/code&gt; function to download pretrained models, you will need to manually download your pipeline/model from &lt;a href=&#34;https://nlp.johnsnowlabs.com/models&#34;&gt;Models Hub&lt;/a&gt;, extract it, and load it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of &lt;code&gt;SparkSession&lt;/code&gt; with Fat JAR to have Spark NLP offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;spark = SparkSession.builder \&#xA;    .appName(&#34;Spark NLP&#34;)\&#xA;    .master(&#34;local[*]&#34;)\&#xA;    .config(&#34;spark.driver.memory&#34;,&#34;16G&#34;)\&#xA;    .config(&#34;spark.driver.maxResultSize&#34;, &#34;0&#34;) \&#xA;    .config(&#34;spark.kryoserializer.buffer.max&#34;, &#34;2000M&#34;)\&#xA;    .config(&#34;spark.jars&#34;, &#34;/tmp/spark-nlp-assembly-3.4.4.jar&#34;)\&#xA;    .getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download provided Fat JARs from each &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp/releases&#34;&gt;release notes&lt;/a&gt;, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x)&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/spark-nlp-assembly-3.4.4.jar&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of using pretrained Models and Pipelines in offline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# instead of using pretrained() for online:&#xA;# french_pos = PerceptronModel.pretrained(&#34;pos_ud_gsd&#34;, lang=&#34;fr&#34;)&#xA;# you download this model, extract it, and use .load&#xA;french_pos = PerceptronModel.load(&#34;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&#34;)\&#xA;      .setInputCols(&#34;document&#34;, &#34;token&#34;)\&#xA;      .setOutputCol(&#34;pos&#34;)&#xA;&#xA;# example for pipelines&#xA;# instead of using PretrainedPipeline&#xA;# pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;)&#xA;# you download this pipeline, extract it, and use PipelineModel&#xA;PipelineModel.load(&#34;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you&lt;/li&gt; &#xA; &lt;li&gt;If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., &lt;code&gt;hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Need more &lt;strong&gt;examples&lt;/strong&gt;? Check out our dedicated &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;Spark NLP Showcase&lt;/a&gt; repository to showcase all Spark NLP use cases!&lt;/p&gt; &#xA;&lt;p&gt;Also, don&#39;t forget to check &lt;a href=&#34;https://nlp.johnsnowlabs.com/demo&#34;&gt;Spark NLP in Action&lt;/a&gt; built by Streamlit.&lt;/p&gt; &#xA;&lt;h3&gt;All examples: &lt;a href=&#34;https://github.com/JohnSnowLabs/spark-nlp-workshop&#34;&gt;spark-nlp-workshop&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nlp.johnsnowlabs.com/learn&#34;&gt;Check our Articles and Videos page here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;We have published a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2665963821000063&#34;&gt;paper&lt;/a&gt; that you can cite for the Spark NLP library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{KOCAMAN2021100058,&#xA;    title = {Spark NLP: Natural language understanding at scale},&#xA;    journal = {Software Impacts},&#xA;    pages = {100058},&#xA;    year = {2021},&#xA;    issn = {2665-9638},&#xA;    doi = {https://doi.org/10.1016/j.simpa.2021.100058},&#xA;    url = {https://www.sciencedirect.com/science/article/pii/S2665963.2.100063},&#xA;    author = {Veysel Kocaman and David Talby},&#xA;    keywords = {Spark, Natural language processing, Deep learning, Tensorflow, Cluster},&#xA;    abstract = {Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant &amp;amp; accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world‚Äôs most widely used NLP library in the enterprise.}&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate any sort of contributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ideas&lt;/li&gt; &#xA; &lt;li&gt;feedback&lt;/li&gt; &#xA; &lt;li&gt;documentation&lt;/li&gt; &#xA; &lt;li&gt;bug reports&lt;/li&gt; &#xA; &lt;li&gt;NLP training and testing corpora&lt;/li&gt; &#xA; &lt;li&gt;Development and testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone the repo and submit your pull-requests! Or directly create issues in this repo.&lt;/p&gt; &#xA;&lt;h2&gt;John Snow Labs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://johnsnowlabs.com&#34;&gt;http://johnsnowlabs.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TheHive-Project/TheHive</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/TheHive-Project/TheHive</id>
    <link href="https://github.com/TheHive-Project/TheHive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TheHive: a Scalable, Open Source and Free Security Incident Response Platform&lt;/p&gt;&lt;hr&gt;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/thehive-logo.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://chat.thehive-project.org&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-7289da.svg?sanitize=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href&gt;&lt;img src=&#34;https://drone.strangebee.com/api/badges/TheHive-Project/TheHive/status.svg?ref=refs/heads/master-th4&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/LICENSE&#34; target&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/TheHive-Project/TheHive&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;TheHive&lt;/a&gt; is a scalable 3-in-1 open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. It is the perfect companion to &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt;. You can synchronize it with one or multiple MISP instances to start investigations out of MISP events. You can also export an investigation&#39;s results as a MISP event to help your peers detect and react to attacks you&#39;ve dealt with. Additionally, when TheHive is used in conjunction with &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt;, security analysts and researchers can easily analyze tens if not hundred of observables.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Current_cases.png&#34; alt=&#34;Current Cases View&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Collaborate&lt;/h2&gt; &#xA;&lt;p&gt;Collaboration is at the heart of TheHive.&lt;/p&gt; &#xA;&lt;p&gt;Multiple analysts from one organisations can work together on the same case simultaneously. For example, an analyst may deal with malware analysis while another may work on tracking C2 beaconing activity on proxy logs as soon as IOCs have been added by their coworker. Using TheHive&#39;s live stream, everyone can keep an eye on what&#39;s happening on the platform, in real time.&lt;/p&gt; &#xA;&lt;p&gt;Multi-tenancy and fine grained user profiles let organisations and analysts work and collaborate on a same case accross organisations. For example, one case can be created by a first organisation who start investigating and ask for contribution from other teams or escalate to another organisation.&lt;/p&gt; &#xA;&lt;h2&gt;Elaborate&lt;/h2&gt; &#xA;&lt;p&gt;Within TheHive, every investigation corresponds to a case. Cases can be created from scratch or from &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; events, SIEM alerts, email reports and any other noteworthy source of security events.&lt;/p&gt; &#xA;&lt;p&gt;Each case can be broken down into one or more tasks. Instead of adding the same tasks to a given type of case every time one is created, analysts can use TheHive&#39;s template engine to create them once and for all. Case templates can also be used to associate metrics to specific case types in order to drive the team&#39;s activity, identify the type of investigations that take significant time and seek to automate tedious tasks.&lt;/p&gt; &#xA;&lt;p&gt;Each task can be assigned to a given analyst. Team members can also take charge of a task without waiting for someone to assign it to them.&lt;/p&gt; &#xA;&lt;p&gt;Tasks may contain multiple work logs that contributing analysts can use to describe what they are up to, what was the outcome, attach pieces of evidence or noteworthy files and so on. Logs can be written using a rich text editor or Markdown.&lt;/p&gt; &#xA;&lt;h2&gt;Analyze&lt;/h2&gt; &#xA;&lt;p&gt;You can add one or hundreds if not thousands of observables to each case you create. You can also create a case out of a &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; event. TheHive can be very easily linked to one or several MISP instances and MISP events can be previewed to decide whether they warrant an investigation or not. If an investigation is in order, the analyst can then add the event to an existing case or import it as a new case using a customizable template.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://thehive-project.org/#section_thehive4py&#34;&gt;TheHive4py&lt;/a&gt;, TheHive&#39;s Python API client, it is possible to send SIEM alerts, phishing and other suspicious emails and other security events to TheHive. They will appear in its &lt;code&gt;Alerts&lt;/code&gt; panel along with new or updated MISP events, where they can be previewed, imported into cases or ignored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/images/Alerts_Panel.png&#34; alt=&#34;The Alerts Pane&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TheHive has the ability to automatically identify observables that have been already seen in previous cases. Observables can also be associated with a TLP and the source which provided or generated them using tags. The analyst can also easily mark observables as IOCs and isolate those using a search query then export them for searching in a SIEM or other data stores.&lt;/p&gt; &#xA;&lt;p&gt;Analysts can analyze tens or hundreds of observables in a few clicks by leveraging the analyzers of one or several &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; instances depending on your OPSEC needs: DomainTools, VirusTotal, PassiveTotal, Joe Sandbox, geolocation, threat feed lookups and so on.&lt;/p&gt; &#xA;&lt;p&gt;Security analysts with a knack for scripting can easily add their own analyzers to Cortex in order to automate actions that must be performed on observables or IOCs. They can also decide how analyzers behave according to the TLP. For example, a file added as observable can be submitted to VirusTotal if the associated TLP is WHITE or GREEN. If it&#39;s AMBER, its hash is computed and submitted to VT but not the file. If it&#39;s RED, no VT lookup is done.&lt;/p&gt; &#xA;&lt;h1&gt;Try it&lt;/h1&gt; &#xA;&lt;p&gt;To try TheHive, you can use the &lt;a href=&#34;https://www.strangebee.com/tryit&#34;&gt;training VM&lt;/a&gt; or install it by reading the &lt;a href=&#34;https://docs.thehive-project.org/thehive/installation-and-configuration/installation/step-by-step-guide/&#34;&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Details&lt;/h1&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;We have made several guides available in the &lt;a href=&#34;https://docs.thehive-project.org/thehive/&#34;&gt;Documentation repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Main features&lt;/h2&gt; &#xA;&lt;h3&gt;Multi-tenancy&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a special multi-tenancy support. It allows the following strategies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use a siloed multi-tenancy: many organisations can be defined without allowing them to share data;&lt;/li&gt; &#xA; &lt;li&gt;Use a collaborative multi-tenancy: a set of organisations can be allowed to collaborate on specific cases/tasks/observables, using custom defined user profiles (RBAC).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RBAC&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a set of permissions and several pre-configured user profiles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;admin&lt;/code&gt;: full administrative permissions on the platform ; can&#39;t manage any Cases or other data related to investigations;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;org-admin&lt;/code&gt;: manage users and all organisation-level configuration, can create and edit Cases, Tasks, Observables and run Analyzers and Responders;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;analyst&lt;/code&gt;: can create and edit &lt;em&gt;Cases&lt;/em&gt;, &lt;em&gt;Tasks&lt;/em&gt;, &lt;em&gt;Observables&lt;/em&gt; and run &lt;em&gt;Analyzers&lt;/em&gt; &amp;amp; &lt;em&gt;Responders&lt;/em&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;read-only&lt;/code&gt;: Can only read, Cases, Tasks and Observables details;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;New profiles can be created by administrators of the platform.&lt;/p&gt; &#xA;&lt;h3&gt;Authentication&lt;/h3&gt; &#xA;&lt;p&gt;TheHive 4 supports authentication methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;local accounts&lt;/li&gt; &#xA; &lt;li&gt;Active Directory&lt;/li&gt; &#xA; &lt;li&gt;LDAP&lt;/li&gt; &#xA; &lt;li&gt;Basic Auth&lt;/li&gt; &#xA; &lt;li&gt;API keys&lt;/li&gt; &#xA; &lt;li&gt;OAUTH2&lt;/li&gt; &#xA; &lt;li&gt;Multi Factor Authentication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Statistics &amp;amp; Dashboards&lt;/h3&gt; &#xA;&lt;p&gt;TheHive comes with a powerful statistics module that allows you to create meaningful dashboards to drive your activity and support your budget requests.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;h3&gt;MISP and Cortex&lt;/h3&gt; &#xA;&lt;p&gt;TheHive can be configured to import events from one or multiple &lt;a href=&#34;http://www.misp-project.org/&#34;&gt;MISP&lt;/a&gt; instances. You can also use TheHive to export cases as MISP events to one or several MISP servers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Cortex/&#34;&gt;Cortex&lt;/a&gt; is the perfect companion for TheHive. Use one or several to analyze observables at scale.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Digital Shadows&lt;/h3&gt; &#xA;&lt;p&gt;TheHive Project provides &lt;a href=&#34;https://github.com/TheHive-Project/DigitalShadows2TH&#34;&gt;DigitalShadows2TH&lt;/a&gt;, a free, open source &lt;a href=&#34;https://www.digitalshadows.com/&#34;&gt;Digital Shadows&lt;/a&gt; alert feeder for TheHive. You can use it to import Digital Shadows &lt;em&gt;incidents&lt;/em&gt; and &lt;em&gt;intel-incidents&lt;/em&gt; as alerts in TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;Integration with Zerofox&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TheHive-Project/Zerofox2TH&#34;&gt;Zerofox2TH&lt;/a&gt; is a free, open source &lt;a href=&#34;https://www.zerofox.com/&#34;&gt;ZeroFOX&lt;/a&gt; alert feeder for TheHive, written by TheHive Project. You can use it to feed ZeroFOX alerts into TheHive, where they can be previewed and transformed into new cases using pre-defined incident response templates or added into existing ones.&lt;/p&gt; &#xA;&lt;h3&gt;And many more&lt;/h3&gt; &#xA;&lt;p&gt;Lots of &lt;strong&gt;awesome&lt;/strong&gt; integrations shared by the community could be listed there. If you&#39;re looking for a specific one, &lt;strong&gt;a dedicated repository&lt;/strong&gt; containing all known details and references about existing integrations is updated frequently, and can be found here: &lt;a href=&#34;https://github.com/TheHive-Project/awesome&#34;&gt;https://github.com/TheHive-Project/awesome&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;TheHive is an open source and free software released under the &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/raw/master/LICENSE&#34;&gt;AGPL&lt;/a&gt; (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.&lt;/p&gt; &#xA;&lt;h1&gt;Updates&lt;/h1&gt; &#xA;&lt;p&gt;Information, news and updates are regularly posted on &lt;a href=&#34;https://twitter.com/thehive_project&#34;&gt;TheHive Project Twitter account&lt;/a&gt; and on &lt;a href=&#34;https://blog.thehive-project.org/&#34;&gt;the blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/TheHive-Project/TheHive/main/code_of_conduct.md&#34;&gt;Code of conduct&lt;/a&gt;. We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests via &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;Please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive/issues&#34;&gt;open an issue on GitHub&lt;/a&gt; if you&#39;d like to report a bug or request a feature. We are also available on &lt;a href=&#34;https://chat.thehive-project.org&#34;&gt;Discord&lt;/a&gt; to help you out.&lt;/p&gt; &#xA;&lt;p&gt;If you need to contact the project team, send an email to &lt;a href=&#34;mailto:support@thehive-project.org&#34;&gt;support@thehive-project.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have problems with &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py&#34;&gt;TheHive4py&lt;/a&gt;, please &lt;a href=&#34;https://github.com/TheHive-Project/TheHive4py/issues/new&#34;&gt;open an issue on its dedicated repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you encounter an issue with Cortex or would like to request a Cortex-related feature, please &lt;a href=&#34;https://github.com/TheHive-Project/Cortex/issues/new&#34;&gt;open an issue on its dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the &lt;a href=&#34;https://github.com/TheHive-Project/cortex-analyzers/issues/new&#34;&gt;analyzers&#39; dedicated GitHub repository&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community Discussions&lt;/h1&gt; &#xA;&lt;p&gt;We have set up a Google forum at &lt;a href=&#34;https://groups.google.com/a/thehive-project.org/d/forum/users&#34;&gt;https://groups.google.com/a/thehive-project.org/d/forum/users&lt;/a&gt;. To request access, you need a Google account. You may create one &lt;a href=&#34;https://accounts.google.com/SignUp?hl=en&#34;&gt;using a Gmail address&lt;/a&gt; or &lt;a href=&#34;https://accounts.google.com/SignUpWithoutGmail?hl=en&#34;&gt;without it&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Website&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://thehive-project.org/&#34;&gt;https://thehive-project.org/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/spark</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/apache/spark</id>
    <link href="https://github.com/apache/spark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; &#xA;&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/&#34;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush&#34;&gt;&lt;img src=&#34;https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;GitHub Action Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark&#34;&gt;&lt;img src=&#34;https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&amp;amp;logo=appveyor&#34; alt=&#34;AppVeyor Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/spark&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;PySpark Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&#34;https://spark.apache.org/documentation.html&#34;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Spark&lt;/h2&gt; &#xA;&lt;p&gt;Spark is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./build/mvn -DskipTests clean package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; &#xA;&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html&#34;&gt;&#34;Building Spark&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;&#34;Useful Developer Tools&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/spark-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Programs&lt;/h2&gt; &#xA;&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will run the Pi example locally.&lt;/p&gt; &#xA;&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be a mesos:// or spark:// URL, &#34;yarn&#34; to run on YARN, and &#34;local&#34; to run locally with one thread, or &#34;local[N]&#34; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; &#xA;&lt;h2&gt;Running Tests&lt;/h2&gt; &#xA;&lt;p&gt;Testing first requires &lt;a href=&#34;https://raw.githubusercontent.com/apache/spark/master/#building-spark&#34;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the guidance on how to &lt;a href=&#34;https://spark.apache.org/developer-tools.html#individual-tests&#34;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; &#xA;&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; &#xA;&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the build documentation at &lt;a href=&#34;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&#34;&gt;&#34;Specifying the Hadoop Version and Enabling YARN&#34;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html&#34;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please review the &lt;a href=&#34;https://spark.apache.org/contributing.html&#34;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/openwhisk</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/apache/openwhisk</id>
    <link href="https://github.com/apache/openwhisk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache OpenWhisk is an open source serverless cloud platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenWhisk&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/github/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://travis-ci.com/apache/openwhisk.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openwhisk-team.slack.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join-slack-9B69A0.svg?sanitize=true&#34; alt=&#34;Join Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/apache/openwhisk&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/apache/openwhisk/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=openwhisk&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/openwhisk.svg?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenWhisk is a serverless functions platform for building cloud applications. OpenWhisk offers a rich programming model for creating serverless APIs from functions, composing functions into serverless workflows, and connecting events to functions using rules and triggers. Learn more at &lt;a href=&#34;http://openwhisk.apache.org&#34;&gt;http://openwhisk.apache.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#quick-start&#34;&gt;Quick Start&lt;/a&gt; (Deploy and Use OpenWhisk on your machine)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#deploy-to-kubernetes&#34;&gt;Deploy to Kubernetes&lt;/a&gt; (For development and production)&lt;/li&gt; &#xA; &lt;li&gt;For project contributors and Docker deployments: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/macos/README.md&#34;&gt;Deploy to Docker for Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/tools/ubuntu-setup/README.md&#34;&gt;Deploy to Docker for Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#learn-concepts-and-commands&#34;&gt;Learn Concepts and Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#openwhisk-community-and-support&#34;&gt;OpenWhisk Community and Support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/#project-repository-structure&#34;&gt;Project Repository Structure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to start using OpenWhisk is to install the &#34;Standalone&#34; OpenWhisk stack. This is a full-featured OpenWhisk stack running as a Java process for convenience. Serverless functions run within Docker containers. You will need &lt;a href=&#34;https://docs.docker.com/install&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://java.com/en/download/help/download_options.xml&#34;&gt;Java&lt;/a&gt; and &lt;a href=&#34;https://nodejs.org&#34;&gt;Node.js&lt;/a&gt; available on your machine.&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk.git&#xA;cd openwhisk&#xA;./gradlew core:standalone:bootRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When the OpenWhisk stack is up, it will open your browser to a functions &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/playground-ui.png&#34;&gt;Playground&lt;/a&gt;, typically served from &lt;a href=&#34;http://localhost:3232&#34;&gt;http://localhost:3232&lt;/a&gt;. The Playground allows you create and run functions directly from your browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make use of all OpenWhisk features, you will need the OpenWhisk command line tool called &lt;code&gt;wsk&lt;/code&gt; which you can download from &lt;a href=&#34;https://s.apache.org/openwhisk-cli-download&#34;&gt;https://s.apache.org/openwhisk-cli-download&lt;/a&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/cli.md&#34;&gt;CLI configuration&lt;/a&gt; for additional details. Typically you configure the CLI for Standalone OpenWhisk as follows:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;wsk property set \&#xA;  --apihost &#39;http://localhost:3233&#39; \&#xA;  --auth &#39;23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Standalone OpenWhisk can be configured to deploy additional capabilities when that is desirable. Additional resources are available &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/core/standalone/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deploy to Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;OpenWhisk can also be installed on a Kubernetes cluster. You can use a managed Kubernetes cluster provisioned from a public cloud provider (e.g., AKS, EKS, IKS, GKE), or a cluster you manage yourself. Additionally for local development, OpenWhisk is compatible with Minikube, and Kubernetes for Mac using the support built into Docker 18.06 (or higher).&lt;/p&gt; &#xA;&lt;p&gt;To get started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/apache/openwhisk-deploy-kube.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then follow the instructions in the &lt;a href=&#34;https://github.com/apache/openwhisk-deploy-kube/raw/master/README.md&#34;&gt;OpenWhisk on Kubernetes README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Learn Concepts and Commands&lt;/h3&gt; &#xA;&lt;p&gt;Browse the &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/&#34;&gt;documentation&lt;/a&gt; to learn more. Here are some topics you may be interested in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/about.md&#34;&gt;System overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/README.md&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions.md&#34;&gt;Create and invoke actions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/triggers_rules.md&#34;&gt;Create triggers and rules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/packages.md&#34;&gt;Use and create packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/catalog.md&#34;&gt;Browse and use the catalog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/reference.md&#34;&gt;OpenWhisk system details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/feeds.md&#34;&gt;Implementing feeds&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/actions-actionloop.md&#34;&gt;Developing a runtime for a new language&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenWhisk Community and Support&lt;/h3&gt; &#xA;&lt;p&gt;Report bugs, ask questions and request features &lt;a href=&#34;https://raw.githubusercontent.com/apache/issues&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also join the OpenWhisk Team on Slack &lt;a href=&#34;https://openwhisk-team.slack.com&#34;&gt;https://openwhisk-team.slack.com&lt;/a&gt; and chat with developers. To get access to our public Slack team, request an invite &lt;a href=&#34;https://openwhisk.apache.org/slack.html&#34;&gt;https://openwhisk.apache.org/slack.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Project Repository Structure&lt;/h3&gt; &#xA;&lt;p&gt;The OpenWhisk system is built from a &lt;a href=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/dev/modules.md&#34;&gt;number of components&lt;/a&gt;. The picture below groups the components by their GitHub repos. Please open issues for a component against the appropriate repo (if in doubt just open against the main openwhisk repo).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/openwhisk/master/docs/images/components_to_repos.png&#34; alt=&#34;component/repo mapping&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spark-examples/spark-scala-examples</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/spark-examples/spark-scala-examples</id>
    <link href="https://github.com/spark-examples/spark-scala-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project provides Apache Spark SQL, RDD, DataFrame and Dataset examples in Scala language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Explanation of all Spark SQL, RDD, DataFrame and Dataset examples present on this project are available at &lt;a href=&#34;https://sparkbyexamples.com/&#34;&gt;https://sparkbyexamples.com/&lt;/a&gt; , All these examples are coded in Scala language and tested in our development environment.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents (Spark Examples in Scala)&lt;/h1&gt; &#xA;&lt;h2&gt;Spark RDD Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/&#34;&gt;Create a Spark RDD using Parallelize&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-read-multiple-text-files-into-a-single-rdd/&#34;&gt;Spark ‚Äì Read multiple text files into single RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-load-csv-file-into-rdd/&#34;&gt;Spark load CSV file into RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/different-ways-to-create-spark-rdd/&#34;&gt;Different ways to create Spark RDD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-how-to-create-an-empty-rdd/&#34;&gt;Spark ‚Äì How to create an empty RDD?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/&#34;&gt;Spark RDD Transformations with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/&#34;&gt;Spark RDD Actions with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-pair-rdd-functions/&#34;&gt;Spark Pair RDD Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/&#34;&gt;Spark Repartition() vs Coalesce()&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-shuffle-partitions/&#34;&gt;Spark Shuffle Partitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-persistence-storage-levels/&#34;&gt;Spark Persistence Storage Levels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-cache-and-persist-example/&#34;&gt;Spark RDD Cache and Persist with Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-broadcast-variables/&#34;&gt;Spark Broadcast Variables&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-accumulators/&#34;&gt;Spark Accumulators Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/apache-spark-rdd/convert-spark-rdd-to-dataframe-dataset/&#34;&gt;Convert Spark RDD to DataFrame | Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/&#34;&gt;Spark Create DataFrame with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/&#34;&gt;Spark DataFrame withColumn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/rename-a-column-on-spark-dataframes/&#34;&gt;Ways to Rename column on Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-drop-column-from-dataframe-dataset/&#34;&gt;Spark ‚Äì How to Drop a DataFrame/Dataset column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-where-filter/&#34;&gt;Working with Spark DataFrame Where Filter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-case-when-otherwise-example/&#34;&gt;Spark SQL ‚Äúcase when‚Äù and ‚Äúwhen otherwise‚Äù&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-collect/&#34;&gt;Collect() ‚Äì Retrieve data from Spark RDD/DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-remove-duplicate-rows/&#34;&gt;Spark ‚Äì How to remove duplicate rows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/&#34;&gt;How to Pivot and Unpivot a Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/&#34;&gt;Spark SQL Data Types with Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-structtype-on-dataframe/&#34;&gt;Spark SQL StructType &amp;amp; StructField with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-schema-explained-with-examples/&#34;&gt;Spark schema ‚Äì explained with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-groupby-on-dataframe/&#34;&gt;Spark Groupby Example with DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-how-to-sort-dataframe-column-explained/&#34;&gt;Spark ‚Äì How to Sort DataFrame column explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-dataframe-join/&#34;&gt;Spark SQL Join Types with examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-union-and-union-all/&#34;&gt;Spark DataFrame Union and UnionAll&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/&#34;&gt;Spark map vs mapPartitions transformation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-foreachpartition-vs-foreach-explained/&#34;&gt;Spark foreachPartition vs foreach | what to use?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/&#34;&gt;Spark DataFrame Cache and Persist Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-udf/&#34;&gt;Spark SQL UDF (User Defined Functions)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-array-arraytype-dataframe-column/&#34;&gt;Spark SQL DataFrame Array (ArrayType) Column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-dataframe-map-maptype-column/&#34;&gt;Working with Spark DataFrame Map (MapType) column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-struct-column/&#34;&gt;Spark SQL ‚Äì Flatten Nested Struct column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-flatten-nested-array-column-to-single-column/&#34;&gt;Spark ‚Äì Flatten nested array to single array column&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/explode-spark-array-and-map-dataframe-column/&#34;&gt;Spark explode array and map columns to rows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark SQL Functions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/&#34;&gt;Spark SQL String Functions Explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/&#34;&gt;Spark SQL Date and Time Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-array-functions/&#34;&gt;Spark SQL Array functions complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-map-functions/&#34;&gt;Spark SQL Map functions ‚Äì complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-sort-functions/&#34;&gt;Spark SQL Sort functions ‚Äì complete list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/&#34;&gt;Spark SQL Aggregate Functions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-sql-window-functions/&#34;&gt;Spark Window Functions with Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Data Source API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&#34;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-and-write-json-file/&#34;&gt;Spark Read and Write JSON file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/&#34;&gt;Spark Read and Write Apache Parquet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-xml/&#34;&gt;Spark Read XML file using Databricks API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/read-write-avro-file-spark-dataframe/&#34;&gt;Read &amp;amp; Write Avro files using Spark DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/using-avro-data-files-from-spark-sql-2-3-x/&#34;&gt;Using Avro Data Files From Spark SQL 2.3.x or earlier&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-write-using-hbase-spark-connector/&#34;&gt;Spark Read from &amp;amp; Write to HBase table | Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/create-spark-dataframe-from-hbase-using-hortonworks/&#34;&gt;Create Spark DataFrame from HBase using Hortonworks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-orc-file-into-dataframe/&#34;&gt;Spark Read ORC file into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/&#34;&gt;Spark 3.0 Read Binary File into DataFrame&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Spark Streaming &amp;amp; Kafka&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-outputmode/&#34;&gt;Spark Streaming ‚Äì Different Output modes explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-read-json-files-from-directory/&#34;&gt;Spark Streaming files from a directory&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-from-tcp-socket/&#34;&gt;Spark Streaming ‚Äì Reading data from TCP Socket&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-with-kafka/&#34;&gt;Spark Streaming with Kafka Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-streaming-consume-and-produce-kafka-messages-in-avro-format/&#34;&gt;Spark Streaming ‚Äì Kafka messages in Avro format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sparkbyexamples.com/spark/spark-batch-processing-produce-consume-kafka-topic/&#34;&gt;Spark SQL Batch Processing ‚Äì Produce and Consume Apache Kafka Topic&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>databricks/sjsonnet</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/databricks/sjsonnet</id>
    <link href="https://github.com/databricks/sjsonnet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sjsonnet&lt;/h1&gt; &#xA;&lt;p&gt;A JVM implementation of the &lt;a href=&#34;https://jsonnet.org/&#34;&gt;Jsonnet&lt;/a&gt; configuration language.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet can be used from Java:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;com.databricks&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;sjsonnet_2.13&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.4.2&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    new String[]{&#34;foo.jsonnet&#34;},&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.package$.MODULE$.pwd(),&#xA;    scala.None$.empty()&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From Scala:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&#34;com.databricks&#34; %% &#34;sjsonnet&#34; % &#34;0.4.2&#34; // SBT&#xA;ivy&#34;com.databricks::sjsonnet:0.4.2&#34; // Mill&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sjsonnet.SjsonnetMain.main0(&#xA;    Array(&#34;foo.jsonnet&#34;),&#xA;    new DefaultParseCache,&#xA;    System.in,&#xA;    System.out,&#xA;    System.err,&#xA;    os.pwd, // working directory&#xA;    None&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As a standalone executable assembly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&#34;&gt;https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.jar &amp;gt; sjsonnet.jar&#xA;&#xA;$ chmod +x sjsonnet.jar&#xA;&#xA;$ ./sjsonnet.jar&#xA;error: Need to pass in a jsonnet file to evaluate&#xA;usage: sjsonnet [sjsonnet-options] script-file&#xA;&#xA;  -i, --interactive  Run Mill in interactive mode, suitable for opening REPLs and taking user input&#xA;  -n, --indent       How much to indent your output JSON&#xA;  -J, --jpath        Specify an additional library search dir (right-most wins)&#xA;  -o, --output-file  Write to the output file rather than stdout&#xA;  ...&#xA;&#xA;$ ./sjsonnet.jar foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or from Javascript:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$ curl -L https://github.com/databricks/sjsonnet/releases/download/0.4.2/sjsonnet.js &amp;gt; sjsonnet.js&#xA;&#xA;$ node&#xA;&#xA;&amp;gt; require(&#34;./sjsonnet.js&#34;)&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#34;local f = function(x) x * x; f(11)&#34;, {}, {}, &#34;&#34;, (wd, imported) =&amp;gt; null)&#xA;121&#xA;&#xA;&amp;gt; SjsonnetMain.interpret(&#xA;    &#34;local f = import &#39;foo&#39;; f + &#39;bar&#39;&#34;, // code&#xA;    {}, // extVars&#xA;    {}, // tlaVars&#xA;    &#34;&#34;, // initial working directory&#xA;&#xA;    // import callback: receives a base directory and the imported path string,&#xA;    // returns a tuple of the resolved file path and file contents or file contents resolve method&#xA;    (wd, imported) =&amp;gt; [wd + &#34;/&#34; + imported, &#34;local bar = 123; bar + bar&#34;],&#xA;    // loader callback: receives the tuple from the import callback and returns the file contents&#xA;    ([path, content]) =&amp;gt; content&#xA;    )&#xA;&#39;246bar&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that since Javascript does not necessarily have access to the filesystem, you have to provide an explicit import callback that you can use to resolve imports yourself (whether through Node&#39;s &lt;code&gt;fs&lt;/code&gt; module, or by emulating a filesystem in-memory)&lt;/p&gt; &#xA;&lt;h3&gt;Running deeply recursive Jsonnet programs&lt;/h3&gt; &#xA;&lt;p&gt;The depth of recursion is limited by JVM stack size. You can run Sjsonnet with increased stack size as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java -Xss100m -cp sjsonnet.jar sjsonnet.SjsonnetMain foo.jsonnet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The -Xss option above is responsible for JVM stack size. Please try this if you ever run into &lt;code&gt;sjsonnet.Error: Internal Error ... Caused by: java.lang.StackOverflowError ...&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is no analog of &lt;code&gt;--max-stack&lt;/code&gt;/&lt;code&gt;-s&lt;/code&gt; option of &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;. The only stack size limit is the one of the JVM.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet is implementated as an optimizing interpreter. There are roughly 4 phases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Parser&lt;/code&gt;: parses an input &lt;code&gt;String&lt;/code&gt; into a &lt;code&gt;sjsonnet.Expr&lt;/code&gt;, which is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_syntax_tree&#34;&gt;Syntax Tree&lt;/a&gt; representing the Jsonnet document syntax, using the &lt;a href=&#34;https://github.com/lihaoyi/fastparse&#34;&gt;Fastparse&lt;/a&gt; parsing library&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.StaticOptimizer&lt;/code&gt; is a single AST transform that performs static checking, essential rewriting (e.g. assigning indices in the symbol table for variables) and optimizations. The result is another &lt;code&gt;sjsonnet.Expr&lt;/code&gt; per input file that can be stored in the parse cache and reused.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Evaluator&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Expr&lt;/code&gt; produced by the optimizer and converts it into a &lt;code&gt;sjsonnet.Val&lt;/code&gt;, a data structure representing the Jsonnet runtime values (basically lazy JSON which can contain function values).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Materializer&lt;/code&gt;: recurses over the &lt;code&gt;sjsonnet.Val&lt;/code&gt; and converts it into an output &lt;code&gt;ujson.Expr&lt;/code&gt;: a non-lazy JSON structure without any remaining un-evaluated function values. This can be serialized to a string formatted in a variety of ways&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These three phases are encapsulated in the &lt;code&gt;sjsonnet.Interpreter&lt;/code&gt; object.&lt;/p&gt; &#xA;&lt;p&gt;Some notes on the values used in parts of the pipeline:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Expr&lt;/code&gt;: this represents &lt;code&gt;{...}&lt;/code&gt; object literal nodes, &lt;code&gt;a + b&lt;/code&gt; binary operation nodes, &lt;code&gt;function(a) {...}&lt;/code&gt; definitions and &lt;code&gt;f(a)&lt;/code&gt; invocations, etc.. Also keeps track of source-offset information so failures can be correlated with line numbers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sjsonnet.Val&lt;/code&gt;: essentially the JSON structure (objects, arrays, primitives) but with two modifications. The first is that functions like &lt;code&gt;function(a){...}&lt;/code&gt; can still be present in the structure: in Jsonnet you can pass around functions as values and call then later on. The second is that object values &amp;amp; array entries are &lt;em&gt;lazy&lt;/em&gt;: e.g. &lt;code&gt;[error 123, 456][1]&lt;/code&gt; does not raise an error because the first (erroneous) entry of the array is un-used and thus not evaluated.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classes representing literals extend &lt;code&gt;sjsonnet.Val.Literal&lt;/code&gt; which in turn extends &lt;em&gt;both&lt;/em&gt;, &lt;code&gt;Expr&lt;/code&gt; and &lt;code&gt;Val&lt;/code&gt;. This allows the evaluator to skip over them instead of having to convert them from one representation to the other.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Due to pervasive caching, sjsonnet is much faster than google/jsonnet. See this blog post for more details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://databricks.com/blog/2018/10/12/writing-a-faster-jsonnet-compiler.html&#34;&gt;Writing a Faster Jsonnet Compiler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s the latest set of benchmarks I&#39;ve run comparing Sjsonnet against google/jsonnet and google/go-jsonnet, measuring the time taken to&lt;br&gt; evaluate the &lt;code&gt;test_suite/&lt;/code&gt; folder (smaller is better):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.5&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Sjsonnet 0.1.6&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.26ms ¬± 0.22&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.59ms ¬± 0.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Scala 2.12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.07ms ¬± 0.30&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.29ms ¬± 0.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/jsonnet&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;google/go-jsonnet&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~1277ms&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;~274ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;google/jsonnet was built from source on commit f59758d1904bccda99598990f582dd2e1e9ad263, while google/go-jsonnet was &lt;code&gt;go get&lt;/code&gt;ed at version &lt;code&gt;v0.13.0&lt;/code&gt;. You can see the source code of the benchmark in&lt;br&gt; &lt;a href=&#34;https://github.com/databricks/sjsonnet/raw/master/sjsonnet/test/src-jvm/sjsonnet/SjsonnetTestMain.scala&#34;&gt;SjsonnetTestMain.scala&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sjsonnet 0.4.0 and 0.4.1 further improve the performance significantly on our internal benchmarks. A set of new JMH benchmarks provide detailed performance data of an entire run (&lt;code&gt;MainBenchmark&lt;/code&gt;) and the non-evaluation-related parts (&lt;code&gt;MaterializerBenchmark&lt;/code&gt;, &lt;code&gt;OptimizerBenchmark&lt;/code&gt;, &lt;code&gt;ParserBenchmark&lt;/code&gt;). They can be run from the (JVM / Scala 2.13 only) sbt build. The Sjsonnet profiler is located in the same sbt project:&lt;/p&gt; &#xA;&lt;p&gt;The Sjsonnet command line which is run by all of these is defined in &lt;code&gt;MainBenchmark.mainArgs&lt;/code&gt;. You need to change it to point to a suitable input before running a benchmark or the profiler. (For Databricks employees who want to reproduce our benchmarks, the pre-configured command line is expected to be run against databricks/universe @ 7cbd8d7cb071983077d41fcc34f0766d0d2a247d).&lt;/p&gt; &#xA;&lt;p&gt;Benchmark example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/jmh:run -jvmArgs &#34;-XX:+UseStringDeduplication&#34; sjsonnet.MainBenchmark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profiler:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt bench/run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Laziness&lt;/h2&gt; &#xA;&lt;p&gt;The Jsonnet language is &lt;em&gt;lazy&lt;/em&gt;: expressions don&#39;t get evaluated unless their value is needed, and thus even erroneous expressions do not cause a failure if un-used. This is represented in the Sjsonnet codebase by &lt;code&gt;sjsonnet.Lazy&lt;/code&gt;: a wrapper type that encapsulates an arbitrary computation that returns a &lt;code&gt;sjsonnet.Val&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sjsonnet.Lazy&lt;/code&gt; is used in several places, representing where laziness is present in the language:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Scope&lt;/code&gt;, representing local variable name bindings&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Arr&lt;/code&gt;, representing the contents of array cells&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;sjsonnet.Val.Obj&lt;/code&gt;, representing the contents of object values&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Val&lt;/code&gt; extends &lt;code&gt;Lazy&lt;/code&gt; so that an already computed value can be treated as lazy without having to wrap it.&lt;/p&gt; &#xA;&lt;p&gt;Unlike &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet caches the results of lazy computations the first time they are evaluated, avoiding wasteful re-computation when a value is used more than once.&lt;/p&gt; &#xA;&lt;h2&gt;Standard Library&lt;/h2&gt; &#xA;&lt;p&gt;Different from &lt;a href=&#34;https://github.com/google/jsonnet&#34;&gt;google/jsonnet&lt;/a&gt;, Sjsonnet does not implement the Jsonnet standard library &lt;code&gt;std&lt;/code&gt; in Jsonnet code. Rather, those functions are implemented as intrinsics directly in the host language (in &lt;code&gt;Std.scala&lt;/code&gt;). This allows both better error messages when the input types are wrong, as well as better performance for the more computationally-intense builtin functions.&lt;/p&gt; &#xA;&lt;h2&gt;Client-Server&lt;/h2&gt; &#xA;&lt;p&gt;Sjsonnet comes with a built in thin-client and background server, to help mitigate the unfortunate JVM warmup overhead that adds ~1s to every invocation down to 0.2-0.3s. For the simple non-client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create the executable. For the client-server executable, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill show sjsonnet[2.13.0].server.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the Sjsonnet background server lives in &lt;code&gt;~/.sjsonnet&lt;/code&gt;, and lasts 5 minutes before shutting itself when inactive.&lt;/p&gt; &#xA;&lt;p&gt;Since the Sjsonnet client still has 0.2-0.3s of overhead, if using Sjsonnet heavily it is still better to include it in your JVM classpath and invoke it programmatically via &lt;code&gt;new Interpreter(...).interpret(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Publishing&lt;/h2&gt; &#xA;&lt;p&gt;To publish, make sure the version number in &lt;code&gt;build.sc&lt;/code&gt; is correct, then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mill -i mill.scalalib.PublishModule/publishAll --sonatypeCreds lihaoyi:$SONATYPE_PASSWORD --publishArtifacts __.publishArtifacts --release true&#xA;&#xA;./mill -i show sjsonnet[2.13.4].js.fullOpt&#xA;./mill -i show sjsonnet[2.13.4].jvm.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;0.4.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make lazy initialization of static Val.Obj thread-safe &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/136&#34;&gt;#136&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deduplicate strings in the parser &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/137&#34;&gt;#137&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Update the JS example &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/141&#34;&gt;#141&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional significant performance improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/119&#34;&gt;#119&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Error handling fixes and improvements &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/125&#34;&gt;#125&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.4.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements with lots of internal changes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/117&#34;&gt;#117&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump uJson version to 1.3.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avoid catching fatal exceptions during evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.3.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--yaml-debug&lt;/code&gt; flag to add source-line comments showing where each line of YAML came from &lt;a href=&#34;&#34;&gt;#105&lt;/a&gt;&lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/105&#34;&gt;https://github.com/databricks/sjsonnet/pull/105&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;objectValues&lt;/code&gt; and &lt;code&gt;objectVlauesAll&lt;/code&gt; to stdlib &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/104&#34;&gt;#104&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow direct YAML output generation via &lt;code&gt;--yaml-out&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Do not allow duplicate field in object when evaluating list list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/100&#34;&gt;#100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix compiler crash when &#39;+&#39; signal is true in a field declaration inside a list comprehension &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/98&#34;&gt;#98&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fix error message for too many arguments with at least one named arg &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/97&#34;&gt;#97&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming JSON output to disk for lower memory usage &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/85&#34;&gt;#85&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Static detection of duplicate fields &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/86&#34;&gt;#86&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Strict mode to disallow error-prone adjacent object literals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/88&#34;&gt;#88&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.flatMap&lt;/code&gt;, &lt;code&gt;std.repeat&lt;/code&gt;, &lt;code&gt;std.clamp&lt;/code&gt;, &lt;code&gt;std.member&lt;/code&gt;, &lt;code&gt;std.stripChars&lt;/code&gt;, &lt;code&gt;std.rstripChars&lt;/code&gt;, &lt;code&gt;std.lstripChars&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add support for syntactical key ordering &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/53&#34;&gt;#53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bump dependency versions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.2.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump verion of Scalatags, uPickle&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.9&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump version of FastParse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.8&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bump versions of OS-Lib, uJson, Scalatags&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.7&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support std lib methods that take a key lambda &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/40&#34;&gt;#40&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Handle hex in unicode escaoes &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/41&#34;&gt;#41&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add encodeUTF8, decodeUTF8 std lib methdos &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/42&#34;&gt;#42&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Properly fail on non-boolean conditionals &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/44&#34;&gt;#44&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support YAML-steam output &lt;a href=&#34;https://github.com/databricks/sjsonnet/pull/45&#34;&gt;#45&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.6&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~2x performance increase&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Javascript support, allowing Sjsonnet to be used in the browser or on Node.js&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.4&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala 2.13 support&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.3&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;std.mod&lt;/code&gt;, &lt;code&gt;std.min&lt;/code&gt; and &lt;code&gt;std.max&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.2&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improvements to error reporting when types do not match&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance improvements to the parser via upgrading to Fastparse 2.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;0.1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First release&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>delta-io/delta</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/delta-io/delta</id>
    <link href="https://github.com/delta-io/delta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://docs.delta.io/latest/_static/delta-lake-white.png&#34; width=&#34;200&#34; alt=&#34;Delta Lake Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/delta-spark/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&#34;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&#34;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io&#34;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/quick-start.html&#34;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; &#xA; &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&#34;https://github.com/delta-io&#34;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&#34;https://github.com/delta-io/connectors&#34;&gt;connectors&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta&#34;&gt;delta&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-rs&#34;&gt;delta-rs&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/delta-sharing&#34;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&#34;https://github.com/delta-io/kafka-delta-ingest&#34;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&#34;https://github.com/delta-io/website&#34;&gt;website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&#34;https://delta.io/integrations/&#34;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/&#34;&gt;Apache Spark‚Ñ¢&lt;/a&gt;: This connector allows Apache Spark‚Ñ¢ to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/delta-io/connectors/tree/master/flink&#34;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prestodb.io/docs/current/connector/deltalake.html&#34;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://trino.io/docs/current/connector/delta-lake.html&#34;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/hive-integration.html&#34;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.rs/deltalake/latest/deltalake/&#34;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&#34;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&#34;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&#34;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#building&#34;&gt;Building&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&#34;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&#34;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&#34;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&#34;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Latest Binaries&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/&#34;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; &#xA;&lt;h2&gt;API Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/java/index.html&#34;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.delta.io/latest/api/python/index.html&#34;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.delta.io/latest/delta-standalone.html&#34;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table‚Äôs metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; &#xA;&lt;h3&gt;API Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&#34;https://docs.delta.io/latest/delta-apidoc.html&#34;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; &#xA; &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; &#xA; &lt;li&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/releases.html&#34;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; &#xA;&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; &#xA;&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/core/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&#34;&gt;action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&#34;http://delta.io/roadmap&#34;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For the detailed timeline, see the &lt;a href=&#34;https://github.com/delta-io/delta/milestones&#34;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Transaction Protocol&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&#34;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.delta.io/latest/delta-storage.html&#34;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Concurrency Control&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&#34;https://docs.delta.io/latest/delta-concurrency.html&#34;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting issues&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/delta-io/delta/issues&#34;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&#34;https://raw.githubusercontent.com/delta-io/delta/master/#community&#34;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We also adhere to the &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;Delta Lake is compiled using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To compile, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate artifacts, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute tests, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSuite&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;build/sbt &#39;testOnly *.OptimizeCompactionSuite -- -z &#34;optimize command: on partitioned table - all partitions&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&#34;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; &#xA;&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; &#xA;&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Verification&lt;/h3&gt; &#xA;&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you see errors of the form&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser&#xA;import io.delta.sql.parser.DeltaSqlBaseParser._&#xA;...&#xA;Error:(91, 22) not found: type DeltaSqlBaseParser&#xA;    val parser = new DeltaSqlBaseParser(tokenStream)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt compile&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-core&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache License 2.0, see &lt;a href=&#34;https://github.com/delta-io/delta/raw/master/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Public Slack Channel &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/delta-users/shared_invite/zt-165gcm2g7-0Sc57w7dX0FbfilR9EPwVQ&#34;&gt;Register here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://delta-users.slack.com/&#34;&gt;Login here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/deltalake&#34;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/c/deltalake&#34;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public &lt;a href=&#34;https://groups.google.com/forum/#!forum/delta-users&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>streamxhub/streamx</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/streamxhub/streamx</id>
    <link href="https://github.com/streamxhub/streamx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make stream processing easier! Flink &amp; Spark development scaffold, The original intention of StreamX is to make the development of Flink easier. StreamX focuses on the management of development phases and tasks. Our ultimate goal is to build a one-stop big data solution integrating stream processing, batch processing, data warehouse and data laker.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt; &lt;a href=&#34;http://www.streamxhub.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt; &lt;img width=&#34;600&#34; src=&#34;https://user-images.githubusercontent.com/13284744/166133644-ed3cc4f5-aae5-45bc-bfbe-29c540612446.png&#34; alt=&#34;StreamX logo&#34;&gt; &lt;/a&gt; &lt;/h1&gt; &#xA; &lt;strong style=&#34;font-size: 1.5rem&#34;&gt;Make stream processing easier!!!&lt;/strong&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-4EB1BA.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://tokei.rs/b1/github/streamxhub/streamx&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/streamxhub/streamx.svg?sanitize=true&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/streamxhub/streamx&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/streamxhub/streamx&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/count/streamxhub/streamx&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.streamxhub.com&#34;&gt;Official Website&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamxhub/streamx/dev/#&#34;&gt;Change Log&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href=&#34;https://www.streamxhub.com/docs/intro&#34;&gt;Document&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/streamxhub/streamx/dev/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h1&gt;StreamX&lt;/h1&gt; &#xA;&lt;p&gt;Make stream processing easier&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A magical framework that make stream processing easier!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üöÄ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The original intention of &lt;code&gt;StreamX&lt;/code&gt; is to make stream processing easier. &lt;code&gt;StreamX&lt;/code&gt; focuses on the management of development phases and tasks. Our ultimate goal is to build a one-stop big data solution integrating stream processing, batch processing, data warehouse and data laker.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://assets.streamxhub.com/streamx-video.mp4&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/166101616-50a44d38-3ffb-4296-8a77-92f76a4c21b5.png&#34; alt=&#34;StreamX video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üéâ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scaffolding&lt;/li&gt; &#xA; &lt;li&gt;Out-of-the-box connectors&lt;/li&gt; &#xA; &lt;li&gt;Support maven compilation&lt;/li&gt; &#xA; &lt;li&gt;Configuration&lt;/li&gt; &#xA; &lt;li&gt;Multi version flink support(1.12.x,1.13.x,1.14.x, 1.15.x)&lt;/li&gt; &#xA; &lt;li&gt;Scala 2.11 / 2.12 support&lt;/li&gt; &#xA; &lt;li&gt;restapi support.&lt;/li&gt; &#xA; &lt;li&gt;All Flink deployment mode support(&lt;code&gt;Remote&lt;/code&gt;/&lt;code&gt;K8s-Native-Application&lt;/code&gt;/&lt;code&gt;K8s-Native-Session&lt;/code&gt;/&lt;code&gt;YARN-Application&lt;/code&gt;/&lt;code&gt;YARN-Per-Job&lt;/code&gt;/&lt;code&gt;YARN-Session&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, &lt;code&gt;savepoint&lt;/code&gt;, resume from &lt;code&gt;savepoint&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Various companies and organizations use &lt;code&gt;StreamX&lt;/code&gt; for production and commercial products.&lt;/li&gt; &#xA; &lt;li&gt;Flame graph&lt;/li&gt; &#xA; &lt;li&gt;Notebook&lt;/li&gt; &#xA; &lt;li&gt;Project configuration and dependency version management&lt;/li&gt; &#xA; &lt;li&gt;Task backup and rollback&lt;/li&gt; &#xA; &lt;li&gt;Manage dependencies&lt;/li&gt; &#xA; &lt;li&gt;UDF&lt;/li&gt; &#xA; &lt;li&gt;Flink SQL Connector&lt;/li&gt; &#xA; &lt;li&gt;Flink SQL WebIDE&lt;/li&gt; &#xA; &lt;li&gt;Catalog„ÄÅHive&lt;/li&gt; &#xA; &lt;li&gt;Full support from task &lt;code&gt;development&lt;/code&gt; to &lt;code&gt;deployment&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746863-856ef1cd-fa0e-4010-b359-c16ca2ad2fb7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746864-d807d728-423f-41c3-b90d-45ce2c21936b.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üè≥‚Äçüåà Components&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Streamx&lt;/code&gt; consists of three parts,&lt;code&gt;streamx-core&lt;/code&gt;,&lt;code&gt;streamx-pump&lt;/code&gt; and &lt;code&gt;streamx-console&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746859-f6a4dedc-ec42-4ed5-933b-c27d559b9988.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1Ô∏è‚É£ streamx-core&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;streamx-core&lt;/code&gt; is a framework that focuses on coding, standardizes configuration, and develops in a way that is better than configuration by convention. Also it provides a development-time &lt;code&gt;RunTime Content&lt;/code&gt; and a series of &lt;code&gt;Connector&lt;/code&gt; out of the box. At the same time, it extends &lt;code&gt;DataStream&lt;/code&gt; some methods, and integrates &lt;code&gt;DataStream&lt;/code&gt; and &lt;code&gt;Flink sql&lt;/code&gt; api to simplify tedious operations, focus on the business itself, and improve development efficiency and development experience.&lt;/p&gt; &#xA;&lt;h3&gt;2Ô∏è‚É£ streamx-pump&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;streamx-pump&lt;/code&gt; is a planned data extraction component, similar to &lt;code&gt;flinkx&lt;/code&gt;. Based on the various &lt;code&gt;connector&lt;/code&gt; provided in &lt;code&gt;streamx-core&lt;/code&gt;, the purpose is to create a convenient, fast, out-of-the-box real-time data extraction and migration component for big data, and it will be integrated into the &lt;code&gt;streamx-console&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3Ô∏è‚É£ streamx-console&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;streamx-console&lt;/code&gt; is a stream processing and &lt;code&gt;Low Code&lt;/code&gt; platform, capable of managing &lt;code&gt;Flink&lt;/code&gt; tasks, integrating project compilation, deploy, configuration, startup, &lt;code&gt;savepoint&lt;/code&gt;, &lt;code&gt;flame graph&lt;/code&gt;, &lt;code&gt;Flink SQL&lt;/code&gt;, monitoring and many other features. Simplify the daily operation and maintenance of the &lt;code&gt;Flink&lt;/code&gt; task.&lt;/p&gt; &#xA;&lt;p&gt;Our ultimate goal is to build a one-stop big data solution integrating stream processing, batch processing, data warehouse and data laker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://flink.apache.org&#34;&gt;Apache Flink&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://hadoop.apache.org&#34;&gt;Apache YARN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spring.io/projects/spring-boot/&#34;&gt;Spring Boot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.mybatis.org&#34;&gt;Mybatis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://mp.baomidou.com&#34;&gt;Mybatis-Plus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.brendangregg.com/FlameGraphs&#34;&gt;Flame Graph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/uber-common/jvm-profiler&#34;&gt;JVM-Profiler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cn.vuejs.org/&#34;&gt;Vue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vuepress.vuejs.org/&#34;&gt;VuePress&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://antdv.com/&#34;&gt;Ant Design of Vue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pro.antdv&#34;&gt;ANTD PRO VUE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xtermjs.org/&#34;&gt;xterm.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/monaco-editor/&#34;&gt;Monaco Editor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to the above excellent open source projects and many outstanding open source projects that are not mentioned, for giving the greatest respect,Thanks to &lt;a href=&#34;http://flink.apache.org&#34;&gt;Apache Flink&lt;/a&gt; for creating a great project! Thanks to the &lt;a href=&#34;http://zeppelin.apache.org&#34;&gt;Apache Zeppelin&lt;/a&gt; project for the early inspiration.&lt;/p&gt; &#xA;&lt;h3&gt;üöÄ Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;click &lt;a href=&#34;http://www.streamxhub.com/zh-CN/docs/intro/&#34;&gt;Document&lt;/a&gt; for more information&lt;/p&gt; &#xA;&lt;h2&gt;üíã our users&lt;/h2&gt; &#xA;&lt;p&gt;Various companies and organizations use StreamX for research, production and commercial products. Are you using this project ? &lt;a href=&#34;https://github.com/streamxhub/streamx/issues/163&#34;&gt;you can add your company&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/160220085-11f1e011-e7a0-421f-9294-c14213c0bc22.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèÜ Our honor&lt;/h2&gt; &#xA;&lt;p&gt;We have received some precious honors, which belong to everyone who contributes to StreamX, Thank you !&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746797-85ebf7b4-4105-4b5b-a023-0689c7fd1d2d.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/163530071-a5b6f334-9af5-439c-96c9-2bb9b4eec6a6.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contribution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/streamxhub/streamx/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can submit any ideas as &lt;a href=&#34;https://github.com/streamxhub/streamx/pulls&#34;&gt;pull requests&lt;/a&gt; or as &lt;a href=&#34;https://github.com/streamxhub/streamx/issues/new/choose&#34;&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re new to posting issues, we ask that you read &lt;a href=&#34;http://www.catb.org/~esr/faqs/smart-questions.html&#34;&gt;&lt;em&gt;How To Ask Questions The Smart Way&lt;/em&gt;&lt;/a&gt; (&lt;strong&gt;This guide does not provide actual support services for this project!&lt;/strong&gt;), &lt;a href=&#34;http://www.chiark.greenend.org.uk/~sgtatham/bugs.html&#34;&gt;How to Report Bugs Effectively&lt;/a&gt; prior to posting. Well written bug reports help us help you!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Thank you to all the people who already contributed to StreamX!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/streamxhub/streamx/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/streamx/contributors.svg?width=890&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚è∞ Contributor Over Time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://git-contributor.com?chart=contributorOverTime&amp;amp;repo=streamxhub/streamx&#34;&gt;&lt;img src=&#34;https://contributor-overtime-api.git-contributor.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=streamxhub/streamx&#34; alt=&#34;Contributor Over Time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí∞ Donation&lt;/h2&gt; &#xA;&lt;p&gt;Are you &lt;strong&gt;enjoying this project&lt;/strong&gt; ? üëã&lt;/p&gt; &#xA;&lt;p&gt;If you like this framework, and appreciate the work done for it to exist, you can still support the developers by donating ‚òÄÔ∏è üëä&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;WeChat Pay&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Alipay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746857-35e7f823-7160-4505-be3f-e748a2d0a233.png&#34; alt=&#34;Buy Me A Coffee&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/13284744/142746860-e14a8183-d973-44ca-83bf-e5f9d4da1510.png&#34; alt=&#34;Buy Me A Coffee&#34; width=&#34;150&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üèÜ Our sponsors (Coffee Suppliers)&lt;/h2&gt; &#xA;&lt;h3&gt;üíú Monthly Supplier&lt;/h3&gt; &#xA;&lt;p&gt;Welcome individuals and enterprises to sponsor, your support will help us better develop the project&lt;/p&gt; &#xA;&lt;h3&gt;ü•á Gold Supplier&lt;/h3&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/wolfboys&#34; alt=&#34;benjobs&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/13284744?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Kitming25&#34; alt=&#34;Kitming25&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/11773106?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Narcasserun&#34; alt=&#34;Narcasserun&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/39329477?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;ü•à Platinum Supplier&lt;/h3&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/lianxiaobao&#34; alt=&#34;lianxiaobao&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/36557317?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/su94998&#34; alt=&#34;su94998&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/33316193?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;ü•à Silver Supplier&lt;/h3&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/CrazyJugger&#34; alt=&#34;leohantaoluo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/30514978?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhaizhirui&#34; alt=&#34;zhaizhirui&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/39609947?v=4&#34; height=&#34;50&#34; width=&#34;50&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://www.jetbrains.com/?from=streamx&#34;&gt;JetBrains&lt;/a&gt; for supporting us free open source licenses.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jetbrains.com/?from=streamx&#34;&gt;&lt;img src=&#34;https://img.alicdn.com/tfs/TB1sSomo.z1gK0jSZLeXXb9kVXa-120-130.svg?sanitize=true&#34; alt=&#34;JetBrains&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üèÖ Backers&lt;/h3&gt; &#xA;&lt;p&gt;Thank you to all our backers!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üí¨ Join us&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamxhub/streamx/dev/(http://www.streamxhub.com/#/)&#34;&gt;StreamX&lt;/a&gt; enters the high-speed development stage, we need your contribution.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://starchart.cc/streamxhub/streamx.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/13284744/152627523-de455a4d-97c7-46cd-815f-3328a3fe3663.png&#34; alt=&#34;Join the Group&#34; height=&#34;300px&#34;&gt;&#xA; &lt;br&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/aws-sqsd</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/olxbr/aws-sqsd</id>
    <link href="https://github.com/olxbr/aws-sqsd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple alternative to the Amazon SQS Daemon (&#34;sqsd&#34;) used on AWS Beanstalk worker tier instances, based on https://github.com/mozart-analytics/sqsd&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AWS SQS Worker Daemon&lt;/h1&gt; &#xA;&lt;p&gt;A simple alternative to the Amazon SQS Daemon (&#34;sqsd&#34;) used on AWS Beanstalk worker tier instances.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;IMPORTANT:&lt;/em&gt; In order for &lt;code&gt;sqsd&lt;/code&gt; to work, you have to have configured the AWS Authentication Keys on you environment either as ENV VARS or using any of the other methods that AWS provides. For ways to do this, go &lt;a href=&#34;http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Using Environment Variables&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Property&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_DEFAULT_REGION&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;us-east-1&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;The region name of the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;The access key to access the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;The secret key to access the AWS SQS queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_QUEUE_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;Your queue URL.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_CONCURRENCY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Max number of messages process in parallel.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WAIT_TIME_SECONDS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;20&lt;/code&gt; (max: &lt;code&gt;20&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Long polling wait time when querying the queue.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HTTP_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://127.0.0.1:80/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Your service endpoint/path where to POST the messages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HTTP_REQUEST_CONTENT_TYPE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;application/json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;Message MIME Type.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_TIMEOUT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Max time that waiting for a worker response in milliseconds.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HEALTH_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;http://127.0.0.1:80/&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Your service endpoint/path for your service health.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SQSD_WORKER_HEALTH_WAIT_TIME&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;Time to between health checks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt compile&#xA;sbt universal:packageZipTarball&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using an SBT&#39;s docker image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm -it -v $PWD:/target -v $HOME/.ivy2:/root/.ivy2 -v $HOME/.m2:/root/.m2 -w /target hseeberger/scala-sbt:8u151-2.12.4-1.1.1 sbt compile&#xA;docker run --rm -it -v $PWD:/target -v $HOME/.ivy2:/root/.ivy2 -v $HOME/.m2:/root/.m2 -w /target hseeberger/scala-sbt:8u151-2.12.4-1.1.1 sbt universal:packageZipTarball&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to build the docker image&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build --tag aws-sqsd:&amp;lt;some version&amp;gt; .&#xA;docker build --tag &amp;lt;some_company&amp;gt;/aws-sqsd:&amp;lt;some version&amp;gt; .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;You should use the pre created GZVR&#39;s image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e SQSD_QUEUE_URL=&amp;lt;queue-url&amp;gt; -it -d run vivareal/aws-sqsd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or the image created by yourself&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e SQSD_QUEUE_URL=&amp;lt;queue-url&amp;gt; -it -d run some_image&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug in the source code or if you want to contribute with new features, you can help submitting an issue, even better if you can submit a pull request :)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spotify/scio</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/spotify/scio</id>
    <link href="https://github.com/spotify/scio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Scala API for Apache Beam and Google Cloud Dataflow.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Scio&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/spotify/scio/actions?query=workflow%3Aci&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/spotify/scio/ci&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/spotify/scio?branch=master&#34;&gt;&lt;img src=&#34;https://codecov.io/github/spotify/scio/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spotify/scio/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/spotify/scio.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/com.spotify/scio-core_2.12&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/com.spotify/scio-core_2.12.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://spotify.github.io/scio/api/com/spotify/scio/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/scaladoc-latest-blue.svg?sanitize=true&#34; alt=&#34;Scaladoc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://scala-steward.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Scala_Steward-helping-brightgreen.svg?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAQCAMAAAARSr4IAAAAVFBMVEUAAACHjojlOy5NWlrKzcYRKjGFjIbp293YycuLa3pYY2LSqql4f3pCUFTgSjNodYRmcXUsPD/NTTbjRS+2jomhgnzNc223cGvZS0HaSD0XLjbaSjElhIr+AAAAAXRSTlMAQObYZgAAAHlJREFUCNdNyosOwyAIhWHAQS1Vt7a77/3fcxxdmv0xwmckutAR1nkm4ggbyEcg/wWmlGLDAA3oL50xi6fk5ffZ3E2E3QfZDCcCN2YtbEWZt+Drc6u6rlqv7Uk0LdKqqr5rk2UCRXOk0vmQKGfc94nOJyQjouF9H/wCc9gECEYfONoAAAAASUVORK5CYII=&#34; alt=&#34;Scala Steward badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.github.com/spotify/scio/master/site/src/main/paradox/images/scio.png&#34; alt=&#34;Scio Logo&#34; width=&#34;250&#34;&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Ecclesiastical Latin IPA: /Àà Éi.o/, [Àà ÉiÀê.o], [Àà Éi.iÃØo] Verb: I can, know, understand, have knowledge.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Scio is a Scala API for &lt;a href=&#34;http://beam.incubator.apache.org/&#34;&gt;Apache Beam&lt;/a&gt; and &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowJavaSDK&#34;&gt;Google Cloud Dataflow&lt;/a&gt; inspired by &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; and &lt;a href=&#34;https://github.com/twitter/scalding&#34;&gt;Scalding&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Scio 0.3.0 and future versions depend on Apache Beam (&lt;code&gt;org.apache.beam&lt;/code&gt;) while earlier versions depend on Google Cloud Dataflow SDK (&lt;code&gt;com.google.cloud.dataflow&lt;/code&gt;). See this &lt;a href=&#34;https://spotify.github.io/scio/Apache-Beam.html&#34;&gt;page&lt;/a&gt; for a list of breaking changes.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala API close to that of Spark and Scalding core APIs&lt;/li&gt; &#xA; &lt;li&gt;Unified batch and streaming programming model&lt;/li&gt; &#xA; &lt;li&gt;Fully managed service&lt;sup&gt;*&lt;/sup&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integration with Google Cloud products: Cloud Storage, BigQuery, Pub/Sub, Datastore, Bigtable&lt;/li&gt; &#xA; &lt;li&gt;JDBC, &lt;a href=&#34;http://tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt; TFRecords, Cassandra, Elasticsearch and Parquet I/O&lt;/li&gt; &#xA; &lt;li&gt;Interactive mode with Scio REPL&lt;/li&gt; &#xA; &lt;li&gt;Type safe BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Integration with &lt;a href=&#34;https://github.com/twitter/algebird&#34;&gt;Algebird&lt;/a&gt; and &lt;a href=&#34;https://github.com/scalanlp/breeze&#34;&gt;Breeze&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pipeline orchestration with &lt;a href=&#34;http://docs.scala-lang.org/overviews/core/futures.html&#34;&gt;Scala Futures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Distributed cache&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; provided by Google Cloud Dataflow&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;Download and install the &lt;a href=&#34;https://adoptopenjdk.net/index.html&#34;&gt;Java Development Kit (JDK)&lt;/a&gt; version 8.&lt;/p&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Setup.html&#34;&gt;sbt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use our &lt;a href=&#34;https://github.com/spotify/scio.g8&#34;&gt;giter8 template&lt;/a&gt; to quickly create a new Scio job repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;sbt new spotify/scio.g8&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Switch to the new repo (default &lt;code&gt;scio-job&lt;/code&gt;) and build it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd scio-job&#xA;sbt stage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the included word count example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;target/universal/stage/bin/scio-job --output=wc&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;List result files and inspect content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ls -l wc&#xA;cat wc/part-00000-of-00004.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://spotify.github.io/scio/Getting-Started.html&#34;&gt;Getting Started&lt;/a&gt; is the best place to start with Scio. If you are new to Apache Beam and distributed data processing, check out the &lt;a href=&#34;https://beam.apache.org/documentation/programming-guide/&#34;&gt;Beam Programming Guide&lt;/a&gt; first for a detailed explanation of the Beam programming model and concepts. If you have experience with other Scala data processing libraries, check out this comparison between &lt;a href=&#34;https://spotify.github.io/scio/Scio,-Scalding-and-Spark.html&#34;&gt;Scio, Scalding and Spark&lt;/a&gt;. Finally check out this document about the relationship between &lt;a href=&#34;https://spotify.github.io/scio/Scio,-Beam-and-Dataflow.html&#34;&gt;Scio, Beam and Dataflow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example Scio pipelines and tests can be found under &lt;a href=&#34;https://github.com/spotify/scio/tree/master/scio-examples/src&#34;&gt;scio-examples&lt;/a&gt;. A lot of them are direct ports from Beam&#39;s Java &lt;a href=&#34;https://github.com/apache/beam/tree/master/examples&#34;&gt;examples&lt;/a&gt;. See this &lt;a href=&#34;http://spotify.github.io/scio/examples/&#34;&gt;page&lt;/a&gt; for some of them with side-by-side explanation. Also see &lt;a href=&#34;https://github.com/spotify/big-data-rosetta-code&#34;&gt;Big Data Rosetta Code&lt;/a&gt; for common data processing code snippets in Scio, Scalding and Spark.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spotify.github.io/scio/&#34;&gt;Scio Docs&lt;/a&gt; - main documentation site&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://spotify.github.io/scio/api/&#34;&gt;Scio Scaladocs&lt;/a&gt; - current API documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://spotify.github.io/scio/examples/&#34;&gt;Scio Examples&lt;/a&gt; - examples with side-by-side explanation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Artifacts&lt;/h1&gt; &#xA;&lt;p&gt;Scio includes the following artifacts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-core&lt;/code&gt;: core library&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-test&lt;/code&gt;: test utilities, add to your project as a &#34;test&#34; dependency&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-avro&lt;/code&gt;: add-on for Avro, can also be used standalone&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-google-cloud-platform&lt;/code&gt;: add-on for Google Cloud IO&#39;s: BigQuery, Bigtable, Pub/Sub, Datastore, Spanner&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-cassandra*&lt;/code&gt;: add-ons for Cassandra&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-elasticsearch*&lt;/code&gt;: add-ons for Elasticsearch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-extra&lt;/code&gt;: extra utilities for working with collections, Breeze, etc., best effort support&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-jdbc&lt;/code&gt;: add-on for JDBC IO&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-parquet&lt;/code&gt;: add-on for Parquet&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-tensorflow&lt;/code&gt;: add-on for TensorFlow TFRecords IO and prediction&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-redis&lt;/code&gt;: add-on for Redis&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-smb&lt;/code&gt;: add-on for Sort Merge Bucket operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scio-repl&lt;/code&gt;: extension of the Scala REPL with Scio specific operations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Copyright 2021 Spotify AB.&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0: &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>xxf098/shadowsocksr-v2ray-trojan-android</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/xxf098/shadowsocksr-v2ray-trojan-android</id>
    <link href="https://github.com/xxf098/shadowsocksr-v2ray-trojan-android" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A V2Ray, Trojan, ShadowsocksR client for Android&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;A ShadowsocksR, V2Ray and Trojan Client for Android&lt;/h2&gt; &#xA;&lt;p&gt;A fully featured &lt;a href=&#34;https://github.com/breakwa11/shadowsocks-rss/&#34;&gt;ShadowsocksR&lt;/a&gt;, &lt;a href=&#34;https://github.com/v2ray/v2ray-core&#34;&gt;V2Ray&lt;/a&gt; and &lt;a href=&#34;https://trojan-gfw.github.io/trojan/protocol&#34;&gt;Trojan&lt;/a&gt; client for Android, written in Scala.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/xxf098/shadowsocksr-v2ray-android/workflows/build/badge.svg?branch=xxf098%2Fmaster&amp;amp;event=push&#34; alt=&#34;build&#34;&gt; &lt;a href=&#34;https://github.com/xxf098/shadowsocksr-v2ray-android/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/xxf098/shadowsocksr-v2ray-android&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/xxf098/shadowsocksr-v2ray-android/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/xxf098/shadowsocksr-v2ray-android.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;PREREQUISITES&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A really fast proxy&lt;/li&gt; &#xA; &lt;li&gt;JDK 1.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    sudo apt-get install openjdk-8-jdk&#xA;    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64&#xA;    export PATH=$PATH:$JAVA_HOME/bin&#xA;    java -version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SBT &lt;a href=&#34;https://www.scala-sbt.org/download.html&#34;&gt;0.13.8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Android SDK &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Build Tools 30+&lt;/li&gt; &#xA;   &lt;li&gt;Android Support Repository and Google Repository (see &lt;code&gt;build.sbt&lt;/code&gt; for version)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Android NDK r21e+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;BUILD with Android Studio&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Warnning: Cannot build in windows&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://developer.android.com/studio&#34;&gt;Android Studio&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://developer.android.com/ndk/downloads/older_releases&#34;&gt;Android NDK r20b&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://plugins.jetbrains.com/plugin/1347-scala&#34;&gt;Scala&lt;/a&gt; plugin for IntelliJ IDEA&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set proxy for Android Studio: &lt;code&gt;File | Settings | Appearance &amp;amp; Behavior | System Settings | HTTP Proxy&lt;/code&gt;&lt;br&gt; Set proxy for sbt: &lt;code&gt;File | Settings | Build, Execution, Deployment | Build Tools | sbt&lt;/code&gt;, in &lt;code&gt;VM parameters&lt;/code&gt; input:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  -Dhttps.proxyHost=127.0.0.1&#xA;  -Dhttps.proxyPort=8080&#xA;  -Dhttp.proxyHost=127.0.0.1&#xA;  -Dhttp.proxyPort=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set environment variable &lt;code&gt;ANDROID_HOME&lt;/code&gt; to &lt;code&gt;/path/to/Android/Sdk&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set environment variable &lt;code&gt;ANDROID_NDK_HOME&lt;/code&gt; to &lt;code&gt;/path/to/Android/android-ndk-r21e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create your key following the instructions at &lt;a href=&#34;https://developer.android.com/studio/publish/app-signing.html&#34;&gt;https://developer.android.com/studio/publish/app-signing.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Put your key in ~/.keystore or any other place&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create &lt;code&gt;local.properties&lt;/code&gt; from &lt;code&gt;local.properties.example&lt;/code&gt; with your own key information&lt;/p&gt; &lt;pre&gt;&lt;code&gt;  key.alias: abc&#xA;  key.store: /path/to/Android/abc.jks&#xA;  key.store.password: abc&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;if you installed multiple versions of Java, use &lt;code&gt;sudo update-alternatives --config java&lt;/code&gt; to select Java 8&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Before build apk, make sure inside &lt;code&gt;./project/build.properties&lt;/code&gt;, sbt.version=0.13.18&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Invoke the building like this&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    export https_proxy=http://127.0.0.1:8080 # !important&#xA;    export ANDROID_HOME=/path/to/Android/Sdk&#xA;    export ANDROID_NDK_HOME=/path/to/Android/android-ndk-r20b&#xA;    # install and update all git submodule&#xA;    git submodule update --init&#xA;    # cd ./src/main/jni/shadowsocks-libev &amp;amp;&amp;amp; git checkout Akkariiin/master&#xA;    # Build the App and fix the problems as the error messages indicated&#xA;    sbt native-build clean android:package-release&#xA;    # run app&#xA;    sbt android:run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;If you use x64 linux like Archlinux x86_64, or your linux have new version ncurses lib, you may need install the 32bit version ncurses and link it as follow (make sure all these *.so files in the right location under your system, otherwise you have to copy them to /usr/lib/ and /usr/lib32/ directory):&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    # use Archlinux x86_64 as example&#xA;    &#xA;    # install ncurses x64 and x86 version&#xA;    sudo pacman -S lib32-ncurses ncurses&#xA;    &#xA;    # link the version-6 ncurses to version-5&#xA;    sudo ln -s /usr/lib/libncursesw.so /usr/lib/libncurses.so.5&#xA;    sudo ln -s /usr/lib32/libncursesw.so /usr/lib32/libncurses.so.5&#xA;    &#xA;    # link libncurses to libtinfo&#xA;    sudo ln -s /usr/lib/libncurses.so.5 /usr/lib/libtinfo.so.5&#xA;    sudo ln -s /usr/lib32/libncurses.so.5 /usr/lib32/libtinfo.so.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Integration with Android Studio&lt;/h4&gt; &#xA;&lt;p&gt;Checkout this &lt;a href=&#34;http://srodrigo.me/setting-up-scala-on-android/&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;rm -rf ~/.android/sbt/exploded-aars/*&lt;/li&gt; &#xA; &lt;li&gt;In Project Settings -&amp;gt; Modules -&amp;gt; shadowsocksr-v2ray-trojan-android, change &lt;code&gt;Structure&lt;/code&gt;, &lt;code&gt;Generated Sources&lt;/code&gt; to correct file path&lt;/li&gt; &#xA; &lt;li&gt;In Run/Debug Configuration -&amp;gt; Before launch, replace &lt;code&gt;Gradle-aware Make&lt;/code&gt; with &lt;code&gt;android:run&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;BUILD on Mac OS X (with HomeBrew)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Android SDK and NDK by run &lt;code&gt;brew install android-ndk android-sdk&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;export ANDROID_HOME=/usr/local/Cellar/android-sdk/$version&lt;/code&gt; to your .bashrc , then reopen the shell to load it.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;export ANDROID_NDK_HOME=/usr/local/Cellar/android-ndk/$version&lt;/code&gt; to your .bashrc , then reopen the shell to load it.&lt;/li&gt; &#xA; &lt;li&gt;echo &#34;y&#34; | android update sdk --filter tools,platform-tools,build-tools-23.0.2,android-23,extra-google-m2repository --no-ui -a&lt;/li&gt; &#xA; &lt;li&gt;echo &#34;y&#34; | android update sdk --filter extra-android-m2repository --no-ui --no-https -a&lt;/li&gt; &#xA; &lt;li&gt;Create your key following the instructions at &lt;a href=&#34;http://developer.android.com/guide/publishing/app-signing.html#cert&#34;&gt;http://developer.android.com/guide/publishing/app-signing.html#cert&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Put your key in ~/.keystore&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;code&gt;local.properties&lt;/code&gt; from &lt;code&gt;local.properties.example&lt;/code&gt; with your own key information .&lt;/li&gt; &#xA; &lt;li&gt;Invoke the building like this&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    git submodule update --init&#xA;&#xA;    # Build native binaries&#xA;    ./build.sh&#xA;&#xA;    # Build the apk&#xA;    sbt clean android:package-release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;OPEN SOURCE LICENSES&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;shadowsocks-libev: &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-libev/raw/master/LICENSE&#34;&gt;GPLv3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;tun2socks: &lt;a href=&#34;https://github.com/shadowsocks/badvpn/raw/shadowsocks-android/COPYING&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;redsocks: &lt;a href=&#34;https://github.com/shadowsocks/redsocks/raw/master/README&#34;&gt;APL 2.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenSSL: &lt;a href=&#34;https://github.com/shadowsocks/openssl-android/raw/master/NOTICE&#34;&gt;OpenSSL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pdnsd: &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-android/raw/master/src/main/jni/pdnsd/COPYING&#34;&gt;GPLv3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;libev: &lt;a href=&#34;https://github.com/shadowsocks/shadowsocks-android/raw/master/src/main/jni/libev/LICENSE&#34;&gt;GPLv2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;libevent: &lt;a href=&#34;https://github.com/shadowsocks/libevent/raw/master/LICENSE&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;v2ray-core: &lt;a href=&#34;https://github.com/v2fly/v2ray-core/raw/master/LICENSE&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;go-tun2socks: &lt;a href=&#34;https://github.com/eycorsican/go-tun2socks/raw/master/LICENSE&#34;&gt;BSD&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LICENSE&lt;/h3&gt; &#xA;&lt;p&gt;This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt; &#xA;&lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt; &#xA;&lt;p&gt;You should have received a copy of the GNU General Public License along with this program. If not, see &lt;a href=&#34;http://www.gnu.org/licenses/&#34;&gt;http://www.gnu.org/licenses/&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yanns/play2-scala-DI</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/yanns/play2-scala-DI</id>
    <link href="https://github.com/yanns/play2-scala-DI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;play2-scala-DI&lt;/h1&gt; &#xA;&lt;p&gt;Technical prototyp to test different dependency injection solutions.&lt;/p&gt; &#xA;&lt;p&gt;The target platform is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Play framework v2&lt;/li&gt; &#xA; &lt;li&gt;Scala&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Tested:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cake pattern&lt;/li&gt; &#xA; &lt;li&gt;injection with implicits.&lt;/li&gt; &#xA; &lt;li&gt;spring&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/scala-utils</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/olxbr/scala-utils</id>
    <link href="https://github.com/olxbr/scala-utils" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Utility code for Scala: logging, testing, configuration and more&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;scala-utils&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/grupozap/scala-utils&#34;&gt;&lt;img src=&#34;https://travis-ci.org/grupozap/scala-utils.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;scala-utils&lt;/code&gt; is an utility library that attempts to add useful code rapidly in your development pipeline, so that you can focus on what is really needed. It does not replace any existing library, instead it allows you to add production-ready features.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt;: Add features such as a configurable GELF log formatter without the need of a full Graylog connector library (&lt;a href=&#34;https://github.com/grupozap/scala-utils/tree/master/src/main/scala/com/grupozap/scalautils/logging&#34;&gt;https://github.com/grupozap/scala-utils/tree/master/src/main/scala/com/grupozap/scalautils/logging&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Use it in your project&lt;/h2&gt; &#xA;&lt;h3&gt;SBT&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;br.com.gzvr&#34; %% &#34;scala-utils&#34; % &#34;1.1.0&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll need to add our JFrog repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;resolvers += &#34;Artifactory&#34; at &#34;https://squadzapquality.jfrog.io/artifactory/olxbr-sbt-release/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supported Scala versions: &lt;code&gt;2.11&lt;/code&gt; and &lt;code&gt;2.12&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Renato Silva (&lt;a href=&#34;https://github.com/resilva87&#34;&gt;https://github.com/resilva87&lt;/a&gt;) - maintainer&lt;/li&gt; &#xA; &lt;li&gt;Thiago Pereira (&lt;a href=&#34;https://github.com/thiagoandrade6&#34;&gt;https://github.com/thiagoandrade6&lt;/a&gt;) - maintainer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you found a bug in the source code or if you want to contribute with new features, you can help submitting an issue; even better if you can submit a pull request :)&lt;/p&gt; &#xA;&lt;h3&gt;Publish&lt;/h3&gt; &#xA;&lt;p&gt;Once you merge your code to the master branch, &lt;a href=&#34;https://github.com/olxbr/scala-utils/actions&#34;&gt;GitHub Actions&lt;/a&gt; should automatically publish it.&lt;/p&gt; &#xA;&lt;p&gt;To publish manually, create a &lt;code&gt;credentials.properties&lt;/code&gt; file in the project&#39;s directory, with &lt;a href=&#34;https://vault.grupozap.io/ui/vault/secrets/squad-quality/show/servicos/jfrog-quality&#34;&gt;the content you can find here&lt;/a&gt;, and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sbt clean compile package publish&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>apache/tvm-vta</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/apache/tvm-vta</id>
    <link href="https://github.com/apache/tvm-vta" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open, Modular, Deep Learning Accelerator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VTA Hardware Design Stack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ci.tlcpack.ai/job/tvm-vta/job/main/&#34;&gt;&lt;img src=&#34;https://ci.tlcpack.ai/job/tvm-vta/job/main/badge/icon&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VTA (versatile tensor accelerator) is an open-source deep learning accelerator complemented with an end-to-end TVM-based compiler stack.&lt;/p&gt; &#xA;&lt;p&gt;The key features of VTA include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generic, modular, open-source hardware &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Streamlined workflow to deploy to FPGAs.&lt;/li&gt; &#xA;   &lt;li&gt;Simulator support to prototype compilation passes on regular workstations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Driver and JIT runtime for both simulator and FPGA hardware back-end.&lt;/li&gt; &#xA; &lt;li&gt;End-to-end TVM stack integration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Direct optimization and deployment of models from deep learning frameworks via TVM.&lt;/li&gt; &#xA;   &lt;li&gt;Customized and extensible TVM compiler back-end.&lt;/li&gt; &#xA;   &lt;li&gt;Flexible RPC support to ease deployment, and program FPGAs with the convenience of Python.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>circe/circe</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/circe/circe</id>
    <link href="https://github.com/circe/circe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Yet another JSON library for Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;circe&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/circe/circe/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/circe/circe/Continuous%20Integration.svg?sanitize=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/circe/circe&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/circe/circe/master.svg?sanitize=true&#34; alt=&#34;Coverage status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/circe/circe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/gitter-join%20chat-green.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/io.circe/circe-core_2.13&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/io.circe/circe-core_2.13.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;circe is a JSON library for Scala (and &lt;a href=&#34;http://www.scala-js.org/&#34;&gt;Scala.js&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://circe.github.io/circe/&#34;&gt;guide&lt;/a&gt; for more information about why circe exists and how to use it.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;h3&gt;Adopters&lt;/h3&gt; &#xA;&lt;p&gt;Are you using circe? Please consider opening a pull request to list your organization here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://abacusprotocol.com/&#34;&gt;Abacus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://anduintransact.com/&#34;&gt;Anduin Transactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apolloagriculture.com/&#34;&gt;Apollo Agriculture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.autoscout24.com/&#34;&gt;AutoScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.babylonhealth.com/&#34;&gt;Babylon Health&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://banno.com/&#34;&gt;Banno inside of Jack Henry &amp;amp; Associates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.bbc.co.uk&#34;&gt;BBC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.broadinstitute.org/data-sciences-platform&#34;&gt;Broad Institute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chartboost.com/&#34;&gt;Chartboost&lt;/a&gt; (sending hundreds of thousands of messages per second on our Ad Exchange)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.cibotechnologies.com&#34;&gt;CiBO Technologies&lt;/a&gt; (using circe to (de)serialize data in support of a sustainable revolution in agriculture)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.clearscore.com&#34;&gt;ClearScore&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.codacy.com&#34;&gt;Codacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.colisweb.com&#34;&gt;Colisweb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.connio.com&#34;&gt;Connio&lt;/a&gt; (creating and managing digital twins with Circe and Akka)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coya.com/&#34;&gt;Coya&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.criteo.com/&#34;&gt;Criteo&lt;/a&gt; (&lt;a href=&#34;https://medium.com/criteo-labs/migrate-a-service-getting-200kqps-from-jackson-to-circe-a475b2718206&#34;&gt;collecting 200.000 events per second from our banners&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://datalogue.io&#34;&gt;Datalogue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.dreamlines.com/&#34;&gt;Dreamlines&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drivetribe.com&#34;&gt;DriveTribe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.earnest.com&#34;&gt;Earnest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.elastic.co&#34;&gt;Elastic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://emmy-sharing.de/en/&#34;&gt;Emmy Sharing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://folio-sec.com/&#34;&gt;FOLIO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://glngn.com&#34;&gt;GLNGN Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gympass.com/&#34;&gt;Gympass&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gutefrage.net&#34;&gt;Gutefrage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hellosoda.com/&#34;&gt;Hello Soda&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.here.com/&#34;&gt;HERE Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.holidaycheck.de&#34;&gt;HolidayCheck&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.immobilienscout24.de/&#34;&gt;ImmobilienScout24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.immozentral.com/&#34;&gt;Immozentral&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.indix.com&#34;&gt;Indix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.itv.com/&#34;&gt;ITV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kinoplan.ru/&#34;&gt;Kinoplan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.latitudefinancial.com.au/&#34;&gt;Latitude Financial Services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.matchesfashion.com&#34;&gt;MatchesFashion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://merits.com&#34;&gt;Merit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.metacommerce.ru&#34;&gt;Metacommerce&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://miras-tech.com/&#34;&gt;Miras Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mobile.de&#34;&gt;Mobile GmbH&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.new-work.se/en/&#34;&gt;New Work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ocadotechnology.com&#34;&gt;Ocado Technology&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onairentertainment.com/&#34;&gt;On Air Entertainment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://onfocus.io&#34;&gt;Onfocus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opt-technologies.jp/&#34;&gt;Opt Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ovoenergy.com&#34;&gt;OVO Energy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://panaseer.com&#34;&gt;Panaseer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://permutive.com&#34;&gt;Permutive&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prezi.com&#34;&gt;Prezi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.projectseptember.com&#34;&gt;Project September&lt;/a&gt; (using circe to exchange and store data within the platform and serve data using GraphQL with Sangria)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/raster-foundry/raster-foundry/&#34;&gt;Raster Foundry&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://ravellaw.com/technology/&#34;&gt;Ravel Law&lt;/a&gt; (using circe to (de)serialize data for search, analytics, and visualization of tens of millions of legal opinions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.realestate.com.au/&#34;&gt;REA Group - realestate.com.au&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reonomy.com/&#34;&gt;Reonomy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://resilientplc.com/&#34;&gt;Resilient plc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sky.com/&#34;&gt;Sky&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://snowplowanalytics.com/&#34;&gt;Snowplow Analytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.soundcloud.com&#34;&gt;SoundCloud&lt;/a&gt; (transforming 200,000,000 JSON events every hour in MapReduce ETLs)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.spotify.com&#34;&gt;Spotify&lt;/a&gt; (using circe for JSON IO in &lt;a href=&#34;https://github.com/spotify/scio&#34;&gt;Scio&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.spotx.tv/&#34;&gt;SpotX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stripe.com&#34;&gt;Stripe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stylight.de&#34;&gt;Stylight&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://tabmo-group.io/&#34;&gt;TabMo&lt;/a&gt; (parsing more than 100k events per second with Akka Stream and Spark)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinkoff.ru/&#34;&gt;Tinkoff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.theguardian.com&#34;&gt;The Guardian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.threatstack.com/&#34;&gt;Threat Stack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tranzzo.com/&#34;&gt;Tranzzo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.twilio.com&#34;&gt;Twilio&lt;/a&gt; (sending many, many millions of messages a day with Circe and Akka)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://veact.net/&#34;&gt;VEACT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wework.com&#34;&gt;WeWork&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://whisk.com&#34;&gt;Whisk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zalando.de&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zendesk.com&#34;&gt;Zendesk&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other circe organization projects&lt;/h3&gt; &#xA;&lt;p&gt;Please get in touch on &lt;a href=&#34;https://gitter.im/circe/circe&#34;&gt;Gitter&lt;/a&gt; if you have a circe-related project that you&#39;d like to discuss hosting under the &lt;a href=&#34;https://github.com/circe&#34;&gt;circe organization&lt;/a&gt; on GitHub.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-benchmarks&#34;&gt;circe-benchmarks&lt;/a&gt;: Benchmarks for comparing the performance of circe and other JSON libraries for the JVM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-config&#34;&gt;circe-config&lt;/a&gt;: A library for translating between HOCON, Java properties, and JSON documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-derivation&#34;&gt;circe-derivation&lt;/a&gt;: Experimental generic derivation with improved compile times.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-fs2&#34;&gt;circe-fs2&lt;/a&gt;: A library that provides streaming JSON parsing and decoding built on &lt;a href=&#34;https://github.com/functional-streams-for-scala/fs2&#34;&gt;fs2&lt;/a&gt; and &lt;a href=&#34;https://github.com/non/jawn&#34;&gt;Jawn&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-iteratee&#34;&gt;circe-iteratee&lt;/a&gt;: A library that provides streaming JSON parsing and decoding built on &lt;a href=&#34;https://github.com/travisbrown/iteratee&#34;&gt;iteratee.io&lt;/a&gt; and &lt;a href=&#34;https://github.com/non/jawn&#34;&gt;Jawn&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-jackson&#34;&gt;circe-jackson&lt;/a&gt;: A library that provides &lt;a href=&#34;https://github.com/FasterXML/jackson&#34;&gt;Jackson&lt;/a&gt;-supported parsing and printing for circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-spray&#34;&gt;circe-spray&lt;/a&gt;: A library that provides JSON marshallers and unmarshallers for &lt;a href=&#34;http://spray.io/&#34;&gt;Spray&lt;/a&gt; using circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circe/circe-yaml&#34;&gt;circe-yaml&lt;/a&gt;: A library that uses &lt;a href=&#34;https://bitbucket.org/asomov/snakeyaml&#34;&gt;SnakeYAML&lt;/a&gt; to support parsing YAML 1.1 into circe&#39;s &lt;code&gt;Json&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Related projects&lt;/h3&gt; &#xA;&lt;p&gt;The following open source projects are either built on circe or provide circe support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://actor.im/&#34;&gt;Actor Messenger&lt;/a&gt;: A platform for instant messaging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hseeberger/akka-http-json&#34;&gt;akka-http-json&lt;/a&gt;: A library that supports using circe for JSON marshalling and unmarshalling in &lt;a href=&#34;http://doc.akka.io/docs/akka/current/scala/http/&#34;&gt;Akka HTTP&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/knutwalker/akka-stream-json&#34;&gt;akka-stream-json&lt;/a&gt;: A library that provides JSON support for stream based applications using Jawn as a parser with a convenience example for circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishfenton/Argus&#34;&gt;Argus&lt;/a&gt;: Generates models and circe encoders and decoders from JSON schemas.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/blackdoor/jose&#34;&gt;Blackdoor JOSE&lt;/a&gt;: circe JSON support for blackdoor JOSE and JWT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sirthias.github.io/borer/&#34;&gt;borer&lt;/a&gt;: Allows circe encoders/decoders to be reused for CBOR (de)serialization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/compstak/circe-debezium&#34;&gt;circe-debezium&lt;/a&gt;: Circe codecs for &lt;a href=&#34;https://debezium.io/&#34;&gt;Debezium&lt;/a&gt; payload types&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/compstak/circe-geojson&#34;&gt;circe-geojson&lt;/a&gt;: Circe support for GeoJSON (RFC 7946)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NeQuissimus/circe-kafka&#34;&gt;circe-kafka&lt;/a&gt;: Implicit conversion of Encoder and Decoder into Kafka Serializer/Deserializer/Serde&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/agourlay/cornichon&#34;&gt;cornichon&lt;/a&gt;: A DSL for JSON API testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dcos/cosmos&#34;&gt;Cosmos&lt;/a&gt;: An API for &lt;a href=&#34;https://dcos.io/&#34;&gt;DCOS&lt;/a&gt; services that uses circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fthomas/crjdt&#34;&gt;crjdt&lt;/a&gt;: A conflict-free replicated JSON datatype in Scala.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gnieh/diffson&#34;&gt;diffson&lt;/a&gt;: A Scala diff / patch library for JSON.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sksamuel/elastic4s&#34;&gt;elastic4s&lt;/a&gt;: A Scala client for &lt;a href=&#34;https://www.elastic.co/&#34;&gt;Elasticsearch&lt;/a&gt; with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lloydmeta/enumeratum&#34;&gt;Enumeratum&lt;/a&gt;: Enumerations for Scala with circe integration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/finagle/featherbed&#34;&gt;Featherbed&lt;/a&gt;: A REST client library with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/finagle/finch&#34;&gt;Finch&lt;/a&gt;: A library for building web services with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/daviddenton/fintrospect&#34;&gt;fintrospect&lt;/a&gt;: HTTP contracts for &lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;Finagle&lt;/a&gt; with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tkrs/fluflu&#34;&gt;fluflu&lt;/a&gt;: A &lt;a href=&#34;http://www.fluentd.org/&#34;&gt;Fluentd&lt;/a&gt; logger.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/47deg/github4s&#34;&gt;Github4s&lt;/a&gt;: A GitHub API wrapper written in Scala.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guardian/content-api-models&#34;&gt;content-api-models&lt;/a&gt;: The Guardian&#39;s Content API Thrift models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/http4s/http4s&#34;&gt;http4s&lt;/a&gt;: A purely functional HTTP library for client and server applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pshirshov/izumi-r2&#34;&gt;IdeaLingua&lt;/a&gt;: Staged Interface Definition and Data Modeling Language &amp;amp; RPC system currently targeting Scala, Go, C# and TypeScript. Scala codegen generates models and JSON codecs using circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snowplow/iglu&#34;&gt;Iglu Schema Repository&lt;/a&gt;: A &lt;a href=&#34;http://json-schema.org/&#34;&gt;JSON Schema&lt;/a&gt; repository with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/codemettle/jsactor&#34;&gt;jsactor&lt;/a&gt;: An actor library for Scala.js with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plokhotnyuk/jsoniter-scala/tree/master/jsoniter-scala-circe&#34;&gt;jsoniter-scala-circe&lt;/a&gt;: A booster for faster parsing/printing to/from circe AST and decoding/encoding of &lt;code&gt;java.time._&lt;/code&gt; and &lt;code&gt;BigInt&lt;/code&gt; types.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://pauldijou.fr/jwt-scala/samples/jwt-circe/&#34;&gt;jwt-circe&lt;/a&gt;: A &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32&#34;&gt;JSON Web Token&lt;/a&gt; implementation with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bitbucket.org/atlassian/kadai-log&#34;&gt;kadai-log&lt;/a&gt;: A logging library with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/msgpack4z/msgpack4z-circe&#34;&gt;msgpack4z-circe&lt;/a&gt;: A &lt;a href=&#34;https://github.com/msgpack/msgpack/raw/master/spec.md&#34;&gt;MessagePack&lt;/a&gt; implementation with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/djx314/ohNoMyCirce&#34;&gt;ohNoMyCirce&lt;/a&gt;: Friendly compile error messages for &lt;a href=&#34;https://github.com/milessabin/shapeless&#34;&gt;shapeless&lt;/a&gt;&#39;s Generic, &lt;a href=&#34;https://github.com/circe&#34;&gt;circe&lt;/a&gt;&#39;s Encoder &amp;amp; Decoder and &lt;a href=&#34;http://slick.lightbend.com/&#34;&gt;slick&lt;/a&gt;&#39;s case class mapping.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jilen/play-circe&#34;&gt;play-circe&lt;/a&gt;: circe support for &lt;a href=&#34;https://www.playframework.com/&#34;&gt;Play!&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sksamuel/pulsar4s&#34;&gt;pulsar4s&lt;/a&gt;: A Scala client for &lt;a href=&#34;https://pulsar.apache.org/&#34;&gt;Apache-Pulsar&lt;/a&gt; with circe support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rapture.io/&#34;&gt;Rapture&lt;/a&gt;: Support for using circe&#39;s parsing and AST in Rapture JSON.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/finagle/roc&#34;&gt;roc&lt;/a&gt;: A PostgreSQL client built on Finagle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sangria-graphql/sangria-circe&#34;&gt;sangria-circe&lt;/a&gt;: circe marshalling for &lt;a href=&#34;http://sangria-graphql.org/&#34;&gt;Sangria&lt;/a&gt;, a &lt;a href=&#34;http://graphql.org/docs/getting-started/&#34;&gt;GraphQL&lt;/a&gt; implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vpavkin/scalist&#34;&gt;scalist&lt;/a&gt;: A &lt;a href=&#34;https://developer.todoist.com/&#34;&gt;Todoist&lt;/a&gt; API client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/scala-jsonapi/scala-jsonapi&#34;&gt;scala-jsonapi&lt;/a&gt;: Scala support library for integrating the JSON API spec with Spray, Play! or Circe&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shogowada/scala-json-rpc&#34;&gt;scala-json-rpc&lt;/a&gt;: &lt;a href=&#34;http://www.jsonrpc.org&#34;&gt;JSON-RPC&lt;/a&gt; 2.0 library for Scala and Scala.js&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephennancekivell/scalatest-json&#34;&gt;scalatest-json-circe&lt;/a&gt;: Scalatest matchers for Json with appropriate equality and descriptive error messages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/scio&#34;&gt;Scio&lt;/a&gt;: A Scala API for Apache Beam and Google Cloud Dataflow, uses circe for JSON IO&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/durban/seals/&#34;&gt;seals&lt;/a&gt;: Tools for schema evolution and language-integrated schemata (derives circe encoders and decoders).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/labra/shaclex&#34;&gt;shaclex&lt;/a&gt;: RDF validation using SHACL or ShEx.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tminglei/slick-pg&#34;&gt;Slick-pg&lt;/a&gt;: &lt;a href=&#34;http://slick.lightbend.com/&#34;&gt;Slick&lt;/a&gt; extensions for PostgreSQL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/softwaremill/sttp&#34;&gt;sttp&lt;/a&gt;: Scala HTTP client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mrdimosthenis.github.io/Synapses&#34;&gt;Synapses&lt;/a&gt;: A lightweight Neural Network library, for js, jvm and .net.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nikdon/telepooz&#34;&gt;telepooz&lt;/a&gt;: A Scala wrapper for the &lt;a href=&#34;https://core.telegram.org/bots/api&#34;&gt;Telegram Bot API&lt;/a&gt; built on circe.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sungiant/zenith&#34;&gt;Zenith&lt;/a&gt;: Functional HTTP library built on circe.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;The following projects provide examples, templates, or benchmarks that include circe:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alanphillips78/akka-http-microservice-blueprint&#34;&gt;https://github.com/alanphillips78/akka-http-microservice-blueprint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bneil/fcs_boilerplate&#34;&gt;https://github.com/bneil/fcs_boilerplate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gvolpe/simple-http4s-api&#34;&gt;https://github.com/gvolpe/simple-http4s-api&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vitorsvieira/akka-http-circe-json-template&#34;&gt;https://github.com/vitorsvieira/akka-http-circe-json-template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephennancekivell/some-jmh-json-benchmarks-circe-jackson&#34;&gt;https://github.com/stephennancekivell/some-jmh-json-benchmarks-circe-jackson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pauljamescleary/scala-pet-store&#34;&gt;https://github.com/pauljamescleary/scala-pet-store&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors and participation&lt;/h2&gt; &#xA;&lt;p&gt;circe is a fork of &lt;a href=&#34;http://argonaut.io/&#34;&gt;Argonaut&lt;/a&gt;, and if you find it at all useful, you should thank &lt;a href=&#34;https://github.com/markhibberd&#34;&gt;Mark Hibberd&lt;/a&gt;, &lt;a href=&#34;https://github.com/tonymorris&#34;&gt;Tony Morris&lt;/a&gt;, &lt;a href=&#34;https://github.com/xuwei-k&#34;&gt;Kenji Yoshida&lt;/a&gt;, and the rest of the &lt;a href=&#34;https://github.com/argonaut-io/argonaut/graphs/contributors&#34;&gt;Argonaut contributors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;circe is currently maintained by &lt;a href=&#34;https://github.com/zarthross/&#34;&gt;Darren Gibson&lt;/a&gt; and &lt;a href=&#34;https://github.com/zmccoy/&#34;&gt;Zach McCoy&lt;/a&gt;. After the 1.0 release, all pull requests will require two sign-offs by a maintainer to be merged.&lt;/p&gt; &#xA;&lt;p&gt;The circe project supports the &lt;a href=&#34;https://www.scala-lang.org/conduct/&#34;&gt;Scala code of conduct&lt;/a&gt; and wants all of its channels (Gitter, GitHub, etc.) to be inclusive environments.&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/circe/circe/main/CONTRIBUTING.md&#34;&gt;contributors&#39; guide&lt;/a&gt; for details on how to submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;circe is licensed under the &lt;strong&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/strong&gt; (the &#34;License&#34;); you may not use this software except in compliance with the License.&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-csv</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/databricks/spark-csv</id>
    <link href="https://github.com/databricks/spark-csv" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CSV Data Source for Apache Spark 1.x&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CSV Data Source for Apache Spark 1.x&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE: This functionality has been inlined in Apache Spark 2.x. This package is in maintenance mode and we only accept critical bug fixes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;A library for parsing and querying CSV data with Apache Spark, for Spark SQL and DataFrames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-csv&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-csv.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://codecov.io/github/databricks/spark-csv?branch=master&#34;&gt;&lt;img src=&#34;http://codecov.io/github/databricks/spark-csv/coverage.svg?branch=master&#34; alt=&#34;codecov.io&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This library requires Spark 1.3+&lt;/p&gt; &#xA;&lt;h2&gt;Linking&lt;/h2&gt; &#xA;&lt;p&gt;You can link against this library in your program at the following coordinates:&lt;/p&gt; &#xA;&lt;h3&gt;Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.10&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;groupId: com.databricks&#xA;artifactId: spark-csv_2.11&#xA;version: 1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using with Spark shell&lt;/h2&gt; &#xA;&lt;p&gt;This package can be added to Spark using the &lt;code&gt;--packages&lt;/code&gt; command line option. For example, to include it when starting the spark shell:&lt;/p&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.11&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Spark compiled with Scala 2.10&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This package allows reading CSV files in local or distributed filesystem as &lt;a href=&#34;https://spark.apache.org/docs/1.6.0/sql-programming-guide.html&#34;&gt;Spark DataFrames&lt;/a&gt;. When reading files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files. Similar to Spark can accept standard Hadoop globbing expressions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true the first line of files will be used to name columns and will not be included in data. All types will be assumed string. Default value is false.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. Delimiters inside quotes are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are ignored&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;parserLib&lt;/code&gt;: by default it is &#34;commons&#34; can be set to &#34;univocity&#34; to use that library for CSV parsing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: determines the parsing mode. By default it is PERMISSIVE. Possible values are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;PERMISSIVE&lt;/code&gt;: tries to parse all lines: nulls are inserted for missing tokens and extra tokens are ignored.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;DROPMALFORMED&lt;/code&gt;: drops lines which have fewer or more tokens than expected or tokens which do not match the schema&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;FAILFAST&lt;/code&gt;: aborts with a RuntimeException if encounters any malformed line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;charset&lt;/code&gt;: defaults to &#39;UTF-8&#39; but can be set to other valid charset names&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inferSchema&lt;/code&gt;: automatically infers column types. It requires one extra pass over the data and is false by default&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;comment&lt;/code&gt;: skip lines beginning with this character. Default is &lt;code&gt;&#34;#&#34;&lt;/code&gt;. Disable comments by setting this to &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, any fields matching this string will be set as nulls in the DataFrame&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use when reading dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. By default, it is &lt;code&gt;null&lt;/code&gt; which means trying to parse times and date by &lt;code&gt;java.sql.Timestamp.valueOf()&lt;/code&gt; and &lt;code&gt;java.sql.Date.valueOf()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The package also supports saving simple (non-nested) DataFrame. When writing files the API accepts several options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;header&lt;/code&gt;: when set to true, the header (from the schema in the DataFrame) will be written at the first line.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;delimiter&lt;/code&gt;: by default columns are delimited using &lt;code&gt;,&lt;/code&gt;, but delimiter can be set to any character&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quote&lt;/code&gt;: by default the quote character is &lt;code&gt;&#34;&lt;/code&gt;, but can be set to any character. This is written according to &lt;code&gt;quoteMode&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;escape&lt;/code&gt;: by default the escape character is &lt;code&gt;\&lt;/code&gt;, but can be set to any character. Escaped quote characters are written.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nullValue&lt;/code&gt;: specifies a string that indicates a null value, nulls in the DataFrame will be written as this string.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dateFormat&lt;/code&gt;: specifies a string that indicates the date format to use writing dates or timestamps. Custom date formats follow the formats at &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;java.text.SimpleDateFormat&lt;/code&gt;&lt;/a&gt;. This applies to both &lt;code&gt;DateType&lt;/code&gt; and &lt;code&gt;TimestampType&lt;/code&gt;. If no dateFormat is specified, then &#34;yyyy-MM-dd HH:mm:ss.S&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;codec&lt;/code&gt;: compression codec to use when saving to file. Should be the fully qualified name of a class implementing &lt;code&gt;org.apache.hadoop.io.compress.CompressionCodec&lt;/code&gt; or one of case-insensitive shorten names (&lt;code&gt;bzip2&lt;/code&gt;, &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;lz4&lt;/code&gt;, and &lt;code&gt;snappy&lt;/code&gt;). Defaults to no compression when a codec is not specified.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;quoteMode&lt;/code&gt;: when to quote fields (&lt;code&gt;ALL&lt;/code&gt;, &lt;code&gt;MINIMAL&lt;/code&gt; (default), &lt;code&gt;NON_NUMERIC&lt;/code&gt;, &lt;code&gt;NONE&lt;/code&gt;), see &lt;a href=&#34;https://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/QuoteMode.html&#34;&gt;Quote Modes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These examples use a CSV file available for download &lt;a href=&#34;https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#34;&gt;here&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL API&lt;/h3&gt; &#xA;&lt;p&gt;CSV data source for Spark can infer data types:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;, inferSchema &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify column names and types in DDL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE cars (yearMade double, carMake string, carModel string, comments string, blank string)&#xA;USING com.databricks.spark.csv&#xA;OPTIONS (path &#34;cars.csv&#34;, header &#34;true&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Scala API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .schema(customSchema)&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.read&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;) // Use first line of all files as header&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;) // Automatically infer data types&#xA;    .load(&#34;cars.csv&#34;)&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.write&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv.gz&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;, &#34;inferSchema&#34; -&amp;gt; &#34;true&#34;))&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify the schema when reading data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};&#xA;&#xA;val sqlContext = new SQLContext(sc)&#xA;val customSchema = StructType(Array(&#xA;    StructField(&#34;year&#34;, IntegerType, true),&#xA;    StructField(&#34;make&#34;, StringType, true),&#xA;    StructField(&#34;model&#34;, StringType, true),&#xA;    StructField(&#34;comment&#34;, StringType, true),&#xA;    StructField(&#34;blank&#34;, StringType, true)))&#xA;&#xA;val df = sqlContext.load(&#xA;    &#34;com.databricks.spark.csv&#34;,&#xA;    schema = customSchema,&#xA;    Map(&#34;path&#34; -&amp;gt; &#34;cars.csv&#34;, &#34;header&#34; -&amp;gt; &#34;true&#34;))&#xA;&#xA;val selectedData = df.select(&#34;year&#34;, &#34;model&#34;)&#xA;selectedData.save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Java API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .schema(customSchema)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;DataFrame df = sqlContext.read()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;inferSchema&#34;, &#34;true&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .load(&#34;cars.csv&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).write()&#xA;    .format(&#34;com.databricks.spark.csv&#34;)&#xA;    .option(&#34;header&#34;, &#34;true&#34;)&#xA;    .option(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;    .save(&#34;newcars.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.types.*;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;StructType customSchema = new StructType(new StructField[] {&#xA;    new StructField(&#34;year&#34;, DataTypes.IntegerType, true, Metadata.empty()),&#xA;    new StructField(&#34;make&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;model&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;comment&#34;, DataTypes.StringType, true, Metadata.empty()),&#xA;    new StructField(&#34;blank&#34;, DataTypes.StringType, true, Metadata.empty())&#xA;});&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, customSchema, options);&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import org.apache.spark.sql.SQLContext;&#xA;import org.apache.spark.sql.SaveMode;&#xA;&#xA;SQLContext sqlContext = new SQLContext(sc);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; options = new HashMap&amp;lt;String, String&amp;gt;();&#xA;options.put(&#34;header&#34;, &#34;true&#34;);&#xA;options.put(&#34;path&#34;, &#34;cars.csv&#34;);&#xA;options.put(&#34;inferSchema&#34;, &#34;true&#34;);&#xA;&#xA;DataFrame df = sqlContext.load(&#34;com.databricks.spark.csv&#34;, options);&#xA;&#xA;HashMap&amp;lt;String, String&amp;gt; saveOptions = new HashMap&amp;lt;String, String&amp;gt;();&#xA;saveOptions.put(&#34;header&#34;, &#34;true&#34;);&#xA;saveOptions.put(&#34;path&#34;, &#34;newcars.csv&#34;);&#xA;saveOptions.put(&#34;codec&#34;, &#34;org.apache.hadoop.io.compress.GzipCodec&#34;);&#xA;&#xA;df.select(&#34;year&#34;, &#34;model&#34;).save(&#34;com.databricks.spark.csv&#34;, SaveMode.Overwrite,&#xA;                                saveOptions);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.read \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .options(header=&#39;true&#39;) \&#xA;    .load(&#39;cars.csv&#39;, schema = customSchema)&#xA;&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write \&#xA;    .format(&#39;com.databricks.spark.csv&#39;) \&#xA;    .save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.read.format(&#39;com.databricks.spark.csv&#39;).options(header=&#39;true&#39;, inferschema=&#39;true&#39;).load(&#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).write.format(&#39;com.databricks.spark.csv&#39;).options(codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;).save(&#39;newcars.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.3:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;from pyspark.sql.types import *&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;customSchema = StructType([ \&#xA;    StructField(&#34;year&#34;, IntegerType(), True), \&#xA;    StructField(&#34;make&#34;, StringType(), True), \&#xA;    StructField(&#34;model&#34;, StringType(), True), \&#xA;    StructField(&#34;comment&#34;, StringType(), True), \&#xA;    StructField(&#34;blank&#34;, StringType(), True)])&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, schema = customSchema, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;df = sqlContext.load(source=&#34;com.databricks.spark.csv&#34;, header = &#39;true&#39;, inferSchema = &#39;true&#39;, path = &#39;cars.csv&#39;)&#xA;df.select(&#39;year&#39;, &#39;model&#39;).save(&#39;newcars.csv&#39;, &#39;com.databricks.spark.csv&#39;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;R API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spark 1.4+:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Automatically infer schema (data types), otherwise everything is assumed string:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually specify schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;customSchema &amp;lt;- structType(&#xA;    structField(&#34;year&#34;, &#34;integer&#34;),&#xA;    structField(&#34;make&#34;, &#34;string&#34;),&#xA;    structField(&#34;model&#34;, &#34;string&#34;),&#xA;    structField(&#34;comment&#34;, &#34;string&#34;),&#xA;    structField(&#34;blank&#34;, &#34;string&#34;))&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, schema = customSchema)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can save with compressed output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(SparkR)&#xA;&#xA;Sys.setenv(&#39;SPARKR_SUBMIT_ARGS&#39;=&#39;&#34;--packages&#34; &#34;com.databricks:spark-csv_2.10:1.4.0&#34; &#34;sparkr-shell&#34;&#39;)&#xA;sqlContext &amp;lt;- sparkRSQL.init(sc)&#xA;&#xA;df &amp;lt;- read.df(sqlContext, &#34;cars.csv&#34;, source = &#34;com.databricks.spark.csv&#34;, inferSchema = &#34;true&#34;)&#xA;&#xA;write.df(df, &#34;newcars.csv&#34;, &#34;com.databricks.spark.csv&#34;, &#34;overwrite&#34;, codec=&#34;org.apache.hadoop.io.compress.GzipCodec&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building From Source&lt;/h2&gt; &#xA;&lt;p&gt;This library is built with &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html&#34;&gt;SBT&lt;/a&gt;, which is automatically downloaded by the included shell script. To build a JAR file simply run &lt;code&gt;sbt/sbt package&lt;/code&gt; from the project root. The build configuration includes support for both Scala 2.10 and 2.11.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/sbt-databricks</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/databricks/sbt-databricks</id>
    <link href="https://github.com/databricks/sbt-databricks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An sbt plugin for deploying code to Databricks Cloud&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;sbt-databricks &lt;a href=&#34;http://travis-ci.org/databricks/sbt-databricks&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/sbt-databricks.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;sbt plugin to deploy your projects to Databricks!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://go.databricks.com/register-for-dbc&#34;&gt;http://go.databricks.com/register-for-dbc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;An Account on Databricks: &lt;a href=&#34;https://accounts.cloud.databricks.com/registration.html#signup&#34;&gt;Sign up for a free trial.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Just add the following line to &lt;code&gt;project/plugins.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;addSbtPlugin(&#34;com.databricks&#34; %% &#34;sbt-databricks&#34; % &#34;0.1.5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are running Databricks version 2.18 or greater you must use sbt-databricks version 0.1.5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are running Databricks version 2.8 or greater you must use sbt-databricks version 0.1.3&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Enable sbt-databricks for all your projects&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;sbt-databricks&lt;/code&gt; can be enabled as a &lt;a href=&#34;http://www.scala-sbt.org/0.13/tutorial/Using-Plugins.html#Global+plugins&#34;&gt;global plugin&lt;/a&gt; for use in all of your projects in two easy steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the following line to &lt;code&gt;~/.sbt/0.13/plugins/build.sbt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;addSbtPlugin(&#34;com.databricks&#34; %% &#34;sbt-databricks&#34; % &#34;0.1.5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the settings defined &lt;a href=&#34;https://raw.githubusercontent.com/databricks/sbt-databricks/master/#settings&#34;&gt;here&lt;/a&gt; in &lt;code&gt;~/.sbt/0.13/databricks.sbt&lt;/code&gt;. You&#39;ll have to add the line&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import sbtdatabricks.DatabricksPlugin.autoImport._&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to that file in order to import this plugin&#39;s settings into that configuration file.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h3&gt;Cluster Controls&lt;/h3&gt; &#xA;&lt;p&gt;There are three primary cluster related actions: Create, Resize and Delete.&lt;/p&gt; &#xA;&lt;p&gt;Creating a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcCreateCluster // Attempts to create a cluster on DBC&#xA;// The following parameters must be set when attempting to create a cluster&#xA;dbcNumWorkerContainers := // Integer: The desired size of the cluster (in worker containers). &#xA;dbcSpotInstance := // Boolean for choosing whether to use Spot or On-Demand instances&#xA;dbcSparkVersion := // String: The Spark version to be used e.g. &#34;1.6.x&#34;&#xA;dbcZoneId := // String: AWS zone e.g. ap-southeast-2&#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Resizing a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcResizeCluster // Attempts to resize a cluster on DBC&#xA;// The following parameters must be set when attempting to resize a cluster&#xA;dbcNumWorkerContainers := // Integer: The desired size of the cluster (in worker containers). &#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Deleting a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcDeleteCluster // Attempts to delete a cluster on DBC&#xA;// The following parameters must be set when attempting to resize a cluster&#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;There are four major commands that can be used. Please check the next section for mandatory settings before running these commands.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcDeploy&lt;/code&gt;: Uploads your Library to Databricks Cloud, attaches it to specified clusters, and restarts the clusters if a previous version of the library was attached. This method encapsulates the following commands. Only libraries with &lt;code&gt;SNAPSHOT&lt;/code&gt; versions will be deleted and re-uploaded as it is assumed that dependencies will not change very frequently. If you change the version of one of your dependencies, that dependency must be deleted manually in Databricks Cloud to prevent unexpected behavior.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcUpload&lt;/code&gt;: Uploads your Library to Databricks Cloud. Deletes the older version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcAttach&lt;/code&gt;: Attaches your Library to the specified clusters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcRestartClusters&lt;/code&gt;: Restarts the specified clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Command Execution&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;`dbcExecuteCommand` // Runs a command on a specified DBC Cluster&#xA;// The context/command language that will be employed when dbcExecuteCommand is called&#xA;dbcExecutionLanguage := // One of DBCScala, DBCPython, DBCSQL&#xA;// The file containing the code that is to be processed on the DBC cluster&#xA;dbcCommandFile := // Type File&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example, using just an sbt invocation is below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt&#xA;&amp;gt; set dbcClusters := Seq(&#34;CLUSTER_NAME&#34;)&#xA;&amp;gt; set dbcCommandFile := new File(&#34;/Path/to/file.py&#34;)&#xA;&amp;gt; set dbcExecutionLanguage := DBCPython&#xA;&amp;gt; dbcExecuteCommand&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;p&gt;Other helpful commands are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcListClusters&lt;/code&gt;: View the states of available clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a name=&#34;settings&#34;&gt;Settings&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;There are a few configuration settings that need to be made in the build file. Please set the following parameters according to your setup:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Your username to login to Databricks Cloud&#xA;dbcUsername := // e.g. &#34;admin&#34;&#xA;&#xA;// Your password (Can be set as an environment variable)&#xA;dbcPassword := // e.g. &#34;admin&#34; or System.getenv(&#34;DBCLOUD_PASSWORD&#34;)&#xA;&#xA;// The URL to the Databricks Cloud DB Api.!&#xA;// Note: this plugin currently does not support the /api/2.0 endpoint, so values using that&#xA;// endpoint will be automatically rewritten to use /api/1.2.&#xA;dbcApiUrl := // https://organization.cloud.databricks.com/api/1.2&#xA;&#xA;// Add any clusters that you would like to deploy your work to. e.g. &#34;My Cluster&#34;&#xA;// or run dbcExecuteCommand&#xA;dbcClusters += // Add &#34;ALL_CLUSTERS&#34; if you want to attach your work to all clusters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using dbcDeploy, if you wish to upload an assembly jar instead of every library by itself, you may override dbcClasspath as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath := Seq(assembly.value)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other optional parameters are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// The location to upload your libraries to in the workspace e.g. &#34;/Users/alice&#34;&#xA;dbcLibraryPath := // Default is &#34;/&#34;&#xA;&#xA;// Whether to restart the clusters every time a new version is uploaded to Databricks Cloud&#xA;dbcRestartOnAttach := // Default true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT Tips and Tricks (FAQ)&lt;/h3&gt; &#xA;&lt;p&gt;Here are some SBT tips and tricks to improve your experience with sbt-databricks.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;I have a multi-project build. I don&#39;t want to upload the entire project to Databricks Cloud. What should I do?&lt;/p&gt; &lt;p&gt;In a multi-project build, you may run an SBT task (such as dbcDeploy, dbcUpload, etc...) just for that project by &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Tasks.html#Task+Scope&#34;&gt;&lt;em&gt;scoping&lt;/em&gt;&lt;/a&gt; the task. You may &lt;em&gt;scope&lt;/em&gt; the task by using the project id before that task.&lt;/p&gt; &lt;p&gt;For example, assume we have a project with sub-projects &lt;code&gt;core&lt;/code&gt;, &lt;code&gt;ml&lt;/code&gt;, and &lt;code&gt;sql&lt;/code&gt;. Assume &lt;code&gt;ml&lt;/code&gt; depends on &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;sql&lt;/code&gt;, &lt;code&gt;sql&lt;/code&gt; only depends on &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;core&lt;/code&gt; doesn&#39;t depend on anything. Here is what would happen for the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&amp;gt; dbcUpload          // Uploads core, ml, and sql&#xA;&amp;gt; core/dbcUpload     // Uploads only core&#xA;&amp;gt; sql/dbcUpload      // Uploads core and sql&#xA;&amp;gt; ml/dbcUpload       // Uploads core, ml, and sql&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I want to pass parameters to &lt;code&gt;dbcDeploy&lt;/code&gt;. For example, in my build file &lt;code&gt;dbcClusters&lt;/code&gt; is set as &lt;code&gt;clusterA&lt;/code&gt; but I want to deploy to &lt;code&gt;clusterB&lt;/code&gt; once in a while. What should I do?&lt;/p&gt; &lt;p&gt;In the SBT console, one way of overriding settings for your session is by using the &lt;code&gt;set&lt;/code&gt; command. Using the example above.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&amp;gt; core/dbcDeploy   // Deploys core to clusterA (clusterA was set inside the build file)&#xA;&amp;gt; set dbcClusters := Seq(&#34;clusterB&#34;)  // change cluster to clusterB&#xA;&amp;gt; ml/dbcDeploy     // Deploys core, sql, and ml to clusterB&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I want to upload an assembly jar rather than tens of individual jars. How can I do that?&lt;/p&gt; &lt;p&gt;You may override &lt;code&gt;dbcClasspath&lt;/code&gt; such as:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath := Seq(assembly.value)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... in your build file, (or using set on the console) in order to upload a single fat jar instead of many individual ones. Beware of dependency conflicts!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Hey, I followed #3, but I&#39;m still uploading &lt;code&gt;core&lt;/code&gt;, and &lt;code&gt;sql&lt;/code&gt; individually after&lt;code&gt;sql/dbcUpload&lt;/code&gt;. What&#39;s going on!?&lt;/p&gt; &lt;p&gt;Remember scoping tasks? You will need to scope both &lt;code&gt;dbcClasspath&lt;/code&gt; and &lt;code&gt;assembly&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath in sql := Seq((assembly in sql).value)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then &lt;code&gt;sql/dbcUpload&lt;/code&gt; should upload an assembly jar of &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;sql&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Tests&lt;/h1&gt; &#xA;&lt;p&gt;Run tests using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the very last line starts with &lt;code&gt;[success]&lt;/code&gt;, then that means that the tests have passed.&lt;/p&gt; &#xA;&lt;p&gt;Run scalastyle checks using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dev/lint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you encounter bugs or want to contribute, feel free to submit an issue or pull request.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>olxbr/load-test</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/olxbr/load-test</id>
    <link href="https://github.com/olxbr/load-test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;load-test&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;strong&gt;SearchAPI&lt;/strong&gt; uses &lt;a href=&#34;http://gatling.io&#34;&gt;Gatling&lt;/a&gt; to executes load tests.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/src/gatling/resources/conf/application.conf&#34;&gt;application.conf&lt;/a&gt; file that provides all configuration to load tests.&lt;/p&gt; &#xA;&lt;p&gt;You can override each config above using Java property. For example, if you can override &lt;code&gt;gatling.users&lt;/code&gt; property you must use &lt;code&gt;-Dgatling.users=&amp;lt;value&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more details about how to do override, see &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/#how-to-run&#34;&gt;How To Run&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;How To Build&lt;/h2&gt; &#xA;&lt;p&gt;To build this project, you need to execute these commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;vivareal/load-test:load-test&lt;/code&gt; docker image will be created.&lt;/p&gt; &#xA;&lt;h2&gt;How To Run&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://gatling.io&#34;&gt;Gatling&lt;/a&gt; works with a &lt;a href=&#34;http://gatling.io/docs/current/quickstart/#a-word-on-scala&#34;&gt;Simulation&lt;/a&gt; concept and for &lt;strong&gt;SearchAPI&lt;/strong&gt; we creates the &lt;a href=&#34;https://raw.githubusercontent.com/olxbr/load-test/master/src/gatling/scala/com/vivareal/search/simulation/SearchAPIv2Simulation.scala&#34;&gt;SearchAPIv2Simulation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There some steps when you run the load tests:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If not already built, builds and runs &lt;code&gt;load-test&lt;/code&gt; docker image.&lt;/li&gt; &#xA; &lt;li&gt;Executes your simulations.&lt;/li&gt; &#xA; &lt;li&gt;Uploads you simulation report on Amazon S3.&lt;/li&gt; &#xA; &lt;li&gt;Notifies report status on Slack with link to access them.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There are two parameters to use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LT_ENDPOINT&lt;/code&gt;*: load tests target endpoint.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;: gatling extra args&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The environment variables with &lt;code&gt;*&lt;/code&gt; are required. You can override each config using &lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;p&gt;To run local, you simple use &lt;code&gt;make run-local&lt;/code&gt; with the target ip to load test, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run-local LT_ENDPOINT=&#34;&amp;lt;TARGET_IP&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Kubernetes&lt;/h3&gt; &#xA;&lt;p&gt;To run using &lt;a href=&#34;https://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt;, you simple use &lt;code&gt;make run&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make run LT_ENDPOINT=&#34;&amp;lt;TARGET_IP&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Yon can use &lt;code&gt;K8S_RUN_ARGS&lt;/code&gt; to configure &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-overview&#34;&gt;kubectl run&lt;/a&gt; params.&lt;/p&gt; &#xA;&lt;h3&gt;Running with Gradle&lt;/h3&gt; &#xA;&lt;p&gt;To run using Gradle too and you simple use &lt;code&gt;gatlingRun&lt;/code&gt; task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlinRun&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The upload and notification process is separated and to do this you must use &lt;code&gt;uploadReport&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlinRun uploadReport&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to Test&lt;/h3&gt; &#xA;&lt;p&gt;No tests currently implemented&lt;/p&gt; &#xA;&lt;h2&gt;How To Deploy&lt;/h2&gt; &#xA;&lt;p&gt;load-test is a tool/lib project and the deploy is subjective according to the project that uses.&lt;/p&gt; &#xA;&lt;p&gt;You may use docker to deploy: &lt;code&gt;make push&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;To push successful docker image, make sure you setup docker credentials.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Creating your own Simulation&lt;/h2&gt; &#xA;&lt;p&gt;Just implement the simulation in package &lt;code&gt;com.vivareal.search.simulation&lt;/code&gt; and sent it in &lt;code&gt;gatling.simulations.include&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./gradlew gatlingRun -Dscenario.users=1 -Dgatling.rampUp=30 -Dgatling.maxDuration=60 -Dgatling.simulations.include=**/SimpleRequestSimulation.scala&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can send it in &lt;code&gt;LT_EXTRA_ARGS&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make run LT_EXTRA_ARGS=&#34;-Dgatling.simulations.include=**/SimpleRequestSimulation.scala&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Simple Request Simularion&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s a &lt;code&gt;Simulation&lt;/code&gt; that downloads a &lt;code&gt;csv&lt;/code&gt; file from &lt;code&gt;Graylog&lt;/code&gt; API based on a query and executes the resulting &lt;code&gt;URI&lt;/code&gt;s requests in &lt;code&gt;circle&lt;/code&gt; and &lt;code&gt;during&lt;/code&gt; a configured time.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Param&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.query&lt;/td&gt; &#xA;   &lt;td&gt;Graylog query to fetch URIs&lt;/td&gt; &#xA;   &lt;td&gt;application:cloudflare&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.urisFile&lt;/td&gt; &#xA;   &lt;td&gt;file name to save the Graylog API result&lt;/td&gt; &#xA;   &lt;td&gt;uris.csv&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.uriField&lt;/td&gt; &#xA;   &lt;td&gt;the field name of the csv file generated by Graylog API&lt;/td&gt; &#xA;   &lt;td&gt;ClientRequestURI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.authorization&lt;/td&gt; &#xA;   &lt;td&gt;Graylog API encoded basic auth header&lt;/td&gt; &#xA;   &lt;td&gt;Basic LALALALA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.range&lt;/td&gt; &#xA;   &lt;td&gt;Time range in seconds for Graylog query&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;graylog.limit&lt;/td&gt; &#xA;   &lt;td&gt;Limit of the results for Graylog query&lt;/td&gt; &#xA;   &lt;td&gt;5000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All above params can be overriden using System Properties.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/devbox</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/databricks/devbox</id>
    <link href="https://github.com/databricks/devbox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Databricks main line of development is now in the monorepo. Please see &lt;code&gt;devtools/devbox&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Devbox syncer&lt;/h1&gt; &#xA;&lt;p&gt;A one-way sync from laptop to an EC2 instance.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;To prepare an assembly jar, ready to be tested and deployed in the universe/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill launcher.assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result can be found in &lt;code&gt;out/launcher/assembly/dest/out.jar&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To run all tests (takes a long time):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill devbox.test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive console (REPL)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./mill -i devbox.repl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;There is a &lt;a href=&#34;https://github.com/databricks/devbox/actions?query=workflow%3ARelease&#34;&gt;Github Action&lt;/a&gt; to release Devbox.&lt;/p&gt; &#xA;&lt;p&gt;Just run the workflow on the target branch (usually master) with the new version number and check the &lt;a href=&#34;https://github.com/databricks/devbox/releases&#34;&gt;releases&lt;/a&gt; page&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-integration-tests</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/databricks/spark-integration-tests</id>
    <link href="https://github.com/databricks/spark-integration-tests" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Integration tests for Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Integration Tests&lt;/h1&gt; &#xA;&lt;p&gt;This project contains &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;-based integration tests for Spark, including fault-tolerance tests for Spark&#39;s standalone cluster manager.&lt;/p&gt; &#xA;&lt;h2&gt;Installation / Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install Docker&lt;/h3&gt; &#xA;&lt;p&gt;This project depends on Docker &amp;gt;= 1.3.0 (it may work with earlier versions, but this hasn&#39;t been tested).&lt;/p&gt; &#xA;&lt;h4&gt;On Linux&lt;/h4&gt; &#xA;&lt;p&gt;Install Docker. This test suite requires that Docker can run without &lt;code&gt;sudo&lt;/code&gt; (see &lt;a href=&#34;http://docs.docker.io/en/latest/use/basics/&#34;&gt;http://docs.docker.io/en/latest/use/basics/&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;On OSX&lt;/h4&gt; &#xA;&lt;p&gt;On OSX, these integration tests can be run using &lt;a href=&#34;https://github.com/boot2docker/boot2docker&#34;&gt;boot2docker&lt;/a&gt;. First, &lt;a href=&#34;https://github.com/boot2docker/osx-installer/releases/tag/v1.3.2&#34;&gt;download &lt;code&gt;boot2docker&lt;/code&gt;&lt;/a&gt;, run the installer, then run &lt;code&gt;~/Applications/boot2docker&lt;/code&gt; to perform some one-time setup (create the VM, etc.). This project has been tested with &lt;code&gt;boot2docker&lt;/code&gt; 1.3.0+.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;boot2docker&lt;/code&gt;, the Docker containers will be run inside of a VirtualBox VM, which creates some difficulties for communication between the Mac host and the containers. Follow these instructions to work around those issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Network access&lt;/strong&gt;: Our tests currently run the SparkContext from outside of the containers, so we need both host &amp;lt;-&amp;gt; container and container &amp;lt;-&amp;gt; container networking to work properly. This is complicated by the fact that &lt;code&gt;boot2docker&lt;/code&gt; runs the containers behind a NAT in VirtualBox.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/boot2docker/boot2docker/issues/528&#34;&gt;One workaround&lt;/a&gt; is to add a routing table entry that routes traffic to containers to the VirtualBox VM&#39;s IP address:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo route -n add 172.17.0.0/16 `boot2docker ip`    &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You&#39;ll have to re-run this command if you restart your computer or assign a new IP to the VirtualBox VM.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install Docker images&lt;/h3&gt; &#xA;&lt;p&gt;The integration tests depend on several Docker images. To set them up, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./docker/build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to build our custom Docker images and download other images from the Docker repositories. This needs to download a fair amount of stuff, so make sure that you&#39;re on a fast internet connection (or be prepared to wait a while).&lt;/p&gt; &#xA;&lt;h3&gt;Configure your environment&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;: Running &lt;code&gt;./init.sh&lt;/code&gt; will perform environment sanity checking and tell you which shell exports to perform.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;code&gt;SPARK_HOME&lt;/code&gt; environment variable should to a Spark source checkout where an assembly has been built. This directory will be shared with Docker containers; Spark workers and masters will use this &lt;code&gt;SPARK_HOME/work&lt;/code&gt; as their work directory. This effectively treats host machine&#39;s &lt;code&gt;SPARK_HOME&lt;/code&gt; directory as a directory on a network-mounted filesystem.&lt;/p&gt; &lt;p&gt;Additionally, this Spark sbt project will added as a dependency of this sbt project, so the integration test code will be compiled against that Spark version.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test-specific requirements&lt;/h3&gt; &#xA;&lt;h4&gt;Mesos&lt;/h4&gt; &#xA;&lt;p&gt;The Mesos integration tests require &lt;code&gt;MESOS_NATIVE_LIBRARY&lt;/code&gt; to be set. For Mac users, the easiest way to install Mesos is through Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install mesos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export MESOS_NATIVE_LIBRARY=$(brew --repository)/lib/libmesos.dylib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark on Mesos requires a Spark binary distribution &lt;code&gt;.tgz&lt;/code&gt; file. To build this, run &lt;code&gt;./make-distribution.sh --tgz&lt;/code&gt; in your Spark checkout.&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;These integration tests are implemented as ScalaTest suites and can be run through sbt. Note that you will probably need to give sbt extra memory; with newer versions of the sbt launcher script, this can be done with the &lt;code&gt;-mem&lt;/code&gt; option, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt -mem 2048 test:package &#34;test-only org.apache.spark.integrationtests.MesosSuite&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Although our Docker-based test suites attempt to clean up the containers that they create, this cleanup may not be performed if the test runner&#39;s JVM exits abruptly. To kill &lt;strong&gt;all&lt;/strong&gt; Docker containers (including ones that may not have been launched by our tests), you can run &lt;code&gt;docker kill $(docker ps -q)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See LICENSE for full license text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chipsalliance/chisel3</title>
    <updated>2022-05-29T02:50:42Z</updated>
    <id>tag:github.com,2022-05-29:/chipsalliance/chisel3</id>
    <link href="https://github.com/chipsalliance/chisel3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chisel 3: A Modern Hardware Design Language&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/chisel_logo.svg?sanitize=true&#34; alt=&#34;Chisel 3&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Upcoming Events&lt;/h2&gt; &#xA;&lt;h3&gt;Chisel Dev Meeting&lt;/h3&gt; &#xA;&lt;p&gt;Chisel/FIRRTL development meetings happen every Monday and Tuesday from 1100--1200 PT.&lt;/p&gt; &#xA;&lt;p&gt;Call-in info and meeting notes are available &lt;a href=&#34;https://docs.google.com/document/d/1BLP2DYt59DqI-FgFCcjw8Ddl4K-WU0nHmQu0sZ_wAGo/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chisel Community Conference 2021, Shanghai, China.&lt;/h3&gt; &#xA;&lt;p&gt;CCC is an annual gathering of Chisel community enthusiasts and technical exchange workshop. This year with the support of the Chisel development community and RISC-V World Conference China 2021 Committee, we have brought together designers and developers with hands-on experience in Chisel from home and abroad to share cutting-edge results and experiences from both the open source community as well as industry.&lt;br&gt; English translated recordings version will be updated soon.&lt;br&gt; Looking forward to CCC 2022! See you then!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/chisel3?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/chipsalliance/chisel3.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/freechipsproject/chisel3&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/chipsalliance/chisel3/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt; &lt;a href=&#34;https://github.com/chipsalliance/chisel3/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/tag/chipsalliance/chisel3.svg?include_prereleases&amp;amp;sort=semver&#34; alt=&#34;GitHub tag (latest SemVer)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.chisel-lang.org&#34;&gt;&lt;strong&gt;Chisel&lt;/strong&gt;&lt;/a&gt; is a hardware design language that facilitates &lt;strong&gt;advanced circuit generation and design reuse for both ASIC and FPGA digital logic designs&lt;/strong&gt;. Chisel adds hardware construction primitives to the &lt;a href=&#34;https://www.scala-lang.org&#34;&gt;Scala&lt;/a&gt; programming language, providing designers with the power of a modern programming language to write complex, parameterizable circuit generators that produce synthesizable Verilog. This generator methodology enables the creation of re-usable components and libraries, such as the FIFO queue and arbiters in the &lt;a href=&#34;https://www.chisel-lang.org/api/latest/#chisel3.util.package&#34;&gt;Chisel Standard Library&lt;/a&gt;, raising the level of abstraction in design while retaining fine-grained control.&lt;/p&gt; &#xA;&lt;p&gt;For more information on the benefits of Chisel see: &lt;a href=&#34;https://stackoverflow.com/questions/53007782/what-benefits-does-chisel-offer-over-classic-hardware-description-languages&#34;&gt;&#34;What benefits does Chisel offer over classic Hardware Description Languages?&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chisel is powered by &lt;a href=&#34;https://github.com/chipsalliance/firrtl&#34;&gt;FIRRTL (Flexible Intermediate Representation for RTL)&lt;/a&gt;, a hardware compiler framework that performs optimizations of Chisel-generated circuits and supports custom user-defined circuit transformations.&lt;/p&gt; &#xA;&lt;h2&gt;What does Chisel code look like?&lt;/h2&gt; &#xA;&lt;p&gt;Consider an FIR filter that implements a convolution operation, as depicted in this block diagram:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/fir_filter.svg?sanitize=true&#34; width=&#34;512&#34;&gt; &#xA;&lt;p&gt;While Chisel provides similar base primitives as synthesizable Verilog, and &lt;em&gt;could&lt;/em&gt; be used as such:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// 3-point moving sum implemented in the style of a FIR filter&#xA;class MovingSum3(bitWidth: Int) extends Module {&#xA;  val io = IO(new Bundle {&#xA;    val in = Input(UInt(bitWidth.W))&#xA;    val out = Output(UInt(bitWidth.W))&#xA;  })&#xA;&#xA;  val z1 = RegNext(io.in)&#xA;  val z2 = RegNext(z1)&#xA;&#xA;  io.out := (io.in * 1.U) + (z1 * 1.U) + (z2 * 1.U)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the power of Chisel comes from the ability to create generators, such as an FIR filter that is defined by the list of coefficients:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Generalized FIR filter parameterized by the convolution coefficients&#xA;class FirFilter(bitWidth: Int, coeffs: Seq[UInt]) extends Module {&#xA;  val io = IO(new Bundle {&#xA;    val in = Input(UInt(bitWidth.W))&#xA;    val out = Output(UInt(bitWidth.W))&#xA;  })&#xA;  // Create the serial-in, parallel-out shift register&#xA;  val zs = Reg(Vec(coeffs.length, UInt(bitWidth.W)))&#xA;  zs(0) := io.in&#xA;  for (i &amp;lt;- 1 until coeffs.length) {&#xA;    zs(i) := zs(i-1)&#xA;  }&#xA;&#xA;  // Do the multiplies&#xA;  val products = VecInit.tabulate(coeffs.length)(i =&amp;gt; zs(i) * coeffs(i))&#xA;&#xA;  // Sum up the products&#xA;  io.out := products.reduce(_ + _)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and use and re-use them across designs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val movingSum3Filter = Module(new FirFilter(8, Seq(1.U, 1.U, 1.U)))  // same 3-point moving sum filter as before&#xA;val delayFilter = Module(new FirFilter(8, Seq(0.U, 1.U)))  // 1-cycle delay as a FIR filter&#xA;val triangleFilter = Module(new FirFilter(8, Seq(1.U, 2.U, 3.U, 2.U, 1.U)))  // 5-point FIR filter with a triangle impulse response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above can be converted to Verilog using &lt;code&gt;ChiselStage&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import chisel3.stage.{ChiselStage, ChiselGeneratorAnnotation}&#xA;&#xA;(new chisel3.stage.ChiselStage).execute(&#xA;  Array(&#34;-X&#34;, &#34;verilog&#34;),&#xA;  Seq(ChiselGeneratorAnnotation(() =&amp;gt; new FirFilter(8, Seq(1.U, 1.U, 1.U)))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you may generate some Verilog directly for inspection:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val verilogString = chisel3.emitVerilog(new FirFilter(8, Seq(0.U, 1.U)))&#xA;println(verilogString)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Bootcamp Interactive Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://mybinder.org/v2/gh/freechipsproject/chisel-bootcamp/master&#34;&gt;&lt;strong&gt;online Chisel Bootcamp&lt;/strong&gt;&lt;/a&gt; is the recommended way to get started with and learn Chisel. &lt;strong&gt;No setup is required&lt;/strong&gt; (it runs in the browser), nor does it assume any prior knowledge of Scala.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/ucb-bar/chisel-tutorial&#34;&gt;&lt;strong&gt;classic Chisel tutorial&lt;/strong&gt;&lt;/a&gt; contains small exercises and runs on your computer.&lt;/p&gt; &#xA;&lt;h3&gt;A Textbook on Chisel&lt;/h3&gt; &#xA;&lt;p&gt;If you like a textbook to learn Chisel and also a bit of digital design in general, you may be interested in reading &lt;a href=&#34;http://www.imm.dtu.dk/~masca/chisel-book.html&#34;&gt;&lt;strong&gt;Digital Design with Chisel&lt;/strong&gt;&lt;/a&gt;. It is available in English, Chinese, Japanese, and Vietnamese.&lt;/p&gt; &#xA;&lt;h3&gt;Build Your Own Chisel Projects&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/SETUP.md&#34;&gt;the setup instructions&lt;/a&gt; for how to set up your environment to build Chisel locally.&lt;/p&gt; &#xA;&lt;p&gt;When you&#39;re ready to build your own circuits in Chisel, &lt;strong&gt;we recommend starting from the &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;Chisel Template&lt;/a&gt; repository&lt;/strong&gt;, which provides a pre-configured project, example design, and testbench. Follow the &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;chisel-template README&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;p&gt;If you insist on setting up your own project from scratch, your project needs to depend on both the chisel3-plugin (Scalac plugin) and the chisel3 library. For example, in SBT this could be expressed as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// build.sbt&#xA;scalaVersion := &#34;2.13.7&#34;&#xA;addCompilerPlugin(&#34;edu.berkeley.cs&#34; % &#34;chisel3-plugin&#34; % &#34;3.5.0&#34; cross CrossVersion.full)&#xA;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chisel3&#34; % &#34;3.5.0&#34;&#xA;// We also recommend using chiseltest for writing unit tests &#xA;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chiseltest&#34; % &#34;0.5.0&#34; % &#34;test&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Guide For New Contributors&lt;/h3&gt; &#xA;&lt;p&gt;If you are trying to make a contribution to this project, please read &lt;a href=&#34;https://github.com/Burnleydev1/chisel3/raw/recent_PR/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Design Verification&lt;/h3&gt; &#xA;&lt;p&gt;These simulation-based verification tools are available for Chisel:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/chisel-testers&#34;&gt;&lt;strong&gt;iotesters&lt;/strong&gt;&lt;/a&gt;, specifically &lt;a href=&#34;https://github.com/freechipsproject/chisel-testers/wiki/Using%20the%20PeekPokeTester&#34;&gt;PeekPokeTester&lt;/a&gt;, provides constructs (&lt;code&gt;peek&lt;/code&gt;, &lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;expect&lt;/code&gt;) similar to a non-synthesizable Verilog testbench.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-bar/chisel-testers2&#34;&gt;&lt;strong&gt;testers2&lt;/strong&gt;&lt;/a&gt; is an in-development replacement for PeekPokeTester, providing the same base constructs but with a streamlined interface and concurrency support with &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;join&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Useful Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/freechipsproject/chisel-cheatsheet/releases/latest/download/chisel_cheatsheet.pdf&#34;&gt;&lt;strong&gt;Cheat Sheet&lt;/strong&gt;&lt;/a&gt;, a 2-page reference of the base Chisel syntax and libraries&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chisel-lang.org/api/latest/chisel3/index.html&#34;&gt;&lt;strong&gt;ScalaDoc&lt;/strong&gt;&lt;/a&gt;, a listing, description, and examples of the functionality exposed by Chisel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitter.im/freechipsproject/chisel3&#34;&gt;&lt;strong&gt;Gitter&lt;/strong&gt;&lt;/a&gt;, where you can ask questions or discuss anything Chisel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chisel-lang.org&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; (&lt;a href=&#34;https://github.com/freechipsproject/www.chisel-lang.org/&#34;&gt;source&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scastie.scala-lang.org/9ga9i2DvQymKlA5JjS1ieA&#34;&gt;&lt;strong&gt;Scastie (3.5.0)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.asic-world.com/verilog/veritut.html&#34;&gt;&lt;strong&gt;asic-world&lt;/strong&gt;&lt;/a&gt; If you aren&#39;t familiar with verilog, this is a good tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you are migrating from Chisel2, see &lt;a href=&#34;https://www.chisel-lang.org/chisel3/chisel3-vs-chisel2.html&#34;&gt;the migration guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data Types Overview&lt;/h3&gt; &#xA;&lt;p&gt;These are the base data types for defining circuit components:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/docs/src/images/type_hierarchy.svg?sanitize=true&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributor Documentation&lt;/h2&gt; &#xA;&lt;p&gt;This section describes how to get started contributing to Chisel itself, including how to test your version locally against other projects that pull in Chisel using &lt;a href=&#34;https://www.scala-sbt.org/1.x/docs/Library-Dependencies.html&#34;&gt;sbt&#39;s managed dependencies&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Useful Resources for Contributors&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/#useful-resources&#34;&gt;Useful Resources&lt;/a&gt; for users are also helpful for contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/114YihixFBPCfUnv1inqAL8UjsiWfcNWdPHX7SeqlRQc&#34;&gt;&lt;strong&gt;Chisel Breakdown Slides&lt;/strong&gt;&lt;/a&gt;, an introductory talk about Chisel&#39;s internals&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compiling and Testing Chisel&lt;/h3&gt; &#xA;&lt;p&gt;You must first install required dependencies to build Chisel locally, please see &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/SETUP.md&#34;&gt;the setup instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Clone and build the Chisel library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/chipsalliance/chisel3.git&#xA;cd chisel3&#xA;sbt compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to run the following unit tests, you will need several tools on your &lt;code&gt;PATH&lt;/code&gt;, namely &lt;a href=&#34;https://www.veripool.org/verilator/&#34;&gt;verilator&lt;/a&gt;, &lt;a href=&#34;http://www.clifford.at/yosys/&#34;&gt;yosys&lt;/a&gt;, &lt;a href=&#34;https://github.com/chipsalliance/espresso&#34;&gt;espresso&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Z3Prover/z3&#34;&gt;z3&lt;/a&gt;. Check that each is installed on your &lt;code&gt;PATH&lt;/code&gt; by running &lt;code&gt;which verilator&lt;/code&gt; and so on.&lt;/p&gt; &#xA;&lt;p&gt;If the compilation succeeded and the dependencies noted above are installed, you can then run the included unit tests by invoking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Projects Against Local Chisel&lt;/h3&gt; &#xA;&lt;p&gt;To use the development version of Chisel (&lt;code&gt;master&lt;/code&gt; branch), you will need to build from source and &lt;code&gt;publishLocal&lt;/code&gt;. The repository version can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/build.sbt&#34;&gt;build.sbt&lt;/a&gt; file. As of the time of writing it was:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;version := &#34;3.6-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To publish your version of Chisel to the local Ivy (sbt&#39;s dependency manager) repository, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt publishLocal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The compiled version gets placed in &lt;code&gt;~/.ivy2/local/edu.berkeley.cs/&lt;/code&gt;. If you need to un-publish your local copy of Chisel, remove the directory generated in &lt;code&gt;~/.ivy2/local/edu.berkeley.cs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In order to have your projects use this version of Chisel, you should update the &lt;code&gt;libraryDependencies&lt;/code&gt; setting in your project&#39;s build.sbt file to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;edu.berkeley.cs&#34; %% &#34;chisel3&#34; % &#34;3.6-SNAPSHOT&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building Chisel with FIRRTL in the same SBT Project&lt;/h3&gt; &#xA;&lt;p&gt;While we recommend using the library dependency approach as described above, it is possible to build Chisel and FIRRTL in a single SBT project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This only works for the &#34;main&#34; configuration; you cannot build the Chisel tests this way because &lt;code&gt;treadle&lt;/code&gt; is only supported as a library dependency.&lt;/li&gt; &#xA; &lt;li&gt;Do not &lt;code&gt;publishLocal&lt;/code&gt; when building this way. The published artifact will be missing the FIRRTL dependency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This works by using &lt;a href=&#34;http://eed3si9n.com/hot-source-dependencies-using-sbt-sriracha&#34;&gt;sbt-sriracha&lt;/a&gt;, an SBT plugin for toggling between source and library dependencies. It provides two JVM system properties that, when set, will tell SBT to include FIRRTL as a source project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt.sourcemode&lt;/code&gt; - when set to true, SBT will look for FIRRTL in the workspace&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sbt.workspace&lt;/code&gt; - sets the root directory of the workspace&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# From root of this repo&#xA;git clone git@github.com:chipsalliance/firrtl.git&#xA;sbt -Dsbt.sourcemode=true -Dsbt.workspace=$PWD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is primarily useful for building projects that themselves want to include Chisel as a source dependency. As an example, see &lt;a href=&#34;https://github.com/chipsalliance/rocket-chip&#34;&gt;Rocket Chip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chisel3 Architecture Overview&lt;/h3&gt; &#xA;&lt;p&gt;The Chisel3 compiler consists of these main parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The frontend&lt;/strong&gt;, &lt;code&gt;chisel3.*&lt;/code&gt;, which is the publicly visible &#34;API&#34; of Chisel and what is used in Chisel RTL. These just add data to the...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Builder&lt;/strong&gt;, &lt;code&gt;chisel3.internal.Builder&lt;/code&gt;, which maintains global state (like the currently open Module) and contains commands, generating...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The intermediate data structures&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.*&lt;/code&gt;, which are syntactically very similar to Firrtl. Once the entire circuit has been elaborated, the top-level object (a &lt;code&gt;Circuit&lt;/code&gt;) is then passed to...&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Firrtl emitter&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.Emitter&lt;/code&gt;, which turns the intermediate data structures into a string that can be written out into a Firrtl file for further processing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also included is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The standard library&lt;/strong&gt; of circuit generators, &lt;code&gt;chisel3.util.*&lt;/code&gt;. These contain commonly used interfaces and constructors (like &lt;code&gt;Decoupled&lt;/code&gt;, which wraps a signal with a ready-valid pair) as well as fully parameterizable circuit generators (like arbiters and multiplexors).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chisel Stage&lt;/strong&gt;, &lt;code&gt;chisel3.stage.*&lt;/code&gt;, which contains compilation and test functions that are invoked in the standard Verilog generation and simulation testing infrastructure. These can also be used as part of custom flows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Chisel Sub-Projects&lt;/h3&gt; &#xA;&lt;p&gt;Chisel consists of 4 Scala projects; each is its own separate compilation unit:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/core&#34;&gt;&lt;code&gt;core&lt;/code&gt;&lt;/a&gt; is the bulk of the source code of Chisel, depends on &lt;code&gt;macros&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/src/main&#34;&gt;&lt;code&gt;src/main&lt;/code&gt;&lt;/a&gt; is the &#34;main&#34; that brings it all together and includes a &lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/src/main/scala/chisel3/util&#34;&gt;&lt;code&gt;util&lt;/code&gt;&lt;/a&gt; library, which depends on &lt;code&gt;core&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/plugin&#34;&gt;&lt;code&gt;plugin&lt;/code&gt;&lt;/a&gt; is the compiler plugin, no internal dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chipsalliance/chisel3/master/macros&#34;&gt;&lt;code&gt;macros&lt;/code&gt;&lt;/a&gt; is most of the macros used in Chisel, no internal dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code that touches lots of APIs that are private to the &lt;code&gt;chisel3&lt;/code&gt; package should belong in &lt;code&gt;core&lt;/code&gt;, while code that is pure Chisel should belong in &lt;code&gt;src/main&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Which version should I use?&lt;/h3&gt; &#xA;&lt;p&gt;We encourage Chisel users (as opposed to Chisel developers), to use the latest release version of Chisel. This &lt;a href=&#34;https://github.com/freechipsproject/chisel-template&#34;&gt;chisel-template&lt;/a&gt; repository is kept up-to-date, depending on the most recent version of Chisel. The recommended version is also captured near the top of this README, and in the &lt;a href=&#34;https://github.com/chipsalliance/chisel3/releases&#34;&gt;Github releases&lt;/a&gt; section of this repo. If you encounter an issue with a released version of Chisel, please file an issue on GitHub mentioning the Chisel version and provide a simple test case (if possible). Try to reproduce the issue with the associated latest minor release (to verify that the issue hasn&#39;t been addressed).&lt;/p&gt; &#xA;&lt;p&gt;For more information on our versioning policy and what versions of the various Chisel ecosystem projects work together, see &lt;a href=&#34;https://www.chisel-lang.org/chisel3/docs/appendix/versioning.html&#34;&gt;Chisel Project Versioning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re developing a Chisel library (or &lt;code&gt;chisel3&lt;/code&gt; itself), you&#39;ll probably want to work closer to the tip of the development trunk. By default, the master branches of the chisel repositories are configured to build and publish their version of the code as &lt;code&gt;Z.Y-SNAPSHOT&lt;/code&gt;. Updated SNAPSHOTs are publised on every push to master. You are encouraged to do your development against the latest SNAPSHOT, but note that neither API nor ABI compatibility is guaranteed so your code may break at any time.&lt;/p&gt;</summary>
  </entry>
</feed>