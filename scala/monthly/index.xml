<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-01T01:49:21Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>calinburloiu/Scala-for-the-Impatient--Exercises</title>
    <updated>2024-05-01T01:49:21Z</updated>
    <id>tag:github.com,2024-05-01:/calinburloiu/Scala-for-the-Impatient--Exercises</id>
    <link href="https://github.com/calinburloiu/Scala-for-the-Impatient--Exercises" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solutions to exercises from the book &#34;Scala for the Impacient&#34; by Cay S. Horstmann.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Solutions to exercises from &#34;Scala for the Impatient&#34; by Cay S. Horstmann&lt;/h1&gt; &#xA;&lt;p&gt;I (Călin-Andrei Burloiu) created this project to include my solutions to exercises from the book &#34;Scala for the Impatient&#34; by Cay S. Horstmann. Feel free to fork this project in order to add unsolved exercises, better solutions or alternatives. You may issue a pull request so I can update this project if I find your solutions appropriate.&lt;/p&gt; &#xA;&lt;p&gt;DISCLAIMER: I (Călin-Andrei Burloiu) do not guarantee that the solutions are correct or are the most efficient. The project does not include solutions for all exercises, but it does include most of them.&lt;/p&gt; &#xA;&lt;h1&gt;How do I find a particular exercise?&lt;/h1&gt; &#xA;&lt;p&gt;If you have a UNIX-like environment (Linux, Mac OS X, cygwin, UNIX etc.) and you can run bash scripts, you can use &lt;code&gt;find-exercise.sh&lt;/code&gt; script to locate an exercise. Run it without arguments to prompt usage. To find an exercise pass the chapter number as the first argument and the exercise number as the second one. For example, to locate Exercise 6 of Chapter 17 run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./find-exercise.sh 17 6&#xA;Exercise 06 from Chapter 17 can be found in:&#xA;  file &#39;src/exercises/c17_type_parameters/package.scala&#39;,&#xA;  line 4.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Exercises are grouped by chapters in packages following this package name format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;package exercises.c&amp;lt;no&amp;gt;_&amp;lt;description&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;no&amp;gt;&lt;/code&gt; is the chapter number and &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; is a short form for the chapter name. Each package has its directory and all the code is placed in &lt;code&gt;src&lt;/code&gt;. For example package &lt;code&gt;exercises.c17_type_parameters&lt;/code&gt;, located in &lt;code&gt;src/exercises/c17_type_parameters/&lt;/code&gt;, corresponds to Chapter 17, &#34;Type Parameters&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The solution to any exercise should be in a Scala file after a comment with this format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Exer&amp;lt;no&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;&amp;lt;no&amp;gt;&lt;/code&gt; is the number of the exercise with two digits format (eg. &lt;code&gt;// Exer02&lt;/code&gt;, &lt;code&gt;// Exer10&lt;/code&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-sql-perf</title>
    <updated>2024-05-01T01:49:21Z</updated>
    <id>tag:github.com,2024-05-01:/databricks/spark-sql-perf</id>
    <link href="https://github.com/databricks/spark-sql-perf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark SQL Performance Tests&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/databricks/spark-sql-perf&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/spark-sql-perf.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a performance testing framework for &lt;a href=&#34;https://spark.apache.org/sql/&#34;&gt;Spark SQL&lt;/a&gt; in &lt;a href=&#34;https://spark.apache.org/&#34;&gt;Apache Spark&lt;/a&gt; 2.2+.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This README is still under development. Please also check our source code for more information.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Running from command line.&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bin/run --help&#xA;&#xA;spark-sql-perf 0.2.0&#xA;Usage: spark-sql-perf [options]&#xA;&#xA;  -b &amp;lt;value&amp;gt; | --benchmark &amp;lt;value&amp;gt;&#xA;        the name of the benchmark to run&#xA;  -m &amp;lt;value&amp;gt; | --master &amp;lt;value&#xA;        the master url to use&#xA;  -f &amp;lt;value&amp;gt; | --filter &amp;lt;value&amp;gt;&#xA;        a filter on the name of the queries to run&#xA;  -i &amp;lt;value&amp;gt; | --iterations &amp;lt;value&amp;gt;&#xA;        the number of iterations to run&#xA;  --help&#xA;        prints this usage text&#xA;        &#xA;$ bin/run --benchmark DatasetPerformance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first run of &lt;code&gt;bin/run&lt;/code&gt; will build the library.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;sbt package&lt;/code&gt; or &lt;code&gt;sbt assembly&lt;/code&gt; to build the library jar.&lt;br&gt; Use &lt;code&gt;sbt +package&lt;/code&gt; to build for scala 2.11 and 2.12.&lt;/p&gt; &#xA;&lt;h2&gt;Local performance tests&lt;/h2&gt; &#xA;&lt;p&gt;The framework contains twelve benchmarks that can be executed in local mode. They are organized into three classes and target different components and functions of Spark:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/DatasetPerformance.scala&#34;&gt;DatasetPerformance&lt;/a&gt; compares the performance of the old RDD API with the new Dataframe and Dataset APIs. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark DatasetPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/JoinPerformance.scala&#34;&gt;JoinPerformance&lt;/a&gt; compares the performance of joining different table sizes and shapes with different join types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark JoinPerformance&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks/spark-sql-perf/raw/master/src/main/scala/com/databricks/spark/sql/perf/AggregationPerformance.scala&#34;&gt;AggregationPerformance&lt;/a&gt; compares the performance of aggregating different table sizes using different aggregation types. These benchmarks can be launched with the command &lt;code&gt;bin/run --benchmark AggregationPerformance&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;MLlib tests&lt;/h1&gt; &#xA;&lt;p&gt;To run MLlib tests, run &lt;code&gt;/bin/run-ml yamlfile&lt;/code&gt;, where &lt;code&gt;yamlfile&lt;/code&gt; is the path to a YAML configuration file describing tests to run and their parameters.&lt;/p&gt; &#xA;&lt;h1&gt;TPC-DS&lt;/h1&gt; &#xA;&lt;h2&gt;Setup a benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Before running any query, a dataset needs to be setup by creating a &lt;code&gt;Benchmark&lt;/code&gt; object. Generating the TPCDS data requires dsdgen built and available on the machines. We have a fork of dsdgen that you will need. The fork includes changes to generate TPCDS data to stdout, so that this library can pipe them directly to Spark, without intermediate files. Therefore, this library will not work with the vanilla TPCDS kit.&lt;/p&gt; &#xA;&lt;p&gt;TPCDS kit needs to be installed on all cluster executor nodes under the same path!&lt;/p&gt; &#xA;&lt;p&gt;It can be found &lt;a href=&#34;https://github.com/databricks/tpcds-kit&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Generate the data&#xA;build/sbt &#34;test:runMain com.databricks.spark.sql.perf.tpcds.GenTPCDSData -d &amp;lt;dsdgenDir&amp;gt; -s &amp;lt;scaleFactor&amp;gt; -l &amp;lt;location&amp;gt; -f &amp;lt;format&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Create the specified database&#xA;sql(s&#34;create database $databaseName&#34;)&#xA;// Create metastore tables in a specified database for your data.&#xA;// Once tables are created, the current database will be switched to the specified database.&#xA;tables.createExternalTables(rootDir, &#34;parquet&#34;, databaseName, overwrite = true, discoverPartitions = true)&#xA;// Or, if you want to create temporary tables&#xA;// tables.createTemporaryTables(location, format)&#xA;&#xA;// For CBO only, gather statistics on all columns:&#xA;tables.analyzeTables(databaseName, analyzeColumns = true) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run benchmarking queries&lt;/h2&gt; &#xA;&lt;p&gt;After setup, users can use &lt;code&gt;runExperiment&lt;/code&gt; function to run benchmarking queries and record query execution time. Taking TPC-DS as an example, you can start an experiment by using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import com.databricks.spark.sql.perf.tpcds.TPCDS&#xA;&#xA;val tpcds = new TPCDS (sqlContext = sqlContext)&#xA;// Set:&#xA;val databaseName = ... // name of database with TPCDS data.&#xA;val resultLocation = ... // place to write results&#xA;val iterations = 1 // how many iterations of queries to run.&#xA;val queries = tpcds.tpcds2_4Queries // queries to run.&#xA;val timeout = 24*60*60 // timeout, in seconds.&#xA;// Run:&#xA;sql(s&#34;use $databaseName&#34;)&#xA;val experiment = tpcds.runExperiment(&#xA;  queries, &#xA;  iterations = iterations,&#xA;  resultLocation = resultLocation,&#xA;  forkThread = true)&#xA;experiment.waitForFinish(timeout)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, experiment will be started in a background thread. For every experiment run (i.e. every call of &lt;code&gt;runExperiment&lt;/code&gt;), Spark SQL Perf will use the timestamp of the start time to identify this experiment. Performance results will be stored in the sub-dir named by the timestamp in the given &lt;code&gt;spark.sql.perf.results&lt;/code&gt; (for example &lt;code&gt;/tmp/results/timestamp=1429213883272&lt;/code&gt;). The performance results are stored in the JSON format.&lt;/p&gt; &#xA;&lt;h2&gt;Retrieve results&lt;/h2&gt; &#xA;&lt;p&gt;While the experiment is running you can use &lt;code&gt;experiment.html&lt;/code&gt; to get a summary, or &lt;code&gt;experiment.getCurrentResults&lt;/code&gt; to get complete current results. Once the experiment is complete, you can still access &lt;code&gt;experiment.getCurrentResults&lt;/code&gt;, or you can load the results from disk.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// Get all experiments results.&#xA;val resultTable = spark.read.json(resultLocation)&#xA;resultTable.createOrReplaceTempView(&#34;sqlPerformance&#34;)&#xA;sqlContext.table(&#34;sqlPerformance&#34;)&#xA;// Get the result of a particular run by specifying the timestamp of that run.&#xA;sqlContext.table(&#34;sqlPerformance&#34;).filter(&#34;timestamp = 1429132621024&#34;)&#xA;// or&#xA;val specificResultTable = spark.read.json(experiment.resultPath)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get a basic summary by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;experiment.getCurrentResults // or: spark.read.json(resultLocation).filter(&#34;timestamp = 1429132621024&#34;)&#xA;  .withColumn(&#34;Name&#34;, substring(col(&#34;name&#34;), 2, 100))&#xA;  .withColumn(&#34;Runtime&#34;, (col(&#34;parsingTime&#34;) + col(&#34;analysisTime&#34;) + col(&#34;optimizationTime&#34;) + col(&#34;planningTime&#34;) + col(&#34;executionTime&#34;)) / 1000.0)&#xA;  .select(&#39;Name, &#39;Runtime)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;TPC-H&lt;/h1&gt; &#xA;&lt;p&gt;TPC-H can be run similarly to TPC-DS replacing &lt;code&gt;tpcds&lt;/code&gt; for &lt;code&gt;tpch&lt;/code&gt;. Take a look at the data generator and &lt;code&gt;tpch_run&lt;/code&gt; notebook code below.&lt;/p&gt; &#xA;&lt;h2&gt;Running in Databricks workspace (or spark-shell)&lt;/h2&gt; &#xA;&lt;p&gt;There are example notebooks in &lt;code&gt;src/main/notebooks&lt;/code&gt; for running TPCDS and TPCH in the Databricks environment. &lt;em&gt;These scripts can also be run from spark-shell command line with minor modifications using &lt;code&gt;:load file_name.scala&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TPC-multi_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook (or scala script) can be use to generate both TPCDS and TPCH data at selected scale factors. It is a newer version from the &lt;code&gt;tpcds_datagen&lt;/code&gt; notebook below. To use it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edit the config variables the top of the script.&lt;/li&gt; &#xA; &lt;li&gt;Run the whole notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpcds_datagen notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to install dsdgen on all worker nodes, run data generation, and create the TPCDS database. Note that because of the way dsdgen is installed, it will not work on an autoscaling cluster, and &lt;code&gt;num_workers&lt;/code&gt; has to be updated to the number of worker instances on the cluster. Data generation may also break if any of the workers is killed - the restarted worker container will not have &lt;code&gt;dsdgen&lt;/code&gt; anymore.&lt;/p&gt; &#xA;&lt;h3&gt;tpcds_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCDS queries.&lt;/p&gt; &#xA;&lt;p&gt;For running parallel TPCDS streams:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Cluster and attach the spark-sql-perf library to it.&lt;/li&gt; &#xA; &lt;li&gt;Create a Job using the notebook and attaching to the created cluster as &#34;existing cluster&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Allow concurrent runs of the created job.&lt;/li&gt; &#xA; &lt;li&gt;Launch appriopriate number of Runs of the Job to run in parallel on the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;tpch_run notebook&lt;/h3&gt; &#xA;&lt;p&gt;This notebook can be used to run TPCH queries. Data needs be generated first.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>scalagwt/scalagwt-jribble</title>
    <updated>2024-05-01T01:49:21Z</updated>
    <id>tag:github.com,2024-05-01:/scalagwt/scalagwt-jribble</id>
    <link href="https://github.com/scalagwt/scalagwt-jribble" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jribble library&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This project defines jribble AST, syntax and provides jribble parsers and printers.&lt;/p&gt; &#xA;&lt;p&gt;In order to build, you need to have simple build tool (sbt) installed. Installation is easy as executing following commands (on Linux):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget http://simple-build-tool.googlecode.com/files/sbt-launch-0.7.4.jar -O ~/bin/sbt-launch-0.7.4.jar&#xA;echo &#39;java -Xmx512M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m -jar `dirname $0`/sbt-launch-0.7.4.jar &#34;$@&#34;&#39; &amp;gt; ~/bin/sbt&#xA;chmod u+x ~/bin/sbt&#xA;echo &#39;export PATH=$PATH:~/bin&#39; &amp;gt;&amp;gt; ~/.bashrc&#xA;source ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or on Mac:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget http://simple-build-tool.googlecode.com/files/sbt-launch-0.7.4.jar -O ~/bin/sbt-launch-0.7.4.jar&#xA;echo &#39;java -Xmx512M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256m -jar `dirname $0`/sbt-launch-0.7.4.jar &#34;$@&#34;&#39; &amp;gt; ~/bin/sbt&#xA;chmod u+x ~/bin/sbt&#xA;echo &#39;export PATH=$PATH:~/bin&#39; &amp;gt;&amp;gt; ~/.bash_profile&#xA;source ~/.bash_profile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then type &#39;sbt&#39; and you should see sbt&#39;s console. Useful commands are &#39;compile&#39; and &#39;test&#39;.&lt;/p&gt; &#xA;&lt;p&gt;For more information consult sbt&#39;s homepage: &lt;a href=&#34;http://code.google.com/p/simple-build-tool/&#34;&gt;http://code.google.com/p/simple-build-tool/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>