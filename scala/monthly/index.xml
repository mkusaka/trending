<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Monthly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-01T02:05:49Z</updated>
  <subtitle>Monthly Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ucb-bar/gemmini</title>
    <updated>2025-03-01T02:05:49Z</updated>
    <id>tag:github.com,2025-03-01:/ucb-bar/gemmini</id>
    <link href="https://github.com/ucb-bar/gemmini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Berkeley&#39;s Spatial Array Generator&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1000&#34; src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/full-logo.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Gemmini&lt;/h1&gt; &#xA;&lt;p&gt;The Gemmini project is developing a full-system, full-stack DNN hardware exploration and evaluation platform. Gemmini enables architects to make useful insights into how different components of the system and software stack (outside of just the accelerator itself) interact to affect overall DNN performance.&lt;/p&gt; &#xA;&lt;p&gt;Gemmini is part of the &lt;a href=&#34;https://github.com/ucb-bar/chipyard&#34;&gt;Chipyard&lt;/a&gt; ecosystem, and was developed using the &lt;a href=&#34;https://www.chisel-lang.org/&#34;&gt;Chisel&lt;/a&gt; hardware description language.&lt;/p&gt; &#xA;&lt;p&gt;This document is intended to provide information for beginners wanting to try out Gemmini, as well as more advanced in-depth information for those who might want to start hacking on Gemmini&#39;s source code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/gemmini-system.png&#34; alt=&#34;Gemmini&#39;s high-level architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;We provide here a quick guide to installing Gemmini&#39;s dependencies (Chipyard and Spike), building Gemmini hardware and software, and then running that software on our hardware simulators.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Before beginning, install the &lt;a href=&#34;https://chipyard.readthedocs.io/en/latest/Chipyard-Basics/Initial-Repo-Setup.html#default-requirements-installation&#34;&gt;Chipyard dependencies&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installing Chipyard and Spike&lt;/h2&gt; &#xA;&lt;p&gt;Run these steps to install Chipyard and Spike (make sure to checkout the correct Chipyard and Spike commits as shown below):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/ucb-bar/chipyard.git&#xA;cd chipyard&#xA;./build-setup.sh&#xA;&#xA;source env.sh&#xA;&#xA;cd generators/gemmini&#xA;make -C software/libgemmini install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building Gemmini Software&lt;/h2&gt; &#xA;&lt;p&gt;Run the steps below to compile Gemmini programs, including large DNN models like ResNet50, and small matrix-multiplication tests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/generators/gemmini/software/gemmini-rocc-tests&#xA;./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, you&#39;ll find RISC-V binaries in &lt;code&gt;build/&lt;/code&gt;, for &#34;baremetal&#34; environments, Linux environments, and &#34;proxy-kernel&#34; environments.&lt;/p&gt; &#xA;&lt;p&gt;Linux binaries are meant to be executed on SoCs that run Linux. These binaries are dynamically linked, and support all syscalls. Typically, our users run them on &lt;a href=&#34;https://fires.im/&#34;&gt;FireSim&lt;/a&gt; simulators.&lt;/p&gt; &#xA;&lt;p&gt;Baremetal binaries are meant to be run in an environment without any operating system available. They lack support for most syscalls, and do not support virtual memory either. Our users typically run them on cycle-accurate simulators like Verilator or VCS.&lt;/p&gt; &#xA;&lt;p&gt;&#34;Proxy-kernel&#34; binaries are meant to be run on a stripped down version of Linux, called the &lt;a href=&#34;https://github.com/riscv-software-src/riscv-pk&#34;&gt;&#34;RISC-V Proxy Kernel.&#34;&lt;/a&gt; These binaries support virtual memory, and are typically run on cycle-accurate simulators like Verilator.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; Proxy-kernel binaries have limited heap space, so some Gemmini programs that work correctly in baremetal or Linux environments may fail on the proxy-kernel.&lt;/p&gt; &#xA;&lt;h2&gt;Building Gemmini Hardware and Cycle-Accurate Simulators&lt;/h2&gt; &#xA;&lt;p&gt;Run the instructions below to build a cycle-accurate Gemmini simulator using Verilator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/sims/verilator&#xA;make CONFIG=GemminiRocketConfig&#xA;&#xA;# Or, if you want a simulator that can generate waveforms, run this:&#xA;make debug CONFIG=GemminiRocketConfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running this, in addition to the cycle-accurate simulator, you will be able to find the Verilog description of your SoC in &lt;code&gt;generated-src/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Using Gemmini Functional Simulators&lt;/h2&gt; &#xA;&lt;p&gt;Spike typically runs &lt;em&gt;much&lt;/em&gt; faster than cycle-accurate simulators like Verilator or VCS. However, Spike can only verify functional correctness; it cannot give accurate performance metrics or profiling information.&lt;/p&gt; &#xA;&lt;h2&gt;Run Simulators&lt;/h2&gt; &#xA;&lt;p&gt;Run the instructions below to run the Gemmini RISCV binaries that we built previously, using the simulators that we built above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd chipyard/sims/verilator&#xA;&#xA;# Run a large DNN workload in the functional simulator&#xA;spike --extension=gemmini pk ../../generators/gemmini/software/gemmini-rocc-tests/build/imagenet/resnet50-pk&#xA;&#xA;# Run a small DNN workload in the functional simulator&#xA;spike --extension=gemmini ../../generators/gemmini/software/gemmini-rocc-tests/build/imagenet/resnet50-baremetal&#xA;&#xA;# Run a smaller workload in baremetal mode, on a cycle-accurate simulator&#xA;make CONFIG=GemminiRocketConfig run-binary BINARY=../../generators/gemmini/software/gemmini-rocc-tests/build/baremetalC/template-baremetal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://sites.google.com/berkeley.edu/gemmini-tutorial-mlsys-2022&#34;&gt;MLSys 2022 tutorial&lt;/a&gt; (or our earlier but more out-of-date &lt;a href=&#34;https://sites.google.com/berkeley.edu/gemminitutorialiiswc2021/&#34;&gt;IISWC 2021 tutorial&lt;/a&gt;) to learn how to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;build different types of diverse accelerators using Gemmini.&lt;/li&gt; &#xA; &lt;li&gt;add custom datatypes to Gemmini.&lt;/li&gt; &#xA; &lt;li&gt;write your own Gemmini programs.&lt;/li&gt; &#xA; &lt;li&gt;profile your workloads using Gemmini&#39;s performance counters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, consider learning about &lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/fires.im&#34;&gt;FireSim&lt;/a&gt;, a platform for FPGA-accelerated cycle-accurate simulation. We use FireSim to run end-to-end DNN workloads that would take too long to run on Verilator/VCS. FireSim also allows users to check that their Gemmini hardware/software will work when running on a Linux environment.&lt;/p&gt; &#xA;&lt;p&gt;Or, continue reading the rest of this document for descriptions of Gemmini&#39;s architecture, ISA, and configuration parameters.&lt;/p&gt; &#xA;&lt;h1&gt;Architecture&lt;/h1&gt; &#xA;&lt;p&gt;Gemmini is implemented as a RoCC accelerator with non-standard RISC-V custom instructions. The Gemmini unit uses the RoCC port of a Rocket or BOOM &lt;em&gt;tile&lt;/em&gt;, and by default connects to the memory system through the System Bus (i.e., directly to the L2 cache).&lt;/p&gt; &#xA;&lt;p&gt;At the heart of the accelerator lies a systolic array which performs matrix multiplications. By default, the matrix multiplication support both &lt;em&gt;output-stationary&lt;/em&gt; and &lt;em&gt;weight-stationary&lt;/em&gt; dataflows, which programmers can pick between at runtime. However, the dataflow can also be hardened at elaboration time.&lt;/p&gt; &#xA;&lt;p&gt;The systolic array&#39;s inputs and outputs are stored in an explicity managed scratchpad, made up of banked SRAMs. A DMA engine facilitates the transfer of data between main memory (which is visible to the host CPU) and the scratchpad.&lt;/p&gt; &#xA;&lt;p&gt;Because weight-stationary dataflows require an accumulator outside the systolic array, we add a final SRAM bank, equipped with adder units, which can be conceptually considered an extension of the scratchpad memory space. The systolic array can store results to any address in the accumulator, and can also read new inputs from any address in the accumulator. The DMA engine can also tranfer data directly between the accumulator and main memory, which is often necessary to load in biases.&lt;/p&gt; &#xA;&lt;p&gt;Gemmini also includes peripheral circuitry to optionally apply activation functions such as ReLU or ReLU6, scale results down by powers-of-2 to support quantized workloads, or to transpose matrices before feeding them into the systolic array to support the output-stationary dataflow.&lt;/p&gt; &#xA;&lt;h2&gt;Generator Parameters&lt;/h2&gt; &#xA;&lt;p&gt;Major parameters of interest include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Systolic array dimensions (&lt;code&gt;tileRows&lt;/code&gt;, &lt;code&gt;tileColumns&lt;/code&gt;, &lt;code&gt;meshRows&lt;/code&gt;, &lt;code&gt;meshColumns&lt;/code&gt;): The systolic array is composed of a 2-level hierarchy, in which each tile is fully combinational, while a mesh of tiles has pipeline registers between each tile.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/gemmini-systolic-array.png&#34; alt=&#34;Gemmini&#39;s systolic two-tiered hierarchy&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Dataflow parameters (&lt;code&gt;dataflow&lt;/code&gt;): Determine whether the systolic array in Gemmini is output-stationary or weight-stationary, or whether it supports both dataflows so that programmers may choose between them at runtime.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Scratchpad and accumulator memory parameters (&lt;code&gt;sp_banks&lt;/code&gt;, &lt;code&gt;sp_capacity&lt;/code&gt;, &lt;code&gt;acc_capacity&lt;/code&gt;): Determine the properties of the Gemmini scratchpad memory: overall capacity of the scratchpad or accumulators (in KiB), and the number of banks the scratchpad is divided into.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Type parameters (&lt;code&gt;inputType&lt;/code&gt;, &lt;code&gt;outputType&lt;/code&gt;, &lt;code&gt;accType&lt;/code&gt;): Determine the data-types flowing through different parts of a Gemmini accelerator. For example, &lt;code&gt;inputType&lt;/code&gt; may be an 8-bit fixed-point number, while &lt;code&gt;accType&lt;/code&gt;, which determines the type of partial accumulations in a matrix multiplication, may be a 32-bit integer. &lt;code&gt;outputType&lt;/code&gt; only determines the type of the data passed between two processing elements (PEs); for example, an 8-bit multiplication may produce a 16-bit result which must be shared between PEs in a systolic array.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Examples of possible datatypes are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;SInt(8.W)&lt;/code&gt; for a signed 8-bit integer&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;UInt(32.W)&lt;/code&gt; for an unsigned 32-bit integer&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;Float(8, 24)&lt;/code&gt; for a single-precision IEEE floating point number&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;If your datatype is a floating-point number, then you might also want to change the &lt;code&gt;pe_latency&lt;/code&gt; parameter, which specifies how many shift registers to add inside the PEs. This might be necessary if your datatype cannot complete a multiply-accumulate operation within a single cycle.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Access-execute queue parameters (&lt;code&gt;ld_queue_length&lt;/code&gt;, &lt;code&gt;st_queue_length&lt;/code&gt;, &lt;code&gt;ex_queue_length&lt;/code&gt;, &lt;code&gt;rob_entries&lt;/code&gt;): To implement access-execute decoupling, a Gemmini accelerator has a load instruction queue, a store instruction queue, and an execute instruction queue. The relative sizes of these queue determine the level of access-execute decoupling. Gemmini also implements a reorder buffer (ROB) - the number of entries in the ROB determines possible dependency management limitations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;DMA parameters (&lt;code&gt;dma_maxbytes&lt;/code&gt;, &lt;code&gt;dma_buswidth&lt;/code&gt;, &lt;code&gt;mem_pipeline&lt;/code&gt;): Gemmini implements a DMA to move data from main memory to the Gemmini scratchpad, and from the Gemmini accumulators to main memory. The size of these DMA transactions is determined by the DMA parameters. These DMA parameters are tightly coupled with Rocket Chip SoC system parameters: in particular &lt;code&gt;dma_buswidth&lt;/code&gt; is associated with the &lt;code&gt;SystemBusKey&lt;/code&gt; &lt;code&gt;beatBytes&lt;/code&gt; parameter, and &lt;code&gt;dma_maxbytes&lt;/code&gt; is associated with &lt;code&gt;cacheblockbytes&lt;/code&gt; Rocket Chip parameters.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are also optional features, which can be either enabled or left out of Gemmini at elaboration-time. For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scaling during &#34;move-in&#34; operations (&lt;code&gt;mvin_scale_args&lt;/code&gt;, &lt;code&gt;mvin_scale_acc_args&lt;/code&gt;): When data is being moved in from DRAM or main memory into Gemmini&#39;s local scratchpad memory, it can optionally be multiplied by a scaling factor. These parameters specify what the datatype of the scaling factor is, and how the scaling is actually done. If these are set to &lt;code&gt;None&lt;/code&gt;, then this optional feature will be disabled at elaboration time. If both the scratchpad inputs are accumulator inputs are to be scaled in the same say, then the &lt;code&gt;mvin_scale_shared&lt;/code&gt; parameter can be set to &lt;code&gt;true&lt;/code&gt; so that the multipliers and functional units are shared.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Major Components&lt;/h2&gt; &#xA;&lt;p&gt;This subsection is aimed towards those who wish to start hacking on Gemmini&#39;s RTL. Here, we briefly describe Gemmini&#39;s main hardware components, and how they fit together. If you have no interest in changing Gemmini&#39;s hardware (besides just changing configuration parameters), then feel free to skip this section.&lt;/p&gt; &#xA;&lt;h3&gt;Decoupled Access/Execute&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini is a decoupled access/execute architecture, which means that &#34;memory-access&#34; and &#34;execute&#34; instructions happen concurrently, in different regions of the hardware. We divide the hardware broadly into three &#34;controllers&#34;: one for &#34;execute&#34; instructions, another for &#34;load&#34; instructions, and a third for &#34;store&#34; instructions. Each of these controllers consume direct ISA commands from the programmer, decode this commands, and execute them, while sharing access to the scratchpad and acccumulator SRAMs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ExecuteController&lt;/code&gt;: This module is responsible for executing &#34;execute&#34;-type ISA commands, such as matrix multiplications. It includes a systolic array for dot-products, and a transposer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;LoadController&lt;/code&gt;: This module is responsible for all instructions that move data from main memory into Gemmini&#39;s private scratchpad or accumulator.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;StoreController&lt;/code&gt;: This module is responsible for all instructions that move data from Gemmini&#39;s private SRAMs into main memory. This module is also responsible for &#34;max-pooling&#34; instructions, because Gemmini performs pooling when moving unpooled data from the private SRAMs into main memory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Scratchpad and Accumulator&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini stores inputs and outputs for the systolic array in a set of private SRAMs, which we call the &#34;scratchpad&#34; and the &#34;accumulator&#34;. Typically, inputs are stored in the scratchpad, while partial sums and final results are stored in the the accumulator.&lt;/p&gt; &#xA;&lt;p&gt;The scratchpad and accumulator are both instantiated within &lt;code&gt;Scratchpad.scala&lt;/code&gt;. The scratchpad banks are implemented by the &lt;code&gt;ScratchpadBank&lt;/code&gt; module, and the accumulator banks are implemented by the &lt;code&gt;AccumulatorMem&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;Each row of the scratchpad and accumulator SRAMs is &lt;code&gt;DIM&lt;/code&gt; &#34;elements&#34; wide, where &lt;code&gt;DIM&lt;/code&gt; is the number of PEs along the width of the systolic array. Each &#34;element&#34; represents a single scalar value that Gemmini operates upon.&lt;/p&gt; &#xA;&lt;p&gt;Each &#34;element&#34; in the scratchpad is of type &lt;code&gt;inputType&lt;/code&gt; (which, in the default config, is an 8-bit integer). Each &#34;element&#34; in the acccumulator is of type &lt;code&gt;accType&lt;/code&gt; (which, in the default config, is a 32-bit integer).&lt;/p&gt; &#xA;&lt;p&gt;So, for example, in the default config, which has a 16x16 systolic array, the scratchpad banks have a row-width of &lt;code&gt;16*bits(inputType)=128&lt;/code&gt; bits, and the accumulatorr banks have a row-width of &lt;code&gt;16*bits(accType)=512&lt;/code&gt; bits.&lt;/p&gt; &#xA;&lt;p&gt;Both inputs and outputs to the scratchpad must be of type &lt;code&gt;inputType&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Both inputs and outputs from the accumulator can be either of type &lt;code&gt;accType&lt;/code&gt; &lt;em&gt;or&lt;/em&gt; &lt;code&gt;inputType&lt;/code&gt;. If &lt;code&gt;inputType&lt;/code&gt; values are input to the accumulator, they will be cast up to &lt;code&gt;accType&lt;/code&gt;. If &lt;code&gt;inputType&lt;/code&gt; values are output from the accumulator, they will first be &#34;scaled&#34; down to be of type &lt;code&gt;inputType&lt;/code&gt;. The exact &#34;scaling&#34; function can be configured as the as the user wishes, but in the default config, the scaling function is a simple multiplication by a &lt;code&gt;float32&lt;/code&gt; value that casts an &lt;code&gt;int32&lt;/code&gt; down to an &lt;code&gt;int8&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The scratchpad banks are very simple, comprising little more than an SRAM and a queue.&lt;/p&gt; &#xA;&lt;p&gt;The accumulator banks are a bit more complex: in addition to the underlying SRAM, they also include a set of adders to support in-place accumulations. In addition, they have a set of &#34;scalers&#34; (described above), and activation function units. The scaling and activation functions are applied when the programmer wishes to transform &lt;code&gt;accType&lt;/code&gt; values down to &lt;code&gt;inputType&lt;/code&gt; values while reading data out of the accumulator. This is typically done to transform the partial-sum outputs of one layer into the low-bitwidth quantized inputs of the next layer.&lt;/p&gt; &#xA;&lt;h3&gt;Systolic Array and Transposer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;MeshWithDelays&lt;/code&gt;, which is instantiated within the &lt;code&gt;ExecuteController&lt;/code&gt;, contains the systolic array (&lt;code&gt;Mesh&lt;/code&gt;), a transposer (&lt;code&gt;Transposer&lt;/code&gt;), and a set of delay registers which shift the inputs to the systolic array. The &lt;code&gt;MeshWithDelays&lt;/code&gt; module takes in three matrices one row at a time per cycle (&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;), and outputs the result &lt;code&gt;C = A * B + D&lt;/code&gt; one row at a time per cycle.&lt;/p&gt; &#xA;&lt;p&gt;In the weight-stationary mode, the &lt;code&gt;B&lt;/code&gt; values are &#34;preloaded&#34; into the systolic array, and &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;D&lt;/code&gt; values are fed through. In the output-stationary mode, the &lt;code&gt;D&lt;/code&gt; values are &#34;preloaded&#34; into the systolic array, and &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; values are fed through.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt; are all of type &lt;code&gt;inputType&lt;/code&gt;, while &lt;code&gt;C&lt;/code&gt; is of type &lt;code&gt;outputType&lt;/code&gt;. If the programmer wishes to write &lt;code&gt;C&lt;/code&gt; into the scratchpad, then &lt;code&gt;C&lt;/code&gt; is cast down to &lt;code&gt;inputType&lt;/code&gt;. However, if the programmer instead wishes to write &lt;code&gt;C&lt;/code&gt; into the accumulator, then &lt;code&gt;C&lt;/code&gt; is cast up to &lt;code&gt;accType&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that in the weight-stationary mode, an &lt;code&gt;inputType&lt;/code&gt; D usually has insufficient bitwidth to accurately represent partial sums. Therefore, in the weight-stationary mode, &lt;code&gt;D&lt;/code&gt; is usually just the 0-matrix, while the &lt;code&gt;accType&lt;/code&gt; accumulator SRAMs are used to accumulate partial sum outputs of the systolic array instead.&lt;/p&gt; &#xA;&lt;p&gt;The inputs (&lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;) must be delayed with shift-registers so that each input from one matrix reaches the correct PE at exactly the right time to be multiplied-and-accumulated with the correct input from another matrix. The diagram below shows an example of a 2x2 output-stationary matmul (ignoring &lt;code&gt;D&lt;/code&gt;), with the appropriate delay registers at the inputs and outputs of the systolic array:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/delay-registers.png&#34; alt=&#34;Systolic array with delay registers&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The systolic array itself (implemented in &lt;code&gt;Mesh.scala&lt;/code&gt;), is composed of a two-tier hierarchy of &lt;code&gt;Tiles&lt;/code&gt; and &lt;code&gt;PEs&lt;/code&gt;. The &lt;code&gt;Mesh&lt;/code&gt; is composed of a set of &lt;code&gt;Tiles&lt;/code&gt;, separated by pipeline registers. Every &lt;code&gt;Tile&lt;/code&gt; is composed of a combinational set of &lt;code&gt;PEs&lt;/code&gt;, where each PE performs a single matmul operation, with either the weight-stationary, or output-stationary dataflow.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/gemmini-systolic-array.png&#34; alt=&#34;Systolic array&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;MeshWithDelays&lt;/code&gt; module also includes a number of counters and configuration registers. &lt;code&gt;MeshWithDelays&lt;/code&gt; assumes that every matmul operation will be exactly of size &lt;code&gt;DIM x DIM&lt;/code&gt;, where &lt;code&gt;DIM&lt;/code&gt; is the number of PEs across the width of the systolic array itself (16 in the default config). These counters count up to &lt;code&gt;DIM&lt;/code&gt;, and then update the configuration registers from the inputs to &lt;code&gt;MeshWithDelays&lt;/code&gt;. These configuration registers control which of &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are to be transposed before being fed into the systolic array. They also control whether the preloaded values in the systolic array are to be maintained for the next matmul, or whether they are to be overwritten and replaced.&lt;/p&gt; &#xA;&lt;p&gt;The transposer itself is implemented as a very simple systolic array, which transports inputs from left-to-right for &lt;code&gt;DIM&lt;/code&gt; cycles, and then down-to-up for another &lt;code&gt;DIM&lt;/code&gt; cycles. This is illustrated in the diagram below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/transposer.png&#34; alt=&#34;Transposer&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that for output-stationary matmuls, the transposer is used even when the programmer does not request a transposition. This is because the systolic array expects inputs from the same row of &lt;code&gt;A&lt;/code&gt; to enter the same PE in the output-stationary mode, but all values in a single row of &lt;code&gt;A&lt;/code&gt; are stored within the same scratchpad SRAM row. Therefore, the rows have to be transposed after being read out of the scratchpad, so that elements on the same row can be fed into the same PE one-after-another, rather than being fed into adjacent PEs.&lt;/p&gt; &#xA;&lt;h3&gt;DMA&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini includes two DMAs, one for reading data from main memory into Gemmini&#39;s private SRAMs, and another for moving data from Gemmini&#39;s private SRAMs into main memory. Both these modules are implemented in &lt;code&gt;DMA.scala&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Both DMAs operate on virtual addresses, and share access to a TLB to translate these into physical main memory addresses. If the TLB misses, it transparently falls back to a PTW that is shared with Gemmini&#39;s host CPU.&lt;/p&gt; &#xA;&lt;p&gt;After physical addresses are obtained from Gemmini&#39;s private TLB, the DMAs break large memory requests up into smaller &lt;a href=&#34;https://sifive.cdn.prismic.io/sifive%2Fcab05224-2df1-4af8-adee-8d9cba3378cd_tilelink-spec-1.8.0.pdf&#34;&gt;TileLink&lt;/a&gt; read and write requests. To satisfy the TileLink protocol, each memory request must be aligned to the number of bytes requested from/to main memory, and the size of each memory request (in bytes) must be a power of 2. The DMAs generally attempt to minimize the number of TileLink requests as much as possible, even if this requires reading a larger total amount of data from main memory. Empirically, we have found that an excessive number TileLink requests can limit performance more than reading a small amount of extra data.&lt;/p&gt; &#xA;&lt;p&gt;The DMAWriter, which writes data from private SRAMs to main memory, also includes a set of &lt;code&gt;&amp;gt;&lt;/code&gt; comparators that are used for max-pooling data during a memory-write operation.&lt;/p&gt; &#xA;&lt;h3&gt;ROB&lt;/h3&gt; &#xA;&lt;p&gt;Due to Gemmini&#39;s decoupled access-execute architecture, instructions in the &lt;code&gt;LoadController&lt;/code&gt;, &lt;code&gt;StoreController&lt;/code&gt;, and &lt;code&gt;ExecuteController&lt;/code&gt; may operate concurrently and out-of-order with respect to instructions in other controllers. Gemmini includes an ROB which is meant to detect hazards between instructions in different controllers. The instructions in the ROB are only issued to their respective controllers once they have no dependencies on instructions in other controllers.&lt;/p&gt; &#xA;&lt;p&gt;Note that instructions that are destined for the same controller are issued in-order. The ROB does not check hazards between instructions within the same controller, because each controller is obligated to handle it&#39;s own dependencies and hazards internally, assuming that it receives it&#39;s own instructions in program-order.&lt;/p&gt; &#xA;&lt;h3&gt;Matmul and Conv Loop Unrollers&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini&#39;s systolic array can only operate on matmuls that are up to &lt;code&gt;DIMxDIM&lt;/code&gt; elements large. When performing matmuls and convolutions that are larger than this, programmers must tile their matmuls into a sequence of smaller &lt;code&gt;DIMxDIM&lt;/code&gt; matmuls.&lt;/p&gt; &#xA;&lt;p&gt;However, tiling these operations efficiently can be difficult for programmers, due to CPU and loop overheads, and the difficulty of unrolling and pipelining software loops.&lt;/p&gt; &#xA;&lt;p&gt;To alleviate this difficulty, Gemmini&#39;s ISA includes high-level CISC-type instructions, which automatically tile and unroll large matmuls and convolutions. These are implemented in the &lt;code&gt;LoopMatmul&lt;/code&gt; and &lt;code&gt;LoopConv&lt;/code&gt; modules.&lt;/p&gt; &#xA;&lt;p&gt;These modules are implemented as FSMs, which double-buffer matmul/conv tiles to maximize performance, and which monitor the proportion of load/store/execute instructions in the ROB to maximize overlap between memory accesses and dot-product computations. For example, if the ROB is dominated by matmul instructions, without leaving any slots for incoming load instructions, then the FSMs will pause the issuing of matmul instructions to allow more space for concurrent load instructions in Gemmini&#39;s datapath.&lt;/p&gt; &#xA;&lt;h1&gt;Software&lt;/h1&gt; &#xA;&lt;p&gt;The Gemmini ISA is specified in the &lt;code&gt;ISA&lt;/code&gt; section below. The ISA includes configuration instructions, data movement instructions (from main memory to/from Gemmini&#39;s private memory), and matrix multiplication execution instructions.&lt;/p&gt; &#xA;&lt;p&gt;Since Gemmini instructions are not exposed through the GNU binutils assembler, several C macros are provided in order to construct the instruction encodings to call these instructions.&lt;/p&gt; &#xA;&lt;p&gt;The Gemmini generator includes a C library which wraps the calls to the custom Gemmini instructions into common DNN operators like matmuls, convolutions (with or without pooling), matrix-additions, etc. The &lt;code&gt;software&lt;/code&gt; directory of the generator includes the aforementioned library and macros, as well as baremetal tests, and some FireMarshal workloads to run the tests in a Linux environment. In particular, the C library can be found in the &lt;code&gt;software/gemmini-rocc-tests/include/gemmini.h&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;The Gemmini generator generates a C header file based on the generator parameters. This header files gets compiled together with the C library to tune library performance. The generated header file can be found under &lt;code&gt;software/gemmini-rocc-tests/include/gemmini_params.h&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gemmini can also be used to run ONNX-specified neural-networks through a port of Microsoft&#39;s ONNX-Runtime framework. The port is included as the &lt;a href=&#34;https://github.com/pranav-prakash/onnxruntime-riscv&#34;&gt;onnxruntime-riscv&lt;/a&gt; repository submoduled in the &lt;code&gt;software&lt;/code&gt; directory. To start using ONNX-Runtime, run &lt;code&gt;git submodule update --init --recursive software/onnxruntime-riscv&lt;/code&gt;, and read the documentation &lt;a href=&#34;https://github.com/pranav-prakash/onnxruntime-riscv/raw/systolic/systolic_runner/docs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build and Run Gemmini Tests&lt;/h2&gt; &#xA;&lt;p&gt;To build the Gemmini tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd software/gemmini-rocc-tests/&#xA;./build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, the test binaries will be found in &lt;code&gt;software/gemmini-rocc-tests/build&lt;/code&gt;. Binaries whose names end in &lt;code&gt;-baremetal&lt;/code&gt; are meant to be run in a bare-metal environment, while binaries whose names end in &lt;code&gt;-linux&lt;/code&gt; are meant to run in a Linux environment. You can run the tests either on a cycle-accurate RTL simulator, or on a (much faster) functional ISA simulator called Spike.&lt;/p&gt; &#xA;&lt;p&gt;We use a special extension of Spike, found &lt;a href=&#34;https://github.com/ucb-bar/libgemmini&#34;&gt;here&lt;/a&gt;, which has support for Gemmini instructions. If you are using Chipyard, you can easily build Spike by running &lt;code&gt;./scripts/build-toolchains.sh riscv-tools&lt;/code&gt; from Chipyard&#39;s root directory, then by running &lt;code&gt;make -C software/libgemmini install&lt;/code&gt; in the Gemmini directory. Then, to run the &lt;code&gt;mvin_mvout&lt;/code&gt; test, which simply moves a matrix into Gemmini&#39;s scratchpad before moving it back out into main memory, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd build/bareMetalC&#xA;spike --extension=gemmini mvin_mvout-baremetal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Writing Your Own Gemmini Tests&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;software/gemmini-rocc-tests/bareMetalC/template.c&lt;/code&gt; is a template Gemmini test that you can base your own Gemmini tests off of. To write your own Gemmini test, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd software/gemmini-rocc-tests/&#xA;cp bareMetalC/template.c bareMetalC/my_test.c&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, add &lt;code&gt;my_test&lt;/code&gt; to the &lt;code&gt;tests&lt;/code&gt; list at the top of &lt;code&gt;bareMetalC/Makefile&lt;/code&gt;. Afterwards, running &lt;code&gt;./build.sh&lt;/code&gt; will install &lt;code&gt;my_test-baremetal&lt;/code&gt; in &lt;code&gt;build/bareMetalC&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;DNN Tests&lt;/h2&gt; &#xA;&lt;p&gt;Example DNNs, such as ResNet50, can be found in &lt;code&gt;software/gemmini-rocc-tests/imagenet&lt;/code&gt; and &lt;code&gt;software/gemmini-rocc-tests/mlps&lt;/code&gt;. These tests are built and run the same way as the other tests described above, but they typically take too long to run in a software simulator like VCS or Verilator. We recommend instead that you run these tests through &lt;a href=&#34;https://fires.im/&#34;&gt;Firesim&lt;/a&gt;, an FPGA-accelerated simulation platform, which will reduce your runtime from days to minutes.&lt;/p&gt; &#xA;&lt;p&gt;Note that the DNN tests rely upon our C library of common DNN operators (found in &lt;code&gt;gemmini.h&lt;/code&gt;). They call very few direct Gemmini ISA instructions, and mostly call the wrappers around them found in the C library.&lt;/p&gt; &#xA;&lt;h1&gt;Memory Addressing Scheme&lt;/h1&gt; &#xA;&lt;p&gt;Gemmini&#39;s private memory is &#34;row-addressed&#34;, where each row is &lt;code&gt;DIM&lt;/code&gt; elements wide, where &lt;code&gt;DIM&lt;/code&gt; is the number of PEs across the width of the systolic array (16 in the default config). These elements will be of type &lt;code&gt;inputType&lt;/code&gt; in the scratchpad, and of type &lt;code&gt;accType&lt;/code&gt; in the accumulator.&lt;/p&gt; &#xA;&lt;p&gt;Every private Gemmini memory address is 32 bits long. The three most signficant bits are reserved, and have special meanings:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bit 31 (the MSB) is 0 if we are addressing the scratchpad, and 1 if we are addressing the accumulator.&lt;/li&gt; &#xA; &lt;li&gt;Bit 30 is ignored if we are addressing the scratchpad, or if we are reading from the accumulator. If, instead, we are writing to the accumulator, then bit 30 is 0 if we want to overwrite the data at that address, and 1 if we want to accumulate on top of the data already at that address.&lt;/li&gt; &#xA; &lt;li&gt;Bit 29 is ignored if we are addressing the scratchpad, or if we are writing to the accumulator. If, instead, we are reading from the accumulator, then bit 29 is 0 if we want to read scaled-down &lt;code&gt;inputType&lt;/code&gt; data from the accumulator, and 1 if we want to read &lt;code&gt;accType&lt;/code&gt; data from the accumulator. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If bit 29 is 1 for an accumulator read address, then we do not apply activation functions or scaling to the output of the accumulator.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The memory addressing scheme for a Gemmini config with a 2x2 systolic array is illustrated below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/memory-addressing.png&#34; alt=&#34;Gemmini&#39;s memory addressing scheme&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gemmini accesses main memory addresses (which are also visible to the CPU) through their software-visible virtual addresses. Physical translation addresses are handled by Gemmini, transparently to the programmer.&lt;/p&gt; &#xA;&lt;h1&gt;ISA&lt;/h1&gt; &#xA;&lt;p&gt;This section describes Gemmini&#39;s assembly-level ISA which is made up of custom RISC-V instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Data Movement&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;mvin&lt;/code&gt; Move Data From Main Memory to Scratchpad&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;mvin rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; = virtual DRAM address (byte addressed) to load into scratchpad&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad or accumulator address&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns to load in&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows to load in. Must be less than or equal to &lt;code&gt;DIM&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; Scratchpad[rs2] &amp;lt;= DRAM[Translate[rs1]]&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loads a 2D matrix from main memory into Gemmini&#39;s private memory.&lt;/li&gt; &#xA; &lt;li&gt;Load is sequential from the rs1/rs2 base addresses.&lt;/li&gt; &#xA; &lt;li&gt;Main memory stride must be set by the &lt;code&gt;config_mvin&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;If the number of columns we load in are greater than &lt;code&gt;DIM&lt;/code&gt;, then multiple submatrices will be moved in. The private-memory stride between these submatrices is set by the &lt;code&gt;config_mvin&lt;/code&gt; command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The figure below illustrates how the &lt;code&gt;mvin&lt;/code&gt; command works:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/mvin.png&#34; alt=&#34;Gemmini&#39;s mvin command&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition, the figure below illustrates the special case where the number of columns moved-in is greater than &lt;code&gt;DIM&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/block-mvin.png&#34; alt=&#34;Gemmini&#39;s mvin command with many cols&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are actually &lt;strong&gt;three&lt;/strong&gt; &lt;code&gt;mvin&lt;/code&gt; instructions in Gemmini: &lt;code&gt;mvin&lt;/code&gt;, &lt;code&gt;mvin2&lt;/code&gt;, and &lt;code&gt;mvin3&lt;/code&gt;. &lt;code&gt;mvin2&lt;/code&gt; and &lt;code&gt;mvin3&lt;/code&gt; are completely identical to &lt;code&gt;mvin&lt;/code&gt;, except that they have their own independent set of configuration registers. When calling &lt;code&gt;config_mvin&lt;/code&gt; (described below), the programmer can choose which &lt;code&gt;mvin&lt;/code&gt; instruction they want to configure.&lt;/li&gt; &#xA; &lt;li&gt;The reason we have three &lt;code&gt;mvin&lt;/code&gt; instructions is so that the programmer can overlap loads for A, B, and D matrices (for a &lt;code&gt;A*B+D&lt;/code&gt; matmul), where A, B, and D may all have different main-memory-strides.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;mvout&lt;/code&gt; Move Data from Scratchpad to L2/DRAM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;mvout rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; = virtual DRAM address (byte addressed) to write to from scratchpad&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad address&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns to store&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows to store&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; DRAM[Translate[rs1]] &amp;lt;= Scratchpad[rs2]&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stores a 2D matrix from the scratchpad to main-memory&lt;/li&gt; &#xA; &lt;li&gt;Store is sequential from the rs1/rs2 base addresses. Stride must be set by the &lt;code&gt;config_mvout&lt;/code&gt; command&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_ex&lt;/code&gt; configures the Execute pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_ex rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[1:0]&lt;/code&gt; must be &lt;code&gt;00&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[2]&lt;/code&gt; determines if output (0) or weight (1) stationary&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[3]&lt;/code&gt; = activation function: either relu (1) or no activation function (0)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[8]&lt;/code&gt; = should A be transposed?&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[9]&lt;/code&gt; = should B be transposed?&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:16]&lt;/code&gt; = the stride (in scratchpad addresses) by which the rows of A are fed into the systolic array. &#34;A&#34; in this context refers to the left-hand matrix A in the matmul represented by A * B = C. If this stride is 1, then we feed consecutive rows in the scratchpad, starting from the starting address of A, into the systolic array as the A matrix. If the stride is 2, then we feed every other row into the systolic array instead.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:32]&lt;/code&gt; = the scalar value by which we scale the &lt;code&gt;accType&lt;/code&gt; output of the accumulator down to &lt;code&gt;inputType&lt;/code&gt; values when reading from the accumulator. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In the default config, &lt;code&gt;rs1[63:32]&lt;/code&gt; is of type &lt;code&gt;float32&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = the number of bits by which the accumulated result of a matmul is right-shifted when leaving the systolic array &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This parameter is only relevant in output-stationary mode, when partial sums must be accumulated within the systolic array itself, and scaled-down when leaving the systolic array and being written into the scratchpad.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; mode &amp;lt;= rs1(2); shift &amp;lt;= rs2; A_stride &amp;lt;= rs1[31:16]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;As of now, certain combinations of transpose options cannot be performed unless the right dataflow is chosen. This limitation may be lifted in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataflow&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transpose A&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Transpose B&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Permitted?&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_mvin&lt;/code&gt; configures the Load pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_mvin rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[1:0]&lt;/code&gt; must be &lt;code&gt;01&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[2]&lt;/code&gt; is 0 if &lt;code&gt;mvin&lt;/code&gt;s to the accumulator are of type &lt;code&gt;accType&lt;/code&gt;, and 1 if they are &lt;code&gt;inputType&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[4:3]&lt;/code&gt; is 0 if the stride is being set for &lt;code&gt;mvin&lt;/code&gt;, 1 if the stride is being set for &lt;code&gt;mvin2&lt;/code&gt;, and 2 if the stride is being set for &lt;code&gt;mvin3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:16]&lt;/code&gt; is the scratchpad-memory stride (also called the &#34;private-memory stride&#34; above)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:32]&lt;/code&gt; is the &#34;scale&#34; by which to multiply data as it&#39;s being moved in to the scratchpad. This is ignored if Gemmini isn&#39;t configured to have the ability to scale values during &lt;code&gt;mvin&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2&lt;/code&gt; is the main-memory stride in bytes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; stride &amp;lt;= rs2; scale &amp;lt;= rs1[63:32]&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_mvout&lt;/code&gt; configures the Store pipeline&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_mvout rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[1:0]&lt;/code&gt; must be &lt;code&gt;10&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2&lt;/code&gt; = the stride in bytes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;During &lt;code&gt;mvout&lt;/code&gt; operations, Gemmini can also perform max-pooling. &lt;strong&gt;This is an experimental feature, and is subject to change.&lt;/strong&gt; This feature assumes that data is stored in the scratchpad or accumulator in NHWC format. The parameters controlling this feature are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[5:4]&lt;/code&gt; = max-pooling stride. If this is 0, then max-pooling is deactivated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[7:6]&lt;/code&gt; = max-pooling window size&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[9:8]&lt;/code&gt; = upper zero-padding&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[11:10]&lt;/code&gt; = left zero-padding&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:24]&lt;/code&gt; = output dimension of image after pooling&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[39:32]&lt;/code&gt; = number of pooled rows to output&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[47:40]&lt;/code&gt; = number of pooled columns to output&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[55:48]&lt;/code&gt; = number of unpooled rows to pool&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:56]&lt;/code&gt; = number of unpooled columns to pool&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; stride &amp;lt;= rs2; max-pooling parameters &amp;lt;= rs1&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;config_norm&lt;/code&gt; configures normalization commands&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;config_norm rs1 rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;config_norm&lt;/code&gt; is an &lt;strong&gt;experimental&lt;/strong&gt; command added primarily to support an integer-only variant of BERT called &lt;a href=&#34;https://arxiv.org/abs/2101.01321&#34;&gt;I-BERT&lt;/a&gt; on Gemmini. The command allows users to set scalar constants that are used by I-BERT&#39;s GELU, layernorm, and softmax variants.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;flush&lt;/code&gt; flushes the TLB&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;flush rs1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; = If &lt;code&gt;rs1[0]&lt;/code&gt; is 1, then the current TLB request is skipped (if it has hit a page-fault and is waiting for an interrupt). Otherwise, the current TLB request is repeated.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This instruction executes &lt;em&gt;as soon as it is received&lt;/em&gt; without waiting for other instructions which may be queued up. It is the programmer&#39;s responsibility to insert fences if necessary.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Core Matmul Sequences&lt;/h2&gt; &#xA;&lt;p&gt;Every single matrix multiply operation is a combination of &lt;code&gt;matmul.preload&lt;/code&gt; and &lt;code&gt;matmul.compute&lt;/code&gt; (due to the length of a single instruction, it was split into two instructions). &lt;code&gt;matmul.preload&lt;/code&gt; should precede the &lt;code&gt;matmul.compute&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//// OS matmul example ////&#xA;// rs1 = InputD&#xA;// rs2 = OutputC&#xA;// rs3 = InputA&#xA;// rs4 = InputB&#xA;// matmul InputA InputB OutputC InputD&#xA;1. matmul.preload $rs1 $rs2&#xA;2. matmul.compute $rs3 $rs4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Action:&lt;/strong&gt; Scratchpad[rs2] &amp;lt;= Scratchpad[rs3] * Scratchpad[rs4] + Scratchpad[rs1]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes on addressing:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For B or D, the address can be replaced with all high bits to input a 0 matrix instead.&lt;/li&gt; &#xA; &lt;li&gt;For A, the address can be replaced with all high bits to input a matrix with undefined garbage data instead.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preloading&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;matmul.preload rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:0]&lt;/code&gt; = local scratchpad address of D matrix (when output-stationary), or B matrix (when weight-stationary)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[47:32]&lt;/code&gt; = number of columns of D/B matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:48]&lt;/code&gt; = number of rows of D/B matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad address of C matrix. If this is set to all high bits, then C will not be written to the scratchpad or accumulator.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns of C matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows of C matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 6&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Commit Behavior:&lt;/strong&gt; This instruction commits on the cycle after the systolic array receives it. The systolic array remains idle until the subsequent OS/WS specific instructions are seen.&lt;/p&gt; &#xA;&lt;h3&gt;Computing&lt;/h3&gt; &#xA;&lt;h4&gt;Explicitly Preloaded&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;matmul.compute.preloaded rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[31:0]&lt;/code&gt; = local scratchpad address (systolic array single-axis addressed) of A matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[47:32]&lt;/code&gt; = number of columns of A matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1[63:48]&lt;/code&gt; = number of rows of A matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[31:0]&lt;/code&gt; = local scratchpad address (systolic array single-axis addressed) of B matrix (when output-stationary), or D matrix (when weight-stationary)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[47:32]&lt;/code&gt; = number of columns of B/D matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs2[63:48]&lt;/code&gt; = number of rows of B/D matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 4&lt;/li&gt; &#xA; &lt;li&gt;This instruction will compute on the value preloaded (D if output-stationary, or B if weight-stationary)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Re-use Previous Preloads&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; &lt;code&gt;matmul.compute.accumulated rs1, rs2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;funct&lt;/code&gt; = 5&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; have the same encoding as the &lt;code&gt;matmul.compute.preloaded&lt;/code&gt; encoding&lt;/li&gt; &#xA; &lt;li&gt;If output-stationary, this instruction will compute on the previously computed result (C) in the systolic array, accumulating on top of it&lt;/li&gt; &#xA; &lt;li&gt;If weight-stationary, this instruction will compute on the previously preloaded weights (B) in the systolic array&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Loop Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Gemmini includes CISC-type instructions which can perform matmuls and convolutions on data that is much larger than &lt;code&gt;DIMxDIM&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s nothing these CISC instructions do which a programmer couldn&#39;t do by tiling and looping through the other ISA instructions described above; however, these CISC instructions may achieve higher throughput than such tiled loops written by non-expert programmers. The CISC instructions should be considered performance enhancers; they do not give the accelerator any new functionality that it wouldn&#39;t have otherwise.&lt;/p&gt; &#xA;&lt;p&gt;The CISC instructions have too many operands to fit into a single RISC-V custom instruction. Therefore, they are implemented as a sequence of many RISC-V custom instructions which must be called consecutively by the programmer.&lt;/p&gt; &#xA;&lt;p&gt;These instructions can be found &lt;code&gt;software/gemmini-rocc-tests/include/gemmini.h&lt;/code&gt;, together with example usages. We list below their arguments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;These loop instructions are experimental and subject to change.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;gemmini_loop_ws&lt;/code&gt; Matmul Loop (WS Dataflow)&lt;/h3&gt; &#xA;&lt;p&gt;This instruction calculates &lt;code&gt;A * B + D = C&lt;/code&gt;, but &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; can all be larger than &lt;code&gt;DIMxDIM&lt;/code&gt;. &lt;code&gt;A&lt;/code&gt;, and &lt;code&gt;B&lt;/code&gt; must be of type &lt;code&gt;inputType&lt;/code&gt;, but both &lt;code&gt;D&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; can be &lt;em&gt;either&lt;/em&gt; &lt;code&gt;inputType&lt;/code&gt; or &lt;code&gt;accType&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The sizes of these matrices are represented by &lt;code&gt;I&lt;/code&gt;, &lt;code&gt;J&lt;/code&gt;, and &lt;code&gt;K&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scratchpad rows of A = I * K * DIM&#xA;scratchpad rows of B = K * J * DIM&#xA;accumulator rows of D = I * J * DIM&#xA;accumulator rows of C = I * J * DIM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, the total number of scratchpad rows taken up by a single &lt;code&gt;gemmini_loop_ws&lt;/code&gt; must be at most &lt;strong&gt;half&lt;/strong&gt; of the total scratchpad size, because Gemmini performs double-buffering during CISC instructions. To compute larger matrix multiplies, the loop instructions must also be tiled within an outer loop.&lt;/p&gt; &#xA;&lt;p&gt;To support outer-tiling of the &lt;code&gt;gemmini_loop_ws&lt;/code&gt; instruction, we include an argument called &lt;code&gt;ex_accumulate&lt;/code&gt;, which determines whether to perform a matmul on top of the partial sums that already exist within the accumulator (from previous calls to &lt;code&gt;gemmini_loop_ws&lt;/code&gt; within the same outer-loop).&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; Conv Loop (WS Dataflow)&lt;/h3&gt; &#xA;&lt;p&gt;Gemmini also includes a CISC instruction for convolutions, implemented similarly to the matmul CISC instruction. &lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; will perform a convolution with the WS dataflow, and also supports features such as max-pooling, transpose convolutions, and various preprocessing transformations on the weight and input data.&lt;/p&gt; &#xA;&lt;p&gt;Like &lt;code&gt;gemmini_loop_ws&lt;/code&gt;, the inputs to a single &lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; call must fit within half of Gemmini&#39;s private memory, to support double-buffering. If the programmer would like to perform larger convolutions, they must tile and wrap &lt;code&gt;gemmini_loop_conv_ws&lt;/code&gt; within an outer-loop.&lt;/p&gt; &#xA;&lt;h1&gt;Citing Gemmini&lt;/h1&gt; &#xA;&lt;p&gt;If Gemmini helps you in your academic research, you are encouraged to cite our paper. Here is an example bibtex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@INPROCEEDINGS{gemmini-dac,&#xA;  author={Genc, Hasan and Kim, Seah and Amid, Alon and Haj-Ali, Ameer and Iyer, Vighnesh and Prakash, Pranav and Zhao, Jerry and Grubb, Daniel and Liew, Harrison and Mao, Howard and Ou, Albert and Schmidt, Colin and Steffl, Samuel and Wright, John and Stoica, Ion and Ragan-Kelley, Jonathan and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},&#xA;  booktitle={Proceedings of the 58th Annual Design Automation Conference (DAC)}, &#xA;  title={Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration}, &#xA;  year={2021},&#xA;  volume={},&#xA;  number={},&#xA;  pages={}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project was, in part, funded by the U.S. Government under the DARPA RTML program (contract FA8650-20-2-7006). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.&lt;/li&gt; &#xA; &lt;li&gt;The Gemmini &lt;a href=&#34;https://raw.githubusercontent.com/ucb-bar/gemmini/master/img/full-logo.svg&#34;&gt;logo&lt;/a&gt; was designed by Dima Nikiforov (&lt;a href=&#34;https://github.com/CobbledSteel&#34;&gt;@CobbledSteel&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>SpinalHDL/SpinalHDL</title>
    <updated>2025-03-01T02:05:49Z</updated>
    <id>tag:github.com,2025-03-01:/SpinalHDL/SpinalHDL</id>
    <link href="https://github.com/SpinalHDL/SpinalHDL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala based HDL&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;About SpinalHDL&lt;/h2&gt; &#xA;&lt;p&gt;SpinalHDL is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A language to describe digital hardware&lt;/li&gt; &#xA; &lt;li&gt;Compatible with EDA tools, as it generates VHDL/Verilog files&lt;/li&gt; &#xA; &lt;li&gt;Much more powerful than VHDL, Verilog, and SystemVerilog in its syntax and features&lt;/li&gt; &#xA; &lt;li&gt;Much less verbose than VHDL, Verilog, and SystemVerilog&lt;/li&gt; &#xA; &lt;li&gt;Not an HLS, nor based on the event-driven paradigm&lt;/li&gt; &#xA; &lt;li&gt;Only generates what you asked it in a one-to-one way (no black-magic, no black box)&lt;/li&gt; &#xA; &lt;li&gt;Not introducing area/performance overheads in your design (versus a hand-written VHDL/Verilog design)&lt;/li&gt; &#xA; &lt;li&gt;Based on the RTL description paradigm, but can go much further&lt;/li&gt; &#xA; &lt;li&gt;Allowing you to use Object-Oriented Programming and Functional Programming to elaborate your hardware and verify it&lt;/li&gt; &#xA; &lt;li&gt;Free and can be used in the industry without any license&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Documentation &lt;br&gt; &lt;a href=&#34;https://spinalhdl.github.io/SpinalDoc-RTD/&#34;&gt;https://spinalhdl.github.io/SpinalDoc-RTD/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chinese documentation &lt;br&gt; &lt;a href=&#34;https://spinalhdl.github.io/SpinalDoc-RTD/zh_CN/&#34;&gt;https://spinalhdl.github.io/SpinalDoc-RTD/zh_CN/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chinese documentation (v1.7.2 from thuCGRA) &lt;br&gt; &lt;a href=&#34;https://thucgra.github.io/SpinalHDL_Chinese_Doc/&#34;&gt;https://thucgra.github.io/SpinalHDL_Chinese_Doc/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;API reference &lt;br&gt; &lt;a href=&#34;https://spinalhdl.github.io/SpinalHDL/dev/spinal/index.html&#34;&gt;https://spinalhdl.github.io/SpinalHDL/dev/spinal/index.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Presentation of the language &lt;br&gt; &lt;a href=&#34;https://spinalhdl.github.io/SpinalDoc-RTD/master/SpinalHDL/Getting%20Started/presentation.html&#34;&gt;https://spinalhdl.github.io/SpinalDoc-RTD/master/SpinalHDL/Getting%20Started/presentation.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SBT base project &lt;br&gt; &lt;a href=&#34;https://github.com/SpinalHDL/SpinalTemplateSbt&#34;&gt;https://github.com/SpinalHDL/SpinalTemplateSbt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradle base project &lt;br&gt; &lt;a href=&#34;https://github.com/SpinalHDL/SpinalTemplateGradle&#34;&gt;https://github.com/SpinalHDL/SpinalTemplateGradle&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Jupyter bootcamp &lt;br&gt; &lt;a href=&#34;https://github.com/SpinalHDL/Spinal-bootcamp&#34;&gt;https://github.com/SpinalHDL/Spinal-bootcamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Workshop &lt;br&gt; &lt;a href=&#34;https://github.com/SpinalHDL/SpinalWorkshop&#34;&gt;https://github.com/SpinalHDL/SpinalWorkshop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google group &lt;br&gt; &lt;a href=&#34;https://groups.google.com/forum/#!forum/spinalhdl-hardware-description-language&#34;&gt;https://groups.google.com/forum/#!forum/spinalhdl-hardware-description-language&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Donation channel &lt;br&gt; &lt;a href=&#34;https://opencollective.com/spinalhdl&#34;&gt;https://opencollective.com/spinalhdl&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;English: &lt;a href=&#34;https://gitter.im/SpinalHDL/SpinalHDL?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/SpinalHDL/SpinalHDL.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/SpinalHDL/SpinalHDL&#34;&gt;&lt;/a&gt; 中文: &lt;a href=&#34;https://gitter.im/SpinalHDL-CN/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/SpinalHDL/SpinalHDL.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/SpinalHDL-CN/community&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get it&lt;/h2&gt; &#xA;&lt;p&gt;SpinalHDL is simply a set of Scala libraries. Include them into your project and you&#39;re good to go! If you&#39;re unsure about what to do, simply clone one of our example projects (see links above).&lt;/p&gt; &#xA;&lt;h3&gt;SBT (Scala build tool)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scalaVersion := &#34;2.11.12&#34;&#xA;&#xA;libraryDependencies ++= Seq(&#xA;  &#34;com.github.spinalhdl&#34; % &#34;spinalhdl-core_2.11&#34; % &#34;latest.release&#34;,&#xA;  &#34;com.github.spinalhdl&#34; % &#34;spinalhdl-lib_2.11&#34; % &#34;latest.release&#34;,&#xA;  compilerPlugin(&#34;com.github.spinalhdl&#34; % &#34;spinalhdl-idsl-plugin_2.11&#34; % &#34;latest.release&#34;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can force SBT to pick a specific SpinalHDL version by replacing &lt;code&gt;latest.release&lt;/code&gt; with a specific version. See the &lt;a href=&#34;https://github.com/SpinalHDL/SpinalTemplateSbt/raw/master/build.sbt&#34;&gt;SpinalHDL SBT Template project&#39;s &lt;code&gt;build.sbt&lt;/code&gt; file&lt;/a&gt; for a full SBT example.&lt;/p&gt; &#xA;&lt;h3&gt;Gradle&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-kotlin&#34;&gt;repositories {&#xA;&#x9;mavenCentral()&#xA;}&#xA;&#xA;dependencies {&#xA;&#x9;compile group: &#39;com.github.spinalhdl&#39;, name: &#39;spinalhdl-core_2.11&#39;, version: &#39;1.6.4&#39;&#xA;&#x9;compile group: &#39;com.github.spinalhdl&#39;, name: &#39;spinalhdl-lib_2.11&#39;, version: &#39;1.6.4&#39;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mill(Build Tool)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import mill._&#xA;import mill.scalalib._&#xA;&#xA;object MySpinalModule extends ScalaModule {&#xA;  def scalaVersion = &#34;2.11.12&#34;&#xA;&#xA;  def ivyDeps = Agg(&#xA;    ivy&#34;com.github.spinalhdl::spinalhdl-core:1.6.4&#34;,&#xA;    ivy&#34;com.github.spinalhdl::spinalhdl-lib:1.6.4&#34;,&#xA;  )&#xA;&#xA;  def scalacPluginIvyDeps = Agg(ivy&#34;com.github.spinalhdl::spinalhdl-idsl-plugin:1.6.4&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;JAR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://oss.sonatype.org/content/groups/public/com/github/spinalhdl/spinalhdl-core_2.11/&#xA;https://oss.sonatype.org/content/groups/public/com/github/spinalhdl/spinalhdl-lib_2.11/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The files are available &lt;a href=&#34;https://mvnrepository.com/artifact/com.github.spinalhdl&#34;&gt;on Maven&lt;/a&gt; as well.&lt;/p&gt; &#xA;&lt;h2&gt;Change logs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SpinalHDL/SpinalHDL/tags&#34;&gt;https://github.com/SpinalHDL/SpinalHDL/tags&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The SpinalHDL core is using the LGPL3 license while SpinalHDL lib and others are using the MIT license. That&#39;s for the formalities. But there are some practical statements implied by those licenses:&lt;/p&gt; &#xA;&lt;p&gt;Your freedoms are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can use SpinalHDL core and lib in your closed/commercial projects.&lt;/li&gt; &#xA; &lt;li&gt;The generated RTL is yours (.vhd/.v files)&lt;/li&gt; &#xA; &lt;li&gt;Your hardware description is yours (.scala files)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your obligations (and my wish) are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you modify the SpinalHDL core (the compiler itself), please, share your improvements.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, SpinalHDL is provided &#34;as is&#34;, without warranty of any kind.&lt;/p&gt;</summary>
  </entry>
</feed>