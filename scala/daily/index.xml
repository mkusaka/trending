<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-26T01:46:36Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AbsaOSS/spline</title>
    <updated>2023-06-26T01:46:36Z</updated>
    <id>tag:github.com,2023-06-26:/AbsaOSS/spline</id>
    <link href="https://github.com/AbsaOSS/spline" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data Lineage Tracking And Visualization Solution&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://search.maven.org/search?q=g:za.co.absa.spline&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/za.co.absa.spline/parent-pom/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://teamcity.jetbrains.com/viewType.html?buildTypeId=OpenSourceProjects_AbsaOSSSpline_AutomaticBuildsWithTests_Spark24&amp;amp;branch=develop&amp;amp;tab=buildTypeStatusDiv&#34;&gt;&lt;img src=&#34;https://teamcity.jetbrains.com/app/rest/builds/aggregated/strob:%28locator:%28buildType:%28id:OpenSourceProjects_AbsaOSSSpline_AutomaticBuildsWithTests_Spark24%29,branch:develop%29%29/statusIcon.svg?sanitize=true&#34; alt=&#34;TeamCity build (develop)&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codacy.com/gh/AbsaOSS/spline?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=AbsaOSS/spline&amp;amp;utm_campaign=Badge_Grade_Settings&#34;&gt;&lt;img src=&#34;https://api.codacy.com/project/badge/Grade/9b7ba650a3874c2888dba2d25fa73d88&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline&amp;amp;metric=alert_status&#34; alt=&#34;Sonarcloud Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline&amp;amp;metric=sqale_rating&#34; alt=&#34;SonarCloud Maintainability&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline&amp;amp;metric=reliability_rating&#34; alt=&#34;SonarCloud Reliability&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline&amp;amp;metric=security_rating&#34; alt=&#34;SonarCloud Security&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/absaoss/spline-rest-server/&#34;&gt;&lt;img src=&#34;https://badgen.net/docker/pulls/absaoss/spline-rest-server?icon=docker&amp;amp;label=pulls&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Spline â€” an open-source data lineage tracking solution for data processing frameworks like Apache Spark and others&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/Bz_Ml6pNH2E&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/795479/193536311-d6ce6ed8-36ca-43fa-addb-4f9dcf59e974.png&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://absaoss.github.io/spline/&#34;&gt;Spline GitHub Pages&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Getting started&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/AbsaOSS/spline-getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build project&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mvn install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build Docker containers&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/AbsaOSS/spline-getting-started/raw/main/building-docker.md&#34;&gt;Building Docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Building from source code&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Java 11 and Maven 3.6 or above&lt;/li&gt; &#xA; &lt;li&gt;Run Maven build&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# this will produce standard Java artifacts (JAR and WAR files)&#xA;mvn install&#xA;&#xA;# or, if you also want Docker images use this command&#xA;mvn install -Ddocker -Ddockerfile.repositoryUrl=my&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Spline server&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://absaoss.github.io/spline/#step-by-step&#34;&gt;https://absaoss.github.io/spline/#step-by-step&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Versioning strategy&lt;/h3&gt; &#xA;&lt;h5&gt;Application version&lt;/h5&gt; &#xA;&lt;p&gt;Spline server follows &lt;em&gt;Semantic Versioning&lt;/em&gt;[^1] principles. The &lt;em&gt;Public API&lt;/em&gt; in terms of &lt;em&gt;Semantic Versioning&lt;/em&gt; is defined as a combination of API of all Spline modules, including Producer API (REST and Kafka), Consumer REST API, as well as a set of all command-line interfaces (e.g. Admin CLI). Any incompatible change introduced in any of those APIs or CLIs will be accompanied by incrementing the &lt;em&gt;major version&lt;/em&gt; component.&lt;/p&gt; &#xA;&lt;h5&gt;Database schema version&lt;/h5&gt; &#xA;&lt;p&gt;The database schema version number does &lt;strong&gt;not&lt;/strong&gt; follow the &lt;em&gt;Semantic Versioning&lt;/em&gt; principles, it does not directly correlate with the application version and can only be compared to itself. The only relation between the database schema version number and the application version is that the former indicates in which application version the given database schema was introduced.&lt;/p&gt; &#xA;&lt;p&gt;[^1]: Semantic Versioning - &lt;a href=&#34;https://semver.org/&#34;&gt;https://semver.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to measure code coverage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./mvn verify -Dcode-coverage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If module contains measurable data the code coverage report will be generated on path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{local-path}\spline\{module}\target\site\jacoco&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2019 ABSA Group Limited&#xA;&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;    http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>AbsaOSS/spline-spark-agent</title>
    <updated>2023-06-26T01:46:36Z</updated>
    <id>tag:github.com,2023-06-26:/AbsaOSS/spline-spark-agent</id>
    <link href="https://github.com/AbsaOSS/spline-spark-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spline agent for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Agent / Harvester&lt;/h1&gt; &#xA;&lt;p&gt;The Spline agent for Apache Spark is a complementary module to the &lt;a href=&#34;https://absaoss.github.io/spline/&#34;&gt;Spline project&lt;/a&gt; that captures runtime lineage information from the Apache Spark jobs.&lt;/p&gt; &#xA;&lt;p&gt;The agent is a Scala library that is embedded into the Spark driver, listening to Spark events, and capturing logical execution plans. The collected metadata is then handed over to the lineage dispatcher, from where it can either be sent to the Spline server (e.g. via REST API or Kafka), or used in another way, depending on selected dispatcher type (see &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#dispatchers&#34;&gt;Lineage Dispatchers&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The agent can be used with or without a Spline server, depending on your use case. See &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#references&#34;&gt;References&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://search.maven.org/search?q=g:za.co.absa.spline.agent.spark&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/za.co.absa.spline.agent.spark/agent-core_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://teamcity.jetbrains.com/viewType.html?buildTypeId=OpenSourceProjects_AbsaOSS_SplineAgentSpark_AutoBuildSpark24scala212&amp;amp;branch=develop&amp;amp;tab=buildTypeStatusDiv&#34;&gt;&lt;img src=&#34;https://teamcity.jetbrains.com/app/rest/builds/aggregated/strob:%28locator:%28buildType:%28id:OpenSourceProjects_AbsaOSS_SplineAgentSpark_AutoBuildSpark24scala212%29,branch:develop%29%29/statusIcon.svg?sanitize=true&#34; alt=&#34;TeamCity build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline-spark-agent&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline-spark-agent&amp;amp;metric=alert_status&#34; alt=&#34;Sonarcloud Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline-spark-agent&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline-spark-agent&amp;amp;metric=sqale_rating&#34; alt=&#34;SonarCloud Maintainability&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline-spark-agent&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline-spark-agent&amp;amp;metric=reliability_rating&#34; alt=&#34;SonarCloud Reliability&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/dashboard?id=AbsaOSS_spline-spark-agent&#34;&gt;&lt;img src=&#34;https://sonarcloud.io/api/project_badges/measure?project=AbsaOSS_spline-spark-agent&amp;amp;metric=security_rating&#34; alt=&#34;SonarCloud Security&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/absaoss/spline-spark-agent/&#34;&gt;&lt;img src=&#34;https://badgen.net/docker/pulls/absaoss/spline-spark-agent?icon=docker&amp;amp;label=pulls&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;!--ts--&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#versioning&#34;&gt;Versioning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#compat-matrix&#34;&gt;Spark / Scala version compatibility matrix&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#selecting-artifact&#34;&gt;Selecting artifact&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#initialization&#34;&gt;Initialization&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#initialization-codeless&#34;&gt;Codeless&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#initialization-programmatic&#34;&gt;Programmatic&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#configuration&#34;&gt;Configuration&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#properties&#34;&gt;Properties&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#dispatchers&#34;&gt;Lineage Dispatchers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#filters&#34;&gt;Post Processing Filters&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#spark-coverage&#34;&gt;Spark features coverage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#dev-doc&#34;&gt;Developer documentation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#plugins&#34;&gt;Plugin API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#building&#34;&gt;Building for different Scala and Spark versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#references&#34;&gt;References and Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Added by: wajda, at: Fri 14 May 18:05:53 CEST 2021 --&gt; &#xA;&lt;!--te--&gt; &#xA;&lt;p&gt;&lt;a id=&#34;versioning&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Versioning&lt;/h2&gt; &#xA;&lt;p&gt;The Spline Spark Agent follows the &lt;a href=&#34;https://semver.org/&#34;&gt;Semantic Versioning&lt;/a&gt; principles. The &lt;em&gt;Public API&lt;/em&gt; is defined as a set of entry-point classes (&lt;code&gt;SparkLineageInitializer&lt;/code&gt;, &lt;code&gt;SplineSparkSessionWrapper&lt;/code&gt;), extension APIs (Plugin API, filters, dispatchers), configuration properties and a set of supported Spark versions. In other words, the &lt;em&gt;Spline Spark Agent Public API&lt;/em&gt; in terms of &lt;em&gt;SemVer&lt;/em&gt; covers all entities and abstractions that are designed to be used or extended by client applications.&lt;/p&gt; &#xA;&lt;p&gt;The version number &lt;strong&gt;does not&lt;/strong&gt; directly reflect the relation of the Agent to the Spline Producer API (the Spline server). Both the Spline Server and the Agent are designed to be as much mutually compatible as possible, assuming long-term operation and a possibly significant gap in the server and the agent release dates. Such requirement is dictated by the nature of the Agent that could be embedded into some Spark jobs and only rarely if ever updated without posing a risk to stop working because of eventual Spline server update. Likewise, it should be possible to update the Agent anytime (e.g. to fix a bug or support a newer Spark version or a feature that earlier agent version didn&#39;t support) without requiring a Spline server upgrade.&lt;/p&gt; &#xA;&lt;p&gt;Although not required by the above statement, for minimizing user astonishment when the compatibility between too distant &lt;em&gt;Agent&lt;/em&gt; and &lt;em&gt;Server&lt;/em&gt; versions is dropped, we&#39;ll increment the &lt;em&gt;Major&lt;/em&gt; version component.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;compat-matrix&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Spark / Scala version compatibility matrix&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Scala 2.11&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Scala 2.12&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Spark 2.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;(no SQL; no codeless init)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;â€”&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Spark 2.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;(no Delta support)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;â€”&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Spark 2.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Spark 3.0 or newer&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;â€”&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a id=&#34;usage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;selecting-artifact&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Selecting artifact&lt;/h3&gt; &#xA;&lt;p&gt;There are two main agent artifacts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;agent-core&lt;/code&gt; is a Java library that you can use with any compatible Spark version. Use this one if you want to include Spline agent into your custom Spark application, and you want to manage all transitive dependencies yourself.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;spark-spline-agent-bundle&lt;/code&gt; is a fat jar that is designed to be embedded into the Spark driver, either by manually copying it to the Spark&#39;s &lt;code&gt;/jars&lt;/code&gt; directory, or by using &lt;code&gt;--jars&lt;/code&gt; or &lt;code&gt;--packages&lt;/code&gt; argument for the &lt;code&gt;spark-submit&lt;/code&gt;, &lt;code&gt;spark-shell&lt;/code&gt; or &lt;code&gt;pyspark&lt;/code&gt; commands. This artifact is self-sufficient and is &lt;strong&gt;aimed to be used by most users&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Because the bundle is pre-built with all necessary dependencies, it is important to select a proper version of it that matches the minor Spark and Scala versions of your target Spark installation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark-A.B-spline-agent-bundle_X.Y.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;here &lt;code&gt;A.B&lt;/code&gt; is the first two Spark version numbers and &lt;code&gt;X.Y&lt;/code&gt; is the first two Scala version numbers. For example, if you have Spark 2.4.4 pre-built with Scala 2.12.10 then select the following agent bundle:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark-2.4-spline-agent-bundle_2.12.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;AWS Glue Note&lt;/strong&gt;: dependency on &lt;code&gt;org.yaml:snakeyaml:1.33&lt;/code&gt; is &lt;strong&gt;missing&lt;/strong&gt; in Glue flavour of Spark. Please add this dependency on the classpath.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;initialization&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Initialization&lt;/h3&gt; &#xA;&lt;p&gt;Spline agent is basically a Spark query listener that needs to be registered in a Spark session before is can be used. Depending on if you are using it as a library in your custom Spark application, or as a standalone bundle you can choose one of the following initialization approaches.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;initialization-codeless&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Codeless Initialization&lt;/h4&gt; &#xA;&lt;p&gt;This way is the most convenient one, can be used in majority use-cases. Simply include the Spline listener into the &lt;code&gt;spark.sql.queryExecutionListeners&lt;/code&gt; config property (see &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html#static-sql-configuration&#34;&gt;Static SQL Configuration&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pyspark \&#xA;  --packages za.co.absa.spline.agent.spark:spark-2.4-spline-agent-bundle_2.12:&amp;lt;VERSION&amp;gt; \&#xA;  --conf &#34;spark.sql.queryExecutionListeners=za.co.absa.spline.harvester.listener.SplineQueryExecutionListener&#34; \&#xA;  --conf &#34;spark.spline.lineageDispatcher.http.producer.url=http://localhost:9090/producer&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same approach works for &lt;code&gt;spark-submit&lt;/code&gt; and &lt;code&gt;spark-shell&lt;/code&gt; commands.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: all Spline properties set via Spark conf should be prefixed with &lt;code&gt;spark.&lt;/code&gt; prefix in order to be visible to the Spline agent.&lt;br&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#configuration&#34;&gt;Configuration&lt;/a&gt; section for details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;initialization-programmatic&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Programmatic Initialization&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: starting from Spline 0.6 most agent components can be configured or even replaced in a declarative manner either using &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#configuration&#34;&gt;Configuration&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#plugins&#34;&gt;Plugin API&lt;/a&gt;. So normally there should be no need to use a programmatic initialization method. &lt;strong&gt;We recommend to use &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#initialization-codeless&#34;&gt;Codeless Initialization&lt;/a&gt; instead&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;But if for some reason, Codeless Initialization doesn&#39;t fit your needs, or you want to do more customization on Spark agent, you can use programmatic initialization method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Scala&#34;&gt;// given a Spark session ...&#xA;val sparkSession: SparkSession = ???&#xA;&#xA;// ... enable data lineage tracking with Spline&#xA;import za.co.absa.spline.harvester.SparkLineageInitializer._&#xA;sparkSession.enableLineageTracking()&#xA;&#xA;// ... then run some Dataset computations as usual.&#xA;// The lineage will be captured and sent to the configured Spline Producer endpoint.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or in Java syntax:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;import za.co.absa.spline.harvester.SparkLineageInitializer;&#xA;// ...&#xA;SparkLineageInitializer.enableLineageTracking(session);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The method &lt;code&gt;enableLineageTracking()&lt;/code&gt; accepts optional &lt;code&gt;AgentConfig&lt;/code&gt; object that can be used to customize Spline behavior. This is an alternative way to configure Spline. The other one if via the &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#configuration&#34;&gt;property based configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The instance of &lt;code&gt;AgentConfig&lt;/code&gt; can be created by using a builder or one of the factory methods.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// from a sequence of key-value pairs &#xA;val config = AgentConfig.from(???: Iterable[(String, Any)])&#xA;&#xA;// from a Common Configuration&#xA;val config = AgentConfig.from(???: org.apache.commons.configuration.Configuration)&#xA;&#xA;// using a builder&#xA;val config = AgentConfig.builder()&#xA;  // call some builder methods here...&#xA;  .build()&#xA;&#xA;sparkSession.enableLineageTracking(config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;code&gt;AgentConfig&lt;/code&gt; object doesn&#39;t override the standard configuration stack. Instead, it serves as an additional configuration mean with the precedence set between the &lt;code&gt;spline.yaml&lt;/code&gt; and &lt;code&gt;spline.default.yaml&lt;/code&gt; files (see below).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;configuration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The agent looks for configuration in the following sources (listed in order of precedence):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hadoop configuration (&lt;code&gt;core-site.xml&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Spark configuration&lt;/li&gt; &#xA; &lt;li&gt;JVM system properties&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;spline.properties&lt;/code&gt; file on classpath&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;spline.yaml&lt;/code&gt; file on classpath&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AgentConfig&lt;/code&gt; object&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;spline.default.yaml&lt;/code&gt; file on classpath&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The file &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/core/src/main/resources/spline.default.yaml&#34;&gt;spline.default.yaml&lt;/a&gt; contains default values for all Spline properties along with additional documentation. It&#39;s a good idea to look in the file to see what properties are available.&lt;/p&gt; &#xA;&lt;p&gt;The order of precedence might look counter-intuitive, as one would expect that explicitly provided config (&lt;code&gt;AgentConfig&lt;/code&gt; instance) should override ones defined in the outer scope. However, prioritizing global config to local one makes it easier to manage Spline settings centrally on clusters, while still allowing room for customization by job developers.&lt;/p&gt; &#xA;&lt;p&gt;For example, a company could require lineage metadata from jobs executed on a particular cluster to be sanitized, enhanced with some metrics and credentials and stored in a certain metadata store (a database, file, Spline server etc). The Spline configuration needs to be set globally and applied to all Spark jobs automatically. However, some jobs might contain hardcoded properties that the developers used locally or on a testing environment, and forgot to remove them before submitting jobs into a production. In such situation we want cluster settings to have precedence over the job settings. Assuming that hardcoded settings would most likely be defined in the &lt;code&gt;AgentConfig&lt;/code&gt; object, a property file or a JVM properties, on the cluster we could define them in the Spark config or Hadoop config.&lt;/p&gt; &#xA;&lt;p&gt;In case of multiple definitions of property the first occurrence wins, but &lt;code&gt;spline.lineageDispatcher&lt;/code&gt; and &lt;code&gt;spline.postProcessingFilter&lt;/code&gt; properties are composed instead. E.g. if a &lt;em&gt;LineageDispatcher&lt;/em&gt; is set to be &lt;em&gt;Kafka&lt;/em&gt; in one config source and &#39;Http&#39; in another, they would be implicitly wrapped by a composite dispatcher, so both would be called in the order corresponding the config source precedence. See &lt;code&gt;CompositeLineageDispatcher&lt;/code&gt; and &lt;code&gt;CompositePostProcessingFilter&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Every config property is resolved independently. So, for instance, if a &lt;code&gt;DataSourcePasswordReplacingFilter&lt;/code&gt; is used some of its properties might be taken from one config source and the other ones form another, according to the conflict resolution rules described above. This allows administrators to tweak settings of individual Spline components (filters, dispatchers or plugins) without having to redefine and override the whole piece of configuration for a given component.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;properties&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Properties&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;code&gt;spline.mode&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;ENABLED&lt;/code&gt; [default]&lt;/p&gt; &lt;p&gt;Spline will try to initialize itself, but if it fails it switches to DISABLED mode allowing the Spark application to proceed normally without Lineage tracking.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;DISABLED&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Lineage tracking is completely disabled and Spline is unhooked from Spark.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;code&gt;spline.lineageDispatcher&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The logical name of the root lineage dispatcher. See &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#dispatchers&#34;&gt;Lineage Dispatchers&lt;/a&gt; chapter.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;spline.postProcessingFilter&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The logical name of the root post-processing filter. See &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#filters&#34;&gt;Post Processing Filters&lt;/a&gt; chapter.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;dispatchers&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Lineage Dispatchers&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;LineageDispatcher&lt;/code&gt; trait is responsible for sending out the captured lineage information. By default, the &lt;code&gt;HttpLineageDispatcher&lt;/code&gt; is used, that sends the lineage data to the Spline REST endpoint (see Spline Producer API).&lt;/p&gt; &#xA;&lt;p&gt;Available dispatchers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;HttpLineageDispatcher&lt;/code&gt; - sends lineage to an HTTP endpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;KafkaLineageDispatcher&lt;/code&gt; - sends lineage to a Kafka topic&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ConsoleLineageDispatcher&lt;/code&gt; - writes lineage to the console&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LoggingLineageDispatcher&lt;/code&gt; - logs lineage using the Spark logger&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FallbackLineageDispatcher&lt;/code&gt; - sends lineage to a fallback dispatcher if the primary one fails&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CompositeLineageDispatcher&lt;/code&gt; - allows to combine multiple dispatchers to send lineage to multiple endpoints&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each dispatcher can have different configuration parameters. To make the configs clearly separated each dispatcher has its own namespace in which all it&#39;s parameters are defined. I will explain it on a Kafka example.&lt;/p&gt; &#xA;&lt;p&gt;Defining dispatcher&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher=kafka&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you defined the dispatcher all other parameters will have a namespace &lt;code&gt;spline.lineageDispatcher.{{dipatcher-name}}.&lt;/code&gt; as a prefix. In this case it is &lt;code&gt;spline.lineageDispatcher.kafka.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To find out which parameters you can use look into &lt;code&gt;spline.default.yaml&lt;/code&gt;. For kafka I would have to define at least these two properties:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.kafka.topic=foo&#xA;spline.lineageDispatcher.kafka.producer.bootstrap.servers=localhost:9092&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using the Http Dispatcher&lt;/h4&gt; &#xA;&lt;p&gt;This dispatcher is used by default. The only mandatory configuration is url of the producer API rest endpoint (&lt;code&gt;spline.lineageDispatcher.http.producer.url&lt;/code&gt;). Additionally, timeouts, apiVersion and multiple custom headers can be set.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.http.producer.url=&#xA;spline.lineageDispatcher.http.timeout.connection=2000&#xA;spline.lineageDispatcher.http.timeout.read=120000&#xA;spline.lineageDispatcher.http.apiVersion=LATEST&#xA;spline.lineageDispatcher.http.header.X-CUSTOM-HEADER=custom-header-value&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the producer requires token based authentication for requests, below mentioned details must be included in configuration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.http.authentication.type=OAUTH&#xA;spline.lineageDispatcher.http.authentication.grantType=client_credentials&#xA;spline.lineageDispatcher.http.authentication.clientId=&amp;lt;client_id&amp;gt;&#xA;spline.lineageDispatcher.http.authentication.clientSecret=&amp;lt;secret&amp;gt;&#xA;spline.lineageDispatcher.http.authentication.scope=&amp;lt;scope&amp;gt;&#xA;spline.lineageDispatcher.http.authentication.tokenUrl=&amp;lt;token_url&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example: Azure HTTP trigger template API key header can be set like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.http.header.X-FUNCTIONS-KEY=USER_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example: AWS Rest API key header can be set like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.http.header.X-API-Key=USER_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using the Fallback Dispatcher&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;FallbackDispatcher&lt;/code&gt; is a proxy dispatcher that sends lineage to the primary dispatcher first, and then &lt;em&gt;if&lt;/em&gt; there is an error it calls the fallback one.&lt;/p&gt; &#xA;&lt;p&gt;In the following example the &lt;code&gt;HttpLineageDispatcher&lt;/code&gt; will be used as a primary, and the &lt;code&gt;ConsoleLineageDispatcher&lt;/code&gt; as fallback.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher=fallback&#xA;spline.lineageDispatcher.fallback.primaryDispatcher=http&#xA;spline.lineageDispatcher.fallback.fallbackDispatcher=console&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using the Composite Dispatcher&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;CompositeDispatcher&lt;/code&gt; is a proxy dispatcher that forwards lineage data to multiple dispatchers.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want the lineage data to be sent to an HTTP endpoint and to be logged to the console at the same time you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher=composite&#xA;spline.lineageDispatcher.composite.dispatchers=http,console&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, if some dispatchers in the list fail, the others are still attempted. If you want the error in any dispatcher to be treated as fatal, and be propagated to the main process, you set the &lt;code&gt;failOnErrors&lt;/code&gt; property to &lt;code&gt;true&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.composite.failOnErrors=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Creating your own dispatcher&lt;/h4&gt; &#xA;&lt;p&gt;There is also a possibility to create your own dispatcher. It must implement &lt;code&gt;LineageDispatcher&lt;/code&gt; trait and have a constructor with a single parameter of type &lt;code&gt;org.apache.commons.configuration.Configuration&lt;/code&gt;. To use it you must define name and class and also all other parameters you need. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher=my-dispatcher&#xA;spline.lineageDispatcher.my-dispatcher.className=org.example.spline.MyDispatcherImpl&#xA;spline.lineageDispatcher.my-dispatcher.prop1=value1&#xA;spline.lineageDispatcher.my-dispatcher.prop2=value2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Combining dispatchers (complex example)&lt;/h4&gt; &#xA;&lt;p&gt;If you need, you can combine multiple dispatchers into a single one using &lt;code&gt;CompositeLineageDispatcher&lt;/code&gt; and &lt;code&gt;FallbackLineageDispatcher&lt;/code&gt; in any combination as you wish.&lt;/p&gt; &#xA;&lt;p&gt;In the following example the lineage will be first sent to the HTTP endpoint &#34;&lt;a href=&#34;http://10.20.111.222/lineage-primary&#34;&gt;http://10.20.111.222/lineage-primary&lt;/a&gt;&#34;, if that fails it&#39;s redirected to the &#34;&lt;a href=&#34;http://10.20.111.222/lineage-secondary&#34;&gt;http://10.20.111.222/lineage-secondary&lt;/a&gt;&#34; endpoint, and if that one fails as well, lineage is logged to the ERROR logs and the console at the same time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.lineageDispatcher.http1.className=za.co.absa.spline.harvester.dispatcher.HttpLineageDispatcher&#xA;spline.lineageDispatcher.http1.producer.url=http://10.20.111.222/lineage-primary&#xA;&#xA;spline.lineageDispatcher.http2.className=za.co.absa.spline.harvester.dispatcher.HttpLineageDispatcher&#xA;spline.lineageDispatcher.http2.producer.url=http://10.20.111.222/lineage-secondary&#xA;&#xA;spline.lineageDispatcher.errorLogs.className=za.co.absa.spline.harvester.dispatcher.LoggingLineageDispatcher&#xA;spline.lineageDispatcher.errorLogs.level=ERROR&#xA;&#xA;spline.lineageDispatcher.disp1.className=za.co.absa.spline.harvester.dispatcher.FallbackLineageDispatcher&#xA;spline.lineageDispatcher.disp1.primaryDispatcher=http1&#xA;spline.lineageDispatcher.disp1.fallbackDispatcher=disp2&#xA;&#xA;spline.lineageDispatcher.disp2.className=za.co.absa.spline.harvester.dispatcher.FallbackLineageDispatcher&#xA;spline.lineageDispatcher.disp2.primaryDispatcher=http2&#xA;spline.lineageDispatcher.disp2.fallbackDispatcher=disp3&#xA;&#xA;spline.lineageDispatcher.disp3.className=za.co.absa.spline.harvester.dispatcher.CompositeLineageDispatcher&#xA;spline.lineageDispatcher.composite.dispatchers=errorLogs,console&#xA;&#xA;spline.lineageDispatcher=disp1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a id=&#34;filters&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Post Processing Filters&lt;/h3&gt; &#xA;&lt;p&gt;Filters can be used to enrich the lineage with your own custom data or to remove unwanted data like passwords. All filters are applied after the Spark plan is converted to Spline DTOs, but before the dispatcher is called.&lt;/p&gt; &#xA;&lt;p&gt;The procedure how filters are registered and configured is similar to the &lt;code&gt;LineageDispatcher&lt;/code&gt; registration and configuration procedure. A custom filter class must implement &lt;code&gt;za.co.absa.spline.harvester.postprocessing.PostProcessingFilter&lt;/code&gt; trait and declare a constructor with a single parameter of type &lt;code&gt;org.apache.commons.configuration.Configuration&lt;/code&gt;. Then register and configure it like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.postProcessingFilter=my-filter&#xA;spline.postProcessingFilter.my-filter.className=my.awesome.CustomFilter&#xA;spline.postProcessingFilter.my-filter.prop1=value1&#xA;spline.postProcessingFilter.my-filter.prop2=value2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use pre-registered &lt;code&gt;CompositePostProcessingFilter&lt;/code&gt; to chain up multiple filters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.postProcessingFilter=composite&#xA;spline.postProcessingFilter.composite.filters=myFilter1,myFilter2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(see &lt;code&gt;spline.default.yaml&lt;/code&gt; for details and examples)&lt;/p&gt; &#xA;&lt;h4&gt;Using MetadataCollectingFilter&lt;/h4&gt; &#xA;&lt;p&gt;MetadataCollectingFilter provides a way to add additional data to lineage produced by Spline Agent.&lt;/p&gt; &#xA;&lt;p&gt;Data can be added to the following lineage entities: &lt;code&gt;executionPlan&lt;/code&gt;, &lt;code&gt;executionEvent&lt;/code&gt;, &lt;code&gt;operation&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Inside each entity is dedicated map named &lt;code&gt;extra&lt;/code&gt; that can store any additional user data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;executionPlan&lt;/code&gt; and &lt;code&gt;executionEvent&lt;/code&gt; have additional map &lt;code&gt;labels&lt;/code&gt;. Labels are intended for identification and filtering on the server.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.postProcessingFilter=userExtraMeta&#xA;spline.postProcessingFilter.userExtraMeta.rules=file:///path/to/json-with-rules.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;json-with-rules.json could look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;executionPlan&#34;: {&#xA;        &#34;extra&#34;: {&#xA;            &#34;my-extra-1&#34;: 42,&#xA;            &#34;my-extra-2&#34;: [ &#34;aaa&#34;, &#34;bbb&#34;, &#34;ccc&#34; ]&#xA;        },&#xA;        &#34;labels&#34;: {&#xA;            &#34;my-label&#34;: &#34;my-value&#34;&#xA;        }&#xA;    },&#xA;    &#34;write&#34;: {&#xA;        &#34;extra&#34;: {&#xA;            &#34;foo&#34;: &#34;extra-value&#34;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;spline.postProcessingFilter.userExtraMeta.rules&lt;/code&gt; can be either url pointing to json file or a json string. The rules definition can be quite long and when providing string directly a lot of escaping may be necessary so using a file is recommended.&lt;/p&gt; &#xA;&lt;p&gt;Example of escaping the rules string in Scala String:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.config(&#34;spline.postProcessingFilter.userExtraMeta.rules&#34;, &#34;{\&#34;executionPlan\&#34;:{\&#34;extra\&#34;:{\&#34;qux\&#34;:42\\,\&#34;tags\&#34;:[\&#34;aaa\&#34;\\,\&#34;bbb\&#34;\\,\&#34;ccc\&#34;]}}}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;&lt;/code&gt; needs to be escaped because it would end the string&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;,&lt;/code&gt; needs to be escaped because when passing configuration via Java properties the comma is used as a separator under the hood and must be explicitly escaped.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example of escaping the rules string as VM option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-Dspline.postProcessingFilter.userExtraMeta.rules={\&#34;executionPlan\&#34;:{\&#34;extra\&#34;:{\&#34;qux\&#34;:42\,\&#34;tags\&#34;:[\&#34;aaa\&#34;\,\&#34;bbb\&#34;\,\&#34;ccc\&#34;]}}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A convenient way how to provide rules json without need for escaping may be to specify the property in yaml config file. An example of this can be seen in &lt;a href=&#34;https://github.com/AbsaOSS/spline-spark-agent/raw/develop/examples/src/main/resources/spline.yaml&#34;&gt;spline examples yaml config&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is also option to get environment variables using &lt;code&gt;$env&lt;/code&gt;, jvm properties using &lt;code&gt;$jvm&lt;/code&gt; and execute javascript using &lt;code&gt;$js&lt;/code&gt;. See the following example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;executionPlan&#34;: {&#xA;        &#34;extra&#34;: {&#xA;            &#34;my-extra-1&#34;: 42,&#xA;            &#34;my-extra-2&#34;: [ &#34;aaa&#34;, &#34;bbb&#34;, &#34;ccc&#34; ],&#xA;            &#34;bar&#34;: { &#34;$env&#34;: &#34;BAR_HOME&#34; },&#xA;            &#34;baz&#34;: { &#34;$jvm&#34;: &#34;some.jvm.prop&#34; },&#xA;            &#34;daz&#34;: { &#34;$js&#34;: &#34;session.conf().get(&#39;k&#39;)&#34; },&#xA;            &#34;appName&#34;: { &#34;$js&#34;:&#34;session.sparkContext().appName()&#34; }&#xA;       }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the javascript evaluation following variables are available by default:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;variable&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Scala Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;session&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;org.apache.spark.sql.SparkSession&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;logicalPlan&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;executedPlanOpt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Option[org.apache.spark.sql.execution.SparkPlan]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Using those objects it should be possible to extract almost any relevant information from Spark.&lt;/p&gt; &#xA;&lt;p&gt;The rules can be conditional, meaning the specified params will be added only when some condition is met. See the following example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;executionEvent[@.timestamp &amp;gt; 65]&#34;: {&#xA;        &#34;extra&#34;: { &#34;tux&#34;: 1 }&#xA;    },&#xA;    &#34;executionEvent[@.extra[&#39;foo&#39;] == &#39;a&#39; &amp;amp;&amp;amp; @.extra[&#39;bar&#39;] == &#39;x&#39;]&#34;: {&#xA;        &#34;extra&#34;: { &#34;bux&#34;: 2 }&#xA;    },&#xA;    &#34;executionEvent[@.extra[&#39;foo&#39;] == &#39;a&#39; &amp;amp;&amp;amp; !@.extra[&#39;bar&#39;]]&#34;: {&#xA;        &#34;extra&#34;: { &#34;dux&#34;: 3 }&#xA;    },&#xA;    &#34;executionEvent[@.extra[&#39;baz&#39;][2] &amp;gt;= 3]&#34;: {&#xA;        &#34;extra&#34;: { &#34;mux&#34;: 4 }&#xA;    },&#xA;    &#34;executionEvent[@.extra[&#39;baz&#39;][2] &amp;lt; 3]&#34;: {&#xA;        &#34;extra&#34;: { &#34;fux&#34;: 5 }&#xA;    },&#xA;    &#34;executionEvent[session.sparkContext.conf[&#39;spark.ui.enabled&#39;] == &#39;false&#39;]&#34;: {&#xA;      &#34;extra&#34;: { &#34;tux&#34;: 1 }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The condition is enclosed by &lt;code&gt;[]&lt;/code&gt; after entity name. Here the &lt;code&gt;@&lt;/code&gt; serves as a reference to currently processed entity, in this case executionEvent. The &lt;code&gt;[]&lt;/code&gt; inside the condition statement can also serve as a way to access maps and sequences. Logical and comparison operators are available.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;session&lt;/code&gt; and other variables available for js are available here as well.&lt;/p&gt; &#xA;&lt;p&gt;For more examples of usage please se &lt;code&gt;MetadataCollectingFilterSpec&lt;/code&gt; test class.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;spark-coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Spark features coverage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Dataset&lt;/em&gt; operations are fully supported&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;RDD&lt;/em&gt; transformations aren&#39;t supported due to Spark internal architecture specifics, but they might be supported semi-automatically in the future Spline versions (see #33)&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;SQL&lt;/em&gt; dialect is mostly supported.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;DDL&lt;/em&gt; operations are not supported, excepts for &lt;code&gt;CREATE TABLE ... AS SELECT ...&lt;/code&gt; which is supported.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: By default, the lineage is only captured on persistent (write) actions. To capture in-memory actions like &lt;code&gt;collect()&lt;/code&gt;, &lt;code&gt;show()&lt;/code&gt; etc the corresponding plugin needs to be activated by setting up the following configuration property:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;spline.plugins.za.co.absa.spline.harvester.plugin.embedded.NonPersistentActionsCapturePlugin.enabled=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(See &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/core/src/main/resources/spline.default.yaml#L230&#34;&gt;spline.default.yaml&lt;/a&gt; for more information)&lt;/p&gt; &#xA;&lt;p&gt;The following data formats and providers are supported out of the box:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Avro&lt;/li&gt; &#xA; &lt;li&gt;Cassandra&lt;/li&gt; &#xA; &lt;li&gt;COBOL&lt;/li&gt; &#xA; &lt;li&gt;Delta&lt;/li&gt; &#xA; &lt;li&gt;ElasticSearch&lt;/li&gt; &#xA; &lt;li&gt;Excel&lt;/li&gt; &#xA; &lt;li&gt;HDFS&lt;/li&gt; &#xA; &lt;li&gt;Hive&lt;/li&gt; &#xA; &lt;li&gt;JDBC&lt;/li&gt; &#xA; &lt;li&gt;Kafka&lt;/li&gt; &#xA; &lt;li&gt;MongoDB&lt;/li&gt; &#xA; &lt;li&gt;XML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Although Spark being an extensible piece of software can support much more, it doesn&#39;t provide any universal API that Spline can utilize to capture reads and write from/to everything that Spark supports. Support for most of different data sources and formats has to be added to Spline one by one. Fortunately starting with Spline 0.5.4 the auto discoverable &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/#plugins&#34;&gt;Plugin API&lt;/a&gt; has been introduced to make this process easier.&lt;/p&gt; &#xA;&lt;p&gt;Below is the break-down of the read/write command list that we have come through.&lt;br&gt; Some commands are implemented, others have yet to be implemented, and finally there are such that bear no lineage information and hence are ignored.&lt;/p&gt; &#xA;&lt;p&gt;All commands inherit from &lt;code&gt;org.apache.spark.sql.catalyst.plans.logical.Command&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can see how to produce unimplemented commands in &lt;code&gt;za.co.absa.spline.harvester.SparkUnimplementedCommandsSpec&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;spark-coverage-done&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Implemented&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateDataSourceTableAsSelectCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateHiveTableAsSelectCommand&lt;/code&gt; (org.apache.spark.sql.hive.execution)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DropTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;InsertIntoDataSourceDirCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;InsertIntoHadoopFsRelationCommand&lt;/code&gt; (org.apache.spark.sql.execution.datasources)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;InsertIntoHiveDirCommand&lt;/code&gt; (org.apache.spark.sql.hive.execution)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;InsertIntoHiveTable&lt;/code&gt; (org.apache.spark.sql.hive.execution)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SaveIntoDataSourceCommand&lt;/code&gt; (org.apache.spark.sql.execution.datasources)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a id=&#34;spark-coverage-todo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;To be implemented&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableAddColumnsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableChangeColumnCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableRenameCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableSetLocationCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateDataSourceTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateDatabaseCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateTableLikeCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DropDatabaseCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LoadDataCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TruncateTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When one of these commands occurs spline will let you know by logging a warning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;spark-coverage-ignored&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Ignored&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;AddFileCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AddJarCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterDatabasePropertiesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableAddPartitionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableDropPartitionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableRecoverPartitionsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableRenamePartitionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableSerDePropertiesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableSetPropertiesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterTableUnsetPropertiesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AlterViewAsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AnalyzeColumnCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AnalyzePartitionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AnalyzeTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CacheTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ClearCacheCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateFunctionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateTempViewUsing&lt;/code&gt; (org.apache.spark.sql.execution.datasources)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CreateViewCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DescribeColumnCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DescribeDatabaseCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DescribeFunctionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DescribeTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DropFunctionCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ExplainCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;InsertIntoDataSourceCommand&lt;/code&gt; (org.apache.spark.sql.execution.datasources) *&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ListFilesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ListJarsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RefreshResource&lt;/code&gt; (org.apache.spark.sql.execution.datasources)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RefreshTable&lt;/code&gt; (org.apache.spark.sql.execution.datasources)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ResetCommand$&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SetCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SetDatabaseCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowColumnsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowCreateTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowDatabasesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowFunctionsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowPartitionsCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowTablePropertiesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ShowTablesCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;StreamingExplainCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;UncacheTableCommand&lt;/code&gt; (org.apache.spark.sql.execution.command)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a id=&#34;dev-doc&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Developer documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;plugins&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Plugin API&lt;/h3&gt; &#xA;&lt;p&gt;Using a plugin API you can capture lineage from a 3rd party data source provider. Spline discover plugins automatically by scanning a classpath, so no special steps required to register and configure a plugin. All you need is to create a class extending the &lt;code&gt;za.co.absa.spline.harvester.plugin.Plugin&lt;/code&gt; marker trait mixed with one or more &lt;code&gt;*Processing&lt;/code&gt; traits, depending on your intention.&lt;/p&gt; &#xA;&lt;p&gt;There are three general processing traits:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DataSourceFormatNameResolving&lt;/code&gt; - returns a name of a data provider/format in use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ReadNodeProcessing&lt;/code&gt; - detects a read-command and gather meta information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WriteNodeProcessing&lt;/code&gt; - detects a write-command and gather meta information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are also two additional trait that handle common cases of reading and writing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;BaseRelationProcessing&lt;/code&gt; - similar to &lt;code&gt;ReadNodeProcessing&lt;/code&gt;, but instead of capturing all logical plan nodes it only reacts on &lt;code&gt;LogicalRelation&lt;/code&gt; (see &lt;code&gt;LogicalRelationPlugin&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RelationProviderProcessing&lt;/code&gt; - similar to &lt;code&gt;WriteNodeProcessing&lt;/code&gt;, but it only captures &lt;code&gt;SaveIntoDataSourceCommand&lt;/code&gt; (see &lt;code&gt;SaveIntoDataSourceCommandPlugin&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The best way to illustrate how plugins work is to look at the real working example, e.g. &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/core/src/main/scala/za/co/absa/spline/harvester/plugin/embedded/JDBCPlugin.scala&#34;&gt;&lt;code&gt;za.co.absa.spline.harvester.plugin.embedded.JDBCPlugin&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The most common simplified pattern looks like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package my.spline.plugin&#xA;&#xA;import javax.annotation.Priority&#xA;import za.co.absa.spline.harvester.builder._&#xA;import za.co.absa.spline.harvester.plugin.Plugin._&#xA;import za.co.absa.spline.harvester.plugin._&#xA;&#xA;@Priority(Precedence.User) // not required, but can be used to control your plugin precedence in the plugin chain. Default value is `User`.  &#xA;class FooBarPlugin&#xA;  extends Plugin&#xA;    with BaseRelationProcessing&#xA;    with RelationProviderProcessing {&#xA;&#xA;  override def baseRelationProcessor: PartialFunction[(BaseRelation, LogicalRelation), ReadNodeInfo] = {&#xA;    case (FooBarRelation(a, b, c, d), lr) if /*more conditions*/ =&amp;gt;&#xA;      val dataFormat: Option[AnyRef] = ??? // data format being read (will be resolved by the `DataSourceFormatResolver` later)&#xA;      val dataSourceURI: String = ??? // a unique URI for the data source&#xA;      val params: Map[String, Any] = ??? // additional parameters characterizing the read-command. E.g. (connection protocol, access mode, driver options etc)&#xA;&#xA;      (SourceIdentifier(dataFormat, dataSourceURI), params)&#xA;  }&#xA;&#xA;  override def relationProviderProcessor: PartialFunction[(AnyRef, SaveIntoDataSourceCommand), WriteNodeInfo] = {&#xA;    case (provider, cmd) if provider == &#34;foobar&#34; || provider.isInstanceOf[FooBarProvider] =&amp;gt;&#xA;      val dataFormat: Option[AnyRef] = ??? // data format being written (will be resolved by the `DataSourceFormatResolver` later)&#xA;      val dataSourceURI: String = ??? // a unique URI for the data source&#xA;      val writeMode: SaveMode = ??? // was it Append or Overwrite?&#xA;      val query: LogicalPlan = ??? // the logical plan to get the rest of the lineage from&#xA;      val params: Map[String, Any] = ??? // additional parameters characterizing the write-command&#xA;&#xA;      (SourceIdentifier(dataFormat, dataSourceURI), writeMode, query, params)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: to avoid unwanted possible shadowing the other plugins (including the future ones), make sure that the pattern-matching criteria are as much selective as possible for your plugin needs.&lt;/p&gt; &#xA;&lt;p&gt;A plugin class is expected to only have a single constructor. The constructor can have no arguments, or one or more of the following types (the values will be autowired):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;SparkSession&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PathQualifier&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PluginRegistry&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Compile you plugin and drop it into the Spline/Spark classpath. Spline will pick it up automatically.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;building&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Building for different Scala and Spark versions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The project requires Java version 1.8 (strictly) and &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt; for building.&lt;/p&gt; &#xA;&lt;p&gt;Check the build environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mvn --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Verify that Maven is configured to run on Java 1.8. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Apache Maven 3.6.3 (Red Hat 3.6.3-8)&#xA;Maven home: /usr/share/maven&#xA;Java version: 1.8.0_302, vendor: Red Hat, Inc., runtime: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-2.fc34.x86_64/jre&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are several maven profiles that makes it easy to build the project with different versions of Spark and Scala.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scala profiles: &lt;code&gt;scala-2.11&lt;/code&gt;, &lt;code&gt;scala-2.12&lt;/code&gt; (default)&lt;/li&gt; &#xA; &lt;li&gt;Spark profiles: &lt;code&gt;spark-2.2&lt;/code&gt;, &lt;code&gt;spark-2.3&lt;/code&gt;, &lt;code&gt;spark-2.4&lt;/code&gt; (default), &lt;code&gt;spark-3.0&lt;/code&gt;, &lt;code&gt;spark-3.1&lt;/code&gt;, &lt;code&gt;spark-3.2&lt;/code&gt;, &lt;code&gt;spark-3.3&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, to build an agent for Spark 2.4 and Scala 2.11:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Change Scala version in pom.xml.&#xA;mvn scala-cross-build:change-version -Pscala-2.11&#xA;&#xA;# now you can build for Scala 2.11&#xA;mvn clean install -Pscala-2.11,spark-2.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build docker image&lt;/h3&gt; &#xA;&lt;p&gt;The agent docker image is mainly used to run &lt;a href=&#34;https://raw.githubusercontent.com/AbsaOSS/spline-spark-agent/develop/examples/&#34;&gt;example jobs&lt;/a&gt; and pre-fill the database with the sample lineage data.&lt;/p&gt; &#xA;&lt;p&gt;(Spline docker images are available on the DockerHub repo - &lt;a href=&#34;https://hub.docker.com/u/absaoss&#34;&gt;https://hub.docker.com/u/absaoss&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mvn install -Ddocker -Ddockerfile.repositoryUrl=my&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/AbsaOSS/spline-getting-started/raw/main/building-docker.md&#34;&gt;How to build Spline Docker images&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;references&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to measure code coverage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./mvn verify -Dcode-coverage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If module contains measurable data the code coverage report will be generated on path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{local-path}\spline-spark-agent\{module}\target\site\jacoco&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References and examples&lt;/h2&gt; &#xA;&lt;p&gt;Although the primary goal of Spline agent is to be used in combination with the &lt;a href=&#34;https://github.com/AbsaOSS/spline&#34;&gt;Spline server&lt;/a&gt;, it is flexible enough to be used in isolation or integration with other data lineage tracking solutions including custom ones.&lt;/p&gt; &#xA;&lt;p&gt;Below is a couple of examples of such integration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://intellishore.dk/data-lineage-from-databricks-to-azure-purview/&#34;&gt;Databricks Lineage In Azure Purview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://firststr.com/2021/04/26/spark-compute-lineage-to-datahub&#34;&gt;Spark Compute Lineage to Datahub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2019 ABSA Group Limited&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;    http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>