<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-17T01:44:53Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>databricks/sbt-databricks</title>
    <updated>2023-05-17T01:44:53Z</updated>
    <id>tag:github.com,2023-05-17:/databricks/sbt-databricks</id>
    <link href="https://github.com/databricks/sbt-databricks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An sbt plugin for deploying code to Databricks Cloud&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;sbt-databricks &lt;a href=&#34;http://travis-ci.org/databricks/sbt-databricks&#34;&gt;&lt;img src=&#34;https://travis-ci.org/databricks/sbt-databricks.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;sbt plugin to deploy your projects to Databricks!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://go.databricks.com/register-for-dbc&#34;&gt;http://go.databricks.com/register-for-dbc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;An Account on Databricks: &lt;a href=&#34;https://accounts.cloud.databricks.com/registration.html#signup&#34;&gt;Sign up for a free trial.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Just add the following line to &lt;code&gt;project/plugins.sbt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;addSbtPlugin(&#34;com.databricks&#34; %% &#34;sbt-databricks&#34; % &#34;0.1.5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are running Databricks version 2.18 or greater you must use sbt-databricks version 0.1.5&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are running Databricks version 2.8 or greater you must use sbt-databricks version 0.1.3&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Enable sbt-databricks for all your projects&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;sbt-databricks&lt;/code&gt; can be enabled as a &lt;a href=&#34;http://www.scala-sbt.org/0.13/tutorial/Using-Plugins.html#Global+plugins&#34;&gt;global plugin&lt;/a&gt; for use in all of your projects in two easy steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the following line to &lt;code&gt;~/.sbt/0.13/plugins/build.sbt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;addSbtPlugin(&#34;com.databricks&#34; %% &#34;sbt-databricks&#34; % &#34;0.1.5&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the settings defined &lt;a href=&#34;https://raw.githubusercontent.com/databricks/sbt-databricks/master/#settings&#34;&gt;here&lt;/a&gt; in &lt;code&gt;~/.sbt/0.13/databricks.sbt&lt;/code&gt;. You&#39;ll have to add the line&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import sbtdatabricks.DatabricksPlugin.autoImport._&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to that file in order to import this plugin&#39;s settings into that configuration file.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h3&gt;Cluster Controls&lt;/h3&gt; &#xA;&lt;p&gt;There are three primary cluster related actions: Create, Resize and Delete.&lt;/p&gt; &#xA;&lt;p&gt;Creating a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcCreateCluster // Attempts to create a cluster on DBC&#xA;// The following parameters must be set when attempting to create a cluster&#xA;dbcNumWorkerContainers := // Integer: The desired size of the cluster (in worker containers). &#xA;dbcSpotInstance := // Boolean for choosing whether to use Spot or On-Demand instances&#xA;dbcSparkVersion := // String: The Spark version to be used e.g. &#34;1.6.x&#34;&#xA;dbcZoneId := // String: AWS zone e.g. ap-southeast-2&#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Resizing a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcResizeCluster // Attempts to resize a cluster on DBC&#xA;// The following parameters must be set when attempting to resize a cluster&#xA;dbcNumWorkerContainers := // Integer: The desired size of the cluster (in worker containers). &#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Deleting a cluster&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcDeleteCluster // Attempts to delete a cluster on DBC&#xA;// The following parameters must be set when attempting to resize a cluster&#xA;dbcClusters := // See notes below regarding this parameter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;There are four major commands that can be used. Please check the next section for mandatory settings before running these commands.:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcDeploy&lt;/code&gt;: Uploads your Library to Databricks Cloud, attaches it to specified clusters, and restarts the clusters if a previous version of the library was attached. This method encapsulates the following commands. Only libraries with &lt;code&gt;SNAPSHOT&lt;/code&gt; versions will be deleted and re-uploaded as it is assumed that dependencies will not change very frequently. If you change the version of one of your dependencies, that dependency must be deleted manually in Databricks Cloud to prevent unexpected behavior.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcUpload&lt;/code&gt;: Uploads your Library to Databricks Cloud. Deletes the older version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcAttach&lt;/code&gt;: Attaches your Library to the specified clusters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcRestartClusters&lt;/code&gt;: Restarts the specified clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Command Execution&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;`dbcExecuteCommand` // Runs a command on a specified DBC Cluster&#xA;// The context/command language that will be employed when dbcExecuteCommand is called&#xA;dbcExecutionLanguage := // One of DBCScala, DBCPython, DBCSQL&#xA;// The file containing the code that is to be processed on the DBC cluster&#xA;dbcCommandFile := // Type File&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example, using just an sbt invocation is below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt&#xA;&amp;gt; set dbcClusters := Seq(&#34;CLUSTER_NAME&#34;)&#xA;&amp;gt; set dbcCommandFile := new File(&#34;/Path/to/file.py&#34;)&#xA;&amp;gt; set dbcExecutionLanguage := DBCPython&#xA;&amp;gt; dbcExecuteCommand&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Other&lt;/h3&gt; &#xA;&lt;p&gt;Other helpful commands are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dbcListClusters&lt;/code&gt;: View the states of available clusters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a name=&#34;settings&#34;&gt;Settings&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;There are a few configuration settings that need to be made in the build file. Please set the following parameters according to your setup:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Your username to login to Databricks Cloud&#xA;dbcUsername := // e.g. &#34;admin&#34;&#xA;&#xA;// Your password (Can be set as an environment variable)&#xA;dbcPassword := // e.g. &#34;admin&#34; or System.getenv(&#34;DBCLOUD_PASSWORD&#34;)&#xA;&#xA;// The URL to the Databricks Cloud DB Api.!&#xA;// Note: this plugin currently does not support the /api/2.0 endpoint, so values using that&#xA;// endpoint will be automatically rewritten to use /api/1.2.&#xA;dbcApiUrl := // https://organization.cloud.databricks.com/api/1.2&#xA;&#xA;// Add any clusters that you would like to deploy your work to. e.g. &#34;My Cluster&#34;&#xA;// or run dbcExecuteCommand&#xA;dbcClusters += // Add &#34;ALL_CLUSTERS&#34; if you want to attach your work to all clusters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When using dbcDeploy, if you wish to upload an assembly jar instead of every library by itself, you may override dbcClasspath as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath := Seq(assembly.value)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other optional parameters are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;// The location to upload your libraries to in the workspace e.g. &#34;/Users/alice&#34;&#xA;dbcLibraryPath := // Default is &#34;/&#34;&#xA;&#xA;// Whether to restart the clusters every time a new version is uploaded to Databricks Cloud&#xA;dbcRestartOnAttach := // Default true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SBT Tips and Tricks (FAQ)&lt;/h3&gt; &#xA;&lt;p&gt;Here are some SBT tips and tricks to improve your experience with sbt-databricks.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;I have a multi-project build. I don&#39;t want to upload the entire project to Databricks Cloud. What should I do?&lt;/p&gt; &lt;p&gt;In a multi-project build, you may run an SBT task (such as dbcDeploy, dbcUpload, etc...) just for that project by &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Tasks.html#Task+Scope&#34;&gt;&lt;em&gt;scoping&lt;/em&gt;&lt;/a&gt; the task. You may &lt;em&gt;scope&lt;/em&gt; the task by using the project id before that task.&lt;/p&gt; &lt;p&gt;For example, assume we have a project with sub-projects &lt;code&gt;core&lt;/code&gt;, &lt;code&gt;ml&lt;/code&gt;, and &lt;code&gt;sql&lt;/code&gt;. Assume &lt;code&gt;ml&lt;/code&gt; depends on &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;sql&lt;/code&gt;, &lt;code&gt;sql&lt;/code&gt; only depends on &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;core&lt;/code&gt; doesn&#39;t depend on anything. Here is what would happen for the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&amp;gt; dbcUpload          // Uploads core, ml, and sql&#xA;&amp;gt; core/dbcUpload     // Uploads only core&#xA;&amp;gt; sql/dbcUpload      // Uploads core and sql&#xA;&amp;gt; ml/dbcUpload       // Uploads core, ml, and sql&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I want to pass parameters to &lt;code&gt;dbcDeploy&lt;/code&gt;. For example, in my build file &lt;code&gt;dbcClusters&lt;/code&gt; is set as &lt;code&gt;clusterA&lt;/code&gt; but I want to deploy to &lt;code&gt;clusterB&lt;/code&gt; once in a while. What should I do?&lt;/p&gt; &lt;p&gt;In the SBT console, one way of overriding settings for your session is by using the &lt;code&gt;set&lt;/code&gt; command. Using the example above.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;&amp;gt; core/dbcDeploy   // Deploys core to clusterA (clusterA was set inside the build file)&#xA;&amp;gt; set dbcClusters := Seq(&#34;clusterB&#34;)  // change cluster to clusterB&#xA;&amp;gt; ml/dbcDeploy     // Deploys core, sql, and ml to clusterB&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I want to upload an assembly jar rather than tens of individual jars. How can I do that?&lt;/p&gt; &lt;p&gt;You may override &lt;code&gt;dbcClasspath&lt;/code&gt; such as:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath := Seq(assembly.value)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... in your build file, (or using set on the console) in order to upload a single fat jar instead of many individual ones. Beware of dependency conflicts!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Hey, I followed #3, but I&#39;m still uploading &lt;code&gt;core&lt;/code&gt;, and &lt;code&gt;sql&lt;/code&gt; individually after&lt;code&gt;sql/dbcUpload&lt;/code&gt;. What&#39;s going on!?&lt;/p&gt; &lt;p&gt;Remember scoping tasks? You will need to scope both &lt;code&gt;dbcClasspath&lt;/code&gt; and &lt;code&gt;assembly&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;dbcClasspath in sql := Seq((assembly in sql).value)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then &lt;code&gt;sql/dbcUpload&lt;/code&gt; should upload an assembly jar of &lt;code&gt;core&lt;/code&gt; and &lt;code&gt;sql&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Tests&lt;/h1&gt; &#xA;&lt;p&gt;Run tests using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dev/run-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the very last line starts with &lt;code&gt;[success]&lt;/code&gt;, then that means that the tests have passed.&lt;/p&gt; &#xA;&lt;p&gt;Run scalastyle checks using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dev/lint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you encounter bugs or want to contribute, feel free to submit an issue or pull request.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks/spark-integration-tests</title>
    <updated>2023-05-17T01:44:53Z</updated>
    <id>tag:github.com,2023-05-17:/databricks/spark-integration-tests</id>
    <link href="https://github.com/databricks/spark-integration-tests" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Integration tests for Spark&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Integration Tests&lt;/h1&gt; &#xA;&lt;p&gt;This project contains &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt;-based integration tests for Spark, including fault-tolerance tests for Spark&#39;s standalone cluster manager.&lt;/p&gt; &#xA;&lt;h2&gt;Installation / Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install Docker&lt;/h3&gt; &#xA;&lt;p&gt;This project depends on Docker &amp;gt;= 1.3.0 (it may work with earlier versions, but this hasn&#39;t been tested).&lt;/p&gt; &#xA;&lt;h4&gt;On Linux&lt;/h4&gt; &#xA;&lt;p&gt;Install Docker. This test suite requires that Docker can run without &lt;code&gt;sudo&lt;/code&gt; (see &lt;a href=&#34;http://docs.docker.io/en/latest/use/basics/&#34;&gt;http://docs.docker.io/en/latest/use/basics/&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;On OSX&lt;/h4&gt; &#xA;&lt;p&gt;On OSX, these integration tests can be run using &lt;a href=&#34;https://github.com/boot2docker/boot2docker&#34;&gt;boot2docker&lt;/a&gt;. First, &lt;a href=&#34;https://github.com/boot2docker/osx-installer/releases/tag/v1.3.2&#34;&gt;download &lt;code&gt;boot2docker&lt;/code&gt;&lt;/a&gt;, run the installer, then run &lt;code&gt;~/Applications/boot2docker&lt;/code&gt; to perform some one-time setup (create the VM, etc.). This project has been tested with &lt;code&gt;boot2docker&lt;/code&gt; 1.3.0+.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;boot2docker&lt;/code&gt;, the Docker containers will be run inside of a VirtualBox VM, which creates some difficulties for communication between the Mac host and the containers. Follow these instructions to work around those issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Network access&lt;/strong&gt;: Our tests currently run the SparkContext from outside of the containers, so we need both host &amp;lt;-&amp;gt; container and container &amp;lt;-&amp;gt; container networking to work properly. This is complicated by the fact that &lt;code&gt;boot2docker&lt;/code&gt; runs the containers behind a NAT in VirtualBox.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/boot2docker/boot2docker/issues/528&#34;&gt;One workaround&lt;/a&gt; is to add a routing table entry that routes traffic to containers to the VirtualBox VM&#39;s IP address:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo route -n add 172.17.0.0/16 `boot2docker ip`    &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You&#39;ll have to re-run this command if you restart your computer or assign a new IP to the VirtualBox VM.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install Docker images&lt;/h3&gt; &#xA;&lt;p&gt;The integration tests depend on several Docker images. To set them up, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./docker/build.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to build our custom Docker images and download other images from the Docker repositories. This needs to download a fair amount of stuff, so make sure that you&#39;re on a fast internet connection (or be prepared to wait a while).&lt;/p&gt; &#xA;&lt;h3&gt;Configure your environment&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;: Running &lt;code&gt;./init.sh&lt;/code&gt; will perform environment sanity checking and tell you which shell exports to perform.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;code&gt;SPARK_HOME&lt;/code&gt; environment variable should to a Spark source checkout where an assembly has been built. This directory will be shared with Docker containers; Spark workers and masters will use this &lt;code&gt;SPARK_HOME/work&lt;/code&gt; as their work directory. This effectively treats host machine&#39;s &lt;code&gt;SPARK_HOME&lt;/code&gt; directory as a directory on a network-mounted filesystem.&lt;/p&gt; &lt;p&gt;Additionally, this Spark sbt project will added as a dependency of this sbt project, so the integration test code will be compiled against that Spark version.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test-specific requirements&lt;/h3&gt; &#xA;&lt;h4&gt;Mesos&lt;/h4&gt; &#xA;&lt;p&gt;The Mesos integration tests require &lt;code&gt;MESOS_NATIVE_LIBRARY&lt;/code&gt; to be set. For Mac users, the easiest way to install Mesos is through Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install mesos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export MESOS_NATIVE_LIBRARY=$(brew --repository)/lib/libmesos.dylib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark on Mesos requires a Spark binary distribution &lt;code&gt;.tgz&lt;/code&gt; file. To build this, run &lt;code&gt;./make-distribution.sh --tgz&lt;/code&gt; in your Spark checkout.&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;These integration tests are implemented as ScalaTest suites and can be run through sbt. Note that you will probably need to give sbt extra memory; with newer versions of the sbt launcher script, this can be done with the &lt;code&gt;-mem&lt;/code&gt; option, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt -mem 2048 test:package &#34;test-only org.apache.spark.integrationtests.MesosSuite&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Although our Docker-based test suites attempt to clean up the containers that they create, this cleanup may not be performed if the test runner&#39;s JVM exits abruptly. To kill &lt;strong&gt;all&lt;/strong&gt; Docker containers (including ones that may not have been launched by our tests), you can run &lt;code&gt;docker kill $(docker ps -q)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License. See LICENSE for full license text.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jrudolph/bit-extractors</title>
    <updated>2023-05-17T01:44:53Z</updated>
    <id>tag:github.com,2023-05-17:/jrudolph/bit-extractors</id>
    <link href="https://github.com/jrudolph/bit-extractors" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scala extractors for pattern matching values from bits&lt;/p&gt;&lt;hr&gt;&lt;p&gt;A library to provide extractors for bitwise matching and extracting values from bit streams.&lt;/p&gt;</summary>
  </entry>
</feed>