<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-26T01:54:24Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pulse-club/gta-brasil</title>
    <updated>2022-06-26T01:54:24Z</updated>
    <id>tag:github.com,2022-06-26:/pulse-club/gta-brasil</id>
    <link href="https://github.com/pulse-club/gta-brasil" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üáßüá∑ Convers√£o Total para GTA San Andreas com foco numa est√©tica verdadeiramente brasileira.&lt;/p&gt;&lt;hr&gt;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/106138998/173198102-d3df4752-456f-40b1-ac1f-0ba212f6e848.png&#34; width=&#34;400&#34; alt=&#34;Logo&#34;&gt;&lt;br&gt; GTA Brasil üáßüá∑ &lt;/h3&gt; &#xA;&lt;h6 align=&#34;center&#34;&gt; &lt;a href=&#34;https://pulseclub.net/resources/gta-brasil.30/&#34;&gt;T√≥pico Oficial&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/raw/main/MANUAL.md&#34;&gt;Manual&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/raw/main/CREDITOS.md&#34;&gt;Cr√©ditos&lt;/a&gt; &lt;/h6&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/stargazers&#34;&gt;&lt;img alt=&#34;GitHub stars&#34; src=&#34;https://img.shields.io/github/stars/Pulse-Club/GTA-Brasil?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=C9CBFF&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt;&lt;/a&gt;  &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/releases/latest&#34;&gt;&lt;img alt=&#34;GitHub release (latest by date)&#34; src=&#34;https://img.shields.io/github/v/release/Pulse-Club/GTA-Brasil?style=for-the-badge&amp;amp;logo=github&amp;amp;color=F2CDCD&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/issues&#34;&gt;&lt;img alt=&#34;GitHub issues&#34; src=&#34;https://img.shields.io/github/issues/Pulse-Club/GTA-Brasil?style=for-the-badge&amp;amp;logo=gitbook&amp;amp;color=B5E8E0&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/Q4z9wGAShj&#34;&gt;&lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/978383675756515368?style=for-the-badge&amp;amp;logo=discord&amp;amp;color=DDB6F2&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;GTA Brasil&lt;/b&gt; √© um projeto de convers√£o total criado para o GTA San Andreas com o objetivo de trazer uma est√©tica verdadeiramente brasileira de forma que mantenha a ess√™ncia lowpoly do jogo original. O mod inclu√≠ diversos props, texturas do mapa traduzidas para o portugu√™s, novos pedestres, armas, ve√≠culos, estruturas, localiza√ß√µes e scripts para tornar a experi√™ncia o mais imersiva o poss√≠vel. &lt;/p&gt; &#xA;&lt;h3&gt;üß† Filosofia&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Manter a ess√™ncia&lt;/strong&gt;: o jogo original usa modelos &lt;em&gt;low-poly&lt;/em&gt; por limita√ß√µes da √©poca, nossa ideia √© seguir este padr√£o em todos os modelos adicionados ao mapa.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fazer do jeito certo&lt;/strong&gt;: sem gambiarras! Foi-se o tempo em que mods para GTA eram feitos de qualquer maneira, buscamos sempre fazer da melhor forma poss√≠vel para evitar inconsist√™ncias visuais e de gameplay.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Gr√°tis&lt;/strong&gt;: o projeto √© e sempre ser√° gr√°tis para baixar, e a partir de agora, qualquer um pode contribuir para o projeto! Basta criar um &lt;em&gt;fork&lt;/em&gt;, fazer as altera√ß√µes que deseja e enviar um &lt;em&gt;pull request&lt;/em&gt;, se sua mudan√ßa for aceita, voc√™ ser√° creditado no projeto!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üì¶ Conte√∫do&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;GTA Brasil&lt;/strong&gt; se consiste em v√°rias mudan√ßas no jogo base, incluindo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Novas texturas para o mapa, com fachadas de lojas traduzidas e v√°rios elementos brasileiros.&lt;/li&gt; &#xA; &lt;li&gt;Objetos espalhados pelo mapa, como lombadas, placas, orelh√µes etc.&lt;/li&gt; &#xA; &lt;li&gt;Novos estabelecimentos e casas brasileiras, remetendo a marcas do mundo real para maior imers√£o.&lt;/li&gt; &#xA; &lt;li&gt;Nova Interface com telas de carregamento desenhadas a m√£o.&lt;/li&gt; &#xA; &lt;li&gt;Novas mini miss√µes, como entregador e motoboy.&lt;/li&gt; &#xA; &lt;li&gt;Personagem principal pr√≥prio com diversas op√ß√µes de customiza√ß√£o e roupas.&lt;/li&gt; &#xA; &lt;li&gt;Novos ve√≠culos substituindo a maior parte do tr√¢nsito do jogo.&lt;/li&gt; &#xA; &lt;li&gt;Mitos, lendas e easter eggs para encontrar.&lt;/li&gt; &#xA; &lt;li&gt;Novos pedestres.&lt;/li&gt; &#xA; &lt;li&gt;V√°rios &lt;a href=&#34;https://tcgtabrasil.wixsite.com/tcgtabrasil/downloads&#34;&gt;opcionais&lt;/a&gt; para incrementar sua experi√™ncia (sons ambiente e r√°dio, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üìã Regras de Uso&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;GTA Brasil&lt;/strong&gt; est√° sob a licen√ßa &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/raw/main/LICENSE&#34;&gt;GPL-3.0&lt;/a&gt;, voc√™ tem direito de usar e modificar os arquivos para uso pessoal/distribui√ß√£o contanto que voc√™ n√£o venda o conte√∫do ou remova cr√©ditos. &lt;strong&gt;√â estritamente proibido fazer reupload do projeto em sites que monetizam o download como o sharemods.&lt;/strong&gt; N√£o √© proibido fazer reupload em outros casos, apesar de pedirmos gentilmente para que voc√™ linke o download para a aba Releases deste reposit√≥rio. Caso voc√™ queira fazer uma modifica√ß√£o que julgue ben√©fica ao projeto, o convidamos a fazer um fork do reposit√≥rio e utilizar a fun√ß√£o de pull request. Para mais informa√ß√µes, veja como ajudar o projeto abaixo.&lt;/p&gt; &#xA;&lt;h3&gt;üëê Contribuir&lt;/h3&gt; &#xA;&lt;p&gt;Veja &lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/raw/main/CONTRIBUIR.md&#34;&gt;CONTRIBUIR.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Copyright ¬© 2022-presente &lt;a href=&#34;https://pulseclub.net/&#34; target=&#34;_blank&#34;&gt;Pulse Club&lt;/a&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Pulse-Club/GTA-Brasil/raw/main/LICENSE&#34;&gt;&lt;img alt=&#34;GitHub license&#34; src=&#34;https://img.shields.io/github/license/Pulse-Club/GTA-Brasil?style=for-the-badge&#34;&gt;&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>spark-jobserver/spark-jobserver</title>
    <updated>2022-06-26T01:54:24Z</updated>
    <id>tag:github.com,2022-06-26:/spark-jobserver/spark-jobserver</id>
    <link href="https://github.com/spark-jobserver/spark-jobserver" rel="alternate"></link>
    <summary type="html">&lt;p&gt;REST job server for Apache Spark&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://travis-ci.org/spark-jobserver/spark-jobserver&#34;&gt;&lt;img src=&#34;https://travis-ci.org/spark-jobserver/spark-jobserver.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/spark-jobserver/spark-jobserver/branch/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/spark-jobserver/spark-jobserver/master.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/spark-jobserver/spark-jobserver?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/spark-jobserver/spark-jobserver&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;spark-jobserver provides a RESTful interface for submitting and managing &lt;a href=&#34;http://spark-project.org&#34;&gt;Apache Spark&lt;/a&gt; jobs, jars, and job contexts. This repo contains the complete Spark job server project, including unit tests and deploy scripts. It was originally started at &lt;a href=&#34;http://www.ooyala.com&#34;&gt;Ooyala&lt;/a&gt;, but this is now the main development repo.&lt;/p&gt; &#xA;&lt;p&gt;Other useful links: &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/troubleshooting.md&#34;&gt;Troubleshooting&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/cluster.md&#34;&gt;cluster&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/yarn.md&#34;&gt;YARN client&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/EMR.md&#34;&gt;YARN on EMR&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/mesos.md&#34;&gt;Mesos&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/jmx.md&#34;&gt;JMX tips&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also see &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/chinese/job-server.md&#34;&gt;Chinese docs / ‰∏≠Êñá&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt; &lt;em&gt;generated with &lt;a href=&#34;https://github.com/thlorenz/doctoc&#34;&gt;DocToc&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#users&#34;&gt;Users&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#version-information&#34;&gt;Version Information&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#getting-started-with-spark-job-server&#34;&gt;Getting Started with Spark Job Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#development-mode&#34;&gt;Development mode&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#wordcountexample-walk-through&#34;&gt;WordCountExample walk-through&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#package-jar---send-to-server&#34;&gt;Package Jar - Send to Server&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#ad-hoc-mode---single-unrelated-jobs-transient-context&#34;&gt;Ad-hoc Mode - Single, Unrelated Jobs (Transient Context)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#persistent-context-mode---faster--required-for-related-jobs&#34;&gt;Persistent Context Mode - Faster &amp;amp; Required for Related Jobs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#debug-mode&#34;&gt;Debug mode&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#create-a-job-server-project&#34;&gt;Create a Job Server Project&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#creating-a-project-from-scratch-using-giter8-template&#34;&gt;Creating a project from scratch using giter8 template&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#creating-a-project-manually-assuming-that-you-already-have-sbt-project-structure&#34;&gt;Creating a project manually assuming that you already have sbt project structure&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#new-sparkjob-api&#34;&gt;NEW SparkJob API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#new-sparkjob-api-with-spark-v21&#34;&gt;NEW SparkJob API with Spark v2.1&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#dependency-jars&#34;&gt;Dependency jars&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#named-objects&#34;&gt;Named Objects&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#using-named-rdds&#34;&gt;Using Named RDDs&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#using-named-objects&#34;&gt;Using Named Objects&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#https--ssl-configuration&#34;&gt;HTTPS / SSL Configuration&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#server-authentication&#34;&gt;Server authentication&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#client-authentication&#34;&gt;Client authentication&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#access-control&#34;&gt;Access Control&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#shiro-authentication&#34;&gt;Shiro Authentication&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#keycloak-authentication&#34;&gt;Keycloak Authentication&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#user-authorization&#34;&gt;User Authorization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#deployment&#34;&gt;Deployment&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#manual-steps&#34;&gt;Manual steps&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#context-per-jvm&#34;&gt;Context per JVM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#configuring-spark-jobserver-backend&#34;&gt;Configuring Spark Jobserver backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#ha-deployment-beta&#34;&gt;HA Deployment (beta)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#chef&#34;&gt;Chef&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#api&#34;&gt;API&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#binaries&#34;&gt;Binaries&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#contexts&#34;&gt;Contexts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#jobs&#34;&gt;Jobs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#data&#34;&gt;Data&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#data-api-example&#34;&gt;Data API Example&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#context-configuration&#34;&gt;Context configuration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#other-configuration-settings&#34;&gt;Other configuration settings&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#job-result-serialization&#34;&gt;Job Result Serialization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#http-override&#34;&gt;HTTP Override&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#clients&#34;&gt;Clients&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#contribution-and-development&#34;&gt;Contribution and Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#contact&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;h2&gt;Users&lt;/h2&gt; &#xA;&lt;p&gt;(Please add yourself to this list!)&lt;/p&gt; &#xA;&lt;p&gt;Spark Job Server is included in Datastax Enterprise!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.ooyala.com&#34;&gt;Ooyala&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.netflix.com&#34;&gt;Netflix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.avenida.com&#34;&gt;Avenida.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GumGum&lt;/li&gt; &#xA; &lt;li&gt;Fuse Elements&lt;/li&gt; &#xA; &lt;li&gt;Frontline Solvers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.arubanetworks.com/&#34;&gt;Aruba Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.zed.com&#34;&gt;Zed Worldwide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.knime.org/&#34;&gt;KNIME&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://azavea.com&#34;&gt;Azavea&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://maana.io/&#34;&gt;Maana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.newsweaver.com&#34;&gt;Newsweaver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.instaclustr.com&#34;&gt;Instaclustr&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.snappydata.io&#34;&gt;SnappyData&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.linkfluence.com&#34;&gt;Linkfluence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.smartsct.com&#34;&gt;Smartsct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.datadoghq.com/&#34;&gt;Datadog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.planalytics.com&#34;&gt;Planalytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.target.com/&#34;&gt;Target&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://branch.io&#34;&gt;Branch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.informatica.com/&#34;&gt;Informatica&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cadenz.ai/&#34;&gt;Cadenz.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&#34;Spark as a Service&#34;&lt;/em&gt;: Simple REST interface (including HTTPS) for all aspects of job, context management&lt;/li&gt; &#xA; &lt;li&gt;Support for Spark SQL, Hive, Streaming Contexts/jobs and custom job contexts! See &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/contexts.md&#34;&gt;Contexts&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/python.md&#34;&gt;Python&lt;/a&gt;, Scala, and &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/javaapi.md&#34;&gt;Java&lt;/a&gt; (see &lt;a href=&#34;https://github.com/spark-jobserver/spark-jobserver/raw/master/job-server-api/src/main/java/spark/jobserver/api/TestJob.java&#34;&gt;TestJob.java&lt;/a&gt;) support&lt;/li&gt; &#xA; &lt;li&gt;LDAP Auth support via Apache Shiro integration&lt;/li&gt; &#xA; &lt;li&gt;Separate JVM per SparkContext for isolation (EXPERIMENTAL)&lt;/li&gt; &#xA; &lt;li&gt;Supports sub-second low-latency jobs via long-running job contexts&lt;/li&gt; &#xA; &lt;li&gt;Start and stop job contexts for RDD sharing and low-latency jobs; change resources on restart&lt;/li&gt; &#xA; &lt;li&gt;Kill running jobs via stop context and delete job&lt;/li&gt; &#xA; &lt;li&gt;Separate jar uploading step for faster job startup&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous and synchronous job API. Synchronous API is great for low latency jobs!&lt;/li&gt; &#xA; &lt;li&gt;Works with Standalone Spark as well on &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/cluster.md&#34;&gt;cluster&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/mesos.md&#34;&gt;Mesos&lt;/a&gt;, YARN &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/yarn.md&#34;&gt;client&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/EMR.md&#34;&gt;on EMR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Job and jar info is persisted via a pluggable DAO interface&lt;/li&gt; &#xA; &lt;li&gt;Named Objects (such as RDDs or DataFrames) to cache and retrieve RDDs or DataFrames by name, improving object sharing and reuse among jobs.&lt;/li&gt; &#xA; &lt;li&gt;Supports Scala 2.11 and 2.12&lt;/li&gt; &#xA; &lt;li&gt;Support for supervise mode of Spark (EXPERIMENTAL)&lt;/li&gt; &#xA; &lt;li&gt;Possible to be deployed in an &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#ha-deployment-beta&#34;&gt;HA setup&lt;/a&gt; of multiple jobservers (beta)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Version Information&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Spark Version&lt;/th&gt; &#xA;   &lt;th&gt;Scala Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.8.1&lt;/td&gt; &#xA;   &lt;td&gt;2.2.0&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.10.2&lt;/td&gt; &#xA;   &lt;td&gt;2.4.4&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.11.1&lt;/td&gt; &#xA;   &lt;td&gt;2.4.4&lt;/td&gt; &#xA;   &lt;td&gt;2.11, 2.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For release notes, look in the &lt;code&gt;notes/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/&#34;&gt;Due to the sunset of Bintray&lt;/a&gt; all previous release binaries were deleted. Jobserver had to migrate to JFrog Platform and only recent releases are available there. To use Spark Jobserver in your SBT project please include the following resolver in you &lt;code&gt;build.sbt&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;resolvers += &#34;Artifactory&#34; at &#34;https://sparkjobserver.jfrog.io/artifactory/jobserver/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#creating-a-project-manually-assuming-that-you-already-have-sbt-project-structure&#34;&gt;Creating a project manually assuming that you already have sbt project structure&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;If you need non-released jars, please visit &lt;a href=&#34;https://jitpack.io&#34;&gt;Jitpack&lt;/a&gt; - they provide non-release jar builds for any Git repo. :)&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started with Spark Job Server&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get started is to try the &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/docker.md&#34;&gt;Docker container&lt;/a&gt; which prepackages a Spark distribution with the job server and lets you start and deploy it.&lt;/p&gt; &#xA;&lt;p&gt;Alternatives:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build and run Job Server in local &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#development-mode&#34;&gt;development mode&lt;/a&gt; within SBT. NOTE: This does NOT work for YARN, and in fact is only recommended with &lt;code&gt;spark.master&lt;/code&gt; set to &lt;code&gt;local[*]&lt;/code&gt;. Please deploy if you want to try with YARN or other real cluster.&lt;/li&gt; &#xA; &lt;li&gt;Deploy job server to a cluster. There are two alternatives (see the &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#deployment&#34;&gt;deployment section&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;server_deploy.sh&lt;/code&gt; deploys job server to a directory on a remote host.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;server_package.sh&lt;/code&gt; deploys job server to a local directory, from which you can deploy the directory, or create a .tar.gz for Mesos or YARN deployment.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;EC2 Deploy scripts - follow the instructions in &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/EC2.md&#34;&gt;EC2&lt;/a&gt; to spin up a Spark cluster with job server and an example application.&lt;/li&gt; &#xA; &lt;li&gt;EMR Deploy instruction - follow the instruction in &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/EMR.md&#34;&gt;EMR&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;NOTE: Spark Job Server can optionally run &lt;code&gt;SparkContext&lt;/code&gt;s in their own, forked JVM process when the config option &lt;code&gt;spark.jobserver.context-per-jvm&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt;. This option does not currently work for SBT/local dev mode. See &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#deployment&#34;&gt;Deployment&lt;/a&gt; section for more info.&lt;/p&gt; &#xA;&lt;h2&gt;Development mode&lt;/h2&gt; &#xA;&lt;p&gt;The example walk-through below shows you how to use the job server with an included example job, by running the job server in local development mode in SBT. This is not an example of usage in production.&lt;/p&gt; &#xA;&lt;p&gt;You need to have &lt;a href=&#34;http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html&#34;&gt;SBT&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;To set the current version, do something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export VER=`sbt version | tail -1 | cut -f2`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From SBT shell, simply type &#34;reStart&#34;. This uses a default configuration file. An optional argument is a path to an alternative config file. You can also specify JVM parameters after &#34;---&#34;. Including all the options looks like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;job-server-extras/reStart /path/to/my.conf --- -Xmx8g&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that reStart (SBT Revolver) forks the job server in a separate process. If you make a code change, simply type reStart again at the SBT shell prompt, it will compile your changes and restart the jobserver. It enables very fast turnaround cycles.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE2&lt;/strong&gt;: You cannot do &lt;code&gt;sbt reStart&lt;/code&gt; from the OS shell. SBT will start job server and immediately kill it.&lt;/p&gt; &#xA;&lt;p&gt;For example jobs see the job-server-tests/ project / folder.&lt;/p&gt; &#xA;&lt;p&gt;When you use &lt;code&gt;reStart&lt;/code&gt;, the log file goes to &lt;code&gt;job-server/job-server-local.log&lt;/code&gt;. There is also an environment variable EXTRA_JAR for adding a jar to the classpath.&lt;/p&gt; &#xA;&lt;h3&gt;WordCountExample walk-through&lt;/h3&gt; &#xA;&lt;h4&gt;Package Jar - Send to Server&lt;/h4&gt; &#xA;&lt;p&gt;First, to package the test jar containing the WordCountExample: &lt;code&gt;sbt job-server-tests/package&lt;/code&gt;. Then go ahead and start the job server using the instructions above.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s upload the jar:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X POST localhost:8090/binaries/test -H &#34;Content-Type: application/java-archive&#34; --data-binary @job-server-tests/target/scala-2.12/job-server-tests_2.12-$VER.jar&#xA;OK‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Ad-hoc Mode - Single, Unrelated Jobs (Transient Context)&lt;/h4&gt; &#xA;&lt;p&gt;The above jar is uploaded as app &lt;code&gt;test&lt;/code&gt;. Next, let&#39;s start an ad-hoc word count job, meaning that the job server will create its own SparkContext, and return a job ID for subsequent querying:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -d &#34;input.string = a b c a b see&#34; &#34;localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&#34;&#xA;{&#xA;  &#34;duration&#34;: &#34;Job not done yet&#34;,&#xA;  &#34;classPath&#34;: &#34;spark.jobserver.WordCountExample&#34;,&#xA;  &#34;startTime&#34;: &#34;2016-06-19T16:27:12.196+05:30&#34;,&#xA;  &#34;context&#34;: &#34;b7ea0eb5-spark.jobserver.WordCountExample&#34;,&#xA;  &#34;status&#34;: &#34;STARTED&#34;,&#xA;  &#34;jobId&#34;: &#34;5453779a-f004-45fc-a11d-a39dae0f9bf4&#34;&#xA;}‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: If you want to feed in a text file config and POST using curl, you want the &lt;code&gt;--data-binary&lt;/code&gt; option, otherwise curl will munge your line separator chars. Like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl --data-binary @my-job-config.json &#34;localhost:8090/jobs?appNam=...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE2: If you want to send in UTF-8 chars, make sure you pass in a proper header to CURL for the encoding, otherwise it may assume an encoding which is not what you expect.&lt;/p&gt; &#xA;&lt;p&gt;From this point, you could asynchronously query the status and results:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl localhost:8090/jobs/5453779a-f004-45fc-a11d-a39dae0f9bf4&#xA;{&#xA;  &#34;duration&#34;: &#34;6.341 secs&#34;,&#xA;  &#34;classPath&#34;: &#34;spark.jobserver.WordCountExample&#34;,&#xA;  &#34;startTime&#34;: &#34;2015-10-16T03:17:03.127Z&#34;,&#xA;  &#34;context&#34;: &#34;b7ea0eb5-spark.jobserver.WordCountExample&#34;,&#xA;  &#34;result&#34;: {&#xA;    &#34;a&#34;: 2,&#xA;    &#34;b&#34;: 2,&#xA;    &#34;c&#34;: 1,&#xA;    &#34;see&#34;: 1&#xA;  },&#xA;  &#34;status&#34;: &#34;FINISHED&#34;,&#xA;  &#34;jobId&#34;: &#34;5453779a-f004-45fc-a11d-a39dae0f9bf4&#34;&#xA;}‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you could append &lt;code&gt;&amp;amp;sync=true&lt;/code&gt; when you POST to /jobs to get the results back in one request, but for real clusters and most jobs this may be too slow.&lt;/p&gt; &#xA;&lt;p&gt;You can also append &lt;code&gt;&amp;amp;timeout=XX&lt;/code&gt; to extend the request timeout for &lt;code&gt;sync=true&lt;/code&gt; requests.&lt;/p&gt; &#xA;&lt;h4&gt;Persistent Context Mode - Faster &amp;amp; Required for Related Jobs&lt;/h4&gt; &#xA;&lt;p&gt;Another way of running this job is in a pre-created context. Start a new context:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -d &#34;&#34; &#34;localhost:8090/contexts/test-context?num-cpu-cores=4&amp;amp;memory-per-node=512m&#34;&#xA;OK‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can verify that the context has been created:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl localhost:8090/contexts&#xA;[&#34;test-context&#34;]‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s run the job in the context and get the results back right away:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -d &#34;input.string = a b c a b see&#34; &#34;localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&amp;amp;context=test-context&amp;amp;sync=true&#34;&#xA;{&#xA;  &#34;result&#34;: {&#xA;    &#34;a&#34;: 2,&#xA;    &#34;b&#34;: 2,&#xA;    &#34;c&#34;: 1,&#xA;    &#34;see&#34;: 1&#xA;  }&#xA;}‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the addition of &lt;code&gt;context=&lt;/code&gt; and &lt;code&gt;sync=true&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Debug mode&lt;/h3&gt; &#xA;&lt;p&gt;Spark job server is started using SBT Revolver (which forks a new JVM), so debugging directly in an IDE is not feasible. To enable debugging, the Spark job server should be started from the SBT shell with the following Java options :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;job-server-extras/reStart /absolute/path/to/your/dev.conf --- -Xdebug -Xrunjdwp:transport=dt_socket,address=15000,server=y,suspend=y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command starts a remote debugging server on port 15000. The Spark job server is not started until a debugging client (Intellij, Eclipse, telnet, ...) connects to the exposed port.&lt;/p&gt; &#xA;&lt;p&gt;In your IDE you just have to start a Remote debugging debug job and use the above defined port. Once the client connects to the debugging server the Spark job server is started and you can start adding breakpoints and debugging requests.&lt;/p&gt; &#xA;&lt;p&gt;Note that you might need to adjust some server parameters to avoid short Spary/Akka/Spark timeouts, in your &lt;code&gt;dev.conf&lt;/code&gt; add the following values :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;spark {&#xA;  jobserver {&#xA;    # Dev debug timeouts&#xA;    context-creation-timeout = 1000000 s&#xA;    yarn-context-creation-timeout = 1000000 s&#xA;    default-sync-timeout = 1000000 s&#xA;  }&#xA;&#xA;  context-settings {&#xA;    # Dev debug timeout&#xA;    context-init-timeout = 1000000 s&#xA;  }&#xA;}&#xA;akka.http.server {&#xA;      # Debug timeouts&#xA;      idle-timeout = infinite&#xA;      request-timeout = infinite&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, you might have to increase the Akka Timeouts by adding the following query parameter &lt;code&gt;timeout=1000000&lt;/code&gt; in your HTTP requests :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -d &#34;input.string = a b c a b see&#34; &#34;localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&amp;amp;sync=true&amp;amp;timeout=100000&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Create a Job Server Project&lt;/h2&gt; &#xA;&lt;h3&gt;Creating a project from scratch using giter8 template&lt;/h3&gt; &#xA;&lt;p&gt;There is a giter8 template available at &lt;a href=&#34;https://github.com/spark-jobserver/spark-jobserver.g8&#34;&gt;https://github.com/spark-jobserver/spark-jobserver.g8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ sbt new spark-jobserver/spark-jobserver.g8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Answer the questions to generate a project structure for you. This contains Word Count example spark job using both old API and new one.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd /path/to/project/directory&#xA;$ sbt package&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you could remove example application and start adding your one.&lt;/p&gt; &#xA;&lt;h3&gt;Creating a project manually assuming that you already have sbt project structure&lt;/h3&gt; &#xA;&lt;p&gt;In your &lt;code&gt;build.sbt&lt;/code&gt;, add this to use the job server jar:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    resolvers += &#34;Artifactory&#34; at &#34;https://sparkjobserver.jfrog.io/artifactory/jobserver/&#34;&#xA;&#xA;    libraryDependencies += &#34;spark.jobserver&#34; %% &#34;job-server-api&#34; % &#34;0.11.1&#34; % &#34;provided&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If a SQL or Hive job/context is desired, you also want to pull in &lt;code&gt;job-server-extras&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libraryDependencies += &#34;spark.jobserver&#34; %% &#34;job-server-extras&#34; % &#34;0.11.1&#34; % &#34;provided&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For most use cases it&#39;s better to have the dependencies be &#34;provided&#34; because you don&#39;t want SBT assembly to include the whole job server jar.&lt;/p&gt; &#xA;&lt;p&gt;To create a job that can be submitted through the job server, the job must implement the &lt;code&gt;SparkJob&lt;/code&gt; trait. Your job will look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;object SampleJob extends SparkJob {&#xA;    override def runJob(sc: SparkContext, jobConfig: Config): Any = ???&#xA;    override def validate(sc: SparkContext, config: Config): SparkJobValidation = ???&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;runJob&lt;/code&gt; contains the implementation of the Job. The SparkContext is managed by the JobServer and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;validate&lt;/code&gt; allows for an initial validation of the context and any provided configuration. If the context and configuration are OK to run the job, returning &lt;code&gt;spark.jobserver.SparkJobValid&lt;/code&gt; will let the job execute, otherwise returning &lt;code&gt;spark.jobserver.SparkJobInvalid(reason)&lt;/code&gt; prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an &lt;code&gt;HTTP/1.1 400 Bad Request&lt;/code&gt; status code. &lt;code&gt;validate&lt;/code&gt; helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NEW SparkJob API&lt;/h3&gt; &#xA;&lt;p&gt;Note: As of version 0.7.0, a new SparkJob API that is significantly better than the old SparkJob API will take over. Existing jobs should continue to compile against the old &lt;code&gt;spark.jobserver.SparkJob&lt;/code&gt; API, but this will be deprecated in the future. Note that jobs before 0.7.0 will need to be recompiled, older jobs may not work with the current SJS example. The new API looks like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;object WordCountExampleNewApi extends NewSparkJob {&#xA;  type JobData = Seq[String]&#xA;  type JobOutput = collection.Map[String, Long]&#xA;&#xA;  def runJob(sc: SparkContext, runtime: JobEnvironment, data: JobData): JobOutput =&#xA;    sc.parallelize(data).countByValue&#xA;&#xA;  def validate(sc: SparkContext, runtime: JobEnvironment, config: Config):&#xA;    JobData Or Every[ValidationProblem] = {&#xA;    Try(config.getString(&#34;input.string&#34;).split(&#34; &#34;).toSeq)&#xA;      .map(words =&amp;gt; Good(words))&#xA;      .getOrElse(Bad(One(SingleProblem(&#34;No input.string param&#34;))))&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is much more type safe, separates context configuration, job ID, named objects, and other environment variables into a separate JobEnvironment input, and allows the validation method to return specific data for the runJob method. See the &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/job-server-tests/src/main/scala/spark/jobserver/WordCountExample.scala&#34;&gt;WordCountExample&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/job-server-tests/src/main/scala/spark/jobserver/LongPiJob.scala&#34;&gt;LongPiJob&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s try running our sample job with an invalid configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -i -d &#34;bad.input=abc&#34; &#34;localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&#34;&#xA;HTTP/1.1 400 Bad Request&#xA;Server: spray-can/1.3.4&#xA;Date: Thu, 14 Sep 2017 12:01:37 GMT&#xA;Access-Control-Allow-Origin: *&#xA;Content-Type: application/json; charset=UTF-8&#xA;Content-Length: 738&#xA;&#xA;{&#xA;  &#34;status&#34;: &#34;VALIDATION FAILED&#34;,&#xA;  &#34;result&#34;: {&#xA;    &#34;message&#34;: &#34;One(SparkJobInvalid(No input.string config param))&#34;,&#xA;    &#34;errorClass&#34;: &#34;java.lang.Throwable&#34;,&#xA;    &#34;stack&#34;: &#34;java.lang.Throwable: One(SparkJobInvalid(No input.string config param))\n\tat spark.jobserver.JobManagerActor$$anonfun$getJobFuture$4.apply(JobManagerActor.scala:327)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;NEW SparkJob API with Spark v2.1&lt;/h3&gt; &#xA;&lt;p&gt;Deploying Spark JobServer with Spark v2.x cluster, you can create a SparkSession context which enables Spark-SQL and Hive support&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;curl -i -d &#34;&#34; &#39;http://localhost:8090/contexts/sql-context-1?num-cpu-cores=2&amp;amp;memory-per-node=512M&amp;amp;context-factory=spark.jobserver.context.SessionContextFactory&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark JobServer application shall extend from the SparkSessionJob to use the spark.jobserver.context.SessionContextFactory, here is an example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.typesafe.config.Config&#xA;import org.apache.spark.sql.SparkSession&#xA;import org.scalactic._&#xA;import spark.jobserver.SparkSessionJob&#xA;import spark.jobserver.api.{JobEnvironment, SingleProblem, ValidationProblem}&#xA;&#xA;import scala.util.Try&#xA;&#xA;object WordCountExampleSparkSession extends SparkSessionJob {&#xA;  type JobData = Seq[String]&#xA;  type JobOutput = collection.Map[String, Long]&#xA;&#xA;  override def runJob(sparkSession: SparkSession, runtime: JobEnvironment, data: JobData): JobOutput =&#xA;    sparkSession.sparkContext.parallelize(data).countByValue&#xA;&#xA;  override def validate(sparkSession: SparkSession, runtime: JobEnvironment, config: Config):&#xA;  JobData Or Every[ValidationProblem] = {&#xA;    Try(config.getString(&#34;input.string&#34;).split(&#34; &#34;).toSeq)&#xA;      .map(words =&amp;gt; Good(words))&#xA;      .getOrElse(Bad(One(SingleProblem(&#34;No input.string param&#34;))))&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dependency jars&lt;/h3&gt; &#xA;&lt;p&gt;For Java/Scala applications you have a couple options to package and upload dependency jars.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The easiest is to use something like &lt;a href=&#34;https://github.com/sbt/sbt-assembly&#34;&gt;sbt-assembly&lt;/a&gt; to produce a fat jar. Be sure to mark the Spark and job-server dependencies as &#34;provided&#34; so it won&#39;t blow up the jar size. This works well if the number of dependencies is not large.&lt;/li&gt; &#xA; &lt;li&gt;When the dependencies are sizeable and/or you don&#39;t want to load them with every different job, you can package the dependencies separately and use one of several options: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use the &lt;code&gt;dependent-jar-uris&lt;/code&gt; context configuration param. Then the jar gets loaded for every job.&lt;/li&gt; &#xA;   &lt;li&gt;The &lt;code&gt;dependent-jar-uris&lt;/code&gt; can also be used in job configuration param when submitting a job. On an ad-hoc context this has the same effect as &lt;code&gt;dependent-jar-uris&lt;/code&gt; context configuration param. On a persistent context the jars will be loaded for the current job and then for every job that will be executed on the persistent context. &lt;pre&gt;&lt;code&gt;curl -d &#34;&#34; &#34;localhost:8090/contexts/test-context?num-cpu-cores=4&amp;amp;memory-per-node=512m&#34;&#xA;OK‚èé&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;curl &#34;localhost:8090/jobs?appName=test&amp;amp;classPath=spark.jobserver.WordCountExample&amp;amp;context=test-context&amp;amp;sync=true&#34; -d &#39;{&#xA;    dependent-jar-uris = [&#34;file:///myjars/deps01.jar&#34;, &#34;file:///myjars/deps02.jar&#34;],&#xA;    input.string = &#34;a b c a b see&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; The jars /myjars/deps01.jar &amp;amp; /myjars/deps02.jar (present only on the SJS node) will be loaded and made available for the Spark driver &amp;amp; executors. Please note that only only &lt;code&gt;file&lt;/code&gt;, &lt;code&gt;local&lt;/code&gt;, &lt;code&gt;ftp&lt;/code&gt;, &lt;code&gt;http&lt;/code&gt; protocols will work (URIs will be added to standard java class loader). Recent changes also allow to use names of the binaries, which were uploaded to Jobserver.&lt;/li&gt; &#xA;   &lt;li&gt;Use the &lt;code&gt;--package&lt;/code&gt; option with Maven coordinates with &lt;code&gt;server_start.sh&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Recent changes also allow you to use new parameters for the &lt;code&gt;POST /jobs&lt;/code&gt; request: &lt;pre&gt;&lt;code&gt;POST /jobs?cp=someURI,binName1,binName2&amp;amp;mainClass=some.main.Class&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;code&gt;cp&lt;/code&gt; accepts list of binary names (under which you uploaded binary to Jobserver) and URIs, &lt;code&gt;mainClass&lt;/code&gt; is the main class of your application. Main advantage of this approach in comparison to using &lt;code&gt;dependent-jar-uris&lt;/code&gt; is that you don&#39;t need to specify which jar is the main one and can just send all of needed jars in one list.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Named Objects&lt;/h3&gt; &#xA;&lt;h4&gt;Using Named RDDs&lt;/h4&gt; &#xA;&lt;p&gt;Initially, the job server only supported Named RDDs. For backwards compatibility and convenience, the following is still supported even though it is now possible to use the more generic Named Object support described in the next section.&lt;/p&gt; &#xA;&lt;p&gt;Named RDDs are a way to easily share RDDs among jobs. Using this facility, computed RDDs can be cached with a given name and later on retrieved. To use this feature, the SparkJob needs to mixin &lt;code&gt;NamedRddSupport&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;object SampleNamedRDDJob  extends SparkJob with NamedRddSupport {&#xA;    override def runJob(sc:SparkContext, jobConfig: Config): Any = ???&#xA;    override def validate(sc:SparkContext, config: Config): SparkJobValidation = ???&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in the implementation of the job, RDDs can be stored with a given name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;this.namedRdds.update(&#34;french_dictionary&#34;, frenchDictionaryRDD)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other job running in the same context can retrieve and use this RDD later on:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd = this.namedRdds.get[(String, String)](&#34;french_dictionary&#34;).get&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(note the explicit type provided to get. This will allow to cast the retrieved RDD that otherwise is of type RDD[_])&lt;/p&gt; &#xA;&lt;p&gt;For jobs that depends on a named RDDs it&#39;s a good practice to check for the existence of the NamedRDD in the &lt;code&gt;validate&lt;/code&gt; method as explained earlier:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def validate(sc:SparkContext, config: Config): SparkJobValidation = {&#xA;  ...&#xA;  val rdd = this.namedRdds.get[(Long, scala.Seq[String])](&#34;dictionary&#34;)&#xA;  if (rdd.isDefined) SparkJobValid else SparkJobInvalid(s&#34;Missing named RDD [dictionary]&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using Named Objects&lt;/h4&gt; &#xA;&lt;p&gt;Named Objects are a way to easily share RDDs, DataFrames or other objects among jobs. Using this facility, computed objects can be cached with a given name and later on retrieved. To use this feature, the SparkJob needs to mixin &lt;code&gt;NamedObjectSupport&lt;/code&gt;. It is also necessary to define implicit persisters for each desired type of named objects. For convencience, we have provided implementations for RDD persistence and for DataFrame persistence (defined in &lt;code&gt;job-server-extras&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;object SampleNamedObjectJob  extends SparkJob with NamedObjectSupport {&#xA;&#xA;  implicit def rddPersister[T] : NamedObjectPersister[NamedRDD[T]] = new RDDPersister[T]&#xA;  implicit val dataFramePersister = new DataFramePersister&#xA;&#xA;    override def runJob(sc:SparkContext, jobConfig: Config): Any = ???&#xA;    override def validate(sc:SparkContext, config: Config): SparkJobValidation = ???&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in the implementation of the job, RDDs can be stored with a given name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;this.namedObjects.update(&#34;rdd:french_dictionary&#34;, NamedRDD(frenchDictionaryRDD, forceComputation = false, storageLevel = StorageLevel.NONE))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;DataFrames can be stored like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;this.namedObjects.update(&#34;df:some df&#34;, NamedDataFrame(frenchDictionaryDF, forceComputation = false, storageLevel = StorageLevel.NONE))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is advisable to use different name prefixes for different types of objects to avoid confusion.&lt;/p&gt; &#xA;&lt;p&gt;Another job running in the same context can retrieve and use these objects later on:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val NamedRDD(frenchDictionaryRDD, _ ,_) = namedObjects.get[NamedRDD[(String, String)]](&#34;rdd:french_dictionary&#34;).get&#xA;&#xA;val NamedDataFrame(frenchDictionaryDF, _, _) = namedObjects.get[NamedDataFrame](&#34;df:some df&#34;).get&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Note the explicit type provided to get. This will allow to cast the retrieved RDD/DataFrame object to the proper result type.)&lt;/p&gt; &#xA;&lt;p&gt;For jobs that depends on a named objects it&#39;s a good practice to check for the existence of the NamedObject in the &lt;code&gt;validate&lt;/code&gt; method as explained earlier:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def validate(sc:SparkContext, config: Config): SparkJobValidation = {&#xA;  ...&#xA;  val obj = this.namedObjects.get(&#34;dictionary&#34;)&#xA;  if (obj.isDefined) SparkJobValid else SparkJobInvalid(s&#34;Missing named object [dictionary]&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;HTTPS / SSL Configuration&lt;/h3&gt; &#xA;&lt;h4&gt;Server authentication&lt;/h4&gt; &#xA;&lt;p&gt;To activate server authentication and ssl communication, set these flags in your application.conf file (Section &#39;akka.http.server&#39;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  ssl-encryption = on&#xA;  # absolute path to keystore file&#xA;  keystore = &#34;/some/path/sjs.jks&#34;&#xA;  keystorePW = &#34;changeit&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will need a keystore that contains the server certificate. The bare minimum is achieved with this command which creates a self-signed certificate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; keytool -genkey -keyalg RSA -alias jobserver -keystore ~/sjs.jks -storepass changeit -validity 360 -keysize 2048&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may place the keystore anywhere. Here is an example of a simple curl command that utilizes ssl:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -k https://localhost:8090/contexts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;-k&lt;/code&gt; flag tells curl to &#34;Allow connections to SSL sites without certs&#34;. Export your server certificate and import it into the client&#39;s truststore to fully utilize ssl security.&lt;/p&gt; &#xA;&lt;h4&gt;Client authentication&lt;/h4&gt; &#xA;&lt;p&gt;Client authentication can be enabled by simply pointing Job Server to a valid Trust Store. As for server authentication, this is done by setting appropriate values in the application.conf. The minimum set of parameters to enable client authentication consists of:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  # truststore = &#34;/some/path/server-truststore.jks&#34;&#xA;  # truststorePW = &#34;changeit&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note, client authentication implies server authentication, therefore client authentication will only be enabled once server authentication is activated.&lt;/p&gt; &#xA;&lt;h3&gt;Access Control&lt;/h3&gt; &#xA;&lt;p&gt;By default, access to the Job Server is not limited. Basic authentication (username and password) support is provided via the &lt;a href=&#34;http://shiro.apache.org/index.html&#34;&gt;Apache Shiro&lt;/a&gt; framework or &lt;a href=&#34;https://www.keycloak.org/&#34;&gt;Keycloak&lt;/a&gt;. Both authentication frameworks have to be explicitly activated in the configuration file.&lt;/p&gt; &#xA;&lt;p&gt;After the configuration, you can provide credentials via basic auth. Here is an example of a simple curl command that authenticates a user and uses ssl (you may want to use -H to hide the credentials, this is just a simple example to get you started):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -k --basic --user &#39;user:pw&#39; https://localhost:8090/contexts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Shiro Authentication&lt;/h4&gt; &#xA;&lt;p&gt;The Shiro Authenticator can be activated in the configuration file by changing the authentication provider and providing a shiro configuration file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;access-control {&#xA;  provider = spark.jobserver.auth.ShiroAccessControl&#xA;&#xA;  # absolute path to shiro config file, including file name&#xA;  shiro.config.path = &#34;/some/path/shiro.ini&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Shiro-specific configuration options should be placed into a file named &#39;shiro.ini&#39; in the directory as specified by the config option &#39;config.path&#39;. Here is an example that configures LDAP with user group verification:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# use this for basic ldap authorization, without group checking&#xA;# activeDirectoryRealm = org.apache.shiro.realm.ldap.JndiLdapRealm&#xA;# use this for checking group membership of users based on the &#39;member&#39; attribute of the groups:&#xA;activeDirectoryRealm = spark.jobserver.auth.LdapGroupRealm&#xA;# search base for ldap groups (only relevant for LdapGroupRealm):&#xA;activeDirectoryRealm.contextFactory.environment[ldap.searchBase] = dc=xxx,dc=org&#xA;# allowed groups (only relevant for LdapGroupRealm):&#xA;activeDirectoryRealm.contextFactory.environment[ldap.allowedGroups] = &#34;cn=group1,ou=groups&#34;, &#34;cn=group2,ou=groups&#34;&#xA;activeDirectoryRealm.contextFactory.environment[java.naming.security.credentials] = password&#xA;activeDirectoryRealm.contextFactory.url = ldap://localhost:389&#xA;activeDirectoryRealm.userDnTemplate = cn={0},ou=people,dc=xxx,dc=org&#xA;&#xA;cacheManager = org.apache.shiro.cache.MemoryConstrainedCacheManager&#xA;&#xA;securityManager.cacheManager = $cacheManager&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure to edit the url, credentials, userDnTemplate, ldap.allowedGroups and ldap.searchBase settings in accordance with your local setup.&lt;/p&gt; &#xA;&lt;p&gt;The Shiro authenticator is able to perform fine-grained &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#user-authorization&#34;&gt;user authorization&lt;/a&gt;. Permissions are extracted from the provided user roles. Each role that matches a known permission is added to the authenticated user. Unknown roles are ignored. For a list of available permissions see &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/permissions.md&#34;&gt;Permissions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Keycloak Authentication&lt;/h4&gt; &#xA;&lt;p&gt;The Keycloak Authenticator can be activated in the configuration file by changing the authentication provider and providing a keycloak configuration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;access-control {&#xA;  provider = spark.jobserver.auth.KeycloakAccessControl&#xA;&#xA;  keycloak {&#xA;    authServerUrl = &#34;https://example.com&#34;&#xA;    realmName = &#34;master&#34;&#xA;    client = &#34;client&#34;&#xA;    clientSecret = &#34;secret&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Mandatory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;authServerUrl&lt;/td&gt; &#xA;   &lt;td&gt;The URL to reach the keycloak instance.&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;realmName&lt;/td&gt; &#xA;   &lt;td&gt;The realm to authenticate against.&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;client&lt;/td&gt; &#xA;   &lt;td&gt;The client to authenticate against.&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;clientSecret&lt;/td&gt; &#xA;   &lt;td&gt;An according client secret, if it exists.&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For better performance, authentication requests against Keycloak can be cached locally.&lt;/p&gt; &#xA;&lt;p&gt;The Keycloak authenticator is able to perform fine-grained &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/#user-authorization&#34;&gt;user authorization&lt;/a&gt;. Permissions are extracted from the provided client&#39;s roles. Each client role that matches a known permission is added to the authenticated user. Unknown client roles are ignored. For a list of available permissions see &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/permissions.md&#34;&gt;Permissions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Important:&lt;/em&gt; If no client role matches a permission, the user is assigned the &lt;code&gt;ALLOW_ALL&lt;/code&gt; role.&lt;/p&gt; &#xA;&lt;h3&gt;User Authorization&lt;/h3&gt; &#xA;&lt;p&gt;Spark job server implements a basic authorization management system to control access to single resources. By default, users always have access to all resources (&lt;code&gt;ALLOW_ALL&lt;/code&gt;). Authorization is implemented by checking the &lt;em&gt;permissions&lt;/em&gt; of a user with the required permissions of an endpoint. For a detailed list of all available permissions see &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/permissions.md&#34;&gt;Permissions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;See also running on &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/cluster.md&#34;&gt;cluster&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/yarn.md&#34;&gt;YARN client&lt;/a&gt;, on &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/EMR.md&#34;&gt;EMR&lt;/a&gt; and running on &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/mesos.md&#34;&gt;Mesos&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Manual steps&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;config/local.sh.template&lt;/code&gt; to &lt;code&gt;&amp;lt;environment&amp;gt;.sh&lt;/code&gt; and edit as appropriate. NOTE: be sure to set SPARK_VERSION if you need to compile against a different version.&lt;/li&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;config/shiro.ini.template&lt;/code&gt; to &lt;code&gt;shiro.ini&lt;/code&gt; and edit as appropriate. NOTE: only required when &lt;code&gt;access-control.provider = spark.jobserver.auth.ShiroAccessControl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;config/local.conf.template&lt;/code&gt; to &lt;code&gt;&amp;lt;environment&amp;gt;.conf&lt;/code&gt; and edit as appropriate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bin/server_deploy.sh &amp;lt;environment&amp;gt;&lt;/code&gt; -- this packages the job server along with config files and pushes it to the remotes you have configured in &lt;code&gt;&amp;lt;environment&amp;gt;.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;On the remote server, start it in the deployed directory with &lt;code&gt;server_start.sh&lt;/code&gt; and stop it with &lt;code&gt;server_stop.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The &lt;code&gt;server_start.sh&lt;/code&gt; script uses &lt;code&gt;spark-submit&lt;/code&gt; under the hood and may be passed any of the standard extra arguments from &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Under the hood, the deploy scripts generate an assembly jar from the &lt;code&gt;job-server-extras&lt;/code&gt; project. Generating assemblies from other projects may not include all the necessary components for job execution.&lt;/p&gt; &#xA;&lt;h3&gt;Context per JVM&lt;/h3&gt; &#xA;&lt;p&gt;Each context can be a separate process launched using SparkLauncher, if &lt;code&gt;context-per-jvm&lt;/code&gt; is set to true. This can be especially desirable when you want to run many contexts at once, or for certain types of contexts such as StreamingContexts which really need their own processes.&lt;/p&gt; &#xA;&lt;p&gt;Also, the extra processes talk to the master HTTP process via random ports using the Akka Cluster gossip protocol. If for some reason the separate processes causes issues, set &lt;code&gt;spark.jobserver.context-per-jvm&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, which will cause the job server to use a single JVM for all contexts.&lt;/p&gt; &#xA;&lt;p&gt;Among the known issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launched contexts do not shut down by themselves. You need to manually kill each separate process, or do &lt;code&gt;-X DELETE /contexts/&amp;lt;context-name&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Log files are separated out for each context (assuming &lt;code&gt;context-per-jvm&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;) in their own subdirs under the &lt;code&gt;LOG_DIR&lt;/code&gt; configured in &lt;code&gt;settings.sh&lt;/code&gt; in the deployed directory.&lt;/p&gt; &#xA;&lt;p&gt;Note: to test out the deploy to a local staging dir, or package the job server for Mesos, use &lt;code&gt;bin/server_package.sh &amp;lt;environment&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Configuring Spark Jobserver backend&lt;/h3&gt; &#xA;&lt;p&gt;Please visit &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/dao-setup.md&#34;&gt;setting up dao&lt;/a&gt; documentation page. Currently supported backend options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;H2&lt;/li&gt; &#xA; &lt;li&gt;MySQL&lt;/li&gt; &#xA; &lt;li&gt;PostgreSQL&lt;/li&gt; &#xA; &lt;li&gt;HDFS for binaries with Zookeeper for metadata&lt;/li&gt; &#xA; &lt;li&gt;HDFS for binaries with H2/MySQL/PostgreSQL for metadata&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;HA Deployment (beta)&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to run multiple Spark Jobservers in a highly available setup. For a documentation of a Jobserver HA setup, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/HA.md&#34;&gt;Jobserver HA documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Chef&lt;/h3&gt; &#xA;&lt;p&gt;There is also a &lt;a href=&#34;https://github.com/spark-jobserver/chef-spark-jobserver&#34;&gt;Chef cookbook&lt;/a&gt; which can be used to deploy Spark Jobserver.&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The job server is intended to be run as one or more independent processes, separate from the Spark cluster (though it very well may be collocated with say the Master).&lt;/p&gt; &#xA;&lt;p&gt;At first glance, it seems many of these functions (eg job management) could be integrated into the Spark standalone master. While this is true, we believe there are many significant reasons to keep it separate:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We want the job server to work for Mesos and YARN as well&lt;/li&gt; &#xA; &lt;li&gt;Spark and Mesos masters are organized around &#34;applications&#34; or contexts, but the job server supports running many discrete &#34;jobs&#34; inside a single context&lt;/li&gt; &#xA; &lt;li&gt;We want it to support Shark functionality in the future&lt;/li&gt; &#xA; &lt;li&gt;Loose coupling allows for flexible HA arrangements (multiple job servers targeting same standalone master, or possibly multiple Spark clusters per job server)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Flow diagrams are checked in in the doc/ subdirectory. .diagram files are for websequencediagrams.com... check them out, they really will help you understand the flow of messages between actors.&lt;/p&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;A comprehensive (manually created) swagger specification of the spark jobserver WebApi can be found &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/WebApi/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Binaries&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET /binaries               - lists all current binaries&#xA;GET /binaries /&amp;lt;appName&amp;gt;    - gets info about the last binary uploaded under this name (app-name, binary-type, upload-time)&#xA;POST /binaries/&amp;lt;appName&amp;gt;    - upload a new binary file&#xA;DELETE /binaries/&amp;lt;appName&amp;gt;  - delete defined binary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When POSTing new binaries, the content-type header must be set to one of the types supported by the subclasses of the &lt;code&gt;BinaryType&lt;/code&gt; trait. e.g. &#34;application/java-archive&#34;, &#34;application/python-egg&#34; or &#34;application/python-wheel&#34;. If you are using curl command, then you must pass for example &#34;-H &#39;Content-Type: application/python-wheel&#39;&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;Contexts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET /contexts               - lists all current contexts&#xA;GET /contexts/&amp;lt;name&amp;gt;        - gets info about a context, such as the spark UI url&#xA;POST /contexts/&amp;lt;name&amp;gt;       - creates a new context&#xA;DELETE /contexts/&amp;lt;name&amp;gt;     - stops a context and all jobs running in it. Additionally, you can pass ?force=true to stop a context forcefully. This is equivalent to killing the application from SparkUI (works for spark standalone only).&#xA;PUT /contexts?reset=reboot  - shuts down all contexts and re-loads only the contexts from config. Use ?sync=false to execute asynchronously.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spark context configuration params can follow &lt;code&gt;POST /contexts/&amp;lt;name&amp;gt;&lt;/code&gt; as query params. See section below for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Jobs&lt;/h3&gt; &#xA;&lt;p&gt;Jobs submitted to the job server must implement a &lt;code&gt;SparkJob&lt;/code&gt; trait. It has a main &lt;code&gt;runJob&lt;/code&gt; method which is passed a SparkContext and a typesafe Config object. Results returned by the method are made available through the REST API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET /jobs                - Lists the last N jobs&#xA;POST /jobs               - Starts a new job, use ?sync=true to wait for results&#xA;GET /jobs/&amp;lt;jobId&amp;gt;        - Gets the result or status of a specific job&#xA;DELETE /jobs/&amp;lt;jobId&amp;gt;     - Kills the specified job&#xA;GET /jobs/&amp;lt;jobId&amp;gt;/config - Gets the job configuration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional information on &lt;code&gt;POST /jobs&lt;/code&gt; check out &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/submitting-jobs.md&#34;&gt;submitting jobs documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For details on the Typesafe config format used for input (JSON also works), see the &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe Config docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;It is sometime necessary to programmatically upload files to the server. Use these paths to manage such files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GET /data                - Lists previously uploaded files that were not yet deleted&#xA;POST /data/&amp;lt;prefix&amp;gt;      - Uploads a new file, the full path of the file on the server is returned, the&#xA;                           prefix is the prefix of the actual filename used on the server (a timestamp is&#xA;                           added to ensure uniqueness)&#xA;DELETE /data/&amp;lt;filename&amp;gt;  - Deletes the specified file (only if under control of the JobServer)&#xA;PUT /data?reset=reboot   - Deletes all uploaded files. Use ?sync=false to execute asynchronously.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These files are uploaded to the server and are stored in a local temporary directory where the JobServer runs. The POST command returns the full pathname and filename of the uploaded file so that later jobs can work with this just the same as with any other server-local file. A job could therefore add this file to HDFS or distribute it to worker nodes via the SparkContext.addFile command. For files that are larger than a few hundred MB, it is recommended to manually upload these files to the server or to directly add them to your HDFS.&lt;/p&gt; &#xA;&lt;h4&gt;Data API Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ curl -d &#34;Test data file api&#34; http://localhost:8090/data/test_data_file_upload.txt&#xA;{&#xA;  &#34;result&#34;: {&#xA;    &#34;filename&#34;: &#34;/tmp/spark-jobserver/upload/test_data_file_upload.txt-2016-07-04T09_09_57.928+05_30.dat&#34;&#xA;  }&#xA;}&#xA;&#xA;$ curl http://localhost:8090/data&#xA;[&#34;/tmp/spark-jobserver/upload/test_data_file_upload.txt-2016-07-04T09_09_57.928+05_30.dat&#34;]&#xA;&#xA;$ curl -X DELETE http://localhost:8090/data/%2Ftmp%2Fspark-jobserver%2Fupload%2Ftest_data_file_upload.txt-2016-07-04T09_09_57.928%2B05_30.dat&#xA;OK&#xA;&#xA;$ curl http://localhost:8090/data&#xA;[]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Both POST and DELETE requests takes URI encoded file names.&lt;/p&gt; &#xA;&lt;h3&gt;Context configuration&lt;/h3&gt; &#xA;&lt;p&gt;A number of context-specific settings can be controlled when creating a context (POST /contexts) or running an ad-hoc job (which creates a context on the spot). For example, add urls of dependent jars for a context.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;POST &#39;/contexts/my-new-context?dependent-jar-uris=file:///some/path/of/my-foo-lib.jar&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Only the latest &lt;code&gt;dependent-jar-uris&lt;/code&gt; (btw it‚Äôs jar-uris, not jars-uri) takes effect. You can specify multiple URIs by comma-separating them. So like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;amp;dependent-jar-uris=file:///path/a.jar,file:///path/b.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When creating a context via POST /contexts, the query params are used to override the default configuration in spark.context-settings. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;POST /contexts/my-new-context?num-cpu-cores=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;would override the default spark.context-settings.num-cpu-cores setting.&lt;/p&gt; &#xA;&lt;p&gt;When starting a job, and the &lt;code&gt;context=&lt;/code&gt; query param is not specified, then an ad-hoc context is created. Any settings specified in spark.context-settings will override the defaults in the job server config when it is started up.&lt;/p&gt; &#xA;&lt;p&gt;Any spark configuration param can be overridden either in POST /contexts query params, or through &lt;code&gt;spark .context-settings&lt;/code&gt; job configuration. In addition, &lt;code&gt;num-cpu-cores&lt;/code&gt; maps to &lt;code&gt;spark.cores.max&lt;/code&gt;, and &lt;code&gt;mem-per- node&lt;/code&gt; maps to &lt;code&gt;spark.executor.memory&lt;/code&gt;. Therefore the following are all equivalent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;POST /contexts/my-new-context?num-cpu-cores=10&#xA;&#xA;POST /contexts/my-new-context?spark.cores.max=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or in the job config when using POST /jobs,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark.context-settings {&#xA;    spark.cores.max = 10&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;User impersonation for an already Kerberos authenticated user is supported via &lt;code&gt;spark.proxy.user&lt;/code&gt; query param:&lt;/p&gt; &#xA;&lt;p&gt;POST /contexts/my-new-context?spark.proxy.user=&#xA; &lt;user-to-impersonate&gt;&lt;/user-to-impersonate&gt;&lt;/p&gt; &#xA;&lt;p&gt;However, whenever the flag &lt;code&gt;access-control.shiro.use-as-proxy-user&lt;/code&gt; is set to &lt;code&gt;on&lt;/code&gt; (and Shiro is used as provider) then this parameter is ignored and the name of the authenticated user is &lt;em&gt;always&lt;/em&gt; used as the value of the &lt;code&gt;spark.proxy.user&lt;/code&gt; parameter when creating contexts.&lt;/p&gt; &#xA;&lt;p&gt;To pass settings directly to the sparkConf that do not use the &#34;spark.&#34; prefix &#34;as-is&#34;, use the &#34;passthrough&#34; section.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark.context-settings {&#xA;    spark.cores.max = 10&#xA;    passthrough {&#xA;      some.custom.hadoop.config = &#34;192.168.1.1&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To add to the underlying Hadoop configuration in a Spark context, add the hadoop section to the context settings&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;spark.context-settings {&#xA;    hadoop {&#xA;        mapreduce.framework.name = &#34;Foo&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;stop-context-on-job-error=true&lt;/code&gt; can be passed to context if you want the context to stop immediately after first error is reported by a job. The default value is false but for StreamingContextFactory the default is true. You can also change the default value globally in application.conf &lt;code&gt;context-settings&lt;/code&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;For the exact context configuration parameters, see JobManagerActor docs as well as application.conf.&lt;/p&gt; &#xA;&lt;h3&gt;Other configuration settings&lt;/h3&gt; &#xA;&lt;p&gt;For all of the Spark Job Server configuration settings, see &lt;code&gt;job-server/src/main/resources/application.conf&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Job Result Serialization&lt;/h3&gt; &#xA;&lt;p&gt;The result returned by the &lt;code&gt;SparkJob&lt;/code&gt; &lt;code&gt;runJob&lt;/code&gt; method is serialized by the job server into JSON for routes that return the result (GET /jobs with sync=true, GET /jobs/&#xA; &lt;jobid&gt;&#xA;  ). Currently the following types can be serialized properly:&#xA; &lt;/jobid&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;String, Int, Long, Double, Float, Boolean&lt;/li&gt; &#xA; &lt;li&gt;Scala Map&#39;s with string key values (non-string keys may be converted to strings)&lt;/li&gt; &#xA; &lt;li&gt;Scala Seq&#39;s&lt;/li&gt; &#xA; &lt;li&gt;Array&#39;s&lt;/li&gt; &#xA; &lt;li&gt;Anything that implements Product (Option, case classes) -- they will be serialized as lists&lt;/li&gt; &#xA; &lt;li&gt;Subclasses of java.util.List&lt;/li&gt; &#xA; &lt;li&gt;Subclasses of java.util.Map with string key values (non-string keys may be converted to strings)&lt;/li&gt; &#xA; &lt;li&gt;Maps, Seqs, Java Maps and Java Lists may contain nested values of any of the above&lt;/li&gt; &#xA; &lt;li&gt;If a job result is of scala&#39;s Stream[Byte] type it will be serialised directly as a chunk encoded stream. This is useful if your job result payload is large and may cause a timeout serialising as objects. Beware, this will not currently work as desired with context-per-jvm=true configuration, since it would require serialising Stream[_] blob between processes. For now use Stream[_] job results in context-per-jvm=false configuration, pending potential future enhancements to support this in context-per-jvm=true mode.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If we encounter a data type that is not supported, then the entire result will be serialized to a string.&lt;/p&gt; &#xA;&lt;h3&gt;HTTP Override&lt;/h3&gt; &#xA;&lt;p&gt;Spark Job Server offers HTTP override functionality. Often reverse proxies and firewall implement access limitations to, for example, DELETE and PUT requests. HTTP override allows overcoming these limitations by wrapping, for example, a DELETE request into a POST request.&lt;/p&gt; &#xA;&lt;p&gt;Requesting the destruction of a context can be accomplished through HTTP override using the following syntax:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ curl -X POST &#34;localhost:8090/contexts/test_context?_method=DELETE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, a DELETE request is passed to Spark Job Server &#34;through&#34; a POST request.&lt;/p&gt; &#xA;&lt;h2&gt;Clients&lt;/h2&gt; &#xA;&lt;p&gt;Spark Jobserver project has a &lt;a href=&#34;https://github.com/spark-jobserver/python-sjsclient&#34;&gt;python binding&lt;/a&gt; package. This can be used to quickly develop python applications that can interact with Spark Jobserver programmatically.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution and Development&lt;/h2&gt; &#xA;&lt;p&gt;Contributions via Github Pull Request are welcome. Please start by taking a look at the &lt;a href=&#34;https://raw.githubusercontent.com/spark-jobserver/spark-jobserver/master/doc/contribution-guidelines.md&#34;&gt;contribution guidelines&lt;/a&gt; and check the TODO for some contribution ideas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you need to build with a specific scala version use ++x.xx.x followed by the regular command, for instance: &lt;code&gt;sbt ++2.12.12 job-server/compile&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;From the &#34;master&#34; project, please run &#34;test&#34; to ensure nothing is broken. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You may need to set &lt;code&gt;SPARK_LOCAL_IP&lt;/code&gt; to &lt;code&gt;localhost&lt;/code&gt; to ensure Akka port can bind successfully&lt;/li&gt; &#xA;   &lt;li&gt;Note for Windows users: very few tests fail on Windows. Thus, run &lt;code&gt;testOnly -- -l WindowsIgnore&lt;/code&gt; from SBT shell to ignore them.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Logging for tests goes to &#34;job-server-test.log&#34;. To see test logging in console also, add the following to your log4j.properties (&lt;code&gt;job-server/src/test/resources/log4j.properties&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-$xslt&#34;&gt;log4j.rootLogger=INFO, LOGFILE, console&#xA;&#xA;log4j.appender.console=org.apache.log4j.ConsoleAppender&#xA;log4j.appender.console.target=System.err&#xA;log4j.appender.console.layout=org.apache.log4j.PatternLayout&#xA;log4j.appender.console.layout.ConversionPattern=[%d] %-5p %.26c [%X{testName}] [%X{akkaSource}] - %m%n&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;sbt clean coverage test&lt;/code&gt; to check the code coverage and improve it. You can generate reports by running &lt;code&gt;sbt coverageReport&lt;/code&gt; or &lt;code&gt;sbt coverageAggregate&lt;/code&gt; for the full overview. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Windows users: run &lt;code&gt;; coverage ; testOnly -- -l WindowsIgnore ; coverageReport&lt;/code&gt; from SBT shell.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Please run scalastyle to ensure your code changes don&#39;t break the style guide.&lt;/li&gt; &#xA; &lt;li&gt;Do &#34;reStart&#34; from SBT for quick restarts of the job server process&lt;/li&gt; &#xA; &lt;li&gt;Please update the g8 template if you change the SparkJob API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Profiling software generously provided by &lt;img src=&#34;https://www.yourkit.com/images/yklogo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;YourKit supports open source projects with its full-featured &lt;a href=&#34;https://www.yourkit.com/java/profiler/index.jsp&#34;&gt;Java Profiler&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;For user/dev questions, we are using google group for discussions: &lt;a href=&#34;https://groups.google.com/forum/#!forum/spark-jobserver&#34;&gt;https://groups.google.com/forum/#!forum/spark-jobserver&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please report bugs/problems to: &lt;a href=&#34;https://github.com/spark-jobserver/spark-jobserver/issues&#34;&gt;https://github.com/spark-jobserver/spark-jobserver/issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache 2.0, see LICENSE.md&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;More debugging for classpath issues&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add Swagger support. See the spray-swagger project.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Implement an interactive SQL window. See: &lt;a href=&#34;https://github.com/adatao/spark-admin&#34;&gt;spark-admin&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Stream the current job progress via a Listener&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add routes to return stage info for a job. Persist it via DAO so that we can always retrieve stage / performance info even for historical jobs. This would be pretty kickass.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>