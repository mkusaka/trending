<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-05T01:36:57Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nixiesearch/nixiesearch</title>
    <updated>2024-10-05T01:36:57Z</updated>
    <id>tag:github.com,2024-10-05:/nixiesearch/nixiesearch</id>
    <link href="https://github.com/nixiesearch/nixiesearch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hybrid search engine, combining best features of text and semantic search worlds&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nixiesearch: batteries included search engine&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nixiesearch/nixiesearch/actions&#34;&gt;&lt;img src=&#34;https://github.com/nixiesearch/nixiesearch/workflows/Tests/badge.svg?sanitize=true&#34; alt=&#34;CI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache2-green.svg?sanitize=true&#34; alt=&#34;License: Apache 2&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/nixiesearch/nixiesearch&#34; alt=&#34;Last commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/nixiesearch/nixiesearch&#34; alt=&#34;Last release&#34;&gt; &lt;a href=&#34;https://communityinviter.com/apps/nixiesearch/nixiesearch&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join%20the%20community-blue?logo=slack&amp;amp;style=social&#34; alt=&#34;Join our slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://demo.nixiesearch.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/visit-demo-blue&#34; alt=&#34;Visit demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Nixiesearch?&lt;/h2&gt; &#xA;&lt;p&gt;Nixiesearch is a &lt;strong&gt;hybrid search engine&lt;/strong&gt; that fine-tunes to your data.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Designed to be cloud-native with &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/distributed/persistence/s3.md&#34;&gt;S3-compatible index persistence&lt;/a&gt;. Distributed with stateless searchers and scale-to-zero. No more &lt;code&gt;status: red&lt;/code&gt; on your cluster.&lt;/li&gt; &#xA; &lt;li&gt;Built on top of battle-tested &lt;a href=&#34;https://lucene.apache.org&#34;&gt;Apache Lucene&lt;/a&gt; library: &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/reference/languages.md&#34;&gt;39 languages&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/facet.md&#34;&gt;facets&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/filter.md&#34;&gt;advanced filters&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/autocomplete/index.md&#34;&gt;autocomplete suggestions&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/sort.md&#34;&gt;sorting&lt;/a&gt; out of the box.&lt;/li&gt; &#xA; &lt;li&gt;Batteries included: &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/rag.md&#34;&gt;RAG queries&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/reference/models/index.md&#34;&gt;vector search&lt;/a&gt; within a &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/standalone.md&#34;&gt;single container&lt;/a&gt; with a fully local CPU and &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/gpu.md&#34;&gt;GPU inference&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Can learn the intent of a visitor by &lt;a href=&#34;https://github.com/nixiesearch/nixietune&#34;&gt;fine-tuning an embedding model&lt;/a&gt; to your data. Is &#34;ketchup&#34; relevant to a &#34;tomato&#34; query? It depends, but Nixiesearch can predict that from past user behavior.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Want to learn more? Go straight to the &lt;a href=&#34;https://www.nixiesearch.ai/quickstart/&#34;&gt;quickstart&lt;/a&gt; and check out &lt;a href=&#34;https://demo.nixiesearch.ai&#34;&gt;the live demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Why Nixiesearch?&lt;/h3&gt; &#xA;&lt;p&gt;Unlike Elastic/SOLR:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Can run over &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/distributed/persistence/s3.md&#34;&gt;S3-compatible block storage&lt;/a&gt;: Rapid auto-scaling (even down to zero!) and much easier operations (your index is just a directory in S3 bucket!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/rag.md&#34;&gt;RAG&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/query.md&#34;&gt;text&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/indexing/types/images.md&#34;&gt;image&lt;/a&gt; embeddings are first class search methods: no need for complex hand-written indexing pipelines.&lt;/li&gt; &#xA; &lt;li&gt;All LLM inference &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/reference/models/index.md&#34;&gt;can be run fully locally&lt;/a&gt; on CPU and &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/gpu.md&#34;&gt;GPU&lt;/a&gt;, no need to send all your queries and private documents to OpenAI API. But &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/reference/models/index.md&#34;&gt;you can&lt;/a&gt;, if you wish.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Unlike other vector search engines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supports &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/facet.md&#34;&gt;facets&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/filter.md&#34;&gt;rich filtering&lt;/a&gt;, sorting and &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/autocomplete/index.md&#34;&gt;autocomplete&lt;/a&gt;&lt;/strong&gt;: things you got used to in traditional search engines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text in, text out&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/reference/models/embedding.md&#34;&gt;text embedding&lt;/a&gt; is handled by the search engine, not by you.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exact-match search&lt;/strong&gt;: Nixiesearch is a hybrid retrieval engine searching over terms and embeddings. Your brand or SKU search queries will return what you expect, and not what the LLM hallucinates about.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The project is in active development and does not yet have backwards compatibility for configuration and data. Stay tuned and &lt;a href=&#34;https://www.metarank.ai/contact&#34;&gt;reach out&lt;/a&gt; if you want to try it!&lt;/p&gt; &#xA;&lt;h3&gt;Why NOT Nixiesearch?&lt;/h3&gt; &#xA;&lt;p&gt;Nixiesearch has the following design limitations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Does not support sharding&lt;/strong&gt;: sharding requires multi-node coordination and consensus, and we would like to avoid having any distributed state in the cluster - at least in the v1. If you plan to use Nixiesearch for searching 1TB of logs, please don&#39;t: consider &lt;a href=&#34;https://www.elastic.co/elastic-stack&#34;&gt;ELK&lt;/a&gt; or &lt;a href=&#34;https://github.com/quickwit-oss/quickwit&#34;&gt;Quickwit&lt;/a&gt; as better alternatives.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query language is &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/query.md&#34;&gt;simple&lt;/a&gt;&lt;/strong&gt;: supporting analytical queries over deeply-nested documents is out of scope for the project. Nixiesearch is about consumer-facing search, and for analytical cases consider using &lt;a href=&#34;https://github.com/ClickHouse/ClickHouse&#34;&gt;Clickhouse&lt;/a&gt; or &lt;a href=&#34;https://www.snowflake.com/en/&#34;&gt;Snowflake&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Get the sample &lt;a href=&#34;https://github.com/metarank/msrd&#34;&gt;MSRD: Movie Search Ranking Dataset&lt;/a&gt; dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -o movies.jsonl.gz https://nixiesearch.ai/data/movies.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current&#xA;                                 Dload  Upload   Total   Spent    Left  Speed&#xA;100   162  100   162    0     0   3636      0 --:--:-- --:--:-- --:--:--  3681&#xA;100 32085  100 32085    0     0   226k      0 --:--:-- --:--:-- --:--:--  226k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create an index mapping for &lt;code&gt;movies&lt;/code&gt; index in a file &lt;code&gt;config.yml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;inference:&#xA;  embedding:&#xA;    e5-small:&#xA;      provider: onnx&#xA;      model: nixiesearch/e5-small-v2-onnx&#xA;      prompt:&#xA;        query: &#34;query: &#34;&#xA;        doc: &#34;passage: &#34;&#xA;schema:&#xA;  movies: # index name&#xA;    fields:&#xA;      title: # field name&#xA;        type: text&#xA;        search: &#xA;          type: hybrid&#xA;          model: e5-small&#xA;        language: en # language is needed for lexical search&#xA;        suggest: true&#xA;      overview:&#xA;        type: text&#xA;        search: &#xA;          type: hybrid&#xA;          model: e5-small&#xA;        language: en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the Nixiesearch &lt;a href=&#34;https://hub.docker.com/r/nixiesearch/nixiesearch&#34;&gt;docker container&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -itp 8080:8080 -v .:/data nixiesearch/nixiesearch:latest standalone -c /data/config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;a.nixiesearch.index.sync.LocalIndex$ - Local index movies opened&#xA;ai.nixiesearch.index.Searcher$ - opening index movies&#xA;a.n.main.subcommands.StandaloneMode$ - ███╗   ██╗██╗██╗  ██╗██╗███████╗███████╗███████╗ █████╗ ██████╗  ██████╗██╗  ██╗&#xA;a.n.main.subcommands.StandaloneMode$ - ████╗  ██║██║╚██╗██╔╝██║██╔════╝██╔════╝██╔════╝██╔══██╗██╔══██╗██╔════╝██║  ██║&#xA;a.n.main.subcommands.StandaloneMode$ - ██╔██╗ ██║██║ ╚███╔╝ ██║█████╗  ███████╗█████╗  ███████║██████╔╝██║     ███████║&#xA;a.n.main.subcommands.StandaloneMode$ - ██║╚██╗██║██║ ██╔██╗ ██║██╔══╝  ╚════██║██╔══╝  ██╔══██║██╔══██╗██║     ██╔══██║&#xA;a.n.main.subcommands.StandaloneMode$ - ██║ ╚████║██║██╔╝ ██╗██║███████╗███████║███████╗██║  ██║██║  ██║╚██████╗██║  ██║&#xA;a.n.main.subcommands.StandaloneMode$ - ╚═╝  ╚═══╝╚═╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚══════╝╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝╚═╝  ╚═╝&#xA;a.n.main.subcommands.StandaloneMode$ -                                                                                &#xA;o.h.ember.server.EmberServerBuilder - Ember-Server service bound to address: [::]:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build an index for a hybrid search:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -XPUT -d @movies.jsonl http://localhost:8080/movies/_index&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;result&#34;:&#34;created&#34;,&#34;took&#34;:8256}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Send the search query:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -XPOST -d &#39;{&#34;query&#34;: {&#34;match&#34;: {&#34;title&#34;:&#34;matrix&#34;}},&#34;fields&#34;: [&#34;title&#34;], &#34;size&#34;:3}&#39;\&#xA;   http://localhost:8080/movies/_search&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;took&#34;: 1,&#xA;  &#34;hits&#34;: [&#xA;    {&#xA;      &#34;_id&#34;: &#34;605&#34;,&#xA;      &#34;title&#34;: &#34;The Matrix Revolutions&#34;,&#xA;      &#34;_score&#34;: 0.016666668&#xA;    },&#xA;    {&#xA;      &#34;_id&#34;: &#34;604&#34;,&#xA;      &#34;title&#34;: &#34;The Matrix Reloaded&#34;,&#xA;      &#34;_score&#34;: 0.016393442&#xA;    },&#xA;    {&#xA;      &#34;_id&#34;: &#34;624860&#34;,&#xA;      &#34;title&#34;: &#34;The Matrix Resurrections&#34;,&#xA;      &#34;_score&#34;: 0.016129032&#xA;    }&#xA;  ],&#xA;  &#34;aggs&#34;: {},&#xA;  &#34;ts&#34;: 1722441735886&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also open &lt;code&gt;http://localhost:8080/_ui&lt;/code&gt; in your web browser for a basic web UI:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.nixiesearch.ai/img/webui.png&#34; alt=&#34;web ui&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details, see a complete &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/quickstart.md&#34;&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Design&lt;/h2&gt; &#xA;&lt;p&gt;Nixiesearch is inspired by an Amazon search engine design described in a talk &lt;a href=&#34;https://www.youtube.com/watch?v=EkkzSLstSAE&#34;&gt;E-Commerce search at scale on Apache Lucene&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.nixiesearch.ai/img/arch.png&#34; alt=&#34;NS design diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Compared to traditional search engines like Elasticsearch/Solr:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Independent stateful indexer and stateless search backends&lt;/strong&gt;: with index sync happening via &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/distributed/persistence/s3.md&#34;&gt;S3-compatible block storage&lt;/a&gt;. No more red index status and cluster split-brains due to indexer overload.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pull-based indexing&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/distributed/indexing/kafka.md&#34;&gt;pull updated documents&lt;/a&gt; right from &lt;a href=&#34;https://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt; in real-time, no need for separate indexing ETL jobs with limited throughput.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Nixiesearch uses &lt;a href=&#34;https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf&#34;&gt;RRF&lt;/a&gt; for combining text and neural search results.&lt;/p&gt; &#xA;&lt;h3&gt;Hybrid search&lt;/h3&gt; &#xA;&lt;p&gt;Nixiesearch transparently uses two Lucene-powered search indices for both lexical and semantic search, combining search results into a single list with &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/index.md#hybrid-search-with-reciprocal-rank-fusion&#34;&gt;Reciprocal Rank Fusion&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/img/hybridsearch.png&#34; alt=&#34;RRF&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Compared to just a single lexical or semantic search approach:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;hybrid search allows combining best of two worlds: being able to &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/index.md#search&#34;&gt;perform exact match searches&lt;/a&gt; over keywords, but at the same time retrieving documents with similar context.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/features/search/index.md#hybrid-search-with-reciprocal-rank-fusion&#34;&gt;RRF ranking&lt;/a&gt; requires almost zero configuration for reasonably good results while mixing search results from different indices.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;!!! note&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;This feature is in development and planned for the v0.3 release.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Embedding-based semantic search is a great way to increase search recall: it will match all the similar documents based on the search query even when there are no keyword matches. But in practice a customer expects good enough precision of top-N results, and a good balance between precision and recall is important.&lt;/p&gt; &#xA;&lt;p&gt;Nixiesearch can incorporate explicit customer feedback about search relevance directly into the embedding LLM by &lt;a href=&#34;https://github.com/nixiesearch/nixietune&#34;&gt;fine-tuning it&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/img/fine-tuned.png&#34; alt=&#34;fine-tuning&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Two main sources of relevance labels can be used as a customer feedback:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explicit relevance judgments made by human raters. You can use open-source tools like &lt;a href=&#34;https://quepid.com/&#34;&gt;Quepid&lt;/a&gt; and SaaS platforms like &lt;a href=&#34;https://toloka.ai/search-relevance/&#34;&gt;Toloka.ai&lt;/a&gt; and &lt;a href=&#34;https://www.mturk.com/&#34;&gt;Amazon MTurk&lt;/a&gt; to build such datasets.&lt;/li&gt; &#xA; &lt;li&gt;Implicit judgments made from aggregating real customer behavior, based on query-document CTR and Conversion rates.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Using customer feedback, you can teach the underlying LLM which documents are truly relevant in your particular case.&lt;/p&gt; &#xA;&lt;h3&gt;Pull-based indexing&lt;/h3&gt; &#xA;&lt;p&gt;Existing search engines require you to build a satellite indexing app, which pushes documents to the search engine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;indexer should maintain back-pressure not to write too many documents which can overflow the internal indexing queue. Queue overflow may cause a search cluster node crash and affect normal search operations. But writing too few documents means a suboptimal indexing throughput.&lt;/li&gt; &#xA; &lt;li&gt;you should also implement full re-indexing capability to re-process all the documents in a case of incompatible index mapping change.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Architecturally, your app pushes documents to the search engine and maintains the best rate.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/img/pullpush.png&#34; alt=&#34;pull-push indexing&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In comparison, Nixiesearch is a &lt;a href=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/deployment/distributed/indexing/index.md&#34;&gt;pull-based system&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;it pulls the next document batch immediately when indexing resources become available. This approach allows to have a perfect resource utilization and the most optimal indexing throughput.&lt;/li&gt; &#xA; &lt;li&gt;it does not have an internal indexing queue, so there is no way to overflow it.&lt;/li&gt; &#xA; &lt;li&gt;no need for a specialized indexing app with complicated back-pressure logic. You can use Kafka topic or a set of files on S3 block storage as a source of documents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;!!! note&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Nixiesearch can emulate a push-based indexing behavior using a traditional [indexing API](features/indexing/api.md), but a pull-based approach is recommended.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;S3 index storage and auto-scaling&lt;/h3&gt; &#xA;&lt;p&gt;Distributed cluster state is the most complicated part of existing search engines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Re-balance of a large index is an expensive and fragile operation due to large amount of data shuffled through the network.&lt;/li&gt; &#xA; &lt;li&gt;A subtle bug in consensus algorithm may result in &lt;a href=&#34;https://www.slideshare.net/DilumBandara/cap-theorem-and-split-brain-syndrome&#34;&gt;split-brain&lt;/a&gt; scenarios and incur data loss.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nixiesearch/nixiesearch/master/img/s3-index.png&#34; alt=&#34;s3 index&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Nixiesearch uses an S3-compatible block storage (like &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;AWS S3&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/storage&#34;&gt;Google GCS&lt;/a&gt; and &lt;a href=&#34;https://azure.microsoft.com/en-us/products/storage/blobs&#34;&gt;Azure Blob Storage&lt;/a&gt;) for index synchronization, which greatly simplifies cloud operations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Search replicas can now be spawned immediately, as there is no need for node-to-node data transfers. No need to have persistent volumes for your k8s Pods. Complete index can be loaded from the object storage, allowing you to have &lt;strong&gt;seamless load-based auto-scaling&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;As indexer runs separately from Search replicas, it is possible to have a &lt;strong&gt;scale-to-zero autoscaling&lt;/strong&gt;: search backend can be spawned as a lambda function only when there is an incoming search request.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;h3&gt;Lack of sharding&lt;/h3&gt; &#xA;&lt;p&gt;Nixiesearch does not support index sharding out-of-the-box (but nothing stops you from implementing sharding client-side over multiple Nixiesearch clusters).&lt;/p&gt; &#xA;&lt;p&gt;Main reason for this design decision is much simplified search replica coordination during auto-scaling: each replica always contains a complete copy of the index, there is no need to maintain a specific set of shard-replicas while up-down scaling.&lt;/p&gt; &#xA;&lt;h3&gt;Low indexing throughput&lt;/h3&gt; &#xA;&lt;p&gt;As Nixiesearch computes text embeddings on a CPU by default, indexing large sets of documents is a resource-intensive task - as you need to embed all of them.&lt;/p&gt; &#xA;&lt;p&gt;Nixiesearch implements multiple technical optimizations to make indexing throughput higher (like using ONNX runtime for running LLMs and caching embeddings for frequent text strings), but still expect a throughput of 100-500 documents per second.&lt;/p&gt; &#xA;&lt;h3&gt;GPU needed for fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;Fine-tuning adapts an LLM to a specific training dataset, which requires running tens of thousands of forward-backward passes over a complete dataset. Fine-tuning can take 1-2 hours on a GPU, and can be unreasonably slow even on a fastest CPU.&lt;/p&gt; &#xA;&lt;p&gt;In practice, CPU fine-tuning is 30x-50x slower than GPU one and can take multiple days instead of 1-2 hours.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is released under the Apache 2.0 license, as specified in the &lt;a href=&#34;https://github.com/nixiesearch/nixiesearch/raw/master/LICENSE&#34;&gt;License&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>