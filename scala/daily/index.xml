<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-08T01:41:44Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LucaCanali/sparkMeasure</title>
    <updated>2022-09-08T01:41:44Z</updated>
    <id>tag:github.com,2022-09-08:/LucaCanali/sparkMeasure</id>
    <link href="https://github.com/LucaCanali/sparkMeasure" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the development repository for sparkMeasure, a tool for performance troubleshooting of Apache Spark workloads. It simplifies the collection and analysis of Spark task and stage metrics data.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sparkMeasure&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/LucaCanali/sparkMeasure/workflows/sparkMeasure%20CI/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;sparkMeasure CI&#34;&gt; &lt;a href=&#34;https://maven-badges.herokuapp.com/maven-central/ch.cern.sparkmeasure/spark-measure_2.12&#34;&gt;&lt;img src=&#34;https://maven-badges.herokuapp.com/maven-central/ch.cern.sparkmeasure/spark-measure_2.12/badge.svg?sanitize=true&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SparkMeasure is a tool for performance troubleshooting of Apache Spark jobs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why:&lt;/strong&gt; Troubleshooting and understanding the root causes of issues and errors from Spark jobs is often complicated.&lt;br&gt; SparkMeasure simplifies the collection and analysis of Spark performance metrics.&lt;br&gt; Use sparkMeasure for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;troubleshooting &lt;strong&gt;interactive&lt;/strong&gt; Spark workloads (use with notebooks and spark shell/pyspark).&lt;/li&gt; &#xA; &lt;li&gt;troubleshooting &lt;strong&gt;batch&lt;/strong&gt; jobs using the &#34;flight recorder&#34; mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;monitoring&lt;/strong&gt;, by sinking metrics to external systems like InfluxDB, Apache Kafka, Prometheus gateway.&lt;/li&gt; &#xA; &lt;li&gt;comparing Spark jobs&#39; execution metrics with evolving configurations or code (for &lt;strong&gt;development&lt;/strong&gt;, CI/CD, etc).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;learning&lt;/strong&gt;: it is a working example of how to use Spark Listeners for collecting Spark task metrics data.&lt;/li&gt; &#xA; &lt;li&gt;Link to &lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/#one-tool-for-different-use-cases-links-to-documentation-and-examples&#34;&gt;documentation, examples and API reference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compatibility:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SparkMeasure works with Spark 3.x and Spark 2.x &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use it from Scala, Python, and Java&lt;/li&gt; &#xA;   &lt;li&gt;For Scala 2.12 and 2.13, please use the latest version, 0.21&lt;/li&gt; &#xA;   &lt;li&gt;For Scala 2.11, with Spark 2.4 or 2.3, use version 0.19&lt;/li&gt; &#xA;   &lt;li&gt;For Spark 2.1 and 2.2, use version 0.16&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Main author and contact: &lt;a href=&#34;mailto:Luca.Canali@cern.ch&#34;&gt;Luca.Canali@cern.ch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Related work: &lt;strong&gt;&lt;a href=&#34;https://github.com/cerndb/spark-dashboard&#34;&gt;Spark Dashboard&lt;/a&gt;&lt;/strong&gt; is meant to streamline the deployment of an Apache Spark Performance Dashboard using containers technology. It is implemented using Grafana, InfluxDB, and the &lt;a href=&#34;https://spark.apache.org/docs/latest/monitoring.html#metrics&#34;&gt;Spark metrics system&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Getting started with sparkMeasure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Spark 3.x and 2.4 with Scala 2.12:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Scala:&lt;/strong&gt; &lt;code&gt;bin/spark-shell --packages ch.cern.sparkmeasure:spark-measure_2.12:0.21&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Python:&lt;/strong&gt; &lt;code&gt;bin/pyspark --packages ch.cern.sparkmeasure:spark-measure_2.12:0.21&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;note: you also need &lt;code&gt;pip install sparkmeasure&lt;/code&gt; to get the &lt;a href=&#34;https://pypi.org/project/sparkmeasure/&#34;&gt;Python wrapper API&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Spark 3.3.0 and higher with Scala 2.13:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Scala: &lt;code&gt;bin/spark-shell --packages ch.cern.sparkmeasure:spark-measure_2.13:0.21&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Python: &lt;code&gt;bin/pyspark --packages ch.cern.sparkmeasure:spark-measure_2.13:0.21&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;note: &lt;code&gt;pip install sparkmeasure&lt;/code&gt; to get the Python wrapper API&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Spark 2.4 and 2.3 with Scala 2.11:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Scala: &lt;code&gt;bin/spark-shell --packages ch.cern.sparkmeasure:spark-measure_2.11:0.19&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Python: &lt;code&gt;bin/pyspark --packages ch.cern.sparkmeasure:spark-measure_2.11:0.19&lt;/code&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;note: &lt;code&gt;pip install sparkmeasure&lt;/code&gt; to get the Python wrapper API&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Notes:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See the list of sparkMeasure versions available on &lt;a href=&#34;https://mvnrepository.com/artifact/ch.cern.sparkmeasure/spark-measure&#34;&gt;Maven Central&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You can find the latest jars built as artifacts in &lt;a href=&#34;https://github.com/LucaCanali/sparkMeasure/actions&#34;&gt;GitHub actions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Bleeding edge: build sparkMeasure jar from master using sbt: &lt;code&gt;sbt +package&lt;/code&gt; and use &lt;code&gt;--jars&lt;/code&gt; with the jar just built.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Examples of interactive use of sparkMeasure, using notebooks and CLI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/LucaCanali/sparkMeasure/blob/master/examples/SparkMeasure_Jupyter_Colab_Example.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/googlecolab/open_in_colab/master/images/icon128.png&#34; height=&#34;50&#34;&gt; Jupyter notebook on Google Colab Research&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/examples/SparkMeasure_Jupyter_Python_getting_started.ipynb&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/250px-Jupyter_logo.svg.png&#34; height=&#34;50&#34;&gt; Local Python/Jupyter Notebook&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2061385495597958/2729765977711377/442806354506758/latest.html&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png&#34; height=&#34;40&#34;&gt; Scala notebook on Databricks&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2061385495597958/3856830937265976/442806354506758/latest.html&#34;&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png&#34; height=&#34;40&#34;&gt; Python notebook on Databricks&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Stage-level metrics from command line:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Scala CLI&#xA;bin/spark-shell --packages ch.cern.sparkmeasure:spark-measure_2.12:0.21&#xA;&#xA;val stageMetrics = ch.cern.sparkmeasure.StageMetrics(spark)&#xA;stageMetrics.runAndMeasure(spark.sql(&#34;select count(*) from range(1000) cross join range(1000) cross join range(1000)&#34;).show())&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;# Python CLI&#xA;pip install sparkmeasure&#xA;bin/pyspark --packages ch.cern.sparkmeasure:spark-measure_2.12:0.21&#xA;&#xA;from sparkmeasure import StageMetrics&#xA;stagemetrics = StageMetrics(spark)&#xA;stagemetrics.runandmeasure(globals(), &#39;spark.sql(&#34;select count(*) from range(1000) cross join range(1000) cross join range(1000)&#34;).show()&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The output should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Scheduling mode = FIFO&#xA;Spark Context default degree of parallelism = 8&#xA;&#xA;Aggregated Spark stage metrics:&#xA;numStages =&amp;gt; 3&#xA;numTasks =&amp;gt; 17&#xA;elapsedTime =&amp;gt; 1291 (1 s)&#xA;stageDuration =&amp;gt; 1058 (1 s)&#xA;executorRunTime =&amp;gt; 2774 (3 s)&#xA;executorCpuTime =&amp;gt; 2004 (2 s)&#xA;executorDeserializeTime =&amp;gt; 2868 (3 s)&#xA;executorDeserializeCpuTime =&amp;gt; 1051 (1 s)&#xA;resultSerializationTime =&amp;gt; 5 (5 ms)&#xA;jvmGCTime =&amp;gt; 88 (88 ms)&#xA;shuffleFetchWaitTime =&amp;gt; 0 (0 ms)&#xA;shuffleWriteTime =&amp;gt; 16 (16 ms)&#xA;resultSize =&amp;gt; 16091 (15.0 KB)&#xA;diskBytesSpilled =&amp;gt; 0 (0 Bytes)&#xA;memoryBytesSpilled =&amp;gt; 0 (0 Bytes)&#xA;peakExecutionMemory =&amp;gt; 0&#xA;recordsRead =&amp;gt; 2000&#xA;bytesRead =&amp;gt; 0 (0 Bytes)&#xA;recordsWritten =&amp;gt; 0&#xA;bytesWritten =&amp;gt; 0 (0 Bytes)&#xA;shuffleRecordsRead =&amp;gt; 8&#xA;shuffleTotalBlocksFetched =&amp;gt; 8&#xA;shuffleLocalBlocksFetched =&amp;gt; 8&#xA;shuffleRemoteBlocksFetched =&amp;gt; 0&#xA;shuffleTotalBytesRead =&amp;gt; 472 (472 Bytes)&#xA;shuffleLocalBytesRead =&amp;gt; 472 (472 Bytes)&#xA;shuffleRemoteBytesRead =&amp;gt; 0 (0 Bytes)&#xA;shuffleRemoteBytesReadToDisk =&amp;gt; 0 (0 Bytes)&#xA;shuffleBytesWritten =&amp;gt; 472 (472 Bytes)&#xA;shuffleRecordsWritten =&amp;gt; 8&#xA;&#xA;Stages and their duration:&#xA;Stage 0 duration =&amp;gt; 593 (0.6 s)&#xA;Stage 1 duration =&amp;gt; 416 (0.4 s)&#xA;Stage 3 duration =&amp;gt; 49 (49 ms)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stage metrics collection mode has an optional memory report command &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;new in sparkMeasure since version 0.21, use with Spark versions 3.1 and above&lt;/li&gt; &#xA;   &lt;li&gt;note: this report requires per-stage memory (executor metrics) data which is sent by the executors at each heartbeat to the driver, there could be a small delay or the order of a few seconds between the end of the job and the time the last metrics value is received.&lt;/li&gt; &#xA;   &lt;li&gt;If you receive the error message java.util.NoSuchElementException: key not found, retry running the report after waiting for a few seconds.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;(scala)&amp;gt; stageMetrics.printMemoryReport&#xA;(python)&amp;gt; stagemetrics.print_memory_report()&#xA;&#xA;Additional stage-level executor metrics (memory usasge info):&#xA;&#xA;Stage 0 JVMHeapMemory maxVal bytes =&amp;gt; 322888344 (307.9 MB)&#xA;Stage 0 OnHeapExecutionMemory maxVal bytes =&amp;gt; 0 (0 Bytes)&#xA;Stage 1 JVMHeapMemory maxVal bytes =&amp;gt; 322888344 (307.9 MB)&#xA;Stage 1 OnHeapExecutionMemory maxVal bytes =&amp;gt; 0 (0 Bytes)&#xA;Stage 3 JVMHeapMemory maxVal bytes =&amp;gt; 322888344 (307.9 MB)&#xA;Stage 3 OnHeapExecutionMemory maxVal bytes =&amp;gt; 0 (0 Bytes)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Task-level metrics, from command line: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;this is slightly different from the example above as it collects metrics at the Task-level rather than Stage-level&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;# Scala CLI&#xA;bin/spark-shell --packages ch.cern.sparkmeasure:spark-measure_2.12:0.21&#xA;&#xA;val taskMetrics = ch.cern.sparkmeasure.TaskMetrics(spark)&#xA;taskMetrics.runAndMeasure(spark.sql(&#34;select count(*) from range(1000) cross join range(1000) cross join range(1000)&#34;).show())&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code&gt;# Python CLI&#xA;pip install sparkmeasure&#xA;bin/pyspark --packages ch.cern.sparkmeasure:spark-measure_2.12:0.21&#xA;&#xA;from sparkmeasure import TaskMetrics&#xA;taskmetrics = TaskMetrics(spark)&#xA;taskmetrics.runandmeasure(globals(), &#39;spark.sql(&#34;select count(*) from range(1000) cross join range(1000) cross join range(1000)&#34;).show()&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;One tool for different use cases, links to documentation and examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive mode&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use sparkMeasure to collect and analyze Spark workload metrics in interactive mode when working with shell or notebook environments, as &lt;code&gt;spark-shell&lt;/code&gt; (Scala), &lt;code&gt;PySpark&lt;/code&gt; (Python) and/or from &lt;code&gt;jupyter-notebook&lt;/code&gt;. You can use Python and Scala kernels.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Scala_shell_and_notebooks.md&#34;&gt;Scala shell and notebooks&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Python_shell_and_Jupyter.md&#34;&gt;PySpark and Jupyter notebooks&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Batch and code instrumentation&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Instrument your code with the sparkMeasure API, for collecting, saving, and analyzing Spark workload metrics data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Instrument_Scala_code.md&#34;&gt;Instrument Scala code&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Instrument_Python_code.md&#34;&gt;Instrument Python code&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flight Recorder mode&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SparkMeasure in flight recorder will collect metrics transparently, without any need for you to change your code.&lt;/li&gt; &#xA;   &lt;li&gt;Metrics can be saved to a file, locally or to a Hadoop-compliant filesystem&lt;/li&gt; &#xA;   &lt;li&gt;or you can write metrics in near-realtime to an InfluxDB instance or to Apache Kafka&lt;/li&gt; &#xA;   &lt;li&gt;More details: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Flight_recorder_mode_FileSink.md&#34;&gt;Flight Recorder mode with file sink&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Flight_recorder_mode_InfluxDBSink.md&#34;&gt;Flight Recorder mode with InfluxDB sink&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Flight_recorder_mode_KafkaSink.md&#34;&gt;Flight Recorder mode with Apache Kafka sink&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Reference_SparkMeasure_API_and_Configs.md&#34;&gt;SparkMeasure API and configuration&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Additional documentation&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Presentations at Spark/Data+AI Summit: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://databricks.com/session_eu19/performance-troubleshooting-using-apache-spark-metrics&#34;&gt;Performance Troubleshooting Using Apache Spark Metrics&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;http://canali.web.cern.ch/docs/Spark_Summit_2017EU_Performance_Luca_Canali_CERN.pdf&#34;&gt;Apache Spark Performance Troubleshooting at Scale, Challenges, Tools, and Methodologies&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Blog articles: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://db-blog.web.cern.ch/blog/luca-canali/2018-08-sparkmeasure-tool-performance-troubleshooting-apache-spark-workloads&#34;&gt;2018: SparkMeasure, a tool for performance troubleshooting of Apache Spark workloads&lt;/a&gt;,&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;http://db-blog.web.cern.ch/blog/luca-canali/2017-03-measuring-apache-spark-workload-metrics-performance-troubleshooting&#34;&gt;2017: SparkMeasure blog post&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/TODO_and_issues.md&#34;&gt;TODO list and known issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Architecture diagram&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/sparkMeasure_architecture_diagram.png&#34; alt=&#34;sparkMeasure architecture diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Main concepts underlying sparkMeasure implementation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tool is based on the Spark Listener interface. Listeners transport Spark executor &lt;a href=&#34;https://github.com/LucaCanali/Miscellaneous/raw/master/Spark_Notes/Spark_TaskMetrics.md&#34;&gt;Task Metrics&lt;/a&gt; data from the executor to the driver. They are a standard part of Spark instrumentation, used by the Spark Web UI and History Server for example.&lt;/li&gt; &#xA; &lt;li&gt;The tool is built on multiple modules implemented as classes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;metrics collection and processing can be at the Stage-level or Task-level. The user chooses which mode to use with the API.&lt;/li&gt; &#xA;   &lt;li&gt;metrics are can be buffered into memory for real-time reporting or they can be dumped to an external system in the &#34;flight recorder mode&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;supported external systems are File Systems supported by the Hadoop API, InfluxDB, and Apache Kafka.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Metrics are flattened and collected into local memory structures in the driver (ListBuffer of a custom case class). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sparkMeasure in flight recorder mode with InfluxDB sink and Apache Kafka do not buffer, but rather write the collected metrics directly&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Metrics processing: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;metrics can be aggregated into a report showing the cumulative values (see example above)&lt;/li&gt; &#xA;   &lt;li&gt;metrics can be converted into a Spark DataFrame for custom querying&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Metrics data and reports can be saved for offline analysis.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;FAQ:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Why measuring performance with workload metrics instrumentation rather than just using execution time measurements?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Measuring elapsed time, treats your workload as &#34;a black box&#34; and most often does not allow you to understand the root causes of the performance regression.&lt;br&gt; With workload metrics you can (attempt to) go further in understanding with root cause analysis, bottleneck identification, and resource usage measurement.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;What are Apache Spark task metrics and what can I use them for?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Apache Spark measures several details of each task execution, including run time, CPU time, information on garbage collection time, shuffle metrics, and task I/O. See also Spark documentation for a description of the &lt;a href=&#34;https://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics&#34;&gt;Spark Task Metrics&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How is sparkMeasure different from Web UI/Spark History Server and EventLog?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sparkMeasure uses the same ListenerBus infrastructure used to collect data for the Web UI and Spark EventLog. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Spark collects metrics and other execution details and exposes them via the Web UI.&lt;/li&gt; &#xA;     &lt;li&gt;Notably Task execution metrics are also available through the &lt;a href=&#34;https://spark.apache.org/docs/latest/monitoring.html#rest-api&#34;&gt;REST API&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;In addition, Spark writes all details of the task execution in the EventLog file (see config of &lt;code&gt;spark.eventlog.enabled&lt;/code&gt; and &lt;code&gt;spark.eventLog.dir&lt;/code&gt;)&lt;/li&gt; &#xA;     &lt;li&gt;The EventLog is used by the Spark History server + other tools and programs can read and parse the EventLog file(s) for workload analysis and performance troubleshooting, see a &lt;a href=&#34;https://github.com/LucaCanali/Miscellaneous/raw/master/Spark_Notes/Spark_EventLog.md&#34;&gt;proof-of-concept example of reading the EventLog with Spark SQL&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;There are key differences that motivate this development: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;sparkMeasure can collect data at the stage completion-level, which is more lightweight than measuring all the tasks, in case you only need to compute aggregated performance metrics. When needed, sparkMeasure can also collect data at the task granularity level.&lt;/li&gt; &#xA;     &lt;li&gt;sparkMeasure has an API that makes it simple to add instrumentation/performance measurements in notebooks and application code.&lt;/li&gt; &#xA;     &lt;li&gt;sparkMeasure collects data in a flat structure, which makes it natural to use Spark SQL for workload data processing, which provides a simple and powerful interface&lt;/li&gt; &#xA;     &lt;li&gt;sparkMeasure can sink metrics data into external systems (Filesystem, InfluxDB, Apache Kafka)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;What are known limitations and gotchas?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sparkMeasure does not collect all the data available in the EventLog&lt;/li&gt; &#xA;   &lt;li&gt;See also the &lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/TODO_and_issues.md&#34;&gt;TODO and issues doc&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The currently available Spark task metrics can give you precious quantitative information on resources used by the executors, however there do not allow to fully perform time-based analysis of the workload performance, notably they do not expose the time spent doing I/O or network traffic.&lt;/li&gt; &#xA;   &lt;li&gt;Metrics are collected on the driver, which can be quickly become a bottleneck. This is true in general for ListenerBus instrumentation, in addition sparkMeasure in the current version buffers all data in the driver memory. The notable exception is the Flight recorder mode with InfluxDB and Apache Kafka sink, in this case metrics are directly sent to InfluxDB/Kafka&lt;/li&gt; &#xA;   &lt;li&gt;Task metrics values are collected by sparkMeasure only for successfully executed tasks. Note that resources used by failed tasks are not collected in the current version. The notable exception is with the Flight recorder mode with InfluxDB sink and with Apache Kafka.&lt;/li&gt; &#xA;   &lt;li&gt;Task metrics are collected by Spark executors running on the JVM, resources utilized outside the JVM are currently not directly accounted for (notably the resources used when running Python code inside the python.daemon in the case of PySpark).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When should I use Stage-level metrics and when should I use Task-level metrics?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use stage metrics whenever possible as they are much more lightweight. Collect metrics at the task granularity if you need the extra information, for example if you want to study effects of skew, long tails and task stragglers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How can I save/sink the collected metrics?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can print metrics data and reports to standard output or save them to files, using a locally mounted filesystem or a Hadoop compliant filesystem (including HDFS). Additionally, you can sink metrics to external systems (such as Prometheus). The Flight Recorder mode can sink to InfluxDB and Apache Kafka.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How can I process metrics data?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can use Spark to read the saved metrics data and perform further post-processing and analysis. See the also &lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/Notes_on_metrics_analysis.md&#34;&gt;Notes on metrics analysis&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How can I contribute to sparkMeasure?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SparkMeasure has already profited from users submitting PR contributions. Additional contributions are welcome. See the &lt;a href=&#34;https://raw.githubusercontent.com/LucaCanali/sparkMeasure/master/docs/TODO_and_issues.md&#34;&gt;TODO_and_issues list&lt;/a&gt; for a list of known issues and ideas on what you can contribute.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>