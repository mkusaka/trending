<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-15T01:42:29Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>emartech/scala-logger</title>
    <updated>2023-05-15T01:42:29Z</updated>
    <id>tag:github.com,2023-05-15:/emartech/scala-logger</id>
    <link href="https://github.com/emartech/scala-logger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Scala Logger &lt;a href=&#34;https://github.com/emartech/scala-logger/actions?query=workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/emartech/scala-logger/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://search.maven.org/search?q=g:%22com.emarsys%22%20AND%20a:%22scala-logger_2.12%22&#34;&gt;&lt;img src=&#34;https://img.shields.io/maven-central/v/com.emarsys/scala-logger_2.12.svg?label=Maven%20Central&#34; alt=&#34;Maven Central&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A simple logging wrapper library that provides scala idiomatic context propagation wrapping Logback logger.&lt;/p&gt; &#xA;&lt;h2&gt;Usage (for 0.8.0 and up)&lt;/h2&gt; &#xA;&lt;p&gt;Add to sbt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbt&#34;&gt;libraryDependencies += &#34;com.emarsys&#34; %% &#34;scala-logger&#34; % &#34;x.y.z&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The latest released version can be found on the maven badge above.&lt;/p&gt; &#xA;&lt;h3&gt;Logging creation&lt;/h3&gt; &#xA;&lt;h4&gt;Cats Effect 2.x&lt;/h4&gt; &#xA;&lt;p&gt;In order to use this library with cats effect series 2.x, add the &lt;code&gt;ce2&lt;/code&gt; interop module to your dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbt&#34;&gt;libraryDependencies += &#34;com.emarsys&#34; %% &#34;scala-logger-ce2&#34; % &#34;x.y.z&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You create a &lt;code&gt;Logging&lt;/code&gt; instance either for a generic &lt;code&gt;F[_]&lt;/code&gt; that implements &lt;code&gt;Sync&lt;/code&gt;, or specifically for &lt;code&gt;IO&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;implicit val ioLogging: IO[Logging[IO]] = CatsEffectLogging.createEffectLogger[IO](&#34;application&#34;)&#xA;implicit val fLogging: F[Logging[F]] = CatsEffectLogging.createEffectLogger[F](&#34;application&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In some scenarios (e.g. when using ReaderT aka. Logged), it is necessary to create the logger in a different effect then the one the logger will use. To do this, you can use the &lt;code&gt;CatsEffectLogging.createEffectLoggerG&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;implicit val ioLogging: F[Logging[Logged[F]]] =&#xA;  CatsEffectLogging.createEffectLoggerG[F, Logged[F]](&#34;application&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Cats Effect 3.x&lt;/h4&gt; &#xA;&lt;p&gt;There were several breaking changes in cats effect 3 which means a separate interop module is necessary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sbt&#34;&gt;libraryDependencies += &#34;com.emarsys&#34; %% &#34;scala-logger-ce3&#34; % &#34;x.y.z&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a &lt;code&gt;Logging&lt;/code&gt; instance, you can use the exact same methods as in the case of cats effect 2.&lt;/p&gt; &#xA;&lt;h4&gt;Sync logging&lt;/h4&gt; &#xA;&lt;p&gt;If you need to log in a context where it is not possible to provide a &lt;code&gt;Logging&lt;/code&gt; instance (e.g. in a JVM shutdown hook), you can use the unsafe logger utilities to create a global &lt;code&gt;Logging[Id]&lt;/code&gt; instance, which will be used to log instead. To access this instance, import the contents of the unsafe package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.emarsys.logger.unsafe._&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Future logging&lt;/h4&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;Given an implicit &lt;code&gt;Logging&lt;/code&gt; instance, you can use the expected functions in the &lt;code&gt;log&lt;/code&gt; package to log messages. Most log functions accept a &lt;code&gt;String&lt;/code&gt; message, a &lt;a href=&#34;https://raw.githubusercontent.com/emartech/scala-logger/master/#context-propagation&#34;&gt;LoggingContext&lt;/a&gt; and warn or error accepts a &lt;code&gt;Throwable&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.emarsys.logger.log&#xA;&#xA;implicit val logging: Logging[F] = ???&#xA;&#xA;val context = LoggingContext(&#34;main&#34;)&#xA;&#xA;log.info(&#34;Hello there.&#34;, context)&#xA;log.error(new RuntimeException(), &#34;Oh snap!&#34;, context)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Context propagation&lt;/h3&gt; &#xA;&lt;p&gt;All log methods expect some form of &lt;code&gt;LoggingContext&lt;/code&gt;. This can be propagated several ways depending on the &lt;code&gt;F&lt;/code&gt; you use as effect.&lt;/p&gt; &#xA;&lt;h4&gt;Implicit parameters&lt;/h4&gt; &#xA;&lt;p&gt;The most straightforward way is to manually propagate the &lt;code&gt;LoggingContext&lt;/code&gt; across functions&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def handleRequest[F[_]: Monad: Logging](request: Request)(implicit context: LoggingContext): F[Response] = {&#xA;  log.debug(&#34;Received request&#34;) *&amp;gt;&#xA;    doStuffInDb(request.user) *&amp;gt;&#xA;    respondWith200&#xA;}&#xA;&#xA;def doStuffInDb[F[_]: Monad: Logging](user: User)(implicit context: LoggingContext): F[Unit] =&#xA;  accessDb *&amp;gt;&#xA;    log.info(&#34;User accessed database&#34;, context.addParameters(&#34;user&#34; -&amp;gt; user.name))&#xA;&#xA;def main() = {&#xA;  CatsEffectLogging.createEffectLogger[IO](&#34;application&#34;).flatMap { implicit logging&#xA;    val request = ???&#xA;    implicit val context = LoggingContext(request.id)&#xA;&#xA;    handleRequest[IO](request)    &#xA;  }  &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As you can see from the &#34;Request received&#34; log, you don&#39;t have to pass the context explicitly, but it certainly is an option. Propagating this way is simple, but extending the context is not easy while keeping it implicit, as declaring a new, modified &lt;code&gt;LoggingContext&lt;/code&gt; implicit inside a function will cause ambiguous implicit error.&lt;/p&gt; &#xA;&lt;h4&gt;Kleisli (ReaderT)&lt;/h4&gt; &#xA;&lt;p&gt;Arguably the most complicated method of passing context around is via the &lt;a href=&#34;https://typelevel.org/cats/datatypes/kleisli.html&#34;&gt;Kleisli&lt;/a&gt; monad transformer. This allows passing context around without any effect on the function signature.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;‼&lt;/span&gt; &lt;strong&gt;Kleisli is not &lt;a href=&#34;https://github.com/typelevel/cats/issues/2212&#34;&gt;stack safe&lt;/a&gt; for all operations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Accessing the request log is done through the &lt;code&gt;Context[F]&lt;/code&gt; typeclass, which is an alias of &lt;code&gt;Local[F, LoggingContext]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def handleRequest[F[_]: Monad: Logging: Context](request: Request): F[Response] = {&#xA;  log.debug(&#34;Received request&#34;) *&amp;gt;&#xA;    doStuffInDb(request.user) *&amp;gt;&#xA;    respondWith200&#xA;}&#xA;&#xA;def doStuffInDb[F[_]: Monad: Logging: Context](user: User): F[Unit] = for {&#xA;  context &amp;lt;- log.getContext&#xA;  _ &amp;lt;- accessDb *&amp;gt; log.info(&#34;User accessed database&#34;, context.addParameters(&#34;user&#34; -&amp;gt; user.name))&#xA;} yield ()&#xA;&#xA;def main() = {&#xA;  CatsEffectLogging.createEffectLoggerG[LoggedIO, IO](&#34;application&#34;).flatMap { implicit logging&#xA;    val request = ???&#xA;    val context = LoggingContext(request.id)&#xA;&#xA;    handleRequest[LoggedIO](request).run(context)    &#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Fiber local data (Cats Effect 3 only)&lt;/h4&gt; &#xA;&lt;p&gt;Cats Effect 3 supports fiber local data through the new &lt;code&gt;IOLocal&lt;/code&gt; class. This class allows storing globally accessible data belonging to a single fiber.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def handleRequest[F[_]: Monad: Logging: Context](request: Request): F[Response] = {&#xA;  log.debug(&#34;Received request&#34;) *&amp;gt;&#xA;    doStuffInDb(request.user) *&amp;gt;&#xA;    respondWith200&#xA;}&#xA;&#xA;def doStuffInDb[F[_]: Monad: Logging: Context](user: User): F[Unit] = for {&#xA;  context &amp;lt;- log.getContext&#xA;  _ &amp;lt;- accessDb *&amp;gt; log.info(&#34;User accessed database&#34;, context.addParameters(&#34;user&#34; -&amp;gt; user.name))&#xA;} yield ()&#xA;&#xA;def main() = {&#xA;  CatsEffectLogging.createEffectLogger[IO](&#34;application&#34;).flatMap { implicit logging&#xA;    val mainContext = LoggingContext(&#34;main&#34;)&#xA;    CatsEffectLogging.createIOLocalContext(context).flatMap { implicit context =&amp;gt;&#xA;      val request = ???&#xA;      val context = LoggingContext(request.id)&#xA;      &#xA;      log.setContext(context) {&#xA;        handleRequest[IO](request)&#xA;      }&#xA;    }&#xA;  }  &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This method has the advantage of being faster than Kleisli and it is stack safe.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Every time you call &lt;code&gt;createIOLocalContext&lt;/code&gt;, an &lt;code&gt;IOLocal&lt;/code&gt; gets permanently associated to the current running fiber. This means calling it several times on the same fiber will cause memory leak if you reuse that fiber. You should prefer creating one context and changing it&#39;s contents using log.setContext(...).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Manipulating context&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;log.extendContext(&#34;user&#34; -&amp;gt; user.name) {&#xA;  log.info(&#34;hello1&#34;) // will log user name&#xA;} *&amp;gt;&#xA;  log.info(&#34;hello2&#34;) // will not log user name&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>apache/incubator-pekko-grpc</title>
    <updated>2023-05-15T01:42:29Z</updated>
    <id>tag:github.com,2023-05-15:/apache/incubator-pekko-grpc</id>
    <link href="https://github.com/apache/incubator-pekko-grpc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Pekko gRPC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Pekko gRPC&lt;/h1&gt; &#xA;&lt;p&gt;Support for building streaming gRPC servers and clients on top of Apache Pekko Streams.&lt;/p&gt; &#xA;&lt;p&gt;This library is meant to be used as a building block in projects using the Pekko toolkit.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pekko.apache.org/docs/pekko-grpc/current/&#34;&gt;Pekko gRPC reference&lt;/a&gt; documentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Status&lt;/h2&gt; &#xA;&lt;p&gt;This library is ready to be used in production, but API&#39;s and build system plugins are still expected to be improved and &lt;a href=&#34;https://doc.akka.io/docs/akka/current/common/may-change.html&#34;&gt;may change&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The API on both sides (Client and Server) is a simple Pekko Streams-based one.&lt;/p&gt; &#xA;&lt;p&gt;The client side is currently implemented on top of &lt;a href=&#34;https://mvnrepository.com/artifact/io.grpc/grpc-netty-shaded&#34;&gt;io.grpc:grpc-netty-shaded&lt;/a&gt;, we plan to replace this by just &lt;a href=&#34;https://mvnrepository.com/artifact/io.grpc/grpc-core&#34;&gt;io.grpc:grpc-core&lt;/a&gt; and @extref&lt;a href=&#34;pekko-http:&#34;&gt;Pekko HTTP&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As for performance, we are currently relying on the JVM TLS implementation, which is sufficient for many use cases, but is planned to be replaced with &lt;a href=&#34;https://github.com/google/conscrypt&#34;&gt;conscrypt&lt;/a&gt; or &lt;a href=&#34;https://netty.io/wiki/forked-tomcat-native.html&#34;&gt;netty-tcnative&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;General overview&lt;/h2&gt; &#xA;&lt;p&gt;gRPC is a schema-first RPC framework, where your protocol is declared in a protobuf definition, and requests and responses will be streamed over an HTTP/2 connection.&lt;/p&gt; &#xA;&lt;p&gt;Based on a protobuf service definition, pekko-grpc can generate:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model classes (using plain protoc for Java or scalapb for Scala)&lt;/li&gt; &#xA; &lt;li&gt;The API (as an interface for Java or a trait for Scala), expressed in Pekko Streams &lt;code&gt;Source&lt;/code&gt;s&lt;/li&gt; &#xA; &lt;li&gt;On the server side, code to create a Pekko HTTP route based on your implementation of the API&lt;/li&gt; &#xA; &lt;li&gt;On the client side, a client for the API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;p&gt;The project is split up in a number of subprojects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;codegen: code generation shared among plugins&lt;/li&gt; &#xA; &lt;li&gt;runtime: run-time utilities used by the generated code&lt;/li&gt; &#xA; &lt;li&gt;sbt-plugin: the sbt plugin&lt;/li&gt; &#xA; &lt;li&gt;scalapb-protoc-plugin: the scalapb Scala model code generation packaged as a protoc plugin, to be used from gradle&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apache/incubator-pekko-grpc/main/interop-tests/README.md&#34;&gt;interop-tests&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, &#39;plugin-tester-java&#39; and &#39;plugin-tester-scala&#39; contain an example project in Java and Scala respectively, with both sbt and Gradle configurations.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility &amp;amp; support&lt;/h2&gt; &#xA;&lt;p&gt;If used with JDK 8 prior to version 1.8.0_251 you must add an &lt;a href=&#34;https://doc.akka.io/docs/akka-http/10.1/server-side/http2.html#application-layer-protocol-negotiation-alpn-&#34;&gt;ALPN agent&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Pekko gRPC is Open Source and available under the Apache 2 License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InsightEdge/insightedge</title>
    <updated>2023-05-15T01:42:29Z</updated>
    <id>tag:github.com,2023-05-15:/InsightEdge/insightedge</id>
    <link href="https://github.com/InsightEdge/insightedge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InsightEdge Core&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InsightEdge&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href=&#34;http://insightedge.io/docs/010/index.html&#34;&gt;User Guide&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Community:&lt;/strong&gt; &lt;a href=&#34;http://insightedge-slack.herokuapp.com/&#34;&gt;Slack Channel&lt;/a&gt;, &lt;a href=&#34;http://stackoverflow.com/questions/tagged/insightedge&#34;&gt;StackOverflow tag&lt;/a&gt;, &lt;a href=&#34;mailto:hello@insightedge.io&#34;&gt;Email&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Contributing:&lt;/strong&gt; &lt;a href=&#34;https://github.com/InsightEdge/insightedge/raw/branch-1.0/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Issue Tracker:&lt;/strong&gt; &lt;a href=&#34;https://insightedge.atlassian.net&#34;&gt;Jira&lt;/a&gt;&lt;br&gt; &lt;strong&gt;License:&lt;/strong&gt; &lt;a href=&#34;https://github.com/InsightEdge/insightedge/raw/master/LICENSE.md&#34;&gt;Apache 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InsightEdge&lt;/strong&gt; is a Spark distribution on top of in-memory &lt;a href=&#34;https://github.com/InsightEdge/insightedge-datagrid&#34;&gt;Data Grid&lt;/a&gt;. A single platform for analytical and transactional workloads.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Exposes Data Grid as Spark RDDs&lt;/li&gt; &#xA; &lt;li&gt;Saves Spark RDDs to Data Grid&lt;/li&gt; &#xA; &lt;li&gt;Full DataFrames and Dataset API support with persistence&lt;/li&gt; &#xA; &lt;li&gt;Geospatial API for RDD and DataFrames. Geospatial indexes.&lt;/li&gt; &#xA; &lt;li&gt;Transparent integration with SparkContext using Scala implicits&lt;/li&gt; &#xA; &lt;li&gt;Data Grid side filtering with ability apply indexes&lt;/li&gt; &#xA; &lt;li&gt;Running SQL queries in Spark over Data Grid&lt;/li&gt; &#xA; &lt;li&gt;Data locality between Spark and Data Grid nodes&lt;/li&gt; &#xA; &lt;li&gt;Storing MLlib models in Data Grid&lt;/li&gt; &#xA; &lt;li&gt;Continuously saving Spark Streaming computation to Data Grid&lt;/li&gt; &#xA; &lt;li&gt;Off-Heap persistence&lt;/li&gt; &#xA; &lt;li&gt;Interactive Web Notebook&lt;/li&gt; &#xA; &lt;li&gt;Python support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Building InsightEdge&lt;/h2&gt; &#xA;&lt;p&gt;InsightEdge is built using &lt;a href=&#34;https://maven.apache.org/&#34;&gt;Apache Maven&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, compile and install InsightEdge Core libraries:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# without unit tests&#xA;mvn clean install -DskipTests=true&#xA;&#xA;# with unit tests&#xA;mvn clean install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build InsightEdge zip distribution you need the following binary dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://xap.github.io/&#34;&gt;insightedge-datagrid 12.3.0&lt;/a&gt;: download a copy of the XAP 12.x Open Source Edition&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InsightEdge/insightedge-examples&#34;&gt;insightedge-examples&lt;/a&gt;: use the same branch as in this repo, find build instructions in repository readme&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InsightEdge/insightedge-zeppelin&#34;&gt;insightedge-zeppelin&lt;/a&gt;: use the same branch as in this repo, run &lt;code&gt;./dev/change_scala_version.sh 2.11&lt;/code&gt;, then build with &lt;code&gt;mvn clean install -DskipTests -P spark-2.1 -P scala-2.11 -P build-distr -Dspark.version=2.1.1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://spark.apache.org/downloads.html&#34;&gt;Apache Spark 2.3.0&lt;/a&gt;: download zip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Package InsightEdge distribution:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mvn clean package -P package-open -DskipTests=true -Ddist.spark=&amp;lt;path to spark.tgz&amp;gt; -Ddist.xap=file:///&amp;lt;path to xap.zip&amp;gt; -Ddist.zeppelin=&amp;lt;path to zeppelin.tar.gz&amp;gt; -Ddist.examples.target=&amp;lt;path to examples target&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The archive is generated under &lt;code&gt;insightedge-packager/target/open&lt;/code&gt; directory. The archive content is under &lt;code&gt;insightedge-packager/target/contents-community&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To run integration tests refer to the &lt;a href=&#34;https://github.com/InsightEdge/insightedge/wiki/Integration-tests&#34;&gt;wiki page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Build the project and start InsightEdge demo mode with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd insightedge-packager/target/contents-community&#xA;./bin/insightedge -demo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It starts Zeppelin at &lt;a href=&#34;http://127.0.0.1:9090&#34;&gt;http://127.0.0.1:9090&lt;/a&gt; with InsightEdge tutorial and example notebooks you can play with. The full documentation is available at &lt;a href=&#34;http://insightedge.io/docs/010/index.html&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>