<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Scala Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-29T01:36:07Z</updated>
  <subtitle>Daily Trending of Scala in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gemini-hlsw/explore</title>
    <updated>2022-11-29T01:36:07Z</updated>
    <id>tag:github.com,2022-11-29:/gemini-hlsw/explore</id>
    <link href="https://github.com/gemini-hlsw/explore" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Explore&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scala-steward.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Scala_Steward-helping-blue.svg?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAQCAMAAAARSr4IAAAAVFBMVEUAAACHjojlOy5NWlrKzcYRKjGFjIbp293YycuLa3pYY2LSqql4f3pCUFTgSjNodYRmcXUsPD/NTTbjRS+2jomhgnzNc223cGvZS0HaSD0XLjbaSjElhIr+AAAAAXRSTlMAQObYZgAAAHlJREFUCNdNyosOwyAIhWHAQS1Vt7a77/3fcxxdmv0xwmckutAR1nkm4ggbyEcg/wWmlGLDAA3oL50xi6fk5ffZ3E2E3QfZDCcCN2YtbEWZt+Drc6u6rlqv7Uk0LdKqqr5rk2UCRXOk0vmQKGfc94nOJyQjouF9H/wCc9gECEYfONoAAAAASUVORK5CYII=&#34; alt=&#34;Scala Steward badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Launch on local development&lt;/h2&gt; &#xA;&lt;p&gt;We are now using FontAwesome Pro which requires a license. To build the app locally request a TOKEN from the admins and you need to setup an env variable containing it like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export FONTAWESOME_NPM_AUTH_TOKEN=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-fish&#34;&gt;set -x FONTAWESOME_NPM_AUTH_TOKEN ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For auth to work you need your app to run on the &lt;code&gt;lucuma.xyz&lt;/code&gt; domain, the simplest way is to setup &lt;code&gt;/etc/host&lt;/code&gt; adding an alias for &lt;code&gt;localhost&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;127.0.0.1   localhost local.lucuma.xyz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively if you run a home DNS server you can provide a local alias. This has the benefit of opening testing to any device in the network&lt;/p&gt; &#xA;&lt;p&gt;First you need to build a copy of the fastLinkJS version of explore&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt explore/fastLinkJS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To launch explore you can use vite development server going to the &lt;code&gt;explore&lt;/code&gt; dir and calling the command &lt;code&gt;npx vite&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You may need to update your node modules via &lt;code&gt;npm install&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;With that you can open the app at: &lt;a href=&#34;http://local.lucuma.xyz:8080/&#34;&gt;http://local.lucuma.xyz:8080/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Test full deployment&lt;/h2&gt; &#xA;&lt;p&gt;In same cases you may want to test locally how the app looks without deploying. In that case you need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build a full link version of explore&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sbt explore/fullLinkJS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build it with vite and launch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx vite build &amp;amp;&amp;amp; npx vite preview&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As before you can now see the app locally but in a different port &lt;a href=&#34;http://local.lucuma.xyz:5000/&#34;&gt;http://local.lucuma.xyz:5000/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Bundle sizes&lt;/h2&gt; &#xA;&lt;p&gt;You can check the evolution of bundle sizes &lt;a href=&#34;https://app.bundlemon.dev/projects/61a698e5de59ab000954f941/reports?branch=master&amp;amp;resolution=all&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shafiab/HashtagCashtag</title>
    <updated>2022-11-29T01:36:07Z</updated>
    <id>tag:github.com,2022-11-29:/shafiab/HashtagCashtag</id>
    <link href="https://github.com/shafiab/HashtagCashtag" rel="alternate"></link>
    <summary type="html">&lt;p&gt;My Insight Data Engineering Fellowship project. I implemented a big data processing pipeline based on ​lambda architecture​, that aggregates Twitter and US stock market data for user sentiment analysis using open source tools - ​Apache Kafka ​for data ingestions, Apache Spark ​&amp; ​Spark Streaming ​for batch &amp; real-time processing, ​Apache Cassand…&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;#Cashtag&lt;/h1&gt; &#xA;&lt;h2&gt;Big data pipeline for user sentiment analysis on US stock market&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.hashtagcashtag.com&#34;&gt;www.hashtagcashtag.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;#Cashtag is a big data pipeline to aggregate twitter data relevant to different stocks for New York Stock Exchange (NYSE) and NASDAQ stock market and provides an analytic framework to perform user sentiment analysis on different stocks and finding the correlation with the corresponding stock price.&lt;/p&gt; &#xA;&lt;h2&gt;What #Cashtag Does&lt;/h2&gt; &#xA;&lt;p&gt;#Cashtag allows user to easily check the top trending stocks @twitter at different time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/tab1.png&#34; alt=&#34;#Cashtag Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Users can look into the historical data and discover the top trending stocks and sentiment of twitter users on that stock at different hours of the day. &lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/tab2.png&#34; alt=&#34;#Cashtag Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Users can also find the time series information about a stock - how many time the stock has been mentioned over time as well as corresponding sentiment. &lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/tab3.png&#34; alt=&#34;#Cashtag Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;#Cashtag also allows user to find the correlation betweent the number of mentions of a stock @twitter and the stocks price fluctuation over time. &lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/tab4.png&#34; alt=&#34;#Cashtag Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How #Cashtag Works&lt;/h1&gt; &#xA;&lt;p&gt;#Cashtag pipeline is based on λ architecture.The pipeline consists of an ingestion layer, batch layer, speed layer, serving layer and frontend. The pipeline diagram is shown below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/pipeline.png&#34; alt=&#34;Data Pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Ingestion&lt;/h2&gt; &#xA;&lt;p&gt;#Cashtag works by pulling twitter data and stock market data. #Cashtag uses the twitter streaming API. Due to the limitation of this API, the current version is limited to pulling data for about 250 stocks including NASDAQ 100, NYSE 100 and some other popular stocks from these to exchange.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/twitter.png&#34; alt=&#34;Raw Twitter File&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;#Cashtag fetches stock price information from &lt;a href=&#34;http://www.netfonds.no&#34;&gt;www.netfonds.no&lt;/a&gt; . The website provides sub-second interval level-1 stock prices delayed by an hour. #Cashtag fetches the stock information for each individual stock every 5 seconds. Some pre-processing is done on this stock information - e.g. adding a ticker information and a timestamp.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/netfonds.png&#34; alt=&#34;Raw Stock File&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A multi-consumer multi-topic Kafka instance acts as the data ingestion platform. Both twitter data and stock data are stored in the Kafka first. Python scripts are written to perform these tasks.&lt;/p&gt; &#xA;&lt;h1&gt;Source of Truth&lt;/h1&gt; &#xA;&lt;p&gt;Initially, the source of truth for #Cashtag was HDFS drive in the master EC2 node. Kafka consumer programs, written in python, fetch data from Kafka and write it to HDFS. Later, the source of truth was moved to Amazon S3 drives due to the ease of use.&lt;/p&gt; &#xA;&lt;h1&gt;Batch Layer&lt;/h1&gt; &#xA;&lt;p&gt;The choice of tool for batch layer in #Cashtag is Spark. Codes are written in Scala. Several batch layer jobs are running periodically throughout the day, where the batch layer jobs are collecting raw data from S3 disk and performing necessary tasks and saving the results in the Serving layer. Azkaban is the tool of choice in #Cashtag to perform scheduling of different batch jobs. The reason for choosing Azkaban over crontab was the nice visual user interface, the ease of parallelizing of different tasks in the flow as well as its ability to restart a program in case of failure - and of course the fact that its called Azkaban!&lt;/p&gt; &#xA;&lt;p&gt;Several taks are performed by the batch layer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;number of mentions of a stock over different time granularity - minute, hour, day, year etc.&lt;/li&gt; &#xA; &lt;li&gt;users&#39; sentiment of the stock over different time granularity&lt;/li&gt; &#xA; &lt;li&gt;top trending stocks at different time granularity&lt;/li&gt; &#xA; &lt;li&gt;computing the high, low, open, close and volume data of different stocks at different time granularity.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Sentiment Analysis&lt;/h2&gt; &#xA;&lt;p&gt;#Cashtag also performs a very simple sentiment analysis over different trending stocks. While the main motivation behind creating #Cashtag was to create the underlying data pipeline that will enable the end users - data scientist, analyst to perform advanced algorithmic analysis to find the sentimets, I also felt it would be interesting to showcase the ability of this platform by showing a simple sentiment analysis. For this task, #Cashtag looks for keywords in each tweets and provides a score of +1 for every positive word it encounters and a -1 for every negative word. The overall sentiment of that tweet then is the sum of all the score of all the words in the tweet.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/sentiment.png&#34; alt=&#34;Raw Stock File&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/batch_result0.png&#34; alt=&#34;Raw Stock File&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/batch_result1.png&#34; alt=&#34;Raw Stock File&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Speed Layer&lt;/h1&gt; &#xA;&lt;p&gt;Speed layer in #Cashtag performs multiple operations. The tool of choice for speed layer is Spark Streaming. Codes are written in scala.&lt;/p&gt; &#xA;&lt;h2&gt;Incremental algorithm to supplement batch layer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shafiab/HashtagCashtag/master/Figures/batch_streaming.png&#34; alt=&#34;Raw Stock File&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;One of the motivation behind having a speed layer in lambda architecture is to supplement the batch layer. The batch layer implements re-computation algorithm that works on the entire sets of raw data and compute the necessary results. Since batch layer is working on the entire raw data set, it is generally time intensive and takes long time to process the entire sets of data. In such cases, speed layer works on the most recent data and provide real time result for these data. While the algorithm in batch layer was re-computation algorithm, the algorithm in speed layer is incremental in nature.&lt;/p&gt; &#xA;&lt;p&gt;In #Cashtag, one of the job of the speed layer is to find the number of mentions of a stock at different granularity. The same operation is also performed in batch layer. In speed layer in spark streaming, the data work on a window of data and make use of the &#39;updateStateByKey&#39; reduce operation in Spark Streaming. Here, the key is ticker and minute level time. Whenever, the streaming job encounter a particular ticker at a particular time, it search for existing key. If found, it updates that key; if not found, it creates a new key.&lt;/p&gt; &#xA;&lt;h2&gt;Speed layer only operation&lt;/h2&gt; &#xA;&lt;p&gt;Speed layer also calculates the top trending stocks over the last 10 minutes - this is a dashboard only operation that is being updated every 5 seconds. The spark streaming reduce operation &#39;reduceByKeyAndWindow&#39; is specifally suitable for this task.&lt;/p&gt; &#xA;&lt;h1&gt;Serving Layer&lt;/h1&gt; &#xA;&lt;p&gt;The choice of database for serving layer in #Cashtag is Cassandra. #Cashtag deals with a lot of time series data. Time series data operation is an excellent use case for Cassandra. An excellent article on the use of time series data for Cassandra can be found in &lt;a href=&#34;http://planetcassandra.org/getting-started-with-time-series-data-modeling/&#34;&gt;http://planetcassandra.org/getting-started-with-time-series-data-modeling/&lt;/a&gt;. Data from batch layer and speed layer are saved in Cassandra. We can refer to these tables as batch view and real-time view.&lt;/p&gt; &#xA;&lt;p&gt;#Cashtag saves data in multiple de-normalized tables. For efficient read and write operation for both Spark and Spark Streaming to Cassandra connector, as well as for Cassandra to Frontend, the scheme design is very important. The schema for each table has been chosen carefully to make these operation efficient and simple. The schemas are described below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Twitter Time Series for Tab 3 and 4&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;partitioned by ticker symbol&lt;/li&gt; &#xA; &lt;li&gt;clustering order by (year, month, day, hour, minute) 2.Top Trending stocks for Tab 2&lt;/li&gt; &#xA; &lt;li&gt;partitioned by (year, month, day, hour)&lt;/li&gt; &#xA; &lt;li&gt;clustering order by number of mentions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Stock Time Series for Tab 4&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;partitioned by ticker symbol&lt;/li&gt; &#xA; &lt;li&gt;clustering order by (year, month, day, hour, minute)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The schema design for the rolling count dashboard in Tab 1 to represent the top trending stock in the last 10 minutes was quite tricky. Cassandra is generally a good choice for creating a rolling count dashboard due to its support for TTL (time to live) for each entry. Unfortunately, support for TTL is not available in the current Spark/Spark Streaming to Cassandra connector. As a result I need to improvise. While writing the top trending tickers to cassandra, I added a timestamp to each entry. The Cassandra table was partitioned by ranking and clustering ordered by the time stamp. In the dashboard, the query was to select the top five stock by the ranking id and ordered descendingly by the timestamp. This gives me the latest entries from the table.&lt;/p&gt; &#xA;&lt;h1&gt;Front End&lt;/h1&gt; &#xA;&lt;p&gt;The front end of the project is running on Flask server. The charts were created using HighStock and Google graphs. Bootstrap was the choice for rending CSS.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lensesio/stream-reactor</title>
    <updated>2022-11-29T01:36:07Z</updated>
    <id>tag:github.com,2022-11-29:/lensesio/stream-reactor</id>
    <link href="https://github.com/lensesio/stream-reactor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Streaming reference architecture for ETL with Kafka and Kafka-Connect. You can find more on http://lenses.io on how we provide a unified solution to manage your connectors, most advanced SQL engine for Kafka and Kafka Streams, cluster monitoring and alerting, and more.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/lensesio/stream-reactor/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt; &lt;a href=&#34;https://docs.lenses.io/connectors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs--orange.svg?&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Join us on slack &lt;a href=&#34;https://launchpass.com/lensesio&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lensesio/stream-reactor/master/images/slack.jpeg&#34; alt=&#34;Alt text&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Lenses for Apache Kafka&lt;/h1&gt; &#xA;&lt;p&gt;Lenses offers SQL (for data browsing and Kafka Streams), Kafka Connect connector management, cluster monitoring and more.&lt;/p&gt; &#xA;&lt;p&gt;You can find more on &lt;a href=&#34;http://www.lenses.io&#34;&gt;lenses.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Stream Reactor&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lensesio/stream-reactor/master/images/streamreactor-logo.png&#34; alt=&#34;Alt text&#34;&gt; &lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Flensesio%2Fstream-reactor?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Flensesio%2Fstream-reactor.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A collection of components to build a real time ingestion pipeline.&lt;/p&gt; &#xA;&lt;h2&gt;Kafka Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;From version 4.0.0 separate artifacts are built targeting Kafka 3.1 and Kafka 2.8.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kafka 3.1 (Confluent 7.1) - Stream Reactor 4.0.0 (Kafka 3.1 Build)&lt;/li&gt; &#xA; &lt;li&gt;Kafka 2.8 (Confluent 6.2) - Stream Reactor 4.0.0 (Kafka 2.8 Build)&lt;/li&gt; &#xA; &lt;li&gt;Kafka 2.5 (Confluent 5.5) - Stream reactor 2.0.0+&lt;/li&gt; &#xA; &lt;li&gt;Kafka 2.0 -&amp;gt; 2.4 (Confluent 5.4) - Stream reactor 1.2.7&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;DEPRECATION NOTICE&lt;/h2&gt; &#xA;&lt;p&gt;The following connectors have been deprecated and are no longer included in any release from 3.0.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Blockchain&lt;/li&gt; &#xA; &lt;li&gt;Bloomberg&lt;/li&gt; &#xA; &lt;li&gt;Rethink&lt;/li&gt; &#xA; &lt;li&gt;VoltDB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following connectors have been deprecated and are no longer included in any release from 4.0.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Coap&lt;/li&gt; &#xA; &lt;li&gt;Hive 1.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Connectors&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please take a moment and read the documentation and make sure the software prerequisites are met!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Connector&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Docs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AWS S3&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to AWS S3.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/4.0/integrations/connectors/stream-reactor/sinks/s3sinkconnector/&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AzureDocumentDb&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka and Azure Document Db.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/azuredocdb.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cassandra&lt;/td&gt; &#xA;   &lt;td&gt;Source&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Cassandra to Kafka.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/source/cassandra.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;*Cassandra&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Certified DSE Cassandra, copy data from Kafka to Cassandra.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/cassandra.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Elastic 6&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to Elastic Search 6.x w. tcp or http&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/elastic6.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FTP/HTTP&lt;/td&gt; &#xA;   &lt;td&gt;Source&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from FTP/HTTP to Kafka.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/source/ftp.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hazelcast&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to Hazelcast.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/hazelcast.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HBase&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to HBase.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/hbase.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hive&lt;/td&gt; &#xA;   &lt;td&gt;Source&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Hive/HDFS to Kafka.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/source/hive.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hive&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to Hive/HDFS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/hive.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InfluxDb&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to InfluxDb.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/4.0/integrations/connectors/stream-reactor/sinks/influxsinkconnector/&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kudu&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to Kudu.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/kudu.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JMS&lt;/td&gt; &#xA;   &lt;td&gt;Source&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from JMS topics/queues to Kafka.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/source/jms.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JMS&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to JMS.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/jms.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MongoDB&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to MongoDB.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/mongo.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MQTT&lt;/td&gt; &#xA;   &lt;td&gt;Source&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from MQTT to Kafka.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/source/mqtt.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MQTT&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to MQTT.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/mqtt.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pulsar&lt;/td&gt; &#xA;   &lt;td&gt;Source&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Pulsar to Kafka.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/source/pulsar.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pulsar&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to Pulsar.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/pulsar.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Redis&lt;/td&gt; &#xA;   &lt;td&gt;Sink&lt;/td&gt; &#xA;   &lt;td&gt;Copy data from Kafka to Redis.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.lenses.io/connectors/sink/redis.html&#34;&gt;Docs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Release Notes&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;em&gt;&lt;a href=&#34;https://docs.lenses.io/4.3/integrations/connectors/sr-release-notes/&#34;&gt;Stream Reactor Release Notes at Lenses Documentation&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;To build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt clean compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create assemblies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build a particular project for kafka 2.8:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt &#34;project cassandra-kafka-2-8&#34; compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build a particular project for kafka 3.1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt &#34;project cassandra-kafka-3-1&#34; compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test a particular project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt &#34;project cassandra-kafka-2-8&#34; test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a jar of a particular project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt &#34;project cassandra-kafka-2-8&#34; assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running E2E tests&lt;/h3&gt; &#xA;&lt;p&gt;If not already built, you must first build the connector archives:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt &#34;project cassandra-kafka-2-8&#34; assembly&#xA;sbt &#34;project elastic6-kafka-2-8&#34; assembly &#xA;sbt &#34;project mongodb-kafka-2-8&#34; assembly&#xA;sbt &#34;project redis-kafka-2-8&#34; assembly&#xA;&#xA;sbt &#34;project cassandra-kafka-3-1&#34; assembly &#xA;sbt &#34;project elastic6-kafka-3-1&#34; assembly &#xA;sbt &#34;project mongodb-kafka-3-1&#34; assembly &#xA;sbt &#34;project redis-kafka-3-1&#34; assembly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbt e2e:test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;d love to accept your contributions! Please use GitHub pull requests: fork the repo, develop and test your code, &lt;a href=&#34;http://karma-runner.github.io/1.0/dev/git-commit-msg.html&#34;&gt;semantically commit&lt;/a&gt; and submit a pull request. Thanks!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2Flensesio%2Fstream-reactor?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2Flensesio%2Fstream-reactor.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>